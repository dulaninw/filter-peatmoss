repo_url,filepath,commit_date,message
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2023-09-15T23:15:19Z,Keep task level checkpoint key name generic (#5330)
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2023-09-15T19:01:49Z,initial revision (#5328)
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2022-12-13T07:36:17Z,remove missing config entries when loading task from checkpoint (#4905)
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2022-12-12T16:53:56Z,"data2vec v2.0 (#4903)

data2v2c 2.0
Co-authored-by: Arun Babu <arbabu@fb.com>
Co-authored-by: Wei-Ning Hsu <wnhsu@csail.mit.edu>"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2022-06-22T21:03:17Z,"add check for OC version in fairseq

Summary: fairseq patches a omegaconf internal util function that no longer exists in OmegaConf 2.2. This is a fix to make it compatible with both versions.

Reviewed By: dianaml0

Differential Revision: D37323720

fbshipit-source-id: 1b15b86decc70776303afe4a9a4c63acfef27ffc"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2022-04-14T22:39:36Z,"Fix typo in exception value (#4334)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes typo

## PR review

## Did you have fun?

Pull Request resolved: https://github.com/pytorch/fairseq/pull/4334

Reviewed By: Mortimerp9

Differential Revision: D35503972

Pulled By: dianaml0

fbshipit-source-id: 09893de009d398e7a048ec89f757634ddc10139d"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2022-01-21T17:27:49Z,"Fix breakage from D33649708

Summary: https://www.internalfb.com/diff/D33649708 (https://github.com/pytorch/fairseq/commit/995c204337d16a6146a433cee360e5a5bfbc9a6f)?src_version_fbid=1030479880843010&dst_version_fbid=247617347518523&transaction_fbid=1601081576900014

Reviewed By: alexeib

Differential Revision: D33696937

fbshipit-source-id: 9a17610e3f4eb3dd2b2131a3f9fb42732a31b47f"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2022-01-20T08:02:16Z,"Data2vec prelim (#2929)

Summary:
Preliminaries for data2vec release, include some minor improvements and bug fixes

Most important change is that we now default to raising an exception when fields in config do not have a corresponding field in the model dataclass

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2929

Reviewed By: wnhsu

Differential Revision: D33649708

Pulled By: alexeib

fbshipit-source-id: 629bdb4c361550740b451c570c2005bb956c6fcb"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2022-01-19T03:29:52Z,"Decode using EMA model in IPL recipe

Summary: Add option to use the EMA model for decoding in transducer IPL recipe by passing --ipl-decode-ema. Note EMA should be enabled as in the diff D24238379 (https://github.com/pytorch/fairseq/commit/8feccf94412424a4683b01090de36fa77cb4951d) using options --store-ema --ema-start-update and --ema-decay.

Reviewed By: cruvadom

Differential Revision: D31983366

fbshipit-source-id: 2bf63b3f7d1b5fa8804b3a7e9bfab71a463ca957"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2022-01-07T08:39:11Z,"Formatting fix: get CI green (#2860)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Applies `black` and `isort` to files

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2860

Reviewed By: Mortimerp9

Differential Revision: D33456637

Pulled By: dianaml0

fbshipit-source-id: 560b8d3a8f589cbecc92d0d21163596b5d47d609"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-12-31T02:38:12Z,"add strict option to checkpoint_utils. load_pretrained_component_from_model()

Summary: Add strict option to checkpoint_utils. load_pretrained_component_from_model()

Reviewed By: sravyapopuri388

Differential Revision: D33304224

fbshipit-source-id: 2284a21dfea7810ec212f15daadeeeb45c6dca1b"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-12-17T00:11:19Z,"formatting fix (#2816)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
fix `black` failures

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2816

Reviewed By: alexeib

Differential Revision: D33172615

Pulled By: dianaml0

fbshipit-source-id: 36b141f42941670f1bfa981041d878042feb0428"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-12-11T00:55:12Z,"Add loading from HuggingFace Hub

Summary: Add loading from HuggingFace Hub. Revised from and to replace D32697723 (accepted).

Reviewed By: pipibjc, dianaml0

Differential Revision: D32964041

fbshipit-source-id: 39676aa0ecb10454ae76b70968d5abe96ab6da54"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-11-29T20:32:59Z,"Add linting with black (#2678)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2678

Reviewed By: Mortimerp9

Differential Revision: D32653381

Pulled By: dianaml0

fbshipit-source-id: 2810d14867cd7d64f4d340740e2b590b82de47fe"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-11-19T00:38:31Z,"Merge --use-ontology-for-* into --use-ontology

Summary:
There are three options for the ontology:
* `--use-ontology-for-training`
* `--use-ontology-for-validation`
* `--use-ontology-for-balancing`

The first two must always be set together.

In the past, I observed that it's best not to use ontology for data balancing even if we use ontology for training and validation. But now I no longer observe this.

Therefore, I'm merging all these three options into one (`--use-ontology`).

In addition, I'm also moving the logic of avoiding loading teacher models out of `checkpoint_utils.py`. If you want to load a student model without loading its teachers (e.g. for prediction only), specify `arg_overrides={""ignore_teachers"": True}` when calling `load_model_ensemble`.

Reviewed By: xiaoxiao26

Differential Revision: D32518830

fbshipit-source-id: 103c6458f7927ec5ca7470109c8f956c00f514a2"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-10-23T02:51:09Z,"set num_update before loading state dict (#2491)

Summary:
## What does this PR do?
Set `model.num_updates` in `load_model_ensemble_and_task()` before loading `state_dict`, like what's done in `fairseq/trainer.py`, because a model's `state_dict` may depend on `num_update`.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2491

Reviewed By: xuqiantong

Differential Revision: D31863368

Pulled By: wnhsu

fbshipit-source-id: c70051f898819cc43b02c9f5765429e9f194aed5"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-09-13T20:20:35Z,"Back out ""Fairseq needs to store and load metadata from model state_dict"" (#3861)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/3861

backout fairseq changes. fix with a suggested, more optimal changes in checkopint utils.

Reviewed By: zhengwy888

Differential Revision: D30886481

fbshipit-source-id: 12b6dd4d5107ab4371b73a58d9a044a17c733260"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-09-09T01:18:30Z,"Fairseq needs to store and load metadata from model state_dict

Summary:
## TL;DR
Fairseq checkpoint saving and loading should mirror torch's checkpoint by saving and loading ""state_dict()._metadata"".

## Long Story:

#### What happened:
During model loading and saving, Quantization-aware-training models in Pytorch encounters a weird bug that says state_dict ""fake_weight_quant.weight.min_val"" is mismatched to ""min_vals"".

#### What was the reason:
- We found the issue in that torch uses state_dict()._metadata to store module._version, but the metadata was never store in checkpoint, nor are they loaded during checkpoint loading in fairseq.

Reviewed By: frankseide

Differential Revision: D30649933

fbshipit-source-id: ce262486b9b95fbcece463fa05c4e1903d4232d7"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-09-01T19:29:51Z,"EMA

Summary:
Adds Exponential moving average (EMA) model for Kaizen semi-supervised training https://arxiv.org/abs/2106.07759

1. Add `ema.store_ema` to enable storing EMA. EMA will be written to extra_state in the state dict while saving checkpoint.
2. `ema.ema_start_update` to control when the EMA starts accumulating
3. Tasks can use `uses_ema` property to decide if the EMA should be passed to the task. (Default is False)
4. `load_ema_from_checkpoint` can be used to load EMA model in place of the model to be used for evalutation. Pyspeech has eval-ema option for this.

```
This module has the EMA class used to store a copy of the exponentially decayed
model params.

Typical usage of EMA class involves initializing an object using an existing
model (random or from a seed model) and setting the config like ema_decay,
ema_start_update which determine how the EMA model is updated. After every
update of the model i.e. at the end of the train_step, the EMA should be updated
by passing the new model to the EMA.step function. The EMA model state dict
can be stored in the extra state under the key of ""ema"" and dumped
into a checkpoint and loaded. The EMA object can be passed to tasks
by setting task.uses_ema property.
EMA is a smoothed/ensemble model which might have better performance
when used for inference or further fine-tuning. EMA class has a
reverse function to load the EMA params into a model and use it
like a regular model.
```

Reviewed By: cruvadom

Differential Revision: D24238379

fbshipit-source-id: 879d3ba5070a614b7d365f9503af357001e875b2"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-08-02T21:36:32Z,"fixing checkpoint config upgrade for generation print_alignment (#2125)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes config upgrade conditions for upgrading generation. print_alignment

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2125

Reviewed By: myleott

Differential Revision: D30049140

Pulled By: jingfeidu

fbshipit-source-id: e613821e94d0cdb876c35bc6e3fede7affbf4628"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-07-29T23:30:41Z,"seed random suffix in checkpoint to be consistent across shards

Summary:
Currently the random suffix for saving sharded checkpoints can be different for each shard when training with FSDP and use_sharded_state=True. This makes it difficult for downstream applications to load the checkpoints properly, since each shard may have different suffixes.

This diff seeds the random suffix to be consistent across shards

Reviewed By: zhengwy888

Differential Revision: D29951167

fbshipit-source-id: 65749357e62a28978f3b46b71767204a508e1f61"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-07-29T23:30:41Z,"use pathmanager to delete old checkpoints

Summary: --keep-best-checkpoints doesn't work for Manifold paths because the current code assumes normal paths. Lets use  PathManager to properly remove them.

Reviewed By: myleott, sshleifer

Differential Revision: D29947965

fbshipit-source-id: 237a7b5aaa8293bb203ad05937decdf3b3ae2fc0"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-07-29T23:30:40Z,"use suffix when saving best checkpoints with metric

Summary: This is needed when training with FSDP + sharded state, as the checkpoints should have `-shard0.pt`

Reviewed By: sshleifer

Differential Revision: D29947728

fbshipit-source-id: d2cb7c23e1e6d027d115cb734e2f2f1c34239eba"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-07-09T23:25:02Z,"Roll back os.path.abspath change

Reviewed By: donhusa

Differential Revision: D29641968

fbshipit-source-id: eca379158055f3e38e9c053b06db56842265e53a"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-07-08T22:27:57Z,"Fix static container (#2036)

Summary:
fixes StatefulContainer being static which is a problem when you load a checkpoint with task that already has the same keys in the container

also print full path to checkpoints when saving (useful with hydra) and crash if repeatedly failing to save a checkpoint

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2036

Reviewed By: arbabu123

Differential Revision: D29608430

Pulled By: alexeib

fbshipit-source-id: 1b65c8f839e02de9110af3ec53f1e7d48a4908f7"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-07-06T22:07:31Z,"fixes tests/test_train.py to mock checkpoint.save_dir config node (#3675)

Summary:
## What does this PR do?
Some downstream users reported that errors when passing Namespace to load_checkpoint().

A recent change made the assumption that the passed object is dict like (dict or DictConfig) that have a get function.
This changes that and make sure the mocked config have checkpoint.save_dir to allow the test to run.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3675

Reviewed By: omry

Differential Revision: D29564805

Pulled By: lematt1991

fbshipit-source-id: 89308811da382667f6c5d3152ee2d6480416ee62"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-07-01T13:37:47Z,"Hydra 1.1 compatibility: Use an explicit schema for the primary config (#3659)

Summary:
## What does this PR do?
Fixes compatibility with Hydra 1.1.
The result is compatible with both Hydra 1.0 and Hydra 1.1, and will allow a smoother migration to Hydra 1.1.

At this point I am not yet removing the restriction on the Hydra version from setup.py:
1. It depends on some Hydra 1.1 changes that are not yet released (It will be compatible with 1.1.1).
2. Upgrading will result in deprecation warnings, and fixing them will break compatibility with Hydra 1.0.

There will be some followup to make the code fully compatible with 1.1 once Hydra 1.1 is the default version in fbcode.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3659

Reviewed By: omry

Differential Revision: D29498036

Pulled By: lematt1991

fbshipit-source-id: 96999cde5daad6749ef4d3ddf6a36a1e984ff201"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-06-15T21:09:52Z,"Check attributes in trainer and checkpoint loading before using them (#1970)

Summary:
## What does this PR do?
Fixes None exception when some attributes in  don't exist in cfg.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1970

Reviewed By: alexeib

Differential Revision: D29140036

Pulled By: hikushalhere

fbshipit-source-id: 7d941bcae6bb000c281a43ca2cd0876a49912ab9"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-06-04T23:19:56Z,"Fix loading some TALNet models

Summary:
D28728718 cleaned up the ""kd_binary_cross_entropy"" criterion, but this caused loading old models trained with this criterion to fail.
This diff replaces the ""kd_binary_cross_entropy"" criterion with the ""wav2vec"" criterion when loading models, and fixes this error.

It also removes the ""log_keys"" argument if it's `None`. Some criteria (e.g. wav2vec) require this argument to be a list, and will supply a default value of `[]` when it's absent. The presence of the `None` value prevents the use of this default value and causes an error.

Differential Revision: D28901263

fbshipit-source-id: 9b33aed35e76d2c734d1d4e2cbca1ff193a8c920"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-06-04T00:49:12Z,"Teacher-student learning for TALNet

Summary:
This diff implements teacher-student learning for TALNet.

Three classes take part in the teacher-student learning:
* The task loads the teacher models;
* The model generates predictions using the teacher models, and mixes them with the original targets;
* The `Wav2VecCriterion` reads the mixed targets to compute the loss. However, it still uses the original targets to compute the MAP and MAUC metrics.

There are two types of teachers:
* Static teachers: a file that stores predictions on training data which have been produced by running a model offline;
* Dynamic teachers: model files that are loaded at the beginning of training and executed on the fly to produce predictions.

We actually no longer use static teachers. The code about static teachers are copied over from the `KnowledgeDistillationBinaryCrossEntropyCriterion` class. This class will be cleaned up in D28728718.

The teacher models are stored in the task object, and will not be saved into checkpoints.

Reviewed By: alexeib

Differential Revision: D28728707

fbshipit-source-id: 0fcfc00db2e7194a6f7ee687cad9fa72e82a028b"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-05-26T00:45:51Z,"support FSDP sharded_state checkpoint loading during inference

Summary:
using the very useful feature added by QuentinDuval https://github.com/facebookresearch/fairscale/pull/683/files , we can consolidate sharded states into a full regular states. this allows inferences on sharded state almost transparently.

The main complexity comes from trying to be smart about what kind of checkpoint the user wants to load. not sure if this is over-engineering
1. if the file checkpoint-shard0.pt exists, and `--checkpoint-shard-count` is > 1, then we load sharded FSDP checkpoint
2. if checkpoint-shard0.pt exists but --checkpoint-shard-count=1, we load consolidated FSDP checkpoint
3. if checkpoint-shard0.pt does not exist, but --checkpoint-shard-count > 1, we load model parallel checkpoint
4. otherwise we are loading a single, plain checkpoint.

In theory we could be even smarter and load shard0.pt to check how many more checkpoints are needed. this is not implemented, though it will save the user having to specify --checkpoint-shard-count.

Reviewed By: sshleifer

Differential Revision: D28563441

fbshipit-source-id: dcafcaa7c9eaf5c9ff94f55c16bb3424c98dfa59"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-05-22T07:22:05Z,"fixing s2t transformer and N-best checkpoint saving

Summary:
- fixing the default value for `encoder_freezing_updates` in s2t transformer
- fixing N-best checkpoint saving: the previous implementation compares the new checkpoint with only the previous best one but not the previous N best ones. This leads to suboptimal results on N-best checkpoint averaging.

Reviewed By: jmp84

Differential Revision: D28546493

fbshipit-source-id: 44ec6d5ab49347f392d71269c5dcfd154b00c11e"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-05-21T23:18:21Z,"attempt to make non-sharded FSDP checkpoint behave like regular checkpoint

Summary:
overall just wondering if feature is desirable. if it is, the next diff which supports loading sharded checkpoint into a consolidated state dict cleaner.

a couple advantages
1. allows resuming from other DDP trainers.
2. allows resuming into other DDP trainers. or FSDP of a different configuration.
3. none-sharded FSDP checkpoint can be loaded with regular load_model_ensemble_and_task()

For old training workflow that's not using `--use-sharded-state`, please rename the checkpoint to remove the ""-shard0"" for resuming training.

Reviewed By: sshleifer

Differential Revision: D28563032

fbshipit-source-id: ced72bed969319ab6306059721f56e29b2c3d892"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-04-14T08:50:07Z,"add keep_interval_updates_pattern

Summary:
Motivation:

I want to save checkpoints frequently, due to unreliable jobs in FB cluster that restart frequently. I want to do this without spamming Manifold storage, but still save some historical checkpoints (i.e. every 10k updates), so I can track how WER evolves over time.

To save frequently, I can use a small --save-interval-updates.

To delete old checkpoints to save storage, I can use --keep-interval-updates.

However, this deletes all old checkpoints. This is where --keep-interval-updates-pattern comes in. If I now do:

```
--save-interval-updates 1000
--keep-interval-updates 1
--keep-interval-updates-pattern 10000
```

This will:
1. checkpoint every 1000 updates so that job restarts don't impact us significantly
2. keep only the latest checkpoint to avoid saving a bunch of huge models in manifold
3. make an exception for #2 for every 10k updates so we can track WER over time

Reviewed By: myleott

Differential Revision: D27578403

fbshipit-source-id: 5aec2dc9a22778015f7a3daa017210190af81240"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-04-14T08:50:06Z,"enable manifold checkpoints with --keep-interval-updates

Summary: Useful to enable --keep-interval-updates with Manifold checkpoints

Reviewed By: myleott

Differential Revision: D27577116

fbshipit-source-id: 6d4ae5aaccc07ecaed8ba6a333b6ab78b148187a"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-03-30T01:02:50Z,"BASE layers (#1654)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1654

Reviewed By: myleott

Differential Revision: D27128074

Pulled By: shruti-bh

fbshipit-source-id: ac86d383cd53c9c9bdd946fea839a37b719d95e3"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-03-26T14:18:59Z,"FSDP uses new optimizer gathering to save optimizer state (#1744)

Summary:
- Full unflattened optimizer state dict is in `checkpoints/shard_0.pt`, other checkpoint files do not have the `last_optimizer_state` key.
- requires master version of fairscale (eventually fairscale>=0.3.3)

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1744

Reviewed By: myleott

Differential Revision: D27342305

Pulled By: sshleifer

fbshipit-source-id: 7442b8c6ed01599d8ab0050213e84051f4e98acd"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-03-04T21:32:44Z,"Move checkpoint state_dict creation into Trainer (#1666)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1666

Context: the checkpoint saving call stack has become a bit convoluted:
```
train.py
+ checkpoint_utils.save_checkpoint
 + trainer.save_checkpoint
  + checkpoint_utils.save_state
   + checkpoint_utils.torch_persistent_save
```

This diff slightly simplifies the checkpoint saving logic by exposing a `state_dict` method inside the Trainer. This simplifies the call stack to:
```
train.py
+ checkpoint_utils.save_checkpoint
 + trainer.save_checkpoint
  + checkpoint_utils.torch_persistent_save
```

This new structure is important for the FullyShardedDataParallel diff (next diff in the stack), since it enables the Trainer to save multiple checkpoints for the different optimizer state shards.

Test Plan:
- unit tests
- trained WMT En-De models; confirmed checkpoints save/load properly, resuming from a checkpoint gives identical results
- `buck test fblearner/flow/projects/langtech/translation:tests` (2 failures are in trunk too): https://www.internalfb.com/intern/testinfra/testconsole/testrun/2533274840914654/

Reviewed By: zhengwy888

Differential Revision: D26771146

Pulled By: myleott

fbshipit-source-id: 10f91979cd42205c1d8abcaa9ab56f63eba31e93"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-03-04T05:17:29Z,"minor fixes and improvements (#1671)

Summary:
there are a few changes here:
- convert config persisted in checkpoints into a plain dict when saving and back to omegaconf config when loading: this helps avoid compatibility issues between different versions of python, omegaconf, etc
- update checkpoints that have old print_alignment saved
- add lr_float to composite optimizer to enable sweeping on lr with auto sweepers like ax
- fixing some edge cases for config loading

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1671

Reviewed By: myleott

Differential Revision: D26791583

Pulled By: alexeib

fbshipit-source-id: 124dec74932052925c43b6a93130f4428803cb46"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-03-02T17:26:03Z,"ioPath async - opt-in Fairseq integration (#1635)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1635

**Summary:** Integrate ioPath's async writes feature into Fairseq checkpoint writing.

**Details:**
- Created new checkpoint config param `--write-checkpoints-asynchronously` with default value `False`. Aliased to `--save-async`.
- Added to `PathManager` class in `file_io.py` to include `PathManager.opena(...)` and `PathManager.async_close()`. These new methods use ioPath's async `PathManager`.

**Usage:**
```
python train.py --save-async
```
---------
NOTE: **QUESTIONS**
1. In the current implementation, we don't save `checkpoint_best` and `checkpoint_latest` since ioPath doesn't yet have a ""wait until a file is written and then copy/move it to another path"" feature. Is this okay for now?
2. Should I mimic the atomic vs non-atomic save structure that synchronous Fairseq checkpoint writes have?

**Note to Eric:** Keep this integration in check with D26375501.

Reviewed By: myleott

Differential Revision: D26467815

fbshipit-source-id: 50068ef7bf9a6d5cea4d5e0d13d672604dc4a6b0"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-02-11T22:00:25Z,"save task state in the checkpoint (#1562)

Summary:
this allows tasks to declare some properties they'd like to save in the checkpoint (such as a dictionary), which are loaded when checkpoint is restored.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1562

Test Plan: tested by training a new wav2vec model, then finetuning it, then decoding it and making sure the dict only loaded once, during fine tuning process (and was obtained from checkpoint for decoding)

Reviewed By: myleott, gwenzek

Differential Revision: D25937974

Pulled By: alexeib

fbshipit-source-id: b9908042f76ec8cda943f33885eb9b1f121662ae"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2021-02-02T23:51:11Z,"Fix the task data arg conversion to string.

Summary: We were getting some test failures on our end due to incompatibility of task data argument type. The actual exception is defined in this task: T83395097 and T83395052. Fixing the task data arg to be a string instead of list of strings.

Reviewed By: myleott

Differential Revision: D26205482

fbshipit-source-id: d29d1ee7c469177e8bdad7ca603938f8450bd81c"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-12-31T04:16:56Z,"Fix incorrect local cache for checkpoint_last.pt when training is restarted on the same host

Reviewed By: myleott

Differential Revision: D25719057

fbshipit-source-id: 2bf7dd93b6d223804da0326a0ed347e5e353f1f0"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-12-23T19:16:56Z,"fairseq checkpoint improvements

Reviewed By: myleott

Differential Revision: D25677238

fbshipit-source-id: b43075034c953491211f19a5464148de4758df83"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-12-18T19:45:08Z,"Refactor eval_lm to support library usage (#1513)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1513

Test Plan: Imported from OSS

Reviewed By: alexeib

Differential Revision: D25570467

Pulled By: myleott

fbshipit-source-id: 062f748e287797f4f01c605e0b544ef3e698851f"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-12-18T15:40:49Z,"Support atomic saves for checkpoints (#1520)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1520

Test Plan: Imported from OSS

Reviewed By: stephenroller

Differential Revision: D25632782

Pulled By: myleott

fbshipit-source-id: bdbe2aed6254d0b023b33f8027dfbd939f1fd271"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-12-16T01:47:42Z,"Fix loading of very old checkpoints (#1512)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1512

See https://github.com/pytorch/fairseq/issues/3032 for context

Test Plan: Imported from OSS

Reviewed By: alexeib

Differential Revision: D25570470

Pulled By: myleott

fbshipit-source-id: 9227b1ca36cd81ff72acdb5e03fd574e3e8769be"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-12-08T23:49:12Z,"fix wav2vec scripts (#1494)

Summary:
fixes #2942
+ docs + migration of old models

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1494

Reviewed By: myleott

Differential Revision: D25404601

Pulled By: alexeib

fbshipit-source-id: 092f145602522f8e7ea3eaa709bfe602a4d29d8b"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-12-05T15:37:51Z,"Rename optimization.min_lr -> optimization.stop_min_lr (#1486)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1486

Test Plan: Imported from OSS

Reviewed By: alexeib

Differential Revision: D25342181

Pulled By: myleott

fbshipit-source-id: 7d1cfb26334fff26d688648724ab073e5fb956f5"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-11-30T22:20:36Z,"Add/fix tests (#1468)

Summary:
- add test for loading ensemble checkpoints (and confirmed it fails if I revert: https://github.com/pytorch/fairseq/commit/265791b727b664d4d7da3abd918a3f6fb70d7337)
- add test for LayerDrop (and fix it)

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1468

Reviewed By: alexeib

Differential Revision: D25223272

Pulled By: myleott

fbshipit-source-id: 3f06f753605af251567c70d2961f5506ea423499"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-11-18T01:08:13Z,"fix loading ensembles (#1442)

Summary:
fixes loading ensembles. previous change used the state of the first model for all models in the ensemble

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1442

Reviewed By: chtran

Differential Revision: D25035706

Pulled By: alexeib

fbshipit-source-id: 9029999be0f1703efb1df20bec2890de59449f1f"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-11-11T18:15:10Z,"load dataset with saved task config (optionally) (#1423)

Summary:
this adds an argument to load_dataset that provides task configuration from the checkpoint. different tasks can decide what to do with it afterwards.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1423

Reviewed By: myleott

Differential Revision: D24875706

Pulled By: alexeib

fbshipit-source-id: 5bb1e2b7495520c456024dc7b0751b65cb05b473"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-11-09T23:46:00Z,"migrate wav2vec2 model (#1409)

Summary:
see title
also includes some minor bug fixes

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1409

Reviewed By: myleott

Differential Revision: D24822219

Pulled By: alexeib

fbshipit-source-id: b18f9a8af42ced37880c23dd6ad1ec4df3dfc040"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-11-05T23:29:33Z,"Move TPU grad reductions out of Trainer into TPUDistributedDataParallel (#1397)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1397

Data parallel command: `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --share-decoder-input-output-embed --lr 0.0001`

Data parallel before:
```
2020-11-04 08:20:13 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:20:13 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:20:13 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2020-11-04 08:20:13 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:20:14 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:20:14 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:20:14 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:20:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=5
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108002, wps=160870, ups=4.91, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.8, wps=517232, ups=15.77, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.4, wps=537322, ups=16.38, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492.1, wps=540488, ups=16.48, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603.2, wps=543411, ups=16.57, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=540359, ups=16.47, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:20 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=533446, ups=16.26, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:20 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.4, wps=539734, ups=16.46, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=6
```

Data parallel after:
```
2020-11-04 08:14:02 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:14:02 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:14:02 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2020-11-04 08:14:02 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:14:03 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:14:03 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:14:03 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:14:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108002, wps=157099, ups=4.79, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.8, wps=560049, ups=17.08, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.4, wps=558507, ups=17.03, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492.1, wps=514194, ups=15.68, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603.2, wps=552676, ups=16.85, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:09 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=546402, ups=16.66, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:09 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=508472, ups=15.5, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:09 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.4, wps=552493, ups=16.84, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=6
```

Data parallel command (no_c10d): `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --share-decoder-input-output-embed --lr 0.0001 --dp-backend no_c10d`

Data parallel before:
```
2020-11-04 08:19:25 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:19:25 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:19:25 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2020-11-04 08:19:25 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:19:25 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:19:26 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:19:26 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:19:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:19:31 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108001, wps=141659, ups=4.32, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.9, wps=503762, ups=15.36, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.5, wps=488599, ups=14.9, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492, wps=507855, ups=15.48, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603, wps=503270, ups=15.34, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=7
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=467778, ups=14.26, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=7
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=503800, ups=15.36, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=7
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.3, wps=468486, ups=14.28, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=7
```

Data parallel after:
```
2020-11-04 08:14:50 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:14:50 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:14:50 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2020-11-04 08:14:50 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:14:50 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:14:51 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:14:51 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:14:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108001, wps=137677, ups=4.2, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.9, wps=519541, ups=15.84, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.5, wps=517063, ups=15.76, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492, wps=490728, ups=14.95, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603, wps=505262, ups=15.41, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=508874, ups=15.52, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:57 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=518028, ups=15.79, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:57 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.3, wps=515996, ups=15.73, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=7
```

Model parallel command: `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm_megatron --decoder-layers 4 --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --model-parallel-size 2 --share-decoder-input-output-embed --lr 0.0001`

Model parallel before:
```
2020-11-04 08:18:38 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:18:38 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:18:38 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last-model_part-0.pt
2020-11-04 08:18:38 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:18:38 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:18:39 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:18:39 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:18:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:18:45 | INFO | train_inner | epoch 001:      2 / 7173 loss=55.997, ppl=7.19017e+16, wps=0, ups=0, wpb=16384, bsz=32, num_updates=1, lr=0.0001, gnorm=14.03, loss_scale=64, train_wall=1, wall=7
2020-11-04 08:18:45 | INFO | train_inner | epoch 001:      3 / 7173 loss=28.372, ppl=3.47501e+08, wps=48371.7, ups=2.95, wpb=16384, bsz=32, num_updates=2, lr=0.0001, gnorm=15.339, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      4 / 7173 loss=15.855, ppl=59276.8, wps=72422.5, ups=4.42, wpb=16384, bsz=32, num_updates=3, lr=0.0001, gnorm=4.189, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      5 / 7173 loss=14.713, ppl=26858.7, wps=72933.5, ups=4.45, wpb=16384, bsz=32, num_updates=4, lr=0.0001, gnorm=4.751, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      6 / 7173 loss=13.901, ppl=15299.7, wps=71974.8, ups=4.39, wpb=16384, bsz=32, num_updates=5, lr=0.0001, gnorm=4.361, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      7 / 7173 loss=13.312, ppl=10169.5, wps=72897.8, ups=4.45, wpb=16384, bsz=32, num_updates=6, lr=0.0001, gnorm=3.307, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:18:47 | INFO | train_inner | epoch 001:      8 / 7173 loss=12.914, ppl=7720.21, wps=73044.6, ups=4.46, wpb=16384, bsz=32, num_updates=7, lr=0.0001, gnorm=5.473, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:18:47 | INFO | train_inner | epoch 001:      9 / 7173 loss=12.56, ppl=6036.72, wps=73453.1, ups=4.48, wpb=16384, bsz=32, num_updates=8, lr=0.0001, gnorm=6.112, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:18:47 | INFO | train_inner | epoch 001:     10 / 7173 loss=12.116, ppl=4437.77, wps=73442.6, ups=4.48, wpb=16384, bsz=32, num_updates=9, lr=0.0001, gnorm=4.415, loss_scale=64, train_wall=0, wall=9
```

Model parallel after:
```
2020-11-04 08:12:09 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:12:09 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:12:09 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last-model_part-0.pt
2020-11-04 08:12:09 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:12:09 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:12:10 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:12:10 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:12:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      2 / 7173 loss=55.997, ppl=7.19017e+16, wps=0, ups=0, wpb=16384, bsz=32, num_updates=1, lr=0.0001, gnorm=14.03, loss_scale=64, train_wall=1, wall=8
2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      3 / 7173 loss=28.372, ppl=3.47501e+08, wps=53097, ups=3.24, wpb=16384, bsz=32, num_updates=2, lr=0.0001, gnorm=15.339, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      4 / 7173 loss=15.855, ppl=59276.8, wps=72355.5, ups=4.42, wpb=16384, bsz=32, num_updates=3, lr=0.0001, gnorm=4.189, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      5 / 7173 loss=14.713, ppl=26858.7, wps=70526.4, ups=4.3, wpb=16384, bsz=32, num_updates=4, lr=0.0001, gnorm=4.751, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      6 / 7173 loss=13.901, ppl=15299.7, wps=73063.5, ups=4.46, wpb=16384, bsz=32, num_updates=5, lr=0.0001, gnorm=4.361, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      7 / 7173 loss=13.312, ppl=10169.5, wps=73559.4, ups=4.49, wpb=16384, bsz=32, num_updates=6, lr=0.0001, gnorm=3.307, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      8 / 7173 loss=12.914, ppl=7720.21, wps=72693.2, ups=4.44, wpb=16384, bsz=32, num_updates=7, lr=0.0001, gnorm=5.473, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      9 / 7173 loss=12.56, ppl=6036.72, wps=73531.2, ups=4.49, wpb=16384, bsz=32, num_updates=8, lr=0.0001, gnorm=6.112, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:12:19 | INFO | train_inner | epoch 001:     10 / 7173 loss=12.116, ppl=4437.77, wps=73187.6, ups=4.47, wpb=16384, bsz=32, num_updates=9, lr=0.0001, gnorm=4.415, loss_scale=64, train_wall=0, wall=10
```

Test Plan: Imported from OSS

Reviewed By: ngoyal2707

Differential Revision: D24729295

Pulled By: myleott

fbshipit-source-id: beee8bdece3eaa0419a2e813990420411e507c75"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-10-23T07:07:33Z,"refactor dataclass related files, add proper types for static checkin… (#1371)

Summary:
- refactor dataclass/ hierarchy to make it a bit more sane (while avoiding circular references)
- add top level FairseqConfig
- change typehints to reflect the correct config type if it is known

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1371

Reviewed By: myleott

Differential Revision: D24469026

Pulled By: alexeib

fbshipit-source-id: 01f68918f761d51ec5216286b8959ad35f41a7b2"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-10-22T23:31:49Z,"rename remove_bpe to post_process; add aliasing (#1369)

Summary:
some binaries (e.g. speech based ones) used --post-process, some used --remove-bpe. --post-process seems more appropriate as it does more than just remove bpe at the moment. this renames remove_bpe to post_process, adds alias so existing command lines would work and adds checkpoint upgrades so they continue to work also.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1369

Reviewed By: myleott

Differential Revision: D24465040

Pulled By: alexeib

fbshipit-source-id: 1b3e388291ccc403e76e069ef6606b80ead863a7"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-10-20T07:32:26Z,"Enable Hydra configs in fairseq (#1343) (#1510)

Summary:
Pull Request resolved: https://github.com/facebookresearch/pytext/pull/1510

this is the main pr that switches on hydra functionality in fairseq

we migrate ""args"" object into omegaconf ""DictConfig"" at all legacy entry points

in addition this migrates various components from secondary registries (like bpe encoders and tokenizers) to make the migration smoother

i am going through code that references migrated fairseq components and changing it to inherit from ""Legacy*"" components instead. hopefully tests will catch most of this

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1343

Reviewed By: myleott

Differential Revision: D23973928

Pulled By: alexeib

fbshipit-source-id: dd9554981fff51ea75c1ff343874d1d6e61793c9"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-10-19T01:14:51Z,"Apply black+isort (#1357)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1357

Reviewed By: alexeib

Differential Revision: D24377772

fbshipit-source-id: 51581af041d42d62166b33a35a1a4228b1a76f0c"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-10-12T17:55:40Z,"Support generation with huge pipeline parallel Transformer models (#1297)

Summary:
## What is this PR about?
* Support loading sharded checkpoints (loading and saving checkpoints without sharding for 30B models runs OOM)
* Ability to provide a fixed dictionary for multilingual machine translation to match the dictionary used for training the checkpoint
* Support generation with PipelineParallelTransformer models

## Testing

```
python fairseq_cli/generate.py \
    /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted  \
    --batch-size 1 \
    --path /large_exp/angelafan/checkpoints/shru/mmmt_5b_checkpoints_for_master/checkpoint4.pt \
    -s en -t es --remove-bpe 'sentencepiece' --beam 5 \
    --task translation_multi_simple_epoch \
    --lang-pairs /private/home/angelafan/mmt/lang100_bt_pairs.txt \
    --decoder-langtok --encoder-langtok src --gen-subset valid --fp16 \
    --dataset-impl mmap \
    --distributed-world-size 1 --distributed-no-spawn \
    --pipeline-model-parallel \
    --pipeline-chunks 1 \
    --pipeline-encoder-balance '[26]' \
    --pipeline-encoder-devices '[0]' \
    --pipeline-decoder-balance '[26]' \
    --pipeline-decoder-devices '[0]' \
    --fixed-dictionary /private/home/shru/projects/fairseq-py-kakaoval-mmt-save-redist-gen/dict.100langs.txt
2020-09-23 23:18:27 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might ca
use misalignment in pretraining and finetuning.
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['af', 'am', 'ar', 'arbt', 'ast', 'az', 'azbt', 'ba', 'be', 'bebt', 'bg', 'bgbt', 'bn', 'bnbt', 'br', 'bs', 'ca', 'ceb', 'cs', 'csb
t', 'cy', 'da', 'de', 'debt', 'el', 'en', 'es', 'esbt', 'et', 'etbt', 'fa', 'fabt', 'ff', 'fi', 'fr', 'frbt', 'fy', 'ga', 'gd', 'gl', 'gu', 'ha', 'he', 'hebt', 'hi', 'hibt', 'hr', 'ht', 'hu', 'hubt', 'hy', 'hybt', 'id', 'ig', 'ilo', 'is',
 'it', 'ja', 'jabt', 'jv', 'ka', 'kabt', 'kk', 'km', 'kn', 'ko', 'kobt', 'lb', 'lg', 'ln', 'lo', 'lt', 'lv', 'mg', 'mk', 'mkbt', 'ml', 'mn', 'mr', 'mrbt', 'ms', 'msbt', 'my', 'ne', 'nebt', 'nl', 'no', 'ns', 'oc', 'or', 'pa', 'pl', 'ps', '
pt', 'ro', 'robt', 'ru', 'sd', 'si', 'sk', 'sl', 'so', 'sq', 'sr', 'srbt', 'ss', 'su', 'sv', 'sw', 'ta', 'th', 'tl', 'tn', 'tr', 'trbt', 'uk', 'ur', 'uz', 'vi', 'vibt', 'wo', 'xh', 'yi', 'yo', 'zh', 'zhbt', 'zu']
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [en] dictionary: 128112 types
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [es] dictionary: 128112 types
2020-09-23 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-09-23 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:en-es': 1}
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:en-es src_langtok: 128022; tgt_langtok: 128023
2020-09-23 23:18:27 | INFO | fairseq.data.data_utils | loaded 4231 examples from: /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted/valid.en-es.en
2020-09-23 23:18:27 | INFO | fairseq.data.data_utils | loaded 4231 examples from: /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted/valid.en-es.es
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted valid en-es 4231 examples
2020-09-23 23:18:27 | INFO | fairseq_cli.generate | loading model(s) from /large_exp/angelafan/checkpoints/mmmt_100_langs_new_dict_5b_model/checkpoint4.pt
balance=[29,22,1], devices=[0,1,0], chunks=8, checkpoint=always
2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler order indices time: 0:00:00.004368
2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler filter_by_size time: 0:00:00.057953
2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler batch_by_size time: 0:00:01.158644
2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:01.223290
2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
S-2521  __en__ No.
T-2521  No.
H-2521  -2.4406352043151855     y no.
D-2521  -2.4406352043151855     y no.
P-2521  -5.2677 -4.4289 -0.1922 -2.1578 -0.1565
S-2261  __en__ Why?
T-2261  ¿Por qué?
H-2261  -1.7077901363372803     ¿Y por qué?
D-2261  -1.7077901363372803     ¿Y por qué?
P-2261  -5.2733 -0.7970 -3.7643 -0.5474 -0.3339 -1.0629 -0.1757
```

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1297

Reviewed By: myleott, msbaines

Differential Revision: D23991647

Pulled By: shruti-bh

fbshipit-source-id: 81ea1af64cbe75c8b53e050d2c2339dcb70fe1eb"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-10-09T20:34:59Z,"Improve dictionary & checkpoint reading w/ local caching

Reviewed By: myleott

Differential Revision: D24148700

fbshipit-source-id: 666300639243688939e137be748f7b76fc3c21a6"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-09-02T01:17:33Z,"Initial support for ZeRO optimizer state sharding (#1259)

Summary:
FairseqOSS will work with any optimizer and dtype.

TODO(future PR):
* support reduce instead of all_reduce
* support gradient sharding
* support parameter sharding

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1259

Test Plan:
Verified that checkpoint save and restore work.

Verified that grad_norm, loss, and ppl are identical with and without
sharding enable.

Before:

$ fairseq-train --task language_modeling   data-bin/wikitext-103   --save-dir checkpoints/transformer_wikitext-103   --arch transformer_lm --share-decoder-input-output-embed   --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 512 --sample-break-mode none   --max-tokens 2048 --update-freq 16   --max-update 50000  --memory-efficient-fp16 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50
...
2020-08-27 22:24:51 | INFO | train_inner | epoch 001:     49 / 394 loss=18.84, ppl=469411, wps=269226, ups=1.03, wpb=262144, bsz=512, num_updates=45, lr=5.72388e-06, gnorm=5.769, loss_scale=8, train_wall=1, wall=68
2020-08-27 22:24:52 | INFO | train_inner | epoch 001:     50 / 394 loss=18.787, ppl=452312, wps=256992, ups=0.98, wpb=262144, bsz=512, num_updates=46, lr=5.84885e-06, gnorm=5.512, loss_scale=8, train_wall=1, wall=69
2020-08-27 22:24:53 | INFO | train_inner | epoch 001:     51 / 394 loss=18.74, ppl=437735, wps=259178, ups=0.99, wpb=262144, bsz=512, num_updates=47, lr=5.97383e-06, gnorm=5.298, loss_scale=8, train_wall=1, wall=70
2020-08-27 22:24:54 | INFO | train_inner | epoch 001:     52 / 394 loss=18.683, ppl=420727, wps=257710, ups=0.98, wpb=262144, bsz=512, num_updates=48, lr=6.0988e-06, gnorm=5.094, loss_scale=8, train_wall=1, wall=71
2020-08-27 22:24:55 | INFO | train_inner | epoch 001:     53 / 394 loss=18.623, ppl=403794, wps=269279, ups=1.03, wpb=262144, bsz=512, num_updates=49, lr=6.22378e-06, gnorm=4.893, loss_scale=8, train_wall=1, wall=72
2020-08-27 22:24:56 | INFO | train_inner | epoch 001:     54 / 394 loss=18.574, ppl=390255, wps=264616, ups=1.01, wpb=262144, bsz=512, num_updates=50, lr=6.34875e-06, gnorm=4.684, loss_scale=8, train_wall=1, wall=73
2020-08-27 22:24:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-08-27 22:24:56 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-08-27 22:24:56 | INFO | train | epoch 001 | loss 19.736 | ppl 873122 | wps 264825 | ups 1.01 | wpb 262144 | bsz 512 | num_updates 50 | lr 6.34875e-06 | gnorm 8.898 | loss_scale 8 | train_wall 66 | wall 73
2020-08-27 22:24:56 | INFO | fairseq_cli.train | done training in 72.2 seconds

After:

$ fairseq-train --task language_modeling   data-bin/wikitext-103   --save-dir checkpoints/transformer_wikitext-103   --arch transformer_lm --share-decoder-input-output-embed   --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 512 --sample-break-mode none   --max-tokens 2048 --update-freq 16   --max-update 50000  --memory-efficient-fp16 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --zero-sharding os
...
2020-08-27 22:22:55 | INFO | train_inner | epoch 001:     49 / 394 loss=18.84, ppl=469411, wps=267663, ups=1.02, wpb=262144, bsz=512, num_updates=45, lr=5.72388e-06, gnorm=5.769, loss_scale=8, train_wall=1, wall=68
2020-08-27 22:22:56 | INFO | train_inner | epoch 001:     50 / 394 loss=18.787, ppl=452312, wps=252797, ups=0.96, wpb=262144, bsz=512, num_updates=46, lr=5.84885e-06, gnorm=5.512, loss_scale=8, train_wall=1, wall=69
2020-08-27 22:22:57 | INFO | train_inner | epoch 001:     51 / 394 loss=18.74, ppl=437735, wps=267692, ups=1.02, wpb=262144, bsz=512, num_updates=47, lr=5.97383e-06, gnorm=5.298, loss_scale=8, train_wall=1, wall=70
2020-08-27 22:22:58 | INFO | train_inner | epoch 001:     52 / 394 loss=18.683, ppl=420727, wps=267507, ups=1.02, wpb=262144, bsz=512, num_updates=48, lr=6.0988e-06, gnorm=5.094, loss_scale=8, train_wall=1, wall=71
2020-08-27 22:22:59 | INFO | train_inner | epoch 001:     53 / 394 loss=18.623, ppl=403794, wps=254410, ups=0.97, wpb=262144, bsz=512, num_updates=49, lr=6.22378e-06, gnorm=4.893, loss_scale=8, train_wall=1, wall=72
2020-08-27 22:23:00 | INFO | train_inner | epoch 001:     54 / 394 loss=18.574, ppl=390255, wps=268234, ups=1.02, wpb=262144, bsz=512, num_updates=50, lr=6.34875e-06, gnorm=4.684, loss_scale=8, train_wall=1, wall=73
2020-08-27 22:23:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-08-27 22:23:00 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-08-27 22:23:00 | INFO | train | epoch 001 | loss 19.736 | ppl 873122 | wps 263570 | ups 1.01 | wpb 262144 | bsz 512 | num_updates 50 | lr 6.34875e-06 | gnorm 8.898 | loss_scale 8 | train_wall 66 | wall 73
2020-08-27 22:23:00 | INFO | fairseq_cli.train | done training in 72.3 seconds

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Reviewed By: myleott

Differential Revision: D23432082

Pulled By: msbaines

fbshipit-source-id: 6a020b25e36a3d9283582b7d89a6a53038e5b181"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-08-14T17:24:51Z,"Misc fixes (#2448)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2448

Reviewed By: ngoyal2707

Differential Revision: D23011193

Pulled By: myleott

fbshipit-source-id: 1a29481707108e4465aca78ec1581fb79f05efba"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-08-06T17:20:39Z,"Multilingual v1: Multilingual Training with multiple bitext and monolingual datasets: add finetuning options

Summary:
A first version of XLNMT multilingual project code release: Multilingual Training with multiple bitext

- Minor changes to
    - fairseq/checkpoint_utils.py to add finetuning option instead of using restore_file which will restore from original model when being requeued.

Reviewed By: myleott

Differential Revision: D22483494

fbshipit-source-id: 733300fd6a4d185e561c793ea668047c96f616c6"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-08-04T15:25:50Z,"Fixes checkpoint_path while loading a model-parallel checkpoint (#2365)

Summary:
Fixes https://github.com/pytorch/fairseq/issues/2351

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2365

Reviewed By: pipibjc

Differential Revision: D22727384

Pulled By: myleott

fbshipit-source-id: e2ff703181a6b8f10df9b4ee7aa3f9e128c04b4e"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-05-26T22:59:59Z,"Several small fixes (incl. set default --data-buffer-size=10) (#2163)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2163

Reviewed By: ngoyal2707

Differential Revision: D21665601

Pulled By: myleott

fbshipit-source-id: 47673ff7f07acf0002c4e28380aa08ff917618ee"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-05-11T17:34:42Z,"Generalize moving of tensors to CPU in checkpoints (#2098)

Summary:
This is needed for TPUs
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2098

Reviewed By: ngoyal2707

Differential Revision: D21455095

Pulled By: myleott

fbshipit-source-id: f0f88e715884f44bc254b71f7694ac90200d92fc"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-04-14T17:58:38Z,"update checkpoint mkdir behavior (issue #1986) (#2011)

Summary:
Create checkpoint directory in ```save_checkpoint()```instead of ```load_checkpoint()```.

https://github.com/pytorch/fairseq/issues/1986
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2011

Reviewed By: ngoyal2707

Differential Revision: D21017208

Pulled By: myleott

fbshipit-source-id: 4f76afc1751f987b8ab2c170ca306d07bd5ffe83"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-03-28T03:22:36Z,"adding code to load and save model parallel checkpoint (#1119)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1119

Reviewed By: myleott

Differential Revision: D20712488

fbshipit-source-id: 941ef251c9e2deb8933d88188fac56ee8c5be9b7"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-03-11T07:36:45Z,"PySpeech TALNet: Convert to JIT and quantize

Summary:
Update the TALNet model so it can be converted to JIT format and quantized by the `jit_pyspeech_nn_model` tool.

The updated model structure is slight incompatible with the model files saved before (the model structure has the extra parameters `fc_att.weight` and `fc_att.bias`), so old model files must be loaded with `strict=False`. This diff also makes changes to allow `strict=False` where necessary.

Also renames the options of `jit_pyspeech_nn_model` to make them conform better to convention.

Reviewed By: jay-mahadeokar

Differential Revision: D20369643

fbshipit-source-id: 0950a26abc20d0ae5929c8d0a03f83cab4a59200"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-03-07T17:12:14Z,"Fix epoch reporting when restoring checkpoint (#1075)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1075

Differential Revision: D20322672

Pulled By: myleott

fbshipit-source-id: 8845a10b10442bed77670b7740afa271123a3b5d"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-03-05T00:37:24Z,"Use 1-based indexing for epochs everywhere (#1053)

Summary:
We are somewhat inconsistent in whether we're using 0-based or 1-based indexing for epochs. This should fix things to be 0-based internally, with logging and checkpoint naming still using 1-based indexing.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1053

Reviewed By: spencerp

Differential Revision: D20160715

Pulled By: myleott

fbshipit-source-id: 4ed94f9c371e1bfe29bcfa087fa6756507d6e627"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-02-19T04:07:05Z,"tts synthesis script

Summary: add a synth.py to pyspeech to run tts synthesis for a particular text.

Differential Revision: D19786089

fbshipit-source-id: 2aadd004609dfbcf477e58bc01e2b05df19c0e26"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-01-22T23:36:28Z,"fblearner pyspeech manifold migration

Summary:
was planning to migrate everything at once. but because pyspeech and tuna diverged from D19060980, it's too complicated to do the migration.

migrate save_dir to manifold in fblearner, and copy from manifold to gluster after training to keep downstream workflows happy.

Reviewed By: zdavid1995

Differential Revision: D19433205

fbshipit-source-id: bd8d969c001952d85167a63f4d55fe97b93aceb5"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-01-20T20:15:27Z,"fix the problem of passing None to format() when val_loss is None (e.… (#1633)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ x] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1633

Differential Revision: D19470727

Pulled By: myleott

fbshipit-source-id: a0bbefe20b4692306c57104f3e545912e20055e6"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-01-17T00:14:45Z,"Switch to Python logging (+ lint) (#1627)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1627

Python logging offers a number of benefits, such as logging timestamps, better
cross-library compatibility, ability to add multiple output handlers, etc.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/646

Reviewed By: spencerp

Differential Revision: D15815620

Pulled By: myleott

fbshipit-source-id: 5e64e9929b5e4b9dd5bb49bcdf7c510631907134"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2020-01-16T09:59:15Z,"Split from PR#968. add --keep-best-checkpoints (#990)

Summary:
Fixes https://github.com/fairinternal/fairseq-py/issues/968. Split --keep-best-checkpoints from the original request.
Use scores as the names to save the checkpoints
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/990

Differential Revision: D19411250

Pulled By: MultiPath

fbshipit-source-id: 82b0db614208eee54c9c0e470ad7faa6481747d5"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2019-12-17T01:22:11Z,"More fully deprecate --raw-text and --lazy-load (fixes #1488)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/947

Differential Revision: D19084273

Pulled By: myleott

fbshipit-source-id: de80d9abfac8e3d813a9c9b343b41327c500344e"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2019-12-06T02:24:56Z,"Add wrapper abstraction on top of fvcore PathManager to insulate OSS compatibility

Reviewed By: myleott

Differential Revision: D18736914

fbshipit-source-id: 897ba7463f7a8ebc0969c01c57bfe04fbf9e71c8"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2019-12-02T21:26:48Z,"Apply Black auto-formatting

Reviewed By: sujitoc

Differential Revision: D18738392

fbshipit-source-id: b7b7b75ef97946786c463c1887ef9a8676f030e6"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2019-11-22T01:50:42Z,"Quick fix for Structured Dropout checkpointing (#1406)

Summary:
Here's a quick fix for https://github.com/pytorch/fairseq/issues/1403.

To keep it short, this fix allows the user to checkpoint a translation model properly after applying layer pruning on a restored transformer file.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1406

Differential Revision: D18637540

Pulled By: myleott

fbshipit-source-id: 0f5e91e05e6579f0f459bc5293e9b14cb267322d"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2019-11-21T00:52:59Z,"Refactor data sharding to be specified via caller of task rather than task itself

Summary: Modifying number of shards internally to disable data sharding for batch iteration is dangerous because the caller of these tasks is not limited to fairspeq/train. So therefore we should put the onus of data sharding properly on the caller rather than the task itself.

Reviewed By: myleott

Differential Revision: D18456424

fbshipit-source-id: d46be16c441c50082f9a768d0b259e6c28a4b67b"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2019-10-30T19:55:54Z,"layer drop

Summary: This diff enables layer drop in transformer decoder in production training pipeline (ptt_transformer). It builds on top of the fairseq implementation D18094657 added by Angela Fan, and added additional logic to handle corresponding dropping layers at test time in exported model.

Reviewed By: jhcross

Differential Revision: D18165586

fbshipit-source-id: 373ac00268a25fa9e412edcb483becdfe792d992"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2019-10-27T19:10:53Z,"adding layerdrop code for training, pruning, and readme (#890)

Summary:
TEST 1: EVALUATION TIME WORKS
checked
achieves correct model perplexity: 18.68

TEST 2: TRAINING NEW MODEL WORKS
checked

without layerdrop:
--decoder-layerdrop 0 OR no flag at all
| epoch 001:     10 / 11201 loss=27.469, nll_loss=27.469, ppl=185799477.36, wps=1764, ups=0, wpb=9216.000, bsz=3.000, num_updates=7, lr=0.0004376, gnorm=25.471, clip=1.000, oom=0.000, loss_scale=8.000, wall=37, train_wall=30
| epoch 001:     20 / 11201 loss=27.443, nll_loss=27.443, ppl=182500427.22, wps=2449, ups=0, wpb=9216.000, bsz=3.000, num_updates=17, lr=0.0010626, gnorm=25.273, clip=1.000, oom=0.000, loss_scale=8.000, wall=64, train_wall=57
| epoch 001:     30 / 11201 loss=27.404, nll_loss=27.404, ppl=177612215.78, wps=2720, ups=0, wpb=9216.000, bsz=3.000, num_updates=27, lr=0.0016876, gnorm=25.136, clip=1.000, oom=0.000, loss_scale=8.000, wall=91, train_wall=84
| epoch 001:     40 / 11201 loss=27.009, nll_loss=27.009, ppl=135079983.00, wps=2865, ups=0, wpb=9216.000, bsz=3.000, num_updates=37, lr=0.0023126, gnorm=24.311, clip=1.000, oom=0.000, loss_scale=8.000, wall=119, train_wall=112
| epoch 001:     50 / 11201 loss=26.418, nll_loss=26.418, ppl=89680259.41, wps=2952, ups=0, wpb=9216.000, bsz=3.000, num_updates=47, lr=0.0029376, gnorm=22.775, clip=1.000, oom=0.000, loss_scale=8.000, wall=147, train_wall=140

with layerdrop (regularization effect should be seen in PPL):
--decoder-layerdrop 0.2

| epoch 001:     10 / 11201 loss=25.186, nll_loss=25.186, ppl=38182937.27, wps=2428, ups=0, wpb=9216.000, bsz=3.000, num_updates=8, lr=0.0005001, gnorm=17.082, clip=1.000, oom=0.000, loss_scale=16.000, wall=30, train_wall=24
| epoch 001:     20 / 11201 loss=25.270, nll_loss=25.270, ppl=40451933.50, wps=3173, ups=0, wpb=9216.000, bsz=3.000, num_updates=18, lr=0.0011251, gnorm=17.162, clip=1.000, oom=0.000, loss_scale=16.000, wall=52, train_wall=45
| epoch 001:     30 / 11201 loss=25.349, nll_loss=25.349, ppl=42752256.68, wps=3454, ups=0, wpb=9216.000, bsz=3.000, num_updates=28, lr=0.0017501, gnorm=17.370, clip=1.000, oom=0.000, loss_scale=16.000, wall=75, train_wall=68
| epoch 001:     40 / 11201 loss=25.115, nll_loss=25.115, ppl=36343806.30, wps=3619, ups=0, wpb=9216.000, bsz=3.000, num_updates=38, lr=0.0023751, gnorm=16.945, clip=1.000, oom=0.000, loss_scale=16.000, wall=97, train_wall=90
| epoch 001:     50 / 11201 loss=24.804, nll_loss=24.804, ppl=29284345.78, wps=3716, ups=0, wpb=9216.000, bsz=3.000, num_updates=48, lr=0.0030001, gnorm=16.406, clip=1.000, oom=0.000, loss_scale=16.000, wall=119, train_wall=112

TEST 3: PICKING UP TRAINING FROM EXISTING MODEL
checked

| loaded checkpoint /checkpoint/angelafan/structured_0.1_block_8_sd02/checkpoint_last.pt (epoch 272 @ 381066 updates)
| loading train data for epoch 272
| loaded 1801350 examples from: /private/home/angelafan/lm_work/fairseq-py/data-bin/wikitext-103/train

TEST 4: EVALUATING EXISTING BERT MODEL REPROS RESULTS
| [input] dictionary: 50265 types
| [label] dictionary: 9 types
| Accuracy:  0.9231651376146789
achieves correct accuracy on SST2 for this model

TEST 5: TRAINING NEW BERT MODEL WORKS
checked and works

TEST 6: NMT

without layerdrop
--encoder-layerdrop 0 --decoder-layerdrop 0 OR combinations of flag specified and not specified

| epoch 001:     10 / 92203 loss=15.820, nll_loss=15.830, ppl=58267.93, wps=4902, ups=0, wpb=1477.818, bsz=51.636, num_updates=11, lr=1.47473e-06, gnorm=7.207, clip=0.000, oom=0.000, loss_scale=128.000, wall=60, train_wall=3
| epoch 001:     20 / 92203 loss=15.523, nll_loss=15.501, ppl=46359.29, wps=5037, ups=0, wpb=1496.476, bsz=45.333, num_updates=21, lr=2.72448e-06, gnorm=6.869, clip=0.000, oom=0.000, loss_scale=128.000, wall=63, train_wall=6
| epoch 001:     30 / 92203 loss=15.185, nll_loss=15.123, ppl=35695.79, wps=5085, ups=0, wpb=1519.355, bsz=44.645, num_updates=31, lr=3.97423e-06, gnorm=6.186, clip=0.000, oom=0.000, loss_scale=128.000, wall=66, train_wall=9
| epoch 001:     40 / 92203 loss=14.940, nll_loss=14.849, ppl=29505.60, wps=5116, ups=1, wpb=1521.244, bsz=42.927, num_updates=41, lr=5.22398e-06, gnorm=5.610, clip=0.000, oom=0.000, loss_scale=128.000, wall=69, train_wall=12
| epoch 001:     50 / 92203 loss=14.745, nll_loss=14.630, ppl=25346.87, wps=5070, ups=1, wpb=1507.961, bsz=41.725, num_updates=51, lr=6.47373e-06, gnorm=5.104, clip=0.000, oom=0.000, loss_scale=128.000, wall=71, train_wall=15

with layerdrop (regularization effect should be seen in PPL)

A) works with --encoder-layerdrop 0.2 --decoder-layerdrop 0.2
B) works with different settings --encoder-layerdrop 0.3 --decoder-layerdrop 0.5
C) works with one on and one off --encoder-layerdrop 0.2 --decoder-layerdrop 0

| epoch 001:     10 / 92203 loss=15.817, nll_loss=15.828, ppl=58158.54, wps=5355, ups=0, wpb=1477.818, bsz=51.636, num_updates=11, lr=1.47473e-06, gnorm=6.959, clip=0.000, oom=0.000, loss_scale=128.000, wall=59, train_wall=3
| epoch 001:     20 / 92203 loss=15.650, nll_loss=15.641, ppl=51111.63, wps=5515, ups=0, wpb=1496.476, bsz=45.333, num_updates=21, lr=2.72448e-06, gnorm=6.825, clip=0.000, oom=0.000, loss_scale=128.000, wall=61, train_wall=6
| epoch 001:     30 / 92203 loss=15.440, nll_loss=15.408, ppl=43491.58, wps=5602, ups=0, wpb=1519.355, bsz=44.645, num_updates=31, lr=3.97423e-06, gnorm=6.576, clip=0.000, oom=0.000, loss_scale=128.000, wall=64, train_wall=8
| epoch 001:     40 / 92203 loss=15.247, nll_loss=15.193, ppl=37457.14, wps=5676, ups=1, wpb=1521.244, bsz=42.927, num_updates=41, lr=5.22398e-06, gnorm=6.124, clip=0.000, oom=0.000, loss_scale=128.000, wall=67, train_wall=11
| epoch 001:     50 / 92203 loss=15.055, nll_loss=14.977, ppl=32259.92, wps=5598, ups=1, wpb=1507.961, bsz=41.725, num_updates=51, lr=6.47373e-06, gnorm=5.661, clip=0.000, oom=0.000, loss_scale=128.000, wall=69, train_wall=14

TEST 7: PRUNING TESTCASES

A) after adding the pruning flags, model can evaluate as a full model
checked, reaches correct PPL
num. model params: 246933504
| Evaluated 217646 tokens in 196.3s (1108.99 tokens/s)
| Loss: 2.9275, Perplexity: 18.68

B) after adding pruning flags, model can be pruned. this works with multiple flag settings
checked three cases:
num. model params: 146163712
| Evaluated 217646 tokens in 106.0s (2054.07 tokens/s)
| Loss: 3.0932, Perplexity: 22.05

num. model params: 209144832
| Evaluated 217646 tokens in 162.8s (1336.99 tokens/s)
| Loss: 2.9526, Perplexity: 19.16

C) model can pick up training if you want to finetune the pruned model
checked:
| loading train data for epoch 272
| loaded 1801350 examples from: /private/home/angelafan/lm_work/fairseq-py/data-bin/wikitext-103/train
| WARNING: overflow detected, setting loss scale to: 64.0
| WARNING: overflow detected, setting loss scale to: 32.0
| epoch 272:   1500 / 5601 loss=5.015, nll_loss=5.015, ppl=32.33, wps=11598, ups=1, wpb=18432.000, bsz=6.000, num_updates=98, lr=0.0061251, gnorm=0.613, clip=1.000, oom=0.000, loss_scale=32.000, wall=156, train_wall=252396

D) works with BERT
checked:
without specifying any flags, reproduces the correct standard accuracy
with flags, produces the correct pruned accuracy

| [input] dictionary: 50265 types
| [label] dictionary: 9 types
| Accuracy:  0.9231651376146789

| [input] dictionary: 50265 types
| [label] dictionary: 9 types
| Pruning model to specified layer configuration - this works best if the model was trained with LayerDrop
| Accuracy:  0.9220183486238532
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/890

Reviewed By: edunov

Differential Revision: D18094657

Pulled By: huihuifan

fbshipit-source-id: 2bbaa2ff0039e906782694fc2038b8c17a8693e7"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2019-10-12T04:53:11Z,"Added option to save checkpoints using Path Manager.

Summary: Added option to save checkpoints using Path Manager.

Reviewed By: hudeven

Differential Revision: D17392754

fbshipit-source-id: 4b8e556ef8455a1548e5a083d779ed809cd785be"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2019-10-09T21:53:27Z,"Fix data loading memory issue in pyspeech

Summary:
We currently shard data when creating the batch iterator. This means we first load all indicese/frame lengths/handles into memory, and then do the sharding. This makes it impossible to train on large datasets with a high amount of workers  because each worker will need to load the entire dataset into memory. For training on a million hours of data (i.e. semi-supervised or unsupervised approaches) this data loading just makes it flat out impossible to use 8 GPU's.

3 changes:

1. This diff modifies the data loading such that we do the sharding while we read the handles file, rather than later. This modification is done on a task-by-task basis, since the task specifies how the data is loaded. I've tried to make the code compatible with both sharding during handle loading and sharding during batch iteration. I've currently only done the sharding during handle loading for the aligned_training task.

2. To support data sharding at data loading time and the requirement that all shards must have exactly the same # of batches, I've added a method to do this synchronization where all shards with too many batches would just truncate the extra ones, similar to what we already do.

2. In fairspeq/train.py, we are actually loading the training dataset and batch iterator twice, once in train.py and once when loading the checkpoint (which we always do regardless if there is a checkpoint). This means double the loading time which can be painful for very large files. I've removed the extraneous loading in this diff as well.

Reviewed By: yqwangustc

Differential Revision: D17750715

fbshipit-source-id: 0e6e3d363525fa5661f1c784303390ea13f46377"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2019-09-20T16:34:58Z,"added multilingual masked LM training (#849)

Summary:
The multilingual-RoBERTa training is working with aconneau XLM data.

Two pieces remaining:

1) `XLM` limits batch to be from same language, I am not 100% sure about the reason for that, but should be easy to implement, basically we can add `batch_by_size_and_language` instead of default `batch_by_size` function. If it's not critical, I would want to leave it out as it keeps the code very clean and simple.

2) `sample_ratio` in `ConcatDataset` works with `int` by tiling the datasets based on ratio. Currently I am handling it by sounding off the ratio to `first decimal` and then multiplying by `10`. We can see if some such simple heuristics are good enough, there are other options (we can talk about them offline).
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/849

Differential Revision: D17162460

fbshipit-source-id: d967f3d872f7a1f0aa4ea418bd362b68af9e432f"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2019-08-21T20:43:01Z,"Parameterized criterions (#808)

Summary:
Support criterion with parameters, such as AutoSegmentationCriterion (ASG) used in wav2letter which has a transition matrix parameter. This is needed to integrate wav2letter's ASG into PySpeech.

With this diff, parameters in criterions will be:
(1) updated by optimizers, with a configurable learning rate
(2) saved and loaded from checkpoints, preserving backward compatibility for criterions without parameters
(3) synchronized across nodes in distributed training.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/808

Reviewed By: jcai1

Differential Revision: D16934097

Pulled By: okhonko

fbshipit-source-id: 121ec9382459385c6f9cbef3a8274bec1a434038"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2019-08-12T16:08:16Z,"Lint

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/817

Differential Revision: D16762905

Pulled By: myleott

fbshipit-source-id: d920595bec44ed26b72dfc6fbc15c0aa107b4e56"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2019-08-12T14:17:21Z,"Update --restore-file logic (partially fixes #999)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1007

Differential Revision: D16762490

Pulled By: myleott

fbshipit-source-id: d67137bcf581887850323d188bb4ea643a35ac9e"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2019-08-10T15:16:50Z,"Add WSC task and criterion

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1004

Differential Revision: D16751443

Pulled By: myleott

fbshipit-source-id: f70acd6c7be6d69da45b5b32fe4c4eff021539ab"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2019-08-01T12:55:57Z,"Update PyTorch Hub interface

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/782

Differential Revision: D16542256

Pulled By: myleott

fbshipit-source-id: ea3279e7a1ce4687a5914f32b76787c419be1ffa"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2019-07-30T14:48:23Z,"Relicense fairseq under MIT license (#786)

Summary:
The previous BSD+PATENTS license was controversial. We have been
approved to relicense fairseq under the MIT license.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/786

Differential Revision: D16560654

Pulled By: myleott

fbshipit-source-id: f78b1beb4f2895dd7b9bfc79f5f952a2bfb94034"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2019-07-29T23:06:26Z,"adding glue data preprocessing scripts (#771)

Summary:
1) Added glue data pre-processing script.
2) updated README with usage.

TODO:
1) releasing fairseq dictionary and remove hardcoded path.
2) remove hard-coded path for bpe-encoding,

myleott what do you recommend for above TODOs?
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/771

Reviewed By: myleott

Differential Revision: D16547679

Pulled By: myleott

fbshipit-source-id: 6a6562d9b6215523d048fdf3daee63ffac21e231"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2019-07-24T23:59:07Z,"check save_dir before beginning training

Summary: I sadly discovery that my checkpoint directory wasn't globally readable after 8 hours of training. Adding this check at the beginning of train loop to keep that from happening again!

Reviewed By: myleott

Differential Revision: D16455394

fbshipit-source-id: 35959aa058150b2afb63710c468d01ebc8a12b0c"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2019-07-19T20:13:43Z,"Rename _load_model_ensemble -> load_model_ensemble_and_task

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/738

Differential Revision: D16377803

Pulled By: myleott

fbshipit-source-id: 6beb2f78e7464b70ff65a965d2b747cdca0ca951"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2019-07-01T21:10:49Z,"Fixes checkpointing bug introduced in 89e077c

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/847

Differential Revision: D16075498

Pulled By: myleott

fbshipit-source-id: 62e27a8c4764f53f181c502674dfab1e6b0537e2"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2019-06-30T18:35:33Z,"Add additional options for configuring writing of checkpoints

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/697

Differential Revision: D16068465

Pulled By: myleott

fbshipit-source-id: c2563c3c682e7e8406e6d7c8e895d8afbec551eb"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2019-06-11T18:19:58Z,"Automatically fill in default values from add_args

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/797

Differential Revision: D15761071

Pulled By: myleott

fbshipit-source-id: 257d4a2297e83da7e59baed154dbafd6bfe614bf"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2019-05-30T18:41:40Z,"Add --reset-dataloader

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/613

Differential Revision: D15541384

Pulled By: myleott

fbshipit-source-id: ef2c0b0a51cdf37af2ccff0546f524d49f87e65d"
github.com/facebookresearch/fairseq,fairseq/checkpoint_utils.py,2019-05-17T21:25:56Z,"Small features + lint

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/588

Differential Revision: D15389638

Pulled By: myleott

fbshipit-source-id: 4632ce22d51dc2c74d250bae999630095d849701"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2022-06-22T21:03:17Z,"add check for OC version in fairseq

Summary: fairseq patches a omegaconf internal util function that no longer exists in OmegaConf 2.2. This is a fix to make it compatible with both versions.

Reviewed By: dianaml0

Differential Revision: D37323720

fbshipit-source-id: 1b15b86decc70776303afe4a9a4c63acfef27ffc"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2022-04-14T22:39:36Z,"Fix typo in exception value (#4334)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes typo

## PR review

## Did you have fun?

Pull Request resolved: https://github.com/pytorch/fairseq/pull/4334

Reviewed By: Mortimerp9

Differential Revision: D35503972

Pulled By: dianaml0

fbshipit-source-id: 09893de009d398e7a048ec89f757634ddc10139d"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2022-01-21T17:27:49Z,"Fix breakage from D33649708

Summary: https://www.internalfb.com/diff/D33649708 (https://github.com/pytorch/fairseq/commit/995c204337d16a6146a433cee360e5a5bfbc9a6f)?src_version_fbid=1030479880843010&dst_version_fbid=247617347518523&transaction_fbid=1601081576900014

Reviewed By: alexeib

Differential Revision: D33696937

fbshipit-source-id: 9a17610e3f4eb3dd2b2131a3f9fb42732a31b47f"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2022-01-20T08:02:16Z,"Data2vec prelim (#2929)

Summary:
Preliminaries for data2vec release, include some minor improvements and bug fixes

Most important change is that we now default to raising an exception when fields in config do not have a corresponding field in the model dataclass

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2929

Reviewed By: wnhsu

Differential Revision: D33649708

Pulled By: alexeib

fbshipit-source-id: 629bdb4c361550740b451c570c2005bb956c6fcb"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2022-01-19T03:29:52Z,"Decode using EMA model in IPL recipe

Summary: Add option to use the EMA model for decoding in transducer IPL recipe by passing --ipl-decode-ema. Note EMA should be enabled as in the diff D24238379 (https://github.com/pytorch/fairseq/commit/8feccf94412424a4683b01090de36fa77cb4951d) using options --store-ema --ema-start-update and --ema-decay.

Reviewed By: cruvadom

Differential Revision: D31983366

fbshipit-source-id: 2bf63b3f7d1b5fa8804b3a7e9bfab71a463ca957"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2022-01-07T08:39:11Z,"Formatting fix: get CI green (#2860)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Applies `black` and `isort` to files

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2860

Reviewed By: Mortimerp9

Differential Revision: D33456637

Pulled By: dianaml0

fbshipit-source-id: 560b8d3a8f589cbecc92d0d21163596b5d47d609"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-12-31T02:38:12Z,"add strict option to checkpoint_utils. load_pretrained_component_from_model()

Summary: Add strict option to checkpoint_utils. load_pretrained_component_from_model()

Reviewed By: sravyapopuri388

Differential Revision: D33304224

fbshipit-source-id: 2284a21dfea7810ec212f15daadeeeb45c6dca1b"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-12-17T00:11:19Z,"formatting fix (#2816)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
fix `black` failures

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2816

Reviewed By: alexeib

Differential Revision: D33172615

Pulled By: dianaml0

fbshipit-source-id: 36b141f42941670f1bfa981041d878042feb0428"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-12-11T00:55:12Z,"Add loading from HuggingFace Hub

Summary: Add loading from HuggingFace Hub. Revised from and to replace D32697723 (accepted).

Reviewed By: pipibjc, dianaml0

Differential Revision: D32964041

fbshipit-source-id: 39676aa0ecb10454ae76b70968d5abe96ab6da54"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-11-29T20:32:59Z,"Add linting with black (#2678)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2678

Reviewed By: Mortimerp9

Differential Revision: D32653381

Pulled By: dianaml0

fbshipit-source-id: 2810d14867cd7d64f4d340740e2b590b82de47fe"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-11-19T00:38:31Z,"Merge --use-ontology-for-* into --use-ontology

Summary:
There are three options for the ontology:
* `--use-ontology-for-training`
* `--use-ontology-for-validation`
* `--use-ontology-for-balancing`

The first two must always be set together.

In the past, I observed that it's best not to use ontology for data balancing even if we use ontology for training and validation. But now I no longer observe this.

Therefore, I'm merging all these three options into one (`--use-ontology`).

In addition, I'm also moving the logic of avoiding loading teacher models out of `checkpoint_utils.py`. If you want to load a student model without loading its teachers (e.g. for prediction only), specify `arg_overrides={""ignore_teachers"": True}` when calling `load_model_ensemble`.

Reviewed By: xiaoxiao26

Differential Revision: D32518830

fbshipit-source-id: 103c6458f7927ec5ca7470109c8f956c00f514a2"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-10-23T02:51:09Z,"set num_update before loading state dict (#2491)

Summary:
## What does this PR do?
Set `model.num_updates` in `load_model_ensemble_and_task()` before loading `state_dict`, like what's done in `fairseq/trainer.py`, because a model's `state_dict` may depend on `num_update`.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2491

Reviewed By: xuqiantong

Differential Revision: D31863368

Pulled By: wnhsu

fbshipit-source-id: c70051f898819cc43b02c9f5765429e9f194aed5"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-09-13T20:20:35Z,"Back out ""Fairseq needs to store and load metadata from model state_dict"" (#3861)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/3861

backout fairseq changes. fix with a suggested, more optimal changes in checkopint utils.

Reviewed By: zhengwy888

Differential Revision: D30886481

fbshipit-source-id: 12b6dd4d5107ab4371b73a58d9a044a17c733260"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-09-09T01:18:30Z,"Fairseq needs to store and load metadata from model state_dict

Summary:
## TL;DR
Fairseq checkpoint saving and loading should mirror torch's checkpoint by saving and loading ""state_dict()._metadata"".

## Long Story:

#### What happened:
During model loading and saving, Quantization-aware-training models in Pytorch encounters a weird bug that says state_dict ""fake_weight_quant.weight.min_val"" is mismatched to ""min_vals"".

#### What was the reason:
- We found the issue in that torch uses state_dict()._metadata to store module._version, but the metadata was never store in checkpoint, nor are they loaded during checkpoint loading in fairseq.

Reviewed By: frankseide

Differential Revision: D30649933

fbshipit-source-id: ce262486b9b95fbcece463fa05c4e1903d4232d7"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-09-01T19:29:51Z,"EMA

Summary:
Adds Exponential moving average (EMA) model for Kaizen semi-supervised training https://arxiv.org/abs/2106.07759

1. Add `ema.store_ema` to enable storing EMA. EMA will be written to extra_state in the state dict while saving checkpoint.
2. `ema.ema_start_update` to control when the EMA starts accumulating
3. Tasks can use `uses_ema` property to decide if the EMA should be passed to the task. (Default is False)
4. `load_ema_from_checkpoint` can be used to load EMA model in place of the model to be used for evalutation. Pyspeech has eval-ema option for this.

```
This module has the EMA class used to store a copy of the exponentially decayed
model params.

Typical usage of EMA class involves initializing an object using an existing
model (random or from a seed model) and setting the config like ema_decay,
ema_start_update which determine how the EMA model is updated. After every
update of the model i.e. at the end of the train_step, the EMA should be updated
by passing the new model to the EMA.step function. The EMA model state dict
can be stored in the extra state under the key of ""ema"" and dumped
into a checkpoint and loaded. The EMA object can be passed to tasks
by setting task.uses_ema property.
EMA is a smoothed/ensemble model which might have better performance
when used for inference or further fine-tuning. EMA class has a
reverse function to load the EMA params into a model and use it
like a regular model.
```

Reviewed By: cruvadom

Differential Revision: D24238379

fbshipit-source-id: 879d3ba5070a614b7d365f9503af357001e875b2"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-08-02T21:36:32Z,"fixing checkpoint config upgrade for generation print_alignment (#2125)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes config upgrade conditions for upgrading generation. print_alignment

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2125

Reviewed By: myleott

Differential Revision: D30049140

Pulled By: jingfeidu

fbshipit-source-id: e613821e94d0cdb876c35bc6e3fede7affbf4628"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-07-29T23:30:41Z,"seed random suffix in checkpoint to be consistent across shards

Summary:
Currently the random suffix for saving sharded checkpoints can be different for each shard when training with FSDP and use_sharded_state=True. This makes it difficult for downstream applications to load the checkpoints properly, since each shard may have different suffixes.

This diff seeds the random suffix to be consistent across shards

Reviewed By: zhengwy888

Differential Revision: D29951167

fbshipit-source-id: 65749357e62a28978f3b46b71767204a508e1f61"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-07-29T23:30:41Z,"use pathmanager to delete old checkpoints

Summary: --keep-best-checkpoints doesn't work for Manifold paths because the current code assumes normal paths. Lets use  PathManager to properly remove them.

Reviewed By: myleott, sshleifer

Differential Revision: D29947965

fbshipit-source-id: 237a7b5aaa8293bb203ad05937decdf3b3ae2fc0"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-07-29T23:30:40Z,"use suffix when saving best checkpoints with metric

Summary: This is needed when training with FSDP + sharded state, as the checkpoints should have `-shard0.pt`

Reviewed By: sshleifer

Differential Revision: D29947728

fbshipit-source-id: d2cb7c23e1e6d027d115cb734e2f2f1c34239eba"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-07-09T23:25:02Z,"Roll back os.path.abspath change

Reviewed By: donhusa

Differential Revision: D29641968

fbshipit-source-id: eca379158055f3e38e9c053b06db56842265e53a"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-07-08T22:27:57Z,"Fix static container (#2036)

Summary:
fixes StatefulContainer being static which is a problem when you load a checkpoint with task that already has the same keys in the container

also print full path to checkpoints when saving (useful with hydra) and crash if repeatedly failing to save a checkpoint

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2036

Reviewed By: arbabu123

Differential Revision: D29608430

Pulled By: alexeib

fbshipit-source-id: 1b65c8f839e02de9110af3ec53f1e7d48a4908f7"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-07-06T22:07:31Z,"fixes tests/test_train.py to mock checkpoint.save_dir config node (#3675)

Summary:
## What does this PR do?
Some downstream users reported that errors when passing Namespace to load_checkpoint().

A recent change made the assumption that the passed object is dict like (dict or DictConfig) that have a get function.
This changes that and make sure the mocked config have checkpoint.save_dir to allow the test to run.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3675

Reviewed By: omry

Differential Revision: D29564805

Pulled By: lematt1991

fbshipit-source-id: 89308811da382667f6c5d3152ee2d6480416ee62"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-07-01T13:37:47Z,"Hydra 1.1 compatibility: Use an explicit schema for the primary config (#3659)

Summary:
## What does this PR do?
Fixes compatibility with Hydra 1.1.
The result is compatible with both Hydra 1.0 and Hydra 1.1, and will allow a smoother migration to Hydra 1.1.

At this point I am not yet removing the restriction on the Hydra version from setup.py:
1. It depends on some Hydra 1.1 changes that are not yet released (It will be compatible with 1.1.1).
2. Upgrading will result in deprecation warnings, and fixing them will break compatibility with Hydra 1.0.

There will be some followup to make the code fully compatible with 1.1 once Hydra 1.1 is the default version in fbcode.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3659

Reviewed By: omry

Differential Revision: D29498036

Pulled By: lematt1991

fbshipit-source-id: 96999cde5daad6749ef4d3ddf6a36a1e984ff201"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-06-15T21:09:52Z,"Check attributes in trainer and checkpoint loading before using them (#1970)

Summary:
## What does this PR do?
Fixes None exception when some attributes in  don't exist in cfg.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1970

Reviewed By: alexeib

Differential Revision: D29140036

Pulled By: hikushalhere

fbshipit-source-id: 7d941bcae6bb000c281a43ca2cd0876a49912ab9"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-06-04T23:19:56Z,"Fix loading some TALNet models

Summary:
D28728718 cleaned up the ""kd_binary_cross_entropy"" criterion, but this caused loading old models trained with this criterion to fail.
This diff replaces the ""kd_binary_cross_entropy"" criterion with the ""wav2vec"" criterion when loading models, and fixes this error.

It also removes the ""log_keys"" argument if it's `None`. Some criteria (e.g. wav2vec) require this argument to be a list, and will supply a default value of `[]` when it's absent. The presence of the `None` value prevents the use of this default value and causes an error.

Differential Revision: D28901263

fbshipit-source-id: 9b33aed35e76d2c734d1d4e2cbca1ff193a8c920"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-06-04T00:49:12Z,"Teacher-student learning for TALNet

Summary:
This diff implements teacher-student learning for TALNet.

Three classes take part in the teacher-student learning:
* The task loads the teacher models;
* The model generates predictions using the teacher models, and mixes them with the original targets;
* The `Wav2VecCriterion` reads the mixed targets to compute the loss. However, it still uses the original targets to compute the MAP and MAUC metrics.

There are two types of teachers:
* Static teachers: a file that stores predictions on training data which have been produced by running a model offline;
* Dynamic teachers: model files that are loaded at the beginning of training and executed on the fly to produce predictions.

We actually no longer use static teachers. The code about static teachers are copied over from the `KnowledgeDistillationBinaryCrossEntropyCriterion` class. This class will be cleaned up in D28728718.

The teacher models are stored in the task object, and will not be saved into checkpoints.

Reviewed By: alexeib

Differential Revision: D28728707

fbshipit-source-id: 0fcfc00db2e7194a6f7ee687cad9fa72e82a028b"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-05-26T00:45:51Z,"support FSDP sharded_state checkpoint loading during inference

Summary:
using the very useful feature added by QuentinDuval https://github.com/facebookresearch/fairscale/pull/683/files , we can consolidate sharded states into a full regular states. this allows inferences on sharded state almost transparently.

The main complexity comes from trying to be smart about what kind of checkpoint the user wants to load. not sure if this is over-engineering
1. if the file checkpoint-shard0.pt exists, and `--checkpoint-shard-count` is > 1, then we load sharded FSDP checkpoint
2. if checkpoint-shard0.pt exists but --checkpoint-shard-count=1, we load consolidated FSDP checkpoint
3. if checkpoint-shard0.pt does not exist, but --checkpoint-shard-count > 1, we load model parallel checkpoint
4. otherwise we are loading a single, plain checkpoint.

In theory we could be even smarter and load shard0.pt to check how many more checkpoints are needed. this is not implemented, though it will save the user having to specify --checkpoint-shard-count.

Reviewed By: sshleifer

Differential Revision: D28563441

fbshipit-source-id: dcafcaa7c9eaf5c9ff94f55c16bb3424c98dfa59"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-05-22T07:22:05Z,"fixing s2t transformer and N-best checkpoint saving

Summary:
- fixing the default value for `encoder_freezing_updates` in s2t transformer
- fixing N-best checkpoint saving: the previous implementation compares the new checkpoint with only the previous best one but not the previous N best ones. This leads to suboptimal results on N-best checkpoint averaging.

Reviewed By: jmp84

Differential Revision: D28546493

fbshipit-source-id: 44ec6d5ab49347f392d71269c5dcfd154b00c11e"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-05-21T23:18:21Z,"attempt to make non-sharded FSDP checkpoint behave like regular checkpoint

Summary:
overall just wondering if feature is desirable. if it is, the next diff which supports loading sharded checkpoint into a consolidated state dict cleaner.

a couple advantages
1. allows resuming from other DDP trainers.
2. allows resuming into other DDP trainers. or FSDP of a different configuration.
3. none-sharded FSDP checkpoint can be loaded with regular load_model_ensemble_and_task()

For old training workflow that's not using `--use-sharded-state`, please rename the checkpoint to remove the ""-shard0"" for resuming training.

Reviewed By: sshleifer

Differential Revision: D28563032

fbshipit-source-id: ced72bed969319ab6306059721f56e29b2c3d892"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-04-14T08:50:07Z,"add keep_interval_updates_pattern

Summary:
Motivation:

I want to save checkpoints frequently, due to unreliable jobs in FB cluster that restart frequently. I want to do this without spamming Manifold storage, but still save some historical checkpoints (i.e. every 10k updates), so I can track how WER evolves over time.

To save frequently, I can use a small --save-interval-updates.

To delete old checkpoints to save storage, I can use --keep-interval-updates.

However, this deletes all old checkpoints. This is where --keep-interval-updates-pattern comes in. If I now do:

```
--save-interval-updates 1000
--keep-interval-updates 1
--keep-interval-updates-pattern 10000
```

This will:
1. checkpoint every 1000 updates so that job restarts don't impact us significantly
2. keep only the latest checkpoint to avoid saving a bunch of huge models in manifold
3. make an exception for #2 for every 10k updates so we can track WER over time

Reviewed By: myleott

Differential Revision: D27578403

fbshipit-source-id: 5aec2dc9a22778015f7a3daa017210190af81240"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-04-14T08:50:06Z,"enable manifold checkpoints with --keep-interval-updates

Summary: Useful to enable --keep-interval-updates with Manifold checkpoints

Reviewed By: myleott

Differential Revision: D27577116

fbshipit-source-id: 6d4ae5aaccc07ecaed8ba6a333b6ab78b148187a"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-03-30T01:02:50Z,"BASE layers (#1654)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1654

Reviewed By: myleott

Differential Revision: D27128074

Pulled By: shruti-bh

fbshipit-source-id: ac86d383cd53c9c9bdd946fea839a37b719d95e3"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-03-26T14:18:59Z,"FSDP uses new optimizer gathering to save optimizer state (#1744)

Summary:
- Full unflattened optimizer state dict is in `checkpoints/shard_0.pt`, other checkpoint files do not have the `last_optimizer_state` key.
- requires master version of fairscale (eventually fairscale>=0.3.3)

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1744

Reviewed By: myleott

Differential Revision: D27342305

Pulled By: sshleifer

fbshipit-source-id: 7442b8c6ed01599d8ab0050213e84051f4e98acd"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-03-04T21:32:44Z,"Move checkpoint state_dict creation into Trainer (#1666)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1666

Context: the checkpoint saving call stack has become a bit convoluted:
```
train.py
+ checkpoint_utils.save_checkpoint
 + trainer.save_checkpoint
  + checkpoint_utils.save_state
   + checkpoint_utils.torch_persistent_save
```

This diff slightly simplifies the checkpoint saving logic by exposing a `state_dict` method inside the Trainer. This simplifies the call stack to:
```
train.py
+ checkpoint_utils.save_checkpoint
 + trainer.save_checkpoint
  + checkpoint_utils.torch_persistent_save
```

This new structure is important for the FullyShardedDataParallel diff (next diff in the stack), since it enables the Trainer to save multiple checkpoints for the different optimizer state shards.

Test Plan:
- unit tests
- trained WMT En-De models; confirmed checkpoints save/load properly, resuming from a checkpoint gives identical results
- `buck test fblearner/flow/projects/langtech/translation:tests` (2 failures are in trunk too): https://www.internalfb.com/intern/testinfra/testconsole/testrun/2533274840914654/

Reviewed By: zhengwy888

Differential Revision: D26771146

Pulled By: myleott

fbshipit-source-id: 10f91979cd42205c1d8abcaa9ab56f63eba31e93"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-03-04T05:17:29Z,"minor fixes and improvements (#1671)

Summary:
there are a few changes here:
- convert config persisted in checkpoints into a plain dict when saving and back to omegaconf config when loading: this helps avoid compatibility issues between different versions of python, omegaconf, etc
- update checkpoints that have old print_alignment saved
- add lr_float to composite optimizer to enable sweeping on lr with auto sweepers like ax
- fixing some edge cases for config loading

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1671

Reviewed By: myleott

Differential Revision: D26791583

Pulled By: alexeib

fbshipit-source-id: 124dec74932052925c43b6a93130f4428803cb46"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-03-02T17:26:03Z,"ioPath async - opt-in Fairseq integration (#1635)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1635

**Summary:** Integrate ioPath's async writes feature into Fairseq checkpoint writing.

**Details:**
- Created new checkpoint config param `--write-checkpoints-asynchronously` with default value `False`. Aliased to `--save-async`.
- Added to `PathManager` class in `file_io.py` to include `PathManager.opena(...)` and `PathManager.async_close()`. These new methods use ioPath's async `PathManager`.

**Usage:**
```
python train.py --save-async
```
---------
NOTE: **QUESTIONS**
1. In the current implementation, we don't save `checkpoint_best` and `checkpoint_latest` since ioPath doesn't yet have a ""wait until a file is written and then copy/move it to another path"" feature. Is this okay for now?
2. Should I mimic the atomic vs non-atomic save structure that synchronous Fairseq checkpoint writes have?

**Note to Eric:** Keep this integration in check with D26375501.

Reviewed By: myleott

Differential Revision: D26467815

fbshipit-source-id: 50068ef7bf9a6d5cea4d5e0d13d672604dc4a6b0"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-02-11T22:00:25Z,"save task state in the checkpoint (#1562)

Summary:
this allows tasks to declare some properties they'd like to save in the checkpoint (such as a dictionary), which are loaded when checkpoint is restored.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1562

Test Plan: tested by training a new wav2vec model, then finetuning it, then decoding it and making sure the dict only loaded once, during fine tuning process (and was obtained from checkpoint for decoding)

Reviewed By: myleott, gwenzek

Differential Revision: D25937974

Pulled By: alexeib

fbshipit-source-id: b9908042f76ec8cda943f33885eb9b1f121662ae"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2021-02-02T23:51:11Z,"Fix the task data arg conversion to string.

Summary: We were getting some test failures on our end due to incompatibility of task data argument type. The actual exception is defined in this task: T83395097 and T83395052. Fixing the task data arg to be a string instead of list of strings.

Reviewed By: myleott

Differential Revision: D26205482

fbshipit-source-id: d29d1ee7c469177e8bdad7ca603938f8450bd81c"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-12-31T04:16:56Z,"Fix incorrect local cache for checkpoint_last.pt when training is restarted on the same host

Reviewed By: myleott

Differential Revision: D25719057

fbshipit-source-id: 2bf7dd93b6d223804da0326a0ed347e5e353f1f0"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-12-23T19:16:56Z,"fairseq checkpoint improvements

Reviewed By: myleott

Differential Revision: D25677238

fbshipit-source-id: b43075034c953491211f19a5464148de4758df83"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-12-18T19:45:08Z,"Refactor eval_lm to support library usage (#1513)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1513

Test Plan: Imported from OSS

Reviewed By: alexeib

Differential Revision: D25570467

Pulled By: myleott

fbshipit-source-id: 062f748e287797f4f01c605e0b544ef3e698851f"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-12-18T15:40:49Z,"Support atomic saves for checkpoints (#1520)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1520

Test Plan: Imported from OSS

Reviewed By: stephenroller

Differential Revision: D25632782

Pulled By: myleott

fbshipit-source-id: bdbe2aed6254d0b023b33f8027dfbd939f1fd271"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-12-16T01:47:42Z,"Fix loading of very old checkpoints (#1512)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1512

See https://github.com/pytorch/fairseq/issues/3032 for context

Test Plan: Imported from OSS

Reviewed By: alexeib

Differential Revision: D25570470

Pulled By: myleott

fbshipit-source-id: 9227b1ca36cd81ff72acdb5e03fd574e3e8769be"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-12-08T23:49:12Z,"fix wav2vec scripts (#1494)

Summary:
fixes #2942
+ docs + migration of old models

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1494

Reviewed By: myleott

Differential Revision: D25404601

Pulled By: alexeib

fbshipit-source-id: 092f145602522f8e7ea3eaa709bfe602a4d29d8b"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-12-05T15:37:51Z,"Rename optimization.min_lr -> optimization.stop_min_lr (#1486)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1486

Test Plan: Imported from OSS

Reviewed By: alexeib

Differential Revision: D25342181

Pulled By: myleott

fbshipit-source-id: 7d1cfb26334fff26d688648724ab073e5fb956f5"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-11-30T22:20:36Z,"Add/fix tests (#1468)

Summary:
- add test for loading ensemble checkpoints (and confirmed it fails if I revert: https://github.com/pytorch/fairseq/commit/265791b727b664d4d7da3abd918a3f6fb70d7337)
- add test for LayerDrop (and fix it)

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1468

Reviewed By: alexeib

Differential Revision: D25223272

Pulled By: myleott

fbshipit-source-id: 3f06f753605af251567c70d2961f5506ea423499"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-11-18T01:08:13Z,"fix loading ensembles (#1442)

Summary:
fixes loading ensembles. previous change used the state of the first model for all models in the ensemble

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1442

Reviewed By: chtran

Differential Revision: D25035706

Pulled By: alexeib

fbshipit-source-id: 9029999be0f1703efb1df20bec2890de59449f1f"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-11-11T18:15:10Z,"load dataset with saved task config (optionally) (#1423)

Summary:
this adds an argument to load_dataset that provides task configuration from the checkpoint. different tasks can decide what to do with it afterwards.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1423

Reviewed By: myleott

Differential Revision: D24875706

Pulled By: alexeib

fbshipit-source-id: 5bb1e2b7495520c456024dc7b0751b65cb05b473"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-11-09T23:46:00Z,"migrate wav2vec2 model (#1409)

Summary:
see title
also includes some minor bug fixes

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1409

Reviewed By: myleott

Differential Revision: D24822219

Pulled By: alexeib

fbshipit-source-id: b18f9a8af42ced37880c23dd6ad1ec4df3dfc040"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-11-05T23:29:33Z,"Move TPU grad reductions out of Trainer into TPUDistributedDataParallel (#1397)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1397

Data parallel command: `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --share-decoder-input-output-embed --lr 0.0001`

Data parallel before:
```
2020-11-04 08:20:13 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:20:13 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:20:13 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2020-11-04 08:20:13 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:20:14 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:20:14 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:20:14 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:20:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=5
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108002, wps=160870, ups=4.91, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.8, wps=517232, ups=15.77, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.4, wps=537322, ups=16.38, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492.1, wps=540488, ups=16.48, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603.2, wps=543411, ups=16.57, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=540359, ups=16.47, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:20 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=533446, ups=16.26, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:20 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.4, wps=539734, ups=16.46, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=6
```

Data parallel after:
```
2020-11-04 08:14:02 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:14:02 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:14:02 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2020-11-04 08:14:02 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:14:03 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:14:03 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:14:03 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:14:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108002, wps=157099, ups=4.79, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.8, wps=560049, ups=17.08, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.4, wps=558507, ups=17.03, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492.1, wps=514194, ups=15.68, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603.2, wps=552676, ups=16.85, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:09 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=546402, ups=16.66, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:09 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=508472, ups=15.5, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:09 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.4, wps=552493, ups=16.84, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=6
```

Data parallel command (no_c10d): `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --share-decoder-input-output-embed --lr 0.0001 --dp-backend no_c10d`

Data parallel before:
```
2020-11-04 08:19:25 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:19:25 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:19:25 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2020-11-04 08:19:25 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:19:25 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:19:26 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:19:26 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:19:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:19:31 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108001, wps=141659, ups=4.32, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.9, wps=503762, ups=15.36, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.5, wps=488599, ups=14.9, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492, wps=507855, ups=15.48, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603, wps=503270, ups=15.34, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=7
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=467778, ups=14.26, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=7
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=503800, ups=15.36, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=7
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.3, wps=468486, ups=14.28, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=7
```

Data parallel after:
```
2020-11-04 08:14:50 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:14:50 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:14:50 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2020-11-04 08:14:50 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:14:50 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:14:51 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:14:51 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:14:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108001, wps=137677, ups=4.2, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.9, wps=519541, ups=15.84, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.5, wps=517063, ups=15.76, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492, wps=490728, ups=14.95, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603, wps=505262, ups=15.41, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=508874, ups=15.52, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:57 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=518028, ups=15.79, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:57 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.3, wps=515996, ups=15.73, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=7
```

Model parallel command: `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm_megatron --decoder-layers 4 --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --model-parallel-size 2 --share-decoder-input-output-embed --lr 0.0001`

Model parallel before:
```
2020-11-04 08:18:38 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:18:38 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:18:38 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last-model_part-0.pt
2020-11-04 08:18:38 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:18:38 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:18:39 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:18:39 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:18:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:18:45 | INFO | train_inner | epoch 001:      2 / 7173 loss=55.997, ppl=7.19017e+16, wps=0, ups=0, wpb=16384, bsz=32, num_updates=1, lr=0.0001, gnorm=14.03, loss_scale=64, train_wall=1, wall=7
2020-11-04 08:18:45 | INFO | train_inner | epoch 001:      3 / 7173 loss=28.372, ppl=3.47501e+08, wps=48371.7, ups=2.95, wpb=16384, bsz=32, num_updates=2, lr=0.0001, gnorm=15.339, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      4 / 7173 loss=15.855, ppl=59276.8, wps=72422.5, ups=4.42, wpb=16384, bsz=32, num_updates=3, lr=0.0001, gnorm=4.189, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      5 / 7173 loss=14.713, ppl=26858.7, wps=72933.5, ups=4.45, wpb=16384, bsz=32, num_updates=4, lr=0.0001, gnorm=4.751, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      6 / 7173 loss=13.901, ppl=15299.7, wps=71974.8, ups=4.39, wpb=16384, bsz=32, num_updates=5, lr=0.0001, gnorm=4.361, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      7 / 7173 loss=13.312, ppl=10169.5, wps=72897.8, ups=4.45, wpb=16384, bsz=32, num_updates=6, lr=0.0001, gnorm=3.307, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:18:47 | INFO | train_inner | epoch 001:      8 / 7173 loss=12.914, ppl=7720.21, wps=73044.6, ups=4.46, wpb=16384, bsz=32, num_updates=7, lr=0.0001, gnorm=5.473, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:18:47 | INFO | train_inner | epoch 001:      9 / 7173 loss=12.56, ppl=6036.72, wps=73453.1, ups=4.48, wpb=16384, bsz=32, num_updates=8, lr=0.0001, gnorm=6.112, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:18:47 | INFO | train_inner | epoch 001:     10 / 7173 loss=12.116, ppl=4437.77, wps=73442.6, ups=4.48, wpb=16384, bsz=32, num_updates=9, lr=0.0001, gnorm=4.415, loss_scale=64, train_wall=0, wall=9
```

Model parallel after:
```
2020-11-04 08:12:09 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:12:09 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:12:09 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last-model_part-0.pt
2020-11-04 08:12:09 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:12:09 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:12:10 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:12:10 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:12:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      2 / 7173 loss=55.997, ppl=7.19017e+16, wps=0, ups=0, wpb=16384, bsz=32, num_updates=1, lr=0.0001, gnorm=14.03, loss_scale=64, train_wall=1, wall=8
2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      3 / 7173 loss=28.372, ppl=3.47501e+08, wps=53097, ups=3.24, wpb=16384, bsz=32, num_updates=2, lr=0.0001, gnorm=15.339, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      4 / 7173 loss=15.855, ppl=59276.8, wps=72355.5, ups=4.42, wpb=16384, bsz=32, num_updates=3, lr=0.0001, gnorm=4.189, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      5 / 7173 loss=14.713, ppl=26858.7, wps=70526.4, ups=4.3, wpb=16384, bsz=32, num_updates=4, lr=0.0001, gnorm=4.751, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      6 / 7173 loss=13.901, ppl=15299.7, wps=73063.5, ups=4.46, wpb=16384, bsz=32, num_updates=5, lr=0.0001, gnorm=4.361, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      7 / 7173 loss=13.312, ppl=10169.5, wps=73559.4, ups=4.49, wpb=16384, bsz=32, num_updates=6, lr=0.0001, gnorm=3.307, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      8 / 7173 loss=12.914, ppl=7720.21, wps=72693.2, ups=4.44, wpb=16384, bsz=32, num_updates=7, lr=0.0001, gnorm=5.473, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      9 / 7173 loss=12.56, ppl=6036.72, wps=73531.2, ups=4.49, wpb=16384, bsz=32, num_updates=8, lr=0.0001, gnorm=6.112, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:12:19 | INFO | train_inner | epoch 001:     10 / 7173 loss=12.116, ppl=4437.77, wps=73187.6, ups=4.47, wpb=16384, bsz=32, num_updates=9, lr=0.0001, gnorm=4.415, loss_scale=64, train_wall=0, wall=10
```

Test Plan: Imported from OSS

Reviewed By: ngoyal2707

Differential Revision: D24729295

Pulled By: myleott

fbshipit-source-id: beee8bdece3eaa0419a2e813990420411e507c75"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-10-23T07:07:33Z,"refactor dataclass related files, add proper types for static checkin… (#1371)

Summary:
- refactor dataclass/ hierarchy to make it a bit more sane (while avoiding circular references)
- add top level FairseqConfig
- change typehints to reflect the correct config type if it is known

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1371

Reviewed By: myleott

Differential Revision: D24469026

Pulled By: alexeib

fbshipit-source-id: 01f68918f761d51ec5216286b8959ad35f41a7b2"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-10-22T23:31:49Z,"rename remove_bpe to post_process; add aliasing (#1369)

Summary:
some binaries (e.g. speech based ones) used --post-process, some used --remove-bpe. --post-process seems more appropriate as it does more than just remove bpe at the moment. this renames remove_bpe to post_process, adds alias so existing command lines would work and adds checkpoint upgrades so they continue to work also.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1369

Reviewed By: myleott

Differential Revision: D24465040

Pulled By: alexeib

fbshipit-source-id: 1b3e388291ccc403e76e069ef6606b80ead863a7"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-10-20T07:32:26Z,"Enable Hydra configs in fairseq (#1343) (#1510)

Summary:
Pull Request resolved: https://github.com/facebookresearch/pytext/pull/1510

this is the main pr that switches on hydra functionality in fairseq

we migrate ""args"" object into omegaconf ""DictConfig"" at all legacy entry points

in addition this migrates various components from secondary registries (like bpe encoders and tokenizers) to make the migration smoother

i am going through code that references migrated fairseq components and changing it to inherit from ""Legacy*"" components instead. hopefully tests will catch most of this

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1343

Reviewed By: myleott

Differential Revision: D23973928

Pulled By: alexeib

fbshipit-source-id: dd9554981fff51ea75c1ff343874d1d6e61793c9"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-10-19T01:14:51Z,"Apply black+isort (#1357)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1357

Reviewed By: alexeib

Differential Revision: D24377772

fbshipit-source-id: 51581af041d42d62166b33a35a1a4228b1a76f0c"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-10-12T17:55:40Z,"Support generation with huge pipeline parallel Transformer models (#1297)

Summary:
## What is this PR about?
* Support loading sharded checkpoints (loading and saving checkpoints without sharding for 30B models runs OOM)
* Ability to provide a fixed dictionary for multilingual machine translation to match the dictionary used for training the checkpoint
* Support generation with PipelineParallelTransformer models

## Testing

```
python fairseq_cli/generate.py \
    /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted  \
    --batch-size 1 \
    --path /large_exp/angelafan/checkpoints/shru/mmmt_5b_checkpoints_for_master/checkpoint4.pt \
    -s en -t es --remove-bpe 'sentencepiece' --beam 5 \
    --task translation_multi_simple_epoch \
    --lang-pairs /private/home/angelafan/mmt/lang100_bt_pairs.txt \
    --decoder-langtok --encoder-langtok src --gen-subset valid --fp16 \
    --dataset-impl mmap \
    --distributed-world-size 1 --distributed-no-spawn \
    --pipeline-model-parallel \
    --pipeline-chunks 1 \
    --pipeline-encoder-balance '[26]' \
    --pipeline-encoder-devices '[0]' \
    --pipeline-decoder-balance '[26]' \
    --pipeline-decoder-devices '[0]' \
    --fixed-dictionary /private/home/shru/projects/fairseq-py-kakaoval-mmt-save-redist-gen/dict.100langs.txt
2020-09-23 23:18:27 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might ca
use misalignment in pretraining and finetuning.
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['af', 'am', 'ar', 'arbt', 'ast', 'az', 'azbt', 'ba', 'be', 'bebt', 'bg', 'bgbt', 'bn', 'bnbt', 'br', 'bs', 'ca', 'ceb', 'cs', 'csb
t', 'cy', 'da', 'de', 'debt', 'el', 'en', 'es', 'esbt', 'et', 'etbt', 'fa', 'fabt', 'ff', 'fi', 'fr', 'frbt', 'fy', 'ga', 'gd', 'gl', 'gu', 'ha', 'he', 'hebt', 'hi', 'hibt', 'hr', 'ht', 'hu', 'hubt', 'hy', 'hybt', 'id', 'ig', 'ilo', 'is',
 'it', 'ja', 'jabt', 'jv', 'ka', 'kabt', 'kk', 'km', 'kn', 'ko', 'kobt', 'lb', 'lg', 'ln', 'lo', 'lt', 'lv', 'mg', 'mk', 'mkbt', 'ml', 'mn', 'mr', 'mrbt', 'ms', 'msbt', 'my', 'ne', 'nebt', 'nl', 'no', 'ns', 'oc', 'or', 'pa', 'pl', 'ps', '
pt', 'ro', 'robt', 'ru', 'sd', 'si', 'sk', 'sl', 'so', 'sq', 'sr', 'srbt', 'ss', 'su', 'sv', 'sw', 'ta', 'th', 'tl', 'tn', 'tr', 'trbt', 'uk', 'ur', 'uz', 'vi', 'vibt', 'wo', 'xh', 'yi', 'yo', 'zh', 'zhbt', 'zu']
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [en] dictionary: 128112 types
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [es] dictionary: 128112 types
2020-09-23 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-09-23 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:en-es': 1}
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:en-es src_langtok: 128022; tgt_langtok: 128023
2020-09-23 23:18:27 | INFO | fairseq.data.data_utils | loaded 4231 examples from: /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted/valid.en-es.en
2020-09-23 23:18:27 | INFO | fairseq.data.data_utils | loaded 4231 examples from: /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted/valid.en-es.es
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted valid en-es 4231 examples
2020-09-23 23:18:27 | INFO | fairseq_cli.generate | loading model(s) from /large_exp/angelafan/checkpoints/mmmt_100_langs_new_dict_5b_model/checkpoint4.pt
balance=[29,22,1], devices=[0,1,0], chunks=8, checkpoint=always
2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler order indices time: 0:00:00.004368
2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler filter_by_size time: 0:00:00.057953
2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler batch_by_size time: 0:00:01.158644
2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:01.223290
2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
S-2521  __en__ No.
T-2521  No.
H-2521  -2.4406352043151855     y no.
D-2521  -2.4406352043151855     y no.
P-2521  -5.2677 -4.4289 -0.1922 -2.1578 -0.1565
S-2261  __en__ Why?
T-2261  ¿Por qué?
H-2261  -1.7077901363372803     ¿Y por qué?
D-2261  -1.7077901363372803     ¿Y por qué?
P-2261  -5.2733 -0.7970 -3.7643 -0.5474 -0.3339 -1.0629 -0.1757
```

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1297

Reviewed By: myleott, msbaines

Differential Revision: D23991647

Pulled By: shruti-bh

fbshipit-source-id: 81ea1af64cbe75c8b53e050d2c2339dcb70fe1eb"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-10-09T20:34:59Z,"Improve dictionary & checkpoint reading w/ local caching

Reviewed By: myleott

Differential Revision: D24148700

fbshipit-source-id: 666300639243688939e137be748f7b76fc3c21a6"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-09-02T01:17:33Z,"Initial support for ZeRO optimizer state sharding (#1259)

Summary:
FairseqOSS will work with any optimizer and dtype.

TODO(future PR):
* support reduce instead of all_reduce
* support gradient sharding
* support parameter sharding

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1259

Test Plan:
Verified that checkpoint save and restore work.

Verified that grad_norm, loss, and ppl are identical with and without
sharding enable.

Before:

$ fairseq-train --task language_modeling   data-bin/wikitext-103   --save-dir checkpoints/transformer_wikitext-103   --arch transformer_lm --share-decoder-input-output-embed   --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 512 --sample-break-mode none   --max-tokens 2048 --update-freq 16   --max-update 50000  --memory-efficient-fp16 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50
...
2020-08-27 22:24:51 | INFO | train_inner | epoch 001:     49 / 394 loss=18.84, ppl=469411, wps=269226, ups=1.03, wpb=262144, bsz=512, num_updates=45, lr=5.72388e-06, gnorm=5.769, loss_scale=8, train_wall=1, wall=68
2020-08-27 22:24:52 | INFO | train_inner | epoch 001:     50 / 394 loss=18.787, ppl=452312, wps=256992, ups=0.98, wpb=262144, bsz=512, num_updates=46, lr=5.84885e-06, gnorm=5.512, loss_scale=8, train_wall=1, wall=69
2020-08-27 22:24:53 | INFO | train_inner | epoch 001:     51 / 394 loss=18.74, ppl=437735, wps=259178, ups=0.99, wpb=262144, bsz=512, num_updates=47, lr=5.97383e-06, gnorm=5.298, loss_scale=8, train_wall=1, wall=70
2020-08-27 22:24:54 | INFO | train_inner | epoch 001:     52 / 394 loss=18.683, ppl=420727, wps=257710, ups=0.98, wpb=262144, bsz=512, num_updates=48, lr=6.0988e-06, gnorm=5.094, loss_scale=8, train_wall=1, wall=71
2020-08-27 22:24:55 | INFO | train_inner | epoch 001:     53 / 394 loss=18.623, ppl=403794, wps=269279, ups=1.03, wpb=262144, bsz=512, num_updates=49, lr=6.22378e-06, gnorm=4.893, loss_scale=8, train_wall=1, wall=72
2020-08-27 22:24:56 | INFO | train_inner | epoch 001:     54 / 394 loss=18.574, ppl=390255, wps=264616, ups=1.01, wpb=262144, bsz=512, num_updates=50, lr=6.34875e-06, gnorm=4.684, loss_scale=8, train_wall=1, wall=73
2020-08-27 22:24:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-08-27 22:24:56 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-08-27 22:24:56 | INFO | train | epoch 001 | loss 19.736 | ppl 873122 | wps 264825 | ups 1.01 | wpb 262144 | bsz 512 | num_updates 50 | lr 6.34875e-06 | gnorm 8.898 | loss_scale 8 | train_wall 66 | wall 73
2020-08-27 22:24:56 | INFO | fairseq_cli.train | done training in 72.2 seconds

After:

$ fairseq-train --task language_modeling   data-bin/wikitext-103   --save-dir checkpoints/transformer_wikitext-103   --arch transformer_lm --share-decoder-input-output-embed   --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 512 --sample-break-mode none   --max-tokens 2048 --update-freq 16   --max-update 50000  --memory-efficient-fp16 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --zero-sharding os
...
2020-08-27 22:22:55 | INFO | train_inner | epoch 001:     49 / 394 loss=18.84, ppl=469411, wps=267663, ups=1.02, wpb=262144, bsz=512, num_updates=45, lr=5.72388e-06, gnorm=5.769, loss_scale=8, train_wall=1, wall=68
2020-08-27 22:22:56 | INFO | train_inner | epoch 001:     50 / 394 loss=18.787, ppl=452312, wps=252797, ups=0.96, wpb=262144, bsz=512, num_updates=46, lr=5.84885e-06, gnorm=5.512, loss_scale=8, train_wall=1, wall=69
2020-08-27 22:22:57 | INFO | train_inner | epoch 001:     51 / 394 loss=18.74, ppl=437735, wps=267692, ups=1.02, wpb=262144, bsz=512, num_updates=47, lr=5.97383e-06, gnorm=5.298, loss_scale=8, train_wall=1, wall=70
2020-08-27 22:22:58 | INFO | train_inner | epoch 001:     52 / 394 loss=18.683, ppl=420727, wps=267507, ups=1.02, wpb=262144, bsz=512, num_updates=48, lr=6.0988e-06, gnorm=5.094, loss_scale=8, train_wall=1, wall=71
2020-08-27 22:22:59 | INFO | train_inner | epoch 001:     53 / 394 loss=18.623, ppl=403794, wps=254410, ups=0.97, wpb=262144, bsz=512, num_updates=49, lr=6.22378e-06, gnorm=4.893, loss_scale=8, train_wall=1, wall=72
2020-08-27 22:23:00 | INFO | train_inner | epoch 001:     54 / 394 loss=18.574, ppl=390255, wps=268234, ups=1.02, wpb=262144, bsz=512, num_updates=50, lr=6.34875e-06, gnorm=4.684, loss_scale=8, train_wall=1, wall=73
2020-08-27 22:23:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-08-27 22:23:00 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-08-27 22:23:00 | INFO | train | epoch 001 | loss 19.736 | ppl 873122 | wps 263570 | ups 1.01 | wpb 262144 | bsz 512 | num_updates 50 | lr 6.34875e-06 | gnorm 8.898 | loss_scale 8 | train_wall 66 | wall 73
2020-08-27 22:23:00 | INFO | fairseq_cli.train | done training in 72.3 seconds

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Reviewed By: myleott

Differential Revision: D23432082

Pulled By: msbaines

fbshipit-source-id: 6a020b25e36a3d9283582b7d89a6a53038e5b181"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-08-14T17:24:51Z,"Misc fixes (#2448)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2448

Reviewed By: ngoyal2707

Differential Revision: D23011193

Pulled By: myleott

fbshipit-source-id: 1a29481707108e4465aca78ec1581fb79f05efba"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-08-06T17:20:39Z,"Multilingual v1: Multilingual Training with multiple bitext and monolingual datasets: add finetuning options

Summary:
A first version of XLNMT multilingual project code release: Multilingual Training with multiple bitext

- Minor changes to
    - fairseq/checkpoint_utils.py to add finetuning option instead of using restore_file which will restore from original model when being requeued.

Reviewed By: myleott

Differential Revision: D22483494

fbshipit-source-id: 733300fd6a4d185e561c793ea668047c96f616c6"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-08-04T15:25:50Z,"Fixes checkpoint_path while loading a model-parallel checkpoint (#2365)

Summary:
Fixes https://github.com/pytorch/fairseq/issues/2351

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2365

Reviewed By: pipibjc

Differential Revision: D22727384

Pulled By: myleott

fbshipit-source-id: e2ff703181a6b8f10df9b4ee7aa3f9e128c04b4e"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-05-26T22:59:59Z,"Several small fixes (incl. set default --data-buffer-size=10) (#2163)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2163

Reviewed By: ngoyal2707

Differential Revision: D21665601

Pulled By: myleott

fbshipit-source-id: 47673ff7f07acf0002c4e28380aa08ff917618ee"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-05-11T17:34:42Z,"Generalize moving of tensors to CPU in checkpoints (#2098)

Summary:
This is needed for TPUs
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2098

Reviewed By: ngoyal2707

Differential Revision: D21455095

Pulled By: myleott

fbshipit-source-id: f0f88e715884f44bc254b71f7694ac90200d92fc"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-04-14T17:58:38Z,"update checkpoint mkdir behavior (issue #1986) (#2011)

Summary:
Create checkpoint directory in ```save_checkpoint()```instead of ```load_checkpoint()```.

https://github.com/pytorch/fairseq/issues/1986
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2011

Reviewed By: ngoyal2707

Differential Revision: D21017208

Pulled By: myleott

fbshipit-source-id: 4f76afc1751f987b8ab2c170ca306d07bd5ffe83"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-03-28T03:22:36Z,"adding code to load and save model parallel checkpoint (#1119)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1119

Reviewed By: myleott

Differential Revision: D20712488

fbshipit-source-id: 941ef251c9e2deb8933d88188fac56ee8c5be9b7"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-03-11T07:36:45Z,"PySpeech TALNet: Convert to JIT and quantize

Summary:
Update the TALNet model so it can be converted to JIT format and quantized by the `jit_pyspeech_nn_model` tool.

The updated model structure is slight incompatible with the model files saved before (the model structure has the extra parameters `fc_att.weight` and `fc_att.bias`), so old model files must be loaded with `strict=False`. This diff also makes changes to allow `strict=False` where necessary.

Also renames the options of `jit_pyspeech_nn_model` to make them conform better to convention.

Reviewed By: jay-mahadeokar

Differential Revision: D20369643

fbshipit-source-id: 0950a26abc20d0ae5929c8d0a03f83cab4a59200"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-03-07T17:12:14Z,"Fix epoch reporting when restoring checkpoint (#1075)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1075

Differential Revision: D20322672

Pulled By: myleott

fbshipit-source-id: 8845a10b10442bed77670b7740afa271123a3b5d"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-03-05T00:37:24Z,"Use 1-based indexing for epochs everywhere (#1053)

Summary:
We are somewhat inconsistent in whether we're using 0-based or 1-based indexing for epochs. This should fix things to be 0-based internally, with logging and checkpoint naming still using 1-based indexing.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1053

Reviewed By: spencerp

Differential Revision: D20160715

Pulled By: myleott

fbshipit-source-id: 4ed94f9c371e1bfe29bcfa087fa6756507d6e627"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-02-19T04:07:05Z,"tts synthesis script

Summary: add a synth.py to pyspeech to run tts synthesis for a particular text.

Differential Revision: D19786089

fbshipit-source-id: 2aadd004609dfbcf477e58bc01e2b05df19c0e26"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-01-22T23:36:28Z,"fblearner pyspeech manifold migration

Summary:
was planning to migrate everything at once. but because pyspeech and tuna diverged from D19060980, it's too complicated to do the migration.

migrate save_dir to manifold in fblearner, and copy from manifold to gluster after training to keep downstream workflows happy.

Reviewed By: zdavid1995

Differential Revision: D19433205

fbshipit-source-id: bd8d969c001952d85167a63f4d55fe97b93aceb5"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-01-20T20:15:27Z,"fix the problem of passing None to format() when val_loss is None (e.… (#1633)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ x] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1633

Differential Revision: D19470727

Pulled By: myleott

fbshipit-source-id: a0bbefe20b4692306c57104f3e545912e20055e6"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-01-17T00:14:45Z,"Switch to Python logging (+ lint) (#1627)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1627

Python logging offers a number of benefits, such as logging timestamps, better
cross-library compatibility, ability to add multiple output handlers, etc.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/646

Reviewed By: spencerp

Differential Revision: D15815620

Pulled By: myleott

fbshipit-source-id: 5e64e9929b5e4b9dd5bb49bcdf7c510631907134"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2020-01-16T09:59:15Z,"Split from PR#968. add --keep-best-checkpoints (#990)

Summary:
Fixes https://github.com/fairinternal/fairseq-py/issues/968. Split --keep-best-checkpoints from the original request.
Use scores as the names to save the checkpoints
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/990

Differential Revision: D19411250

Pulled By: MultiPath

fbshipit-source-id: 82b0db614208eee54c9c0e470ad7faa6481747d5"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-12-17T01:22:11Z,"More fully deprecate --raw-text and --lazy-load (fixes #1488)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/947

Differential Revision: D19084273

Pulled By: myleott

fbshipit-source-id: de80d9abfac8e3d813a9c9b343b41327c500344e"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-12-06T02:24:56Z,"Add wrapper abstraction on top of fvcore PathManager to insulate OSS compatibility

Reviewed By: myleott

Differential Revision: D18736914

fbshipit-source-id: 897ba7463f7a8ebc0969c01c57bfe04fbf9e71c8"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-12-02T21:26:48Z,"Apply Black auto-formatting

Reviewed By: sujitoc

Differential Revision: D18738392

fbshipit-source-id: b7b7b75ef97946786c463c1887ef9a8676f030e6"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-11-22T01:50:42Z,"Quick fix for Structured Dropout checkpointing (#1406)

Summary:
Here's a quick fix for https://github.com/pytorch/fairseq/issues/1403.

To keep it short, this fix allows the user to checkpoint a translation model properly after applying layer pruning on a restored transformer file.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1406

Differential Revision: D18637540

Pulled By: myleott

fbshipit-source-id: 0f5e91e05e6579f0f459bc5293e9b14cb267322d"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-11-21T00:52:59Z,"Refactor data sharding to be specified via caller of task rather than task itself

Summary: Modifying number of shards internally to disable data sharding for batch iteration is dangerous because the caller of these tasks is not limited to fairspeq/train. So therefore we should put the onus of data sharding properly on the caller rather than the task itself.

Reviewed By: myleott

Differential Revision: D18456424

fbshipit-source-id: d46be16c441c50082f9a768d0b259e6c28a4b67b"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-10-30T19:55:54Z,"layer drop

Summary: This diff enables layer drop in transformer decoder in production training pipeline (ptt_transformer). It builds on top of the fairseq implementation D18094657 added by Angela Fan, and added additional logic to handle corresponding dropping layers at test time in exported model.

Reviewed By: jhcross

Differential Revision: D18165586

fbshipit-source-id: 373ac00268a25fa9e412edcb483becdfe792d992"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-10-27T19:10:53Z,"adding layerdrop code for training, pruning, and readme (#890)

Summary:
TEST 1: EVALUATION TIME WORKS
checked
achieves correct model perplexity: 18.68

TEST 2: TRAINING NEW MODEL WORKS
checked

without layerdrop:
--decoder-layerdrop 0 OR no flag at all
| epoch 001:     10 / 11201 loss=27.469, nll_loss=27.469, ppl=185799477.36, wps=1764, ups=0, wpb=9216.000, bsz=3.000, num_updates=7, lr=0.0004376, gnorm=25.471, clip=1.000, oom=0.000, loss_scale=8.000, wall=37, train_wall=30
| epoch 001:     20 / 11201 loss=27.443, nll_loss=27.443, ppl=182500427.22, wps=2449, ups=0, wpb=9216.000, bsz=3.000, num_updates=17, lr=0.0010626, gnorm=25.273, clip=1.000, oom=0.000, loss_scale=8.000, wall=64, train_wall=57
| epoch 001:     30 / 11201 loss=27.404, nll_loss=27.404, ppl=177612215.78, wps=2720, ups=0, wpb=9216.000, bsz=3.000, num_updates=27, lr=0.0016876, gnorm=25.136, clip=1.000, oom=0.000, loss_scale=8.000, wall=91, train_wall=84
| epoch 001:     40 / 11201 loss=27.009, nll_loss=27.009, ppl=135079983.00, wps=2865, ups=0, wpb=9216.000, bsz=3.000, num_updates=37, lr=0.0023126, gnorm=24.311, clip=1.000, oom=0.000, loss_scale=8.000, wall=119, train_wall=112
| epoch 001:     50 / 11201 loss=26.418, nll_loss=26.418, ppl=89680259.41, wps=2952, ups=0, wpb=9216.000, bsz=3.000, num_updates=47, lr=0.0029376, gnorm=22.775, clip=1.000, oom=0.000, loss_scale=8.000, wall=147, train_wall=140

with layerdrop (regularization effect should be seen in PPL):
--decoder-layerdrop 0.2

| epoch 001:     10 / 11201 loss=25.186, nll_loss=25.186, ppl=38182937.27, wps=2428, ups=0, wpb=9216.000, bsz=3.000, num_updates=8, lr=0.0005001, gnorm=17.082, clip=1.000, oom=0.000, loss_scale=16.000, wall=30, train_wall=24
| epoch 001:     20 / 11201 loss=25.270, nll_loss=25.270, ppl=40451933.50, wps=3173, ups=0, wpb=9216.000, bsz=3.000, num_updates=18, lr=0.0011251, gnorm=17.162, clip=1.000, oom=0.000, loss_scale=16.000, wall=52, train_wall=45
| epoch 001:     30 / 11201 loss=25.349, nll_loss=25.349, ppl=42752256.68, wps=3454, ups=0, wpb=9216.000, bsz=3.000, num_updates=28, lr=0.0017501, gnorm=17.370, clip=1.000, oom=0.000, loss_scale=16.000, wall=75, train_wall=68
| epoch 001:     40 / 11201 loss=25.115, nll_loss=25.115, ppl=36343806.30, wps=3619, ups=0, wpb=9216.000, bsz=3.000, num_updates=38, lr=0.0023751, gnorm=16.945, clip=1.000, oom=0.000, loss_scale=16.000, wall=97, train_wall=90
| epoch 001:     50 / 11201 loss=24.804, nll_loss=24.804, ppl=29284345.78, wps=3716, ups=0, wpb=9216.000, bsz=3.000, num_updates=48, lr=0.0030001, gnorm=16.406, clip=1.000, oom=0.000, loss_scale=16.000, wall=119, train_wall=112

TEST 3: PICKING UP TRAINING FROM EXISTING MODEL
checked

| loaded checkpoint /checkpoint/angelafan/structured_0.1_block_8_sd02/checkpoint_last.pt (epoch 272 @ 381066 updates)
| loading train data for epoch 272
| loaded 1801350 examples from: /private/home/angelafan/lm_work/fairseq-py/data-bin/wikitext-103/train

TEST 4: EVALUATING EXISTING BERT MODEL REPROS RESULTS
| [input] dictionary: 50265 types
| [label] dictionary: 9 types
| Accuracy:  0.9231651376146789
achieves correct accuracy on SST2 for this model

TEST 5: TRAINING NEW BERT MODEL WORKS
checked and works

TEST 6: NMT

without layerdrop
--encoder-layerdrop 0 --decoder-layerdrop 0 OR combinations of flag specified and not specified

| epoch 001:     10 / 92203 loss=15.820, nll_loss=15.830, ppl=58267.93, wps=4902, ups=0, wpb=1477.818, bsz=51.636, num_updates=11, lr=1.47473e-06, gnorm=7.207, clip=0.000, oom=0.000, loss_scale=128.000, wall=60, train_wall=3
| epoch 001:     20 / 92203 loss=15.523, nll_loss=15.501, ppl=46359.29, wps=5037, ups=0, wpb=1496.476, bsz=45.333, num_updates=21, lr=2.72448e-06, gnorm=6.869, clip=0.000, oom=0.000, loss_scale=128.000, wall=63, train_wall=6
| epoch 001:     30 / 92203 loss=15.185, nll_loss=15.123, ppl=35695.79, wps=5085, ups=0, wpb=1519.355, bsz=44.645, num_updates=31, lr=3.97423e-06, gnorm=6.186, clip=0.000, oom=0.000, loss_scale=128.000, wall=66, train_wall=9
| epoch 001:     40 / 92203 loss=14.940, nll_loss=14.849, ppl=29505.60, wps=5116, ups=1, wpb=1521.244, bsz=42.927, num_updates=41, lr=5.22398e-06, gnorm=5.610, clip=0.000, oom=0.000, loss_scale=128.000, wall=69, train_wall=12
| epoch 001:     50 / 92203 loss=14.745, nll_loss=14.630, ppl=25346.87, wps=5070, ups=1, wpb=1507.961, bsz=41.725, num_updates=51, lr=6.47373e-06, gnorm=5.104, clip=0.000, oom=0.000, loss_scale=128.000, wall=71, train_wall=15

with layerdrop (regularization effect should be seen in PPL)

A) works with --encoder-layerdrop 0.2 --decoder-layerdrop 0.2
B) works with different settings --encoder-layerdrop 0.3 --decoder-layerdrop 0.5
C) works with one on and one off --encoder-layerdrop 0.2 --decoder-layerdrop 0

| epoch 001:     10 / 92203 loss=15.817, nll_loss=15.828, ppl=58158.54, wps=5355, ups=0, wpb=1477.818, bsz=51.636, num_updates=11, lr=1.47473e-06, gnorm=6.959, clip=0.000, oom=0.000, loss_scale=128.000, wall=59, train_wall=3
| epoch 001:     20 / 92203 loss=15.650, nll_loss=15.641, ppl=51111.63, wps=5515, ups=0, wpb=1496.476, bsz=45.333, num_updates=21, lr=2.72448e-06, gnorm=6.825, clip=0.000, oom=0.000, loss_scale=128.000, wall=61, train_wall=6
| epoch 001:     30 / 92203 loss=15.440, nll_loss=15.408, ppl=43491.58, wps=5602, ups=0, wpb=1519.355, bsz=44.645, num_updates=31, lr=3.97423e-06, gnorm=6.576, clip=0.000, oom=0.000, loss_scale=128.000, wall=64, train_wall=8
| epoch 001:     40 / 92203 loss=15.247, nll_loss=15.193, ppl=37457.14, wps=5676, ups=1, wpb=1521.244, bsz=42.927, num_updates=41, lr=5.22398e-06, gnorm=6.124, clip=0.000, oom=0.000, loss_scale=128.000, wall=67, train_wall=11
| epoch 001:     50 / 92203 loss=15.055, nll_loss=14.977, ppl=32259.92, wps=5598, ups=1, wpb=1507.961, bsz=41.725, num_updates=51, lr=6.47373e-06, gnorm=5.661, clip=0.000, oom=0.000, loss_scale=128.000, wall=69, train_wall=14

TEST 7: PRUNING TESTCASES

A) after adding the pruning flags, model can evaluate as a full model
checked, reaches correct PPL
num. model params: 246933504
| Evaluated 217646 tokens in 196.3s (1108.99 tokens/s)
| Loss: 2.9275, Perplexity: 18.68

B) after adding pruning flags, model can be pruned. this works with multiple flag settings
checked three cases:
num. model params: 146163712
| Evaluated 217646 tokens in 106.0s (2054.07 tokens/s)
| Loss: 3.0932, Perplexity: 22.05

num. model params: 209144832
| Evaluated 217646 tokens in 162.8s (1336.99 tokens/s)
| Loss: 2.9526, Perplexity: 19.16

C) model can pick up training if you want to finetune the pruned model
checked:
| loading train data for epoch 272
| loaded 1801350 examples from: /private/home/angelafan/lm_work/fairseq-py/data-bin/wikitext-103/train
| WARNING: overflow detected, setting loss scale to: 64.0
| WARNING: overflow detected, setting loss scale to: 32.0
| epoch 272:   1500 / 5601 loss=5.015, nll_loss=5.015, ppl=32.33, wps=11598, ups=1, wpb=18432.000, bsz=6.000, num_updates=98, lr=0.0061251, gnorm=0.613, clip=1.000, oom=0.000, loss_scale=32.000, wall=156, train_wall=252396

D) works with BERT
checked:
without specifying any flags, reproduces the correct standard accuracy
with flags, produces the correct pruned accuracy

| [input] dictionary: 50265 types
| [label] dictionary: 9 types
| Accuracy:  0.9231651376146789

| [input] dictionary: 50265 types
| [label] dictionary: 9 types
| Pruning model to specified layer configuration - this works best if the model was trained with LayerDrop
| Accuracy:  0.9220183486238532
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/890

Reviewed By: edunov

Differential Revision: D18094657

Pulled By: huihuifan

fbshipit-source-id: 2bbaa2ff0039e906782694fc2038b8c17a8693e7"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-10-12T04:53:11Z,"Added option to save checkpoints using Path Manager.

Summary: Added option to save checkpoints using Path Manager.

Reviewed By: hudeven

Differential Revision: D17392754

fbshipit-source-id: 4b8e556ef8455a1548e5a083d779ed809cd785be"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-10-09T21:53:27Z,"Fix data loading memory issue in pyspeech

Summary:
We currently shard data when creating the batch iterator. This means we first load all indicese/frame lengths/handles into memory, and then do the sharding. This makes it impossible to train on large datasets with a high amount of workers  because each worker will need to load the entire dataset into memory. For training on a million hours of data (i.e. semi-supervised or unsupervised approaches) this data loading just makes it flat out impossible to use 8 GPU's.

3 changes:

1. This diff modifies the data loading such that we do the sharding while we read the handles file, rather than later. This modification is done on a task-by-task basis, since the task specifies how the data is loaded. I've tried to make the code compatible with both sharding during handle loading and sharding during batch iteration. I've currently only done the sharding during handle loading for the aligned_training task.

2. To support data sharding at data loading time and the requirement that all shards must have exactly the same # of batches, I've added a method to do this synchronization where all shards with too many batches would just truncate the extra ones, similar to what we already do.

2. In fairspeq/train.py, we are actually loading the training dataset and batch iterator twice, once in train.py and once when loading the checkpoint (which we always do regardless if there is a checkpoint). This means double the loading time which can be painful for very large files. I've removed the extraneous loading in this diff as well.

Reviewed By: yqwangustc

Differential Revision: D17750715

fbshipit-source-id: 0e6e3d363525fa5661f1c784303390ea13f46377"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-09-20T16:34:58Z,"added multilingual masked LM training (#849)

Summary:
The multilingual-RoBERTa training is working with aconneau XLM data.

Two pieces remaining:

1) `XLM` limits batch to be from same language, I am not 100% sure about the reason for that, but should be easy to implement, basically we can add `batch_by_size_and_language` instead of default `batch_by_size` function. If it's not critical, I would want to leave it out as it keeps the code very clean and simple.

2) `sample_ratio` in `ConcatDataset` works with `int` by tiling the datasets based on ratio. Currently I am handling it by sounding off the ratio to `first decimal` and then multiplying by `10`. We can see if some such simple heuristics are good enough, there are other options (we can talk about them offline).
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/849

Differential Revision: D17162460

fbshipit-source-id: d967f3d872f7a1f0aa4ea418bd362b68af9e432f"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-08-21T20:43:01Z,"Parameterized criterions (#808)

Summary:
Support criterion with parameters, such as AutoSegmentationCriterion (ASG) used in wav2letter which has a transition matrix parameter. This is needed to integrate wav2letter's ASG into PySpeech.

With this diff, parameters in criterions will be:
(1) updated by optimizers, with a configurable learning rate
(2) saved and loaded from checkpoints, preserving backward compatibility for criterions without parameters
(3) synchronized across nodes in distributed training.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/808

Reviewed By: jcai1

Differential Revision: D16934097

Pulled By: okhonko

fbshipit-source-id: 121ec9382459385c6f9cbef3a8274bec1a434038"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-08-12T16:08:16Z,"Lint

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/817

Differential Revision: D16762905

Pulled By: myleott

fbshipit-source-id: d920595bec44ed26b72dfc6fbc15c0aa107b4e56"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-08-12T14:17:21Z,"Update --restore-file logic (partially fixes #999)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1007

Differential Revision: D16762490

Pulled By: myleott

fbshipit-source-id: d67137bcf581887850323d188bb4ea643a35ac9e"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-08-10T15:16:50Z,"Add WSC task and criterion

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1004

Differential Revision: D16751443

Pulled By: myleott

fbshipit-source-id: f70acd6c7be6d69da45b5b32fe4c4eff021539ab"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-08-01T12:55:57Z,"Update PyTorch Hub interface

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/782

Differential Revision: D16542256

Pulled By: myleott

fbshipit-source-id: ea3279e7a1ce4687a5914f32b76787c419be1ffa"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-07-30T14:48:23Z,"Relicense fairseq under MIT license (#786)

Summary:
The previous BSD+PATENTS license was controversial. We have been
approved to relicense fairseq under the MIT license.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/786

Differential Revision: D16560654

Pulled By: myleott

fbshipit-source-id: f78b1beb4f2895dd7b9bfc79f5f952a2bfb94034"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-07-29T23:06:26Z,"adding glue data preprocessing scripts (#771)

Summary:
1) Added glue data pre-processing script.
2) updated README with usage.

TODO:
1) releasing fairseq dictionary and remove hardcoded path.
2) remove hard-coded path for bpe-encoding,

myleott what do you recommend for above TODOs?
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/771

Reviewed By: myleott

Differential Revision: D16547679

Pulled By: myleott

fbshipit-source-id: 6a6562d9b6215523d048fdf3daee63ffac21e231"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-07-24T23:59:07Z,"check save_dir before beginning training

Summary: I sadly discovery that my checkpoint directory wasn't globally readable after 8 hours of training. Adding this check at the beginning of train loop to keep that from happening again!

Reviewed By: myleott

Differential Revision: D16455394

fbshipit-source-id: 35959aa058150b2afb63710c468d01ebc8a12b0c"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-07-19T20:13:43Z,"Rename _load_model_ensemble -> load_model_ensemble_and_task

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/738

Differential Revision: D16377803

Pulled By: myleott

fbshipit-source-id: 6beb2f78e7464b70ff65a965d2b747cdca0ca951"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-07-01T21:10:49Z,"Fixes checkpointing bug introduced in 89e077c

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/847

Differential Revision: D16075498

Pulled By: myleott

fbshipit-source-id: 62e27a8c4764f53f181c502674dfab1e6b0537e2"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-06-30T18:35:33Z,"Add additional options for configuring writing of checkpoints

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/697

Differential Revision: D16068465

Pulled By: myleott

fbshipit-source-id: c2563c3c682e7e8406e6d7c8e895d8afbec551eb"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-06-11T18:19:58Z,"Automatically fill in default values from add_args

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/797

Differential Revision: D15761071

Pulled By: myleott

fbshipit-source-id: 257d4a2297e83da7e59baed154dbafd6bfe614bf"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-05-30T18:41:40Z,"Add --reset-dataloader

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/613

Differential Revision: D15541384

Pulled By: myleott

fbshipit-source-id: ef2c0b0a51cdf37af2ccff0546f524d49f87e65d"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-05-17T21:25:56Z,"Small features + lint

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/588

Differential Revision: D15389638

Pulled By: myleott

fbshipit-source-id: 4632ce22d51dc2c74d250bae999630095d849701"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-05-17T04:03:08Z,"Clean up sharded train iterator

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/586

Differential Revision: D15372949

Pulled By: myleott

fbshipit-source-id: c1cf1c645e8d55fc8568f23a47c45677ac9ab1da"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-05-14T19:57:12Z,"Move save/load checkpoint functions to utils

Summary:
Move `load_checkpoint`, `save_checkpoint` and `reload_train` from train.py to checkpoint_utils.py
Move `get_perplexity` from train.py to utils.py.
This will make train.py lighter and allow us to reuse all this utils functionality when fairseq is used as external library.

Reviewed By: myleott

Differential Revision: D15289607

fbshipit-source-id: 4b7c95225ac22e402bcda3497811361809110df1"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-05-11T14:56:45Z,"Add missing options to TransformerDecoderLayer

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/560

Differential Revision: D15260838

Pulled By: myleott

fbshipit-source-id: 5f80dd82775c10ce46a3e1c451ccaf0ef55bfa31"
github.com/freewym/espresso,fairseq/checkpoint_utils.py,2019-05-06T07:17:45Z,"Load pretrained encoder or decoder (#705)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/705

This adds functionality in fairseq to load a pretrained encoder or decoder from another pretrained model into the current model.

Reviewed By: jmp84

Differential Revision: D15207084

fbshipit-source-id: 32a710ff77389928e20793c71d312863df9dd8ae"
github.com/microsoft/NeuralSpeech,VideoDubber/fairseq/checkpoint_utils.py,2023-07-30T12:50:32Z,add
github.com/OFA-Sys/ONE-PEACE,fairseq/fairseq/checkpoint_utils.py,2023-05-23T05:53:57Z,code cleanup
github.com/OFA-Sys/ONE-PEACE,fairseq/fairseq/checkpoint_utils.py,2023-05-19T06:52:54Z,code init
github.com/Victorwz/LongMem,fairseq/fairseq/checkpoint_utils.py,2023-06-13T12:02:52Z,initial
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2022-07-06T12:58:32Z,"[nllb] No Language Left Behind @ 200

Co-authored-by: Shruti Bhosale <7892195+shruti-bh@users.noreply.github.com>
Co-authored-by: Anna Sun <13106449+annasun28@users.noreply.github.com>
Co-authored-by: Maha Elbayad <elbayadm@users.noreply.github.com>
Co-authored-by: Jean Maillard <107696+jeanm@users.noreply.github.com>
Co-authored-by: James Cross <10384938+jhcross@users.noreply.github.com>
Co-authored-by: Onur Çelebi <celebio@users.noreply.github.com>
Co-authored-by: Kevin Heffernan <73017975+heffernankevin@users.noreply.github.com>
Co-authored-by: Kaushik Ram Sadagopan <29103305+kauterry@users.noreply.github.com>
Co-authored-by: Angela Fan <3150717+huihuifan@users.noreply.github.com>"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2022-01-21T17:27:49Z,"Fix breakage from D33649708

Summary: https://www.internalfb.com/diff/D33649708 (https://github.com/pytorch/fairseq/commit/995c204337d16a6146a433cee360e5a5bfbc9a6f)?src_version_fbid=1030479880843010&dst_version_fbid=247617347518523&transaction_fbid=1601081576900014

Reviewed By: alexeib

Differential Revision: D33696937

fbshipit-source-id: 9a17610e3f4eb3dd2b2131a3f9fb42732a31b47f"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2022-01-20T08:02:16Z,"Data2vec prelim (#2929)

Summary:
Preliminaries for data2vec release, include some minor improvements and bug fixes

Most important change is that we now default to raising an exception when fields in config do not have a corresponding field in the model dataclass

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2929

Reviewed By: wnhsu

Differential Revision: D33649708

Pulled By: alexeib

fbshipit-source-id: 629bdb4c361550740b451c570c2005bb956c6fcb"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2022-01-19T03:29:52Z,"Decode using EMA model in IPL recipe

Summary: Add option to use the EMA model for decoding in transducer IPL recipe by passing --ipl-decode-ema. Note EMA should be enabled as in the diff D24238379 (https://github.com/pytorch/fairseq/commit/8feccf94412424a4683b01090de36fa77cb4951d) using options --store-ema --ema-start-update and --ema-decay.

Reviewed By: cruvadom

Differential Revision: D31983366

fbshipit-source-id: 2bf63b3f7d1b5fa8804b3a7e9bfab71a463ca957"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2022-01-07T08:39:11Z,"Formatting fix: get CI green (#2860)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Applies `black` and `isort` to files

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2860

Reviewed By: Mortimerp9

Differential Revision: D33456637

Pulled By: dianaml0

fbshipit-source-id: 560b8d3a8f589cbecc92d0d21163596b5d47d609"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-12-31T02:38:12Z,"add strict option to checkpoint_utils. load_pretrained_component_from_model()

Summary: Add strict option to checkpoint_utils. load_pretrained_component_from_model()

Reviewed By: sravyapopuri388

Differential Revision: D33304224

fbshipit-source-id: 2284a21dfea7810ec212f15daadeeeb45c6dca1b"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-12-17T00:11:19Z,"formatting fix (#2816)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
fix `black` failures

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2816

Reviewed By: alexeib

Differential Revision: D33172615

Pulled By: dianaml0

fbshipit-source-id: 36b141f42941670f1bfa981041d878042feb0428"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-12-11T00:55:12Z,"Add loading from HuggingFace Hub

Summary: Add loading from HuggingFace Hub. Revised from and to replace D32697723 (accepted).

Reviewed By: pipibjc, dianaml0

Differential Revision: D32964041

fbshipit-source-id: 39676aa0ecb10454ae76b70968d5abe96ab6da54"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-11-29T20:32:59Z,"Add linting with black (#2678)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2678

Reviewed By: Mortimerp9

Differential Revision: D32653381

Pulled By: dianaml0

fbshipit-source-id: 2810d14867cd7d64f4d340740e2b590b82de47fe"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-11-19T00:38:31Z,"Merge --use-ontology-for-* into --use-ontology

Summary:
There are three options for the ontology:
* `--use-ontology-for-training`
* `--use-ontology-for-validation`
* `--use-ontology-for-balancing`

The first two must always be set together.

In the past, I observed that it's best not to use ontology for data balancing even if we use ontology for training and validation. But now I no longer observe this.

Therefore, I'm merging all these three options into one (`--use-ontology`).

In addition, I'm also moving the logic of avoiding loading teacher models out of `checkpoint_utils.py`. If you want to load a student model without loading its teachers (e.g. for prediction only), specify `arg_overrides={""ignore_teachers"": True}` when calling `load_model_ensemble`.

Reviewed By: xiaoxiao26

Differential Revision: D32518830

fbshipit-source-id: 103c6458f7927ec5ca7470109c8f956c00f514a2"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-10-23T02:51:09Z,"set num_update before loading state dict (#2491)

Summary:
## What does this PR do?
Set `model.num_updates` in `load_model_ensemble_and_task()` before loading `state_dict`, like what's done in `fairseq/trainer.py`, because a model's `state_dict` may depend on `num_update`.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2491

Reviewed By: xuqiantong

Differential Revision: D31863368

Pulled By: wnhsu

fbshipit-source-id: c70051f898819cc43b02c9f5765429e9f194aed5"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-09-13T20:20:35Z,"Back out ""Fairseq needs to store and load metadata from model state_dict"" (#3861)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/3861

backout fairseq changes. fix with a suggested, more optimal changes in checkopint utils.

Reviewed By: zhengwy888

Differential Revision: D30886481

fbshipit-source-id: 12b6dd4d5107ab4371b73a58d9a044a17c733260"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-09-09T01:18:30Z,"Fairseq needs to store and load metadata from model state_dict

Summary:
## TL;DR
Fairseq checkpoint saving and loading should mirror torch's checkpoint by saving and loading ""state_dict()._metadata"".

## Long Story:

#### What happened:
During model loading and saving, Quantization-aware-training models in Pytorch encounters a weird bug that says state_dict ""fake_weight_quant.weight.min_val"" is mismatched to ""min_vals"".

#### What was the reason:
- We found the issue in that torch uses state_dict()._metadata to store module._version, but the metadata was never store in checkpoint, nor are they loaded during checkpoint loading in fairseq.

Reviewed By: frankseide

Differential Revision: D30649933

fbshipit-source-id: ce262486b9b95fbcece463fa05c4e1903d4232d7"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-09-01T19:29:51Z,"EMA

Summary:
Adds Exponential moving average (EMA) model for Kaizen semi-supervised training https://arxiv.org/abs/2106.07759

1. Add `ema.store_ema` to enable storing EMA. EMA will be written to extra_state in the state dict while saving checkpoint.
2. `ema.ema_start_update` to control when the EMA starts accumulating
3. Tasks can use `uses_ema` property to decide if the EMA should be passed to the task. (Default is False)
4. `load_ema_from_checkpoint` can be used to load EMA model in place of the model to be used for evalutation. Pyspeech has eval-ema option for this.

```
This module has the EMA class used to store a copy of the exponentially decayed
model params.

Typical usage of EMA class involves initializing an object using an existing
model (random or from a seed model) and setting the config like ema_decay,
ema_start_update which determine how the EMA model is updated. After every
update of the model i.e. at the end of the train_step, the EMA should be updated
by passing the new model to the EMA.step function. The EMA model state dict
can be stored in the extra state under the key of ""ema"" and dumped
into a checkpoint and loaded. The EMA object can be passed to tasks
by setting task.uses_ema property.
EMA is a smoothed/ensemble model which might have better performance
when used for inference or further fine-tuning. EMA class has a
reverse function to load the EMA params into a model and use it
like a regular model.
```

Reviewed By: cruvadom

Differential Revision: D24238379

fbshipit-source-id: 879d3ba5070a614b7d365f9503af357001e875b2"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-08-02T21:36:32Z,"fixing checkpoint config upgrade for generation print_alignment (#2125)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes config upgrade conditions for upgrading generation. print_alignment

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2125

Reviewed By: myleott

Differential Revision: D30049140

Pulled By: jingfeidu

fbshipit-source-id: e613821e94d0cdb876c35bc6e3fede7affbf4628"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-07-29T23:30:41Z,"seed random suffix in checkpoint to be consistent across shards

Summary:
Currently the random suffix for saving sharded checkpoints can be different for each shard when training with FSDP and use_sharded_state=True. This makes it difficult for downstream applications to load the checkpoints properly, since each shard may have different suffixes.

This diff seeds the random suffix to be consistent across shards

Reviewed By: zhengwy888

Differential Revision: D29951167

fbshipit-source-id: 65749357e62a28978f3b46b71767204a508e1f61"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-07-29T23:30:41Z,"use pathmanager to delete old checkpoints

Summary: --keep-best-checkpoints doesn't work for Manifold paths because the current code assumes normal paths. Lets use  PathManager to properly remove them.

Reviewed By: myleott, sshleifer

Differential Revision: D29947965

fbshipit-source-id: 237a7b5aaa8293bb203ad05937decdf3b3ae2fc0"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-07-29T23:30:40Z,"use suffix when saving best checkpoints with metric

Summary: This is needed when training with FSDP + sharded state, as the checkpoints should have `-shard0.pt`

Reviewed By: sshleifer

Differential Revision: D29947728

fbshipit-source-id: d2cb7c23e1e6d027d115cb734e2f2f1c34239eba"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-07-09T23:25:02Z,"Roll back os.path.abspath change

Reviewed By: donhusa

Differential Revision: D29641968

fbshipit-source-id: eca379158055f3e38e9c053b06db56842265e53a"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-07-08T22:27:57Z,"Fix static container (#2036)

Summary:
fixes StatefulContainer being static which is a problem when you load a checkpoint with task that already has the same keys in the container

also print full path to checkpoints when saving (useful with hydra) and crash if repeatedly failing to save a checkpoint

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2036

Reviewed By: arbabu123

Differential Revision: D29608430

Pulled By: alexeib

fbshipit-source-id: 1b65c8f839e02de9110af3ec53f1e7d48a4908f7"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-07-06T22:07:31Z,"fixes tests/test_train.py to mock checkpoint.save_dir config node (#3675)

Summary:
## What does this PR do?
Some downstream users reported that errors when passing Namespace to load_checkpoint().

A recent change made the assumption that the passed object is dict like (dict or DictConfig) that have a get function.
This changes that and make sure the mocked config have checkpoint.save_dir to allow the test to run.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3675

Reviewed By: omry

Differential Revision: D29564805

Pulled By: lematt1991

fbshipit-source-id: 89308811da382667f6c5d3152ee2d6480416ee62"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-07-01T13:37:47Z,"Hydra 1.1 compatibility: Use an explicit schema for the primary config (#3659)

Summary:
## What does this PR do?
Fixes compatibility with Hydra 1.1.
The result is compatible with both Hydra 1.0 and Hydra 1.1, and will allow a smoother migration to Hydra 1.1.

At this point I am not yet removing the restriction on the Hydra version from setup.py:
1. It depends on some Hydra 1.1 changes that are not yet released (It will be compatible with 1.1.1).
2. Upgrading will result in deprecation warnings, and fixing them will break compatibility with Hydra 1.0.

There will be some followup to make the code fully compatible with 1.1 once Hydra 1.1 is the default version in fbcode.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3659

Reviewed By: omry

Differential Revision: D29498036

Pulled By: lematt1991

fbshipit-source-id: 96999cde5daad6749ef4d3ddf6a36a1e984ff201"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-06-15T21:09:52Z,"Check attributes in trainer and checkpoint loading before using them (#1970)

Summary:
## What does this PR do?
Fixes None exception when some attributes in  don't exist in cfg.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1970

Reviewed By: alexeib

Differential Revision: D29140036

Pulled By: hikushalhere

fbshipit-source-id: 7d941bcae6bb000c281a43ca2cd0876a49912ab9"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-06-04T23:19:56Z,"Fix loading some TALNet models

Summary:
D28728718 cleaned up the ""kd_binary_cross_entropy"" criterion, but this caused loading old models trained with this criterion to fail.
This diff replaces the ""kd_binary_cross_entropy"" criterion with the ""wav2vec"" criterion when loading models, and fixes this error.

It also removes the ""log_keys"" argument if it's `None`. Some criteria (e.g. wav2vec) require this argument to be a list, and will supply a default value of `[]` when it's absent. The presence of the `None` value prevents the use of this default value and causes an error.

Differential Revision: D28901263

fbshipit-source-id: 9b33aed35e76d2c734d1d4e2cbca1ff193a8c920"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-06-04T00:49:12Z,"Teacher-student learning for TALNet

Summary:
This diff implements teacher-student learning for TALNet.

Three classes take part in the teacher-student learning:
* The task loads the teacher models;
* The model generates predictions using the teacher models, and mixes them with the original targets;
* The `Wav2VecCriterion` reads the mixed targets to compute the loss. However, it still uses the original targets to compute the MAP and MAUC metrics.

There are two types of teachers:
* Static teachers: a file that stores predictions on training data which have been produced by running a model offline;
* Dynamic teachers: model files that are loaded at the beginning of training and executed on the fly to produce predictions.

We actually no longer use static teachers. The code about static teachers are copied over from the `KnowledgeDistillationBinaryCrossEntropyCriterion` class. This class will be cleaned up in D28728718.

The teacher models are stored in the task object, and will not be saved into checkpoints.

Reviewed By: alexeib

Differential Revision: D28728707

fbshipit-source-id: 0fcfc00db2e7194a6f7ee687cad9fa72e82a028b"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-05-26T00:45:51Z,"support FSDP sharded_state checkpoint loading during inference

Summary:
using the very useful feature added by QuentinDuval https://github.com/facebookresearch/fairscale/pull/683/files , we can consolidate sharded states into a full regular states. this allows inferences on sharded state almost transparently.

The main complexity comes from trying to be smart about what kind of checkpoint the user wants to load. not sure if this is over-engineering
1. if the file checkpoint-shard0.pt exists, and `--checkpoint-shard-count` is > 1, then we load sharded FSDP checkpoint
2. if checkpoint-shard0.pt exists but --checkpoint-shard-count=1, we load consolidated FSDP checkpoint
3. if checkpoint-shard0.pt does not exist, but --checkpoint-shard-count > 1, we load model parallel checkpoint
4. otherwise we are loading a single, plain checkpoint.

In theory we could be even smarter and load shard0.pt to check how many more checkpoints are needed. this is not implemented, though it will save the user having to specify --checkpoint-shard-count.

Reviewed By: sshleifer

Differential Revision: D28563441

fbshipit-source-id: dcafcaa7c9eaf5c9ff94f55c16bb3424c98dfa59"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-05-22T07:22:05Z,"fixing s2t transformer and N-best checkpoint saving

Summary:
- fixing the default value for `encoder_freezing_updates` in s2t transformer
- fixing N-best checkpoint saving: the previous implementation compares the new checkpoint with only the previous best one but not the previous N best ones. This leads to suboptimal results on N-best checkpoint averaging.

Reviewed By: jmp84

Differential Revision: D28546493

fbshipit-source-id: 44ec6d5ab49347f392d71269c5dcfd154b00c11e"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-05-21T23:18:21Z,"attempt to make non-sharded FSDP checkpoint behave like regular checkpoint

Summary:
overall just wondering if feature is desirable. if it is, the next diff which supports loading sharded checkpoint into a consolidated state dict cleaner.

a couple advantages
1. allows resuming from other DDP trainers.
2. allows resuming into other DDP trainers. or FSDP of a different configuration.
3. none-sharded FSDP checkpoint can be loaded with regular load_model_ensemble_and_task()

For old training workflow that's not using `--use-sharded-state`, please rename the checkpoint to remove the ""-shard0"" for resuming training.

Reviewed By: sshleifer

Differential Revision: D28563032

fbshipit-source-id: ced72bed969319ab6306059721f56e29b2c3d892"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-04-14T08:50:07Z,"add keep_interval_updates_pattern

Summary:
Motivation:

I want to save checkpoints frequently, due to unreliable jobs in FB cluster that restart frequently. I want to do this without spamming Manifold storage, but still save some historical checkpoints (i.e. every 10k updates), so I can track how WER evolves over time.

To save frequently, I can use a small --save-interval-updates.

To delete old checkpoints to save storage, I can use --keep-interval-updates.

However, this deletes all old checkpoints. This is where --keep-interval-updates-pattern comes in. If I now do:

```
--save-interval-updates 1000
--keep-interval-updates 1
--keep-interval-updates-pattern 10000
```

This will:
1. checkpoint every 1000 updates so that job restarts don't impact us significantly
2. keep only the latest checkpoint to avoid saving a bunch of huge models in manifold
3. make an exception for #2 for every 10k updates so we can track WER over time

Reviewed By: myleott

Differential Revision: D27578403

fbshipit-source-id: 5aec2dc9a22778015f7a3daa017210190af81240"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-04-14T08:50:06Z,"enable manifold checkpoints with --keep-interval-updates

Summary: Useful to enable --keep-interval-updates with Manifold checkpoints

Reviewed By: myleott

Differential Revision: D27577116

fbshipit-source-id: 6d4ae5aaccc07ecaed8ba6a333b6ab78b148187a"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-03-30T01:02:50Z,"BASE layers (#1654)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1654

Reviewed By: myleott

Differential Revision: D27128074

Pulled By: shruti-bh

fbshipit-source-id: ac86d383cd53c9c9bdd946fea839a37b719d95e3"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-03-26T14:18:59Z,"FSDP uses new optimizer gathering to save optimizer state (#1744)

Summary:
- Full unflattened optimizer state dict is in `checkpoints/shard_0.pt`, other checkpoint files do not have the `last_optimizer_state` key.
- requires master version of fairscale (eventually fairscale>=0.3.3)

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1744

Reviewed By: myleott

Differential Revision: D27342305

Pulled By: sshleifer

fbshipit-source-id: 7442b8c6ed01599d8ab0050213e84051f4e98acd"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-03-04T21:32:44Z,"Move checkpoint state_dict creation into Trainer (#1666)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1666

Context: the checkpoint saving call stack has become a bit convoluted:
```
train.py
+ checkpoint_utils.save_checkpoint
 + trainer.save_checkpoint
  + checkpoint_utils.save_state
   + checkpoint_utils.torch_persistent_save
```

This diff slightly simplifies the checkpoint saving logic by exposing a `state_dict` method inside the Trainer. This simplifies the call stack to:
```
train.py
+ checkpoint_utils.save_checkpoint
 + trainer.save_checkpoint
  + checkpoint_utils.torch_persistent_save
```

This new structure is important for the FullyShardedDataParallel diff (next diff in the stack), since it enables the Trainer to save multiple checkpoints for the different optimizer state shards.

Test Plan:
- unit tests
- trained WMT En-De models; confirmed checkpoints save/load properly, resuming from a checkpoint gives identical results
- `buck test fblearner/flow/projects/langtech/translation:tests` (2 failures are in trunk too): https://www.internalfb.com/intern/testinfra/testconsole/testrun/2533274840914654/

Reviewed By: zhengwy888

Differential Revision: D26771146

Pulled By: myleott

fbshipit-source-id: 10f91979cd42205c1d8abcaa9ab56f63eba31e93"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-03-04T05:17:29Z,"minor fixes and improvements (#1671)

Summary:
there are a few changes here:
- convert config persisted in checkpoints into a plain dict when saving and back to omegaconf config when loading: this helps avoid compatibility issues between different versions of python, omegaconf, etc
- update checkpoints that have old print_alignment saved
- add lr_float to composite optimizer to enable sweeping on lr with auto sweepers like ax
- fixing some edge cases for config loading

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1671

Reviewed By: myleott

Differential Revision: D26791583

Pulled By: alexeib

fbshipit-source-id: 124dec74932052925c43b6a93130f4428803cb46"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-03-02T17:26:03Z,"ioPath async - opt-in Fairseq integration (#1635)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1635

**Summary:** Integrate ioPath's async writes feature into Fairseq checkpoint writing.

**Details:**
- Created new checkpoint config param `--write-checkpoints-asynchronously` with default value `False`. Aliased to `--save-async`.
- Added to `PathManager` class in `file_io.py` to include `PathManager.opena(...)` and `PathManager.async_close()`. These new methods use ioPath's async `PathManager`.

**Usage:**
```
python train.py --save-async
```
---------
NOTE: **QUESTIONS**
1. In the current implementation, we don't save `checkpoint_best` and `checkpoint_latest` since ioPath doesn't yet have a ""wait until a file is written and then copy/move it to another path"" feature. Is this okay for now?
2. Should I mimic the atomic vs non-atomic save structure that synchronous Fairseq checkpoint writes have?

**Note to Eric:** Keep this integration in check with D26375501.

Reviewed By: myleott

Differential Revision: D26467815

fbshipit-source-id: 50068ef7bf9a6d5cea4d5e0d13d672604dc4a6b0"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-02-11T22:00:25Z,"save task state in the checkpoint (#1562)

Summary:
this allows tasks to declare some properties they'd like to save in the checkpoint (such as a dictionary), which are loaded when checkpoint is restored.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1562

Test Plan: tested by training a new wav2vec model, then finetuning it, then decoding it and making sure the dict only loaded once, during fine tuning process (and was obtained from checkpoint for decoding)

Reviewed By: myleott, gwenzek

Differential Revision: D25937974

Pulled By: alexeib

fbshipit-source-id: b9908042f76ec8cda943f33885eb9b1f121662ae"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2021-02-02T23:51:11Z,"Fix the task data arg conversion to string.

Summary: We were getting some test failures on our end due to incompatibility of task data argument type. The actual exception is defined in this task: T83395097 and T83395052. Fixing the task data arg to be a string instead of list of strings.

Reviewed By: myleott

Differential Revision: D26205482

fbshipit-source-id: d29d1ee7c469177e8bdad7ca603938f8450bd81c"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-12-31T04:16:56Z,"Fix incorrect local cache for checkpoint_last.pt when training is restarted on the same host

Reviewed By: myleott

Differential Revision: D25719057

fbshipit-source-id: 2bf7dd93b6d223804da0326a0ed347e5e353f1f0"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-12-23T19:16:56Z,"fairseq checkpoint improvements

Reviewed By: myleott

Differential Revision: D25677238

fbshipit-source-id: b43075034c953491211f19a5464148de4758df83"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-12-18T19:45:08Z,"Refactor eval_lm to support library usage (#1513)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1513

Test Plan: Imported from OSS

Reviewed By: alexeib

Differential Revision: D25570467

Pulled By: myleott

fbshipit-source-id: 062f748e287797f4f01c605e0b544ef3e698851f"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-12-18T15:40:49Z,"Support atomic saves for checkpoints (#1520)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1520

Test Plan: Imported from OSS

Reviewed By: stephenroller

Differential Revision: D25632782

Pulled By: myleott

fbshipit-source-id: bdbe2aed6254d0b023b33f8027dfbd939f1fd271"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-12-16T01:47:42Z,"Fix loading of very old checkpoints (#1512)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1512

See https://github.com/pytorch/fairseq/issues/3032 for context

Test Plan: Imported from OSS

Reviewed By: alexeib

Differential Revision: D25570470

Pulled By: myleott

fbshipit-source-id: 9227b1ca36cd81ff72acdb5e03fd574e3e8769be"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-12-08T23:49:12Z,"fix wav2vec scripts (#1494)

Summary:
fixes #2942
+ docs + migration of old models

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1494

Reviewed By: myleott

Differential Revision: D25404601

Pulled By: alexeib

fbshipit-source-id: 092f145602522f8e7ea3eaa709bfe602a4d29d8b"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-12-05T15:37:51Z,"Rename optimization.min_lr -> optimization.stop_min_lr (#1486)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1486

Test Plan: Imported from OSS

Reviewed By: alexeib

Differential Revision: D25342181

Pulled By: myleott

fbshipit-source-id: 7d1cfb26334fff26d688648724ab073e5fb956f5"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-11-30T22:20:36Z,"Add/fix tests (#1468)

Summary:
- add test for loading ensemble checkpoints (and confirmed it fails if I revert: https://github.com/pytorch/fairseq/commit/265791b727b664d4d7da3abd918a3f6fb70d7337)
- add test for LayerDrop (and fix it)

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1468

Reviewed By: alexeib

Differential Revision: D25223272

Pulled By: myleott

fbshipit-source-id: 3f06f753605af251567c70d2961f5506ea423499"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-11-18T01:08:13Z,"fix loading ensembles (#1442)

Summary:
fixes loading ensembles. previous change used the state of the first model for all models in the ensemble

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1442

Reviewed By: chtran

Differential Revision: D25035706

Pulled By: alexeib

fbshipit-source-id: 9029999be0f1703efb1df20bec2890de59449f1f"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-11-11T18:15:10Z,"load dataset with saved task config (optionally) (#1423)

Summary:
this adds an argument to load_dataset that provides task configuration from the checkpoint. different tasks can decide what to do with it afterwards.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1423

Reviewed By: myleott

Differential Revision: D24875706

Pulled By: alexeib

fbshipit-source-id: 5bb1e2b7495520c456024dc7b0751b65cb05b473"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-11-09T23:46:00Z,"migrate wav2vec2 model (#1409)

Summary:
see title
also includes some minor bug fixes

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1409

Reviewed By: myleott

Differential Revision: D24822219

Pulled By: alexeib

fbshipit-source-id: b18f9a8af42ced37880c23dd6ad1ec4df3dfc040"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-11-05T23:29:33Z,"Move TPU grad reductions out of Trainer into TPUDistributedDataParallel (#1397)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1397

Data parallel command: `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --share-decoder-input-output-embed --lr 0.0001`

Data parallel before:
```
2020-11-04 08:20:13 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:20:13 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:20:13 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2020-11-04 08:20:13 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:20:14 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:20:14 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:20:14 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:20:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=5
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108002, wps=160870, ups=4.91, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.8, wps=517232, ups=15.77, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.4, wps=537322, ups=16.38, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492.1, wps=540488, ups=16.48, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603.2, wps=543411, ups=16.57, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=540359, ups=16.47, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:20 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=533446, ups=16.26, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:20 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.4, wps=539734, ups=16.46, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=6
```

Data parallel after:
```
2020-11-04 08:14:02 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:14:02 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:14:02 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2020-11-04 08:14:02 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:14:03 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:14:03 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:14:03 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:14:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108002, wps=157099, ups=4.79, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.8, wps=560049, ups=17.08, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.4, wps=558507, ups=17.03, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492.1, wps=514194, ups=15.68, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603.2, wps=552676, ups=16.85, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:09 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=546402, ups=16.66, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:09 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=508472, ups=15.5, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:09 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.4, wps=552493, ups=16.84, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=6
```

Data parallel command (no_c10d): `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --share-decoder-input-output-embed --lr 0.0001 --dp-backend no_c10d`

Data parallel before:
```
2020-11-04 08:19:25 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:19:25 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:19:25 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2020-11-04 08:19:25 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:19:25 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:19:26 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:19:26 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:19:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:19:31 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108001, wps=141659, ups=4.32, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.9, wps=503762, ups=15.36, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.5, wps=488599, ups=14.9, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492, wps=507855, ups=15.48, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603, wps=503270, ups=15.34, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=7
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=467778, ups=14.26, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=7
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=503800, ups=15.36, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=7
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.3, wps=468486, ups=14.28, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=7
```

Data parallel after:
```
2020-11-04 08:14:50 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:14:50 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:14:50 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2020-11-04 08:14:50 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:14:50 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:14:51 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:14:51 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:14:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108001, wps=137677, ups=4.2, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.9, wps=519541, ups=15.84, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.5, wps=517063, ups=15.76, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492, wps=490728, ups=14.95, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603, wps=505262, ups=15.41, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=508874, ups=15.52, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:57 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=518028, ups=15.79, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:57 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.3, wps=515996, ups=15.73, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=7
```

Model parallel command: `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm_megatron --decoder-layers 4 --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --model-parallel-size 2 --share-decoder-input-output-embed --lr 0.0001`

Model parallel before:
```
2020-11-04 08:18:38 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:18:38 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:18:38 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last-model_part-0.pt
2020-11-04 08:18:38 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:18:38 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:18:39 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:18:39 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:18:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:18:45 | INFO | train_inner | epoch 001:      2 / 7173 loss=55.997, ppl=7.19017e+16, wps=0, ups=0, wpb=16384, bsz=32, num_updates=1, lr=0.0001, gnorm=14.03, loss_scale=64, train_wall=1, wall=7
2020-11-04 08:18:45 | INFO | train_inner | epoch 001:      3 / 7173 loss=28.372, ppl=3.47501e+08, wps=48371.7, ups=2.95, wpb=16384, bsz=32, num_updates=2, lr=0.0001, gnorm=15.339, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      4 / 7173 loss=15.855, ppl=59276.8, wps=72422.5, ups=4.42, wpb=16384, bsz=32, num_updates=3, lr=0.0001, gnorm=4.189, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      5 / 7173 loss=14.713, ppl=26858.7, wps=72933.5, ups=4.45, wpb=16384, bsz=32, num_updates=4, lr=0.0001, gnorm=4.751, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      6 / 7173 loss=13.901, ppl=15299.7, wps=71974.8, ups=4.39, wpb=16384, bsz=32, num_updates=5, lr=0.0001, gnorm=4.361, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      7 / 7173 loss=13.312, ppl=10169.5, wps=72897.8, ups=4.45, wpb=16384, bsz=32, num_updates=6, lr=0.0001, gnorm=3.307, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:18:47 | INFO | train_inner | epoch 001:      8 / 7173 loss=12.914, ppl=7720.21, wps=73044.6, ups=4.46, wpb=16384, bsz=32, num_updates=7, lr=0.0001, gnorm=5.473, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:18:47 | INFO | train_inner | epoch 001:      9 / 7173 loss=12.56, ppl=6036.72, wps=73453.1, ups=4.48, wpb=16384, bsz=32, num_updates=8, lr=0.0001, gnorm=6.112, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:18:47 | INFO | train_inner | epoch 001:     10 / 7173 loss=12.116, ppl=4437.77, wps=73442.6, ups=4.48, wpb=16384, bsz=32, num_updates=9, lr=0.0001, gnorm=4.415, loss_scale=64, train_wall=0, wall=9
```

Model parallel after:
```
2020-11-04 08:12:09 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:12:09 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:12:09 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last-model_part-0.pt
2020-11-04 08:12:09 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:12:09 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:12:10 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:12:10 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:12:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      2 / 7173 loss=55.997, ppl=7.19017e+16, wps=0, ups=0, wpb=16384, bsz=32, num_updates=1, lr=0.0001, gnorm=14.03, loss_scale=64, train_wall=1, wall=8
2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      3 / 7173 loss=28.372, ppl=3.47501e+08, wps=53097, ups=3.24, wpb=16384, bsz=32, num_updates=2, lr=0.0001, gnorm=15.339, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      4 / 7173 loss=15.855, ppl=59276.8, wps=72355.5, ups=4.42, wpb=16384, bsz=32, num_updates=3, lr=0.0001, gnorm=4.189, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      5 / 7173 loss=14.713, ppl=26858.7, wps=70526.4, ups=4.3, wpb=16384, bsz=32, num_updates=4, lr=0.0001, gnorm=4.751, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      6 / 7173 loss=13.901, ppl=15299.7, wps=73063.5, ups=4.46, wpb=16384, bsz=32, num_updates=5, lr=0.0001, gnorm=4.361, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      7 / 7173 loss=13.312, ppl=10169.5, wps=73559.4, ups=4.49, wpb=16384, bsz=32, num_updates=6, lr=0.0001, gnorm=3.307, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      8 / 7173 loss=12.914, ppl=7720.21, wps=72693.2, ups=4.44, wpb=16384, bsz=32, num_updates=7, lr=0.0001, gnorm=5.473, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      9 / 7173 loss=12.56, ppl=6036.72, wps=73531.2, ups=4.49, wpb=16384, bsz=32, num_updates=8, lr=0.0001, gnorm=6.112, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:12:19 | INFO | train_inner | epoch 001:     10 / 7173 loss=12.116, ppl=4437.77, wps=73187.6, ups=4.47, wpb=16384, bsz=32, num_updates=9, lr=0.0001, gnorm=4.415, loss_scale=64, train_wall=0, wall=10
```

Test Plan: Imported from OSS

Reviewed By: ngoyal2707

Differential Revision: D24729295

Pulled By: myleott

fbshipit-source-id: beee8bdece3eaa0419a2e813990420411e507c75"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-10-23T07:07:33Z,"refactor dataclass related files, add proper types for static checkin… (#1371)

Summary:
- refactor dataclass/ hierarchy to make it a bit more sane (while avoiding circular references)
- add top level FairseqConfig
- change typehints to reflect the correct config type if it is known

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1371

Reviewed By: myleott

Differential Revision: D24469026

Pulled By: alexeib

fbshipit-source-id: 01f68918f761d51ec5216286b8959ad35f41a7b2"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-10-22T23:31:49Z,"rename remove_bpe to post_process; add aliasing (#1369)

Summary:
some binaries (e.g. speech based ones) used --post-process, some used --remove-bpe. --post-process seems more appropriate as it does more than just remove bpe at the moment. this renames remove_bpe to post_process, adds alias so existing command lines would work and adds checkpoint upgrades so they continue to work also.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1369

Reviewed By: myleott

Differential Revision: D24465040

Pulled By: alexeib

fbshipit-source-id: 1b3e388291ccc403e76e069ef6606b80ead863a7"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-10-20T07:32:26Z,"Enable Hydra configs in fairseq (#1343) (#1510)

Summary:
Pull Request resolved: https://github.com/facebookresearch/pytext/pull/1510

this is the main pr that switches on hydra functionality in fairseq

we migrate ""args"" object into omegaconf ""DictConfig"" at all legacy entry points

in addition this migrates various components from secondary registries (like bpe encoders and tokenizers) to make the migration smoother

i am going through code that references migrated fairseq components and changing it to inherit from ""Legacy*"" components instead. hopefully tests will catch most of this

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1343

Reviewed By: myleott

Differential Revision: D23973928

Pulled By: alexeib

fbshipit-source-id: dd9554981fff51ea75c1ff343874d1d6e61793c9"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-10-19T01:14:51Z,"Apply black+isort (#1357)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1357

Reviewed By: alexeib

Differential Revision: D24377772

fbshipit-source-id: 51581af041d42d62166b33a35a1a4228b1a76f0c"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-10-12T17:55:40Z,"Support generation with huge pipeline parallel Transformer models (#1297)

Summary:
## What is this PR about?
* Support loading sharded checkpoints (loading and saving checkpoints without sharding for 30B models runs OOM)
* Ability to provide a fixed dictionary for multilingual machine translation to match the dictionary used for training the checkpoint
* Support generation with PipelineParallelTransformer models

## Testing

```
python fairseq_cli/generate.py \
    /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted  \
    --batch-size 1 \
    --path /large_exp/angelafan/checkpoints/shru/mmmt_5b_checkpoints_for_master/checkpoint4.pt \
    -s en -t es --remove-bpe 'sentencepiece' --beam 5 \
    --task translation_multi_simple_epoch \
    --lang-pairs /private/home/angelafan/mmt/lang100_bt_pairs.txt \
    --decoder-langtok --encoder-langtok src --gen-subset valid --fp16 \
    --dataset-impl mmap \
    --distributed-world-size 1 --distributed-no-spawn \
    --pipeline-model-parallel \
    --pipeline-chunks 1 \
    --pipeline-encoder-balance '[26]' \
    --pipeline-encoder-devices '[0]' \
    --pipeline-decoder-balance '[26]' \
    --pipeline-decoder-devices '[0]' \
    --fixed-dictionary /private/home/shru/projects/fairseq-py-kakaoval-mmt-save-redist-gen/dict.100langs.txt
2020-09-23 23:18:27 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might ca
use misalignment in pretraining and finetuning.
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['af', 'am', 'ar', 'arbt', 'ast', 'az', 'azbt', 'ba', 'be', 'bebt', 'bg', 'bgbt', 'bn', 'bnbt', 'br', 'bs', 'ca', 'ceb', 'cs', 'csb
t', 'cy', 'da', 'de', 'debt', 'el', 'en', 'es', 'esbt', 'et', 'etbt', 'fa', 'fabt', 'ff', 'fi', 'fr', 'frbt', 'fy', 'ga', 'gd', 'gl', 'gu', 'ha', 'he', 'hebt', 'hi', 'hibt', 'hr', 'ht', 'hu', 'hubt', 'hy', 'hybt', 'id', 'ig', 'ilo', 'is',
 'it', 'ja', 'jabt', 'jv', 'ka', 'kabt', 'kk', 'km', 'kn', 'ko', 'kobt', 'lb', 'lg', 'ln', 'lo', 'lt', 'lv', 'mg', 'mk', 'mkbt', 'ml', 'mn', 'mr', 'mrbt', 'ms', 'msbt', 'my', 'ne', 'nebt', 'nl', 'no', 'ns', 'oc', 'or', 'pa', 'pl', 'ps', '
pt', 'ro', 'robt', 'ru', 'sd', 'si', 'sk', 'sl', 'so', 'sq', 'sr', 'srbt', 'ss', 'su', 'sv', 'sw', 'ta', 'th', 'tl', 'tn', 'tr', 'trbt', 'uk', 'ur', 'uz', 'vi', 'vibt', 'wo', 'xh', 'yi', 'yo', 'zh', 'zhbt', 'zu']
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [en] dictionary: 128112 types
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [es] dictionary: 128112 types
2020-09-23 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-09-23 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:en-es': 1}
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:en-es src_langtok: 128022; tgt_langtok: 128023
2020-09-23 23:18:27 | INFO | fairseq.data.data_utils | loaded 4231 examples from: /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted/valid.en-es.en
2020-09-23 23:18:27 | INFO | fairseq.data.data_utils | loaded 4231 examples from: /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted/valid.en-es.es
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted valid en-es 4231 examples
2020-09-23 23:18:27 | INFO | fairseq_cli.generate | loading model(s) from /large_exp/angelafan/checkpoints/mmmt_100_langs_new_dict_5b_model/checkpoint4.pt
balance=[29,22,1], devices=[0,1,0], chunks=8, checkpoint=always
2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler order indices time: 0:00:00.004368
2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler filter_by_size time: 0:00:00.057953
2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler batch_by_size time: 0:00:01.158644
2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:01.223290
2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
S-2521  __en__ No.
T-2521  No.
H-2521  -2.4406352043151855     y no.
D-2521  -2.4406352043151855     y no.
P-2521  -5.2677 -4.4289 -0.1922 -2.1578 -0.1565
S-2261  __en__ Why?
T-2261  ¿Por qué?
H-2261  -1.7077901363372803     ¿Y por qué?
D-2261  -1.7077901363372803     ¿Y por qué?
P-2261  -5.2733 -0.7970 -3.7643 -0.5474 -0.3339 -1.0629 -0.1757
```

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1297

Reviewed By: myleott, msbaines

Differential Revision: D23991647

Pulled By: shruti-bh

fbshipit-source-id: 81ea1af64cbe75c8b53e050d2c2339dcb70fe1eb"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-10-09T20:34:59Z,"Improve dictionary & checkpoint reading w/ local caching

Reviewed By: myleott

Differential Revision: D24148700

fbshipit-source-id: 666300639243688939e137be748f7b76fc3c21a6"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-09-02T01:17:33Z,"Initial support for ZeRO optimizer state sharding (#1259)

Summary:
FairseqOSS will work with any optimizer and dtype.

TODO(future PR):
* support reduce instead of all_reduce
* support gradient sharding
* support parameter sharding

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1259

Test Plan:
Verified that checkpoint save and restore work.

Verified that grad_norm, loss, and ppl are identical with and without
sharding enable.

Before:

$ fairseq-train --task language_modeling   data-bin/wikitext-103   --save-dir checkpoints/transformer_wikitext-103   --arch transformer_lm --share-decoder-input-output-embed   --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 512 --sample-break-mode none   --max-tokens 2048 --update-freq 16   --max-update 50000  --memory-efficient-fp16 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50
...
2020-08-27 22:24:51 | INFO | train_inner | epoch 001:     49 / 394 loss=18.84, ppl=469411, wps=269226, ups=1.03, wpb=262144, bsz=512, num_updates=45, lr=5.72388e-06, gnorm=5.769, loss_scale=8, train_wall=1, wall=68
2020-08-27 22:24:52 | INFO | train_inner | epoch 001:     50 / 394 loss=18.787, ppl=452312, wps=256992, ups=0.98, wpb=262144, bsz=512, num_updates=46, lr=5.84885e-06, gnorm=5.512, loss_scale=8, train_wall=1, wall=69
2020-08-27 22:24:53 | INFO | train_inner | epoch 001:     51 / 394 loss=18.74, ppl=437735, wps=259178, ups=0.99, wpb=262144, bsz=512, num_updates=47, lr=5.97383e-06, gnorm=5.298, loss_scale=8, train_wall=1, wall=70
2020-08-27 22:24:54 | INFO | train_inner | epoch 001:     52 / 394 loss=18.683, ppl=420727, wps=257710, ups=0.98, wpb=262144, bsz=512, num_updates=48, lr=6.0988e-06, gnorm=5.094, loss_scale=8, train_wall=1, wall=71
2020-08-27 22:24:55 | INFO | train_inner | epoch 001:     53 / 394 loss=18.623, ppl=403794, wps=269279, ups=1.03, wpb=262144, bsz=512, num_updates=49, lr=6.22378e-06, gnorm=4.893, loss_scale=8, train_wall=1, wall=72
2020-08-27 22:24:56 | INFO | train_inner | epoch 001:     54 / 394 loss=18.574, ppl=390255, wps=264616, ups=1.01, wpb=262144, bsz=512, num_updates=50, lr=6.34875e-06, gnorm=4.684, loss_scale=8, train_wall=1, wall=73
2020-08-27 22:24:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-08-27 22:24:56 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-08-27 22:24:56 | INFO | train | epoch 001 | loss 19.736 | ppl 873122 | wps 264825 | ups 1.01 | wpb 262144 | bsz 512 | num_updates 50 | lr 6.34875e-06 | gnorm 8.898 | loss_scale 8 | train_wall 66 | wall 73
2020-08-27 22:24:56 | INFO | fairseq_cli.train | done training in 72.2 seconds

After:

$ fairseq-train --task language_modeling   data-bin/wikitext-103   --save-dir checkpoints/transformer_wikitext-103   --arch transformer_lm --share-decoder-input-output-embed   --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 512 --sample-break-mode none   --max-tokens 2048 --update-freq 16   --max-update 50000  --memory-efficient-fp16 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --zero-sharding os
...
2020-08-27 22:22:55 | INFO | train_inner | epoch 001:     49 / 394 loss=18.84, ppl=469411, wps=267663, ups=1.02, wpb=262144, bsz=512, num_updates=45, lr=5.72388e-06, gnorm=5.769, loss_scale=8, train_wall=1, wall=68
2020-08-27 22:22:56 | INFO | train_inner | epoch 001:     50 / 394 loss=18.787, ppl=452312, wps=252797, ups=0.96, wpb=262144, bsz=512, num_updates=46, lr=5.84885e-06, gnorm=5.512, loss_scale=8, train_wall=1, wall=69
2020-08-27 22:22:57 | INFO | train_inner | epoch 001:     51 / 394 loss=18.74, ppl=437735, wps=267692, ups=1.02, wpb=262144, bsz=512, num_updates=47, lr=5.97383e-06, gnorm=5.298, loss_scale=8, train_wall=1, wall=70
2020-08-27 22:22:58 | INFO | train_inner | epoch 001:     52 / 394 loss=18.683, ppl=420727, wps=267507, ups=1.02, wpb=262144, bsz=512, num_updates=48, lr=6.0988e-06, gnorm=5.094, loss_scale=8, train_wall=1, wall=71
2020-08-27 22:22:59 | INFO | train_inner | epoch 001:     53 / 394 loss=18.623, ppl=403794, wps=254410, ups=0.97, wpb=262144, bsz=512, num_updates=49, lr=6.22378e-06, gnorm=4.893, loss_scale=8, train_wall=1, wall=72
2020-08-27 22:23:00 | INFO | train_inner | epoch 001:     54 / 394 loss=18.574, ppl=390255, wps=268234, ups=1.02, wpb=262144, bsz=512, num_updates=50, lr=6.34875e-06, gnorm=4.684, loss_scale=8, train_wall=1, wall=73
2020-08-27 22:23:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-08-27 22:23:00 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-08-27 22:23:00 | INFO | train | epoch 001 | loss 19.736 | ppl 873122 | wps 263570 | ups 1.01 | wpb 262144 | bsz 512 | num_updates 50 | lr 6.34875e-06 | gnorm 8.898 | loss_scale 8 | train_wall 66 | wall 73
2020-08-27 22:23:00 | INFO | fairseq_cli.train | done training in 72.3 seconds

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Reviewed By: myleott

Differential Revision: D23432082

Pulled By: msbaines

fbshipit-source-id: 6a020b25e36a3d9283582b7d89a6a53038e5b181"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-08-14T17:24:51Z,"Misc fixes (#2448)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2448

Reviewed By: ngoyal2707

Differential Revision: D23011193

Pulled By: myleott

fbshipit-source-id: 1a29481707108e4465aca78ec1581fb79f05efba"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-08-06T17:20:39Z,"Multilingual v1: Multilingual Training with multiple bitext and monolingual datasets: add finetuning options

Summary:
A first version of XLNMT multilingual project code release: Multilingual Training with multiple bitext

- Minor changes to
    - fairseq/checkpoint_utils.py to add finetuning option instead of using restore_file which will restore from original model when being requeued.

Reviewed By: myleott

Differential Revision: D22483494

fbshipit-source-id: 733300fd6a4d185e561c793ea668047c96f616c6"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-08-04T15:25:50Z,"Fixes checkpoint_path while loading a model-parallel checkpoint (#2365)

Summary:
Fixes https://github.com/pytorch/fairseq/issues/2351

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2365

Reviewed By: pipibjc

Differential Revision: D22727384

Pulled By: myleott

fbshipit-source-id: e2ff703181a6b8f10df9b4ee7aa3f9e128c04b4e"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-05-26T22:59:59Z,"Several small fixes (incl. set default --data-buffer-size=10) (#2163)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2163

Reviewed By: ngoyal2707

Differential Revision: D21665601

Pulled By: myleott

fbshipit-source-id: 47673ff7f07acf0002c4e28380aa08ff917618ee"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-05-11T17:34:42Z,"Generalize moving of tensors to CPU in checkpoints (#2098)

Summary:
This is needed for TPUs
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2098

Reviewed By: ngoyal2707

Differential Revision: D21455095

Pulled By: myleott

fbshipit-source-id: f0f88e715884f44bc254b71f7694ac90200d92fc"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-04-14T17:58:38Z,"update checkpoint mkdir behavior (issue #1986) (#2011)

Summary:
Create checkpoint directory in ```save_checkpoint()```instead of ```load_checkpoint()```.

https://github.com/pytorch/fairseq/issues/1986
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2011

Reviewed By: ngoyal2707

Differential Revision: D21017208

Pulled By: myleott

fbshipit-source-id: 4f76afc1751f987b8ab2c170ca306d07bd5ffe83"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-03-28T03:22:36Z,"adding code to load and save model parallel checkpoint (#1119)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1119

Reviewed By: myleott

Differential Revision: D20712488

fbshipit-source-id: 941ef251c9e2deb8933d88188fac56ee8c5be9b7"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-03-11T07:36:45Z,"PySpeech TALNet: Convert to JIT and quantize

Summary:
Update the TALNet model so it can be converted to JIT format and quantized by the `jit_pyspeech_nn_model` tool.

The updated model structure is slight incompatible with the model files saved before (the model structure has the extra parameters `fc_att.weight` and `fc_att.bias`), so old model files must be loaded with `strict=False`. This diff also makes changes to allow `strict=False` where necessary.

Also renames the options of `jit_pyspeech_nn_model` to make them conform better to convention.

Reviewed By: jay-mahadeokar

Differential Revision: D20369643

fbshipit-source-id: 0950a26abc20d0ae5929c8d0a03f83cab4a59200"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-03-07T17:12:14Z,"Fix epoch reporting when restoring checkpoint (#1075)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1075

Differential Revision: D20322672

Pulled By: myleott

fbshipit-source-id: 8845a10b10442bed77670b7740afa271123a3b5d"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-03-05T00:37:24Z,"Use 1-based indexing for epochs everywhere (#1053)

Summary:
We are somewhat inconsistent in whether we're using 0-based or 1-based indexing for epochs. This should fix things to be 0-based internally, with logging and checkpoint naming still using 1-based indexing.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1053

Reviewed By: spencerp

Differential Revision: D20160715

Pulled By: myleott

fbshipit-source-id: 4ed94f9c371e1bfe29bcfa087fa6756507d6e627"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-02-19T04:07:05Z,"tts synthesis script

Summary: add a synth.py to pyspeech to run tts synthesis for a particular text.

Differential Revision: D19786089

fbshipit-source-id: 2aadd004609dfbcf477e58bc01e2b05df19c0e26"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-01-22T23:36:28Z,"fblearner pyspeech manifold migration

Summary:
was planning to migrate everything at once. but because pyspeech and tuna diverged from D19060980, it's too complicated to do the migration.

migrate save_dir to manifold in fblearner, and copy from manifold to gluster after training to keep downstream workflows happy.

Reviewed By: zdavid1995

Differential Revision: D19433205

fbshipit-source-id: bd8d969c001952d85167a63f4d55fe97b93aceb5"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-01-20T20:15:27Z,"fix the problem of passing None to format() when val_loss is None (e.… (#1633)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ x] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1633

Differential Revision: D19470727

Pulled By: myleott

fbshipit-source-id: a0bbefe20b4692306c57104f3e545912e20055e6"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-01-17T00:14:45Z,"Switch to Python logging (+ lint) (#1627)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1627

Python logging offers a number of benefits, such as logging timestamps, better
cross-library compatibility, ability to add multiple output handlers, etc.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/646

Reviewed By: spencerp

Differential Revision: D15815620

Pulled By: myleott

fbshipit-source-id: 5e64e9929b5e4b9dd5bb49bcdf7c510631907134"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2020-01-16T09:59:15Z,"Split from PR#968. add --keep-best-checkpoints (#990)

Summary:
Fixes https://github.com/fairinternal/fairseq-py/issues/968. Split --keep-best-checkpoints from the original request.
Use scores as the names to save the checkpoints
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/990

Differential Revision: D19411250

Pulled By: MultiPath

fbshipit-source-id: 82b0db614208eee54c9c0e470ad7faa6481747d5"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-12-17T01:22:11Z,"More fully deprecate --raw-text and --lazy-load (fixes #1488)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/947

Differential Revision: D19084273

Pulled By: myleott

fbshipit-source-id: de80d9abfac8e3d813a9c9b343b41327c500344e"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-12-06T02:24:56Z,"Add wrapper abstraction on top of fvcore PathManager to insulate OSS compatibility

Reviewed By: myleott

Differential Revision: D18736914

fbshipit-source-id: 897ba7463f7a8ebc0969c01c57bfe04fbf9e71c8"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-12-02T21:26:48Z,"Apply Black auto-formatting

Reviewed By: sujitoc

Differential Revision: D18738392

fbshipit-source-id: b7b7b75ef97946786c463c1887ef9a8676f030e6"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-11-22T01:50:42Z,"Quick fix for Structured Dropout checkpointing (#1406)

Summary:
Here's a quick fix for https://github.com/pytorch/fairseq/issues/1403.

To keep it short, this fix allows the user to checkpoint a translation model properly after applying layer pruning on a restored transformer file.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1406

Differential Revision: D18637540

Pulled By: myleott

fbshipit-source-id: 0f5e91e05e6579f0f459bc5293e9b14cb267322d"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-11-21T00:52:59Z,"Refactor data sharding to be specified via caller of task rather than task itself

Summary: Modifying number of shards internally to disable data sharding for batch iteration is dangerous because the caller of these tasks is not limited to fairspeq/train. So therefore we should put the onus of data sharding properly on the caller rather than the task itself.

Reviewed By: myleott

Differential Revision: D18456424

fbshipit-source-id: d46be16c441c50082f9a768d0b259e6c28a4b67b"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-10-30T19:55:54Z,"layer drop

Summary: This diff enables layer drop in transformer decoder in production training pipeline (ptt_transformer). It builds on top of the fairseq implementation D18094657 added by Angela Fan, and added additional logic to handle corresponding dropping layers at test time in exported model.

Reviewed By: jhcross

Differential Revision: D18165586

fbshipit-source-id: 373ac00268a25fa9e412edcb483becdfe792d992"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-10-27T19:10:53Z,"adding layerdrop code for training, pruning, and readme (#890)

Summary:
TEST 1: EVALUATION TIME WORKS
checked
achieves correct model perplexity: 18.68

TEST 2: TRAINING NEW MODEL WORKS
checked

without layerdrop:
--decoder-layerdrop 0 OR no flag at all
| epoch 001:     10 / 11201 loss=27.469, nll_loss=27.469, ppl=185799477.36, wps=1764, ups=0, wpb=9216.000, bsz=3.000, num_updates=7, lr=0.0004376, gnorm=25.471, clip=1.000, oom=0.000, loss_scale=8.000, wall=37, train_wall=30
| epoch 001:     20 / 11201 loss=27.443, nll_loss=27.443, ppl=182500427.22, wps=2449, ups=0, wpb=9216.000, bsz=3.000, num_updates=17, lr=0.0010626, gnorm=25.273, clip=1.000, oom=0.000, loss_scale=8.000, wall=64, train_wall=57
| epoch 001:     30 / 11201 loss=27.404, nll_loss=27.404, ppl=177612215.78, wps=2720, ups=0, wpb=9216.000, bsz=3.000, num_updates=27, lr=0.0016876, gnorm=25.136, clip=1.000, oom=0.000, loss_scale=8.000, wall=91, train_wall=84
| epoch 001:     40 / 11201 loss=27.009, nll_loss=27.009, ppl=135079983.00, wps=2865, ups=0, wpb=9216.000, bsz=3.000, num_updates=37, lr=0.0023126, gnorm=24.311, clip=1.000, oom=0.000, loss_scale=8.000, wall=119, train_wall=112
| epoch 001:     50 / 11201 loss=26.418, nll_loss=26.418, ppl=89680259.41, wps=2952, ups=0, wpb=9216.000, bsz=3.000, num_updates=47, lr=0.0029376, gnorm=22.775, clip=1.000, oom=0.000, loss_scale=8.000, wall=147, train_wall=140

with layerdrop (regularization effect should be seen in PPL):
--decoder-layerdrop 0.2

| epoch 001:     10 / 11201 loss=25.186, nll_loss=25.186, ppl=38182937.27, wps=2428, ups=0, wpb=9216.000, bsz=3.000, num_updates=8, lr=0.0005001, gnorm=17.082, clip=1.000, oom=0.000, loss_scale=16.000, wall=30, train_wall=24
| epoch 001:     20 / 11201 loss=25.270, nll_loss=25.270, ppl=40451933.50, wps=3173, ups=0, wpb=9216.000, bsz=3.000, num_updates=18, lr=0.0011251, gnorm=17.162, clip=1.000, oom=0.000, loss_scale=16.000, wall=52, train_wall=45
| epoch 001:     30 / 11201 loss=25.349, nll_loss=25.349, ppl=42752256.68, wps=3454, ups=0, wpb=9216.000, bsz=3.000, num_updates=28, lr=0.0017501, gnorm=17.370, clip=1.000, oom=0.000, loss_scale=16.000, wall=75, train_wall=68
| epoch 001:     40 / 11201 loss=25.115, nll_loss=25.115, ppl=36343806.30, wps=3619, ups=0, wpb=9216.000, bsz=3.000, num_updates=38, lr=0.0023751, gnorm=16.945, clip=1.000, oom=0.000, loss_scale=16.000, wall=97, train_wall=90
| epoch 001:     50 / 11201 loss=24.804, nll_loss=24.804, ppl=29284345.78, wps=3716, ups=0, wpb=9216.000, bsz=3.000, num_updates=48, lr=0.0030001, gnorm=16.406, clip=1.000, oom=0.000, loss_scale=16.000, wall=119, train_wall=112

TEST 3: PICKING UP TRAINING FROM EXISTING MODEL
checked

| loaded checkpoint /checkpoint/angelafan/structured_0.1_block_8_sd02/checkpoint_last.pt (epoch 272 @ 381066 updates)
| loading train data for epoch 272
| loaded 1801350 examples from: /private/home/angelafan/lm_work/fairseq-py/data-bin/wikitext-103/train

TEST 4: EVALUATING EXISTING BERT MODEL REPROS RESULTS
| [input] dictionary: 50265 types
| [label] dictionary: 9 types
| Accuracy:  0.9231651376146789
achieves correct accuracy on SST2 for this model

TEST 5: TRAINING NEW BERT MODEL WORKS
checked and works

TEST 6: NMT

without layerdrop
--encoder-layerdrop 0 --decoder-layerdrop 0 OR combinations of flag specified and not specified

| epoch 001:     10 / 92203 loss=15.820, nll_loss=15.830, ppl=58267.93, wps=4902, ups=0, wpb=1477.818, bsz=51.636, num_updates=11, lr=1.47473e-06, gnorm=7.207, clip=0.000, oom=0.000, loss_scale=128.000, wall=60, train_wall=3
| epoch 001:     20 / 92203 loss=15.523, nll_loss=15.501, ppl=46359.29, wps=5037, ups=0, wpb=1496.476, bsz=45.333, num_updates=21, lr=2.72448e-06, gnorm=6.869, clip=0.000, oom=0.000, loss_scale=128.000, wall=63, train_wall=6
| epoch 001:     30 / 92203 loss=15.185, nll_loss=15.123, ppl=35695.79, wps=5085, ups=0, wpb=1519.355, bsz=44.645, num_updates=31, lr=3.97423e-06, gnorm=6.186, clip=0.000, oom=0.000, loss_scale=128.000, wall=66, train_wall=9
| epoch 001:     40 / 92203 loss=14.940, nll_loss=14.849, ppl=29505.60, wps=5116, ups=1, wpb=1521.244, bsz=42.927, num_updates=41, lr=5.22398e-06, gnorm=5.610, clip=0.000, oom=0.000, loss_scale=128.000, wall=69, train_wall=12
| epoch 001:     50 / 92203 loss=14.745, nll_loss=14.630, ppl=25346.87, wps=5070, ups=1, wpb=1507.961, bsz=41.725, num_updates=51, lr=6.47373e-06, gnorm=5.104, clip=0.000, oom=0.000, loss_scale=128.000, wall=71, train_wall=15

with layerdrop (regularization effect should be seen in PPL)

A) works with --encoder-layerdrop 0.2 --decoder-layerdrop 0.2
B) works with different settings --encoder-layerdrop 0.3 --decoder-layerdrop 0.5
C) works with one on and one off --encoder-layerdrop 0.2 --decoder-layerdrop 0

| epoch 001:     10 / 92203 loss=15.817, nll_loss=15.828, ppl=58158.54, wps=5355, ups=0, wpb=1477.818, bsz=51.636, num_updates=11, lr=1.47473e-06, gnorm=6.959, clip=0.000, oom=0.000, loss_scale=128.000, wall=59, train_wall=3
| epoch 001:     20 / 92203 loss=15.650, nll_loss=15.641, ppl=51111.63, wps=5515, ups=0, wpb=1496.476, bsz=45.333, num_updates=21, lr=2.72448e-06, gnorm=6.825, clip=0.000, oom=0.000, loss_scale=128.000, wall=61, train_wall=6
| epoch 001:     30 / 92203 loss=15.440, nll_loss=15.408, ppl=43491.58, wps=5602, ups=0, wpb=1519.355, bsz=44.645, num_updates=31, lr=3.97423e-06, gnorm=6.576, clip=0.000, oom=0.000, loss_scale=128.000, wall=64, train_wall=8
| epoch 001:     40 / 92203 loss=15.247, nll_loss=15.193, ppl=37457.14, wps=5676, ups=1, wpb=1521.244, bsz=42.927, num_updates=41, lr=5.22398e-06, gnorm=6.124, clip=0.000, oom=0.000, loss_scale=128.000, wall=67, train_wall=11
| epoch 001:     50 / 92203 loss=15.055, nll_loss=14.977, ppl=32259.92, wps=5598, ups=1, wpb=1507.961, bsz=41.725, num_updates=51, lr=6.47373e-06, gnorm=5.661, clip=0.000, oom=0.000, loss_scale=128.000, wall=69, train_wall=14

TEST 7: PRUNING TESTCASES

A) after adding the pruning flags, model can evaluate as a full model
checked, reaches correct PPL
num. model params: 246933504
| Evaluated 217646 tokens in 196.3s (1108.99 tokens/s)
| Loss: 2.9275, Perplexity: 18.68

B) after adding pruning flags, model can be pruned. this works with multiple flag settings
checked three cases:
num. model params: 146163712
| Evaluated 217646 tokens in 106.0s (2054.07 tokens/s)
| Loss: 3.0932, Perplexity: 22.05

num. model params: 209144832
| Evaluated 217646 tokens in 162.8s (1336.99 tokens/s)
| Loss: 2.9526, Perplexity: 19.16

C) model can pick up training if you want to finetune the pruned model
checked:
| loading train data for epoch 272
| loaded 1801350 examples from: /private/home/angelafan/lm_work/fairseq-py/data-bin/wikitext-103/train
| WARNING: overflow detected, setting loss scale to: 64.0
| WARNING: overflow detected, setting loss scale to: 32.0
| epoch 272:   1500 / 5601 loss=5.015, nll_loss=5.015, ppl=32.33, wps=11598, ups=1, wpb=18432.000, bsz=6.000, num_updates=98, lr=0.0061251, gnorm=0.613, clip=1.000, oom=0.000, loss_scale=32.000, wall=156, train_wall=252396

D) works with BERT
checked:
without specifying any flags, reproduces the correct standard accuracy
with flags, produces the correct pruned accuracy

| [input] dictionary: 50265 types
| [label] dictionary: 9 types
| Accuracy:  0.9231651376146789

| [input] dictionary: 50265 types
| [label] dictionary: 9 types
| Pruning model to specified layer configuration - this works best if the model was trained with LayerDrop
| Accuracy:  0.9220183486238532
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/890

Reviewed By: edunov

Differential Revision: D18094657

Pulled By: huihuifan

fbshipit-source-id: 2bbaa2ff0039e906782694fc2038b8c17a8693e7"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-10-12T04:53:11Z,"Added option to save checkpoints using Path Manager.

Summary: Added option to save checkpoints using Path Manager.

Reviewed By: hudeven

Differential Revision: D17392754

fbshipit-source-id: 4b8e556ef8455a1548e5a083d779ed809cd785be"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-10-09T21:53:27Z,"Fix data loading memory issue in pyspeech

Summary:
We currently shard data when creating the batch iterator. This means we first load all indicese/frame lengths/handles into memory, and then do the sharding. This makes it impossible to train on large datasets with a high amount of workers  because each worker will need to load the entire dataset into memory. For training on a million hours of data (i.e. semi-supervised or unsupervised approaches) this data loading just makes it flat out impossible to use 8 GPU's.

3 changes:

1. This diff modifies the data loading such that we do the sharding while we read the handles file, rather than later. This modification is done on a task-by-task basis, since the task specifies how the data is loaded. I've tried to make the code compatible with both sharding during handle loading and sharding during batch iteration. I've currently only done the sharding during handle loading for the aligned_training task.

2. To support data sharding at data loading time and the requirement that all shards must have exactly the same # of batches, I've added a method to do this synchronization where all shards with too many batches would just truncate the extra ones, similar to what we already do.

2. In fairspeq/train.py, we are actually loading the training dataset and batch iterator twice, once in train.py and once when loading the checkpoint (which we always do regardless if there is a checkpoint). This means double the loading time which can be painful for very large files. I've removed the extraneous loading in this diff as well.

Reviewed By: yqwangustc

Differential Revision: D17750715

fbshipit-source-id: 0e6e3d363525fa5661f1c784303390ea13f46377"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-09-20T16:34:58Z,"added multilingual masked LM training (#849)

Summary:
The multilingual-RoBERTa training is working with aconneau XLM data.

Two pieces remaining:

1) `XLM` limits batch to be from same language, I am not 100% sure about the reason for that, but should be easy to implement, basically we can add `batch_by_size_and_language` instead of default `batch_by_size` function. If it's not critical, I would want to leave it out as it keeps the code very clean and simple.

2) `sample_ratio` in `ConcatDataset` works with `int` by tiling the datasets based on ratio. Currently I am handling it by sounding off the ratio to `first decimal` and then multiplying by `10`. We can see if some such simple heuristics are good enough, there are other options (we can talk about them offline).
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/849

Differential Revision: D17162460

fbshipit-source-id: d967f3d872f7a1f0aa4ea418bd362b68af9e432f"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-08-21T20:43:01Z,"Parameterized criterions (#808)

Summary:
Support criterion with parameters, such as AutoSegmentationCriterion (ASG) used in wav2letter which has a transition matrix parameter. This is needed to integrate wav2letter's ASG into PySpeech.

With this diff, parameters in criterions will be:
(1) updated by optimizers, with a configurable learning rate
(2) saved and loaded from checkpoints, preserving backward compatibility for criterions without parameters
(3) synchronized across nodes in distributed training.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/808

Reviewed By: jcai1

Differential Revision: D16934097

Pulled By: okhonko

fbshipit-source-id: 121ec9382459385c6f9cbef3a8274bec1a434038"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-08-12T16:08:16Z,"Lint

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/817

Differential Revision: D16762905

Pulled By: myleott

fbshipit-source-id: d920595bec44ed26b72dfc6fbc15c0aa107b4e56"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-08-12T14:17:21Z,"Update --restore-file logic (partially fixes #999)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1007

Differential Revision: D16762490

Pulled By: myleott

fbshipit-source-id: d67137bcf581887850323d188bb4ea643a35ac9e"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-08-10T15:16:50Z,"Add WSC task and criterion

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1004

Differential Revision: D16751443

Pulled By: myleott

fbshipit-source-id: f70acd6c7be6d69da45b5b32fe4c4eff021539ab"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-08-01T12:55:57Z,"Update PyTorch Hub interface

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/782

Differential Revision: D16542256

Pulled By: myleott

fbshipit-source-id: ea3279e7a1ce4687a5914f32b76787c419be1ffa"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-07-30T14:48:23Z,"Relicense fairseq under MIT license (#786)

Summary:
The previous BSD+PATENTS license was controversial. We have been
approved to relicense fairseq under the MIT license.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/786

Differential Revision: D16560654

Pulled By: myleott

fbshipit-source-id: f78b1beb4f2895dd7b9bfc79f5f952a2bfb94034"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-07-29T23:06:26Z,"adding glue data preprocessing scripts (#771)

Summary:
1) Added glue data pre-processing script.
2) updated README with usage.

TODO:
1) releasing fairseq dictionary and remove hardcoded path.
2) remove hard-coded path for bpe-encoding,

myleott what do you recommend for above TODOs?
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/771

Reviewed By: myleott

Differential Revision: D16547679

Pulled By: myleott

fbshipit-source-id: 6a6562d9b6215523d048fdf3daee63ffac21e231"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-07-24T23:59:07Z,"check save_dir before beginning training

Summary: I sadly discovery that my checkpoint directory wasn't globally readable after 8 hours of training. Adding this check at the beginning of train loop to keep that from happening again!

Reviewed By: myleott

Differential Revision: D16455394

fbshipit-source-id: 35959aa058150b2afb63710c468d01ebc8a12b0c"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-07-19T20:13:43Z,"Rename _load_model_ensemble -> load_model_ensemble_and_task

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/738

Differential Revision: D16377803

Pulled By: myleott

fbshipit-source-id: 6beb2f78e7464b70ff65a965d2b747cdca0ca951"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-07-01T21:10:49Z,"Fixes checkpointing bug introduced in 89e077c

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/847

Differential Revision: D16075498

Pulled By: myleott

fbshipit-source-id: 62e27a8c4764f53f181c502674dfab1e6b0537e2"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-06-30T18:35:33Z,"Add additional options for configuring writing of checkpoints

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/697

Differential Revision: D16068465

Pulled By: myleott

fbshipit-source-id: c2563c3c682e7e8406e6d7c8e895d8afbec551eb"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-06-11T18:19:58Z,"Automatically fill in default values from add_args

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/797

Differential Revision: D15761071

Pulled By: myleott

fbshipit-source-id: 257d4a2297e83da7e59baed154dbafd6bfe614bf"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-05-30T18:41:40Z,"Add --reset-dataloader

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/613

Differential Revision: D15541384

Pulled By: myleott

fbshipit-source-id: ef2c0b0a51cdf37af2ccff0546f524d49f87e65d"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-05-17T21:25:56Z,"Small features + lint

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/588

Differential Revision: D15389638

Pulled By: myleott

fbshipit-source-id: 4632ce22d51dc2c74d250bae999630095d849701"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-05-17T04:03:08Z,"Clean up sharded train iterator

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/586

Differential Revision: D15372949

Pulled By: myleott

fbshipit-source-id: c1cf1c645e8d55fc8568f23a47c45677ac9ab1da"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-05-14T19:57:12Z,"Move save/load checkpoint functions to utils

Summary:
Move `load_checkpoint`, `save_checkpoint` and `reload_train` from train.py to checkpoint_utils.py
Move `get_perplexity` from train.py to utils.py.
This will make train.py lighter and allow us to reuse all this utils functionality when fairseq is used as external library.

Reviewed By: myleott

Differential Revision: D15289607

fbshipit-source-id: 4b7c95225ac22e402bcda3497811361809110df1"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-05-11T14:56:45Z,"Add missing options to TransformerDecoderLayer

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/560

Differential Revision: D15260838

Pulled By: myleott

fbshipit-source-id: 5f80dd82775c10ce46a3e1c451ccaf0ef55bfa31"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-05-06T07:17:45Z,"Load pretrained encoder or decoder (#705)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/705

This adds functionality in fairseq to load a pretrained encoder or decoder from another pretrained model into the current model.

Reviewed By: jmp84

Differential Revision: D15207084

fbshipit-source-id: 32a710ff77389928e20793c71d312863df9dd8ae"
github.com/gordicaleksa/Open-NLLB,fairseq/checkpoint_utils.py,2019-04-30T02:50:58Z,"Merge internal changes (#654)

Summary:
- Add --add-bos-token option to LM task
- Cleanup utils.py and options.py
Pull Request resolved: https://github.com/pytorch/fairseq/pull/654

Differential Revision: D15041794

Pulled By: myleott

fbshipit-source-id: 3ad00007769d5f48308052cfd40de39c5ffa1a6e"
github.com/princeton-nlp/DinkyTrain,fairseq/checkpoint_utils.py,2022-04-26T01:00:09Z,Copy checkpoints with hard linking
github.com/princeton-nlp/DinkyTrain,fairseq/checkpoint_utils.py,2022-04-26T01:00:09Z,"Initial fairseq commit using 7e758841da9e05cb21826a60d30a563a9e189d1d

See https://github.com/pytorch/fairseq/commit/7e758841da9e05cb21826a60d30a563a9e189d1d"
github.com/thu-coai/DA-Transformer,fairseq/checkpoint_utils.py,2023-05-07T13:22:39Z,add demo & fix bugs
github.com/thu-coai/DA-Transformer,fairseq/checkpoint_utils.py,2022-07-15T03:00:28Z,first upload
github.com/gonglinyuan/metro_t0,training/fairseq/checkpoint_utils.py,2023-05-28T18:06:22Z,"Add code for pretraining and finetuning

Not ready yet, needs checking"
github.com/jungokasai/twist_decoding,fairseq/fairseq/checkpoint_utils.py,2022-05-16T22:59:30Z,init
github.com/LUMIA-Group/FourierTransformer,Summarization/fairseq/checkpoint_utils.py,2023-06-30T10:21:53Z,add codes
github.com/ChenxinAn-fdu/CoNT,fairseq/checkpoint_utils.py,2022-10-28T03:08:58Z,update fairseq code
github.com/chenllliang/ParetoMNMT,fairseq/fairseq/checkpoint_utils.py,2023-03-22T03:34:59Z,first commit
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2022-01-07T08:39:11Z,"Formatting fix: get CI green (#2860)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Applies `black` and `isort` to files

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2860

Reviewed By: Mortimerp9

Differential Revision: D33456637

Pulled By: dianaml0

fbshipit-source-id: 560b8d3a8f589cbecc92d0d21163596b5d47d609"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-12-31T02:38:12Z,"add strict option to checkpoint_utils. load_pretrained_component_from_model()

Summary: Add strict option to checkpoint_utils. load_pretrained_component_from_model()

Reviewed By: sravyapopuri388

Differential Revision: D33304224

fbshipit-source-id: 2284a21dfea7810ec212f15daadeeeb45c6dca1b"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-12-17T00:11:19Z,"formatting fix (#2816)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
fix `black` failures

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2816

Reviewed By: alexeib

Differential Revision: D33172615

Pulled By: dianaml0

fbshipit-source-id: 36b141f42941670f1bfa981041d878042feb0428"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-12-11T00:55:12Z,"Add loading from HuggingFace Hub

Summary: Add loading from HuggingFace Hub. Revised from and to replace D32697723 (accepted).

Reviewed By: pipibjc, dianaml0

Differential Revision: D32964041

fbshipit-source-id: 39676aa0ecb10454ae76b70968d5abe96ab6da54"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-11-29T20:32:59Z,"Add linting with black (#2678)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2678

Reviewed By: Mortimerp9

Differential Revision: D32653381

Pulled By: dianaml0

fbshipit-source-id: 2810d14867cd7d64f4d340740e2b590b82de47fe"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-11-19T00:38:31Z,"Merge --use-ontology-for-* into --use-ontology

Summary:
There are three options for the ontology:
* `--use-ontology-for-training`
* `--use-ontology-for-validation`
* `--use-ontology-for-balancing`

The first two must always be set together.

In the past, I observed that it's best not to use ontology for data balancing even if we use ontology for training and validation. But now I no longer observe this.

Therefore, I'm merging all these three options into one (`--use-ontology`).

In addition, I'm also moving the logic of avoiding loading teacher models out of `checkpoint_utils.py`. If you want to load a student model without loading its teachers (e.g. for prediction only), specify `arg_overrides={""ignore_teachers"": True}` when calling `load_model_ensemble`.

Reviewed By: xiaoxiao26

Differential Revision: D32518830

fbshipit-source-id: 103c6458f7927ec5ca7470109c8f956c00f514a2"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-10-23T02:51:09Z,"set num_update before loading state dict (#2491)

Summary:
## What does this PR do?
Set `model.num_updates` in `load_model_ensemble_and_task()` before loading `state_dict`, like what's done in `fairseq/trainer.py`, because a model's `state_dict` may depend on `num_update`.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2491

Reviewed By: xuqiantong

Differential Revision: D31863368

Pulled By: wnhsu

fbshipit-source-id: c70051f898819cc43b02c9f5765429e9f194aed5"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-09-13T20:20:35Z,"Back out ""Fairseq needs to store and load metadata from model state_dict"" (#3861)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/3861

backout fairseq changes. fix with a suggested, more optimal changes in checkopint utils.

Reviewed By: zhengwy888

Differential Revision: D30886481

fbshipit-source-id: 12b6dd4d5107ab4371b73a58d9a044a17c733260"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-09-09T01:18:30Z,"Fairseq needs to store and load metadata from model state_dict

Summary:
## TL;DR
Fairseq checkpoint saving and loading should mirror torch's checkpoint by saving and loading ""state_dict()._metadata"".

## Long Story:

#### What happened:
During model loading and saving, Quantization-aware-training models in Pytorch encounters a weird bug that says state_dict ""fake_weight_quant.weight.min_val"" is mismatched to ""min_vals"".

#### What was the reason:
- We found the issue in that torch uses state_dict()._metadata to store module._version, but the metadata was never store in checkpoint, nor are they loaded during checkpoint loading in fairseq.

Reviewed By: frankseide

Differential Revision: D30649933

fbshipit-source-id: ce262486b9b95fbcece463fa05c4e1903d4232d7"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-09-01T19:29:51Z,"EMA

Summary:
Adds Exponential moving average (EMA) model for Kaizen semi-supervised training https://arxiv.org/abs/2106.07759

1. Add `ema.store_ema` to enable storing EMA. EMA will be written to extra_state in the state dict while saving checkpoint.
2. `ema.ema_start_update` to control when the EMA starts accumulating
3. Tasks can use `uses_ema` property to decide if the EMA should be passed to the task. (Default is False)
4. `load_ema_from_checkpoint` can be used to load EMA model in place of the model to be used for evalutation. Pyspeech has eval-ema option for this.

```
This module has the EMA class used to store a copy of the exponentially decayed
model params.

Typical usage of EMA class involves initializing an object using an existing
model (random or from a seed model) and setting the config like ema_decay,
ema_start_update which determine how the EMA model is updated. After every
update of the model i.e. at the end of the train_step, the EMA should be updated
by passing the new model to the EMA.step function. The EMA model state dict
can be stored in the extra state under the key of ""ema"" and dumped
into a checkpoint and loaded. The EMA object can be passed to tasks
by setting task.uses_ema property.
EMA is a smoothed/ensemble model which might have better performance
when used for inference or further fine-tuning. EMA class has a
reverse function to load the EMA params into a model and use it
like a regular model.
```

Reviewed By: cruvadom

Differential Revision: D24238379

fbshipit-source-id: 879d3ba5070a614b7d365f9503af357001e875b2"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-08-02T21:36:32Z,"fixing checkpoint config upgrade for generation print_alignment (#2125)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes config upgrade conditions for upgrading generation. print_alignment

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2125

Reviewed By: myleott

Differential Revision: D30049140

Pulled By: jingfeidu

fbshipit-source-id: e613821e94d0cdb876c35bc6e3fede7affbf4628"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-07-29T23:30:41Z,"seed random suffix in checkpoint to be consistent across shards

Summary:
Currently the random suffix for saving sharded checkpoints can be different for each shard when training with FSDP and use_sharded_state=True. This makes it difficult for downstream applications to load the checkpoints properly, since each shard may have different suffixes.

This diff seeds the random suffix to be consistent across shards

Reviewed By: zhengwy888

Differential Revision: D29951167

fbshipit-source-id: 65749357e62a28978f3b46b71767204a508e1f61"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-07-29T23:30:41Z,"use pathmanager to delete old checkpoints

Summary: --keep-best-checkpoints doesn't work for Manifold paths because the current code assumes normal paths. Lets use  PathManager to properly remove them.

Reviewed By: myleott, sshleifer

Differential Revision: D29947965

fbshipit-source-id: 237a7b5aaa8293bb203ad05937decdf3b3ae2fc0"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-07-29T23:30:40Z,"use suffix when saving best checkpoints with metric

Summary: This is needed when training with FSDP + sharded state, as the checkpoints should have `-shard0.pt`

Reviewed By: sshleifer

Differential Revision: D29947728

fbshipit-source-id: d2cb7c23e1e6d027d115cb734e2f2f1c34239eba"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-07-09T23:25:02Z,"Roll back os.path.abspath change

Reviewed By: donhusa

Differential Revision: D29641968

fbshipit-source-id: eca379158055f3e38e9c053b06db56842265e53a"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-07-08T22:27:57Z,"Fix static container (#2036)

Summary:
fixes StatefulContainer being static which is a problem when you load a checkpoint with task that already has the same keys in the container

also print full path to checkpoints when saving (useful with hydra) and crash if repeatedly failing to save a checkpoint

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2036

Reviewed By: arbabu123

Differential Revision: D29608430

Pulled By: alexeib

fbshipit-source-id: 1b65c8f839e02de9110af3ec53f1e7d48a4908f7"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-07-06T22:07:31Z,"fixes tests/test_train.py to mock checkpoint.save_dir config node (#3675)

Summary:
## What does this PR do?
Some downstream users reported that errors when passing Namespace to load_checkpoint().

A recent change made the assumption that the passed object is dict like (dict or DictConfig) that have a get function.
This changes that and make sure the mocked config have checkpoint.save_dir to allow the test to run.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3675

Reviewed By: omry

Differential Revision: D29564805

Pulled By: lematt1991

fbshipit-source-id: 89308811da382667f6c5d3152ee2d6480416ee62"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-07-01T13:37:47Z,"Hydra 1.1 compatibility: Use an explicit schema for the primary config (#3659)

Summary:
## What does this PR do?
Fixes compatibility with Hydra 1.1.
The result is compatible with both Hydra 1.0 and Hydra 1.1, and will allow a smoother migration to Hydra 1.1.

At this point I am not yet removing the restriction on the Hydra version from setup.py:
1. It depends on some Hydra 1.1 changes that are not yet released (It will be compatible with 1.1.1).
2. Upgrading will result in deprecation warnings, and fixing them will break compatibility with Hydra 1.0.

There will be some followup to make the code fully compatible with 1.1 once Hydra 1.1 is the default version in fbcode.

Pull Request resolved: https://github.com/pytorch/fairseq/pull/3659

Reviewed By: omry

Differential Revision: D29498036

Pulled By: lematt1991

fbshipit-source-id: 96999cde5daad6749ef4d3ddf6a36a1e984ff201"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-06-15T21:09:52Z,"Check attributes in trainer and checkpoint loading before using them (#1970)

Summary:
## What does this PR do?
Fixes None exception when some attributes in  don't exist in cfg.

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1970

Reviewed By: alexeib

Differential Revision: D29140036

Pulled By: hikushalhere

fbshipit-source-id: 7d941bcae6bb000c281a43ca2cd0876a49912ab9"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-06-04T23:19:56Z,"Fix loading some TALNet models

Summary:
D28728718 cleaned up the ""kd_binary_cross_entropy"" criterion, but this caused loading old models trained with this criterion to fail.
This diff replaces the ""kd_binary_cross_entropy"" criterion with the ""wav2vec"" criterion when loading models, and fixes this error.

It also removes the ""log_keys"" argument if it's `None`. Some criteria (e.g. wav2vec) require this argument to be a list, and will supply a default value of `[]` when it's absent. The presence of the `None` value prevents the use of this default value and causes an error.

Differential Revision: D28901263

fbshipit-source-id: 9b33aed35e76d2c734d1d4e2cbca1ff193a8c920"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-06-04T00:49:12Z,"Teacher-student learning for TALNet

Summary:
This diff implements teacher-student learning for TALNet.

Three classes take part in the teacher-student learning:
* The task loads the teacher models;
* The model generates predictions using the teacher models, and mixes them with the original targets;
* The `Wav2VecCriterion` reads the mixed targets to compute the loss. However, it still uses the original targets to compute the MAP and MAUC metrics.

There are two types of teachers:
* Static teachers: a file that stores predictions on training data which have been produced by running a model offline;
* Dynamic teachers: model files that are loaded at the beginning of training and executed on the fly to produce predictions.

We actually no longer use static teachers. The code about static teachers are copied over from the `KnowledgeDistillationBinaryCrossEntropyCriterion` class. This class will be cleaned up in D28728718.

The teacher models are stored in the task object, and will not be saved into checkpoints.

Reviewed By: alexeib

Differential Revision: D28728707

fbshipit-source-id: 0fcfc00db2e7194a6f7ee687cad9fa72e82a028b"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-05-26T00:45:51Z,"support FSDP sharded_state checkpoint loading during inference

Summary:
using the very useful feature added by QuentinDuval https://github.com/facebookresearch/fairscale/pull/683/files , we can consolidate sharded states into a full regular states. this allows inferences on sharded state almost transparently.

The main complexity comes from trying to be smart about what kind of checkpoint the user wants to load. not sure if this is over-engineering
1. if the file checkpoint-shard0.pt exists, and `--checkpoint-shard-count` is > 1, then we load sharded FSDP checkpoint
2. if checkpoint-shard0.pt exists but --checkpoint-shard-count=1, we load consolidated FSDP checkpoint
3. if checkpoint-shard0.pt does not exist, but --checkpoint-shard-count > 1, we load model parallel checkpoint
4. otherwise we are loading a single, plain checkpoint.

In theory we could be even smarter and load shard0.pt to check how many more checkpoints are needed. this is not implemented, though it will save the user having to specify --checkpoint-shard-count.

Reviewed By: sshleifer

Differential Revision: D28563441

fbshipit-source-id: dcafcaa7c9eaf5c9ff94f55c16bb3424c98dfa59"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-05-22T07:22:05Z,"fixing s2t transformer and N-best checkpoint saving

Summary:
- fixing the default value for `encoder_freezing_updates` in s2t transformer
- fixing N-best checkpoint saving: the previous implementation compares the new checkpoint with only the previous best one but not the previous N best ones. This leads to suboptimal results on N-best checkpoint averaging.

Reviewed By: jmp84

Differential Revision: D28546493

fbshipit-source-id: 44ec6d5ab49347f392d71269c5dcfd154b00c11e"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-05-21T23:18:21Z,"attempt to make non-sharded FSDP checkpoint behave like regular checkpoint

Summary:
overall just wondering if feature is desirable. if it is, the next diff which supports loading sharded checkpoint into a consolidated state dict cleaner.

a couple advantages
1. allows resuming from other DDP trainers.
2. allows resuming into other DDP trainers. or FSDP of a different configuration.
3. none-sharded FSDP checkpoint can be loaded with regular load_model_ensemble_and_task()

For old training workflow that's not using `--use-sharded-state`, please rename the checkpoint to remove the ""-shard0"" for resuming training.

Reviewed By: sshleifer

Differential Revision: D28563032

fbshipit-source-id: ced72bed969319ab6306059721f56e29b2c3d892"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-04-14T08:50:07Z,"add keep_interval_updates_pattern

Summary:
Motivation:

I want to save checkpoints frequently, due to unreliable jobs in FB cluster that restart frequently. I want to do this without spamming Manifold storage, but still save some historical checkpoints (i.e. every 10k updates), so I can track how WER evolves over time.

To save frequently, I can use a small --save-interval-updates.

To delete old checkpoints to save storage, I can use --keep-interval-updates.

However, this deletes all old checkpoints. This is where --keep-interval-updates-pattern comes in. If I now do:

```
--save-interval-updates 1000
--keep-interval-updates 1
--keep-interval-updates-pattern 10000
```

This will:
1. checkpoint every 1000 updates so that job restarts don't impact us significantly
2. keep only the latest checkpoint to avoid saving a bunch of huge models in manifold
3. make an exception for #2 for every 10k updates so we can track WER over time

Reviewed By: myleott

Differential Revision: D27578403

fbshipit-source-id: 5aec2dc9a22778015f7a3daa017210190af81240"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-04-14T08:50:06Z,"enable manifold checkpoints with --keep-interval-updates

Summary: Useful to enable --keep-interval-updates with Manifold checkpoints

Reviewed By: myleott

Differential Revision: D27577116

fbshipit-source-id: 6d4ae5aaccc07ecaed8ba6a333b6ab78b148187a"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-03-30T01:02:50Z,"BASE layers (#1654)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1654

Reviewed By: myleott

Differential Revision: D27128074

Pulled By: shruti-bh

fbshipit-source-id: ac86d383cd53c9c9bdd946fea839a37b719d95e3"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-03-26T14:18:59Z,"FSDP uses new optimizer gathering to save optimizer state (#1744)

Summary:
- Full unflattened optimizer state dict is in `checkpoints/shard_0.pt`, other checkpoint files do not have the `last_optimizer_state` key.
- requires master version of fairscale (eventually fairscale>=0.3.3)

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1744

Reviewed By: myleott

Differential Revision: D27342305

Pulled By: sshleifer

fbshipit-source-id: 7442b8c6ed01599d8ab0050213e84051f4e98acd"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-03-04T21:32:44Z,"Move checkpoint state_dict creation into Trainer (#1666)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1666

Context: the checkpoint saving call stack has become a bit convoluted:
```
train.py
+ checkpoint_utils.save_checkpoint
 + trainer.save_checkpoint
  + checkpoint_utils.save_state
   + checkpoint_utils.torch_persistent_save
```

This diff slightly simplifies the checkpoint saving logic by exposing a `state_dict` method inside the Trainer. This simplifies the call stack to:
```
train.py
+ checkpoint_utils.save_checkpoint
 + trainer.save_checkpoint
  + checkpoint_utils.torch_persistent_save
```

This new structure is important for the FullyShardedDataParallel diff (next diff in the stack), since it enables the Trainer to save multiple checkpoints for the different optimizer state shards.

Test Plan:
- unit tests
- trained WMT En-De models; confirmed checkpoints save/load properly, resuming from a checkpoint gives identical results
- `buck test fblearner/flow/projects/langtech/translation:tests` (2 failures are in trunk too): https://www.internalfb.com/intern/testinfra/testconsole/testrun/2533274840914654/

Reviewed By: zhengwy888

Differential Revision: D26771146

Pulled By: myleott

fbshipit-source-id: 10f91979cd42205c1d8abcaa9ab56f63eba31e93"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-03-04T05:17:29Z,"minor fixes and improvements (#1671)

Summary:
there are a few changes here:
- convert config persisted in checkpoints into a plain dict when saving and back to omegaconf config when loading: this helps avoid compatibility issues between different versions of python, omegaconf, etc
- update checkpoints that have old print_alignment saved
- add lr_float to composite optimizer to enable sweeping on lr with auto sweepers like ax
- fixing some edge cases for config loading

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1671

Reviewed By: myleott

Differential Revision: D26791583

Pulled By: alexeib

fbshipit-source-id: 124dec74932052925c43b6a93130f4428803cb46"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-03-02T17:26:03Z,"ioPath async - opt-in Fairseq integration (#1635)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1635

**Summary:** Integrate ioPath's async writes feature into Fairseq checkpoint writing.

**Details:**
- Created new checkpoint config param `--write-checkpoints-asynchronously` with default value `False`. Aliased to `--save-async`.
- Added to `PathManager` class in `file_io.py` to include `PathManager.opena(...)` and `PathManager.async_close()`. These new methods use ioPath's async `PathManager`.

**Usage:**
```
python train.py --save-async
```
---------
NOTE: **QUESTIONS**
1. In the current implementation, we don't save `checkpoint_best` and `checkpoint_latest` since ioPath doesn't yet have a ""wait until a file is written and then copy/move it to another path"" feature. Is this okay for now?
2. Should I mimic the atomic vs non-atomic save structure that synchronous Fairseq checkpoint writes have?

**Note to Eric:** Keep this integration in check with D26375501.

Reviewed By: myleott

Differential Revision: D26467815

fbshipit-source-id: 50068ef7bf9a6d5cea4d5e0d13d672604dc4a6b0"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-02-11T22:00:25Z,"save task state in the checkpoint (#1562)

Summary:
this allows tasks to declare some properties they'd like to save in the checkpoint (such as a dictionary), which are loaded when checkpoint is restored.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1562

Test Plan: tested by training a new wav2vec model, then finetuning it, then decoding it and making sure the dict only loaded once, during fine tuning process (and was obtained from checkpoint for decoding)

Reviewed By: myleott, gwenzek

Differential Revision: D25937974

Pulled By: alexeib

fbshipit-source-id: b9908042f76ec8cda943f33885eb9b1f121662ae"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2021-02-02T23:51:11Z,"Fix the task data arg conversion to string.

Summary: We were getting some test failures on our end due to incompatibility of task data argument type. The actual exception is defined in this task: T83395097 and T83395052. Fixing the task data arg to be a string instead of list of strings.

Reviewed By: myleott

Differential Revision: D26205482

fbshipit-source-id: d29d1ee7c469177e8bdad7ca603938f8450bd81c"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-12-31T04:16:56Z,"Fix incorrect local cache for checkpoint_last.pt when training is restarted on the same host

Reviewed By: myleott

Differential Revision: D25719057

fbshipit-source-id: 2bf7dd93b6d223804da0326a0ed347e5e353f1f0"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-12-23T19:16:56Z,"fairseq checkpoint improvements

Reviewed By: myleott

Differential Revision: D25677238

fbshipit-source-id: b43075034c953491211f19a5464148de4758df83"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-12-18T19:45:08Z,"Refactor eval_lm to support library usage (#1513)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1513

Test Plan: Imported from OSS

Reviewed By: alexeib

Differential Revision: D25570467

Pulled By: myleott

fbshipit-source-id: 062f748e287797f4f01c605e0b544ef3e698851f"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-12-18T15:40:49Z,"Support atomic saves for checkpoints (#1520)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1520

Test Plan: Imported from OSS

Reviewed By: stephenroller

Differential Revision: D25632782

Pulled By: myleott

fbshipit-source-id: bdbe2aed6254d0b023b33f8027dfbd939f1fd271"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-12-16T01:47:42Z,"Fix loading of very old checkpoints (#1512)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1512

See https://github.com/pytorch/fairseq/issues/3032 for context

Test Plan: Imported from OSS

Reviewed By: alexeib

Differential Revision: D25570470

Pulled By: myleott

fbshipit-source-id: 9227b1ca36cd81ff72acdb5e03fd574e3e8769be"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-12-08T23:49:12Z,"fix wav2vec scripts (#1494)

Summary:
fixes #2942
+ docs + migration of old models

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1494

Reviewed By: myleott

Differential Revision: D25404601

Pulled By: alexeib

fbshipit-source-id: 092f145602522f8e7ea3eaa709bfe602a4d29d8b"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-12-05T15:37:51Z,"Rename optimization.min_lr -> optimization.stop_min_lr (#1486)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1486

Test Plan: Imported from OSS

Reviewed By: alexeib

Differential Revision: D25342181

Pulled By: myleott

fbshipit-source-id: 7d1cfb26334fff26d688648724ab073e5fb956f5"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-11-30T22:20:36Z,"Add/fix tests (#1468)

Summary:
- add test for loading ensemble checkpoints (and confirmed it fails if I revert: https://github.com/pytorch/fairseq/commit/265791b727b664d4d7da3abd918a3f6fb70d7337)
- add test for LayerDrop (and fix it)

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1468

Reviewed By: alexeib

Differential Revision: D25223272

Pulled By: myleott

fbshipit-source-id: 3f06f753605af251567c70d2961f5506ea423499"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-11-18T01:08:13Z,"fix loading ensembles (#1442)

Summary:
fixes loading ensembles. previous change used the state of the first model for all models in the ensemble

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1442

Reviewed By: chtran

Differential Revision: D25035706

Pulled By: alexeib

fbshipit-source-id: 9029999be0f1703efb1df20bec2890de59449f1f"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-11-11T18:15:10Z,"load dataset with saved task config (optionally) (#1423)

Summary:
this adds an argument to load_dataset that provides task configuration from the checkpoint. different tasks can decide what to do with it afterwards.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1423

Reviewed By: myleott

Differential Revision: D24875706

Pulled By: alexeib

fbshipit-source-id: 5bb1e2b7495520c456024dc7b0751b65cb05b473"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-11-09T23:46:00Z,"migrate wav2vec2 model (#1409)

Summary:
see title
also includes some minor bug fixes

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1409

Reviewed By: myleott

Differential Revision: D24822219

Pulled By: alexeib

fbshipit-source-id: b18f9a8af42ced37880c23dd6ad1ec4df3dfc040"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-11-05T23:29:33Z,"Move TPU grad reductions out of Trainer into TPUDistributedDataParallel (#1397)

Summary:
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1397

Data parallel command: `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --share-decoder-input-output-embed --lr 0.0001`

Data parallel before:
```
2020-11-04 08:20:13 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:20:13 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:20:13 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2020-11-04 08:20:13 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:20:14 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:20:14 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:20:14 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:20:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=5
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108002, wps=160870, ups=4.91, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.8, wps=517232, ups=15.77, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.4, wps=537322, ups=16.38, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492.1, wps=540488, ups=16.48, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603.2, wps=543411, ups=16.57, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=540359, ups=16.47, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:20 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=533446, ups=16.26, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:20:20 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.4, wps=539734, ups=16.46, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=6
```

Data parallel after:
```
2020-11-04 08:14:02 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:14:02 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:14:02 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2020-11-04 08:14:02 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:14:03 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:14:03 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:14:03 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:14:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108002, wps=157099, ups=4.79, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.8, wps=560049, ups=17.08, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.4, wps=558507, ups=17.03, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492.1, wps=514194, ups=15.68, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603.2, wps=552676, ups=16.85, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:09 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=546402, ups=16.66, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:09 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=508472, ups=15.5, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:09 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.4, wps=552493, ups=16.84, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=6
```

Data parallel command (no_c10d): `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --share-decoder-input-output-embed --lr 0.0001 --dp-backend no_c10d`

Data parallel before:
```
2020-11-04 08:19:25 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:19:25 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:19:25 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2020-11-04 08:19:25 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:19:25 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:19:26 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:19:26 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:19:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:19:31 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108001, wps=141659, ups=4.32, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.9, wps=503762, ups=15.36, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.5, wps=488599, ups=14.9, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492, wps=507855, ups=15.48, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603, wps=503270, ups=15.34, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=7
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=467778, ups=14.26, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=7
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=503800, ups=15.36, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=7
2020-11-04 08:19:32 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.3, wps=468486, ups=14.28, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=7
```

Data parallel after:
```
2020-11-04 08:14:50 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:14:50 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:14:50 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2020-11-04 08:14:50 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:14:50 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:14:51 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:14:51 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:14:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108001, wps=137677, ups=4.2, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.9, wps=519541, ups=15.84, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.5, wps=517063, ups=15.76, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492, wps=490728, ups=14.95, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603, wps=505262, ups=15.41, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=508874, ups=15.52, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:57 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=518028, ups=15.79, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6
2020-11-04 08:14:57 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.3, wps=515996, ups=15.73, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=7
```

Model parallel command: `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm_megatron --decoder-layers 4 --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --model-parallel-size 2 --share-decoder-input-output-embed --lr 0.0001`

Model parallel before:
```
2020-11-04 08:18:38 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:18:38 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:18:38 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last-model_part-0.pt
2020-11-04 08:18:38 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:18:38 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:18:39 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:18:39 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:18:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:18:45 | INFO | train_inner | epoch 001:      2 / 7173 loss=55.997, ppl=7.19017e+16, wps=0, ups=0, wpb=16384, bsz=32, num_updates=1, lr=0.0001, gnorm=14.03, loss_scale=64, train_wall=1, wall=7
2020-11-04 08:18:45 | INFO | train_inner | epoch 001:      3 / 7173 loss=28.372, ppl=3.47501e+08, wps=48371.7, ups=2.95, wpb=16384, bsz=32, num_updates=2, lr=0.0001, gnorm=15.339, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      4 / 7173 loss=15.855, ppl=59276.8, wps=72422.5, ups=4.42, wpb=16384, bsz=32, num_updates=3, lr=0.0001, gnorm=4.189, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      5 / 7173 loss=14.713, ppl=26858.7, wps=72933.5, ups=4.45, wpb=16384, bsz=32, num_updates=4, lr=0.0001, gnorm=4.751, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      6 / 7173 loss=13.901, ppl=15299.7, wps=71974.8, ups=4.39, wpb=16384, bsz=32, num_updates=5, lr=0.0001, gnorm=4.361, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      7 / 7173 loss=13.312, ppl=10169.5, wps=72897.8, ups=4.45, wpb=16384, bsz=32, num_updates=6, lr=0.0001, gnorm=3.307, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:18:47 | INFO | train_inner | epoch 001:      8 / 7173 loss=12.914, ppl=7720.21, wps=73044.6, ups=4.46, wpb=16384, bsz=32, num_updates=7, lr=0.0001, gnorm=5.473, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:18:47 | INFO | train_inner | epoch 001:      9 / 7173 loss=12.56, ppl=6036.72, wps=73453.1, ups=4.48, wpb=16384, bsz=32, num_updates=8, lr=0.0001, gnorm=6.112, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:18:47 | INFO | train_inner | epoch 001:     10 / 7173 loss=12.116, ppl=4437.77, wps=73442.6, ups=4.48, wpb=16384, bsz=32, num_updates=9, lr=0.0001, gnorm=4.415, loss_scale=64, train_wall=0, wall=9
```

Model parallel after:
```
2020-11-04 08:12:09 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2020-11-04 08:12:09 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8
2020-11-04 08:12:09 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last-model_part-0.pt
2020-11-04 08:12:09 | INFO | fairseq.trainer | loading train data for epoch 1
2020-11-04 08:12:09 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train
2020-11-04 08:12:10 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-04 08:12:10 | INFO | fairseq.trainer | begin training epoch 1
2020-11-04 08:12:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      2 / 7173 loss=55.997, ppl=7.19017e+16, wps=0, ups=0, wpb=16384, bsz=32, num_updates=1, lr=0.0001, gnorm=14.03, loss_scale=64, train_wall=1, wall=8
2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      3 / 7173 loss=28.372, ppl=3.47501e+08, wps=53097, ups=3.24, wpb=16384, bsz=32, num_updates=2, lr=0.0001, gnorm=15.339, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      4 / 7173 loss=15.855, ppl=59276.8, wps=72355.5, ups=4.42, wpb=16384, bsz=32, num_updates=3, lr=0.0001, gnorm=4.189, loss_scale=64, train_wall=0, wall=8
2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      5 / 7173 loss=14.713, ppl=26858.7, wps=70526.4, ups=4.3, wpb=16384, bsz=32, num_updates=4, lr=0.0001, gnorm=4.751, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      6 / 7173 loss=13.901, ppl=15299.7, wps=73063.5, ups=4.46, wpb=16384, bsz=32, num_updates=5, lr=0.0001, gnorm=4.361, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      7 / 7173 loss=13.312, ppl=10169.5, wps=73559.4, ups=4.49, wpb=16384, bsz=32, num_updates=6, lr=0.0001, gnorm=3.307, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      8 / 7173 loss=12.914, ppl=7720.21, wps=72693.2, ups=4.44, wpb=16384, bsz=32, num_updates=7, lr=0.0001, gnorm=5.473, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      9 / 7173 loss=12.56, ppl=6036.72, wps=73531.2, ups=4.49, wpb=16384, bsz=32, num_updates=8, lr=0.0001, gnorm=6.112, loss_scale=64, train_wall=0, wall=9
2020-11-04 08:12:19 | INFO | train_inner | epoch 001:     10 / 7173 loss=12.116, ppl=4437.77, wps=73187.6, ups=4.47, wpb=16384, bsz=32, num_updates=9, lr=0.0001, gnorm=4.415, loss_scale=64, train_wall=0, wall=10
```

Test Plan: Imported from OSS

Reviewed By: ngoyal2707

Differential Revision: D24729295

Pulled By: myleott

fbshipit-source-id: beee8bdece3eaa0419a2e813990420411e507c75"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-10-23T07:07:33Z,"refactor dataclass related files, add proper types for static checkin… (#1371)

Summary:
- refactor dataclass/ hierarchy to make it a bit more sane (while avoiding circular references)
- add top level FairseqConfig
- change typehints to reflect the correct config type if it is known

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1371

Reviewed By: myleott

Differential Revision: D24469026

Pulled By: alexeib

fbshipit-source-id: 01f68918f761d51ec5216286b8959ad35f41a7b2"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-10-22T23:31:49Z,"rename remove_bpe to post_process; add aliasing (#1369)

Summary:
some binaries (e.g. speech based ones) used --post-process, some used --remove-bpe. --post-process seems more appropriate as it does more than just remove bpe at the moment. this renames remove_bpe to post_process, adds alias so existing command lines would work and adds checkpoint upgrades so they continue to work also.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1369

Reviewed By: myleott

Differential Revision: D24465040

Pulled By: alexeib

fbshipit-source-id: 1b3e388291ccc403e76e069ef6606b80ead863a7"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-10-20T07:32:26Z,"Enable Hydra configs in fairseq (#1343) (#1510)

Summary:
Pull Request resolved: https://github.com/facebookresearch/pytext/pull/1510

this is the main pr that switches on hydra functionality in fairseq

we migrate ""args"" object into omegaconf ""DictConfig"" at all legacy entry points

in addition this migrates various components from secondary registries (like bpe encoders and tokenizers) to make the migration smoother

i am going through code that references migrated fairseq components and changing it to inherit from ""Legacy*"" components instead. hopefully tests will catch most of this

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1343

Reviewed By: myleott

Differential Revision: D23973928

Pulled By: alexeib

fbshipit-source-id: dd9554981fff51ea75c1ff343874d1d6e61793c9"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-10-19T01:14:51Z,"Apply black+isort (#1357)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1357

Reviewed By: alexeib

Differential Revision: D24377772

fbshipit-source-id: 51581af041d42d62166b33a35a1a4228b1a76f0c"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-10-12T17:55:40Z,"Support generation with huge pipeline parallel Transformer models (#1297)

Summary:
## What is this PR about?
* Support loading sharded checkpoints (loading and saving checkpoints without sharding for 30B models runs OOM)
* Ability to provide a fixed dictionary for multilingual machine translation to match the dictionary used for training the checkpoint
* Support generation with PipelineParallelTransformer models

## Testing

```
python fairseq_cli/generate.py \
    /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted  \
    --batch-size 1 \
    --path /large_exp/angelafan/checkpoints/shru/mmmt_5b_checkpoints_for_master/checkpoint4.pt \
    -s en -t es --remove-bpe 'sentencepiece' --beam 5 \
    --task translation_multi_simple_epoch \
    --lang-pairs /private/home/angelafan/mmt/lang100_bt_pairs.txt \
    --decoder-langtok --encoder-langtok src --gen-subset valid --fp16 \
    --dataset-impl mmap \
    --distributed-world-size 1 --distributed-no-spawn \
    --pipeline-model-parallel \
    --pipeline-chunks 1 \
    --pipeline-encoder-balance '[26]' \
    --pipeline-encoder-devices '[0]' \
    --pipeline-decoder-balance '[26]' \
    --pipeline-decoder-devices '[0]' \
    --fixed-dictionary /private/home/shru/projects/fairseq-py-kakaoval-mmt-save-redist-gen/dict.100langs.txt
2020-09-23 23:18:27 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might ca
use misalignment in pretraining and finetuning.
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['af', 'am', 'ar', 'arbt', 'ast', 'az', 'azbt', 'ba', 'be', 'bebt', 'bg', 'bgbt', 'bn', 'bnbt', 'br', 'bs', 'ca', 'ceb', 'cs', 'csb
t', 'cy', 'da', 'de', 'debt', 'el', 'en', 'es', 'esbt', 'et', 'etbt', 'fa', 'fabt', 'ff', 'fi', 'fr', 'frbt', 'fy', 'ga', 'gd', 'gl', 'gu', 'ha', 'he', 'hebt', 'hi', 'hibt', 'hr', 'ht', 'hu', 'hubt', 'hy', 'hybt', 'id', 'ig', 'ilo', 'is',
 'it', 'ja', 'jabt', 'jv', 'ka', 'kabt', 'kk', 'km', 'kn', 'ko', 'kobt', 'lb', 'lg', 'ln', 'lo', 'lt', 'lv', 'mg', 'mk', 'mkbt', 'ml', 'mn', 'mr', 'mrbt', 'ms', 'msbt', 'my', 'ne', 'nebt', 'nl', 'no', 'ns', 'oc', 'or', 'pa', 'pl', 'ps', '
pt', 'ro', 'robt', 'ru', 'sd', 'si', 'sk', 'sl', 'so', 'sq', 'sr', 'srbt', 'ss', 'su', 'sv', 'sw', 'ta', 'th', 'tl', 'tn', 'tr', 'trbt', 'uk', 'ur', 'uz', 'vi', 'vibt', 'wo', 'xh', 'yi', 'yo', 'zh', 'zhbt', 'zu']
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [en] dictionary: 128112 types
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [es] dictionary: 128112 types
2020-09-23 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-09-23 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:en-es': 1}
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:en-es src_langtok: 128022; tgt_langtok: 128023
2020-09-23 23:18:27 | INFO | fairseq.data.data_utils | loaded 4231 examples from: /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted/valid.en-es.en
2020-09-23 23:18:27 | INFO | fairseq.data.data_utils | loaded 4231 examples from: /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted/valid.en-es.es
2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted valid en-es 4231 examples
2020-09-23 23:18:27 | INFO | fairseq_cli.generate | loading model(s) from /large_exp/angelafan/checkpoints/mmmt_100_langs_new_dict_5b_model/checkpoint4.pt
balance=[29,22,1], devices=[0,1,0], chunks=8, checkpoint=always
2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler order indices time: 0:00:00.004368
2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler filter_by_size time: 0:00:00.057953
2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler batch_by_size time: 0:00:01.158644
2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:01.223290
2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
S-2521  __en__ No.
T-2521  No.
H-2521  -2.4406352043151855     y no.
D-2521  -2.4406352043151855     y no.
P-2521  -5.2677 -4.4289 -0.1922 -2.1578 -0.1565
S-2261  __en__ Why?
T-2261  ¿Por qué?
H-2261  -1.7077901363372803     ¿Y por qué?
D-2261  -1.7077901363372803     ¿Y por qué?
P-2261  -5.2733 -0.7970 -3.7643 -0.5474 -0.3339 -1.0629 -0.1757
```

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1297

Reviewed By: myleott, msbaines

Differential Revision: D23991647

Pulled By: shruti-bh

fbshipit-source-id: 81ea1af64cbe75c8b53e050d2c2339dcb70fe1eb"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-10-09T20:34:59Z,"Improve dictionary & checkpoint reading w/ local caching

Reviewed By: myleott

Differential Revision: D24148700

fbshipit-source-id: 666300639243688939e137be748f7b76fc3c21a6"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-09-02T01:17:33Z,"Initial support for ZeRO optimizer state sharding (#1259)

Summary:
FairseqOSS will work with any optimizer and dtype.

TODO(future PR):
* support reduce instead of all_reduce
* support gradient sharding
* support parameter sharding

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1259

Test Plan:
Verified that checkpoint save and restore work.

Verified that grad_norm, loss, and ppl are identical with and without
sharding enable.

Before:

$ fairseq-train --task language_modeling   data-bin/wikitext-103   --save-dir checkpoints/transformer_wikitext-103   --arch transformer_lm --share-decoder-input-output-embed   --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 512 --sample-break-mode none   --max-tokens 2048 --update-freq 16   --max-update 50000  --memory-efficient-fp16 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50
...
2020-08-27 22:24:51 | INFO | train_inner | epoch 001:     49 / 394 loss=18.84, ppl=469411, wps=269226, ups=1.03, wpb=262144, bsz=512, num_updates=45, lr=5.72388e-06, gnorm=5.769, loss_scale=8, train_wall=1, wall=68
2020-08-27 22:24:52 | INFO | train_inner | epoch 001:     50 / 394 loss=18.787, ppl=452312, wps=256992, ups=0.98, wpb=262144, bsz=512, num_updates=46, lr=5.84885e-06, gnorm=5.512, loss_scale=8, train_wall=1, wall=69
2020-08-27 22:24:53 | INFO | train_inner | epoch 001:     51 / 394 loss=18.74, ppl=437735, wps=259178, ups=0.99, wpb=262144, bsz=512, num_updates=47, lr=5.97383e-06, gnorm=5.298, loss_scale=8, train_wall=1, wall=70
2020-08-27 22:24:54 | INFO | train_inner | epoch 001:     52 / 394 loss=18.683, ppl=420727, wps=257710, ups=0.98, wpb=262144, bsz=512, num_updates=48, lr=6.0988e-06, gnorm=5.094, loss_scale=8, train_wall=1, wall=71
2020-08-27 22:24:55 | INFO | train_inner | epoch 001:     53 / 394 loss=18.623, ppl=403794, wps=269279, ups=1.03, wpb=262144, bsz=512, num_updates=49, lr=6.22378e-06, gnorm=4.893, loss_scale=8, train_wall=1, wall=72
2020-08-27 22:24:56 | INFO | train_inner | epoch 001:     54 / 394 loss=18.574, ppl=390255, wps=264616, ups=1.01, wpb=262144, bsz=512, num_updates=50, lr=6.34875e-06, gnorm=4.684, loss_scale=8, train_wall=1, wall=73
2020-08-27 22:24:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-08-27 22:24:56 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-08-27 22:24:56 | INFO | train | epoch 001 | loss 19.736 | ppl 873122 | wps 264825 | ups 1.01 | wpb 262144 | bsz 512 | num_updates 50 | lr 6.34875e-06 | gnorm 8.898 | loss_scale 8 | train_wall 66 | wall 73
2020-08-27 22:24:56 | INFO | fairseq_cli.train | done training in 72.2 seconds

After:

$ fairseq-train --task language_modeling   data-bin/wikitext-103   --save-dir checkpoints/transformer_wikitext-103   --arch transformer_lm --share-decoder-input-output-embed   --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 512 --sample-break-mode none   --max-tokens 2048 --update-freq 16   --max-update 50000  --memory-efficient-fp16 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --zero-sharding os
...
2020-08-27 22:22:55 | INFO | train_inner | epoch 001:     49 / 394 loss=18.84, ppl=469411, wps=267663, ups=1.02, wpb=262144, bsz=512, num_updates=45, lr=5.72388e-06, gnorm=5.769, loss_scale=8, train_wall=1, wall=68
2020-08-27 22:22:56 | INFO | train_inner | epoch 001:     50 / 394 loss=18.787, ppl=452312, wps=252797, ups=0.96, wpb=262144, bsz=512, num_updates=46, lr=5.84885e-06, gnorm=5.512, loss_scale=8, train_wall=1, wall=69
2020-08-27 22:22:57 | INFO | train_inner | epoch 001:     51 / 394 loss=18.74, ppl=437735, wps=267692, ups=1.02, wpb=262144, bsz=512, num_updates=47, lr=5.97383e-06, gnorm=5.298, loss_scale=8, train_wall=1, wall=70
2020-08-27 22:22:58 | INFO | train_inner | epoch 001:     52 / 394 loss=18.683, ppl=420727, wps=267507, ups=1.02, wpb=262144, bsz=512, num_updates=48, lr=6.0988e-06, gnorm=5.094, loss_scale=8, train_wall=1, wall=71
2020-08-27 22:22:59 | INFO | train_inner | epoch 001:     53 / 394 loss=18.623, ppl=403794, wps=254410, ups=0.97, wpb=262144, bsz=512, num_updates=49, lr=6.22378e-06, gnorm=4.893, loss_scale=8, train_wall=1, wall=72
2020-08-27 22:23:00 | INFO | train_inner | epoch 001:     54 / 394 loss=18.574, ppl=390255, wps=268234, ups=1.02, wpb=262144, bsz=512, num_updates=50, lr=6.34875e-06, gnorm=4.684, loss_scale=8, train_wall=1, wall=73
2020-08-27 22:23:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-08-27 22:23:00 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-08-27 22:23:00 | INFO | train | epoch 001 | loss 19.736 | ppl 873122 | wps 263570 | ups 1.01 | wpb 262144 | bsz 512 | num_updates 50 | lr 6.34875e-06 | gnorm 8.898 | loss_scale 8 | train_wall 66 | wall 73
2020-08-27 22:23:00 | INFO | fairseq_cli.train | done training in 72.3 seconds

# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �

Reviewed By: myleott

Differential Revision: D23432082

Pulled By: msbaines

fbshipit-source-id: 6a020b25e36a3d9283582b7d89a6a53038e5b181"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-08-14T17:24:51Z,"Misc fixes (#2448)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2448

Reviewed By: ngoyal2707

Differential Revision: D23011193

Pulled By: myleott

fbshipit-source-id: 1a29481707108e4465aca78ec1581fb79f05efba"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-08-06T17:20:39Z,"Multilingual v1: Multilingual Training with multiple bitext and monolingual datasets: add finetuning options

Summary:
A first version of XLNMT multilingual project code release: Multilingual Training with multiple bitext

- Minor changes to
    - fairseq/checkpoint_utils.py to add finetuning option instead of using restore_file which will restore from original model when being requeued.

Reviewed By: myleott

Differential Revision: D22483494

fbshipit-source-id: 733300fd6a4d185e561c793ea668047c96f616c6"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-08-04T15:25:50Z,"Fixes checkpoint_path while loading a model-parallel checkpoint (#2365)

Summary:
Fixes https://github.com/pytorch/fairseq/issues/2351

Pull Request resolved: https://github.com/pytorch/fairseq/pull/2365

Reviewed By: pipibjc

Differential Revision: D22727384

Pulled By: myleott

fbshipit-source-id: e2ff703181a6b8f10df9b4ee7aa3f9e128c04b4e"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-05-26T22:59:59Z,"Several small fixes (incl. set default --data-buffer-size=10) (#2163)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2163

Reviewed By: ngoyal2707

Differential Revision: D21665601

Pulled By: myleott

fbshipit-source-id: 47673ff7f07acf0002c4e28380aa08ff917618ee"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-05-11T17:34:42Z,"Generalize moving of tensors to CPU in checkpoints (#2098)

Summary:
This is needed for TPUs
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2098

Reviewed By: ngoyal2707

Differential Revision: D21455095

Pulled By: myleott

fbshipit-source-id: f0f88e715884f44bc254b71f7694ac90200d92fc"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-04-14T17:58:38Z,"update checkpoint mkdir behavior (issue #1986) (#2011)

Summary:
Create checkpoint directory in ```save_checkpoint()```instead of ```load_checkpoint()```.

https://github.com/pytorch/fairseq/issues/1986
Pull Request resolved: https://github.com/pytorch/fairseq/pull/2011

Reviewed By: ngoyal2707

Differential Revision: D21017208

Pulled By: myleott

fbshipit-source-id: 4f76afc1751f987b8ab2c170ca306d07bd5ffe83"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-03-28T03:22:36Z,"adding code to load and save model parallel checkpoint (#1119)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1119

Reviewed By: myleott

Differential Revision: D20712488

fbshipit-source-id: 941ef251c9e2deb8933d88188fac56ee8c5be9b7"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-03-11T07:36:45Z,"PySpeech TALNet: Convert to JIT and quantize

Summary:
Update the TALNet model so it can be converted to JIT format and quantized by the `jit_pyspeech_nn_model` tool.

The updated model structure is slight incompatible with the model files saved before (the model structure has the extra parameters `fc_att.weight` and `fc_att.bias`), so old model files must be loaded with `strict=False`. This diff also makes changes to allow `strict=False` where necessary.

Also renames the options of `jit_pyspeech_nn_model` to make them conform better to convention.

Reviewed By: jay-mahadeokar

Differential Revision: D20369643

fbshipit-source-id: 0950a26abc20d0ae5929c8d0a03f83cab4a59200"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-03-07T17:12:14Z,"Fix epoch reporting when restoring checkpoint (#1075)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1075

Differential Revision: D20322672

Pulled By: myleott

fbshipit-source-id: 8845a10b10442bed77670b7740afa271123a3b5d"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-03-05T00:37:24Z,"Use 1-based indexing for epochs everywhere (#1053)

Summary:
We are somewhat inconsistent in whether we're using 0-based or 1-based indexing for epochs. This should fix things to be 0-based internally, with logging and checkpoint naming still using 1-based indexing.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1053

Reviewed By: spencerp

Differential Revision: D20160715

Pulled By: myleott

fbshipit-source-id: 4ed94f9c371e1bfe29bcfa087fa6756507d6e627"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-02-19T04:07:05Z,"tts synthesis script

Summary: add a synth.py to pyspeech to run tts synthesis for a particular text.

Differential Revision: D19786089

fbshipit-source-id: 2aadd004609dfbcf477e58bc01e2b05df19c0e26"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-01-22T23:36:28Z,"fblearner pyspeech manifold migration

Summary:
was planning to migrate everything at once. but because pyspeech and tuna diverged from D19060980, it's too complicated to do the migration.

migrate save_dir to manifold in fblearner, and copy from manifold to gluster after training to keep downstream workflows happy.

Reviewed By: zdavid1995

Differential Revision: D19433205

fbshipit-source-id: bd8d969c001952d85167a63f4d55fe97b93aceb5"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-01-20T20:15:27Z,"fix the problem of passing None to format() when val_loss is None (e.… (#1633)

Summary:
# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?
- [ x] Did you write any new necessary tests?

## What does this PR do?
Fixes # (issue).

## PR review
Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding �
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1633

Differential Revision: D19470727

Pulled By: myleott

fbshipit-source-id: a0bbefe20b4692306c57104f3e545912e20055e6"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-01-17T00:14:45Z,"Switch to Python logging (+ lint) (#1627)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1627

Python logging offers a number of benefits, such as logging timestamps, better
cross-library compatibility, ability to add multiple output handlers, etc.

Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/646

Reviewed By: spencerp

Differential Revision: D15815620

Pulled By: myleott

fbshipit-source-id: 5e64e9929b5e4b9dd5bb49bcdf7c510631907134"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2020-01-16T09:59:15Z,"Split from PR#968. add --keep-best-checkpoints (#990)

Summary:
Fixes https://github.com/fairinternal/fairseq-py/issues/968. Split --keep-best-checkpoints from the original request.
Use scores as the names to save the checkpoints
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/990

Differential Revision: D19411250

Pulled By: MultiPath

fbshipit-source-id: 82b0db614208eee54c9c0e470ad7faa6481747d5"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-12-17T01:22:11Z,"More fully deprecate --raw-text and --lazy-load (fixes #1488)

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/947

Differential Revision: D19084273

Pulled By: myleott

fbshipit-source-id: de80d9abfac8e3d813a9c9b343b41327c500344e"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-12-06T02:24:56Z,"Add wrapper abstraction on top of fvcore PathManager to insulate OSS compatibility

Reviewed By: myleott

Differential Revision: D18736914

fbshipit-source-id: 897ba7463f7a8ebc0969c01c57bfe04fbf9e71c8"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-12-02T21:26:48Z,"Apply Black auto-formatting

Reviewed By: sujitoc

Differential Revision: D18738392

fbshipit-source-id: b7b7b75ef97946786c463c1887ef9a8676f030e6"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-11-22T01:50:42Z,"Quick fix for Structured Dropout checkpointing (#1406)

Summary:
Here's a quick fix for https://github.com/pytorch/fairseq/issues/1403.

To keep it short, this fix allows the user to checkpoint a translation model properly after applying layer pruning on a restored transformer file.
Pull Request resolved: https://github.com/pytorch/fairseq/pull/1406

Differential Revision: D18637540

Pulled By: myleott

fbshipit-source-id: 0f5e91e05e6579f0f459bc5293e9b14cb267322d"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-11-21T00:52:59Z,"Refactor data sharding to be specified via caller of task rather than task itself

Summary: Modifying number of shards internally to disable data sharding for batch iteration is dangerous because the caller of these tasks is not limited to fairspeq/train. So therefore we should put the onus of data sharding properly on the caller rather than the task itself.

Reviewed By: myleott

Differential Revision: D18456424

fbshipit-source-id: d46be16c441c50082f9a768d0b259e6c28a4b67b"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-10-30T19:55:54Z,"layer drop

Summary: This diff enables layer drop in transformer decoder in production training pipeline (ptt_transformer). It builds on top of the fairseq implementation D18094657 added by Angela Fan, and added additional logic to handle corresponding dropping layers at test time in exported model.

Reviewed By: jhcross

Differential Revision: D18165586

fbshipit-source-id: 373ac00268a25fa9e412edcb483becdfe792d992"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-10-27T19:10:53Z,"adding layerdrop code for training, pruning, and readme (#890)

Summary:
TEST 1: EVALUATION TIME WORKS
checked
achieves correct model perplexity: 18.68

TEST 2: TRAINING NEW MODEL WORKS
checked

without layerdrop:
--decoder-layerdrop 0 OR no flag at all
| epoch 001:     10 / 11201 loss=27.469, nll_loss=27.469, ppl=185799477.36, wps=1764, ups=0, wpb=9216.000, bsz=3.000, num_updates=7, lr=0.0004376, gnorm=25.471, clip=1.000, oom=0.000, loss_scale=8.000, wall=37, train_wall=30
| epoch 001:     20 / 11201 loss=27.443, nll_loss=27.443, ppl=182500427.22, wps=2449, ups=0, wpb=9216.000, bsz=3.000, num_updates=17, lr=0.0010626, gnorm=25.273, clip=1.000, oom=0.000, loss_scale=8.000, wall=64, train_wall=57
| epoch 001:     30 / 11201 loss=27.404, nll_loss=27.404, ppl=177612215.78, wps=2720, ups=0, wpb=9216.000, bsz=3.000, num_updates=27, lr=0.0016876, gnorm=25.136, clip=1.000, oom=0.000, loss_scale=8.000, wall=91, train_wall=84
| epoch 001:     40 / 11201 loss=27.009, nll_loss=27.009, ppl=135079983.00, wps=2865, ups=0, wpb=9216.000, bsz=3.000, num_updates=37, lr=0.0023126, gnorm=24.311, clip=1.000, oom=0.000, loss_scale=8.000, wall=119, train_wall=112
| epoch 001:     50 / 11201 loss=26.418, nll_loss=26.418, ppl=89680259.41, wps=2952, ups=0, wpb=9216.000, bsz=3.000, num_updates=47, lr=0.0029376, gnorm=22.775, clip=1.000, oom=0.000, loss_scale=8.000, wall=147, train_wall=140

with layerdrop (regularization effect should be seen in PPL):
--decoder-layerdrop 0.2

| epoch 001:     10 / 11201 loss=25.186, nll_loss=25.186, ppl=38182937.27, wps=2428, ups=0, wpb=9216.000, bsz=3.000, num_updates=8, lr=0.0005001, gnorm=17.082, clip=1.000, oom=0.000, loss_scale=16.000, wall=30, train_wall=24
| epoch 001:     20 / 11201 loss=25.270, nll_loss=25.270, ppl=40451933.50, wps=3173, ups=0, wpb=9216.000, bsz=3.000, num_updates=18, lr=0.0011251, gnorm=17.162, clip=1.000, oom=0.000, loss_scale=16.000, wall=52, train_wall=45
| epoch 001:     30 / 11201 loss=25.349, nll_loss=25.349, ppl=42752256.68, wps=3454, ups=0, wpb=9216.000, bsz=3.000, num_updates=28, lr=0.0017501, gnorm=17.370, clip=1.000, oom=0.000, loss_scale=16.000, wall=75, train_wall=68
| epoch 001:     40 / 11201 loss=25.115, nll_loss=25.115, ppl=36343806.30, wps=3619, ups=0, wpb=9216.000, bsz=3.000, num_updates=38, lr=0.0023751, gnorm=16.945, clip=1.000, oom=0.000, loss_scale=16.000, wall=97, train_wall=90
| epoch 001:     50 / 11201 loss=24.804, nll_loss=24.804, ppl=29284345.78, wps=3716, ups=0, wpb=9216.000, bsz=3.000, num_updates=48, lr=0.0030001, gnorm=16.406, clip=1.000, oom=0.000, loss_scale=16.000, wall=119, train_wall=112

TEST 3: PICKING UP TRAINING FROM EXISTING MODEL
checked

| loaded checkpoint /checkpoint/angelafan/structured_0.1_block_8_sd02/checkpoint_last.pt (epoch 272 @ 381066 updates)
| loading train data for epoch 272
| loaded 1801350 examples from: /private/home/angelafan/lm_work/fairseq-py/data-bin/wikitext-103/train

TEST 4: EVALUATING EXISTING BERT MODEL REPROS RESULTS
| [input] dictionary: 50265 types
| [label] dictionary: 9 types
| Accuracy:  0.9231651376146789
achieves correct accuracy on SST2 for this model

TEST 5: TRAINING NEW BERT MODEL WORKS
checked and works

TEST 6: NMT

without layerdrop
--encoder-layerdrop 0 --decoder-layerdrop 0 OR combinations of flag specified and not specified

| epoch 001:     10 / 92203 loss=15.820, nll_loss=15.830, ppl=58267.93, wps=4902, ups=0, wpb=1477.818, bsz=51.636, num_updates=11, lr=1.47473e-06, gnorm=7.207, clip=0.000, oom=0.000, loss_scale=128.000, wall=60, train_wall=3
| epoch 001:     20 / 92203 loss=15.523, nll_loss=15.501, ppl=46359.29, wps=5037, ups=0, wpb=1496.476, bsz=45.333, num_updates=21, lr=2.72448e-06, gnorm=6.869, clip=0.000, oom=0.000, loss_scale=128.000, wall=63, train_wall=6
| epoch 001:     30 / 92203 loss=15.185, nll_loss=15.123, ppl=35695.79, wps=5085, ups=0, wpb=1519.355, bsz=44.645, num_updates=31, lr=3.97423e-06, gnorm=6.186, clip=0.000, oom=0.000, loss_scale=128.000, wall=66, train_wall=9
| epoch 001:     40 / 92203 loss=14.940, nll_loss=14.849, ppl=29505.60, wps=5116, ups=1, wpb=1521.244, bsz=42.927, num_updates=41, lr=5.22398e-06, gnorm=5.610, clip=0.000, oom=0.000, loss_scale=128.000, wall=69, train_wall=12
| epoch 001:     50 / 92203 loss=14.745, nll_loss=14.630, ppl=25346.87, wps=5070, ups=1, wpb=1507.961, bsz=41.725, num_updates=51, lr=6.47373e-06, gnorm=5.104, clip=0.000, oom=0.000, loss_scale=128.000, wall=71, train_wall=15

with layerdrop (regularization effect should be seen in PPL)

A) works with --encoder-layerdrop 0.2 --decoder-layerdrop 0.2
B) works with different settings --encoder-layerdrop 0.3 --decoder-layerdrop 0.5
C) works with one on and one off --encoder-layerdrop 0.2 --decoder-layerdrop 0

| epoch 001:     10 / 92203 loss=15.817, nll_loss=15.828, ppl=58158.54, wps=5355, ups=0, wpb=1477.818, bsz=51.636, num_updates=11, lr=1.47473e-06, gnorm=6.959, clip=0.000, oom=0.000, loss_scale=128.000, wall=59, train_wall=3
| epoch 001:     20 / 92203 loss=15.650, nll_loss=15.641, ppl=51111.63, wps=5515, ups=0, wpb=1496.476, bsz=45.333, num_updates=21, lr=2.72448e-06, gnorm=6.825, clip=0.000, oom=0.000, loss_scale=128.000, wall=61, train_wall=6
| epoch 001:     30 / 92203 loss=15.440, nll_loss=15.408, ppl=43491.58, wps=5602, ups=0, wpb=1519.355, bsz=44.645, num_updates=31, lr=3.97423e-06, gnorm=6.576, clip=0.000, oom=0.000, loss_scale=128.000, wall=64, train_wall=8
| epoch 001:     40 / 92203 loss=15.247, nll_loss=15.193, ppl=37457.14, wps=5676, ups=1, wpb=1521.244, bsz=42.927, num_updates=41, lr=5.22398e-06, gnorm=6.124, clip=0.000, oom=0.000, loss_scale=128.000, wall=67, train_wall=11
| epoch 001:     50 / 92203 loss=15.055, nll_loss=14.977, ppl=32259.92, wps=5598, ups=1, wpb=1507.961, bsz=41.725, num_updates=51, lr=6.47373e-06, gnorm=5.661, clip=0.000, oom=0.000, loss_scale=128.000, wall=69, train_wall=14

TEST 7: PRUNING TESTCASES

A) after adding the pruning flags, model can evaluate as a full model
checked, reaches correct PPL
num. model params: 246933504
| Evaluated 217646 tokens in 196.3s (1108.99 tokens/s)
| Loss: 2.9275, Perplexity: 18.68

B) after adding pruning flags, model can be pruned. this works with multiple flag settings
checked three cases:
num. model params: 146163712
| Evaluated 217646 tokens in 106.0s (2054.07 tokens/s)
| Loss: 3.0932, Perplexity: 22.05

num. model params: 209144832
| Evaluated 217646 tokens in 162.8s (1336.99 tokens/s)
| Loss: 2.9526, Perplexity: 19.16

C) model can pick up training if you want to finetune the pruned model
checked:
| loading train data for epoch 272
| loaded 1801350 examples from: /private/home/angelafan/lm_work/fairseq-py/data-bin/wikitext-103/train
| WARNING: overflow detected, setting loss scale to: 64.0
| WARNING: overflow detected, setting loss scale to: 32.0
| epoch 272:   1500 / 5601 loss=5.015, nll_loss=5.015, ppl=32.33, wps=11598, ups=1, wpb=18432.000, bsz=6.000, num_updates=98, lr=0.0061251, gnorm=0.613, clip=1.000, oom=0.000, loss_scale=32.000, wall=156, train_wall=252396

D) works with BERT
checked:
without specifying any flags, reproduces the correct standard accuracy
with flags, produces the correct pruned accuracy

| [input] dictionary: 50265 types
| [label] dictionary: 9 types
| Accuracy:  0.9231651376146789

| [input] dictionary: 50265 types
| [label] dictionary: 9 types
| Pruning model to specified layer configuration - this works best if the model was trained with LayerDrop
| Accuracy:  0.9220183486238532
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/890

Reviewed By: edunov

Differential Revision: D18094657

Pulled By: huihuifan

fbshipit-source-id: 2bbaa2ff0039e906782694fc2038b8c17a8693e7"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-10-12T04:53:11Z,"Added option to save checkpoints using Path Manager.

Summary: Added option to save checkpoints using Path Manager.

Reviewed By: hudeven

Differential Revision: D17392754

fbshipit-source-id: 4b8e556ef8455a1548e5a083d779ed809cd785be"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-10-09T21:53:27Z,"Fix data loading memory issue in pyspeech

Summary:
We currently shard data when creating the batch iterator. This means we first load all indicese/frame lengths/handles into memory, and then do the sharding. This makes it impossible to train on large datasets with a high amount of workers  because each worker will need to load the entire dataset into memory. For training on a million hours of data (i.e. semi-supervised or unsupervised approaches) this data loading just makes it flat out impossible to use 8 GPU's.

3 changes:

1. This diff modifies the data loading such that we do the sharding while we read the handles file, rather than later. This modification is done on a task-by-task basis, since the task specifies how the data is loaded. I've tried to make the code compatible with both sharding during handle loading and sharding during batch iteration. I've currently only done the sharding during handle loading for the aligned_training task.

2. To support data sharding at data loading time and the requirement that all shards must have exactly the same # of batches, I've added a method to do this synchronization where all shards with too many batches would just truncate the extra ones, similar to what we already do.

2. In fairspeq/train.py, we are actually loading the training dataset and batch iterator twice, once in train.py and once when loading the checkpoint (which we always do regardless if there is a checkpoint). This means double the loading time which can be painful for very large files. I've removed the extraneous loading in this diff as well.

Reviewed By: yqwangustc

Differential Revision: D17750715

fbshipit-source-id: 0e6e3d363525fa5661f1c784303390ea13f46377"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-09-20T16:34:58Z,"added multilingual masked LM training (#849)

Summary:
The multilingual-RoBERTa training is working with aconneau XLM data.

Two pieces remaining:

1) `XLM` limits batch to be from same language, I am not 100% sure about the reason for that, but should be easy to implement, basically we can add `batch_by_size_and_language` instead of default `batch_by_size` function. If it's not critical, I would want to leave it out as it keeps the code very clean and simple.

2) `sample_ratio` in `ConcatDataset` works with `int` by tiling the datasets based on ratio. Currently I am handling it by sounding off the ratio to `first decimal` and then multiplying by `10`. We can see if some such simple heuristics are good enough, there are other options (we can talk about them offline).
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/849

Differential Revision: D17162460

fbshipit-source-id: d967f3d872f7a1f0aa4ea418bd362b68af9e432f"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-08-21T20:43:01Z,"Parameterized criterions (#808)

Summary:
Support criterion with parameters, such as AutoSegmentationCriterion (ASG) used in wav2letter which has a transition matrix parameter. This is needed to integrate wav2letter's ASG into PySpeech.

With this diff, parameters in criterions will be:
(1) updated by optimizers, with a configurable learning rate
(2) saved and loaded from checkpoints, preserving backward compatibility for criterions without parameters
(3) synchronized across nodes in distributed training.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/808

Reviewed By: jcai1

Differential Revision: D16934097

Pulled By: okhonko

fbshipit-source-id: 121ec9382459385c6f9cbef3a8274bec1a434038"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-08-12T16:08:16Z,"Lint

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/817

Differential Revision: D16762905

Pulled By: myleott

fbshipit-source-id: d920595bec44ed26b72dfc6fbc15c0aa107b4e56"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-08-12T14:17:21Z,"Update --restore-file logic (partially fixes #999)

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1007

Differential Revision: D16762490

Pulled By: myleott

fbshipit-source-id: d67137bcf581887850323d188bb4ea643a35ac9e"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-08-10T15:16:50Z,"Add WSC task and criterion

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1004

Differential Revision: D16751443

Pulled By: myleott

fbshipit-source-id: f70acd6c7be6d69da45b5b32fe4c4eff021539ab"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-08-01T12:55:57Z,"Update PyTorch Hub interface

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/782

Differential Revision: D16542256

Pulled By: myleott

fbshipit-source-id: ea3279e7a1ce4687a5914f32b76787c419be1ffa"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-07-30T14:48:23Z,"Relicense fairseq under MIT license (#786)

Summary:
The previous BSD+PATENTS license was controversial. We have been
approved to relicense fairseq under the MIT license.
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/786

Differential Revision: D16560654

Pulled By: myleott

fbshipit-source-id: f78b1beb4f2895dd7b9bfc79f5f952a2bfb94034"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-07-29T23:06:26Z,"adding glue data preprocessing scripts (#771)

Summary:
1) Added glue data pre-processing script.
2) updated README with usage.

TODO:
1) releasing fairseq dictionary and remove hardcoded path.
2) remove hard-coded path for bpe-encoding,

myleott what do you recommend for above TODOs?
Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/771

Reviewed By: myleott

Differential Revision: D16547679

Pulled By: myleott

fbshipit-source-id: 6a6562d9b6215523d048fdf3daee63ffac21e231"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-07-24T23:59:07Z,"check save_dir before beginning training

Summary: I sadly discovery that my checkpoint directory wasn't globally readable after 8 hours of training. Adding this check at the beginning of train loop to keep that from happening again!

Reviewed By: myleott

Differential Revision: D16455394

fbshipit-source-id: 35959aa058150b2afb63710c468d01ebc8a12b0c"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-07-19T20:13:43Z,"Rename _load_model_ensemble -> load_model_ensemble_and_task

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/738

Differential Revision: D16377803

Pulled By: myleott

fbshipit-source-id: 6beb2f78e7464b70ff65a965d2b747cdca0ca951"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-07-01T21:10:49Z,"Fixes checkpointing bug introduced in 89e077c

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/847

Differential Revision: D16075498

Pulled By: myleott

fbshipit-source-id: 62e27a8c4764f53f181c502674dfab1e6b0537e2"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-06-30T18:35:33Z,"Add additional options for configuring writing of checkpoints

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/697

Differential Revision: D16068465

Pulled By: myleott

fbshipit-source-id: c2563c3c682e7e8406e6d7c8e895d8afbec551eb"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-06-11T18:19:58Z,"Automatically fill in default values from add_args

Summary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/797

Differential Revision: D15761071

Pulled By: myleott

fbshipit-source-id: 257d4a2297e83da7e59baed154dbafd6bfe614bf"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-05-30T18:41:40Z,"Add --reset-dataloader

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/613

Differential Revision: D15541384

Pulled By: myleott

fbshipit-source-id: ef2c0b0a51cdf37af2ccff0546f524d49f87e65d"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-05-17T21:25:56Z,"Small features + lint

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/588

Differential Revision: D15389638

Pulled By: myleott

fbshipit-source-id: 4632ce22d51dc2c74d250bae999630095d849701"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-05-17T04:03:08Z,"Clean up sharded train iterator

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/586

Differential Revision: D15372949

Pulled By: myleott

fbshipit-source-id: c1cf1c645e8d55fc8568f23a47c45677ac9ab1da"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-05-14T19:57:12Z,"Move save/load checkpoint functions to utils

Summary:
Move `load_checkpoint`, `save_checkpoint` and `reload_train` from train.py to checkpoint_utils.py
Move `get_perplexity` from train.py to utils.py.
This will make train.py lighter and allow us to reuse all this utils functionality when fairseq is used as external library.

Reviewed By: myleott

Differential Revision: D15289607

fbshipit-source-id: 4b7c95225ac22e402bcda3497811361809110df1"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-05-11T14:56:45Z,"Add missing options to TransformerDecoderLayer

Summary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/560

Differential Revision: D15260838

Pulled By: myleott

fbshipit-source-id: 5f80dd82775c10ce46a3e1c451ccaf0ef55bfa31"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-05-06T07:17:45Z,"Load pretrained encoder or decoder (#705)

Summary:
Pull Request resolved: https://github.com/pytorch/fairseq/pull/705

This adds functionality in fairseq to load a pretrained encoder or decoder from another pretrained model into the current model.

Reviewed By: jmp84

Differential Revision: D15207084

fbshipit-source-id: 32a710ff77389928e20793c71d312863df9dd8ae"
github.com/imatge-upc/sign-topic,fairseq/checkpoint_utils.py,2019-04-30T02:50:58Z,"Merge internal changes (#654)

Summary:
- Add --add-bos-token option to LM task
- Cleanup utils.py and options.py
Pull Request resolved: https://github.com/pytorch/fairseq/pull/654

Differential Revision: D15041794

Pulled By: myleott

fbshipit-source-id: 3ad00007769d5f48308052cfd40de39c5ffa1a6e"
github.com/SillyTavern/SillyTavern-extras,modules/voice_conversion/fairseq/checkpoint_utils.py,2023-08-10T00:58:52Z,Add monkey patched fairseq package to run on python 3.11 (what is needed for our use of RVC at least)
github.com/zjwang21/mix-phoneme-bert,fairseq/fairseq/checkpoint_utils.py,2023-03-21T10:50:41Z,first
github.com/StrongResearch/isc-demos,fairseq/fairseq/checkpoint_utils.py,2023-12-03T22:15:32Z,atomic checkpointing
github.com/StrongResearch/isc-demos,fairseq/fairseq/checkpoint_utils.py,2023-12-01T04:49:47Z,add fairseq how to
github.com/JeremyXSC/DMF,fairseq/fairseq/checkpoint_utils.py,2022-10-26T11:38:01Z,update
github.com/Rongjiehuang/TranSpeech,fairseq/checkpoint_utils.py,2023-03-29T15:24:17Z,first commit
github.com/epfml/pam,submodules/fairseq/fairseq/checkpoint_utils.py,2023-05-31T09:00:54Z,Public release
github.com/TakHemlata/SSL_Anti-spoofing,fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1/fairseq/checkpoint_utils.py,2022-05-02T17:59:04Z,SSL_w2v_spoofing_release
github.com/chenllliang/Gradient-Vaccine,fairseq/fairseq/checkpoint_utils.py,2022-07-27T17:43:39Z,first commit
github.com/fe1ixxu/Intra-Distillation,Machine_Translation/fairseq/checkpoint_utils.py,2022-05-13T23:52:30Z,rename folder
github.com/UncleTensor/AudioSubnet,fseq/fairseq/checkpoint_utils.py,2024-02-12T17:35:20Z,"Added Text to Music models, custom models option and auto_update (yes/no)"
github.com/alibaba/translation-suggestion-psgd,fairseq/checkpoint_utils.py,2023-06-08T10:01:42Z,Initial commit
github.com/jungokasai/beam_with_patience,fairseq/fairseq/checkpoint_utils.py,2022-03-24T07:18:58Z,init
github.com/hadasah/btm,fairseq/fairseq/checkpoint_utils.py,2022-11-12T02:21:49Z,add fairseq fork
github.com/demelin/transformer_lexical_shortcuts,fairseq/fairseq/checkpoint_utils.py,2023-02-14T21:20:11Z,added fairseq implementation
github.com/lsj2408/Graphormer-GD,engine/fairseq/checkpoint_utils.py,2023-03-04T05:40:53Z,init commit
github.com/gonglinyuan/ast_t5,training/fairseq/checkpoint_utils.py,2024-02-06T18:19:44Z,Reorganize
github.com/JeanKaddour/lawa,bert/fairseq/fairseq/checkpoint_utils.py,2022-10-05T16:22:43Z,Change fairseq to standalone repo
github.com/mcao516/rej-summ,fairseq/checkpoint_utils.py,2022-12-20T02:16:54Z,first commit
github.com/gpengzhi/Bi-SimCut,fairseq/fairseq/checkpoint_utils.py,2022-07-25T06:05:38Z,Add fairseq
github.com/0nutation/DUB,fairseq/checkpoint_utils.py,2023-06-01T07:27:07Z,init commit
github.com/keyonvafa/career-code,fairseq/fairseq/checkpoint_utils.py,2022-03-09T04:39:13Z,initial commit
github.com/ictnlp/DASpeech,fairseq/fairseq/checkpoint_utils.py,2023-10-10T13:47:12Z,Initial Release
github.com/Ascend/ModelZoo-PyTorch,PyTorch/contrib/audio/wav2vec2.0/fairseq/checkpoint_utils.py,2022-11-25T02:31:16Z,"!1572 [重庆大学][高校贡献][Pytorch][Wav2vec2.0]--初次提交
!1572 [重庆大学][高校贡献][Pytorch][Wav2vec2.0]--初次提交"
github.com/Ascend/ModelZoo-PyTorch,PyTorch/built-in/nlp/Data2vec_for_PyTorch/fairseq/checkpoint_utils.py,2023-06-01T07:03:34Z,"!4809 【PyTorch】【built-in】【data2vec】初次提交
* data2vec 首次提交"
github.com/Ascend/ModelZoo-PyTorch,PyTorch/built-in/nlp/Scaling-nmt_for_Pytorch/fairseq/checkpoint_utils.py,2023-07-04T13:23:06Z,"!5109 【PyTorch】【built-in】【scaling-nmt】初次提交
* scaling-nmt 初次提交"
github.com/Ascend/ModelZoo-PyTorch,PyTorch/built-in/nlp/Fairseq_Transformer_wmt18_for_PyTorch/fairseq/checkpoint_utils.py,2023-05-26T08:35:33Z,[自研][PyTorch][Fairseq_Transformer_wmt18 for PyTorch] init
github.com/MANGA-UOFA/NAUS,fairseq/checkpoint_utils.py,2022-03-16T17:57:43Z,first commit
github.com/tencent-ailab/TriNet,fairseq/checkpoint_utils.py,2023-05-29T08:04:43Z,add files
github.com/NLP2CT/kNN-TL,kNN-TL/fairseq/checkpoint_utils.py,2023-07-04T08:51:56Z,"	modified:   .DS_Store 
	modified:   README.md
	new file:   inference-scripts/build_parent_datastore.sh
	new file:   inference-scripts/inference_child.sh
	new file:   inference-scripts/inference_pseudo_parent.sh
	new file:   kNN-TL/.circleci/config.yml
	new file:   kNN-TL/.github/CODEOWNERS
	new file:   kNN-TL/.github/ISSUE_TEMPLATE.md
	new file:   kNN-TL/.github/ISSUE_TEMPLATE/bug_report.md
	new file:   kNN-TL/.github/ISSUE_TEMPLATE/documentation.md
	new file:   kNN-TL/.github/ISSUE_TEMPLATE/feature_request.md
	new file:   kNN-TL/.github/ISSUE_TEMPLATE/how-to-question.md
	new file:   kNN-TL/.github/PULL_REQUEST_TEMPLATE.md
	new file:   kNN-TL/.github/stale.yml
	new file:   kNN-TL/.github/workflows/build.yml
	new file:   kNN-TL/.github/workflows/release.yml
	new file:   kNN-TL/.gitignore
	new file:   kNN-TL/.gitmodules
	new file:   kNN-TL/.isort.cfg
	new file:   kNN-TL/.pre-commit-config.yaml
	new file:   kNN-TL/CODE_OF_CONDUCT.md
	new file:   kNN-TL/CONTRIBUTING.md
	new file:   kNN-TL/LICENSE
	new file:   kNN-TL/README.md
	new file:   kNN-TL/RELEASE.md
	new file:   kNN-TL/docs/Makefile
	new file:   kNN-TL/docs/_static/theme_overrides.css
	new file:   kNN-TL/docs/command_line_tools.rst
	new file:   kNN-TL/docs/conf.py
	new file:   kNN-TL/docs/criterions.rst
	new file:   kNN-TL/docs/data.rst
	new file:   kNN-TL/docs/docutils.conf
	new file:   kNN-TL/docs/fairseq.gif
	new file:   kNN-TL/docs/fairseq_logo.png
	new file:   kNN-TL/docs/getting_started.rst
	new file:   kNN-TL/docs/hydra_integration.md
	new file:   kNN-TL/docs/index.rst
	new file:   kNN-TL/docs/lr_scheduler.rst
	new file:   kNN-TL/docs/make.bat
	new file:   kNN-TL/docs/models.rst
	new file:   kNN-TL/docs/modules.rst
	new file:   kNN-TL/docs/optim.rst
	new file:   kNN-TL/docs/overview.rst
	new file:   kNN-TL/docs/requirements.txt
	new file:   kNN-TL/docs/tasks.rst
	new file:   kNN-TL/docs/tutorial_classifying_names.rst
	new file:   kNN-TL/docs/tutorial_simple_lstm.rst
	new file:   kNN-TL/examples/.gitignore
	new file:   kNN-TL/examples/MMPT/.gitignore
	new file:   kNN-TL/examples/MMPT/CONFIG.md
	new file:   kNN-TL/examples/MMPT/DATASET.md
	new file:   kNN-TL/examples/MMPT/README.md
	new file:   kNN-TL/examples/MMPT/endtask.md
	new file:   kNN-TL/examples/MMPT/locallaunch.py
	new file:   kNN-TL/examples/MMPT/mmpt/__init__.py
	new file:   kNN-TL/examples/MMPT/mmpt/datasets/__init__.py
	new file:   kNN-TL/examples/MMPT/mmpt/datasets/fairseqmmdataset.py
	new file:   kNN-TL/examples/MMPT/mmpt/datasets/mmdataset.py
	new file:   kNN-TL/examples/MMPT/mmpt/evaluators/__init__.py
	new file:   kNN-TL/examples/MMPT/mmpt/evaluators/evaluator.py
	new file:   kNN-TL/examples/MMPT/mmpt/evaluators/metric.py
	new file:   kNN-TL/examples/MMPT/mmpt/evaluators/predictor.py
	new file:   kNN-TL/examples/MMPT/mmpt/losses/__init__.py
	new file:   kNN-TL/examples/MMPT/mmpt/losses/fairseqmmloss.py
	new file:   kNN-TL/examples/MMPT/mmpt/losses/loss.py
	new file:   kNN-TL/examples/MMPT/mmpt/losses/nce.py
	new file:   kNN-TL/examples/MMPT/mmpt/models/__init__.py
	new file:   kNN-TL/examples/MMPT/mmpt/models/fairseqmmmodel.py
	new file:   kNN-TL/examples/MMPT/mmpt/models/mmfusion.py
	new file:   kNN-TL/examples/MMPT/mmpt/models/mmfusionnlg.py
	new file:   kNN-TL/examples/MMPT/mmpt/models/transformermodel.py
	new file:   kNN-TL/examples/MMPT/mmpt/modules/__init__.py
	new file:   kNN-TL/examples/MMPT/mmpt/modules/mm.py
	new file:   kNN-TL/examples/MMPT/mmpt/modules/retri.py
	new file:   kNN-TL/examples/MMPT/mmpt/modules/vectorpool.py
	new file:   kNN-TL/examples/MMPT/mmpt/processors/__init__.py
	new file:   kNN-TL/examples/MMPT/mmpt/processors/dedupprocessor.py
	new file:   kNN-TL/examples/MMPT/mmpt/processors/dsprocessor.py
	new file:   kNN-TL/examples/MMPT/mmpt/processors/how2processor.py
	new file:   kNN-TL/examples/MMPT/mmpt/processors/how2retriprocessor.py
	new file:   kNN-TL/examples/MMPT/mmpt/processors/models/s3dg.py
	new file:   kNN-TL/examples/MMPT/mmpt/processors/processor.py
	new file:   kNN-TL/examples/MMPT/mmpt/tasks/__init__.py
	new file:   kNN-TL/examples/MMPT/mmpt/tasks/fairseqmmtask.py
	new file:   kNN-TL/examples/MMPT/mmpt/tasks/milncetask.py
	new file:   kNN-TL/examples/MMPT/mmpt/tasks/retritask.py
	new file:   kNN-TL/examples/MMPT/mmpt/tasks/task.py
	new file:   kNN-TL/examples/MMPT/mmpt/tasks/vlmtask.py
	new file:   kNN-TL/examples/MMPT/mmpt/utils/__init__.py
	new file:   kNN-TL/examples/MMPT/mmpt/utils/load_config.py
	new file:   kNN-TL/examples/MMPT/mmpt/utils/shardedtensor.py
	new file:   kNN-TL/examples/MMPT/mmpt_cli/localjob.py
	new file:   kNN-TL/examples/MMPT/mmpt_cli/predict.py
	new file:   kNN-TL/examples/MMPT/pretraining.md
	new file:   kNN-TL/examples/MMPT/projects/mfmmlm.yaml
	new file:   kNN-TL/examples/MMPT/projects/mtm/mmfusionmtm.yaml
	new file:   kNN-TL/examples/MMPT/projects/mtm/vlm.yaml
	new file:   kNN-TL/examples/MMPT/projects/mtm/vlm/coin.yaml
	new file:   kNN-TL/examples/MMPT/projects/mtm/vlm/crosstask.yaml
	new file:   kNN-TL/examples/MMPT/projects/mtm/vlm/how2.yaml
	new file:   kNN-TL/examples/MMPT/projects/mtm/vlm/test_coin.yaml
	new file:   kNN-TL/examples/MMPT/projects/mtm/vlm/test_crosstask.yaml
	new file:   kNN-TL/examples/MMPT/projects/mtm/vlm/test_crosstask_zs.yaml
	new file:   kNN-TL/examples/MMPT/projects/mtm/vlm/test_vtt.yaml
	new file:   kNN-TL/examples/MMPT/projects/mtm/vlm/test_vttqa.yaml
	new file:   kNN-TL/examples/MMPT/projects/mtm/vlm/test_youcook.yaml
	new file:   kNN-TL/examples/MMPT/projects/mtm/vlm/test_youcookcap.yaml
	new file:   kNN-TL/examples/MMPT/projects/mtm/vlm/vtt.yaml
	new file:   kNN-TL/examples/MMPT/projects/mtm/vlm/vttqa.yaml
	new file:   kNN-TL/examples/MMPT/projects/mtm/vlm/youcook.yaml
	new file:   kNN-TL/examples/MMPT/projects/mtm/vlm/youcookcap.yaml
	new file:   kNN-TL/examples/MMPT/projects/retri/videoclip.yaml
	new file:   kNN-TL/examples/MMPT/projects/retri/videoclip/coin_videoclip.yaml
	new file:   kNN-TL/examples/MMPT/projects/retri/videoclip/crosstask_videoclip.yaml
	new file:   kNN-TL/examples/MMPT/projects/retri/videoclip/how2.yaml
	new file:   kNN-TL/examples/MMPT/projects/retri/videoclip/test_coin_videoclip.yaml
	new file:   kNN-TL/examples/MMPT/projects/retri/videoclip/test_coin_zs.yaml
	new file:   kNN-TL/examples/MMPT/projects/retri/videoclip/test_crosstask_videoclip.yaml
	new file:   kNN-TL/examples/MMPT/projects/retri/videoclip/test_crosstask_zs_videoclip.yaml
	new file:   kNN-TL/examples/MMPT/projects/retri/videoclip/test_didemo_zs.yaml
	new file:   kNN-TL/examples/MMPT/projects/retri/videoclip/test_vtt_videoclip.yaml
	new file:   kNN-TL/examples/MMPT/projects/retri/videoclip/test_vtt_zs.yaml
	new file:   kNN-TL/examples/MMPT/projects/retri/videoclip/test_vttqa_videoclip.yaml
	new file:   kNN-TL/examples/MMPT/projects/retri/videoclip/test_vttqa_zs.yaml
	new file:   kNN-TL/examples/MMPT/projects/retri/videoclip/test_youcook_videoclip.yaml
	new file:   kNN-TL/examples/MMPT/projects/retri/videoclip/test_youcook_zs.yaml
	new file:   kNN-TL/examples/MMPT/projects/retri/videoclip/vtt_videoclip.yaml
	new file:   kNN-TL/examples/MMPT/projects/retri/videoclip/vttqa_videoclip.yaml
	new file:   kNN-TL/examples/MMPT/projects/retri/videoclip/youcook_videoclip.yaml
	new file:   kNN-TL/examples/MMPT/projects/retri/videoretri.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/coin.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/coin_videoclip.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/crosstask.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/crosstask_videoclip.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/default.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/ft.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/how2.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/test.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/test_coin.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/test_coin_videoclip.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/test_coin_zs.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/test_crosstask.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/test_crosstask_videoclip.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/test_crosstask_zs.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/test_crosstask_zs_videoclip.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/test_didemo_zs.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/test_vtt.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/test_vtt_videoclip.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/test_vtt_zs.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/test_vttqa.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/test_vttqa_videoclip.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/test_vttqa_zs.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/test_youcook.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/test_youcook_videoclip.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/test_youcook_zs.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/test_youcookcap.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/vtt.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/vtt_videoclip.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/vttqa.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/vttqa_videoclip.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/youcook.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/youcook_videoclip.yaml
	new file:   kNN-TL/examples/MMPT/projects/task/youcookcap.yaml
	new file:   kNN-TL/examples/MMPT/scripts/text_token_extractor/configs/bert-base-uncased.yaml
	new file:   kNN-TL/examples/MMPT/scripts/text_token_extractor/pretokenization.py
	new file:   kNN-TL/examples/MMPT/scripts/video_feature_extractor/extract.py
	new file:   kNN-TL/examples/MMPT/scripts/video_feature_extractor/how2/s3d.sh
	new file:   kNN-TL/examples/MMPT/scripts/video_feature_extractor/model.py
	new file:   kNN-TL/examples/MMPT/scripts/video_feature_extractor/pathbuilder.py
	new file:   kNN-TL/examples/MMPT/scripts/video_feature_extractor/preprocessing.py
	new file:   kNN-TL/examples/MMPT/scripts/video_feature_extractor/random_sequence_shuffler.py
	new file:   kNN-TL/examples/MMPT/scripts/video_feature_extractor/shard_feature.py
	new file:   kNN-TL/examples/MMPT/scripts/video_feature_extractor/videoreader.py
	new file:   kNN-TL/examples/MMPT/setup.py
	new file:   kNN-TL/examples/MMPT/videoclip.png
	new file:   kNN-TL/examples/MMPT/vlm.png
	new file:   kNN-TL/examples/__init__.py
	new file:   kNN-TL/examples/adaptive_span/README.md
	new file:   kNN-TL/examples/adaptive_span/__init__.py
	new file:   kNN-TL/examples/adaptive_span/adagrad_with_grad_clip.py
	new file:   kNN-TL/examples/adaptive_span/adaptive_span_attention.py
	new file:   kNN-TL/examples/adaptive_span/adaptive_span_loss.py
	new file:   kNN-TL/examples/adaptive_span/adaptive_span_model.py
	new file:   kNN-TL/examples/adaptive_span/adaptive_span_model_wrapper.py
	new file:   kNN-TL/examples/adaptive_span/truncated_bptt_lm_task.py
	new file:   kNN-TL/examples/attention_head_selection/README.md
	new file:   kNN-TL/examples/attention_head_selection/src/__init__.py
	new file:   kNN-TL/examples/attention_head_selection/src/data/__init__.py
	new file:   kNN-TL/examples/attention_head_selection/src/data/speech_to_text_dataset_with_domain.py
	new file:   kNN-TL/examples/attention_head_selection/src/loss/__init__.py
	new file:   kNN-TL/examples/attention_head_selection/src/loss/attention_head_selection.py
	new file:   kNN-TL/examples/attention_head_selection/src/models/__init__.py
	new file:   kNN-TL/examples/attention_head_selection/src/models/head_selection_s2t_transformer.py
	new file:   kNN-TL/examples/attention_head_selection/src/models/head_selection_transformer.py
	new file:   kNN-TL/examples/attention_head_selection/src/modules/__init__.py
	new file:   kNN-TL/examples/attention_head_selection/src/modules/attn_head_selector.py
	new file:   kNN-TL/examples/attention_head_selection/src/modules/head_selection_transformer_layer.py
	new file:   kNN-TL/examples/attention_head_selection/src/modules/multihead_attention_selection.py
	new file:   kNN-TL/examples/attention_head_selection/src/modules/multihead_functional.py
	new file:   kNN-TL/examples/attention_head_selection/src/speech_to_text_head_selection.py
	new file:   kNN-TL/examples/backtranslation/README.md
	new file:   kNN-TL/examples/backtranslation/deduplicate_lines.py
	new file:   kNN-TL/examples/backtranslation/extract_bt_data.py
	new file:   kNN-TL/examples/backtranslation/prepare-de-monolingual.sh
	new file:   kNN-TL/examples/backtranslation/prepare-wmt18en2de.sh
	new file:   kNN-TL/examples/backtranslation/sacrebleu.sh
	new file:   kNN-TL/examples/backtranslation/tokenized_bleu.sh
	new file:   kNN-TL/examples/bart/README.glue.md
	new file:   kNN-TL/examples/bart/README.md
	new file:   kNN-TL/examples/bart/README.summarization.md
	new file:   kNN-TL/examples/bart/summarize.py
	new file:   kNN-TL/examples/byte_level_bpe/README.md
	new file:   kNN-TL/examples/byte_level_bpe/get_bitext.py
	new file:   kNN-TL/examples/byte_level_bpe/get_data.sh
	new file:   kNN-TL/examples/byte_level_bpe/gru_transformer.py
	new file:   kNN-TL/examples/camembert/README.md
	new file:   kNN-TL/examples/constrained_decoding/README.md
	new file:   kNN-TL/examples/constrained_decoding/normalize.py
	new file:   kNN-TL/examples/constrained_decoding/tok.py
	new file:   kNN-TL/examples/conv_seq2seq/README.md
	new file:   kNN-TL/examples/criss/README.md
	new file:   kNN-TL/examples/criss/download_and_preprocess_flores_test.sh
	new file:   kNN-TL/examples/criss/download_and_preprocess_tatoeba.sh
	new file:   kNN-TL/examples/criss/mining/mine.py
	new file:   kNN-TL/examples/criss/mining/mine_example.sh
	new file:   kNN-TL/examples/criss/save_encoder.py
	new file:   kNN-TL/examples/criss/sentence_retrieval/encoder_analysis.py
	new file:   kNN-TL/examples/criss/sentence_retrieval/sentence_retrieval_tatoeba.sh
	new file:   kNN-TL/examples/criss/unsupervised_mt/eval.sh
	new file:   kNN-TL/examples/cross_lingual_language_model/README.md
	new file:   kNN-TL/examples/data2vec/README.md
	new file:   kNN-TL/examples/data2vec/config/audio/pretraining/base_librispeech.yaml
	new file:   kNN-TL/examples/data2vec/config/text/pretraining/base.yaml
	new file:   kNN-TL/examples/data2vec/models/data2vec_audio.py
	new file:   kNN-TL/examples/data2vec/models/data2vec_text.py
	new file:   kNN-TL/examples/discriminative_reranking_nmt/README.md
	new file:   kNN-TL/examples/discriminative_reranking_nmt/__init__.py
	new file:   kNN-TL/examples/discriminative_reranking_nmt/config/deen.yaml
	new file:   kNN-TL/examples/discriminative_reranking_nmt/criterions/__init__.py
	new file:   kNN-TL/examples/discriminative_reranking_nmt/criterions/discriminative_reranking_criterion.py
	new file:   kNN-TL/examples/discriminative_reranking_nmt/drnmt_rerank.py
	new file:   kNN-TL/examples/discriminative_reranking_nmt/models/__init__.py
	new file:   kNN-TL/examples/discriminative_reranking_nmt/models/discriminative_reranking_model.py
	new file:   kNN-TL/examples/discriminative_reranking_nmt/scripts/prep_data.py
	new file:   kNN-TL/examples/discriminative_reranking_nmt/tasks/__init__.py
	new file:   kNN-TL/examples/discriminative_reranking_nmt/tasks/discriminative_reranking_task.py
	new file:   kNN-TL/examples/fast_noisy_channel/README.md
	new file:   kNN-TL/examples/fast_noisy_channel/__init__.py
	new file:   kNN-TL/examples/fast_noisy_channel/noisy_channel_beam_search.py
	new file:   kNN-TL/examples/fast_noisy_channel/noisy_channel_sequence_generator.py
	new file:   kNN-TL/examples/fast_noisy_channel/noisy_channel_translation.py
	new file:   kNN-TL/examples/flores101/README.md
	new file:   kNN-TL/examples/flores101/flores_logo.png
	new file:   kNN-TL/examples/fully_sharded_data_parallel/README.md
	new file:   kNN-TL/examples/gottbert/README.md
	new file:   kNN-TL/examples/hubert/README.md
	new file:   kNN-TL/examples/hubert/config/decode/ax_sweep/ngram.yaml
	new file:   kNN-TL/examples/hubert/config/decode/ax_sweep/transformer.yaml
	new file:   kNN-TL/examples/hubert/config/decode/infer_fsqlm.yaml
	new file:   kNN-TL/examples/hubert/config/decode/infer_kenlm.yaml
	new file:   kNN-TL/examples/hubert/config/decode/infer_viterbi.yaml
	new file:   kNN-TL/examples/hubert/config/decode/run/submitit_slurm.yaml
	new file:   kNN-TL/examples/hubert/config/decode/run/submitit_slurm_8gpu.yaml
	new file:   kNN-TL/examples/hubert/config/finetune/base_10h.yaml
	new file:   kNN-TL/examples/hubert/config/finetune/ckpt/it1.yaml
	new file:   kNN-TL/examples/hubert/config/finetune/lm/ls_4gram.yaml
	new file:   kNN-TL/examples/hubert/config/finetune/run/submitit_reg.yaml
	new file:   kNN-TL/examples/hubert/config/pretrain/data/iter1.yaml
	new file:   kNN-TL/examples/hubert/config/pretrain/data/iter2.yaml
	new file:   kNN-TL/examples/hubert/config/pretrain/hubert_base_librispeech.yaml
	new file:   kNN-TL/examples/hubert/config/pretrain/hubert_large_librivox.yaml
	new file:   kNN-TL/examples/hubert/config/pretrain/hubert_xlarge_librivox.yaml
	new file:   kNN-TL/examples/hubert/config/pretrain/run/submitit_reg.yaml
	new file:   kNN-TL/examples/hubert/measure_teacher_quality.py
	new file:   kNN-TL/examples/hubert/simple_kmeans/README.md
	new file:   kNN-TL/examples/hubert/simple_kmeans/dump_hubert_feature.py
	new file:   kNN-TL/examples/hubert/simple_kmeans/dump_hubert_feature_s2t.py
	new file:   kNN-TL/examples/hubert/simple_kmeans/dump_km_label.py
	new file:   kNN-TL/examples/hubert/simple_kmeans/dump_mfcc_feature.py
	new file:   kNN-TL/examples/hubert/simple_kmeans/dump_w2v2_feature.py
	new file:   kNN-TL/examples/hubert/simple_kmeans/feature_utils.py
	new file:   kNN-TL/examples/hubert/simple_kmeans/learn_kmeans.py
	new file:   kNN-TL/examples/hubert/tests/6313-76958-0021.flac
	new file:   kNN-TL/examples/hubert/tests/sample.base.L9.km500.km
	new file:   kNN-TL/examples/hubert/tests/sample.base.L9.len
	new file:   kNN-TL/examples/hubert/tests/sample.base.L9.npy
	new file:   kNN-TL/examples/hubert/tests/sample.large.L20.len
	new file:   kNN-TL/examples/hubert/tests/sample.large.L20.npy
	new file:   kNN-TL/examples/hubert/tests/sample.large.hypo.word
	new file:   kNN-TL/examples/hubert/tests/sample.xlarge.L30.len
	new file:   kNN-TL/examples/hubert/tests/sample.xlarge.L30.npy
	new file:   kNN-TL/examples/hubert/tests/sample.xlarge.hypo.word
	new file:   kNN-TL/examples/hubert/tests/test_feature_and_unit.sh
	new file:   kNN-TL/examples/hubert/tests/test_finetuned_asr.sh
	new file:   kNN-TL/examples/hubert/update_ckpt.py
	new file:   kNN-TL/examples/joint_alignment_translation/README.md
	new file:   kNN-TL/examples/joint_alignment_translation/prepare-wmt18en2de_no_norm_no_escape_no_agressive.sh
	new file:   kNN-TL/examples/language_model/README.adaptive_inputs.md
	new file:   kNN-TL/examples/language_model/README.conv.md
	new file:   kNN-TL/examples/language_model/README.md
	new file:   kNN-TL/examples/language_model/prepare-wikitext-103.sh
	new file:   kNN-TL/examples/laser/README.md
	new file:   kNN-TL/examples/laser/laser_src/__init__.py
	new file:   kNN-TL/examples/laser/laser_src/laser_lstm.py
	new file:   kNN-TL/examples/laser/laser_src/laser_task.py
	new file:   kNN-TL/examples/laser/laser_src/laser_transformer.py
	new file:   kNN-TL/examples/laser/laser_src/multitask_data_utils.py
	new file:   kNN-TL/examples/latent_depth/README.md
	new file:   kNN-TL/examples/latent_depth/latent_depth_src/__init__.py
	new file:   kNN-TL/examples/latent_depth/latent_depth_src/loss/__init__.py
	new file:   kNN-TL/examples/latent_depth/latent_depth_src/loss/latent_depth.py
	new file:   kNN-TL/examples/latent_depth/latent_depth_src/models/__init__.py
	new file:   kNN-TL/examples/latent_depth/latent_depth_src/models/latent_multilingual_transformer.py
	new file:   kNN-TL/examples/latent_depth/latent_depth_src/models/latent_transformer.py
	new file:   kNN-TL/examples/latent_depth/latent_depth_src/modules/__init__.py
	new file:   kNN-TL/examples/latent_depth/latent_depth_src/modules/latent_layers.py
	new file:   kNN-TL/examples/latent_depth/latent_depth_src/multilingual_translation_latent_depth.py
	new file:   kNN-TL/examples/layerdrop/README.md
	new file:   kNN-TL/examples/linformer/README.md
	new file:   kNN-TL/examples/linformer/linformer_src/__init__.py
	new file:   kNN-TL/examples/linformer/linformer_src/models/__init__.py
	new file:   kNN-TL/examples/linformer/linformer_src/models/linformer_roberta.py
	new file:   kNN-TL/examples/linformer/linformer_src/modules/__init__.py
	new file:   kNN-TL/examples/linformer/linformer_src/modules/linformer_sentence_encoder.py
	new file:   kNN-TL/examples/linformer/linformer_src/modules/linformer_sentence_encoder_layer.py
	new file:   kNN-TL/examples/linformer/linformer_src/modules/multihead_linear_attention.py
	new file:   kNN-TL/examples/m2m_100/README.md
	new file:   kNN-TL/examples/m2m_100/install_dependecies.sh
	new file:   kNN-TL/examples/m2m_100/process_data/clean_histogram.py
	new file:   kNN-TL/examples/m2m_100/process_data/dedup_data.py
	new file:   kNN-TL/examples/m2m_100/process_data/remove_too_much_punc.py
	new file:   kNN-TL/examples/m2m_100/tok.sh
	new file:   kNN-TL/examples/m2m_100/tokenizers/README.md
	new file:   kNN-TL/examples/m2m_100/tokenizers/seg_ja.sh
	new file:   kNN-TL/examples/m2m_100/tokenizers/seg_ko.sh
	new file:   kNN-TL/examples/m2m_100/tokenizers/thirdparty/.gitignore
	new file:   kNN-TL/examples/m2m_100/tokenizers/tokenize_indic.py
	new file:   kNN-TL/examples/m2m_100/tokenizers/tokenize_thai.py
	new file:   kNN-TL/examples/m2m_100/tokenizers/tokenize_zh.py
	new file:   kNN-TL/examples/m2m_100/tokenizers/tokenizer_ar.sh
	new file:   kNN-TL/examples/mbart/README.md
	new file:   kNN-TL/examples/megatron_11b/README.md
	new file:   kNN-TL/examples/megatron_11b/detok.py
	new file:   kNN-TL/examples/moe_lm/README.md
	new file:   kNN-TL/examples/moe_lm/data_card.md
	new file:   kNN-TL/examples/moe_lm/model_card.md
	new file:   kNN-TL/examples/multilingual/ML50_langs.txt
	new file:   kNN-TL/examples/multilingual/README.md
	new file:   kNN-TL/examples/multilingual/data_scripts/README.md
	new file:   kNN-TL/examples/multilingual/data_scripts/binarize.py
	new file:   kNN-TL/examples/multilingual/data_scripts/check_iswlt_test_data.py
	new file:   kNN-TL/examples/multilingual/data_scripts/check_self_overlaps.py
	new file:   kNN-TL/examples/multilingual/data_scripts/check_valid_test_overlaps.py
	new file:   kNN-TL/examples/multilingual/data_scripts/dedup_all.py
	new file:   kNN-TL/examples/multilingual/data_scripts/download_ML50_v1.sh
	new file:   kNN-TL/examples/multilingual/data_scripts/download_af_xh.sh
	new file:   kNN-TL/examples/multilingual/data_scripts/download_flores_data.sh
	new file:   kNN-TL/examples/multilingual/data_scripts/download_iitb.sh
	new file:   kNN-TL/examples/multilingual/data_scripts/download_iwslt_and_extract.sh
	new file:   kNN-TL/examples/multilingual/data_scripts/download_lotus.sh
	new file:   kNN-TL/examples/multilingual/data_scripts/download_ted_and_extract.py
	new file:   kNN-TL/examples/multilingual/data_scripts/download_wat19_my.sh
	new file:   kNN-TL/examples/multilingual/data_scripts/download_wmt19_and_before.py
	new file:   kNN-TL/examples/multilingual/data_scripts/download_wmt20.sh
	new file:   kNN-TL/examples/multilingual/data_scripts/preprocess_ML50_v1.sh
	new file:   kNN-TL/examples/multilingual/data_scripts/remove_valid_test_in_train.py
	new file:   kNN-TL/examples/multilingual/data_scripts/requirement.txt
	new file:   kNN-TL/examples/multilingual/data_scripts/utils/dedup.py
	new file:   kNN-TL/examples/multilingual/data_scripts/utils/fasttext_multi_filter.py
	new file:   kNN-TL/examples/multilingual/data_scripts/utils/strip_sgm.sh
	new file:   kNN-TL/examples/multilingual/finetune_multilingual_model.sh
	new file:   kNN-TL/examples/multilingual/multilingual_fairseq_gen.sh
	new file:   kNN-TL/examples/multilingual/train_multilingual_model.sh
	new file:   kNN-TL/examples/noisychannel/README.md
	new file:   kNN-TL/examples/noisychannel/__init__.py
	new file:   kNN-TL/examples/noisychannel/rerank.py
	new file:   kNN-TL/examples/noisychannel/rerank_generate.py
	new file:   kNN-TL/examples/noisychannel/rerank_options.py
	new file:   kNN-TL/examples/noisychannel/rerank_score_bw.py
	new file:   kNN-TL/examples/noisychannel/rerank_score_lm.py
	new file:   kNN-TL/examples/noisychannel/rerank_tune.py
	new file:   kNN-TL/examples/noisychannel/rerank_utils.py
	new file:   kNN-TL/examples/nonautoregressive_translation/README.md
	new file:   kNN-TL/examples/nonautoregressive_translation/scripts.md
	new file:   kNN-TL/examples/normformer/README.md
	new file:   kNN-TL/examples/normformer/train_lm.sh
	new file:   kNN-TL/examples/operators/alignment_train_cpu.cpp
	new file:   kNN-TL/examples/operators/alignment_train_cuda.cpp
	new file:   kNN-TL/examples/operators/alignment_train_cuda.h
	new file:   kNN-TL/examples/operators/alignment_train_kernel.cu
	new file:   kNN-TL/examples/operators/utils.h
	new file:   kNN-TL/examples/paraphraser/README.md
	new file:   kNN-TL/examples/paraphraser/paraphrase.py
	new file:   kNN-TL/examples/pay_less_attention_paper/README.md
	new file:   kNN-TL/examples/pointer_generator/README.md
	new file:   kNN-TL/examples/pointer_generator/README.xsum.md
	new file:   kNN-TL/examples/pointer_generator/pointer_generator_src/__init__.py
	new file:   kNN-TL/examples/pointer_generator/pointer_generator_src/transformer_pg.py
	new file:   kNN-TL/examples/pointer_generator/postprocess.py
	new file:   kNN-TL/examples/pointer_generator/preprocess.py
	new file:   kNN-TL/examples/quant_noise/README.md
	new file:   kNN-TL/examples/quant_noise/transformer_quantization_config.yaml
	new file:   kNN-TL/examples/roberta/README.custom_classification.md
	new file:   kNN-TL/examples/roberta/README.glue.md
	new file:   kNN-TL/examples/roberta/README.md
	new file:   kNN-TL/examples/roberta/README.pretraining.md
	new file:   kNN-TL/examples/roberta/README.race.md
	new file:   kNN-TL/examples/roberta/commonsense_qa/README.md
	new file:   kNN-TL/examples/roberta/commonsense_qa/__init__.py
	new file:   kNN-TL/examples/roberta/commonsense_qa/commonsense_qa_task.py
	new file:   kNN-TL/examples/roberta/commonsense_qa/download_cqa_data.sh
	new file:   kNN-TL/examples/roberta/config/finetuning/cola.yaml
	new file:   kNN-TL/examples/roberta/config/finetuning/mnli.yaml
	new file:   kNN-TL/examples/roberta/config/finetuning/mrpc.yaml
	new file:   kNN-TL/examples/roberta/config/finetuning/qnli.yaml
	new file:   kNN-TL/examples/roberta/config/finetuning/qqp.yaml
	new file:   kNN-TL/examples/roberta/config/finetuning/rte.yaml
	new file:   kNN-TL/examples/roberta/config/finetuning/sst_2.yaml
	new file:   kNN-TL/examples/roberta/config/finetuning/sts_b.yaml
	new file:   kNN-TL/examples/roberta/config/pretraining/base.yaml
	new file:   kNN-TL/examples/roberta/multiprocessing_bpe_encoder.py
	new file:   kNN-TL/examples/roberta/preprocess_GLUE_tasks.sh
	new file:   kNN-TL/examples/roberta/preprocess_RACE.py
	new file:   kNN-TL/examples/roberta/preprocess_RACE.sh
	new file:   kNN-TL/examples/roberta/wsc/README.md
	new file:   kNN-TL/examples/roberta/wsc/__init__.py
	new file:   kNN-TL/examples/roberta/wsc/wsc_criterion.py
	new file:   kNN-TL/examples/roberta/wsc/wsc_task.py
	new file:   kNN-TL/examples/roberta/wsc/wsc_utils.py
	new file:   kNN-TL/examples/rxf/README.md
	new file:   kNN-TL/examples/rxf/__init__.py
	new file:   kNN-TL/examples/rxf/rxf_src/__init__.py
	new file:   kNN-TL/examples/rxf/rxf_src/label_smoothed_cross_entropy_r3f.py
	new file:   kNN-TL/examples/rxf/rxf_src/sentence_prediction_r3f.py
	new file:   kNN-TL/examples/scaling_nmt/README.md
	new file:   kNN-TL/examples/shuffled_word_order/README.finetuning.md
	new file:   kNN-TL/examples/shuffled_word_order/README.md
	new file:   kNN-TL/examples/simultaneous_translation/README.md
	new file:   kNN-TL/examples/simultaneous_translation/__init__.py
	new file:   kNN-TL/examples/simultaneous_translation/docs/ende-mma.md
	new file:   kNN-TL/examples/simultaneous_translation/docs/enja-waitk.md
	new file:   kNN-TL/examples/simultaneous_translation/eval/agents/simul_t2t_enja.py
	new file:   kNN-TL/examples/simultaneous_translation/models/__init__.py
	new file:   kNN-TL/examples/simultaneous_translation/models/convtransformer_simul_trans.py
	new file:   kNN-TL/examples/simultaneous_translation/models/transformer_monotonic_attention.py
	new file:   kNN-TL/examples/simultaneous_translation/modules/__init__.py
	new file:   kNN-TL/examples/simultaneous_translation/modules/fixed_pre_decision.py
	new file:   kNN-TL/examples/simultaneous_translation/modules/monotonic_multihead_attention.py
	new file:   kNN-TL/examples/simultaneous_translation/modules/monotonic_transformer_layer.py
	new file:   kNN-TL/examples/simultaneous_translation/tests/test_alignment_train.py
	new file:   kNN-TL/examples/simultaneous_translation/tests/test_text_models.py
	new file:   kNN-TL/examples/simultaneous_translation/utils/__init__.py
	new file:   kNN-TL/examples/simultaneous_translation/utils/functions.py
	new file:   kNN-TL/examples/simultaneous_translation/utils/monotonic_attention.py
	new file:   kNN-TL/examples/simultaneous_translation/utils/p_choose_strategy.py
	new file:   kNN-TL/examples/speech_recognition/README.md
	new file:   kNN-TL/examples/speech_recognition/__init__.py
	new file:   kNN-TL/examples/speech_recognition/criterions/ASG_loss.py
	new file:   kNN-TL/examples/speech_recognition/criterions/__init__.py
	new file:   kNN-TL/examples/speech_recognition/criterions/cross_entropy_acc.py
	new file:   kNN-TL/examples/speech_recognition/data/__init__.py
	new file:   kNN-TL/examples/speech_recognition/data/asr_dataset.py
	new file:   kNN-TL/examples/speech_recognition/data/collaters.py
	new file:   kNN-TL/examples/speech_recognition/data/data_utils.py
	new file:   kNN-TL/examples/speech_recognition/data/replabels.py
	new file:   kNN-TL/examples/speech_recognition/datasets/asr_prep_json.py
	new file:   kNN-TL/examples/speech_recognition/datasets/prepare-librispeech.sh
	new file:   kNN-TL/examples/speech_recognition/infer.py
	new file:   kNN-TL/examples/speech_recognition/kaldi/__init__.py
	new file:   kNN-TL/examples/speech_recognition/kaldi/add-self-loop-simple.cc
	new file:   kNN-TL/examples/speech_recognition/kaldi/config/kaldi_initializer.yaml
	new file:   kNN-TL/examples/speech_recognition/kaldi/kaldi_decoder.py
	new file:   kNN-TL/examples/speech_recognition/kaldi/kaldi_initializer.py
	new file:   kNN-TL/examples/speech_recognition/models/__init__.py
	new file:   kNN-TL/examples/speech_recognition/models/vggtransformer.py
	new file:   kNN-TL/examples/speech_recognition/models/w2l_conv_glu_enc.py
	new file:   kNN-TL/examples/speech_recognition/new/README.md
	new file:   kNN-TL/examples/speech_recognition/new/__init__.py
	new file:   kNN-TL/examples/speech_recognition/new/conf/hydra/sweeper/ax.yaml
	new file:   kNN-TL/examples/speech_recognition/new/conf/infer.yaml
	new file:   kNN-TL/examples/speech_recognition/new/decoders/__init__.py
	new file:   kNN-TL/examples/speech_recognition/new/decoders/base_decoder.py
	new file:   kNN-TL/examples/speech_recognition/new/decoders/decoder.py
	new file:   kNN-TL/examples/speech_recognition/new/decoders/decoder_config.py
	new file:   kNN-TL/examples/speech_recognition/new/decoders/flashlight_decoder.py
	new file:   kNN-TL/examples/speech_recognition/new/decoders/viterbi_decoder.py
	new file:   kNN-TL/examples/speech_recognition/new/infer.py
	new file:   kNN-TL/examples/speech_recognition/tasks/__init__.py
	new file:   kNN-TL/examples/speech_recognition/tasks/speech_recognition.py
	new file:   kNN-TL/examples/speech_recognition/utils/wer_utils.py
	new file:   kNN-TL/examples/speech_recognition/w2l_decoder.py
	new file:   kNN-TL/examples/speech_synthesis/README.md
	new file:   kNN-TL/examples/speech_synthesis/__init__.py
	new file:   kNN-TL/examples/speech_synthesis/data_utils.py
	new file:   kNN-TL/examples/speech_synthesis/docs/common_voice_example.md
	new file:   kNN-TL/examples/speech_synthesis/docs/ljspeech_example.md
	new file:   kNN-TL/examples/speech_synthesis/docs/vctk_example.md
	new file:   kNN-TL/examples/speech_synthesis/evaluation/__init__.py
	new file:   kNN-TL/examples/speech_synthesis/evaluation/eval_asr.py
	new file:   kNN-TL/examples/speech_synthesis/evaluation/eval_f0.py
	new file:   kNN-TL/examples/speech_synthesis/evaluation/eval_sp.py
	new file:   kNN-TL/examples/speech_synthesis/evaluation/get_eval_manifest.py
	new file:   kNN-TL/examples/speech_synthesis/generate_waveform.py
	new file:   kNN-TL/examples/speech_synthesis/preprocessing/__init__.py
	new file:  "
kNN-TL/examples/speech_synthesis/preprocessing/denoise_and_vad_audio.py,,,
"	new file:   kNN-TL/examples/speech_synthesis/preprocessing/denoiser/__init__.py",,,
"	new file:   kNN-TL/examples/speech_synthesis/preprocessing/denoiser/demucs.py",,,
"	new file:   kNN-TL/examples/speech_synthesis/preprocessing/denoiser/pretrained.py",,,
"	new file:   kNN-TL/examples/speech_synthesis/preprocessing/denoiser/resample.py",,,
"	new file:   kNN-TL/examples/speech_synthesis/preprocessing/denoiser/utils.py",,,
"	new file:   kNN-TL/examples/speech_synthesis/preprocessing/get_common_voice_audio_manifest.py",,,
"	new file:   kNN-TL/examples/speech_synthesis/preprocessing/get_feature_manifest.py",,,
"	new file:   kNN-TL/examples/speech_synthesis/preprocessing/get_ljspeech_audio_manifest.py",,,
"	new file:   kNN-TL/examples/speech_synthesis/preprocessing/get_speaker_embedding.py",,,
"	new file:   kNN-TL/examples/speech_synthesis/preprocessing/get_vctk_audio_manifest.py",,,
"	new file:   kNN-TL/examples/speech_synthesis/preprocessing/speaker_embedder/__init__.py",,,
"	new file:   kNN-TL/examples/speech_synthesis/preprocessing/vad/__init__.py",,,
"	new file:   kNN-TL/examples/speech_synthesis/utils.py",,,
"	new file:   kNN-TL/examples/speech_text_joint_to_text/README.md",,,
"	new file:   kNN-TL/examples/speech_text_joint_to_text/__init__.py",,,
"	new file:   kNN-TL/examples/speech_text_joint_to_text/configs/mustc_noise.list",,,
"	new file:   kNN-TL/examples/speech_text_joint_to_text/criterions/__init__.py",,,
"	new file:   kNN-TL/examples/speech_text_joint_to_text/criterions/multi_modality_compound.py",,,
"	new file:   kNN-TL/examples/speech_text_joint_to_text/criterions/multi_modality_cross_entropy.py",,,
"	new file:   kNN-TL/examples/speech_text_joint_to_text/criterions/text_guide_cross_entropy_acc.py",,,
"	new file:   kNN-TL/examples/speech_text_joint_to_text/data/pair_denoising_dataset.py",,,
"	new file:   kNN-TL/examples/speech_text_joint_to_text/docs/ende-mustc.md",,,
"	new file:   kNN-TL/examples/speech_text_joint_to_text/docs/iwslt2021.md",,,
"	new file:   kNN-TL/examples/speech_text_joint_to_text/docs/pre-training.md",,,
"	new file:   kNN-TL/examples/speech_text_joint_to_text/models/__init__.py",,,
"	new file:   kNN-TL/examples/speech_text_joint_to_text/models/joint_speech_text_pretrain_transformer.py",,,
"	new file:   kNN-TL/examples/speech_text_joint_to_text/models/s2t_dualinputtransformer.py",,,
"	new file:   kNN-TL/examples/speech_text_joint_to_text/models/s2t_dualinputwavtransformer.py",,,
"	new file:   kNN-TL/examples/speech_text_joint_to_text/models/s2t_dualinputxmtransformer.py",,,
"	new file:   kNN-TL/examples/speech_text_joint_to_text/scripts/convert_model.py",,,
"	new file:   kNN-TL/examples/speech_text_joint_to_text/scripts/g2p_encode.py",,,
"	new file:   kNN-TL/examples/speech_text_joint_to_text/tasks/__init__.py",,,
"	new file:   kNN-TL/examples/speech_text_joint_to_text/tasks/pair_denoising.py",,,
"	new file:   kNN-TL/examples/speech_text_joint_to_text/tasks/speech_text_denoise_pretrain.py",,,
"	new file:   kNN-TL/examples/speech_text_joint_to_text/tasks/speech_text_joint.py",,,
"	new file:   kNN-TL/examples/speech_to_speech/README.md",,,
"	new file:   kNN-TL/examples/speech_to_speech/__init__.py",,,
"	new file:   kNN-TL/examples/speech_to_speech/benchmarking/README.md",,,
"	new file:   kNN-TL/examples/speech_to_speech/benchmarking/configs/2StageS2ST.yaml",,,
"	new file:   kNN-TL/examples/speech_to_speech/benchmarking/configs/3StageS2ST.yaml",,,
"	new file:   kNN-TL/examples/speech_to_speech/benchmarking/configs/DirectS2U.yaml",,,
"	new file:   kNN-TL/examples/speech_to_speech/benchmarking/configs/S2T.yaml",,,
"	new file:   kNN-TL/examples/speech_to_speech/benchmarking/core.py",,,
"	new file:   kNN-TL/examples/speech_to_speech/benchmarking/data_utils.py",,,
"	new file:   kNN-TL/examples/speech_to_speech/benchmarking/get_metrics.py",,,
"	new file:   kNN-TL/examples/speech_to_speech/docs/direct_s2st_discrete_units.md",,,
"	new file:   kNN-TL/examples/speech_to_speech/docs/enhanced_direct_s2st_discrete_units.md",,,
"	new file:   kNN-TL/examples/speech_to_speech/docs/textless_s2st_real_data.md",,,
"	new file:   kNN-TL/examples/speech_to_speech/generate_waveform_from_code.py",,,
"	new file:   kNN-TL/examples/speech_to_speech/preprocessing/__init__.py",,,
"	new file:   kNN-TL/examples/speech_to_speech/preprocessing/data_utils.py",,,
"	new file:   kNN-TL/examples/speech_to_speech/preprocessing/prep_s2spect_data.py",,,
"	new file:   kNN-TL/examples/speech_to_speech/preprocessing/prep_s2ut_data.py",,,
"	new file:   kNN-TL/examples/speech_to_speech/preprocessing/prep_sn_data.py",,,
"	new file:   kNN-TL/examples/speech_to_speech/preprocessing/prep_sn_output_data.py",,,
"	new file:   kNN-TL/examples/speech_to_text/README.md",,,
"	new file:   kNN-TL/examples/speech_to_text/data_utils.py",,,
"	new file:   kNN-TL/examples/speech_to_text/docs/covost_example.md",,,
"	new file:   kNN-TL/examples/speech_to_text/docs/librispeech_example.md",,,
"	new file:   kNN-TL/examples/speech_to_text/docs/mtedx_example.md",,,
"	new file:   kNN-TL/examples/speech_to_text/docs/mustc_example.md",,,
"	new file:   kNN-TL/examples/speech_to_text/docs/simulst_mustc_example.md",,,
"	new file:   kNN-TL/examples/speech_to_text/prep_covost_data.py",,,
"	new file:   kNN-TL/examples/speech_to_text/prep_librispeech_data.py",,,
"	new file:   kNN-TL/examples/speech_to_text/prep_mtedx_data.py",,,
"	new file:   kNN-TL/examples/speech_to_text/prep_mustc_data.py",,,
"	new file:   kNN-TL/examples/speech_to_text/seg_mustc_data.py",,,
"	new file:   kNN-TL/examples/speech_to_text/simultaneous_translation/agents/fairseq_simul_st_agent.py",,,
"	new file:   kNN-TL/examples/stories/README.md",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/README.md",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/metrics/README.md",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/metrics/abx_metrics/README.md",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/metrics/abx_metrics/dump_abx_feats.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/metrics/asr_metrics/README.md",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/metrics/asr_metrics/continuation_eval.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/metrics/asr_metrics/misc/bleu_utils.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/metrics/asr_metrics/misc/cut_as.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/metrics/asr_metrics/misc/dict.ltr.txt",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/metrics/asr_metrics/ppx.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/metrics/asr_metrics/self_auto_bleu.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/speech2unit/README.md",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/speech2unit/__init__.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/speech2unit/clustering/__init__.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/speech2unit/clustering/cluster_kmeans.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/speech2unit/clustering/dump_feats.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/speech2unit/clustering/quantize_with_kmeans.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/speech2unit/clustering/utils.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/speech2unit/pretrained/cpc_feature_reader.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/speech2unit/pretrained/hubert_feature_reader.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/speech2unit/pretrained/logmel_feature_reader.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/speech2unit/pretrained/utils.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/speech2unit/pretrained/w2v2_feature_reader.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/tools/README.md",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/tools/resynthesize_speech.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/ulm/README.md",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/ulm/sample.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/README.md",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/convert_to_16k.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/glow.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/multiproc.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/synthesize_audio_from_units.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/tacotron2/__init__.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/tacotron2/audio_processing.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/tacotron2/cleaners.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/tacotron2/cmudict.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/tacotron2/layers.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/tacotron2/model.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/tacotron2/numbers.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/tacotron2/stft.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/tacotron2/symbols.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/tacotron2/text.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/tacotron2/utils.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/tacotron2/waveglow_denoiser.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/tts_data.py",,,
"	new file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/utils.py",,,
"	new file:   kNN-TL/examples/textless_nlp/pgslm/README.md",,,
"	new file:   kNN-TL/examples/textless_nlp/pgslm/data_utils.py",,,
"	new file:   kNN-TL/examples/textless_nlp/pgslm/eval/__init__.py",,,
"	new file:   kNN-TL/examples/textless_nlp/pgslm/eval/cont_metrics.py",,,
"	new file:   kNN-TL/examples/textless_nlp/pgslm/generate_waveform.py",,,
"	new file:   kNN-TL/examples/textless_nlp/pgslm/inference_dataset.py",,,
"	new file:   kNN-TL/examples/textless_nlp/pgslm/naive_decoder.py",,,
"	new file:   kNN-TL/examples/textless_nlp/pgslm/prepare_dataset.py",,,
"	new file:   kNN-TL/examples/textless_nlp/pgslm/preprocess_f0.py",,,
"	new file:   kNN-TL/examples/textless_nlp/pgslm/quantize_f0.py",,,
"	new file:   kNN-TL/examples/textless_nlp/pgslm/sample/__init__.py",,,
"	new file:   kNN-TL/examples/textless_nlp/pgslm/sample/sample.py",,,
"	new file:   kNN-TL/examples/textless_nlp/pgslm/scripts/join_units_manifest.py",,,
"	new file:   kNN-TL/examples/textless_nlp/pgslm/scripts/prepare_data.sh",,,
"	new file:   kNN-TL/examples/textless_nlp/pgslm/scripts/prepare_f0_quantization.sh",,,
"	new file:   kNN-TL/examples/textless_nlp/pgslm/truncated_laplace.py",,,
"	new file:   kNN-TL/examples/textless_nlp/speech-resynth/README.md",,,
"	new file:   kNN-TL/examples/textless_nlp/speech-resynth/img/fig.png",,,
"	new file:   kNN-TL/examples/translation/README.md",,,
"	new file:   kNN-TL/examples/translation/prepare-iwslt14.sh",,,
"	new file:   kNN-TL/examples/translation/prepare-iwslt17-multilingual.sh",,,
"	new file:   kNN-TL/examples/translation/prepare-wmt14en2de.sh",,,
"	new file:   kNN-TL/examples/translation/prepare-wmt14en2fr.sh",,,
"	new file:   kNN-TL/examples/translation_moe/README.md",,,
"	new file:   kNN-TL/examples/translation_moe/score.py",,,
"	new file:   kNN-TL/examples/translation_moe/translation_moe_src/__init__.py",,,
"	new file:   kNN-TL/examples/translation_moe/translation_moe_src/logsumexp_moe.py",,,
"	new file:   kNN-TL/examples/translation_moe/translation_moe_src/mean_pool_gating_network.py",,,
"	new file:   kNN-TL/examples/translation_moe/translation_moe_src/translation_moe.py",,,
"	new file:   kNN-TL/examples/truncated_bptt/README.md",,,
"	new file:   kNN-TL/examples/truncated_bptt/__init__.py",,,
"	new file:   kNN-TL/examples/truncated_bptt/transformer_xl_model.py",,,
"	new file:   kNN-TL/examples/truncated_bptt/truncated_bptt_lm_task.py",,,
"	new file:   kNN-TL/examples/unsupervised_quality_estimation/README.md",,,
"	new file:   kNN-TL/examples/unsupervised_quality_estimation/aggregate_scores.py",,,
"	new file:   kNN-TL/examples/unsupervised_quality_estimation/meteor.py",,,
"	new file:   kNN-TL/examples/unsupervised_quality_estimation/repeat_lines.py",,,
"	new file:   kNN-TL/examples/wav2vec/README.md",,,
"	new file:   kNN-TL/examples/wav2vec/__init__.py",,,
"	new file:   kNN-TL/examples/wav2vec/config/finetuning/base_100h.yaml",,,
"	new file:   kNN-TL/examples/wav2vec/config/finetuning/base_10h.yaml",,,
"	new file:   kNN-TL/examples/wav2vec/config/finetuning/base_10m.yaml",,,
"	new file:   kNN-TL/examples/wav2vec/config/finetuning/base_1h.yaml",,,
"	new file:   kNN-TL/examples/wav2vec/config/finetuning/base_960h.yaml",,,
"	new file:   kNN-TL/examples/wav2vec/config/finetuning/vox_100h.yaml",,,
"	new file:   kNN-TL/examples/wav2vec/config/finetuning/vox_10h.yaml",,,
"	new file:   kNN-TL/examples/wav2vec/config/finetuning/vox_10m.yaml",,,
"	new file:   kNN-TL/examples/wav2vec/config/finetuning/vox_1h.yaml",,,
"	new file:   kNN-TL/examples/wav2vec/config/finetuning/vox_960h.yaml",,,
"	new file:   kNN-TL/examples/wav2vec/config/pretraining/wav2vec2_base_librispeech.yaml",,,
"	new file:   kNN-TL/examples/wav2vec/config/pretraining/wav2vec2_conformer_base_librispeech.yaml",,,
"	new file:   kNN-TL/examples/wav2vec/config/pretraining/wav2vec2_conformer_large_librivox.yaml",,,
"	new file:   kNN-TL/examples/wav2vec/config/pretraining/wav2vec2_large_librivox.yaml",,,
"	new file:   kNN-TL/examples/wav2vec/config/pretraining/wav2vec2_large_librivox_tpu-pod.yaml",,,
"	new file:   kNN-TL/examples/wav2vec/config/pretraining/wav2vec2_large_librivox_tpu.yaml",,,
"	new file:   kNN-TL/examples/wav2vec/libri_labels.py",,,
"	new file:   kNN-TL/examples/wav2vec/scripts/binarize_manifest.sh",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/README.md",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/__init__.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/config/finetuning/w2v_finetune.yaml",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/config/gan/w2vu.yaml",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/config/generate/viterbi.yaml",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/config/timit_matched/test.uid",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/config/timit_matched/train.uid",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/config/timit_matched/train_text.uid",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/config/timit_matched/valid.uid",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/config/timit_unmatched/test.uid",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/config/timit_unmatched/train.uid",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/config/timit_unmatched/train_text.uid",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/config/timit_unmatched/valid.uid",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/data/__init__.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/data/extracted_features_dataset.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/data/random_input_dataset.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/README.md",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/cmd.sh",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/decode_phone.sh",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/decode_word_step1.sh",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/decode_word_step2.sh",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/local/copy_aligned_text.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/local/decode.sh",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/local/prepare_data_from_w2v.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/local/prepare_lang.sh",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/local/prepare_lang_word.sh",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/local/prepare_lm.sh",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/local/score.sh",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/local/show_wer.sh",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/local/train_subset_lgbeam.sh",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/local/unsup_select.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/local/unsup_select_decode.sh",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/local/unsup_select_decode_word.sh",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/path.sh",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/steps_gan/train_deltas.sh",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/steps_gan/train_lda_mllt.sh",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/steps_gan/train_sat.sh",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/train.sh",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/models/__init__.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/models/wav2vec_u.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/scripts/apply_pca.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/scripts/copy_labels.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/scripts/filter_lexicon.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/scripts/filter_tsv.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/scripts/g2p_wrd_to_phn.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/scripts/ltr_to_wrd.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/scripts/mean_pool.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/scripts/merge_clusters.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/scripts/normalize_and_filter_text.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/scripts/normalize_text.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/scripts/pca.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/scripts/phonemize_with_sil.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/scripts/prepare_audio.sh",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/scripts/prepare_text.sh",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/scripts/prepare_timit.sh",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/scripts/remove_silence.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/scripts/vads.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/scripts/wav2vec_apply_cluster_faiss.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/scripts/wav2vec_cluster_faiss.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/scripts/wav2vec_extract_features.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/scripts/wer.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/scripts/wrd_to_ltr.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/tasks/__init__.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/tasks/unpaired_audio_text.py",,,
"	new file:   kNN-TL/examples/wav2vec/unsupervised/w2vu_generate.py",,,
"	new file:   kNN-TL/examples/wav2vec/vq-wav2vec_featurize.py",,,
"	new file:   kNN-TL/examples/wav2vec/wav2vec_featurize.py",,,
"	new file:   kNN-TL/examples/wav2vec/wav2vec_manifest.py",,,
"	new file:   kNN-TL/examples/wav2vec/xlsr/README.md",,,
"	new file:   kNN-TL/examples/wav2vec/xlsr/config/finetune.yaml",,,
"	new file:   kNN-TL/examples/wmt19/README.md",,,
"	new file:   kNN-TL/examples/wmt20/README.md",,,
"	new file:   kNN-TL/examples/wmt21/README.md",,,
"	new file:   kNN-TL/examples/wmt21/eval.sh",,,
"	new file:   kNN-TL/examples/wmt21/scripts/normalize-punctuation.perl",,,
"	new file:   kNN-TL/examples/wmt21/scripts/replace-unicode-punctuation.perl",,,
"	new file:   kNN-TL/examples/womens_bios/README.md",,,
"	new file:   kNN-TL/examples/womens_bios/query_occupations_from_wikidata.py",,,
"	new file:   kNN-TL/examples/xformers/README.md",,,
"	new file:   kNN-TL/examples/xglm/README.md",,,
"	new file:   kNN-TL/examples/xglm/model_card.md",,,
"	new file:   kNN-TL/examples/xlmr/README.md",,,
"	new file:   kNN-TL/examples/xmod/README.md",,,
"	new file:   kNN-TL/examples/xmod/preprocess_nli.py",,,
"	new file:   kNN-TL/fairseq/__init__.py",,,
"	new file:   kNN-TL/fairseq/benchmark/__init__.py",,,
"	new file:   kNN-TL/fairseq/benchmark/benchmark_multihead_attention.py",,,
"	new file:   kNN-TL/fairseq/benchmark/dummy_dataset.py",,,
"	new file:   kNN-TL/fairseq/benchmark/dummy_lm.py",,,
"	new file:   kNN-TL/fairseq/benchmark/dummy_masked_lm.py",,,
"	new file:   kNN-TL/fairseq/benchmark/dummy_model.py",,,
"	new file:   kNN-TL/fairseq/benchmark/dummy_mt.py",,,
"	new file:   kNN-TL/fairseq/binarizer.py",,,
"	new file:   kNN-TL/fairseq/checkpoint_utils.py",,,
"	new file:   kNN-TL/fairseq/clib/cuda/ngram_repeat_block_cuda.cpp",,,
"	new file:   kNN-TL/fairseq/clib/cuda/ngram_repeat_block_cuda_kernel.cu",,,
"	new file:   kNN-TL/fairseq/clib/libbase/balanced_assignment.cpp",,,
"	new file:   kNN-TL/fairseq/clib/libbleu/libbleu.cpp",,,
"	new file:   kNN-TL/fairseq/clib/libbleu/module.cpp",,,
"	new file:   kNN-TL/fairseq/clib/libnat/edit_dist.cpp",,,
"	new file:   kNN-TL/fairseq/clib/libnat_cuda/binding.cpp",,,
"	new file:   kNN-TL/fairseq/clib/libnat_cuda/edit_dist.cu",,,
"	new file:   kNN-TL/fairseq/clib/libnat_cuda/edit_dist.h",,,
"	new file:   kNN-TL/fairseq/config/__init__.py",,,
"	new file:   kNN-TL/fairseq/config/config.yaml",,,
"	new file:   kNN-TL/fairseq/config/model/transformer_lm/transformer_lm_baevski_gbw.yaml",,,
"	new file:   kNN-TL/fairseq/config/model/transformer_lm/transformer_lm_baevski_wiki103.yaml",,,
"	new file:   kNN-TL/fairseq/config/model/transformer_lm/transformer_lm_big.yaml",,,
"	new file:   kNN-TL/fairseq/config/model/transformer_lm/transformer_lm_gbw.yaml",,,
"	new file:   kNN-TL/fairseq/config/model/transformer_lm/transformer_lm_gpt.yaml",,,
"	new file:   kNN-TL/fairseq/config/model/transformer_lm/transformer_lm_gpt2_big.yaml",,,
"	new file:   kNN-TL/fairseq/config/model/transformer_lm/transformer_lm_gpt2_medium.yaml",,,
"	new file:   kNN-TL/fairseq/config/model/transformer_lm/transformer_lm_gpt2_small.yaml",,,
"	new file:   kNN-TL/fairseq/config/model/transformer_lm/transformer_lm_wiki103.yaml",,,
"	new file:   kNN-TL/fairseq/config/model/wav2vec/vq_wav2vec_gumbel.yaml",,,
"	new file:   kNN-TL/fairseq/config/model/wav2vec2/wav2vec2_base.yaml",,,
"	new file:   kNN-TL/fairseq/config/model/wav2vec2/wav2vec2_large.yaml",,,
"	new file:   kNN-TL/fairseq/criterions/__init__.py",,,
"	new file:   kNN-TL/fairseq/criterions/adaptive_loss.py",,,
"	new file:   kNN-TL/fairseq/criterions/composite_loss.py",,,
"	new file:   kNN-TL/fairseq/criterions/cross_entropy.py",,,
"	new file:   kNN-TL/fairseq/criterions/ctc.py",,,
"	new file:   kNN-TL/fairseq/criterions/fairseq_criterion.py",,,
"	new file:   kNN-TL/fairseq/criterions/fastspeech2_loss.py",,,
"	new file:   kNN-TL/fairseq/criterions/hubert_criterion.py",,,
"	new file:   kNN-TL/fairseq/criterions/label_smoothed_cross_entropy.py",,,
"	new file:   kNN-TL/fairseq/criterions/label_smoothed_cross_entropy_latency_augmented.py",,,
"	new file:   kNN-TL/fairseq/criterions/label_smoothed_cross_entropy_with_alignment.py",,,
"	new file:   kNN-TL/fairseq/criterions/label_smoothed_cross_entropy_with_ctc.py",,,
"	new file:   kNN-TL/fairseq/criterions/label_smoothed_cross_entropy_xkd.py",,,
"	new file:   kNN-TL/fairseq/criterions/legacy_masked_lm.py",,,
"	new file:   kNN-TL/fairseq/criterions/masked_lm.py",,,
"	new file:   kNN-TL/fairseq/criterions/model_criterion.py",,,
"	new file:   kNN-TL/fairseq/criterions/nat_loss.py",,,
"	new file:   kNN-TL/fairseq/criterions/sentence_prediction.py",,,
"	new file:   kNN-TL/fairseq/criterions/sentence_prediction_adapters.py",,,
"	new file:   kNN-TL/fairseq/criterions/sentence_ranking.py",,,
"	new file:   kNN-TL/fairseq/criterions/speech_to_speech_criterion.py",,,
"	new file:   kNN-TL/fairseq/criterions/speech_ulm_criterion.py",,,
"	new file:   kNN-TL/fairseq/criterions/tacotron2_loss.py",,,
"	new file:   kNN-TL/fairseq/criterions/wav2vec_criterion.py",,,
"	new file:   kNN-TL/fairseq/data/__init__.py",,,
"	new file:   kNN-TL/fairseq/data/add_target_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/append_token_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/audio/__init__.py",,,
"	new file:   kNN-TL/fairseq/data/audio/audio_utils.py",,,
"	new file:   kNN-TL/fairseq/data/audio/data_cfg.py",,,
"	new file:   kNN-TL/fairseq/data/audio/feature_transforms/__init__.py",,,
"	new file:   kNN-TL/fairseq/data/audio/feature_transforms/delta_deltas.py",,,
"	new file:   kNN-TL/fairseq/data/audio/feature_transforms/global_cmvn.py",,,
"	new file:   kNN-TL/fairseq/data/audio/feature_transforms/specaugment.py",,,
"	new file:   kNN-TL/fairseq/data/audio/feature_transforms/utterance_cmvn.py",,,
"	new file:   kNN-TL/fairseq/data/audio/frm_text_to_speech_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/audio/hubert_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/audio/multi_modality_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/audio/raw_audio_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/audio/speech_to_speech_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/audio/speech_to_text_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/audio/speech_to_text_joint_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/audio/text_to_speech_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/backtranslation_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/base_wrapper_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/bucket_pad_length_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/codedataset.py",,,
"	new file:   kNN-TL/fairseq/data/colorize_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/concat_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/concat_sentences_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/data_utils.py",,,
"	new file:   kNN-TL/fairseq/data/data_utils_fast.pyx",,,
"	new file:   kNN-TL/fairseq/data/denoising_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/dictionary.py",,,
"	new file:   kNN-TL/fairseq/data/encoders/__init__.py",,,
"	new file:   kNN-TL/fairseq/data/encoders/byte_bpe.py",,,
"	new file:   kNN-TL/fairseq/data/encoders/byte_utils.py",,,
"	new file:   kNN-TL/fairseq/data/encoders/bytes.py",,,
"	new file:   kNN-TL/fairseq/data/encoders/characters.py",,,
"	new file:   kNN-TL/fairseq/data/encoders/fastbpe.py",,,
"	new file:   kNN-TL/fairseq/data/encoders/gpt2_bpe.py",,,
"	new file:   kNN-TL/fairseq/data/encoders/gpt2_bpe_utils.py",,,
"	new file:   kNN-TL/fairseq/data/encoders/hf_bert_bpe.py",,,
"	new file:   kNN-TL/fairseq/data/encoders/hf_byte_bpe.py",,,
"	new file:   kNN-TL/fairseq/data/encoders/moses_tokenizer.py",,,
"	new file:   kNN-TL/fairseq/data/encoders/nltk_tokenizer.py",,,
"	new file:   kNN-TL/fairseq/data/encoders/sentencepiece_bpe.py",,,
"	new file:   kNN-TL/fairseq/data/encoders/space_tokenizer.py",,,
"	new file:   kNN-TL/fairseq/data/encoders/subword_nmt_bpe.py",,,
"	new file:   kNN-TL/fairseq/data/encoders/utils.py",,,
"	new file:   kNN-TL/fairseq/data/fairseq_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/fasta_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/huffman/__init__.py",,,
"	new file:   kNN-TL/fairseq/data/huffman/huffman_coder.py",,,
"	new file:   kNN-TL/fairseq/data/huffman/huffman_mmap_indexed_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/id_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/indexed_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/iterators.py",,,
"	new file:   kNN-TL/fairseq/data/language_pair_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/legacy/__init__.py",,,
"	new file:   kNN-TL/fairseq/data/legacy/block_pair_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/legacy/masked_lm_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/legacy/masked_lm_dictionary.py",,,
"	new file:   kNN-TL/fairseq/data/list_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/lm_context_window_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/lru_cache_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/mask_tokens_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/monolingual_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/multi_corpus_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/multi_corpus_sampled_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/multilingual/__init__.py",,,
"	new file:   kNN-TL/fairseq/data/multilingual/multilingual_data_manager.py",,,
"	new file:   kNN-TL/fairseq/data/multilingual/multilingual_utils.py",,,
"	new file:   kNN-TL/fairseq/data/multilingual/sampled_multi_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/multilingual/sampled_multi_epoch_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/multilingual/sampling_method.py",,,
"	new file:   kNN-TL/fairseq/data/nested_dictionary_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/noising.py",,,
"	new file:   kNN-TL/fairseq/data/num_samples_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/numel_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/offset_tokens_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/pad_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/plasma_utils.py",,,
"	new file:   kNN-TL/fairseq/data/prepend_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/prepend_token_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/raw_label_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/replace_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/resampling_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/roll_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/round_robin_zip_datasets.py",,,
"	new file:   kNN-TL/fairseq/data/shorten_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/sort_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/strip_token_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/subsample_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/text_compressor.py",,,
"	new file:   kNN-TL/fairseq/data/token_block_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/token_block_utils_fast.pyx",,,
"	new file:   kNN-TL/fairseq/data/transform_eos_concat_langpair_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/transform_eos_dataset.py",,,
"	new file:   kNN-TL/fairseq/data/transform_eos_lang_pair_dataset.py",,,
"	new file:   kNN-TL/fairseq/dataclass/__init__.py",,,
"	new file:   kNN-TL/fairseq/dataclass/configs.py",,,
"	new file:   kNN-TL/fairseq/dataclass/constants.py",,,
"	new file:   kNN-TL/fairseq/dataclass/initialize.py",,,
"	new file:   kNN-TL/fairseq/dataclass/utils.py",,,
"	new file:   kNN-TL/fairseq/distributed/__init__.py",,,
"	new file:   kNN-TL/fairseq/distributed/distributed_timeout_wrapper.py",,,
"	new file:   kNN-TL/fairseq/distributed/fully_sharded_data_parallel.py",,,
"	new file:   kNN-TL/fairseq/distributed/legacy_distributed_data_parallel.py",,,
"	new file:   kNN-TL/fairseq/distributed/module_proxy_wrapper.py",,,
"	new file:   kNN-TL/fairseq/distributed/tpu_distributed_data_parallel.py",,,
"	new file:   kNN-TL/fairseq/distributed/utils.py",,,
"	new file:   kNN-TL/fairseq/file_chunker_utils.py",,,
"	new file:   kNN-TL/fairseq/file_io.py",,,
"	new file:   kNN-TL/fairseq/file_utils.py",,,
"	new file:   kNN-TL/fairseq/hub_utils.py",,,
"	new file:   kNN-TL/fairseq/incremental_decoding_utils.py",,,
"	new file:   kNN-TL/fairseq/iterative_refinement_generator.py",,,
"	new file:   kNN-TL/fairseq/logging/__init__.py",,,
"	new file:   kNN-TL/fairseq/logging/meters.py",,,
"	new file:   kNN-TL/fairseq/logging/metrics.py",,,
"	new file:   kNN-TL/fairseq/logging/progress_bar.py",,,
"	new file:   kNN-TL/fairseq/model_parallel/__init__.py",,,
"	new file:   kNN-TL/fairseq/model_parallel/criterions/__init__.py",,,
"	new file:   kNN-TL/fairseq/model_parallel/criterions/vocab_parallel_cross_entropy.py",,,
"	new file:   kNN-TL/fairseq/model_parallel/megatron_trainer.py",,,
"	new file:   kNN-TL/fairseq/model_parallel/models/__init__.py",,,
"	new file:   kNN-TL/fairseq/model_parallel/models/pipeline_parallel_transformer/__init__.py",,,
"	new file:   kNN-TL/fairseq/model_parallel/models/pipeline_parallel_transformer/layers.py",,,
"	new file:   kNN-TL/fairseq/model_parallel/models/pipeline_parallel_transformer/model.py",,,
"	new file:   kNN-TL/fairseq/model_parallel/models/roberta/__init__.py",,,
"	new file:   kNN-TL/fairseq/model_parallel/models/roberta/model.py",,,
"	new file:   kNN-TL/fairseq/model_parallel/models/transformer.py",,,
"	new file:   kNN-TL/fairseq/model_parallel/models/transformer_lm.py",,,
"	new file:   kNN-TL/fairseq/model_parallel/modules/__init__.py",,,
"	new file:   kNN-TL/fairseq/model_parallel/modules/multihead_attention.py",,,
"	new file:   kNN-TL/fairseq/model_parallel/modules/transformer_layer.py",,,
"	new file:   kNN-TL/fairseq/models/__init__.py",,,
"	new file:   kNN-TL/fairseq/models/bart/__init__.py",,,
"	new file:   kNN-TL/fairseq/models/bart/hub_interface.py",,,
"	new file:   kNN-TL/fairseq/models/bart/model.py",,,
"	new file:   kNN-TL/fairseq/models/composite_encoder.py",,,
"	new file:   kNN-TL/fairseq/models/distributed_fairseq_model.py",,,
"	new file:   kNN-TL/fairseq/models/ema/__init__.py",,,
"	new file:   kNN-TL/fairseq/models/ema/ema.py",,,
"	new file:   kNN-TL/fairseq/models/fairseq_decoder.py",,,
"	new file:   kNN-TL/fairseq/models/fairseq_encoder.py",,,
"	new file:   kNN-TL/fairseq/models/fairseq_incremental_decoder.py",,,
"	new file:   kNN-TL/fairseq/models/fairseq_model.py",,,
"	new file:   kNN-TL/fairseq/models/fconv.py",,,
"	new file:   kNN-TL/fairseq/models/fconv_lm.py",,,
"	new file:   kNN-TL/fairseq/models/fconv_self_att.py",,,
"	new file:   kNN-TL/fairseq/models/hubert/__init__.py",,,
"	new file:   kNN-TL/fairseq/models/hubert/hubert.py",,,
"	new file:   kNN-TL/fairseq/models/hubert/hubert_asr.py",,,
"	new file:   kNN-TL/fairseq/models/huggingface/__init__.py",,,
"	new file:   kNN-TL/fairseq/models/huggingface/hf_gpt2.py",,,
"	new file:   kNN-TL/fairseq/models/lightconv.py",,,
"	new file:   kNN-TL/fairseq/models/lightconv_lm.py",,,
"	new file:   kNN-TL/fairseq/models/lstm.py",,,
"	new file:   kNN-TL/fairseq/models/lstm_lm.py",,,
"	new file:   kNN-TL/fairseq/models/masked_lm.py",,,
"	new file:   kNN-TL/fairseq/models/model_utils.py",,,
"	new file:   kNN-TL/fairseq/models/multilingual_transformer.py",,,
"	new file:   kNN-TL/fairseq/models/nat/__init__.py",,,
"	new file:   kNN-TL/fairseq/models/nat/cmlm_transformer.py",,,
"	new file:   kNN-TL/fairseq/models/nat/fairseq_nat_model.py",,,
"	new file:   kNN-TL/fairseq/models/nat/insertion_transformer.py",,,
"	new file:   kNN-TL/fairseq/models/nat/iterative_nonautoregressive_transformer.py",,,
"	new file:   kNN-TL/fairseq/models/nat/levenshtein_transformer.py",,,
"	new file:   kNN-TL/fairseq/models/nat/levenshtein_utils.py",,,
"	new file:   kNN-TL/fairseq/models/nat/nat_crf_transformer.py",,,
"	new file:   kNN-TL/fairseq/models/nat/nonautoregressive_ensembles.py",,,
"	new file:   kNN-TL/fairseq/models/na…""",,,
