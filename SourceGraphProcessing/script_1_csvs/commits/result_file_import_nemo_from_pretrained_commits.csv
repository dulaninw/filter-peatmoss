repo_url,filepath,commit_date,message
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2024-02-06T00:26:43Z,"Support uploading NeMo models to HF via `push_to_hf_hub()` (#8263)

* Initial support for saving unpacked nemo file directly and support for uploading NeMo models to Huggingface Hub

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add support to restore nemo models from HF in unpacked format

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Correct input types for model card

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update License Section to template

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add unit test to restore via unpacked checkpoint

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix typo

Signed-off-by: smajumdar <titu1994@gmail.com>

* Address comments

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Address comments

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add docs and address comments

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2024-01-24T18:25:22Z,"Add support in Neural Typecheck to disable semantic checks (#8212)

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2022-12-12T19:27:42Z,"AIStore for ASR datasets (#5462)

AIStore for ASR datasets

Signed-off-by: Ante JukicÃÅ <ajukic@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2022-11-10T19:43:34Z,"Fix Python type hints according to Python Docs (#5370)

* Remove duplicated type annotations

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

* Fix tuple annotations in function return types

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

* Add necessary imports

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

* Add necessary imports

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

* Fix types in obvious places

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

* Fix types in obvious places

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

* Fix unused import (avoid quotes in type annotations)

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

* Revert ""Fix unused import (avoid quotes in type annotations)""

This reverts commit ea433efcd9916abf8944879e791484a0a1437f83.

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

* Remove problematic import

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

* Fix list_available_models method type

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

* Revert some changes

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

* Revert quotes in list_available_models

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2022-10-24T20:22:44Z,"[TTS] remove LinVocoder and apply Vocoder as parent class. (#5206)

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2022-10-21T16:17:23Z,"[TTS] remove the avoidance of circular imports (#5214)

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2022-10-11T20:22:39Z,"Fixes for docs/typos + remove max_utts parameter from tarred datasets as it causes hang in training (#5118)

* Remove ; from jupyter notebook cells

Signed-off-by: Igor Gitman <igitman@nvidia.com>

* Fix typos in documentation/code

Signed-off-by: Igor Gitman <igitman@nvidia.com>

* Fix output message to have 'or equal'

Signed-off-by: Igor Gitman <igitman@nvidia.com>

* Link formatting fixes

Signed-off-by: Igor Gitman <igitman@nvidia.com>

* Add error if max_utts is used in tarred datasets

Signed-off-by: Igor Gitman <igitman@nvidia.com>

* Remove max_utts parameter from tarred datasets

Signed-off-by: Igor Gitman <igitman@nvidia.com>

* Fix max_utts removal in tests

Signed-off-by: Igor Gitman <igitman@nvidia.com>

* Fix typo if -> is

Signed-off-by: Igor Gitman <igitman@nvidia.com>

Signed-off-by: Igor Gitman <igitman@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2022-07-27T22:12:53Z,"Support listing Hugging Face model info (#4619)

* Support listing Hugging Face model info

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Add documentation about usage

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Add documentation about usage

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Update name of method, support list of model filters

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Improve docstring

Signed-off-by: smajumdar <smajumdar@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2022-06-01T21:28:56Z,"Update common.py (#4304)

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2022-04-28T16:50:07Z,"Add docs for `@typecheck()` (#4079)

* Add docs for `@typecheck()``

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add more fixes to core docs

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Eric Harper <complex451@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2022-04-25T19:21:58Z,"Cherry pick HF integration and bug fixes from 1.8.1 (#4052)

* Patch commons.py (#4039)

* revert export.py changes

Signed-off-by: ericharper <complex451@gmail.com>

* revert hack

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused imports

Signed-off-by: ericharper <complex451@gmail.com>

* Add support for Huggingface Hub to NeMo `from_pretrained()` (#4030)

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Eric Harper <complex451@gmail.com>

* Fixing pretrained name (#4022)

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>

* Add back Citrinet zh (#4040)

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>

* cherry pick

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: Boris Fomitchev <borisfom@users.noreply.github.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Subhankar Ghosh <subhankar2321@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2022-04-20T22:46:30Z,"Merge r1.8.0 main (#4036)

* update version

Signed-off-by: ericharper <complex451@gmail.com>

* Stateless timer fix for PTL 1.6 (#3925)

* Stateless timer fix for PTL 1.6

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Stateless timer PTL test

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix year

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove unused imports

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* GPU test

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* clean import

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: ericharper <complex451@gmail.com>

* Fix issues with librosa deprecations (#3950)

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix notebook bugs for branch r1.8.0 (#3948)

* load the model from ngc

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix all biomegatron notebook

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix the typos

Signed-off-by: Yi Dong <doyend@gmail.com>

* remove output

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix isort

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix merge error

Signed-off-by: Yi Dong <doyend@gmail.com>

* change ntpath for isort workaround

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix unit test

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix ci

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix ci bert pretraining

Signed-off-by: Yi Dong <doyend@gmail.com>

* make it compatible with main

Signed-off-by: Yi Dong <doyend@gmail.com>

* add the teste for biomegatron ner

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix argument

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix usablity issue

Signed-off-by: Yi Dong <doyend@gmail.com>

* work around

Signed-off-by: Yi Dong <doyend@gmail.com>

Co-authored-by: Yi Dong <doyend@gmail.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* Fix global batch fit loop (#3936)

* add lightning module hooks for global batch

Signed-off-by: ericharper <complex451@gmail.com>

* clean scripts

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* DP=1 fix

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* set num dataset workers to 2

Signed-off-by: ericharper <complex451@gmail.com>

* update validation_loop with GlobalDataFetcher

Signed-off-by: ericharper <complex451@gmail.com>

* add test global data fetcher

Signed-off-by: ericharper <complex451@gmail.com>

* Drop last for test ds

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix test epoch end

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix eval

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix reconfigure microbatch in the complete method

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* add comments

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Set init consumed samples

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* fix shuffle

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* add save_restore_connector arg

Signed-off-by: ericharper <complex451@gmail.com>

* Fix padding for labels and loss mask

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* GLUE/XNLI CI tests

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* limit val batches in hydra fix

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Restart CI

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix unittest

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

Co-authored-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Exports 22.03 war (#3957)

* Fixed fastpitch for 22.03

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* cleanup

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Restored mask expansion; added WAR for test container images

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* style

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Refactor restorefrom (#3927)

* update package info (#3926)

Signed-off-by: ericharper <complex451@gmail.com>

* Refactor restore_from

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Move export related python files to scripts/export/

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Return state dict after modification function

* Remove Megatron legacy parameter in common.py restore_from function

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* ability to set log_predictions to false (#3929)

* Bumping Python version

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* fixing style

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* load the model from ngc

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix all biomegatron notebook

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix the typos

Signed-off-by: Yi Dong <doyend@gmail.com>

* remove output

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix isort

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix merge error

Signed-off-by: Yi Dong <doyend@gmail.com>

* change ntpath for isort workaround

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix unit test

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix ci

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix ci bert pretraining

Signed-off-by: Yi Dong <doyend@gmail.com>

* Rearrage export files; Style fix; Extend legacy MegatronBert conversion to NLP models nemo version updation

* Glu activation variants (#3951)

* Temp

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Add reglu and swiglu activations

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style on unrelated file

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* CI changes to test activations

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix unused import

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style fix beacuse of merge from main

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* make it compatible with main

Signed-off-by: Yi Dong <doyend@gmail.com>

* add the teste for biomegatron ner

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix argument

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix usablity issue

Signed-off-by: Yi Dong <doyend@gmail.com>

* FastPitch FT notebook - Improving Speech Quality clarifications (#3954)

* FastPitch FT notebook - Improving Speech Quality clarifications

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Add pynini dependency install to FastPitch FT notebook

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Pin pynini install for FastPitch FT tutorial

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* work around

Signed-off-by: Yi Dong <doyend@gmail.com>

Co-authored-by: Eric Harper <complex451@gmail.com>
Co-authored-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>
Co-authored-by: Dima Rekesh <bmwshop@gmail.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>
Co-authored-by: Yi Dong <doyend@gmail.com>
Co-authored-by: Yi Dong <43824965+yidong72@users.noreply.github.com>
Co-authored-by: Boris Fomitchev <borisfom@users.noreply.github.com>
Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Jocelyn <jocelynh@nvidia.com>

* Bump TTS deprecation version to 1.9 (#3955)

* bump deprecation version

Signed-off-by: Jason <jasoli@nvidia.com>

* update talknet depre

Signed-off-by: Jason <jasoli@nvidia.com>

* added conformer for zh. (#3970)

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* Add pinned pynini and scipy installs to TTS training tutorial (#3967)

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Fix variable name and move models to CPU in Change partition (#3972)

* fixes

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* add CI

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>

* fix misconfiguration (#3975)

Signed-off-by: Yi Dong <doyend@gmail.com>

Co-authored-by: Yi Dong <doyend@gmail.com>

* Fix NMT variable passing bug (#3985)

* fix

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* stylefix

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* Compatability override to load_state_dict for old TTS checkpoints (#3978)

* Compatability override to load_state_dict for old TTS checkpoints

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Tacotron2 training notebook fix - add GPU argument

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Add hann window override warning for old model loading

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Notebook Bug Fixes for r1.8.0 (#3989)

* Made config related bug fixes

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Fixed cfg.get syntax

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Fix compat override for TalkNet Aligner (#3993)

* Fix compatibility override for TalkNet Aligner

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Remove extraneous logging import

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* docs fixes (#3987)

* docs fixes

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* rename files in docs

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* docs improvement

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* arg renamed

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* Fix nemo megatron restore with artifacts (#3997)

* update config_path in register_artifact

Signed-off-by: ericharper <complex451@gmail.com>

* fix register_artifact calls

Signed-off-by: ericharper <complex451@gmail.com>

* fix register_artifact calls

Signed-off-by: ericharper <complex451@gmail.com>

* update log messages to include merges file

Signed-off-by: ericharper <complex451@gmail.com>

* add default prompts to config

Signed-off-by: ericharper <complex451@gmail.com>

* Fixes val_check_interval, skip loading train data during eval (#3968)

* Change stage check

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix bugs in megatron t5 glue eval scripts

Signed-off-by: Yu Yao <yuya@nvidia.com>

* Fix reconfigure

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Change check

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix hasattr

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix typo in cfg structure

Signed-off-by: Yu Yao <yuya@nvidia.com>

* Update megatron t5 glue eval config file

Signed-off-by: Yu Yao <yuya@nvidia.com>

* Reconfigure to avoid drop last

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix for train step reconfigure as well

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Update megatron t5 glue eval config file drop_last to False

Signed-off-by: Yu Yao <yuya@nvidia.com>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* limit test batches

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

Co-authored-by: Yu Yao <54727607+yaoyu-33@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* LogProb calculation performance fix (#3984)

* performance fix for logprob computation

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix redandant assign

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix bug to add gather from TP workers

Signed-off-by: Yi Dong <doyend@gmail.com>

Co-authored-by: Yi Dong <doyend@gmail.com>

* Fix link issues in export example notebook and fix pretrained model info for MegatronBert (#4004)

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

Co-authored-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Fix single GPU training issue + change deprecated Lightning args (#4010)

* change vars

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* style fix

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* Fix P-Tune T5 model (#4001)

* fix ptune t5

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix ci test

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix the ci fail because of the order problem

Signed-off-by: Yi Dong <doyend@gmail.com>

Co-authored-by: Yi Dong <doyend@gmail.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* Megatron work-arounds (#3998)

* WAR around Apex issue, and making sure output is FP32

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixing merge issues; moving dummy Trainer; adding float() casts

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixing ColumnParallelLinear call

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Cleanup

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Cleanup#2

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* fix the broadcast shape mismatch (#4017)

Signed-off-by: Yi Dong <doyend@gmail.com>

Co-authored-by: Yi Dong <doyend@gmail.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* add known issues (#4024)

Signed-off-by: ericharper <complex451@gmail.com>

* update readme with conda env setup instructions

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* update package info

Signed-off-by: ericharper <complex451@gmail.com>

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* update package info

Signed-off-by: ericharper <complex451@gmail.com>

* revert apex guard removal

Signed-off-by: ericharper <complex451@gmail.com>

* revert --language to --lang

Signed-off-by: ericharper <complex451@gmail.com>

* fix apex guard

Signed-off-by: ericharper <complex451@gmail.com>

* remove set_trace

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* fix apex guard

Signed-off-by: ericharper <complex451@gmail.com>

* remove unreachable statement

Signed-off-by: ericharper <complex451@gmail.com>

* remove duplicate lines

Signed-off-by: ericharper <complex451@gmail.com>

* remove duplicate lines

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: Yi Dong <43824965+yidong72@users.noreply.github.com>
Co-authored-by: Yi Dong <doyend@gmail.com>
Co-authored-by: Boris Fomitchev <borisfom@users.noreply.github.com>
Co-authored-by: Ramanathan Arunachalam <ramanathan.arun@rutgers.edu>
Co-authored-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>
Co-authored-by: Dima Rekesh <bmwshop@gmail.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>
Co-authored-by: Jocelyn <jocelynh@nvidia.com>
Co-authored-by: Jason <jasoli@nvidia.com>
Co-authored-by: Vahid Noroozi <VahidooX@users.noreply.github.com>
Co-authored-by: Abhinav Khattar <aklife97@gmail.com>
Co-authored-by: Virginia Adams <78445382+vadam5@users.noreply.github.com>
Co-authored-by: Evelina <10428420+ekmb@users.noreply.github.com>
Co-authored-by: Yu Yao <54727607+yaoyu-33@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2022-04-14T15:56:58Z,"[Core] Fix type checking to be compatible with named tuples (#3986)

* Fix type checking for outputs to be compatible with named tuples

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

* Fix output names in tests

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: Eric Harper <complex451@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2022-04-04T19:12:30Z,"Megatron legacy conversion support (#3919)

* Fix merge from main

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Rollback scripts/export.py changes and create a new file for converting legacy MegatronBert NLP models to current version

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Cleanup legacy conversion code

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Move state dict mapping of legacy Megatron to NLPSaveRestoreConnector; Do transpose op in Self attention if it's a legacy checkpoint; Add a example notebook for exporting NLP Bert models

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Style fix

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

Co-authored-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2022-02-07T18:09:41Z,"Fixed EH and error reporting in restore_from (#3583)

* Fixed EH and error reporting in restore_from

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* --amend

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixed merge issue

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

Co-authored-by: Eric Harper <complex451@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2022-02-04T18:30:37Z,"[TTS] remove CheckInstall (#3577)

* remove CheckInstall

Signed-off-by: Jason <jasoli@nvidia.com>

* remove torch_tts install

Signed-off-by: Jason <jasoli@nvidia.com>

* move torch_tts tests to elsewhere in Jenkins

Signed-off-by: Jason <jasoli@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2022-01-31T22:20:29Z,"Final merge r1.6.0 main (#3570)

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* Fix the tutorial notebooks bug (#3465)

* fix checkpoint loading and model config file

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix style

Signed-off-by: Yi Dong <yidong@nvidia.com>

* Fix checkpoint converter in O2 style (#3486)

* Fix checkpoint converter in O2 style

Signed-off-by: Yu Yao <yuya@nvidia.com>

* Fix style

Signed-off-by: Yu Yao <yuya@nvidia.com>

Co-authored-by: Yu Yao <yuya@nvidia.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* Remove pickled features from tarred dataset (#3491)

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

Co-authored-by: ekmb <ebakhturina@nvidia.com>

* adding missing init files (#3505)

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* typos (#3504)

* typos

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* link fix

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* update titanet conf (#3507)

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* Fix link to NGC page for ASR (#3512)

Signed-off-by: smajumdar <titu1994@gmail.com>

* vad typo fix (#3490)

* remove always broken ptl link

Signed-off-by: fayejf <fayejf07@gmail.com>

* fix typo

Signed-off-by: fayejf <fayejf07@gmail.com>

* Add verification helper function and update docs (#3514)

* Add verification helper function and update docs

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* typo fixes

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* fixed the num_classes bug of conv decoder. (#3525)

* fixed the num_classes bug.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* added logging info.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* Enforce utf-8 on all file r/w (#3520)

* Update paths to subtask

Signed-off-by: smajumdar <titu1994@gmail.com>

* Enforce utf-8 on all file r/w

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Eric Harper <complex451@gmail.com>

* Fixed section typo (#3522)

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Pushing updated WFST Tutorial to r1.6.0 (#3521)

Signed-off-by: tbartley94 <tbartley@nvidia.com>

Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>

* Fixed duplicate cell bug (#3518)

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* WFST tutorial update (#3531)

* Pushing updated WFST Tutorial to r1.6.0

Signed-off-by: tbartley94 <tbartley@nvidia.com>

* Hopefully final corrections to WFST tutorials.

Signed-off-by: tbartley94 <tbartley@nvidia.com>

* [TTS] Fix bug in inference tts notebook (#3532)

* fix bug in inference tts notebook

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* Update Inference_ModelSelect.ipynb

* fix space

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* remove space

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* Prompt tuning documentation (#3541)

* Started prompt tuning doc

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Update prompt_tuning.rst

* Update models.rst

* Update models.rst

* Update and rename megatron_finetuning.rst to megatron_downstream_tasks.rst

* Update intro.rst

* Update intro.rst

* Update and rename megatron_downstream_tasks.rst to megatron_finetuning.rst

* Update megatron_finetuning.rst

* Delete prompt_tuning.rst

* Update README.rst

* Update docs/source/nlp/megatron_finetuning.rst

Co-authored-by: Eric Harper <complex451@gmail.com>

* Fix nmt resume (#3539)

* check for model attr

Signed-off-by: ericharper <complex451@gmail.com>

* update jenkins test

Signed-off-by: ericharper <complex451@gmail.com>

* TN bug fix (#3538)

* ve and cel fixes

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* ve and cel fixes

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* add w to single digit roman and cardinal single digit graph (non det)

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* isn't fix

Signed-off-by: ekmb <ebakhturina@nvidia.com>

Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* fix bug in tutorial (#3546)

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* Update nvidia container check (#3535)

* update nvidia container check

Signed-off-by: ericharper <complex451@gmail.com>

* update minor version

Signed-off-by: ericharper <complex451@gmail.com>

* add check to T5

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* update bert

Signed-off-by: ericharper <complex451@gmail.com>

* forgot import

Signed-off-by: ericharper <complex451@gmail.com>

* remove import

Signed-off-by: ericharper <complex451@gmail.com>

* Fix an issue with wandb not displaying updated config changes (#3552)

Signed-off-by: smajumdar <titu1994@gmail.com>

* remove extra instance (#3551)

Signed-off-by: ericharper <complex451@gmail.com>

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* update package info

Signed-off-by: ericharper <complex451@gmail.com>

* revert

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Yi Dong <43824965+yidong72@users.noreply.github.com>
Co-authored-by: yaoyu-33 <54727607+yaoyu-33@users.noreply.github.com>
Co-authored-by: Yu Yao <yuya@nvidia.com>
Co-authored-by: PeganovAnton <peganoff2@mail.ru>
Co-authored-by: ekmb <ebakhturina@nvidia.com>
Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>
Co-authored-by: Evelina <10428420+ekmb@users.noreply.github.com>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>
Co-authored-by: Vahid Noroozi <VahidooX@users.noreply.github.com>
Co-authored-by: Virginia Adams <78445382+vadam5@users.noreply.github.com>
Co-authored-by: tbartley94 <90423858+tbartley94@users.noreply.github.com>
Co-authored-by: Oktai Tatanov <oktai.tatanov@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2021-11-22T22:41:34Z,CTC Conformer fixes for ONNX/TS export  (#3072)
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2021-11-18T18:33:58Z,"Add support to modify nemo cache directory (#3208)

* Add support to modify nemo cache directory

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct cache dir resolution with absolute path

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2021-11-10T08:37:19Z,"Adding parallel transcribe for ASR models -  suppports multi-gpu/multi-node (#3017)

* added transcribe_speech_parallel.py.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed style.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* removed comments.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed bug.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed bug.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* added comments inside the script.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed speed_collate_fn for TTS.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed speed_collate_fn for TTS.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed speed_collate_fn for TTS.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed speed_collate_fn for TTS.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* added return_sample_id.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* added return_sample_id.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* merged dataset configs.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* merged dataset configs.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* dropped sample_ids from train and validation.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* dropped sample_ids from train and validation.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* reverted tts patches.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* reverted tts patches.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed the default values in the dataset's config

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed the default values in the dataset's config

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* Fixed the bug for optional outputs.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* Fixed the bug for optional outputs.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* addressed some comments.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* disabled dali support for return_sample_id.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* disabled dali support for return_sample_id.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* converted the config to omegaconf.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* converted the config to omegaconf.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* converted the config to omegaconf.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* moved wer/cer calculation to the end.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* moved wer/cer calculation to the end.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* moved wer/cer calculation to the end.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* moved wer/cer calculation to the end.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* moved wer/cer calculation to the end.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* calculates global wer instead of per sample.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* calculates global wer instead of per sample.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* calculates global wer instead of per sample.

Signed-off-by: Vahid <vnoroozi@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2021-10-21T03:06:37Z,"[BigNLP] Merge Megatron GPT to main (#2975)

* fix gpu init after removing debug print in mpu

Signed-off-by: ericharper <complex451@gmail.com>

* add fused_adam

Signed-off-by: ericharper <complex451@gmail.com>

* check ds is not none before logging len

Signed-off-by: ericharper <complex451@gmail.com>

* set fp16 arg to true and fix enum conflict

Signed-off-by: ericharper <complex451@gmail.com>

* make fp16 arg configurable

Signed-off-by: ericharper <complex451@gmail.com>

* add grad clip from megatron

Signed-off-by: ericharper <complex451@gmail.com>

* Linear warmup with cosine annealing and constant holding (#2846)

* Testing cosine schedule

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* More fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* update config for constant steps in schedule

Signed-off-by: ericharper <complex451@gmail.com>

* temporarily import enum from megatron

Signed-off-by: ericharper <complex451@gmail.com>

* add grad clip for fp32

Signed-off-by: ericharper <complex451@gmail.com>

* update check for _del_model_without_trainer

Signed-off-by: ericharper <complex451@gmail.com>

* updating restore for model parallel

Signed-off-by: ericharper <complex451@gmail.com>

* add predict script

Signed-off-by: ericharper <complex451@gmail.com>

* update test iters

Signed-off-by: ericharper <complex451@gmail.com>

* add barrier

Signed-off-by: ericharper <complex451@gmail.com>

* return if clip_val is 0 or None

Signed-off-by: ericharper <complex451@gmail.com>

* when using amp clip grads after they are unscaled

Signed-off-by: ericharper <complex451@gmail.com>

* make native amp scaler hyperparams configurable

Signed-off-by: ericharper <complex451@gmail.com>

* (1) nvfuser, (2) amp-casting decoration (#2894)

* (1) nvfuser, (2) amp-casting decoration

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* support bf16

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* update package info

Signed-off-by: ericharper <complex451@gmail.com>

* add set device to constructor

Signed-off-by: ericharper <complex451@gmail.com>

* set_device in constructor

Signed-off-by: ericharper <complex451@gmail.com>

* [BigNLP] Remove megatron-lm dependency. (#2910)

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* add load_fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* add load_fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* update megatron_init

Signed-off-by: ericharper <complex451@gmail.com>

* add fused kernels

Signed-off-by: ericharper <complex451@gmail.com>

* add fused kernels

Signed-off-by: ericharper <complex451@gmail.com>

* update process batch

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* add megatron clip_grad

Signed-off-by: ericharper <complex451@gmail.com>

* trying to resolve circular import error

Signed-off-by: ericharper <complex451@gmail.com>

* rename file

Signed-off-by: ericharper <complex451@gmail.com>

* remove non-gpt models and datasets from __init__ files

Signed-off-by: ericharper <complex451@gmail.com>

* set device in constructorfor gpu init

Signed-off-by: ericharper <complex451@gmail.com>

* set device in constructorfor gpu init

Signed-off-by: ericharper <complex451@gmail.com>

* set_device in constructor

Signed-off-by: ericharper <complex451@gmail.com>

* clean config

Signed-off-by: ericharper <complex451@gmail.com>

* update MegatronDataset

Signed-off-by: ericharper <complex451@gmail.com>

* clean up MegatronModule

Signed-off-by: ericharper <complex451@gmail.com>

* clean up MegatronModule

Signed-off-by: ericharper <complex451@gmail.com>

* rename fp16 and bf16 flags to fused_softmax_input_in_fp16/bf16

Signed-off-by: ericharper <complex451@gmail.com>

* rename to fused_fp16

Signed-off-by: ericharper <complex451@gmail.com>

* add fused_fp16 arg to LayerNorm calls

Signed-off-by: ericharper <complex451@gmail.com>

* fix arg name

Signed-off-by: ericharper <complex451@gmail.com>

* fix arg name

Signed-off-by: ericharper <complex451@gmail.com>

* fix import

Signed-off-by: ericharper <complex451@gmail.com>

* update arg

Signed-off-by: ericharper <complex451@gmail.com>

* skip warmup default to True

Signed-off-by: ericharper <complex451@gmail.com>

* skip warmup default to True

Signed-off-by: ericharper <complex451@gmail.com>

* Adding complete method to MegatronGPTModel (#2935)

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* make ffn_hidden_size mandatory

Signed-off-by: ericharper <complex451@gmail.com>

* Manually migrating timing of step into branch (#2937)

* 1. Manually migrating timing of step into branch.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Updated file name and content.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Updated to latest code.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

Co-authored-by: Micha Livne <mlivne@nvidia.com>

* remove unused imports

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* check fused_fp16 and fused_bf16 are not both True

Signed-off-by: ericharper <complex451@gmail.com>

* update predict script for model parallel .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>
Co-authored-by: Micha Livne <mlivne@nvidia.com>

* NVfuser (#2943)

* activation checkpoint recompute

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* selective nvfuser setup

* Megatron gpt bfloat support (#2926)

* Save/restore fix

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Another merge

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Bf16 args in init

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Set precision

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove debug stuff

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* add bf16 casting decorator

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* Bfloat layernorm propagation

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* activation checkpoint recompute

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* selective nvfuser setup

* More arg removal

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove BERTDataset

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* update to latest apex and patch transformer autocast

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: ericharper <complex451@gmail.com>

* don't set jit for bf16

Signed-off-by: ericharper <complex451@gmail.com>

* replace apex.mpu

Signed-off-by: ericharper <complex451@gmail.com>

* fix grad clip

Signed-off-by: ericharper <complex451@gmail.com>

* NVFuser fixes (#2951)

* Fuser fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove dummy handler

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove PTL plugin based logic for fusion

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* remove duplicated file

Signed-off-by: ericharper <complex451@gmail.com>

* typo (#2960)

Signed-off-by: ericharper <complex451@gmail.com>

* [BigNLP] Script to convert GPT checkpoint to .nemo (#2958)

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* add load_fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* add load_fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* update megatron_init

Signed-off-by: ericharper <complex451@gmail.com>

* add fused kernels

Signed-off-by: ericharper <complex451@gmail.com>

* add fused kernels

Signed-off-by: ericharper <complex451@gmail.com>

* update process batch

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* add megatron clip_grad

Signed-off-by: ericharper <complex451@gmail.com>

* trying to resolve circular import error

Signed-off-by: ericharper <complex451@gmail.com>

* rename file

Signed-off-by: ericharper <complex451@gmail.com>

* remove non-gpt models and datasets from __init__ files

Signed-off-by: ericharper <complex451@gmail.com>

* set device in constructorfor gpu init

Signed-off-by: ericharper <complex451@gmail.com>

* set device in constructorfor gpu init

Signed-off-by: ericharper <complex451@gmail.com>

* set_device in constructor

Signed-off-by: ericharper <complex451@gmail.com>

* clean config

Signed-off-by: ericharper <complex451@gmail.com>

* update MegatronDataset

Signed-off-by: ericharper <complex451@gmail.com>

* clean up MegatronModule

Signed-off-by: ericharper <complex451@gmail.com>

* clean up MegatronModule

Signed-off-by: ericharper <complex451@gmail.com>

* rename fp16 and bf16 flags to fused_softmax_input_in_fp16/bf16

Signed-off-by: ericharper <complex451@gmail.com>

* rename to fused_fp16

Signed-off-by: ericharper <complex451@gmail.com>

* add fused_fp16 arg to LayerNorm calls

Signed-off-by: ericharper <complex451@gmail.com>

* fix arg name

Signed-off-by: ericharper <complex451@gmail.com>

* fix arg name

Signed-off-by: ericharper <complex451@gmail.com>

* fix import

Signed-off-by: ericharper <complex451@gmail.com>

* update arg

Signed-off-by: ericharper <complex451@gmail.com>

* skip warmup default to True

Signed-off-by: ericharper <complex451@gmail.com>

* skip warmup default to True

Signed-off-by: ericharper <complex451@gmail.com>

* Adding complete method to MegatronGPTModel (#2935)

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* make ffn_hidden_size mandatory

Signed-off-by: ericharper <complex451@gmail.com>

* Manually migrating timing of step into branch (#2937)

* 1. Manually migrating timing of step into branch.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Updated file name and content.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Updated to latest code.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

Co-authored-by: Micha Livne <mlivne@nvidia.com>

* remove unused imports

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* check fused_fp16 and fused_bf16 are not both True

Signed-off-by: ericharper <complex451@gmail.com>

* update predict script for model parallel .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* add script to convert .ckpt to .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* in progress

Signed-off-by: ericharper <complex451@gmail.com>

* update

Signed-off-by: ericharper <complex451@gmail.com>

* convert mp checkpoints to nemo

Signed-off-by: ericharper <complex451@gmail.com>

* update help

Signed-off-by: ericharper <complex451@gmail.com>

* add safeguard for model parallel save_to

Signed-off-by: ericharper <complex451@gmail.com>

* adjust NLPModel save_to to be safer for model parallel

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>
Co-authored-by: Micha Livne <mlivne@nvidia.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* [BigNLP] Update GPT evaluation to work with tensor model parallel  (#2959)

* in progress

Signed-off-by: ericharper <complex451@gmail.com>

* update args

Signed-off-by: ericharper <complex451@gmail.com>

* add request dataset

Signed-off-by: ericharper <complex451@gmail.com>

* tokenize request

Signed-off-by: ericharper <complex451@gmail.com>

* in progress

Signed-off-by: ericharper <complex451@gmail.com>

* able to run

Signed-off-by: ericharper <complex451@gmail.com>

* reduce logits

Signed-off-by: ericharper <complex451@gmail.com>

* capture response

Signed-off-by: ericharper <complex451@gmail.com>

* squeeze and unsqueeze

Signed-off-by: ericharper <complex451@gmail.com>

* handle non model parallel case

Signed-off-by: ericharper <complex451@gmail.com>

* clean imports

Signed-off-by: ericharper <complex451@gmail.com>

* add file

Signed-off-by: ericharper <complex451@gmail.com>

* convert logits to log_probs

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* rename logits to log_probs

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* add megatron gpt pretraining

Signed-off-by: ericharper <complex451@gmail.com>

* add megatron gpt pretraining

Signed-off-by: ericharper <complex451@gmail.com>

* add megatron gpt pretraining

Signed-off-by: ericharper <complex451@gmail.com>

* updating to work with latest megatron

Signed-off-by: ericharper <complex451@gmail.com>

* updating to work with latest megatron

Signed-off-by: ericharper <complex451@gmail.com>

* update _del_model

Signed-off-by: ericharper <complex451@gmail.com>

* adding gpt model

Signed-off-by: ericharper <complex451@gmail.com>

* adding gpt model

Signed-off-by: ericharper <complex451@gmail.com>

* adding gpt model

Signed-off-by: ericharper <complex451@gmail.com>

* instantiate GPTmodel

Signed-off-by: ericharper <complex451@gmail.com>

* adding build dataset

Signed-off-by: ericharper <complex451@gmail.com>

* build megatron dataset in .setup

Signed-off-by: ericharper <complex451@gmail.com>

* setup dataloader

Signed-off-by: ericharper <complex451@gmail.com>

* add vocab_file and merge_file to megatron init

Signed-off-by: ericharper <complex451@gmail.com>

* add forward

Signed-off-by: ericharper <complex451@gmail.com>

* add train loss

Signed-off-by: ericharper <complex451@gmail.com>

* add optimizer

Signed-off-by: ericharper <complex451@gmail.com>

* add exp_manager

Signed-off-by: ericharper <complex451@gmail.com>

* multi-gpu is working

Signed-off-by: ericharper <complex451@gmail.com>

* adding val loop

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* adding val loop

Signed-off-by: ericharper <complex451@gmail.com>

* fix ranks

Signed-off-by: ericharper <complex451@gmail.com>

* fix model parallel checkpoint saving

Signed-off-by: ericharper <complex451@gmail.com>

* fix _del_model

Signed-off-by: ericharper <complex451@gmail.com>

* added megatron batch sampler

Signed-off-by: ericharper <complex451@gmail.com>

* try to fix num steps

Signed-off-by: ericharper <complex451@gmail.com>

* add wandb to config

Signed-off-by: ericharper <complex451@gmail.com>

* log lr

Signed-off-by: ericharper <complex451@gmail.com>

* add warmup ratio to config

Signed-off-by: ericharper <complex451@gmail.com>

* update configs

Signed-off-by: ericharper <complex451@gmail.com>

* update configs

Signed-off-by: ericharper <complex451@gmail.com>

* add cpu init to args

Signed-off-by: ericharper <complex451@gmail.com>

* update config

Signed-off-by: ericharper <complex451@gmail.com>

* update config

Signed-off-by: ericharper <complex451@gmail.com>

* Initial megatron dataset port

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix merge conflicts

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* License fixes and megatron model porting

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* More fixes to import from nemo rather than megatron

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix circular imports

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Revert config file

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Restructure further to avoid circular imports

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* add Makefile

Signed-off-by: ericharper <complex451@gmail.com>

* Add megatron modules

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* add license

Signed-off-by: ericharper <complex451@gmail.com>

* Port from latest megatron

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* update cfg

Signed-off-by: ericharper <complex451@gmail.com>

* update config

Signed-off-by: ericharper <complex451@gmail.com>

* add _del_model_without_trainer

Signed-off-by: ericharper <complex451@gmail.com>

* add data preprocessing script

Signed-off-by: ericharper <complex451@gmail.com>

* update config

Signed-off-by: ericharper <complex451@gmail.com>

* use apex mpu

Signed-off-by: ericharper <complex451@gmail.com>

* replace print_rank_0 with nemo utils logging

Signed-off-by: ericharper <complex451@gmail.com>

* use apex mpu

Signed-off-by: ericharper <complex451@gmail.com>

* use apex mpu

Signed-off-by: ericharper <complex451@gmail.com>

* add use_cpu_initialization

Signed-off-by: ericharper <complex451@gmail.com>

* fixing autoresume in progress

Signed-off-by: ericharper <complex451@gmail.com>

* properly removing last checkpoint

Signed-off-by: ericharper <complex451@gmail.com>

* log consumed samples

Signed-off-by: ericharper <complex451@gmail.com>

* fix mp autoresume

Signed-off-by: ericharper <complex451@gmail.com>

* add NLPSaveRestoreConnector

Signed-off-by: ericharper <complex451@gmail.com>

* Megatron GPT training with NeMo tokenizers (#2818)

* Update files from megatron repo

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove non NLP data related files from megatron

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Merge megatron and nemo tokenizers

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove get_tokenizer() calls from gpt model

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Update tokenizer yaml config

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* add todo

Signed-off-by: ericharper <complex451@gmail.com>

* update config

Signed-off-by: ericharper <complex451@gmail.com>

* make init_method_std configurable

Signed-off-by: ericharper <complex451@gmail.com>

* make gpu init work by setting random seed earlier

Signed-off-by: ericharper <complex451@gmail.com>

* fix gpu init after removing debug print in mpu

Signed-off-by: ericharper <complex451@gmail.com>

* add fused_adam

Signed-off-by: ericharper <complex451@gmail.com>

* check ds is not none before logging len

Signed-off-by: ericharper <complex451@gmail.com>

* set fp16 arg to true and fix enum conflict

Signed-off-by: ericharper <complex451@gmail.com>

* make fp16 arg configurable

Signed-off-by: ericharper <complex451@gmail.com>

* add grad clip from megatron

Signed-off-by: ericharper <complex451@gmail.com>

* Linear warmup with cosine annealing and constant holding (#2846)

* Testing cosine schedule

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* More fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* update config for constant steps in schedule

Signed-off-by: ericharper <complex451@gmail.com>

* temporarily import enum from megatron

Signed-off-by: ericharper <complex451@gmail.com>

* add grad clip for fp32

Signed-off-by: ericharper <complex451@gmail.com>

* update check for _del_model_without_trainer

Signed-off-by: ericharper <complex451@gmail.com>

* updating restore for model parallel

Signed-off-by: ericharper <complex451@gmail.com>

* add predict script

Signed-off-by: ericharper <complex451@gmail.com>

* update test iters

Signed-off-by: ericharper <complex451@gmail.com>

* add barrier

Signed-off-by: ericharper <complex451@gmail.com>

* return if clip_val is 0 or None

Signed-off-by: ericharper <complex451@gmail.com>

* when using amp clip grads after they are unscaled

Signed-off-by: ericharper <complex451@gmail.com>

* make native amp scaler hyperparams configurable

Signed-off-by: ericharper <complex451@gmail.com>

* (1) nvfuser, (2) amp-casting decoration (#2894)

* (1) nvfuser, (2) amp-casting decoration

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* support bf16

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* update package info

Signed-off-by: ericharper <complex451@gmail.com>

* add set device to constructor

Signed-off-by: ericharper <complex451@gmail.com>

* set_device in constructor

Signed-off-by: ericharper <complex451@gmail.com>

* [BigNLP] Remove megatron-lm dependency. (#2910)

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* add load_fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* add load_fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* update megatron_init

Signed-off-by: ericharper <complex451@gmail.com>

* add fused kernels

Signed-off-by: ericharper <complex451@gmail.com>

* add fused kernels

Signed-off-by: ericharper <complex451@gmail.com>

* update process batch

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* add megatron clip_grad

Signed-off-by: ericharper <complex451@gmail.com>

* trying to resolve circular import error

Signed-off-by: ericharper <complex451@gmail.com>

* rename file

Signed-off-by: ericharper <complex451@gmail.com>

* remove non-gpt models and datasets from __init__ files

Signed-off-by: ericharper <complex451@gmail.com>

* set device in constructorfor gpu init

Signed-off-by: ericharper <complex451@gmail.com>

* set device in constructorfor gpu init

Signed-off-by: ericharper <complex451@gmail.com>

* set_device in constructor

Signed-off-by: ericharper <complex451@gmail.com>

* clean config

Signed-off-by: ericharper <complex451@gmail.com>

* update MegatronDataset

Signed-off-by: ericharper <complex451@gmail.com>

* clean up MegatronModule

Signed-off-by: ericharper <complex451@gmail.com>

* clean up MegatronModule

Signed-off-by: ericharper <complex451@gmail.com>

* rename fp16 and bf16 flags to fused_softmax_input_in_fp16/bf16

Signed-off-by: ericharper <complex451@gmail.com>

* rename to fused_fp16

Signed-off-by: ericharper <complex451@gmail.com>

* add fused_fp16 arg to LayerNorm calls

Signed-off-by: ericharper <complex451@gmail.com>

* fix arg name

Signed-off-by: ericharper <complex451@gmail.com>

* fix arg name

Signed-off-by: ericharper <complex451@gmail.com>

* fix import

Signed-off-by: ericharper <complex451@gmail.com>

* update arg

Signed-off-by: ericharper <complex451@gmail.com>

* skip warmup default to True

Signed-off-by: ericharper <complex451@gmail.com>

* skip warmup default to True

Signed-off-by: ericharper <complex451@gmail.com>

* Adding complete method to MegatronGPTModel (#2935)

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* make ffn_hidden_size mandatory

Signed-off-by: ericharper <complex451@gmail.com>

* Manually migrating timing of step into branch (#2937)

* 1. Manually migrating timing of step into branch.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Updated file name and content.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Updated to latest code.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

Co-authored-by: Micha Livne <mlivne@nvidia.com>

* remove unused imports

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* check fused_fp16 and fused_bf16 are not both True

Signed-off-by: ericharper <complex451@gmail.com>

* update predict script for model parallel .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>
Co-authored-by: Micha Livne <mlivne@nvidia.com>

* NVfuser (#2943)

* activation checkpoint recompute

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* selective nvfuser setup

* Megatron gpt bfloat support (#2926)

* Save/restore fix

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Another merge

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Bf16 args in init

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Set precision

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove debug stuff

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* add bf16 casting decorator

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* Bfloat layernorm propagation

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* activation checkpoint recompute

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* selective nvfuser setup

* More arg removal

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove BERTDataset

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* update to latest apex and patch transformer autocast

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: ericharper <complex451@gmail.com>

* don't set jit for bf16

Signed-off-by: ericharper <complex451@gmail.com>

* replace apex.mpu

Signed-off-by: ericharper <complex451@gmail.com>

* fix grad clip

Signed-off-by: ericharper <complex451@gmail.com>

* NVFuser fixes (#2951)

* Fuser fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove dummy handler

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove PTL plugin based logic for fusion

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* remove duplicated file

Signed-off-by: ericharper <complex451@gmail.com>

* typo (#2960)

Signed-off-by: ericharper <complex451@gmail.com>

* [BigNLP] Script to convert GPT checkpoint to .nemo (#2958)

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* add load_fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* add load_fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* update megatron_init

Signed-off-by: ericharper <complex451@gmail.com>

* add fused kernels

Signed-off-by: ericharper <complex451@gmail.com>

* add fused kernels

Signed-off-by: ericharper <complex451@gmail.com>

* update process batch

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* add megatron clip_grad

Signed-off-by: ericharper <complex451@gmail.com>

* trying to resolve circular import error

Signed-off-by: ericharper <complex451@gmail.com>

* rename file

Signed-off-by: ericharper <complex451@gmail.com>

* remove non-gpt models and datasets from __init__ files

Signed-off-by: ericharper <complex451@gmail.com>

* set device in constructorfor gpu init

Signed-off-by: ericharper <complex451@gmail.com>

* set device in constructorfor gpu init

Signed-off-by: ericharper <complex451@gmail.com>

* set_device in constructor

Signed-off-by: ericharper <complex451@gmail.com>

* clean config

Signed-off-by: ericharper <complex451@gmail.com>

* update MegatronDataset

Signed-off-by: ericharper <complex451@gmail.com>

* clean up MegatronModule

Signed-off-by: ericharper <complex451@gmail.com>

* clean up MegatronModule

Signed-off-by: ericharper <complex451@gmail.com>

* rename fp16 and bf16 flags to fused_softmax_input_in_fp16/bf16

Signed-off-by: ericharper <complex451@gmail.com>

* rename to fused_fp16

Signed-off-by: ericharper <complex451@gmail.com>

* add fused_fp16 arg to LayerNorm calls

Signed-off-by: ericharper <complex451@gmail.com>

* fix arg name

Signed-off-by: ericharper <complex451@gmail.com>

* fix arg name

Signed-off-by: ericharper <complex451@gmail.com>

* fix import

Signed-off-by: ericharper <complex451@gmail.com>

* update arg

Signed-off-by: ericharper <complex451@gmail.com>

* skip warmup default to True

Signed-off-by: ericharper <complex451@gmail.com>

* skip warmup default to True

Signed-off-by: ericharper <complex451@gmail.com>

* Adding complete method to MegatronGPTModel (#2935)

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* make ffn_hidden_size mandatory

Signed-off-by: ericharper <complex451@gmail.com>

* Manually migrating timing of step into branch (#2937)

* 1. Manually migrating timing of step into branch.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Updated file name and content.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Updated to latest code.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

Co-authored-by: Micha Livne <mlivne@nvidia.com>

* remove unused imports

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* check fused_fp16 and fused_bf16 are not both True

Signed-off-by: ericharper <complex451@gmail.com>

* update predict script for model parallel .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* add script to convert .ckpt to .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* in progress

Signed-off-by: ericharper <complex451@gmail.com>

* update

Signed-off-by: ericharper <complex451@gmail.com>

* convert mp checkpoints to nemo

Signed-off-by: ericharper <complex451@gmail.com>

* update help

Signed-off-by: ericharper <complex451@gmail.com>

* add safeguard for model parallel save_to

Signed-off-by: ericharper <complex451@gmail.com>

* adjust NLPModel save_to to be safer for model parallel

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>
Co-authored-by: Micha Livne <mlivne@nvidia.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* [BigNLP] Update GPT evaluation to work with tensor model parallel  (#2959)

* in progress

Signed-off-by: ericharper <complex451@gmail.com>

* update args

Signed-off-by: ericharper <complex451@gmail.com>

* add request dataset

Signed-off-by: ericharper <complex451@gmail.com>

* tokenize request

Signed-off-by: ericharper <complex451@gmail.com>

* in progress

Signed-off-by: ericharper <complex451@gmail.com>

* able to run

Signed-off-by: ericharper <complex451@gmail.com>

* reduce logits

Signed-off-by: ericharper <complex451@gmail.com>

* capture response

Signed-off-by: ericharper <complex451@gmail.com>

* squeeze and unsqueeze

Signed-off-by: ericharper <complex451@gmail.com>

* handle non model parallel case

Signed-off-by: ericharper <complex451@gmail.com>

* clean imports

Signed-off-by: ericharper <complex451@gmail.com>

* add file

Signed-off-by: ericharper <complex451@gmail.com>

* convert logits to log_probs

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* rename logits to log_probs

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* fix copyright headers

Signed-off-by: ericharper <complex451@gmail.com>

* fix copyright headers

Signed-off-by: ericharper <complex451@gmail.com>

* remove old TimingCallback

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* update jenkins to use latest apex and sandeep's fork

Signed-off-by: ericharper <complex451@gmail.com>

* update jenkins

Signed-off-by: ericharper <complex451@gmail.com>

* update jenkins

Signed-off-by: ericharper <complex451@gmail.com>

* update jenkins

Signed-off-by: ericharper <complex451@gmail.com>

* update jenkins

Signed-off-by: ericharper <complex451@gmail.com>

* try 2109 container

Signed-off-by: ericharper <complex451@gmail.com>

* try cuda container

Signed-off-by: ericharper <complex451@gmail.com>

* use internal container

Signed-off-by: ericharper <complex451@gmail.com>

* update checkpoint tests

Signed-off-by: ericharper <complex451@gmail.com>

* fix scheduler args

Signed-off-by: ericharper <complex451@gmail.com>

* update eval

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* update jenkins to use ptl 1.5 rc

Signed-off-by: ericharper <complex451@gmail.com>

* add import guard to jenkins

Signed-off-by: ericharper <complex451@gmail.com>

* add import guard to jenkins

Signed-off-by: ericharper <complex451@gmail.com>

* remove deterministic

Signed-off-by: ericharper <complex451@gmail.com>

* install numba .53

Signed-off-by: ericharper <complex451@gmail.com>

* allow for more variance

Signed-off-by: ericharper <complex451@gmail.com>

* update trainer config dataclass

Signed-off-by: ericharper <complex451@gmail.com>

* test_get_optimizer on gpu

Signed-off-by: ericharper <complex451@gmail.com>

* revert comment

Signed-off-by: ericharper <complex451@gmail.com>

* change trainer config default to 32

Signed-off-by: ericharper <complex451@gmail.com>

* [BigNLP] Remove fused kernel code instead use Apex (#2984)

* remove fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* remove fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* remove fused layer norm and fused softmax and use apex instead

Signed-off-by: ericharper <complex451@gmail.com>

* update imports

Signed-off-by: ericharper <complex451@gmail.com>

* remove comment

Signed-off-by: ericharper <complex451@gmail.com>

* use apex enums

Signed-off-by: ericharper <complex451@gmail.com>

* use apex enums

Signed-off-by: ericharper <complex451@gmail.com>

* add tab

Signed-off-by: ericharper <complex451@gmail.com>

* Timer with sliding window (#3002)

Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>

* revert tab

Signed-off-by: ericharper <complex451@gmail.com>

* check for rank zero

Signed-off-by: ericharper <complex451@gmail.com>

* check for rank zero

Signed-off-by: ericharper <complex451@gmail.com>

* try explicit log dir

Signed-off-by: ericharper <complex451@gmail.com>

* add +

Signed-off-by: ericharper <complex451@gmail.com>

* don't rm

Signed-off-by: ericharper <complex451@gmail.com>

* make dir if it doesn't exist

Signed-off-by: ericharper <complex451@gmail.com>

* create mp nemo file in temp directory

Signed-off-by: ericharper <complex451@gmail.com>

* simplify mp save_to

Signed-off-by: ericharper <complex451@gmail.com>

* handle mp 1 case

Signed-off-by: ericharper <complex451@gmail.com>

* style fix

Signed-off-by: ericharper <complex451@gmail.com>

* remove files

Signed-off-by: ericharper <complex451@gmail.com>

* fix consumed_samples when resuming

Signed-off-by: ericharper <complex451@gmail.com>

* fix reinstall.sh

Signed-off-by: ericharper <complex451@gmail.com>

* update req

Signed-off-by: ericharper <complex451@gmail.com>

* add more detailed log for dataloaders

Signed-off-by: ericharper <complex451@gmail.com>

* check if cuda is available before using fused_adam

Signed-off-by: ericharper <complex451@gmail.com>

* revert comment

Signed-off-by: ericharper <complex451@gmail.com>

* update eval script to use model.freeze

Signed-off-by: ericharper <complex451@gmail.com>

* log train loss averaged over gradient accumulation steps

Signed-off-by: ericharper <complex451@gmail.com>

* check copyright earlier

Signed-off-by: ericharper <complex451@gmail.com>

* todo

Signed-off-by: ericharper <complex451@gmail.com>

* override SaveRestoreConnector in NLPModel init

Signed-off-by: ericharper <complex451@gmail.com>

* move to scripts

Signed-off-by: ericharper <complex451@gmail.com>

* remove star import

Signed-off-by: ericharper <complex451@gmail.com>

* remove comments

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused dataset

Signed-off-by: ericharper <complex451@gmail.com>

* removed barrier

Signed-off-by: ericharper <complex451@gmail.com>

* check cfg

Signed-off-by: ericharper <complex451@gmail.com>

* remove logging

Signed-off-by: ericharper <complex451@gmail.com>

* freeze, unfreeze

Signed-off-by: ericharper <complex451@gmail.com>

* return None

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused imports

Signed-off-by: ericharper <complex451@gmail.com>

* add TODO

Signed-off-by: ericharper <complex451@gmail.com>

* typecheck

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* todo

Signed-off-by: ericharper <complex451@gmail.com>

* add common native plugin

Signed-off-by: ericharper <complex451@gmail.com>

* restore with trainer

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* deprecate megatron-lm bert

Signed-off-by: ericharper <complex451@gmail.com>

* deprecate megatron-lm bert

Signed-off-by: ericharper <complex451@gmail.com>

* compile helpers ont he fly

Signed-off-by: ericharper <complex451@gmail.com>

* remove amp_level

Signed-off-by: ericharper <complex451@gmail.com>

* remove amp_level from configs

Signed-off-by: ericharper <complex451@gmail.com>

* add missing import

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* remove amp_level

Signed-off-by: ericharper <complex451@gmail.com>

* use fast huggingface tokenizers by default

Signed-off-by: ericharper <complex451@gmail.com>

* deal with huggingface tokenizer positional args

Signed-off-by: ericharper <complex451@gmail.com>

* deal with huggingface tokenizer positional args

Signed-off-by: ericharper <complex451@gmail.com>

* deal with huggingface tokenizer positional args

Signed-off-by: ericharper <complex451@gmail.com>

* revert use_fast default to False

Signed-off-by: ericharper <complex451@gmail.com>

* return super training_epoch_end

Signed-off-by: ericharper <complex451@gmail.com>

* remove optimizer_idx arg from training_step

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused arg from on_train_epoch_end

Signed-off-by: ericharper <complex451@gmail.com>

* add restore_from_path to nemo config

Signed-off-by: ericharper <complex451@gmail.com>

* add comment

Signed-off-by: ericharper <complex451@gmail.com>

* revert

Signed-off-by: ericharper <complex451@gmail.com>

* override connector if not subclassing NLPSaveRestoreConnector for model parallel save

Signed-off-by: ericharper <complex451@gmail.com>

* update test optimizer

Signed-off-by: ericharper <complex451@gmail.com>

* clean up

Signed-off-by: ericharper <complex451@gmail.com>

* clean up

Signed-off-by: ericharper <complex451@gmail.com>

* clean up

Signed-off-by: ericharper <complex451@gmail.com>

* clean up

Signed-off-by: ericharper <complex451@gmail.com>

* make data_prefix mandatory in config

Signed-off-by: ericharper <complex451@gmail.com>

* update installation instructions on readme

Signed-off-by: ericharper <complex451@gmail.com>

* update dockerfile

Signed-off-by: ericharper <complex451@gmail.com>

* add todo

Signed-off-by: ericharper <complex451@gmail.com>

* raise error if trying to use always_save_nemo with model parallel model

Signed-off-by: ericharper <complex451@gmail.com>

* remove comment

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>
Co-authored-by: Micha Livne <mlivne@nvidia.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2021-08-30T18:53:05Z,"make restore errors more useful (#2750)

Signed-off-by: Ryan Leary <rleary@nvidia.com>

Co-authored-by: Ryan Leary <rleary@nvidia.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2021-08-17T14:55:43Z,"Refactor and Minimize Dependencies (#2643)

* squash

Signed-off-by: Jason <jasoli@nvidia.com>

* add comments

Signed-off-by: Jason <jasoli@nvidia.com>

* style and cleanup

Signed-off-by: Jason <jasoli@nvidia.com>

* cleanup

Signed-off-by: Jason <jasoli@nvidia.com>

* add new test file

Signed-off-by: Jason <jasoli@nvidia.com>

* syntax

Signed-off-by: Jason <jasoli@nvidia.com>

* style

Signed-off-by: Jason <jasoli@nvidia.com>

* typo

Signed-off-by: Jason <jasoli@nvidia.com>

* update

Signed-off-by: Jason <jasoli@nvidia.com>

* update

Signed-off-by: Jason <jasoli@nvidia.com>

* update

Signed-off-by: Jason <jasoli@nvidia.com>

* try again

Signed-off-by: Jason <jasoli@nvidia.com>

* wip

Signed-off-by: Jason <jasoli@nvidia.com>

* style; ci should fail

Signed-off-by: Jason <jasoli@nvidia.com>

* final

Signed-off-by: Jason <jasoli@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2021-08-06T00:45:57Z,"Add save restore connector to ModelPT (#2592)

* add save restore connector

Signed-off-by: ericharper <complex451@gmail.com>

* add save restore connector

Signed-off-by: ericharper <complex451@gmail.com>

* add save restore connector property

Signed-off-by: ericharper <complex451@gmail.com>

* add _default_save_to

Signed-off-by: ericharper <complex451@gmail.com>

* moving globals to app_state

Signed-off-by: ericharper <complex451@gmail.com>

* moving globals to app_state

Signed-off-by: ericharper <complex451@gmail.com>

* add model attribute to connector

Signed-off-by: ericharper <complex451@gmail.com>

* add model attribute to connector

Signed-off-by: ericharper <complex451@gmail.com>

* fix tabs

Signed-off-by: ericharper <complex451@gmail.com>

* fix tabs

Signed-off-by: ericharper <complex451@gmail.com>

* remove ModelPT import

Signed-off-by: ericharper <complex451@gmail.com>

* add default restore

Signed-off-by: ericharper <complex451@gmail.com>

* add default restore

Signed-off-by: ericharper <complex451@gmail.com>

* remove eff globals

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* fix tabs

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* update globals, remove save restore property

Signed-off-by: ericharper <complex451@gmail.com>

* fix typo

Signed-off-by: ericharper <complex451@gmail.com>

* add setter

Signed-off-by: ericharper <complex451@gmail.com>

* add setter

Signed-off-by: ericharper <complex451@gmail.com>

* fix app_state restore flag

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* update paths

Signed-off-by: ericharper <complex451@gmail.com>

* add connector arg to from_pretrained

Signed-off-by: ericharper <complex451@gmail.com>

* update save restore connector after instantiating

Signed-off-by: ericharper <complex451@gmail.com>

* use connector

Signed-off-by: ericharper <complex451@gmail.com>

* get class from config in .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* add TODO

Signed-off-by: ericharper <complex451@gmail.com>

* move extract_state_dict to connector

Signed-off-by: ericharper <complex451@gmail.com>

* add methods for toch save and torch load

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* update mock model conf

Signed-off-by: ericharper <complex451@gmail.com>

* revert

Signed-off-by: ericharper <complex451@gmail.com>

* move mock model to common collection

Signed-off-by: ericharper <complex451@gmail.com>

* update NLPModel

Signed-off-by: ericharper <complex451@gmail.com>

* update test to use connector

Signed-off-by: ericharper <complex451@gmail.com>

* move artifacts to save restore connector

Signed-off-by: ericharper <complex451@gmail.com>

* add save_restore_connector arg to register_artifact

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* default save_restore_connector arg to None

Signed-off-by: ericharper <complex451@gmail.com>

* default save_restore_connector arg to None

Signed-off-by: ericharper <complex451@gmail.com>

* clean commented line

Signed-off-by: ericharper <complex451@gmail.com>

* default save_restore_connector arg to None

Signed-off-by: ericharper <complex451@gmail.com>

* move MockModel

Signed-off-by: ericharper <complex451@gmail.com>

* fix docstrings, remove underscores, default from connector

Signed-off-by: ericharper <complex451@gmail.com>

* update docstring

Signed-off-by: ericharper <complex451@gmail.com>

* update docstring

Signed-off-by: ericharper <complex451@gmail.com>

* change name to is_model_being_restored

Signed-off-by: ericharper <complex451@gmail.com>

* move constants from AppState to SaveRestoreConnector

Signed-off-by: ericharper <complex451@gmail.com>

* encapsulate logic for model parallel checkpoint

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* update mock config

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* add init_subclass, remove connector arg from register_artifact, move MockModel to tests

Signed-off-by: ericharper <complex451@gmail.com>

* remove old import

Signed-off-by: ericharper <complex451@gmail.com>

* Add tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Finalize tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Finalize tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* fixing lgtm

Signed-off-by: ericharper <complex451@gmail.com>

* fix lgtm

Signed-off-by: ericharper <complex451@gmail.com>

* update NLPModel.restore_from

Signed-off-by: ericharper <complex451@gmail.com>

* Fix classpath resolution

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2021-06-07T13:09:37Z,"Update FastPitch (#2249)

* wip

Signed-off-by: Jason <jasoli@nvidia.com>

* c1

Signed-off-by: Jason <jasoli@nvidia.com>

* bug fixes

Signed-off-by: Jason <jasoli@nvidia.com>

* bug fixes

Signed-off-by: Jason <jasoli@nvidia.com>

* v2

Signed-off-by: Jason <jasoli@nvidia.com>

* changes

Signed-off-by: Jason <jasoli@nvidia.com>

* add types, old model working

Signed-off-by: Jason <jasoli@nvidia.com>

* pitch

Signed-off-by: Jason <jasoli@nvidia.com>

* update

Signed-off-by: Jason <jasoli@nvidia.com>

* update

Signed-off-by: Jason <jasoli@nvidia.com>

* let it work

Signed-off-by: Jason <jasoli@nvidia.com>

* fixes

Signed-off-by: Jason <jasoli@nvidia.com>

* add oktai comments

Signed-off-by: Jason <jasoli@nvidia.com>

* debug

Signed-off-by: Jason <jasoli@nvidia.com>

* scale

Signed-off-by: Jason <jasoli@nvidia.com>

* wip

Signed-off-by: Jason <jasoli@nvidia.com>

* fix test for v1

Signed-off-by: Jason <jasoli@nvidia.com>

* merge train and val

Signed-off-by: Jason <jasoli@nvidia.com>

* back to par bin att, add correct encoder settings

Signed-off-by: Jason <jasoli@nvidia.com>

* try

Signed-off-by: Jason <jasoli@nvidia.com>

* undo

Signed-off-by: Jason <jasoli@nvidia.com>

* lgtm:

Signed-off-by: Jason <jasoli@nvidia.com>

* style

Signed-off-by: Jason <jasoli@nvidia.com>

* default to ljs

Signed-off-by: Jason <jasoli@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2021-05-17T23:16:29Z,"Set strict=True everywhere by default. (#2225)

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2021-05-01T00:31:23Z,"Removing graphsurgeon optional dependency, improving import error rep‚Ä¶ (#2144)

* Removing graphsurgeon optional dependency, improving import error reporting

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixing scope error

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2021-04-20T18:30:30Z,"add simple alias to from_pretrained (#2056)

Signed-off-by: Jason <jasoli@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2021-04-05T21:12:03Z,"Add support for subclass overriding ""target"" in Serialization (#2021)

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2021-03-30T02:51:15Z,"Support target classpath resolution for all ModelPT subclasses (#1982)

* Support target classpath resolution for all ModelPT subclasses

Signed-off-by: smajumdar <titu1994@gmail.com>

* Support target classpath resolution for all ModelPT subclasses

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2021-03-15T16:58:23Z,"Properly construct set of pretrained models (#1910)

* Properly construct set of pretrained models

Signed-off-by: smajumdar <titu1994@gmail.com>

* Style fix

Signed-off-by: smajumdar <titu1994@gmail.com>

* Double test equality

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2021-02-25T20:18:37Z,"Add Transcription script for all ASR models  (#1786)

* Add CTC transcription scripts

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add speech transcription script

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add speech transcription script

Signed-off-by: smajumdar <titu1994@gmail.com>

* Revert old changes

Signed-off-by: smajumdar <titu1994@gmail.com>

* Revert old changes

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add jenkins test to run transcribe_speech.py

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add missing apostrophe

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct duplicate stage name

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update jenkins

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update jenkins

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update jenkins

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update jenkins

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update jenkins

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update jenkins

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update jenkins

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update jenkins

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update jenkins

Signed-off-by: smajumdar <titu1994@gmail.com>

* temp remove gpu unittests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update Jenkinsfile

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update Jenkinsfile

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update Jenkinsfile

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update Jenkinsfile

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update Jenkinsfile

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update Jenkinsfile

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update Jenkinsfile

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update Jenkinsfile

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update Jenkinsfile

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update Jenkinsfile

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update Jenkinsfile

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update Jenkinsfile

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update Jenkinsfile

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update Jenkinsfile

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update Jenkinsfile

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update Jenkinsfile

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update Jenkinsfile

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update Jenkinsfile

Signed-off-by: smajumdar <titu1994@gmail.com>

* Give up on Jenkinsfile

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2021-02-11T13:52:40Z,"Add container support to `typecheck` (#1743)

* Add tests for input container support

Signed-off-by: smajumdar <titu1994@gmail.com>

* Support input container typecheking

Signed-off-by: smajumdar <titu1994@gmail.com>

* remove redundant info

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add more tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add support for containers in output type attachment

Signed-off-by: smajumdar <titu1994@gmail.com>

* Start adding docstrings

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add docstrings

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update RNNT typechecking

Signed-off-by: smajumdar <titu1994@gmail.com>

* Remove dangling `data`

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update neural types for waveglow.py

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update neural types for waveglowloss.py

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update neural types for stftlosses.py and squeezewave.py

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update neural types for squeezewave.py

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Jason <jasoli@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2021-01-29T18:42:14Z,"add new quantization nodes in QuartzNet (#1678)

* add new quantization nodes in QuartzNet

Signed-off-by: Vincent Huang <vincenth@nvidia.com>

* Refactor quantization support to be optional and localized

Signed-off-by: smajumdar <titu1994@gmail.com>

* Guard last few self.quantize

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update docstring for override_config_path

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix conf assignment

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct amp via flag

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add missing self.quantize

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-12-03T22:36:50Z,"Update model config automatically inside ModelPT (#1511)

* Add ModelPT level config update to remove `params`

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update docstring

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update imports

Signed-off-by: smajumdar <titu1994@gmail.com>

* Refactor model cfg initialization

Signed-off-by: smajumdar <titu1994@gmail.com>

* Code foratting

Signed-off-by: smajumdar <titu1994@gmail.com>

* Code formatting

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add compat with RNNT BPE models

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct optimizer and schedulers for new configs

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct return value

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correctly support new configs for ASR

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update every ASR config

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add comment tASR BPE models

Signed-off-by: smajumdar <titu1994@gmail.com>

* Reduce batch size from 64 to 32

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update name of methods

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-11-13T22:55:22Z,"Dataset creation tool based on CTC-segmentation (#1450)

Dataset creation tool based on CTC-segmentation
Signed-off-by: ekmb <ebakhturina@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-11-11T06:43:34Z,"Update NeMo core for melgan (#1437)

* update nemo core

Signed-off-by: Jason <jasoli@nvidia.com>

* Update modelPT.py

Remove Optional from setup_validation_data
Signed-off-by: Jason <jasoli@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-10-19T17:36:12Z,"Upgrade PTL to 1.0.2 (#1278)

* update ptl version

Signed-off-by: ericharper <complex451@gmail.com>

* not working yet

Signed-off-by: ericharper <complex451@gmail.com>

* not working

Signed-off-by: ericharper <complex451@gmail.com>

* fixed bug

Signed-off-by: ericharper <complex451@gmail.com>

* safe divide

Signed-off-by: ericharper <complex451@gmail.com>

* couldn't pickle

Signed-off-by: ericharper <complex451@gmail.com>

* fixed bug

Signed-off-by: ericharper <complex451@gmail.com>

* clean up

Signed-off-by: ericharper <complex451@gmail.com>

* style fix

Signed-off-by: ericharper <complex451@gmail.com>

* updated to Metric

Signed-off-by: ericharper <complex451@gmail.com>

* updated

Signed-off-by: ericharper <complex451@gmail.com>

* updated

Signed-off-by: ericharper <complex451@gmail.com>

* rebase

Signed-off-by: ericharper <complex451@gmail.com>

* more updates

Signed-off-by: ericharper <complex451@gmail.com>

* updated

Signed-off-by: ericharper <complex451@gmail.com>

* typo (#1286)

Signed-off-by: Jason <jasoli@nvidia.com>

* updates

Signed-off-by: ericharper <complex451@gmail.com>

* updates

Signed-off-by: ericharper <complex451@gmail.com>

* updates

Signed-off-by: ericharper <complex451@gmail.com>

* updated

Signed-off-by: ericharper <complex451@gmail.com>

* updates

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: Jason <jasoli@nvidia.com>

* remove flag

Signed-off-by: ericharper <complex451@gmail.com>

* updates

Signed-off-by: ericharper <complex451@gmail.com>

* updates

Signed-off-by: ericharper <complex451@gmail.com>

* updates

Signed-off-by: ericharper <complex451@gmail.com>

* style fix

Signed-off-by: ericharper <complex451@gmail.com>

* updates

Signed-off-by: ericharper <complex451@gmail.com>

* updated

Signed-off-by: ericharper <complex451@gmail.com>

* style fix

Signed-off-by: ericharper <complex451@gmail.com>

* updates

Signed-off-by: ericharper <complex451@gmail.com>

* updated

Signed-off-by: ericharper <complex451@gmail.com>

* update speech top k models

Signed-off-by: Jason <jasoli@nvidia.com>

* update megatron metric & lgtm

Signed-off-by: Jason <jasoli@nvidia.com>

* distributed_backend -> accelerator

Signed-off-by: Jason <jasoli@nvidia.com>

* updated

Signed-off-by: ericharper <complex451@gmail.com>

* ptl 1.0.2

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* updated

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* updates

Signed-off-by: ericharper <complex451@gmail.com>

* lower batch size

Signed-off-by: ericharper <complex451@gmail.com>

* batch_size 1

Signed-off-by: ericharper <complex451@gmail.com>

* testing gpu 1 on ci

Signed-off-by: ericharper <complex451@gmail.com>

* testing gpu 1 on ci

Signed-off-by: ericharper <complex451@gmail.com>

* testing gpu 1 on ci

Signed-off-by: ericharper <complex451@gmail.com>

* removing model parallel from jenkins until after upgrade

Signed-off-by: ericharper <complex451@gmail.com>

* remove test

Signed-off-by: ericharper <complex451@gmail.com>

* disable pretrained nlp tests for now

Signed-off-by: Jason <jasoli@nvidia.com>

* update classification report in intent and punc models

Signed-off-by: Jason <jasoli@nvidia.com>

* update jenkinsfile

Signed-off-by: Jason <jasoli@nvidia.com>

* update to new log method

Signed-off-by: Jason <jasoli@nvidia.com>

* update to new log method

Signed-off-by: Jason <jasoli@nvidia.com>

* updated

Signed-off-by: ericharper <complex451@gmail.com>

* adding test back to try ptl fix

Signed-off-by: ericharper <complex451@gmail.com>

* remove trainer.check_val_every_n_epoch during tests

Signed-off-by: Jason <jasoli@nvidia.com>

* temporarily removing NER from_pretrained test

Signed-off-by: ericharper <complex451@gmail.com>

* remove pretrained test for now

Signed-off-by: Jason <jasoli@nvidia.com>

* disabled correct tests

Signed-off-by: Jason <jasoli@nvidia.com>

* add strict, reenable tests

Signed-off-by: Jason <jasoli@nvidia.com>

* fix final tests

Signed-off-by: Jason <jasoli@nvidia.com>

* add strict to punctationmodel

Signed-off-by: Jason <jasoli@nvidia.com>

* MegaBERT tests use GPU 1

Signed-off-by: ericharper <complex451@gmail.com>

* fix bracket

Signed-off-by: ericharper <complex451@gmail.com>

* revert compute_topk functional

Signed-off-by: ericharper <complex451@gmail.com>

* revert compute_topk functional

Signed-off-by: ericharper <complex451@gmail.com>

* style fix

Signed-off-by: ericharper <complex451@gmail.com>

* revert compute_topk functional

Signed-off-by: ericharper <complex451@gmail.com>

* revert compute_topk functional

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* fixed classification report

Signed-off-by: ericharper <complex451@gmail.com>

* fixed classification report

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* multiply precision by 100

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* fix

Signed-off-by: ericharper <complex451@gmail.com>

* update

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* update

Signed-off-by: ericharper <complex451@gmail.com>

* nlp logging update

Signed-off-by: ericharper <complex451@gmail.com>

* nlp logging update

Signed-off-by: ericharper <complex451@gmail.com>

* nlp logging update

Signed-off-by: ericharper <complex451@gmail.com>

* nlp logging update

Signed-off-by: ericharper <complex451@gmail.com>

* nlp logging update

Signed-off-by: ericharper <complex451@gmail.com>

* nlp logging update

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* update docstring

Signed-off-by: ericharper <complex451@gmail.com>

* remove commented line

Signed-off-by: ericharper <complex451@gmail.com>

* remove import

Signed-off-by: ericharper <complex451@gmail.com>

* remove commented lines

Signed-off-by: ericharper <complex451@gmail.com>

* remove commented lines

Signed-off-by: ericharper <complex451@gmail.com>

* remove commented lines

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Jason <jasoli@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-09-28T23:06:48Z,"Fixed typos, improved notebooks (#1221)

* Removed dataset info logging from transcribe method

Signed-off-by: Vitaly Lavrukhin <vlavrukhin@nvidia.com>

* Minor fixes to NeMo Primer notebook

Signed-off-by: Vitaly Lavrukhin <vlavrukhin@nvidia.com>

* Fixed a docstring

Signed-off-by: Vitaly Lavrukhin <vlavrukhin@nvidia.com>

* Fixed typos in NeMo_voice_swap_app notebook

Signed-off-by: Vitaly Lavrukhin <vlavrukhin@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-09-22T06:20:07Z,"hashcache (#1200)

* hashcache

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* more descripting path

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-09-17T21:25:08Z,"Enable loading a model on device + broken link fix (#1183)

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-09-11T21:45:33Z,"Add Pretrained TTS test script; Extend from_pretrained() Functionality (#1142)

* add test script; extend from_pretrained() functionality

Signed-off-by: Jason <jasoli@nvidia.com>

* isort

Signed-off-by: Jason <jasoli@nvidia.com>

* batch inputs to waveglow

Signed-off-by: Jason <jasoli@nvidia.com>

* isort

Signed-off-by: Jason <jasoli@nvidia.com>

* update list_available_models

Signed-off-by: Jason <jasoli@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-09-11T17:53:03Z,"Fix SQuAD tutorial for QA (#1150)

* add import

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* add option for config file in Model.from_pretrained()

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* add section about loading pretrained model to notebook

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-09-07T04:54:52Z,"Improve usability of NeuralModule + Typing (#1129)

* Make FileIO concrete with empty implementations

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add __eq__ for NeuralType

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct return type for result

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct _target_ setup

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-09-01T14:21:53Z,"Update TTS, Add Notebook (#1047)

* add conf

Signed-off-by: Jason <jasoli@nvidia.com>

* merge; trim

Signed-off-by: Jason <jasoli@nvidia.com>

* add sigma

Signed-off-by: Jason <jasoli@nvidia.com>

* add sigma

Signed-off-by: Jason <jasoli@nvidia.com>

* add very WIP notebook

Signed-off-by: Jason <jasoli@nvidia.com>

* update config

Signed-off-by: Jason <jasoli@nvidia.com>

* update conf

Signed-off-by: Jason <jasoli@nvidia.com>

* gitignore

Signed-off-by: Jason <jasoli@nvidia.com>

* update logger creation

Signed-off-by: Jason <jasoli@nvidia.com>

* make resume_from_checkpoint a str

Signed-off-by: Jason <jasoli@nvidia.com>

* make resume_from_checkpoint a str

Signed-off-by: Jason <jasoli@nvidia.com>

* update exp_manager

Signed-off-by: Jason <jasoli@nvidia.com>

* fix

Signed-off-by: Jason <jasoli@nvidia.com>

* fixes

Signed-off-by: Jason <jasoli@nvidia.com>

* copy -> move

Signed-off-by: Jason <jasoli@nvidia.com>

* fixes

Signed-off-by: Jason <jasoli@nvidia.com>

* add sigma

Signed-off-by: Jason <jasoli@nvidia.com>

* clear notebook

Signed-off-by: Jason <jasoli@nvidia.com>

* rm

Signed-off-by: Jason <jasoli@nvidia.com>

* current WIP

Signed-off-by: Jason <jasoli@nvidia.com>

* working v1

Signed-off-by: Jason <jasoli@nvidia.com>

* style

Signed-off-by: Jason <jasoli@nvidia.com>

* fix

Signed-off-by: Jason <jasoli@nvidia.com>

* WIP on glowtts

Signed-off-by: Jason <jasoli@nvidia.com>

* finalize glow tts and gl

Signed-off-by: Jason <jasoli@nvidia.com>

* move

Signed-off-by: Jason <jasoli@nvidia.com>

* fixes

Signed-off-by: Jason <jasoli@nvidia.com>

* update glow tts

Signed-off-by: Jason <jasoli@nvidia.com>

* bugfix

Signed-off-by: Jason <jasoli@nvidia.com>

* stop creating classes during forward

Signed-off-by: Jason <jasoli@nvidia.com>

* update notebook

Signed-off-by: Jason <jasoli@nvidia.com>

* use from_pretrained

Signed-off-by: Jason <jasoli@nvidia.com>

* style

Signed-off-by: Jason <jasoli@nvidia.com>

* update params logic

Signed-off-by: Jason <jasoli@nvidia.com>

* remove experimental import

Signed-off-by: Jason <jasoli@nvidia.com>

* bugfix

Signed-off-by: Jason <jasoli@nvidia.com>

* fix

Signed-off-by: Jason <jasoli@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-08-31T17:14:00Z,"Support overriding the config path when restoring model (#1088)

* Add restore from override option and test

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct strict signature

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add support for extracting state dict from nemo files

Signed-off-by: smajumdar <titu1994@gmail.com>

* correct signature of abstract FileIO method

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-08-21T19:44:20Z,"Update Input Validation (#1051)

* update input validation

Signed-off-by: Jason <jasoli@nvidia.com>

* update wording

Signed-off-by: Jason <jasoli@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-08-11T17:42:51Z,"TTS update (#1016)

* remove from_config_dict

Signed-off-by: Jason <jasoli@nvidia.com>

* remove todo

Signed-off-by: Jason <jasoli@nvidia.com>

* typo

Signed-off-by: Jason <jasoli@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-08-10T21:07:29Z,"Add experimental models + updates to configs for MatchboxNet (#1009)

* Add cn 192 8x pool model

Signed-off-by: smajumdar <titu1994@gmail.com>

* Prepare contextnet 8x pooling model

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add config files and resolve configs prior to serialization and de-serialization

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update configs for ASR

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update docstring of BPE models

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add docstrings for processing scripts

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add docstring for MatchboxNet

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct config name

Signed-off-by: smajumdar <titu1994@gmail.com>

* Style fix

Signed-off-by: smajumdar <titu1994@gmail.com>

* Revert *args and **kwargs

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix TTS loading of dataset

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-08-03T17:59:57Z,"Logging Cleanup (#971)

* cleanup

Signed-off-by: Jason <jasoli@nvidia.com>

* isort

Signed-off-by: Jason <jasoli@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-07-31T06:28:37Z,"Adding TS export format and trying script (#958)

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-07-30T17:46:16Z,"TTS Collection (#874)

* add structure

Signed-off-by: Jason <jasoli@nvidia.com>

* add files

Signed-off-by: Jason <jasoli@nvidia.com>

* add init

Signed-off-by: Jason <jasoli@nvidia.com>

* fix init

Signed-off-by: Jason <jasoli@nvidia.com>

* update taco

Signed-off-by: Jason <jasoli@nvidia.com>

* format

Signed-off-by: Jason <jasoli@nvidia.com>

* typo

Signed-off-by: Jason <jasoli@nvidia.com>

* val change

Signed-off-by: Jason <jasoli@nvidia.com>

* update

Signed-off-by: Jason <jasoli@nvidia.com>

* update

Signed-off-by: Jason <jasoli@nvidia.com>

* update

Signed-off-by: Jason <jasoli@nvidia.com>

* add header

Signed-off-by: Jason <jasoli@nvidia.com>

* add waveglow

Signed-off-by: Jason <jasoli@nvidia.com>

* waveglow fix

Signed-off-by: Jason <jasoli@nvidia.com>

* waveglow fix

Signed-off-by: Jason <jasoli@nvidia.com>

* waveglow fix

Signed-off-by: Jason <jasoli@nvidia.com>

* add O1

Signed-off-by: Jason <jasoli@nvidia.com>

* add todo

Signed-off-by: Jason <jasoli@nvidia.com>

* change batch sizes

Signed-off-by: Jason <jasoli@nvidia.com>

* move

Signed-off-by: Jason <jasoli@nvidia.com>

* move

Signed-off-by: Jason <jasoli@nvidia.com>

* V1

Signed-off-by: Jason <jasoli@nvidia.com>

* V1

Signed-off-by: Jason <jasoli@nvidia.com>

* structure

Signed-off-by: Jason <jasoli@nvidia.com>

* clean up

Signed-off-by: Jason <jasoli@nvidia.com>

* fix tacotron2

Signed-off-by: Jason <jasoli@nvidia.com>

* add files

Signed-off-by: Jason <jasoli@nvidia.com>

* update yamls

Signed-off-by: Jason <jasoli@nvidia.com>

* fix waveglow 1/2

Signed-off-by: Jason <jasoli@nvidia.com>

* fix waveglow 2/3

Signed-off-by: Jason <jasoli@nvidia.com>

* fix waveglow 3/?

Signed-off-by: Jason <jasoli@nvidia.com>

* merge asr and tts

Signed-off-by: Jason <jasoli@nvidia.com>

* update waveglow

Signed-off-by: Jason <jasoli@nvidia.com>

* isort

Signed-off-by: Jason <jasoli@nvidia.com>

* update configs

Signed-off-by: Jason <jasoli@nvidia.com>

* update waveglow's dataloader

Signed-off-by: Jason <jasoli@nvidia.com>

* update and refactor

Signed-off-by: Jason <jasoli@nvidia.com>

* update t2 and waveglow

Signed-off-by: Jason <jasoli@nvidia.com>

* enforce dictconfig; style

Signed-off-by: Jason <jasoli@nvidia.com>

* add fastdevruns

Signed-off-by: Jason <jasoli@nvidia.com>

* name

Signed-off-by: Jason <jasoli@nvidia.com>

* import

Signed-off-by: Jason <jasoli@nvidia.com>

* update tests

Signed-off-by: Jason <jasoli@nvidia.com>

* yaml

Signed-off-by: Jason <jasoli@nvidia.com>

* use hydra rnner

Signed-off-by: Jason <jasoli@nvidia.com>

* style

Signed-off-by: Jason <jasoli@nvidia.com>

* move callback to common

Signed-off-by: Jason <jasoli@nvidia.com>

* fixed tacotron 2; add typing to all neuralmodules

Signed-off-by: Jason <jasoli@nvidia.com>

* flatten

Signed-off-by: Jason <jasoli@nvidia.com>

* Style

Signed-off-by: Jason <jasoli@nvidia.com>

* patch jenkins

Signed-off-by: Jason <jasoli@nvidia.com>

* change typeheck logic

Signed-off-by: Jason <jasoli@nvidia.com>

* fix t2

Signed-off-by: Jason <jasoli@nvidia.com>

* fix wg

Signed-off-by: Jason <jasoli@nvidia.com>

* style

Signed-off-by: Jason <jasoli@nvidia.com>

* update configs

Signed-off-by: Jason <jasoli@nvidia.com>

* add num_workers

Signed-off-by: Jason <jasoli@nvidia.com>

* update config

Signed-off-by: Jason <jasoli@nvidia.com>

* update gitignore

Signed-off-by: Jason <jasoli@nvidia.com>

* config

Signed-off-by: Jason <jasoli@nvidia.com>

* enable fp16 on t2

Signed-off-by: Jason <jasoli@nvidia.com>

* enable fp16 on t2

Signed-off-by: Jason <jasoli@nvidia.com>

* enable fp16 on t2

Signed-off-by: Jason <jasoli@nvidia.com>

* fix t2

Signed-off-by: Jason <jasoli@nvidia.com>

* fixes

Signed-off-by: Jason <jasoli@nvidia.com>

* add typing to models; use nemo loss class

Signed-off-by: Jason <jasoli@nvidia.com>

* make wg work

Signed-off-by: Jason <jasoli@nvidia.com>

* make t2 work

Signed-off-by: Jason <jasoli@nvidia.com>

* style

Signed-off-by: Jason <jasoli@nvidia.com>

* fix wg

Signed-off-by: Jason <jasoli@nvidia.com>

* add better debug info to shape check

Signed-off-by: Jason <jasoli@nvidia.com>

* lgtm import error

Signed-off-by: Jason <jasoli@nvidia.com>

* lower batch size for testing

Signed-off-by: Jason <jasoli@nvidia.com>

* address comments

Signed-off-by: Jason <jasoli@nvidia.com>

* address comments and remove experimental

Signed-off-by: Jason <jasoli@nvidia.com>

* standardize

Signed-off-by: Jason <jasoli@nvidia.com>

* mark back as experimental

Signed-off-by: Jason <jasoli@nvidia.com>

* style

Signed-off-by: Jason <jasoli@nvidia.com>

* experimental

Signed-off-by: Jason <jasoli@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-07-28T21:32:39Z,"Add automatic name assignments in case names are not resolved (#936)

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-07-28T17:16:43Z,"Support type definition override at function level (@typecheck()) (#908)

* Support type override at function level

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update docstring

Signed-off-by: smajumdar <titu1994@gmail.com>

* Remove type preservation inside instance (not required)

Signed-off-by: smajumdar <titu1994@gmail.com>

* Resolve defaults for _validate_*

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add dim assertions to typecheck

Signed-off-by: smajumdar <titu1994@gmail.com>

* Check for none in type axis

Signed-off-by: smajumdar <titu1994@gmail.com>

* Rebase

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-07-28T16:51:08Z,"Nemo file updates (#903)

* add target if not present. restore using torch

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* put test back

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* save/restore weights just using torch state_dict

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* full class name and s2t infer draft for testing

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* fix style

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* adding artifacts to .nemo file

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* update config path

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* bugfix

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* bugfix

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* add register_artifact method

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* minor adjustments to speech_to_text_infer.py

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* download from cloud

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* fix CPU

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* prettyfy inference, add Jenkins test

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* bugfix

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* prettyfy

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* prettyfy inference script. Add jenkins test

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* fixing jenkins file

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* tmp fix jenkins file

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* style and jenkins update

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* bugfix

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* remove unnecessary CI test

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* script header update

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* addressing some review feedback

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-07-27T21:05:14Z,"Add ability to globally disable @typecheck (#925)

* Add ability to globally disable type checks

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add method to init

Signed-off-by: smajumdar <titu1994@gmail.com>

* Convert to staticmethod instead

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-07-24T16:55:54Z,"Patch @typecheck() to support containers as outputs (#901)

* Patch @typecheck() to support container and optionally test and attach types

Signed-off-by: smajumdar <titu1994@gmail.com>

* Style fixes

Signed-off-by: smajumdar <titu1994@gmail.com>

* Patch support for optional dims in input

Signed-off-by: smajumdar <titu1994@gmail.com>

* Patch ner typecheck

Signed-off-by: smajumdar <titu1994@gmail.com>

* Patch punctuation_capitalization_model.py

Signed-off-by: smajumdar <titu1994@gmail.com>

* LGTM fix

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add recursive testing of input types and docstring

Signed-off-by: smajumdar <titu1994@gmail.com>

* Code formatting

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add typecheck tests

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-07-22T21:52:29Z,"Add MatchboxNet + ASR Classification model support (#889)

* Start work on matchboxnet_3x1x64.yaml

Signed-off-by: smajumdar <titu1994@gmail.com>

* Begin matchboxnet support

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add MatchboxNet to candidate

Signed-off-by: smajumdar <titu1994@gmail.com>

* Style fixes

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update docstring

Signed-off-by: smajumdar <titu1994@gmail.com>

* Address reviewer comments

Signed-off-by: smajumdar <titu1994@gmail.com>

* LGTM fixes

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update docstring on how to properly evaluate topk accuracy

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update Jenkinsfile to run speech commands on small dataset

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct formatting and update Jenkinsfile

Signed-off-by: smajumdar <titu1994@gmail.com>

* Remove usage of MFCC

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct config on Jenkinsfile

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct config on Jenkinsfile

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct config on Jenkinsfile

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct config on Jenkinsfile

Signed-off-by: smajumdar <titu1994@gmail.com>

* Parallel asr Jenkinsfile

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update Jenkinsfile to use seperate dirs for speech training

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-07-21T03:52:48Z,"fix speech to text and revert serialization to original state

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-07-20T23:49:18Z," 'safer' deserialization

Signed-off-by: Tomasz Kornuta <tkornuta@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-07-20T21:48:51Z,"removed comment, fixed format

Signed-off-by: Tomasz Kornuta <tkornuta@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-07-20T21:23:48Z,"tests fixed

Signed-off-by: Tomasz Kornuta <tkornuta@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-07-17T05:05:41Z,"address some feedback, add more tests

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-07-16T20:52:14Z,"save/restore to nemo file implementation

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-07-01T06:01:19Z,"adjusting loss to NeMo convention

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-07-01T00:25:25Z,"Style fix

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-07-01T00:02:42Z,"Add new dataset schema as well as new typechecking for dataset items

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-06-30T01:35:50Z,"Refactor @typecheck() decorator to defer checked actions to Typing abstract interface. Add docstring for usage

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-06-30T00:13:35Z,"adjust datasets

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-06-27T00:07:45Z,"Fully support typecheck() decorator for training

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-06-27T00:07:45Z,"Add typecheck decorator

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-06-26T06:23:03Z,"renaming plus loss change

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-06-25T22:01:16Z,"removed NeMo prefix unless necessary

Signed-off-by: ericharper <complex451@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-06-25T21:50:47Z,"removed NeMo prefix unless necessary

Signed-off-by: ericharper <complex451@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-06-25T21:40:29Z,"naming suggestions

Signed-off-by: ericharper <complex451@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-06-25T00:13:40Z,"Many changes:

* update licenses everywhere
* update class structure and names
* fill in blocks and pieces for ASR model
* NON-FUNCTIONAL YET

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-06-22T18:05:15Z,"changes

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/common.py,2020-06-20T05:26:00Z,"clean-plate commit

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2024-03-01T17:13:09Z,"Doc String fix for documentation update (#8554)

* Fix SpeakerDecoder doc string

Signed-off-by: Dong Hyuk Chang <donghyukc@nvidia.com>

* Fix asr RNNT doc strings

Signed-off-by: Dong Hyuk Chang <donghyukc@nvidia.com>

* Fix ctc decoding doc strings

Signed-off-by: Dong Hyuk Chang <donghyukc@nvidia.com>

* More doc string fixes

Signed-off-by: Dong Hyuk Chang <donghyukc@nvidia.com>

* RNNTDecoding doc strings fix

Signed-off-by: Dong Hyuk Chang <donghyukc@nvidia.com>

* More doc string fixes

Signed-off-by: Dong Hyuk Chang <donghyukc@nvidia.com>

* modelPT, dataset doc string fixes

Signed-off-by: Dong Hyuk Chang <donghyukc@nvidia.com>

* Fix generate, encode, decode docstrings

Signed-off-by: Dong Hyuk Chang <donghyukc@nvidia.com>

* Update generate function docstring

Signed-off-by: Dong Hyuk Chang <donghyukc@nvidia.com>

* More generate function docstring update

Signed-off-by: Dong Hyuk Chang <donghyukc@nvidia.com>

---------

Signed-off-by: Dong Hyuk Chang <donghyukc@nvidia.com>
Co-authored-by: Dong Hyuk Chang <donghyukc@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2024-02-28T21:52:51Z,"Fix AccessMixin (#8536)

* fix AccessMixin

Signed-off-by: stevehuang52 <heh@nvidia.com>

* remove caching propagate_model_guid

Signed-off-by: stevehuang52 <heh@nvidia.com>

---------

Signed-off-by: stevehuang52 <heh@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2024-02-03T05:35:28Z," Attention encoder-decoder models for multiple speech-to-text tasks  (#8242)

* Rebasing canary changes at current main

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* Move the changes from asr transformer to nlp transformer as originally intended

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* update eval to strip spaces before punctuations

Signed-off-by: stevehuang52 <heh@nvidia.com>

* update pc strip

Signed-off-by: stevehuang52 <heh@nvidia.com>

* [canary] Refactor: `PromptedAudioToTextLhotseDataset` and `EncDecMultiTaskModel` (#8247)

* Create a separate CanaryDataset and use it inside `transformer_bpe_models.py`. Ditches `token_sequence_format`.

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* [canary] Refactor: move changes in transformer_bpe_models.py to Canar‚Ä¶ (#8252)

* [canary] Refactor: move changes in transformer_bpe_models.py to CanaryModel

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* Rename `CanaryModel` to `EncDecMultiTaskModel` and remove inheritance from `EncDecTransfModelBPE`; add a separate config for this model

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

---------

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* Rename `CanaryDataset` to `PromptedAudioToTextLhotseDataset`; add `prompt_format_fn` argument; clean-up the `_canary_prompt_format` function a bit

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* Move tokenization into `prompt_format_fn`, fix usage, add docs

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* Backward-compatible utterance validation

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* Improve type annotations

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* config and prompt_fn registration changes from review

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

---------

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* fix transcribe config

Signed-off-by: stevehuang52 <heh@nvidia.com>

* Refactor Canary to follow schema of remaining ASR models (#8260)

* Initial draft of multi task beam decoding strategy

Signed-off-by: smajumdar <titu1994@gmail.com>

* Stabilize inference

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update AED Multi Task model to mostly conform to Archetype-Type format. Update config

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add change decoding strategy

Signed-off-by: smajumdar <titu1994@gmail.com>

* Remove redundant imports

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Cleanup

Signed-off-by: smajumdar <titu1994@gmail.com>

* Cleanup

Signed-off-by: smajumdar <titu1994@gmail.com>

* remove asr transformer dependency on nlp

Signed-off-by: stevehuang52 <heh@nvidia.com>

* clean up

Signed-off-by: stevehuang52 <heh@nvidia.com>

* copy token_classifier from nlp to asr

Signed-off-by: stevehuang52 <heh@nvidia.com>

* Address comments

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add typing to beam decoding

Signed-off-by: smajumdar <titu1994@gmail.com>

* Make prompt format configurable

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* drop asr dependency on nlp

Signed-off-by: stevehuang52 <heh@nvidia.com>

---------

Signed-off-by: smajumdar <titu1994@gmail.com>
Signed-off-by: stevehuang52 <heh@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: stevehuang52 <heh@nvidia.com>

* fix transcribe, update asr evaluator

Signed-off-by: stevehuang52 <heh@nvidia.com>

* Extend the docs for the canary prompt_fn

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* Incorporate changes from Nithin's code review

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* training bug fix and adding launch script for speech_multitask (#8270)

* bug fix and adding launch script for speech_multitask

Signed-off-by: Krishna Puvvada <kpuvvada@nvidia.com>

* update launch script example in speech_to_text_aed.py

Signed-off-by: Krishna Puvvada <kpuvvada@nvidia.com>

---------

Signed-off-by: Krishna Puvvada <kpuvvada@nvidia.com>
Co-authored-by: Krishna Puvvada <kpuvvada@nvidia.com>

* Fix: drop_last must be true in validation/test otherwise the training will hang

Signed-off-by: Piotr ≈ªelasko <pzelasko@nvidia.com>

* revert to current transcribe API

Signed-off-by: stevehuang52 <heh@nvidia.com>

* revert changes to NLP, update docs

Signed-off-by: stevehuang52 <heh@nvidia.com>

* update eval utils

Signed-off-by: stevehuang52 <heh@nvidia.com>

* update docs

Signed-off-by: stevehuang52 <heh@nvidia.com>

* Remove DALI; rename compute_audio_loss to compute_loss

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* set default use_model_transcribe=False

Signed-off-by: stevehuang52 <heh@nvidia.com>

* change os.path.dirname to pathlib

Signed-off-by: stevehuang52 <heh@nvidia.com>

* [canary] Test for CanaryTokenizer + refactoring (#8285)

* Test for CanaryTokenizer

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* Attempt at refactor...

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

---------

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* Update config for AED models (#8294)

Signed-off-by: smajumdar <titu1994@gmail.com>

* set default calculate_wer=False in transcribe_speech.py

Signed-off-by: stevehuang52 <heh@nvidia.com>

* Attention encoder-decoder models for multiple speech-to-text tasks

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* Apply suggestions from code review, part 1

Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* Apply suggestions from code review, part 2

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* Document compute_loss

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* update transcribe_speech.py

Signed-off-by: stevehuang52 <heh@nvidia.com>

* add docstring

Signed-off-by: stevehuang52 <heh@nvidia.com>

* Attention encoder-decoder models for multiple speech-to-text tasks

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

---------

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>
Signed-off-by: stevehuang52 <heh@nvidia.com>
Signed-off-by: smajumdar <titu1994@gmail.com>
Signed-off-by: Krishna Puvvada <kpuvvada@nvidia.com>
Signed-off-by: Piotr ≈ªelasko <pzelasko@nvidia.com>
Co-authored-by: stevehuang52 <heh@nvidia.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Krishna Puvvada <93558329+krishnacpuvvada@users.noreply.github.com>
Co-authored-by: Krishna Puvvada <kpuvvada@nvidia.com>
Co-authored-by: He Huang (Steve) <105218074+stevehuang52@users.noreply.github.com>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2024-01-18T17:50:42Z,"Fix learning rate schedule in Megatron models when `max_steps` is not set (#7518)

* Fix learning rate schedule in Megatron models when `max_steps` is not set

Signed-off-by: Olivier Delalleau <507137+odelalleau@users.noreply.github.com>

* Rename `_get_optim_config_copy()` -> `_optim_config_copy()`

Signed-off-by: Olivier Delalleau <507137+odelalleau@users.noreply.github.com>

* Relax some assumptions to avoid crashing in `setup_optimization()`

Signed-off-by: Olivier Delalleau <507137+odelalleau@users.noreply.github.com>

* Remove a useless assert, and add a more relevant one

`trainer.max_steps` can never be `None` since it is always an integer
(with -1 being used for ""not set""), and PTL would have crashed earlier
if someone had tried to use `None`.

Signed-off-by: Olivier Delalleau <507137+odelalleau@users.noreply.github.com>

* Do not instantiate scheduler when we cannot compute `max_steps`

Signed-off-by: Olivier Delalleau <507137+odelalleau@users.noreply.github.com>

* Raise exception when scheduler can't be instantiated

Signed-off-by: Olivier Delalleau <507137+odelalleau@users.noreply.github.com>

---------

Signed-off-by: Olivier Delalleau <507137+odelalleau@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2023-12-18T20:18:25Z,"SFT patch: (1) enable sequence parallelism and (2) enable profile (#7963)

* SFT profile start and end step fix

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* Removed sequence parallelism assertion check

Signed-off-by: Selvaraj Anandaraj <selvaraja@login-eos01.eos.clusters.nvidia.com>

---------

Signed-off-by: Sangkug Lym <slym@nvidia.com>
Signed-off-by: Selvaraj Anandaraj <selvaraja@login-eos01.eos.clusters.nvidia.com>
Co-authored-by: Selvaraj Anandaraj <selvaraja@login-eos01.eos.clusters.nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2023-11-03T22:46:25Z,"Multimodal merge (#7728)

* ControlNet TRT export

* Final MR before release

* SD2 update

* Fixed export issue

* Fix for instruct p2p and reformat

* Fix SD export issue

* Add nemo clip export for DB

* Fix ins pix2pix

* fix sd2 config

* [Mingyuan Ma] BF16 and SD conversion script

* [Imagen] NHWC Feature

* Fix .nemo loading issue for NeMo CLIP in SD

* NeMo r1.20.0 Multimodal Merge

* fix the inductor issue in inference

* Fix inductor loading .nemo issue

* Add Neva Model Support

* Imagen Optimizations

* Neva inference code

* NeMo TOT 1.21 to Internal/main

* Update neva_inference.yaml

* REBASING  for latest code changes

* Update internal/main to main tot

* Parallel DDIM implementation

* 1. Fixing indentation bug. (#7352)

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* NeMo MCore llama2 support + MCore PEFT adapters (#7299)

* start adding gpt from megatron core path

Signed-off-by: ericharper <complex451@gmail.com>

* set model parallel config

Signed-off-by: ericharper <complex451@gmail.com>

* use model parallel config object

Signed-off-by: ericharper <complex451@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update args

Signed-off-by: ericharper <complex451@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* set vp size to none if it is 1

Signed-off-by: ericharper <complex451@gmail.com>

* set vp size to none if it is 1

Signed-off-by: ericharper <complex451@gmail.com>

* add TransformerConfig

Signed-off-by: ericharper <complex451@gmail.com>

* start updating to TransformerConfig

Signed-off-by: ericharper <complex451@gmail.com>

* add todo

Signed-off-by: ericharper <complex451@gmail.com>

* revert to model parallel config

Signed-off-by: ericharper <complex451@gmail.com>

* add hidden_size to model_parallel_config

Signed-off-by: ericharper <complex451@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* remove imports

Signed-off-by: ericharper <complex451@gmail.com>

* revert

Signed-off-by: ericharper <complex451@gmail.com>

* remove import

Signed-off-by: ericharper <complex451@gmail.com>

* small clean up

Signed-off-by: ericharper <complex451@gmail.com>

* update hidden size in peft base model, add mcore commit to jenkins

Signed-off-by: ericharper <complex451@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update module args

Signed-off-by: ericharper <complex451@gmail.com>

* add config obj to flash attention tests

Signed-off-by: ericharper <complex451@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* remove sequence parallel arg

Signed-off-by: ericharper <complex451@gmail.com>

* update args

Signed-off-by: ericharper <complex451@gmail.com>

* add config to self

Signed-off-by: ericharper <complex451@gmail.com>

* update args

Signed-off-by: ericharper <complex451@gmail.com>

* update args

Signed-off-by: ericharper <complex451@gmail.com>

* update args

Signed-off-by: ericharper <complex451@gmail.com>

* add config to test

Signed-off-by: ericharper <complex451@gmail.com>

* get hidden_size from config

Signed-off-by: ericharper <complex451@gmail.com>

* add try except

Signed-off-by: ericharper <complex451@gmail.com>

* use default

Signed-off-by: ericharper <complex451@gmail.com>

* update config with hidden size

Signed-off-by: ericharper <complex451@gmail.com>

* remove arg

Signed-off-by: ericharper <complex451@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* comment out jenkins test

Signed-off-by: ericharper <complex451@gmail.com>

* revert import

Signed-off-by: ericharper <complex451@gmail.com>

* build transformer config

Signed-off-by: ericharper <complex451@gmail.com>

* add model to provider func

Signed-off-by: ericharper <complex451@gmail.com>

* update forward and float16 wrapper

Signed-off-by: ericharper <complex451@gmail.com>

* instantiate model parallel config after init model parallel

Signed-off-by: ericharper <complex451@gmail.com>

* set virtual rank

Signed-off-by: ericharper <complex451@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add GQA config to megatron gpt model (#7096)

* Add GQA config in gpt config file

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* Verify mcore is enabled when using GQA

Signed-off-by: jasonwan <jasonwan@nvidia.com>

---------

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* revert

Signed-off-by: ericharper <complex451@gmail.com>

* mcore llama2 ckpt conversion & small fix

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* Add inference & sft config by Hongbin

Co-authored-by: Hongbin Liu <hongbinl@nvidia.com>

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* fix config

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* add inference param. update TP/PP script to support mcore gpt

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* p-tuning

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* modify ckpt conversion script (adding model cast)

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* ckpt conversion use relative path for config

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* start adding gpt from megatron core path

Signed-off-by: ericharper <complex451@gmail.com>

* set model parallel config

Signed-off-by: ericharper <complex451@gmail.com>

* use model parallel config object

Signed-off-by: ericharper <complex451@gmail.com>

* update args

Signed-off-by: ericharper <complex451@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* set vp size to none if it is 1

Signed-off-by: ericharper <complex451@gmail.com>

* set vp size to none if it is 1

Signed-off-by: ericharper <complex451@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add TransformerConfig

Signed-off-by: ericharper <complex451@gmail.com>

* start updating to TransformerConfig

Signed-off-by: ericharper <complex451@gmail.com>

* add todo

Signed-off-by: ericharper <complex451@gmail.com>

* revert to model parallel config

Signed-off-by: ericharper <complex451@gmail.com>

* add hidden_size to model_parallel_config

Signed-off-by: ericharper <complex451@gmail.com>

* remove imports

Signed-off-by: ericharper <complex451@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* remove import

Signed-off-by: ericharper <complex451@gmail.com>

* small clean up

Signed-off-by: ericharper <complex451@gmail.com>

* update hidden size in peft base model, add mcore commit to jenkins

Signed-off-by: ericharper <complex451@gmail.com>

* update module args

Signed-off-by: ericharper <complex451@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add config obj to flash attention tests

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* remove sequence parallel arg

Signed-off-by: ericharper <complex451@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update args

Signed-off-by: ericharper <complex451@gmail.com>

* add config to self

Signed-off-by: ericharper <complex451@gmail.com>

* update args

Signed-off-by: ericharper <complex451@gmail.com>

* update args

Signed-off-by: ericharper <complex451@gmail.com>

* update args

Signed-off-by: ericharper <complex451@gmail.com>

* add config to test

Signed-off-by: ericharper <complex451@gmail.com>

* get hidden_size from config

Signed-off-by: ericharper <complex451@gmail.com>

* add try except

Signed-off-by: ericharper <complex451@gmail.com>

* use default

Signed-off-by: ericharper <complex451@gmail.com>

* update config with hidden size

Signed-off-by: ericharper <complex451@gmail.com>

* remove arg

Signed-off-by: ericharper <complex451@gmail.com>

* comment out jenkins test

Signed-off-by: ericharper <complex451@gmail.com>

* revert import

Signed-off-by: ericharper <complex451@gmail.com>

* remove optimizer_idx

Signed-off-by: eharper <eharper@nvidia.com>

* prefetch num microbatches

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* start adding gpt from megatron core path

Signed-off-by: ericharper <complex451@gmail.com>

* set model parallel config

Signed-off-by: ericharper <complex451@gmail.com>

* use model parallel config object

Signed-off-by: ericharper <complex451@gmail.com>

* update args

Signed-off-by: ericharper <complex451@gmail.com>

* fix for p-tuning sequence parallel

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* support SFT/distOpt mcore (#7207)

* add inference param. update TP/PP script to support mcore gpt

* p-tuning

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* change layer names for SFT

Signed-off-by: Hongbin Liu <hongbinl@nvidia.com>

* fix bug in SFT

Signed-off-by: Hongbin Liu <hongbinl@nvidia.com>

---------

Signed-off-by: jasonwan <jasonwan@nvidia.com>
Signed-off-by: Hongbin Liu <hongbinl@nvidia.com>
Co-authored-by: Hongbin Liu <hongbinl@nvidia.com>
Co-authored-by: jasonwan <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* start updating to TransformerConfig

Signed-off-by: ericharper <complex451@gmail.com>

* revert to model parallel config

Signed-off-by: ericharper <complex451@gmail.com>

* add hidden_size to model_parallel_config

Signed-off-by: ericharper <complex451@gmail.com>

* remove imports

Signed-off-by: ericharper <complex451@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update module args

Signed-off-by: ericharper <complex451@gmail.com>

* add config to self

Signed-off-by: ericharper <complex451@gmail.com>

* build transformer config

Signed-off-by: ericharper <complex451@gmail.com>

* add model to provider func

Signed-off-by: ericharper <complex451@gmail.com>

* update forward and float16 wrapper

Signed-off-by: ericharper <complex451@gmail.com>

* instantiate model parallel config after init model parallel

Signed-off-by: ericharper <complex451@gmail.com>

* set virtual rank

Signed-off-by: ericharper <complex451@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add GQA config to megatron gpt model (#7096)

* Add GQA config in gpt config file

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* Verify mcore is enabled when using GQA

Signed-off-by: jasonwan <jasonwan@nvidia.com>

---------

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* revert

Signed-off-by: ericharper <complex451@gmail.com>

* remove import

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* rollback model cast for p-tuning

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* update for dist adam

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* use get_gpt_module_list

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update ckpt conversion script

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* ptl2.0 patch for llama config

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* add plugins to trainer in scripts

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* fix activation checkpointing mcore

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* fix variable names

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* overwrite normalization type for mcore/te

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* Update megatron_llama_sft.yaml

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* add PEFT adapter support for mcore gpt path (#7276)

* implementation for mcore adapter/mxins

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* small fix for lora and ptuning

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* support layerwise peft

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* support multiple target layers

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* support lora GQA

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* support amp O2

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* revert & more O2 fix

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* lora inject to attention

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* support lora weight tying

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* add copyright header

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* rollback ptuning name change. full string match mcore target

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* remove comment

Signed-off-by: jasonwan <jasonwan@nvidia.com>

---------

Signed-off-by: jasonwan <jasonwan@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* clean up config

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* Sync llama branch (#7297)

* add inference param. update TP/PP script to support mcore gpt

* p-tuning

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* change layer names for SFT

Signed-off-by: Hongbin Liu <hongbinl@nvidia.com>

* fix bug in SFT

Signed-off-by: Hongbin Liu <hongbinl@nvidia.com>

* fix bug: cpu initialization is not really enabled

Signed-off-by: Hongbin Liu <hongbinl@nvidia.com>

* add use_cpu_initialization to TransformerConfig

Signed-off-by: Hongbin Liu <hongbinl@nvidia.com>

* fix bug: wrong config path when using relative cjpt path

Signed-off-by: Hongbin Liu <hongbinl@nvidia.com>

* revert mcore config change

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

---------

Signed-off-by: jasonwan <jasonwan@nvidia.com>
Signed-off-by: Hongbin Liu <hongbinl@nvidia.com>
Signed-off-by: Jason Wang <jasonwan@nvidia.com>
Co-authored-by: Hongbin Liu <hongbinl@nvidia.com>

* clean up ckpt conversion script

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* rollback git merge errors

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* update mcore, add check for mcore+te

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* formatting

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* make sft test dataset optional. fix indentation in config

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* one more fix for optional test set

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* support merging lora weights in mcore

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* update mcore for cpu init

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update ckpt conversion for code llama

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add seq_len_interpolation_factor support for long-context llama ckpts (#7312)

* add inference param. update TP/PP script to support mcore gpt

* p-tuning

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* add seq_len_interpolation_factor

Signed-off-by: Hongbin Liu <hongbinl@nvidia.com>

---------

Signed-off-by: jasonwan <jasonwan@nvidia.com>
Signed-off-by: Hongbin Liu <hongbinl@nvidia.com>
Co-authored-by: jasonwan <jasonwan@nvidia.com>
Co-authored-by: Hongbin Liu <hongbinl@nvidia.com>

* fix old ptuning model, update mcore to support seq_len_interpolation_factor

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* support fused layernorm linear, fix ptuning O2

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* drop loss mask for mcore for now

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* disable dist ckpt in peft

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix loading non dist ckpt

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* add ckpt conversion to CI

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* update CI

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* mcore_mixin docstring

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* minor change in mcore peft error message

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* fix amp o2 in lora weight tying

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* correct mcore fp8 config

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* add TE installation

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* support mcore adapter tuning

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* comment out new CI test. rollback docker image

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* ignore FA tests, try new CI on 23.08

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* mark new CI as L2, put to beginning to test

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* minor fix for prompt learning

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* rollback to 23.06. comment out CI

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* minor fix ckpt conversion script

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* minor rollback gpt model change

Signed-off-by: jasonwan <jasonwan@nvidia.com>

---------

Signed-off-by: ericharper <complex451@gmail.com>
Signed-off-by: jasonwan <jasonwan@nvidia.com>
Signed-off-by: eharper <eharper@nvidia.com>
Signed-off-by: Hongbin Liu <hongbinl@nvidia.com>
Signed-off-by: Jason Wang <jasonwan@nvidia.com>
Co-authored-by: ericharper <complex451@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: eharper <eharper@nvidia.com>
Co-authored-by: Hongbin Liu <hongbinl@nvidia.com>
Co-authored-by: Kelvin Liu <lhb8125@users.noreply.github.com>

* Hiddens modules documentation (#7303)

* 1. Changed hiddens transformations module from `transformations` to `hiddens`.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* 1. Debugging. Signed-off-by: Micha Livne <mlivne@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* 1. Finished doc.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging. Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging. Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging. Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging. Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging. Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging. Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging. Signed-off-by: Micha Livne <mlivne@nvidia.com>

---------

Signed-off-by: Micha Livne <mlivne@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* Support for flash attention 2.0 (#7063)

* Add flash attn 2

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add FA2 feature

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Remove debugging

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>
Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Signed-off-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>
Co-authored-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* lora merge fix for O2 names (#7325)

* wip

Signed-off-by: arendu <adithyare@nvidia.com>

* adjust key names based on O2

Signed-off-by: arendu <adithyare@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update

Signed-off-by: arendu <adithyare@nvidia.com>

* minor

Signed-off-by: arendu <adithyare@nvidia.com>

---------

Signed-off-by: arendu <adithyare@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* multiple fields can form a context (#7147)

* list of context fields and flexible prompt template

Signed-off-by: arendu <adithya.r@gmail.com>

* list of fields for context

Signed-off-by: arendu <adithya.r@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Add multiple truncation fields and middle truncation

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Compatible to old ckpt

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix tokenize detokenize issue

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Remove detokenization, add truncation augmentation

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Resolve comments

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Remove unused import

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* revert eos

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Add tokenizer space_sensitive attribute

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix error

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Fix erorr and use re

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Change assert logic

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Follow adi suggestion

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Remove merge function

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add example and comment

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Remove context_key and add comment

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Remove random truncation

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix template none

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

---------

Signed-off-by: arendu <adithya.r@gmail.com>
Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Signed-off-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>

* Load buffers in checkpoint (#7357)

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* Add migration guide for lightning 2.0 upgrade (#7360)

* Add lightning 2.0 migration guide in NeMo docs

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add remaining guide for lightning 2.0 upgrade

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Remove line spill over and continue in next line

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add missing dataloader_iter in the guide

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Fix minor typo

Signed-off-by: Abhishree <abhishreetm@gmail.com>

---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* adding bias_dropout_add_fusion option for BERT (#7332)

Signed-off-by: Alexander Jipa <azzhipa@amazon.com>
Co-authored-by: Alexander Jipa <azzhipa@amazon.com>

* [TTS] Change audio codec token type to TokenIndex (#7356)

Signed-off-by: Ryan <rlangman@nvidia.com>

* enable selective unfreeze (#7326)

* wip

Signed-off-by: arendu <adithyare@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* wip

Signed-off-by: arendu <adithyare@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* avoid PTL method conflicts

Signed-off-by: arendu <adithyare@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update

Signed-off-by: arendu <adithyare@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update

Signed-off-by: arendu <adithyare@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: arendu <adithyare@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Fix typos (#7361)

* fix typos

Signed-off-by: omahs <73983677+omahs@users.noreply.github.com>

* fix typo

Signed-off-by: omahs <73983677+omahs@users.noreply.github.com>

* fix typos

Signed-off-by: omahs <73983677+omahs@users.noreply.github.com>

* fix typos

Signed-off-by: omahs <73983677+omahs@users.noreply.github.com>

* fix typo

Signed-off-by: omahs <73983677+omahs@users.noreply.github.com>

* fix typos

Signed-off-by: omahs <73983677+omahs@users.noreply.github.com>

* fix typo

Signed-off-by: omahs <73983677+omahs@users.noreply.github.com>

* fix typo

Signed-off-by: omahs <73983677+omahs@users.noreply.github.com>

* fix typo

Signed-off-by: omahs <73983677+omahs@users.noreply.github.com>

---------

Signed-off-by: omahs <73983677+omahs@users.noreply.github.com>

* pin numba=0.57.1 to fix reinstall.sh error (#7366)

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* Update new conversion script for converting safetensors.

* Upgrade pytorch container to 23.08 (#7353)

* upgrade pytorch container

Signed-off-by: eharper <eharper@nvidia.com>

* use mcore

Signed-off-by: eharper <eharper@nvidia.com>

* revert test change

Signed-off-by: eharper <eharper@nvidia.com>

* pleasefixme

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* check for ampere

Signed-off-by: eharper <eharper@nvidia.com>

* comment test temporarily

Signed-off-by: eharper <eharper@nvidia.com>

---------

Signed-off-by: eharper <eharper@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* enable fp32 optimizer for output_layer in mcore (#7355)

Signed-off-by: lhb8125 <lhb8125@gmail.com>

* revert comment (#7368)

Signed-off-by: eharper <eharper@nvidia.com>

* Update to core 23.08 branch ToT (#7371)

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* upper bounding ptl (#7370)

Signed-off-by: eharper <eharper@nvidia.com>

* fix pipeline parallel inference (#7367)

* fix pp inference

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: jasonwan <jasonwan@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* fix for peft tied weights (#7372)

Signed-off-by: arendu <adithyare@nvidia.com>

* fixed trainer.strategy=auto from None. (#7369)

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* add O2 option in gpt eval (#7358)

* add O2 option in eval

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add doc for O2 config

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* add to llama inference config

Signed-off-by: jasonwan <jasonwan@nvidia.com>

---------

Signed-off-by: jasonwan <jasonwan@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* Move model precision copy (#7336)

* move cfg precision set to megatron base model

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* remove copy from other models

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* modify attribute not arg

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* fix gpt model test for ptl 2.0

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* rename function and add docstring

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* replace precision to dtype conditionals with func call

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* unnecessary function and cfg reset

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* set default value

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* fix precision lookup in a few more places

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* rename mapping function

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* ununsed import

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* save torch datatype to model

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* set weights precision wrt amp o2

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* Revert ""set weights precision wrt amp o2""

This reverts commit 313a4bfe5eb69d771a6d2433898c0685836aef5c.

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* revert half precision at inference attempt

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* move autocast dtype to base model

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* move params dtype to base model, enable fp16 O2 inf

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* unused imports

Signed-off-by: Maanu Grover <maanug@nvidia.com>

---------

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* Fix PEFT checkpoint loading (#7388)

* Fix PEFT checkpoint loading

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Jason Wang <jasonwan@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Use distributed optimizer support for multiple dtypes (#7359)

* Update distopt wrapper with multiple dtype support

Remove manual handling of separate FP32 optimizer.

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Use distopt support for contiguous buffers with multiple dtypes

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Fix typo

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Separate distopt buckets for first GPT layer and non-overlapped params

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Add distopt logic for int dtypes

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Update Apex commit

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Remove unused variables

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Update Apex commit in README and Jenkensfile

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Debug Dockerfile and Jenkinsfile

Signed-off-by: Tim Moon <tmoon@nvidia.com>

---------

Signed-off-by: Tim Moon <tmoon@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* minor fix for llama ckpt conversion script (#7387)

* minor fix for llama ckpt conversion script

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* Update Jenkinsfile

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* remove fast_swiglu configuration

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

---------

Signed-off-by: Jason Wang <jasonwan@nvidia.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* Fix wrong calling of librosa.get_duration() in notebook (#7376)

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>

* [PATCH] PEFT import mcore (#7393)

* [PATCH] PEFT import mcore

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Jason Wang <jasonwan@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* [TTS] Added a callback for logging initial data (#7384)

Signed-off-by: Ante JukicÃÅ <ajukic@nvidia.com>

* Update Core Commit (#7402)

* Update Core Commit

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* update commit

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

---------

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* Use cfg attribute in bert (#7394)

* use cfg attribute instead of arg

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* use torch_dtype in place of cfg.precision

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* move precision copy before super constructor

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* use trainer arg

Signed-off-by: Maanu Grover <maanug@nvidia.com>

---------

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* Add support for bias conversion in Swiglu models (#7386)

* Add support for bias conversion in Swiglu models

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add support for auto extracting tokenizer model

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add support for auto extracting tokenizer model

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix issue with missing tokenizer

Signed-off-by: smajumdar <titu1994@gmail.com>

* Refactor

Signed-off-by: smajumdar <titu1994@gmail.com>

* Refactor

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Update save_to and restore_from for dist checkpointing (#7343)

* add dist ckpt to save to, in progress

Signed-off-by: eharper <eharper@nvidia.com>

* move dist ckpt

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* clean up

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update restore from, need to figure out how to initialize distributed

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* launch distrib if needed when restoring dist ckpt

Signed-off-by: eharper <eharper@nvidia.com>

* when using mcore we can change tp pp on the fly

Signed-off-by: eharper <eharper@nvidia.com>

* add load_from_checkpoint support for dist ckpt

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update llama convert script to save dist .nemo

Signed-off-by: eharper <eharper@nvidia.com>

* fix load dist ckpt

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* setup TE TP groups if needed

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* setup te tp groups if needed

Signed-off-by: eharper <eharper@nvidia.com>

* remove import

Signed-off-by: eharper <eharper@nvidia.com>

---------

Signed-off-by: eharper <eharper@nvidia.com>
Signed-off-by: jasonwan <jasonwan@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: jasonwan <jasonwan@nvidia.com>

* fix forward for with mcore=false (#7403)

Signed-off-by: Jimmy Zhang <jiemingz@nvidia.com>
Co-authored-by: Jimmy Zhang <jiemingz@nvidia.com>

* Fix logging to remove 's/it' from progress bar in Megatron models and add train_step_timing (#7374)

* Add CustomProgressBar class to exp_manager and trainer callbacks

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix the progress bar to reflect total microbatch cnt

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Modify CustomProgressBar class

1) Modify CustomProgressBar class to update progress bar per global_step instead of per microbatch
2) Add the callback to other megatron training/finetuning files that are not using MegatronTrainerBuilder

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add CustomProgressBar callback to tuning files

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Set Activation Checkpointing Defaults (#7404)

* Set Activation Checkpointing Defaults

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* check for None

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* make loss mask default to false (#7407)

Signed-off-by: eharper <eharper@nvidia.com>

* Add dummy userbuffer config files (#7408)

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* add missing ubconf files (#7412)

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* New tutorial on Speech Data Explorer (#7405)

* Added Google Colab based tutorial on Speech Data Explorer 

Signed-off-by: George Zelenfroynd <gzelenfroind@nvidia.com>

* Update ptl training ckpt conversion script to work with dist ckpt (#7416)

* update ptl convert script

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* don't break legacy

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: eharper <eharper@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Allow disabling sanity checking when num_sanity_val_steps=0 (#7413)

* Allow disabling sanity checking when num_sanity_val_steps=0

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Update num_sanity_val_steps to be a multiple of num_microbatches

Signed-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Signed-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Add comprehensive error messages (#7261)

Signed-off-by: Anton Peganov <apeganov@nvidia.com>

* check NEMO_PATH (#7418)

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* layer selection for ia3 (#7417)

* layer selection for ia3

Signed-off-by: arendu <adithyare@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: arendu <adithyare@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Fix missing pip package 'einops' (#7397)

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Fix failure of pyaudio in Google Colab (#7396)

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Update README.md: output_path --> output_manifest_filepath (#7442)

Signed-off-by: Samuele Cornell <cornellsamuele@gmail.com>

* Updating FlashAttention API to match FlashAttentionV2

* Multiple fixes for mm

* Fix CI inductor issue and update to torch compile

* Remove suppress error

* Fix when conversion config uses fp16 and it complains about precision plugin

* Fixing FAv2 API usage

* Initial release of content filtering model

* Added synthetic dataloader for precached and online mode

* Mingyuanm/dreambooth opt

* Add llama2 support in neva training

* Fix sampler length

* Fix all precision issues in nemo multimodal

* Add rope dynamic linear scaling (#7437)

* Add dynamic linear scaling

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

---------

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>

* Fix None dataloader issue in PTL2.0 (#7455)

* Fix None dataloader issue in PTL2.0

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* updating values of self._validation_dl and self._test_dl as well

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* updating values of self._validation_dl and self._test_dl as well

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* [ASR] Confidence measure -> method renames (#7434)

* measure -> method

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Add steps for document of getting dataset 'SF Bilingual Speech' (#7378)

* Add steps for document of getting dataset 'SF Bilingual Speech'

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Update datasets.rst

added a link from a tutorial demonstrating detailed data prep steps.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* RNN-T confidence and alignment bugfix (#7381)

* new frame_confidence and alignments lists are now always created after the while loop

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

* tests added

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

---------

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

* Fix resume from checkpoint in exp_manager (#7424) (#7426)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* Fix checking of cuda/cpu device for inputs of Decoder (#7444)

* Fix checking of cuda/cpu device for inputs of Decoder

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Update tacotron2.py

Signed-off-by: Jason <jasoli@nvidia.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Jason <jasoli@nvidia.com>
Co-authored-by: Jason <jasoli@nvidia.com>

* Fix failure of ljspeech's get_data.py (#7430)

* Fix failure of ljspeech's get_data.py

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* [TTS] Fix audio codec type checks (#7373)

* [TTS] Fix audio codec type checks

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Fix audio codec tests

Signed-off-by: Ryan <rlangman@nvidia.com>

---------

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Add dataset to path of logged artifacts (#7462)

* [TTS] Add dataset to path of logged artifacts

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Revert axis name back to Audio Frames

Signed-off-by: Ryan <rlangman@nvidia.com>

---------

Signed-off-by: Ryan <rlangman@nvidia.com>

* Fix sft dataset truncation (#7464)

* Add fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

---------

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Automatic Lip Reading Recognition (ALR) - ASR/CV (Visual ASR) (#7330)

* striding_conv1d_k5 and dw_striding_conv1d_k5 subsampling

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* transpose conv1d inputs

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* Update subsampling.py

change striding_conv1d_k5 to striding_conv1d

Signed-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>

* cv branch

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* video manifest

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* add collection classes

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add test_step_outputs

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* correct manifest bug when having only audio or only videos

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* correct manifest bug when having only audio or only videos

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* clean references

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* freeze unfreeze transcribe cv models

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* correct manifest get_full_path bug

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* update for PR

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* guard torchvision

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Update nemo/collections/cv/data/video_to_text_dataset.py

Co-authored-by: Igor Gitman <igor.a.gitman@gmail.com>
Signed-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>

* _video_speech_collate_fn in cv/data/video_to_text.py

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* add self.out = None to asr subsampling

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* Update nemo/collections/cv/data/video_to_text_dataset.py

Co-authored-by: Igor Gitman <igor.a.gitman@gmail.com>
Signed-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>

* cv -> multimodal/speech_cv branch

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: mburchi <maxime.burchi@gmail.com>
Signed-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Igor Gitman <igor.a.gitman@gmail.com>

* HF StarCoder to NeMo conversion script (#7421)

* Script to convert HF StarCoder checkpoint to NeMo

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* StarCoder conversion test

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Fix test

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Catch up with save_to changes

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Don't abbreviate args for clarity

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Configurable precision: BF16 vs FP32

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* fix bug when loading dist ckpt in peft (#7452)

Signed-off-by: Hongbin Liu <hongbinl@nvidia.com>
Co-authored-by: Hongbin Liu <hongbinl@nvidia.com>

* Fix adding positional embeddings in-place in transformer module (#7440)

Signed-off-by: Tamerlan Tabolov <tktabolov@gmail.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>

* Fix (#7478)

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* add sleep (#7498) (#7499)

* add sleep



* add sleep onto config instead



* add comment



---------

Signed-off-by: Gerald Shen <geshen@nvidia.com>
Co-authored-by: Gerald Shen <119401249+gshennvm@users.noreply.github.com>

* Fix exp manager check for sleep (#7503) (#7504)

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>

* bugfix: trainer.accelerator=auto from None. (#7492) (#7493)

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>

* [doc] fix broken link (#7481)

Signed-off-by: Stas Bekman <stas00@users.noreply.github.com>

* [TTS] Read audio as int32 to avoid flac read errors (#7477)

* [TTS] Read audio as int32 to avoid flac read errors

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Add comment about read failures

Signed-off-by: Ryan <rlangman@nvidia.com>

---------

Signed-off-by: Ryan <rlangman@nvidia.com>

* Add dataset 'AISHELL-3' from OpenSLR for training mandarin TTS (#7409)

* Add dataset 'AISHELL-3' from OpenSLR for training mandarin TTS
* Train 'AISHELL-3' dataset with multi-speakers

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Update get_data.py

update copyright header

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* Update get_data.py

added a disclaimer

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add new configuration file for AISHELL3 with multispeaker of fastpitch

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* dllogger - log on rank 0 only (#7513)

Signed-off-by: Stas Bekman <stas00@users.noreply.github.com>

* Fix TTS FastPitch tutorial (#7494) (#7516)

* Fix

---------

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>

* Fix get_dist() tensor dimension (#7506) (#7515)

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>
Co-authored-by: Jocelyn <jocelynh@nvidia.com>

* bugfix: specify trainer.strategy=auto when devices=1 (#7509) (#7512)

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>

* fix (#7511)

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* [TTS] Fix FastPitch data prep tutorial (#7524)

Signed-off-by: Ryan <rlangman@nvidia.com>

* add italian tokenization (#7486)

* add italian tokenization

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add more ipa lexicon it

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix error deletion

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* add test

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Replace None strategy with auto in tutorial notebooks (#7521) (#7527)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

* unpin setuptools (#7534) (#7535)

Signed-off-by: fayejf <36722593+fayejf@users.noreply.github.com>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>

* remove auto generated examples (#7510)

* explicitly remove autogenerated examples for data parallel evaluation

Signed-off-by: arendu <adithyare@nvidia.com>

* mark autogenrated and remove it for test

Signed-off-by: arendu <adithyare@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: arendu <adithyare@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Add the `strategy` argument to `MegatronGPTModel.generate()` (#7264)

It is passed as an explicit argument rather than through
`**strategy_args` so as to ensure someone cannot accidentally pass other
arguments that would end up being ignored.

It is a keyword-only argument to ensure that if in the future we want to
update the signature to `**strategy_args`, we can do it without breaking
code.

Signed-off-by: Olivier Delalleau <507137+odelalleau@users.noreply.github.com>

* Fix PTL2.0 related ASR bugs in r1.21.0: Val metrics logging, None dataloader issue (#7531) (#7533)

* fix none dataloader issue ptl2



* ptl2.0 logging fixes for rnnt_models



---------

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>
Co-authored-by: Kunal Dhawan <kunaldhawan97@gmail.com>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>

* gpus -> devices (#7542) (#7545)

Signed-off-by: Nithin Rao Koluguri <nithinraok>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>

* Update FFMPEG version to fix issue with torchaudio (#7551) (#7553)

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>

* PEFT GPT & T5 Refactor (#7308)

* initial implementation of add_adapters API

* correct type hint

* Add config in add_adapters for save and load (@author bobchen)

* Remove AdapterConfig to avoid import error

* Add AdaterConfig back and move adaptermixin to sft model

* Add NLPSaveRestoreConnector as default in NLPModel.restore_from

* Add restore_from_nemo_with_adapter and test script

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* rename t5 file and classes to be consistent with GPT

* add t5 sft dataset

* add support for single-file format with T5SFTDataset

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Various small changes to make T5 SFT work like GPT SFT

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add adapter evaluation test script

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add MultiAdaterConfig for ia3 and fix builder issue

* Make ptuning for T5SFTModel work using mixin

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add IA3_Adapter for AdapterName

* Add adapter name for ptuning and attention adapter

* Make test script GPT/T5 agnostic

* Add layer selection feature

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Integrate adapter name and config

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update gpt peft tuning script to new API

* add t5 peft tuning script with new API

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix IA3 layer selection issue

* Override state_dict on SFT model instead of mixin

* Add load adapter by adapter config

* move peft config map away from example script

* auto get config from nemo adapter

* Move PEFTConfig to new file

* fix ckpt save/load for t5

* name change: add_adapters -> add_adapter

* variable name change

* update t5 script

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix t5 issues

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add weight tying

* update gpt tuning script

* PEFT-API proposal

* Fix according to comments

* update tuning scripts

* move merge_cfg_with to mixin class since it applies to both gpt and t5 and requires the model class for restore

* Add mcore_gpt support for NLPAdapterMixin

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix typo

* variable name change to distinguish ""peft"" and ""adapter""

* override `load_adapters` to support `add_adapter` name change

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update tuning and eval script for adapter save/load

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add Ptuning on first stage only

* add lora tutorial for review

* Fix layer selection for mcore

* add landing page

* fix resume training

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add mcore condition in sharded_state_dict to make sft work

* Update lora_tutorial.md

First edit of this file for PEFT documentation for NeMO

Signed-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>

* rename Adapter to AttentionAdapter to avoid confusion in doc

* Change load_adapters to load .nemo

* add quick start guide

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add load_adapters with .ckpt

* Remove setup_complete changes in load_adapters

* update landing page

* remove typo

* Updated quick_start.md per Chen Cui

Signed-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>

* Add inference config merger and tutorial

* Add doc string for NLPAdapterModelMixin and deprecated warning on MegatronGPTPEFTModel

* add suppor‚Ä¶"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2023-09-29T18:12:43Z,"fix validation_step_outputs initialization for multi-dataloader (#7546) (#7572)

* added correct validation_step_outputs initialization for mutli-dataloader



* changed kernel for display



* Update logic for validation and test step outputs



* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* revert multidataloader changes in multilang ASR notebook



---------

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>
Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: Kunal Dhawan <kunaldhawan97@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2023-09-19T00:48:37Z,"Fix None dataloader issue in PTL2.0 (#7455)

* Fix None dataloader issue in PTL2.0

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* updating values of self._validation_dl and self._test_dl as well

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* updating values of self._validation_dl and self._test_dl as well

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2023-08-05T22:40:17Z,"Upgrade to pytorch lightning 2.0 (#6433)

* Upgrade pytorch lightning version in requirements

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Initial fixes for PTL2.0

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add further fixes to support lightning 2.0

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add replacements for replace_sampler_ddp, resume_from_checkpoint_fit_path and few occurances of validation_epoch_end

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Replace all occurances of validation_epoch_end to on_validation_epoch_end

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Replace training_epoch_end, test_epoch_end with on_train_epoch_end and on_test_epoch_end respectively

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Change logger=None to logger=False in Trainer object

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Remove PTL2.0 deprecated Trainer args from TrainerConfig dataclass

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Modify trainer.precision check and other small edits

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Replace logger=None with logger=False in test_ptl_stateless_timer.py Trainer

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add default values for args to fix Attribute Error

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add the following modifications

1) Remove outputs arg from on_validation_epoch_end, on_test_epoch_end and make it an arg of the class
2) Replace resume_from_checkpoint with ckpt_path as needed
3) Explicitly add accelerator as 'CPU' in UTs being run on CPU

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Remove outputs arg from on_validation_epoch_end, on_test_epoch_end

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Remove outputs arg in on_validation_epoch_end in MultiBinaryAccuracy docstrings

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add val, test outputs as instance vars in PunctuationCapitalizationModel and TokenClassificationModel

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Replace trainer.fit_loop.max_steps with trainer.fit_loop.epoch_loop.max_steps in test_optimizers_schedulers.py

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Revert an extra space that was mistakenly added

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Use self.validation_step_outputs and self.test_step_outputs in test_ema.py for uniformity

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Use self.validation_step_outputs and self.test_step_outputs in test_ptl_stateless_timer.py and check_for_ranks.py for uniformity

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add self.validation_step_outputs.clear() and self.test_step_outputs.clear() wherever missing

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Remove outputs arg from on_train_epoch_end

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Remove outputs from on_validation_epoch_end in multi_binary_acc.py

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Remove output args from on_validation_epoch_end in the docstrings of some ASR files

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Remove output args from on_validation_epoch_end and clear memory from validation_step_outputs

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add on_validation_epoch_end and remove outputs args for nlp models

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Append output of validation_step to validation_step_outputs in EncDecClassificationModel

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add the following changes

1) Index self.validation_step_outputs and self.test_step.outputs with dataloader_idx wherever needed
2) Initialize self.validation_step_outputs and self.test_step.outputs as empty lists and add support for multi dataloaders if they exist
3) Remove self.pre_configure_ddp from NLPDDPStrategy class as its removed in PTL 2.0

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add default value dataloader_idx=0 for on_validation_batch_end() in megatron_base_model.py

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* TypeCast precision to str in attention.py and utils_funcs.py to avoid TypeError

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add if condition check for multiple dataloaders when appending to validation outputs

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Separate validation pass to be used with both validation_step and test_step

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add if condition check for multiple dataloader while appending to test_step_outputs in punctuation_capitalization_model.py

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add condition check for multiple dataloaders based on type of trainer.val/test_dataloaders or self._validation/test_dl instead of len

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Comment Megatron T5 IA3 PP=2 in CI pipeline due to dataloader_iter issue with PTL 2.0

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Modify precision checks to account for 16-mixed and bf16-mixed

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Append output of validation/test_step to self.validation/test_step_outputs in CTCG2PModel

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Modify find_unused_parameters=True in g2p_heteronym model

1) Add find_unused_parameters=True for DDP strategy in g2p_heteronym_classification_train_and_evaluate.py
2) Remove args output in validation/test_step and add instance variables instead for heteronym_classification.py

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Remove outputs from on_test_epoch_end in DialogueGPTClassificationModel

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add validation/test outputs in sgdqa_model and modify dialogue_config.yaml

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add split arg self.test_step_outputs to TextClassificationModel

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add test_step_outputs to dialogue and text classification models

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Change condition check for multiple dataloaders:

1) Replace ds_item as list in dialogue_config.yaml
2) Check for len of val/test_dataloaders or validation/test_dl along with type check of list in sgdqa_model.py while appending outputs of validation/test_step
3) Check for len of _validation/test_dl for creating self.validation/test_step_outputs in ModelPT and punctuation_cpitalization_model.py

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add additional condition for multi dataloaders

Check len(self.trainer.val/test_dataloaders) > 1 along with type(self.trainer.val/test_dataloaders) == list for multi dataloaders in validation/test_step

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add val step outputs and default val for dataloader_idx

1) Append validation_step outout to self.validation_step_outputs in MultiLabelIntentSlotClassificationMode
2) Add default val for dataloader_idx for on_test_batch_start/end in TimingCallback
3) Add self.validation/test_step_outputs in BERTQAModel and remove outputs arg

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add val/test_step_outputs to S2SQAModel and GPTQAModel

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Edit JenkinsFile for bert_pretrainig.py

Edit Jenkinsfile for this test to disable validation as a workaround for trainer.val_dataloader None error

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Modify precision to support 16-mixed, bf16-mixed in megatron_gpt_pretraining.py

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add ddp_find_unused_parameters_true and remove output args

1) Add ddp_find_unused_parameters_true fro trainer.strategy in self_alignment_pretraining.py as it has unused parameters
2) Remove output args and add self.validation/test_step_outputs to validation/test_step in mt_enc_dec_model.py
3) Comment tests in JenkinsFile that need to be fixed

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Precision fix in megatron_nmt_training.py for 16-mixed, bf16-mixed

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Precision fix for megatron_bert_pretraining.py and megatron_bert_model.py

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Precision fix and validation/test_step_outputs

1) Add fix to account for 16-mixed and bf16-mixed in megatron_retro_mutransfer_pretrain.py, megatron_retro_pretraining.py
2) Reset ckpt_path for test in enc_dec_nmt.py
3) Remove outputs args and add validation/test_step_outputs in megatron_retrieval_model.py
4) Comment Megatron Bert Pretraining and Resume Training with Pipeline Paralleism and add back NMT Training Post-LN

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Precision fix and skip few failing tests

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add missing comment lines in JenkinsFile

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Comment jenkin tests and super().on_validation_epoch_end() in megatron_gpt_sft_model.py

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Minor edit JenkinsFile

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Minor edit in jenkins file

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Edit in Jenkins file

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Comment missed lines in Jenkins file

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Fix precision and validation/test outputs

1) Add precision fix to account for 16-mixed and bf16-mixed in megatron_t5_pretraining.py
2) Remove outputs args and add append loss to self.validation/test_step_outputs in megatron_lm_encoder_decoder_model.py
3) Add back resume_from_checkpoint in the megatron_t5_config.yaml
4) Comment out certain tests in Jenkins file

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Fix precision and validation/test/predict errors in megatron_t5_prompt_learning.py

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Precision fix and edit precision typo in all files

1) Account for 16-mixed and bf16-mixed in megatron_bart_pretraining.py and megatron_t5_seq2seq_finetune.py
2) Fix precision typo in all files

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Fix all CI TTS tests and comment few Jenkins tests

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Combine xx_epoch_end and on_xx_epoch_end

Add on_inference_epoch_end to inference_epoch_end function and have a single on_validation/test_epoch_end in megatron_finetune_model.py and megatron_gpt_sft_model.py

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add a missing comment in JenkinsFile

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add try except StopIteration in validation_step for models with dataloader_iter

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Remove pyyaml from requirements

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add try except for inference_step in megatron_finetune_model.py

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Remove limit_val_batches for mockGPTDataset test

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add new self.validation_step_outputs for MegatronGPTSFTModel

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Minor edit Jenkinsfile

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Initialize self.validation/test_step_outputs in megatron_gpt_sft_model.py

Initialize self.validation/test_step_outputs in setup of MegatronGPTSFTModel to take care of cases when datalaoders are not setup in ModelPT for example while restoring the model.

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Remove resume_from_checkpoint if trainer arg in conf yaml files

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Remove resume_from_checkpoint as trainer arg in GPT, T5 configs

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Remove resume_from_checkpoint in duplex_tn_config.yaml

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Fix typos, unused imports and refactor code to remove redundant funcs

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Remove commented code in megatron_nmt_model.py

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Fix overriden functions to match parent class functions

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Prefetch dataloader_iter to prevent hang for PP>1

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Override setup() in NLPDDPStrategy to avoid hang during predict with PP>1

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Uncomment tests in JenkinsFile

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add '16' to precision checks and other minor fixes

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Clear validation/test_step_outputs with dataloader_idx for multi dataloaders

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Minor edits

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Modify precision checks to avoid indexing

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Remove self.validation_step_outputs_sft and add dataloader_idx to clear outputs

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Reference checkpoint with trainer.ckpt_path

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add _prefetch to NLPModel and minor fixes

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add limit_val_batches in JenkinsFile for NMT

1) Add trainer.limit_val_batches in Megatron NMT Training TP=2
2) Remove unused import in ModelPT

Signed-off-by: Abhishree <abhishreetm@gmail.com>

---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2023-05-15T20:05:16Z,"Confidence ensembles implementation (#6614)

* Working version to train conf model + save ensemble class

Signed-off-by: Igor Gitman <igitman@nvidia.com>

* Working version

Signed-off-by: Igor Gitman <igitman@nvidia.com>

* Remove copy of transcribe_speech.py

Signed-off-by: Igor Gitman <igitman@nvidia.com>

* Move models parameter to config

Signed-off-by: Igor Gitman <igitman@nvidia.com>

* Add explicit parameters to transcribe

Signed-off-by: Igor Gitman <igitman@nvidia.com>

* Small cleanups

Signed-off-by: Igor Gitman <igitman@nvidia.com>

* Add temperature and integration tests

Signed-off-by: Igor Gitman <igitman@nvidia.com>

* Add more tests

Signed-off-by: Igor Gitman <igitman@nvidia.com>

* Add pc removal config

Signed-off-by: Igor Gitman <igitman@nvidia.com>

* Cleanup

Signed-off-by: Igor Gitman <igitman@nvidia.com>

* Fix typo

Signed-off-by: Igor Gitman <igitman@nvidia.com>

* Address review comments

Signed-off-by: Igor Gitman <igitman@nvidia.com>

---------

Signed-off-by: Igor Gitman <igitman@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2023-03-10T00:28:26Z,"Save model parallel .nemo in ExpManager (#6115)

* patch to allow using tokenizers without additional_special_tokens_ids attribute

Signed-off-by: arendu <adithya.r@gmail.com>

* save tp pp > 1 .nemo in exp manager

Signed-off-by: arendu <adithya.r@gmail.com>

* Better rank checking for model parallel > 1 .nemo saving

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Safety check

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* check for nlp model

Signed-off-by: arendu <adithya.r@gmail.com>

* custom on save checkpoint for NLPModel

Signed-off-by: arendu <adithya.r@gmail.com>

* minor update

Signed-off-by: arendu <adithya.r@gmail.com>

* minor updates

Signed-off-by: arendu <adithya.r@gmail.com>

* reverting custom save logic

Signed-off-by: arendu <adithya.r@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* reverting custom save logic

Signed-off-by: arendu <adithya.r@gmail.com>

* reverting custom save logic

Signed-off-by: arendu <adithya.r@gmail.com>

* reverting custom save logic

Signed-off-by: arendu <adithya.r@gmail.com>

* reverting custom save logic

Signed-off-by: arendu <adithya.r@gmail.com>

* reverting custom save logic

Signed-off-by: arendu <adithya.r@gmail.com>

* added pleasefixme

Signed-off-by: arendu <adithya.r@gmail.com>

* updated

Signed-off-by: arendu <adithya.r@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: arendu <adithya.r@gmail.com>
Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2023-02-06T18:21:37Z,"Dynamic freezing in Nemo (#5879)

* Initial commit for dynamic freezing logic

Signed-off-by: Daniel Egert <degert@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Updated logic to handle lists and updated docs

Signed-off-by: Daniel Egert <degert@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Transferred dynamic freezing logic to core from asr

Signed-off-by: Daniel Egert <degert@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Revert asr config to original

Signed-off-by: Daniel Egert <degert@nvidia.com>

* Fixed tab indent in core.rst

Signed-off-by: Daniel Egert <degert@nvidia.com>

* Updated modelPT for latest from master

Signed-off-by: Daniel Egert <degert@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fixed indents in docs

Signed-off-by: Daniel Egert <degert@nvidia.com>

---------

Signed-off-by: Daniel Egert <degert@nvidia.com>
Co-authored-by: Daniel Egert <degert@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2023-01-23T19:04:09Z,"Support nested NeMo models (#5671)

Nested NeMo models support

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Sean Naren <sean.narenthiran@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2023-01-23T08:39:08Z,"[TTS][DE] Augment tokenization/G2P to preserve capitalization of words and mix phonemes with word-level graphemes for an input text. (#5805)

* unify german and any locale text processing func.
* add comments and instructions for grapheme_prefix.
* add comments to nomralize_unicode_text func.
* relax the g2p dictionary lookup for mixed cases and added unit tests to verify.
* fix special cases for English.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-12-12T18:23:45Z,"Fix: setup_multiple validation/test data (#5585)

Fix: setup_multiple validation/test data (#5585)

Signed-off-by: Ante JukicÃÅ <ajukic@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-12-09T21:14:12Z,"Add an option to defer data setup from ``__init__`` to ``setup`` (#5569)

* Add an option to defer dataloader setup from __init__ to setup

Signed-off-by: Ante JukicÃÅ <ajukic@nvidia.com>

* Updated doc

Signed-off-by: Ante JukicÃÅ <ajukic@nvidia.com>

Signed-off-by: Ante JukicÃÅ <ajukic@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-10-13T22:39:16Z,"refactor TTS documentation organization and add new contents. (#5137)

* refactor TTS documentation organization and add new contents.
* fix asr api bug.
* fix broken links.
* fix unexpected indentation errors.
* fixed unexpected indentation.
* fixed broken paper reference.
* fixed cross-reference and typos.
* fixed toctree errors.
* revert to 'Augmentors'
* reordered TTS tutorial list in starthere.
* ordered api classes alphabetically for each Section.
* fixed underscore typo for fastpitch checkpoint.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* upcase 'Tuning'

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* fixed typo for RAD-TTS Aligner

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* reorder aligner section after mel-gen and vocoders in models.rst.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* clarify Mixer-TTS-X and reorder model descriptions alphabetically.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* fixed some typos and formats.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* removed old megatron.rst.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* fixed block quote ends without a blank line warnings.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* remove duplicate reference; fixed missing key nlp-megatron-shoeybi2019megatron

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* Revert ""removed old megatron.rst.""

This reverts commit c5ea1dc3f23272eecfe8040e3abfa54fa122cf73.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* removed Russian, a hyphen, and add a note about G2P in tts/config.rst

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* added pynini installation in wfst_text_normalization.rst

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* added description of manifest key/value pairs.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* add toctree in tts/intro

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* replace main branch to stable.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* add 'upcoming' for e2e systems.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* replaced main branch to stable.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-10-11T20:22:39Z,"Fixes for docs/typos + remove max_utts parameter from tarred datasets as it causes hang in training (#5118)

* Remove ; from jupyter notebook cells

Signed-off-by: Igor Gitman <igitman@nvidia.com>

* Fix typos in documentation/code

Signed-off-by: Igor Gitman <igitman@nvidia.com>

* Fix output message to have 'or equal'

Signed-off-by: Igor Gitman <igitman@nvidia.com>

* Link formatting fixes

Signed-off-by: Igor Gitman <igitman@nvidia.com>

* Add error if max_utts is used in tarred datasets

Signed-off-by: Igor Gitman <igitman@nvidia.com>

* Remove max_utts parameter from tarred datasets

Signed-off-by: Igor Gitman <igitman@nvidia.com>

* Fix max_utts removal in tests

Signed-off-by: Igor Gitman <igitman@nvidia.com>

* Fix typo if -> is

Signed-off-by: Igor Gitman <igitman@nvidia.com>

Signed-off-by: Igor Gitman <igitman@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-09-13T00:24:36Z,"merge r1.11 to main (#4920)

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* update package info and dockerfile

Signed-off-by: ericharper <complex451@gmail.com>

* [TTS] bugfix for missing configs. (#4725)

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* Fix pynini install in TTS tutorials (#4729)

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* [TTS] updated config with a German IPA phoneme tokenizer (#4756)

* [TTS] added a German IPA phoneme tokenizer
* [TTS][ASR] enabled customized arguments for trimming the leading and trailing silence.
* [TTS] disabled spline interpolation for beta-binomial distribution. Let it generate align prior and save to disks. Use a new phoneme tokenizer.
* [TTS] use consistent spline interpolation with fastpitch checkpoint when generating mel-spectrograms for hifigan finetune.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* Update r1.11 to new heteronyms list (#4745)

* Update configs to new heteronyms list
* Remove old heteronyms list, add alt 'merchandise' pron to CMUdict
* Update remaining references to old heteronyms list

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* Fix tutorial formatting (#4778)

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* update branch and typos (#4788)

Signed-off-by: ericharper <complex451@gmail.com>

Signed-off-by: ericharper <complex451@gmail.com>

* Adding support for models trained with full context for cache-aware streaming. (#4687)

* added support for models trained with full context.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed style.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* dropped seq_range

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed indexing in caching methods.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed code style.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed code style.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* updated docs.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* addressed comments.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed code style.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed code style.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed code style.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* change frame-wise to cache-aware.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* change frame-wise to cache-aware.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* change frame-wise to cache-aware.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed code style.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* Update megatron encoder decoder model to support py37 for colab (#4791)

* [ASR] Add pretrained ASR models for Croatian (#4682)

* [ASR] Add pretrained ASR models for Croatian

Signed-off-by: Ante JukicÃÅ <ajukic@nvidia.com>

* Fix style for import

Signed-off-by: Ante JukicÃÅ <ajukic@nvidia.com>

Signed-off-by: Ante JukicÃÅ <ajukic@nvidia.com>
Co-authored-by: Ante JukicÃÅ <ajukic@nvidia.com>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>

* added/fixed export for Megatron models (#4712)

* added/fixed export for Megatron models

Signed-off-by: David Mosallanezhad <dmosallanezh@nvidia.com>

* fixed style

Signed-off-by: David Mosallanezhad <dmosallanezh@nvidia.com>

* fixed FusedScaleMaskSoftmax in BioMegatron

Signed-off-by: David Mosallanezhad <dmosallanezh@nvidia.com>

* included comments

Signed-off-by: David Mosallanezhad <dmosallanezh@nvidia.com>

Signed-off-by: David Mosallanezhad <dmosallanezh@nvidia.com>
Co-authored-by: David Mosallanezhad <dmosallanezh@nvidia.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* update branch for qa notebook

Signed-off-by: ericharper <complex451@gmail.com>

* Fix initializing weights from ptl ckpt with exclude (#4807)

Signed-off-by: sam1373 <samuelkriman@gmail.com>

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* Fix index error from addition of voiced_mask and p_voiced (#4811)

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* T5 prompt learning fixes (#4771)

* RPE, hidden size and config fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Update to reflect new config names

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Sentencepiece fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix finetuning

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Add encoder seq len to gpt

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Add finetune eval script

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix name

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Update Jenkinsfile

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Update config

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix CI test

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Update check

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Backward compat

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Update CI test

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Split rank for Enc-Dec models

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Address comments

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Virginia Adams <78445382+vadam5@users.noreply.github.com>

* G2P docs (#4841)

* g2p docs added

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix references

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* address review feedback

Signed-off-by: ekmb <ebakhturina@nvidia.com>

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* Fix providing glue in seq2seq eval (#4843)

* Fix providing glue in seq2seq eval

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Updated inference code and squad scripts (#4835)

* Updated inference code and squad scripts

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Reverted GPT & T5 inference files back to use NLPDDPlugin

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Overwrite frozen LM to use fused adam

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Added padded vocab size

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Fixed val check interval value

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Python format fix

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Make t5 prompt learning preds write to file

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Added back dp=1 check

Signed-off-by: Virginia Adams <vadams@nvidia.com>

Signed-off-by: Virginia Adams <vadams@nvidia.com>
Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>

* Set the number of workers to 0 for validation and test sets in all enc-dec models (#4790)

* Set workers to 0 for validation and test

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Revert pin memory

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Sean Naren <sean.narenthiran@gmail.com>

* Fix Megatron NMT consumed samples and ckpt_to_nemo split rank (#4884)

* Fix nmt and ckpt_to_nemo

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* added utf8 encoding (#4892)

Signed-off-by: Virginia Adams <vadams@nvidia.com>

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* update readme with apex commit

Signed-off-by: ericharper <complex451@gmail.com>

* Add support for Apex distributed Adam optimizer with GPT-3 (#4487)

* Add support for Apex distributed Adam optimizer with GPT-3

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Fix bug in grad clipping with dist Adam

Grad norm was computed over all params, not respecting model parallelism.

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Fix bug with DDP initialization

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Make distopt dependent on megatron_amp_o2

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Fix code formatting

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Handle dist Adam in optimizer unit tests

Signed-off-by: Tim Moon <tmoon@nvidia.com>

Signed-off-by: Tim Moon <tmoon@nvidia.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* update readme

Signed-off-by: ericharper <complex451@gmail.com>

* update readme

Signed-off-by: ericharper <complex451@gmail.com>

* fixed styles

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* removed unsued import.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* removed duplicated func defintion.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* replace 'r1.11.0' with 'main' in Jenkinsfile and all tutorials.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* fix: PRE_RELEASE = 'rc0'

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* replace branch name to main for asr_with_adapters.ipynb.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* fix Fastpitch mixertts tutorial format to align with main to distingshuish diff

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* fix: correct path for tokenizers.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

Signed-off-by: ericharper <complex451@gmail.com>
Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>
Signed-off-by: Vahid <vnoroozi@nvidia.com>
Signed-off-by: Ante JukicÃÅ <ajukic@nvidia.com>
Signed-off-by: David Mosallanezhad <dmosallanezh@nvidia.com>
Signed-off-by: sam1373 <samuelkriman@gmail.com>
Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>
Signed-off-by: ekmb <ebakhturina@nvidia.com>
Signed-off-by: Virginia Adams <vadams@nvidia.com>
Signed-off-by: Tim Moon <tmoon@nvidia.com>
Co-authored-by: ericharper <complex451@gmail.com>
Co-authored-by: Jocelyn <jocelynh@nvidia.com>
Co-authored-by: Vahid Noroozi <VahidooX@users.noreply.github.com>
Co-authored-by: Zhilin Wang <wangzhilin12061996@hotmail.com>
Co-authored-by: anteju <108555623+anteju@users.noreply.github.com>
Co-authored-by: Ante JukicÃÅ <ajukic@nvidia.com>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: David <amosalla@asu.edu>
Co-authored-by: David Mosallanezhad <dmosallanezh@nvidia.com>
Co-authored-by: Samuel Kriman <samuelkriman@gmail.com>
Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Virginia Adams <78445382+vadam5@users.noreply.github.com>
Co-authored-by: Evelina <10428420+ekmb@users.noreply.github.com>
Co-authored-by: Sean Naren <sean.narenthiran@gmail.com>
Co-authored-by: Tim Moon <4406448+timmoon10@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-08-15T23:37:31Z,"Add support for Apex distributed Adam optimizer with GPT-3 (#4487)

* Add support for Apex distributed Adam optimizer with GPT-3

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Fix bug in grad clipping with dist Adam

Grad norm was computed over all params, not respecting model parallelism.

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Fix bug with DDP initialization

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Make distopt dependent on megatron_amp_o2

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Fix code formatting

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Handle dist Adam in optimizer unit tests

Signed-off-by: Tim Moon <tmoon@nvidia.com>

Signed-off-by: Tim Moon <tmoon@nvidia.com>
Co-authored-by: Eric Harper <complex451@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-08-11T15:20:29Z,"upgrade to PTL 1.7 (#4672)

* upgrade to PTL 1.7

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* min version

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* replace progressbar_refresh_rate with enable progressbar, this is callback now

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* progressbar

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* replace removed PTL 1.7 args, fix cpu tests, remove p-tune older script

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* revert ssl test fixes

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* override trainer property and fix numba grad check

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* NLPDDPlugin -> NLPDDPStrategy

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* style fix

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* set max_steps default as -1

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* fix maxsteps in notebooks

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* update trainer config

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* fix speech2label jenkins

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* fix speech2text jenkins

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* DDPPlugin -> DDPStrategy

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* remove provided strategy keys from trainer config nlp

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* check other examples

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* override LightningModule .cuda call to maintain pytorch default behavior

Signed-off-by: ericharper <complex451@gmail.com>

* revert gpt eval jenkins test

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* overwrite cuda class to PTL

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* review feedback

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* remove checkpoint callback from main config

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* patch fix for intentslot classification test

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* style fix

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>
Signed-off-by: ericharper <complex451@gmail.com>
Co-authored-by: ericharper <complex451@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-07-15T22:42:31Z,"Removed NLPDDPPlugin Import check (#4555)

* Removed NLPDDPPlugin Import check

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Python formatting fix

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* changed app to app_state

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* moved num workers check back to bottom

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Python code reformat

Signed-off-by: Virginia Adams <vadams@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-07-13T20:48:41Z,"Add nsys profiling (#4539)

* add nsys profiling

Signed-off-by: ericharper <complex451@gmail.com>

* only access omegaconf in setup

Signed-off-by: ericharper <complex451@gmail.com>

* use robust get_rank function

Signed-off-by: ericharper <complex451@gmail.com>

* simplify

Signed-off-by: ericharper <complex451@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-07-09T02:11:13Z,"Update finetune label models (#4504)

* initial_script

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* move old script

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* remove finetune func from label models

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* style clean

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* updated config

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* update tutorial

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* lgtm fixes

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* updated based on comments

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* update doc

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-07-08T13:12:58Z,"[Add] Support for Different LRs with Param Groups (#4508)

* add support for param groups

Signed-off-by: stevehuang52 <heh@nvidia.com>

* make config more general

Signed-off-by: stevehuang52 <heh@nvidia.com>

Co-authored-by: Eric Harper <complex451@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-06-30T22:37:24Z,"[TTS] created the finetuning Hifigan 44100Hz recipe on HUI-Audio-Corpus-German. (#4478)

* implemented a script of generating mel-spectrograms for finetuning Hifigan using multiprocessing.
* fixed some typos.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-06-28T02:33:34Z,"Add O2 support for RETRO model (#4411)

* make sure post-ln works with new coeff

Signed-off-by: Yi Dong <yidong@nvidia.com>

* add some comments

Signed-off-by: Yi Dong <yidong@nvidia.com>

* second v

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix headscale

Signed-off-by: Yi Dong <yidong@nvidia.com>

* works both for pre-ln and post-ln

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix unittest

Signed-off-by: Yi Dong <yidong@nvidia.com>

* stop gradient

Signed-off-by: Yi Dong <yidong@nvidia.com>

* stop gradient

Signed-off-by: Yi Dong <yidong@nvidia.com>

* use half rotary embedding

Signed-off-by: Yi Dong <yidong@nvidia.com>

* use default grad clip

Signed-off-by: Yi Dong <yidong@nvidia.com>

* turn off rotary embedding

Signed-off-by: Yi Dong <yidong@nvidia.com>

* added o2 support

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix style

Signed-off-by: Yi Dong <yidong@nvidia.com>

* add debugging

Signed-off-by: Yi Dong <yidong@nvidia.com>

* make cyclic lr work

Signed-off-by: Yi Dong <yidong@nvidia.com>

* o2 works with cyclic lr

Signed-off-by: Yi Dong <yidong@nvidia.com>

* remove deepnet

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix merge error

Signed-off-by: Yi Dong <yidong@nvidia.com>

* update the comments

Signed-off-by: Yi Dong <yidong@nvidia.com>

* added output scaling for stable training

Signed-off-by: Yi Dong <yidong@nvidia.com>

* improve the debug code

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix comment

Signed-off-by: Yi Dong <yidong@nvidia.com>

* move debug hook above

Signed-off-by: Yi Dong <yidong@nvidia.com>

* move optimizer config to base class

Signed-off-by: Yi Dong <yidong@nvidia.com>

* address comments

Signed-off-by: Yi Dong <yidong@nvidia.com>

Co-authored-by: Eric Harper <complex451@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-05-26T04:50:43Z,"BigNLP perf regression fix (#4267)

* Temp

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove line that caused perf regression

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-05-17T18:59:54Z,"Add Module-level Adapters, Save-Restore and tests (#4114)

* First draft of model level tests and support for multiple types adapters in same model

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add save restore tests for adapters

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add save restore tests for adapters

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add adapter only save and restore

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update base adapter config

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix collection of get enabled adapters, limiting to each module's scope

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update docs and add support for resolution of module adapter names

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update ASR adapters to only support module adapters

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add state dict match test

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix name resolution for set_enabled_adapters

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct case where name is none for set adapter

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct case where there are no adapters to save

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update config for training

Signed-off-by: smajumdar <titu1994@gmail.com>

* Force update to internal config upon get or set

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add spec augment update support to adapters

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct config update

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add dropout support to linear adapters

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add type to config

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add stochastic depth regularization to adapter merge strategy and related tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add support for dynamic strategy change

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add support for dynamic strategy change

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add more tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add more tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Remove logging of adapter name

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update changes for reviews

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Refactor the utility methods

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Refactor the utility methods

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Fixed configs for optim and spec augment

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Fixed configs for optim and spec augment

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Rename method to subclassable private

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Add support for adapter module names to be pre-specified in config

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Fix imports

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Fix typos

Signed-off-by: smajumdar <smajumdar@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-04-29T19:40:38Z,"NeMo Adapters Support + ASR Adapters (#3942)

* Initial impl of adapters

Signed-off-by: smajumdar <titu1994@gmail.com>

* Stabilize Adapter core api

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix tests for freezing

Signed-off-by: smajumdar <titu1994@gmail.com>

* Initial subclass impl

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add encoder adapters support to Model level

Signed-off-by: smajumdar <titu1994@gmail.com>

* Improve code stability

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix as_frozen()

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix as_frozen()

Signed-off-by: smajumdar <titu1994@gmail.com>

* Localize config for stability and delegate updates

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update name of method

Signed-off-by: smajumdar <titu1994@gmail.com>

* Stabilize codebase

Signed-off-by: smajumdar <titu1994@gmail.com>

* Style fixes

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add download test

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update test

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add docstrings to base adapter mixin

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add docstrings to base adapter mixin

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add docstrings to ASR adapter mixin

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add docs for mixins and adapter modules

Signed-off-by: smajumdar <titu1994@gmail.com>

* Revamp Jasper support for adapters to be dynamic in output dim, add data class support for adapter mixins, add linear adapter dataclass, refactor tests and add more asr tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Style fixes

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update conformer encoder adapter docs

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct indent

Signed-off-by: smajumdar <titu1994@gmail.com>

* [INCORRECT] COMMIT

Signed-off-by: smajumdar <titu1994@gmail.com>

* [INCORRECT] COMMIT

Signed-off-by: smajumdar <titu1994@gmail.com>

* [INCORRECT] COMMIT

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add asr adapter scripts

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct initialization of adapters, fix rnnt debug logging and fix modelpt issue with optimizer param groups

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update recommendations for asr adaptation

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix trailing \

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix missing adapter value for config

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add export test for adapter asr module

Signed-off-by: smajumdar <titu1994@gmail.com>

* Test only on gpu

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix trailing \

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix trailing \

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix configs

Signed-off-by: smajumdar <titu1994@gmail.com>

* Refactor out asr adapter mixin into AdapterModelPTMixin

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add Jenkinsfile test

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add Jenkinsfile test

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct feature dim for small Conformers

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix adapters test position

Signed-off-by: smajumdar <titu1994@gmail.com>

* Temp

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add support for top level config in adapters

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add support for global_cfg for adapter

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add support for adapter global_cfg support, make ASR adapter mixin optionally build encoder adapters

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update script to train adapters

Signed-off-by: smajumdar <titu1994@gmail.com>

* Refactor out LinearAdapter to commons

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix inits

Signed-off-by: smajumdar <titu1994@gmail.com>

* patch model.summarize() deprecation notice

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add adapter registry support

Signed-off-by: smajumdar <titu1994@gmail.com>

* Remove extra import

Signed-off-by: smajumdar <titu1994@gmail.com>

* Remove extra import

Signed-off-by: smajumdar <titu1994@gmail.com>

* Address review comments

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix paths to config

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add execution flow diagrams

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-04-28T00:39:51Z,"[Core] Support pre-extracted nemo checkpoint for restoration (#4061)

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-04-26T18:09:15Z,"Update num worker calculation due to PTL flag changes (#4056)

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-04-20T22:46:30Z,"Merge r1.8.0 main (#4036)

* update version

Signed-off-by: ericharper <complex451@gmail.com>

* Stateless timer fix for PTL 1.6 (#3925)

* Stateless timer fix for PTL 1.6

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Stateless timer PTL test

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix year

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove unused imports

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* GPU test

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* clean import

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: ericharper <complex451@gmail.com>

* Fix issues with librosa deprecations (#3950)

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix notebook bugs for branch r1.8.0 (#3948)

* load the model from ngc

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix all biomegatron notebook

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix the typos

Signed-off-by: Yi Dong <doyend@gmail.com>

* remove output

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix isort

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix merge error

Signed-off-by: Yi Dong <doyend@gmail.com>

* change ntpath for isort workaround

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix unit test

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix ci

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix ci bert pretraining

Signed-off-by: Yi Dong <doyend@gmail.com>

* make it compatible with main

Signed-off-by: Yi Dong <doyend@gmail.com>

* add the teste for biomegatron ner

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix argument

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix usablity issue

Signed-off-by: Yi Dong <doyend@gmail.com>

* work around

Signed-off-by: Yi Dong <doyend@gmail.com>

Co-authored-by: Yi Dong <doyend@gmail.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* Fix global batch fit loop (#3936)

* add lightning module hooks for global batch

Signed-off-by: ericharper <complex451@gmail.com>

* clean scripts

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* DP=1 fix

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* set num dataset workers to 2

Signed-off-by: ericharper <complex451@gmail.com>

* update validation_loop with GlobalDataFetcher

Signed-off-by: ericharper <complex451@gmail.com>

* add test global data fetcher

Signed-off-by: ericharper <complex451@gmail.com>

* Drop last for test ds

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix test epoch end

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix eval

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix reconfigure microbatch in the complete method

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* add comments

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Set init consumed samples

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* fix shuffle

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* add save_restore_connector arg

Signed-off-by: ericharper <complex451@gmail.com>

* Fix padding for labels and loss mask

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* GLUE/XNLI CI tests

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* limit val batches in hydra fix

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Restart CI

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix unittest

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

Co-authored-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Exports 22.03 war (#3957)

* Fixed fastpitch for 22.03

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* cleanup

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Restored mask expansion; added WAR for test container images

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* style

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Refactor restorefrom (#3927)

* update package info (#3926)

Signed-off-by: ericharper <complex451@gmail.com>

* Refactor restore_from

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Move export related python files to scripts/export/

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Return state dict after modification function

* Remove Megatron legacy parameter in common.py restore_from function

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* ability to set log_predictions to false (#3929)

* Bumping Python version

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* fixing style

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* load the model from ngc

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix all biomegatron notebook

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix the typos

Signed-off-by: Yi Dong <doyend@gmail.com>

* remove output

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix isort

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix merge error

Signed-off-by: Yi Dong <doyend@gmail.com>

* change ntpath for isort workaround

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix unit test

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix ci

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix ci bert pretraining

Signed-off-by: Yi Dong <doyend@gmail.com>

* Rearrage export files; Style fix; Extend legacy MegatronBert conversion to NLP models nemo version updation

* Glu activation variants (#3951)

* Temp

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Add reglu and swiglu activations

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style on unrelated file

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* CI changes to test activations

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix unused import

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style fix beacuse of merge from main

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* make it compatible with main

Signed-off-by: Yi Dong <doyend@gmail.com>

* add the teste for biomegatron ner

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix argument

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix usablity issue

Signed-off-by: Yi Dong <doyend@gmail.com>

* FastPitch FT notebook - Improving Speech Quality clarifications (#3954)

* FastPitch FT notebook - Improving Speech Quality clarifications

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Add pynini dependency install to FastPitch FT notebook

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Pin pynini install for FastPitch FT tutorial

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* work around

Signed-off-by: Yi Dong <doyend@gmail.com>

Co-authored-by: Eric Harper <complex451@gmail.com>
Co-authored-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>
Co-authored-by: Dima Rekesh <bmwshop@gmail.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>
Co-authored-by: Yi Dong <doyend@gmail.com>
Co-authored-by: Yi Dong <43824965+yidong72@users.noreply.github.com>
Co-authored-by: Boris Fomitchev <borisfom@users.noreply.github.com>
Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Jocelyn <jocelynh@nvidia.com>

* Bump TTS deprecation version to 1.9 (#3955)

* bump deprecation version

Signed-off-by: Jason <jasoli@nvidia.com>

* update talknet depre

Signed-off-by: Jason <jasoli@nvidia.com>

* added conformer for zh. (#3970)

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* Add pinned pynini and scipy installs to TTS training tutorial (#3967)

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Fix variable name and move models to CPU in Change partition (#3972)

* fixes

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* add CI

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>

* fix misconfiguration (#3975)

Signed-off-by: Yi Dong <doyend@gmail.com>

Co-authored-by: Yi Dong <doyend@gmail.com>

* Fix NMT variable passing bug (#3985)

* fix

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* stylefix

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* Compatability override to load_state_dict for old TTS checkpoints (#3978)

* Compatability override to load_state_dict for old TTS checkpoints

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Tacotron2 training notebook fix - add GPU argument

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Add hann window override warning for old model loading

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Notebook Bug Fixes for r1.8.0 (#3989)

* Made config related bug fixes

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Fixed cfg.get syntax

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Fix compat override for TalkNet Aligner (#3993)

* Fix compatibility override for TalkNet Aligner

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Remove extraneous logging import

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* docs fixes (#3987)

* docs fixes

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* rename files in docs

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* docs improvement

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* arg renamed

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* Fix nemo megatron restore with artifacts (#3997)

* update config_path in register_artifact

Signed-off-by: ericharper <complex451@gmail.com>

* fix register_artifact calls

Signed-off-by: ericharper <complex451@gmail.com>

* fix register_artifact calls

Signed-off-by: ericharper <complex451@gmail.com>

* update log messages to include merges file

Signed-off-by: ericharper <complex451@gmail.com>

* add default prompts to config

Signed-off-by: ericharper <complex451@gmail.com>

* Fixes val_check_interval, skip loading train data during eval (#3968)

* Change stage check

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix bugs in megatron t5 glue eval scripts

Signed-off-by: Yu Yao <yuya@nvidia.com>

* Fix reconfigure

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Change check

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix hasattr

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix typo in cfg structure

Signed-off-by: Yu Yao <yuya@nvidia.com>

* Update megatron t5 glue eval config file

Signed-off-by: Yu Yao <yuya@nvidia.com>

* Reconfigure to avoid drop last

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix for train step reconfigure as well

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Update megatron t5 glue eval config file drop_last to False

Signed-off-by: Yu Yao <yuya@nvidia.com>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* limit test batches

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

Co-authored-by: Yu Yao <54727607+yaoyu-33@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* LogProb calculation performance fix (#3984)

* performance fix for logprob computation

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix redandant assign

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix bug to add gather from TP workers

Signed-off-by: Yi Dong <doyend@gmail.com>

Co-authored-by: Yi Dong <doyend@gmail.com>

* Fix link issues in export example notebook and fix pretrained model info for MegatronBert (#4004)

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

Co-authored-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Fix single GPU training issue + change deprecated Lightning args (#4010)

* change vars

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* style fix

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* Fix P-Tune T5 model (#4001)

* fix ptune t5

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix ci test

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix the ci fail because of the order problem

Signed-off-by: Yi Dong <doyend@gmail.com>

Co-authored-by: Yi Dong <doyend@gmail.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* Megatron work-arounds (#3998)

* WAR around Apex issue, and making sure output is FP32

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixing merge issues; moving dummy Trainer; adding float() casts

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixing ColumnParallelLinear call

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Cleanup

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Cleanup#2

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* fix the broadcast shape mismatch (#4017)

Signed-off-by: Yi Dong <doyend@gmail.com>

Co-authored-by: Yi Dong <doyend@gmail.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* add known issues (#4024)

Signed-off-by: ericharper <complex451@gmail.com>

* update readme with conda env setup instructions

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* update package info

Signed-off-by: ericharper <complex451@gmail.com>

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* update package info

Signed-off-by: ericharper <complex451@gmail.com>

* revert apex guard removal

Signed-off-by: ericharper <complex451@gmail.com>

* revert --language to --lang

Signed-off-by: ericharper <complex451@gmail.com>

* fix apex guard

Signed-off-by: ericharper <complex451@gmail.com>

* remove set_trace

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* fix apex guard

Signed-off-by: ericharper <complex451@gmail.com>

* remove unreachable statement

Signed-off-by: ericharper <complex451@gmail.com>

* remove duplicate lines

Signed-off-by: ericharper <complex451@gmail.com>

* remove duplicate lines

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: Yi Dong <43824965+yidong72@users.noreply.github.com>
Co-authored-by: Yi Dong <doyend@gmail.com>
Co-authored-by: Boris Fomitchev <borisfom@users.noreply.github.com>
Co-authored-by: Ramanathan Arunachalam <ramanathan.arun@rutgers.edu>
Co-authored-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>
Co-authored-by: Dima Rekesh <bmwshop@gmail.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>
Co-authored-by: Jocelyn <jocelynh@nvidia.com>
Co-authored-by: Jason <jasoli@nvidia.com>
Co-authored-by: Vahid Noroozi <VahidooX@users.noreply.github.com>
Co-authored-by: Abhinav Khattar <aklife97@gmail.com>
Co-authored-by: Virginia Adams <78445382+vadam5@users.noreply.github.com>
Co-authored-by: Evelina <10428420+ekmb@users.noreply.github.com>
Co-authored-by: Yu Yao <54727607+yaoyu-33@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-04-01T06:27:27Z,"Upgrade to PTL 1.6.0 (#3890)

* upgrade

Signed-off-by: ericharper <complex451@gmail.com>

* upgrade

Signed-off-by: ericharper <complex451@gmail.com>

* update logger_connector attribute

Signed-off-by: ericharper <complex451@gmail.com>

* update checkpoint_connector attribute

Signed-off-by: ericharper <complex451@gmail.com>

* update trainer dataclass

Signed-off-by: ericharper <complex451@gmail.com>

* use pypi ptl

Signed-off-by: ericharper <complex451@gmail.com>

* update requirements

Signed-off-by: ericharper <complex451@gmail.com>

* wandb offline for unit test

Signed-off-by: ericharper <complex451@gmail.com>

* remove self.training_step_end calls

Signed-off-by: ericharper <complex451@gmail.com>

* create new ddpplugin object

Signed-off-by: ericharper <complex451@gmail.com>

* add storage_options arg

Signed-off-by: ericharper <complex451@gmail.com>

* update val_check_interval and max_steps

Signed-off-by: ericharper <complex451@gmail.com>

* update checkpointing

Signed-off-by: ericharper <complex451@gmail.com>

* skip validation epoch end if no outputs

Signed-off-by: ericharper <complex451@gmail.com>

* pin setuptools

Signed-off-by: ericharper <complex451@gmail.com>

* remove import

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* check for global zero

Signed-off-by: ericharper <complex451@gmail.com>

* if not outputs return in validation epoch end

Signed-off-by: ericharper <complex451@gmail.com>

* update checkpoint_connector

Signed-off-by: ericharper <complex451@gmail.com>

* update checkpoint_connector

Signed-off-by: ericharper <complex451@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-03-30T01:20:18Z,"Megatron support (#3893)

* Add MegatronBert training support for more NLP models; Replace check for nemo_file existence to decide if it's megatron training or not; Add support for finetuning a downstream NLP task model on a different downstream dataset using MegatronBert; Add skelton support for ONNX export of MegatronBert

* Remove duplicate lm_checkpoint key in config

* Adding new Apex classes for export replacement and default trainer support

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixing order of exported inputs in NLP models

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixed typo

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixed ORT check

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Refactor of NLP models initialization

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixed style

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixed runtime check

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Code style fixes

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Code style fixes

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Code style fixes

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Check for existence of downstream key before setting it

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Fix Huggingface download unit tests based on get_lm_model refactor

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Add register artifact for MegatronBert models; Remove unused import

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Fixed style

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixed duplex decoder init

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* moved set_world_size up

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixing _trainer initialization

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixed GPT tokenizer init

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fix Token classification and Q&A forward flag checks for MegatronBert

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Fix world_size init

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Skip GPT eval test

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Fix get_lm_model function calls based on recent refactoring

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Check for presence of keys in the Dict before trying to access them in BERTLMModel

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Check if tokenizer is present before accesing it in lm_utils

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Check for presence of keys in the Dict before trying to register them as artifacts

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Bypass NLP init statements pertaining to MegatronBert when using Ptune downstream task

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Revert NLPModel modification

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Skip the Jenkins Test Megatron P-Tuning GPT LM

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Merge branch 'main' into megatron_support

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

Co-authored-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>
Co-authored-by: Boris Fomitchev <bfomitchev@nvidia.com>
Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Co-authored-by: Boris Fomitchev <borisfom@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-02-23T21:40:46Z,"upgrade PTL trainer flags (#3589)

* upgrade PTL trainer flags

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* remove nlp override changes

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* fix test and update gpus -> devices

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* setting devices=1 for cpu case

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* update jenkins for gpus -> devices

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* single quote to double quote jenkins fix

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* single quote to double quote jenkins fix

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* devices in docs

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* update devices in tutorials :sigh:

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* checkpointing_callback -> enable_checkpointing

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* text normalization decoder trainer name fix

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* enable checkpoint callback name fix

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* NLPDDPPlugin overrides strategy ddp

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* more strategy cleaning for NLPDDPPlugin

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* now DDPPlugin for strategy removal

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* typo fix

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* NLPDDPPlugin more fixes

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* devices require accelerator as mandatory argument

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* moving megatron accelerator from gpu to cpu

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* accelerator null to strategy null fix

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* distrib_type taken from training_type_plugin

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* check for distributed type is removed

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* revert training type

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* revert transcribe_speech

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* style fix

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* typo fix

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* revert distributed backend type

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* remove flush_logs_every_n_steps trainer flag

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* rebase main

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* style fix

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* SGD gen update

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* NLPDDPPlugin change

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* remove strategy from dialogue conf

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* reverting find unused params

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* update jenkins

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* more nlp fixes

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* num_nodes removal from NLPDDPPlugin

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-02-14T17:05:44Z,"[NeMoMegatron] Pipeline parallelism for GPT (#3388)

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* fix File Load Error (#3069)

Signed-off-by: fayejf <fayejf07@gmail.com>

Co-authored-by: Eric Harper <complex451@gmail.com>

* update (#3113)

Signed-off-by: Jason <jasoli@nvidia.com>

* added to avail models (#3044)

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* Notebook bugfixes (#3165)

* zero shot intent slot notebook bug fix

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Added megatronBERT not support in this release message to notebook

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* added apex backend to config

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Removed apex from entity linking and added 1.4.0 nemo info to warning

Co-authored-by: Eric Harper <complex451@gmail.com>

* update app_state.local_rank

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* interpolate precision and sequence length in config

Signed-off-by: ericharper <complex451@gmail.com>

* remove trainer.precision interpolation

Signed-off-by: ericharper <complex451@gmail.com>

* remove nemo_experiments after prallel stage

Signed-off-by: ericharper <complex451@gmail.com>

* remove rm call

Signed-off-by: ericharper <complex451@gmail.com>

* use apex safe_divide

Signed-off-by: ericharper <complex451@gmail.com>

* update model parallel size to tensor model parallel size

Signed-off-by: ericharper <complex451@gmail.com>

* update model parallel size to tensor model parallel size

Signed-off-by: ericharper <complex451@gmail.com>

* add pipeline model paralllel size to config

Signed-off-by: ericharper <complex451@gmail.com>

* add comment

Signed-off-by: ericharper <complex451@gmail.com>

* uncommment pipeline parallel offset

Signed-off-by: ericharper <complex451@gmail.com>

* fake model parallel ranks

Signed-off-by: ericharper <complex451@gmail.com>

* update compute tensor model parallel rank in train script

Signed-off-by: ericharper <complex451@gmail.com>

* add logging for model parallel groups

Signed-off-by: ericharper <complex451@gmail.com>

* add logging for model parallel groups

Signed-off-by: ericharper <complex451@gmail.com>

* model parallel rank is from index of group

Signed-off-by: ericharper <complex451@gmail.com>

* model parallel rank is from index of group

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* add build_model from apex

Signed-off-by: ericharper <complex451@gmail.com>

* add model_parallel_size

Signed-off-by: ericharper <complex451@gmail.com>

* add _optimizer_param_groups to ModelPT

Signed-off-by: ericharper <complex451@gmail.com>

* typos

Signed-off-by: ericharper <complex451@gmail.com>

* check that fake mp ranks match after mp init

Signed-off-by: ericharper <complex451@gmail.com>

* remove list of modules for now

Signed-off-by: ericharper <complex451@gmail.com>

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* update model_parallel_size

Signed-off-by: ericharper <complex451@gmail.com>

* in progress of updating training_step and validation_step with fwd/bwd funcs

Signed-off-by: ericharper <complex451@gmail.com>

* added initialize word embeddings and updated validation for pipeline

Signed-off-by: ericharper <complex451@gmail.com>

* update training step for pipeline

Signed-off-by: ericharper <complex451@gmail.com>

* in progress of adding training step

Signed-off-by: ericharper <complex451@gmail.com>

* training_step in progress. need to figure out logging and grad scaling for pipeline parallel

Signed-off-by: ericharper <complex451@gmail.com>

* remove barriers

Signed-off-by: ericharper <complex451@gmail.com>

* add pipeline to checkpointing

Signed-off-by: ericharper <complex451@gmail.com>

* add get_parameters

Signed-off-by: ericharper <complex451@gmail.com>

* override zero_grad

Signed-off-by: ericharper <complex451@gmail.com>

* add comments for modifying training_step

Signed-off-by: ericharper <complex451@gmail.com>

* add global batch size to config

Signed-off-by: ericharper <complex451@gmail.com>

* add global batch fetching and process batch for apex fwd/bwd

Signed-off-by: ericharper <complex451@gmail.com>

* average losses across micro batches

Signed-off-by: ericharper <complex451@gmail.com>

* update validation_epoch_end

Signed-off-by: ericharper <complex451@gmail.com>

* update training_step to use global batch

Signed-off-by: ericharper <complex451@gmail.com>

* update training_step to use global batch

Signed-off-by: ericharper <complex451@gmail.com>

* remove automatic grad sync and add manual grad all reduce

Signed-off-by: ericharper <complex451@gmail.com>

* repeat attention_mask for apex fwd/bwd functions

Signed-off-by: ericharper <complex451@gmail.com>

* add fwd/bwd no pipeline

Signed-off-by: ericharper <complex451@gmail.com>

* make apex transformer log level configurable

Signed-off-by: ericharper <complex451@gmail.com>

* add comment explaining batch in train/val steps

Signed-off-by: ericharper <complex451@gmail.com>

* add comments

Signed-off-by: ericharper <complex451@gmail.com>

* fix bug with initialize word embeddings

Signed-off-by: ericharper <complex451@gmail.com>

* update set_input_tensor

Signed-off-by: ericharper <complex451@gmail.com>

* refactor dtype for comm

Signed-off-by: ericharper <complex451@gmail.com>

* add dtype for pipeline comm

Signed-off-by: ericharper <complex451@gmail.com>

* rebase after prompt tuning

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* add pipeline plugin to disable ptl autocast

Signed-off-by: ericharper <complex451@gmail.com>

* use torch.float instead of None for autocast_dtype

Signed-off-by: ericharper <complex451@gmail.com>

* rebase after O2 recipe was added

Signed-off-by: ericharper <complex451@gmail.com>

* fix typo

Signed-off-by: ericharper <complex451@gmail.com>

* don't autocast forward when using o2

Signed-off-by: ericharper <complex451@gmail.com>

* import inject model parallel rank

Signed-off-by: ericharper <complex451@gmail.com>

* import inject model parallel rank

Signed-off-by: ericharper <complex451@gmail.com>

* uninject mp rank for resume checkpoint

Signed-off-by: ericharper <complex451@gmail.com>

* replace MasterOptimizer with MainParamsOptimizer

Signed-off-by: ericharper <complex451@gmail.com>

* update fwd/bwd for fp16

Signed-off-by: ericharper <complex451@gmail.com>

* in progress allreduce embeddings for O2

Signed-off-by: ericharper <complex451@gmail.com>

* allreduce first last stage embeddings for O2

Signed-off-by: ericharper <complex451@gmail.com>

* add comments

Signed-off-by: ericharper <complex451@gmail.com>

* in progres updating save restore and ckpt conversion

Signed-off-by: ericharper <complex451@gmail.com>

* pass correct number of gpus to trainer in convert script

Signed-off-by: ericharper <complex451@gmail.com>

* update save_to

Signed-off-by: ericharper <complex451@gmail.com>

* update restore and eval

Signed-off-by: ericharper <complex451@gmail.com>

* adding pipeline for complete method

Signed-off-by: ericharper <complex451@gmail.com>

* pass microbatch to gpu in pipeline

Signed-off-by: ericharper <complex451@gmail.com>

* revert complete method changes

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* update no pipeline calls

Signed-off-by: ericharper <complex451@gmail.com>

* remove duplicate imports

Signed-off-by: ericharper <complex451@gmail.com>

* add persist layer norm arg

Signed-off-by: ericharper <complex451@gmail.com>

* rebase

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* move inject/uninject model parallel rank to prevent circular import

Signed-off-by: ericharper <complex451@gmail.com>

* update package info

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* fix check for mp_rank or tp_rank

Signed-off-by: ericharper <complex451@gmail.com>

* specify param_groups as list

Signed-off-by: ericharper <complex451@gmail.com>

* update configs and exp_manager for model_parallel_size

Signed-off-by: ericharper <complex451@gmail.com>

* add resolver for omegaconf interpolation

Signed-off-by: ericharper <complex451@gmail.com>

* update resume checkpoint in example scripts

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* destroy model parallel groups

Signed-off-by: ericharper <complex451@gmail.com>

* get precision from trainer

Signed-off-by: ericharper <complex451@gmail.com>

* update gpt prompt tuning dataset

Signed-off-by: ericharper <complex451@gmail.com>

* temporarily disable prompt tuning

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* skip prompt unit tests

Signed-off-by: ericharper <complex451@gmail.com>

* add default

Signed-off-by: ericharper <complex451@gmail.com>

* check if pipeline size is none

Signed-off-by: ericharper <complex451@gmail.com>

* update app_state after cpu restore

Signed-off-by: ericharper <complex451@gmail.com>

* revert jenkins change

Signed-off-by: ericharper <complex451@gmail.com>

* revert jenkins change

Signed-off-by: ericharper <complex451@gmail.com>

* revert jenkins change

Signed-off-by: ericharper <complex451@gmail.com>

* revert diff

Signed-off-by: ericharper <complex451@gmail.com>

* revert diff

Signed-off-by: ericharper <complex451@gmail.com>

* trainer grad acc must be 1

Signed-off-by: ericharper <complex451@gmail.com>

* set O2 recipe options automatically

Signed-off-by: ericharper <complex451@gmail.com>

* move require backward grad sync under conditional

Signed-off-by: ericharper <complex451@gmail.com>

* add comment, fix typo

Signed-off-by: ericharper <complex451@gmail.com>

* set find unused to false in bert

Signed-off-by: ericharper <complex451@gmail.com>

* find unused false

Signed-off-by: ericharper <complex451@gmail.com>

* cleanup

Signed-off-by: ericharper <complex451@gmail.com>

* revert

Signed-off-by: ericharper <complex451@gmail.com>

* remove config check

Signed-off-by: ericharper <complex451@gmail.com>

* remove comment

Signed-off-by: ericharper <complex451@gmail.com>

* add pipeline parallel jenkins test

Signed-off-by: ericharper <complex451@gmail.com>

* update jenkins tests

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* add torch.distributed init

Signed-off-by: ericharper <complex451@gmail.com>

* update grad_scaler arg

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>
Co-authored-by: Jason <jasoli@nvidia.com>
Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>
Co-authored-by: vadam5 <78445382+vadam5@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-02-08T03:22:47Z,"Add NeMo version to all new .nemo files (#3605)

* Add NeMo version to all new .nemo files

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add escape clause for data classes

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-01-31T22:20:29Z,"Final merge r1.6.0 main (#3570)

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* Fix the tutorial notebooks bug (#3465)

* fix checkpoint loading and model config file

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix style

Signed-off-by: Yi Dong <yidong@nvidia.com>

* Fix checkpoint converter in O2 style (#3486)

* Fix checkpoint converter in O2 style

Signed-off-by: Yu Yao <yuya@nvidia.com>

* Fix style

Signed-off-by: Yu Yao <yuya@nvidia.com>

Co-authored-by: Yu Yao <yuya@nvidia.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* Remove pickled features from tarred dataset (#3491)

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

Co-authored-by: ekmb <ebakhturina@nvidia.com>

* adding missing init files (#3505)

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* typos (#3504)

* typos

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* link fix

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* update titanet conf (#3507)

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* Fix link to NGC page for ASR (#3512)

Signed-off-by: smajumdar <titu1994@gmail.com>

* vad typo fix (#3490)

* remove always broken ptl link

Signed-off-by: fayejf <fayejf07@gmail.com>

* fix typo

Signed-off-by: fayejf <fayejf07@gmail.com>

* Add verification helper function and update docs (#3514)

* Add verification helper function and update docs

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* typo fixes

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* fixed the num_classes bug of conv decoder. (#3525)

* fixed the num_classes bug.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* added logging info.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* Enforce utf-8 on all file r/w (#3520)

* Update paths to subtask

Signed-off-by: smajumdar <titu1994@gmail.com>

* Enforce utf-8 on all file r/w

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Eric Harper <complex451@gmail.com>

* Fixed section typo (#3522)

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Pushing updated WFST Tutorial to r1.6.0 (#3521)

Signed-off-by: tbartley94 <tbartley@nvidia.com>

Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>

* Fixed duplicate cell bug (#3518)

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* WFST tutorial update (#3531)

* Pushing updated WFST Tutorial to r1.6.0

Signed-off-by: tbartley94 <tbartley@nvidia.com>

* Hopefully final corrections to WFST tutorials.

Signed-off-by: tbartley94 <tbartley@nvidia.com>

* [TTS] Fix bug in inference tts notebook (#3532)

* fix bug in inference tts notebook

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* Update Inference_ModelSelect.ipynb

* fix space

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* remove space

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* Prompt tuning documentation (#3541)

* Started prompt tuning doc

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Update prompt_tuning.rst

* Update models.rst

* Update models.rst

* Update and rename megatron_finetuning.rst to megatron_downstream_tasks.rst

* Update intro.rst

* Update intro.rst

* Update and rename megatron_downstream_tasks.rst to megatron_finetuning.rst

* Update megatron_finetuning.rst

* Delete prompt_tuning.rst

* Update README.rst

* Update docs/source/nlp/megatron_finetuning.rst

Co-authored-by: Eric Harper <complex451@gmail.com>

* Fix nmt resume (#3539)

* check for model attr

Signed-off-by: ericharper <complex451@gmail.com>

* update jenkins test

Signed-off-by: ericharper <complex451@gmail.com>

* TN bug fix (#3538)

* ve and cel fixes

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* ve and cel fixes

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* add w to single digit roman and cardinal single digit graph (non det)

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* isn't fix

Signed-off-by: ekmb <ebakhturina@nvidia.com>

Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* fix bug in tutorial (#3546)

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* Update nvidia container check (#3535)

* update nvidia container check

Signed-off-by: ericharper <complex451@gmail.com>

* update minor version

Signed-off-by: ericharper <complex451@gmail.com>

* add check to T5

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* update bert

Signed-off-by: ericharper <complex451@gmail.com>

* forgot import

Signed-off-by: ericharper <complex451@gmail.com>

* remove import

Signed-off-by: ericharper <complex451@gmail.com>

* Fix an issue with wandb not displaying updated config changes (#3552)

Signed-off-by: smajumdar <titu1994@gmail.com>

* remove extra instance (#3551)

Signed-off-by: ericharper <complex451@gmail.com>

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* update package info

Signed-off-by: ericharper <complex451@gmail.com>

* revert

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Yi Dong <43824965+yidong72@users.noreply.github.com>
Co-authored-by: yaoyu-33 <54727607+yaoyu-33@users.noreply.github.com>
Co-authored-by: Yu Yao <yuya@nvidia.com>
Co-authored-by: PeganovAnton <peganoff2@mail.ru>
Co-authored-by: ekmb <ebakhturina@nvidia.com>
Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>
Co-authored-by: Evelina <10428420+ekmb@users.noreply.github.com>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>
Co-authored-by: Vahid Noroozi <VahidooX@users.noreply.github.com>
Co-authored-by: Virginia Adams <78445382+vadam5@users.noreply.github.com>
Co-authored-by: tbartley94 <90423858+tbartley94@users.noreply.github.com>
Co-authored-by: Oktai Tatanov <oktai.tatanov@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-01-27T20:23:50Z,"Fix bug in multi-checkpoint loading (#3536)

* fix

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* check for both

Signed-off-by: sam1373 <samuelkriman@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-01-27T18:47:49Z,"Added P-Tuning method  (#3488)

* init checking of p-tune method

Signed-off-by: Yi Dong <yidong@nvidia.com>

* training is working

Signed-off-by: Yi Dong <yidong@nvidia.com>

* refactor to seperate prediction and loss computation

Signed-off-by: Yi Dong <yidong@nvidia.com>

* updated the notebook

Signed-off-by: Yi Dong <yidong@nvidia.com>

* match the original hyper parameters

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fixed the loss bug

Signed-off-by: Yi Dong <yidong@nvidia.com>

* better scheduler

Signed-off-by: Yi Dong <yidong@nvidia.com>

* notebook runs

Signed-off-by: Yi Dong <yidong@nvidia.com>

* added neural types

Signed-off-by: Yi Dong <yidong@nvidia.com>

* updated the doc

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fixed the notebook

Signed-off-by: Yi Dong <yidong@nvidia.com>

* updated expected result

Signed-off-by: Yi Dong <yidong@nvidia.com>

* added accuracy

Signed-off-by: Yi Dong <yidong@nvidia.com>

* style fix

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix reassgin

Signed-off-by: Yi Dong <yidong@nvidia.com>

* log accuracy

Signed-off-by: Yi Dong <yidong@nvidia.com>

* load the best checkpoint

Signed-off-by: Yi Dong <yidong@nvidia.com>

* address PR comments

Signed-off-by: Yi Dong <yidong@nvidia.com>

* added ci test

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fixed max_step calculation error due to wrong number of workers

Signed-off-by: Yi Dong <yidong@nvidia.com>

* add import guard for nlp plugin

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fixed the metric report issue when using tensor parallel

Signed-off-by: Yi Dong <yidong@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-01-26T17:44:33Z,"Self-supervised tutorial & update (#3344)

* update

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* update

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* version test

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* version test

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* image for tutorial

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* enc_final in model_defaults

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* enc_final in model_defaults

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* self-supervised tutorial

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* fix

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* fix

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* contextnet ssl config

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* remove test_ds from config

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* update recon decoder

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* don't save -last if val_loss is nan

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* check if val_loss is there

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* keep entries from same file together when tarring

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* keep entries from same file together when tarring

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* print num of files in shard

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* update

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* style

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* moving configs, add docstrings

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* tutorial updates

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* update test

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* update loading

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* update loading

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* update loading

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* update loading

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* fix

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* fix

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* citrinet configs

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* citrinet configs update

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* update

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* default include all

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* comments

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* docstring hydra example

Signed-off-by: sam1373 <samuelkriman@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-01-19T20:41:47Z,"fix/machine translation/minor bug in passing variables (#2954)

* Fix minor bug

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix minor bug

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

Co-authored-by: Aleksey Grinchuk (Oleksii Hrinchuk) <grinchuk.alexey@gmail.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2022-01-07T20:27:12Z,"TalkNet Fix (#3092)

* Fix 'model' key and object collisionn.

Signed-off-by: Stanislav Beliaev <stasbelyaev96@gmail.com>

* Rename model config node to encoder.

Signed-off-by: Stanislav Beliaev <stasbelyaev96@gmail.com>

* Remove line adding.

Signed-off-by: Stanislav Beliaev <stasbelyaev96@gmail.com>

* Return TalkNet to README.

Signed-off-by: Stanislav Beliaev <stasbelyaev96@gmail.com>

* Add model.model test and remove some stuff.

Signed-off-by: Stanislav Beliaev <stasbelyaev96@gmail.com>

* Add moAdd typing to TalkNet.

Signed-off-by: Stanislav Beliaev <stasbelyaev96@gmail.com>

* TalkNet Training notebook fixed.

Signed-off-by: Stanislav Beliaev <stasbelyaev96@gmail.com>

Co-authored-by: Oktai Tatanov <oktai.tatanov@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-11-19T23:56:05Z,"patch comegaconf for cfg (#3224)

Signed-off-by: fayejf <fayejf07@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-11-12T06:25:47Z,"Merge r1.5.0 bugfixes to main (#3173)

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* Always save last checkpoint on train end even if folder does not exist (#2976)

* add fix for no checkpoint folder when training ends

Signed-off-by: Jason <jasoli@nvidia.com>

* update

Signed-off-by: Jason <jasoli@nvidia.com>

* fix test

Signed-off-by: Jason <jasoli@nvidia.com>

* fixes

Signed-off-by: Jason <jasoli@nvidia.com>

* typo

Signed-off-by: Jason <jasoli@nvidia.com>

* change check

Signed-off-by: Jason <jasoli@nvidia.com>

* [NLP] Add Apex import guard (#3041)

* add apex import guard

Signed-off-by: ericharper <complex451@gmail.com>

* add apex import guard

Signed-off-by: ericharper <complex451@gmail.com>

* add apex import guard

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* remove from init add logging to constructor

Signed-off-by: ericharper <complex451@gmail.com>

* remove from init add logging to constructor

Signed-off-by: ericharper <complex451@gmail.com>

* remove import from init

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert encoder logic from NLPModel

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert from init

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert from init

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert from init

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert from init

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert from init

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert from init

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* Update reinstall and cherry-pick bignlp commits (#3065)

* add ptl install to reinstall and update jenkins install

Signed-off-by: ericharper <complex451@gmail.com>

* Add a stateless timer to specify max_time per run instead of global m‚Ä¶ (#3056)

* Add a stateless timer to specify max_time per run instead of global max_time across runs

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* (1) reduce the validation loss within a epoch, (2) convert global-batch-based iteartion counts to micro-batch-based (#3055)

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>

* Timer class monitors total time (train + validation + testing) to monitor when to end training (#3061)

* Check total time in train/validation to exit

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>

* Add PUBLICATIONS.md (#3051)

* Add PUBLICATIONS.md

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add NLP

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update PUBLICATIONS.md

* Update PUBLICATIONS.md

* Fix links

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Eric Harper <complex451@gmail.com>

* fix uninstall

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>

* fix File Load Error (#3069)

Signed-off-by: fayejf <fayejf07@gmail.com>

Co-authored-by: Eric Harper <complex451@gmail.com>

* Exp manager small refactor (#3067)

* Exp manager small refactor

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* move super() call earlier in the function

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>

* update (#3113)

Signed-off-by: Jason <jasoli@nvidia.com>

* Upgrade to PTL 1.5.0 (#3127)

* update for ptl 1.5.0

Signed-off-by: ericharper <complex451@gmail.com>

* update trainer config

Signed-off-by: ericharper <complex451@gmail.com>

* limit cuda visible devices to the first two gpus on check for ranks CI test

Signed-off-by: ericharper <complex451@gmail.com>

* remove comments

Signed-off-by: ericharper <complex451@gmail.com>

* make datasets larger for test

Signed-off-by: ericharper <complex451@gmail.com>

* make datasets larger for test

Signed-off-by: ericharper <complex451@gmail.com>

* update compute_max_steps

Signed-off-by: ericharper <complex451@gmail.com>

* update compute_max_steps

Signed-off-by: ericharper <complex451@gmail.com>

* Update a few packages' version (#3134)

* update torchaudio lower bound in tutorials

Signed-off-by: fayejf <fayejf07@gmail.com>

* update torchtext version in tutorials

Signed-off-by: fayejf <fayejf07@gmail.com>

* Revert ""update torchtext version in tutorials""

This reverts commit 9f7f660f3b0926d3b7d6cdbb92751e0d881d65f3.

* update torchtext in tutorials

Signed-off-by: fayejf <fayejf07@gmail.com>

* update packages version in this notebook

Signed-off-by: fayejf <fayejf07@gmail.com>

* update numba version in tutorial

Signed-off-by: fayejf <fayejf07@gmail.com>

* update requirements

Signed-off-by: fayejf <fayejf07@gmail.com>

* update package version in Dockerfile

Signed-off-by: fayejf <fayejf07@gmail.com>

* Revert ""update torchtext version in tutorials""

This reverts commit 9f7f660f3b0926d3b7d6cdbb92751e0d881d65f3.

Signed-off-by: fayejf <fayejf07@gmail.com>

* added to avail models (#3044)

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* 1. Fixed warmup batches when no timing is measured. (#3140)

Signed-off-by: Micha Livne <mlivne@nvidia.com>

Co-authored-by: Micha Livne <mlivne@nvidia.com>
Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>

* Cannot unset both `source_processor` and `target_processor` for NMT model inference. (#3136)

* Fix processor setting for nmt inference

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add ignore option description to argparse

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix typo

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>

* config link fixes (#3148)

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* fix nemo checkpoint converter (#3149)

Signed-off-by: Yu Yao <yuya@nvidia.com>

Co-authored-by: Yu Yao <yuya@nvidia.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* link fix (#3153)

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* remove (#3154)

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* [BigNLP] Add CI Tests for Megatron GPT (#3124)

* add pretrain CI test

Signed-off-by: ericharper <complex451@gmail.com>

* update test

Signed-off-by: ericharper <complex451@gmail.com>

* update test

Signed-off-by: ericharper <complex451@gmail.com>

* update test

Signed-off-by: ericharper <complex451@gmail.com>

* add .cpp and Makefile to python package_data

Signed-off-by: ericharper <complex451@gmail.com>

* check for local_rank and barrier when compiling helper

Signed-off-by: ericharper <complex451@gmail.com>

* check for local_rank and barrier when compiling helper

Signed-off-by: ericharper <complex451@gmail.com>

* remove comment

Signed-off-by: ericharper <complex451@gmail.com>

* remove comment

Signed-off-by: ericharper <complex451@gmail.com>

* add device, have jenkins test use fp16

Signed-off-by: ericharper <complex451@gmail.com>

* add device, have jenkins test use fp16

Signed-off-by: ericharper <complex451@gmail.com>

* update app_state.local_rank

Signed-off-by: ericharper <complex451@gmail.com>

* update config

Signed-off-by: ericharper <complex451@gmail.com>

* add configure gradient clipping hook

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* add configure gradient clipping hook

Signed-off-by: ericharper <complex451@gmail.com>

* add resume test

Signed-off-by: ericharper <complex451@gmail.com>

* add eval

Signed-off-by: ericharper <complex451@gmail.com>

* add eval

Signed-off-by: ericharper <complex451@gmail.com>

* (1) update auto-casting api, (2) simplifiy precision arguments and remove redundancy (#3142)

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* only set jit fusion options if we are in 21.10 container

Signed-off-by: ericharper <complex451@gmail.com>

* update arg

Signed-off-by: ericharper <complex451@gmail.com>

* update nemo file for eval test

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* move tests down in the jenkinsfile

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* removed fused arg

Signed-off-by: ericharper <complex451@gmail.com>

* interpolate precision and sequence length in config

Signed-off-by: ericharper <complex451@gmail.com>

* remove trainer.precision interpolation

Signed-off-by: ericharper <complex451@gmail.com>

* remove nemo_experiments after prallel stage

Signed-off-by: ericharper <complex451@gmail.com>

* remove rm call

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Sangkug Lym <slym@nvidia.com>

* Fix the asr bucketing parsing the manifest. (#3089)

* Patch LR max_step computation check (#3152)

* Patch LR max_step computation check

Signed-off-by: smajumdar <titu1994@gmail.com>

* Perform < 0 check only

Signed-off-by: smajumdar <titu1994@gmail.com>

* FastPitch Notebook Bugfix (#3161)

* update notebook

Signed-off-by: Jason <jasoli@nvidia.com>

* update test

Signed-off-by: Jason <jasoli@nvidia.com>

* [BugFix] Fix check for model parallel size in TP 1 case (#3162)

* check model parallel size is greater than one before injection

Signed-off-by: ericharper <complex451@gmail.com>

* check model parallel size > 1

Signed-off-by: ericharper <complex451@gmail.com>

* check model parallel size is greater than one before injection

Signed-off-by: ericharper <complex451@gmail.com>

* check model parallel size is greater than one before injection

Signed-off-by: ericharper <complex451@gmail.com>

* tp 1 ckpt conv

Signed-off-by: ericharper <complex451@gmail.com>

* Notebook bugfixes (#3165)

* zero shot intent slot notebook bug fix

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Added megatronBERT not support in this release message to notebook

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* added apex backend to config

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Removed apex from entity linking and added 1.4.0 nemo info to warning

Co-authored-by: Eric Harper <complex451@gmail.com>

* add note that megatron bert is supported in r1.5.0 (#3169)

Signed-off-by: ericharper <complex451@gmail.com>

* Text2sparql fix 1.5.0 (#3166)

* Model neural type changes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Neural types fix

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix typo

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove ipdb

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

Co-authored-by: Eric Harper <complex451@gmail.com>

* Fix cfg issue for r1.5.0 (#3170)

* fix cfg due to ptl updated

Signed-off-by: fayejf <fayejf07@gmail.com>

* fix

Signed-off-by: fayejf <fayejf07@gmail.com>

* style fix

Signed-off-by: fayejf <fayejf07@gmail.com>

Co-authored-by: Eric Harper <complex451@gmail.com>

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* remove cfg

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Jason <jasoli@nvidia.com>
Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>
Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>
Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>
Co-authored-by: Micha Livne <mlivne@nvidia.com>
Co-authored-by: PeganovAnton <peganoff2@mail.ru>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Co-authored-by: yaoyu-33 <54727607+yaoyu-33@users.noreply.github.com>
Co-authored-by: Yu Yao <yuya@nvidia.com>
Co-authored-by: Evelina <10428420+ekmb@users.noreply.github.com>
Co-authored-by: Vahid Noroozi <VahidooX@users.noreply.github.com>
Co-authored-by: vadam5 <78445382+vadam5@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-11-10T23:33:11Z,"Self-supervised pre-training for speech models (#3139)

* self-supervised training

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* test

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* remove imports

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* fix

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* sort imports

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* fix audio_to_text

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* manifest handle no text

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* loss init

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* style

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* remove tokenizer from config

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* config changes

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* remove hydra import

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* always spec augment

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* fixes

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* copyright

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* fix cosine sim

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* fix cosine sim

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* fix cosine sim

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* changes based on comments

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* changes based on comments

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* configs

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* name fix

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* ci config changes

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* renamed to num_negatives

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* minor changes

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* name changes, type annotations

Signed-off-by: sam1373 <samuelkriman@gmail.com>

Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-11-04T16:26:58Z,"Merge r1.5.0 bugfixes and doc updates to main (#3133)

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* Always save last checkpoint on train end even if folder does not exist (#2976)

* add fix for no checkpoint folder when training ends

Signed-off-by: Jason <jasoli@nvidia.com>

* update

Signed-off-by: Jason <jasoli@nvidia.com>

* fix test

Signed-off-by: Jason <jasoli@nvidia.com>

* fixes

Signed-off-by: Jason <jasoli@nvidia.com>

* typo

Signed-off-by: Jason <jasoli@nvidia.com>

* change check

Signed-off-by: Jason <jasoli@nvidia.com>

* [NLP] Add Apex import guard (#3041)

* add apex import guard

Signed-off-by: ericharper <complex451@gmail.com>

* add apex import guard

Signed-off-by: ericharper <complex451@gmail.com>

* add apex import guard

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* remove from init add logging to constructor

Signed-off-by: ericharper <complex451@gmail.com>

* remove from init add logging to constructor

Signed-off-by: ericharper <complex451@gmail.com>

* remove import from init

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert encoder logic from NLPModel

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert from init

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert from init

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert from init

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert from init

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert from init

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert from init

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* Exp manager small refactor (#3067)

* Exp manager small refactor

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* move super() call earlier in the function

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>

* Change container (#3087)

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Eric Harper <complex451@gmail.com>

* Training of machine translation model fails if config parameter `trainer.max_epochs` is used instead of `trainer.max_steps`. (#3112)

* fix: replace distributed_backend for accelarator

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add debug script

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove debug script

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* update (#3113)

Signed-off-by: Jason <jasoli@nvidia.com>

* Fix: punctuation capitalization inference on short queries (#3111)

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

Co-authored-by: Eric Harper <complex451@gmail.com>

* Multiple ASR Fixes to SPE tokenization (#3119)

* Reduce num workers for transcribe

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix SPE tokenizer vocabulary construction

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update tokenizer building script

Signed-off-by: smajumdar <titu1994@gmail.com>

* Remove logs

Signed-off-by: smajumdar <titu1994@gmail.com>

* Megatron GPT training in BCP (#3095)

* BCP megatron training

Signed-off-by: madhukar <madhukar@penguin>

* Add quotes

Signed-off-by: madhukar <madhukar@penguin>

* Style fix

Signed-off-by: madhukar <madhukar@penguin>

Co-authored-by: madhukar <madhukar@penguin>

* Upgrade to PTL 1.5.0 (#3127)

* update for ptl 1.5.0

Signed-off-by: ericharper <complex451@gmail.com>

* update trainer config

Signed-off-by: ericharper <complex451@gmail.com>

* limit cuda visible devices to the first two gpus on check for ranks CI test

Signed-off-by: ericharper <complex451@gmail.com>

* remove comments

Signed-off-by: ericharper <complex451@gmail.com>

* make datasets larger for test

Signed-off-by: ericharper <complex451@gmail.com>

* make datasets larger for test

Signed-off-by: ericharper <complex451@gmail.com>

* update compute_max_steps

Signed-off-by: ericharper <complex451@gmail.com>

* update compute_max_steps

Signed-off-by: ericharper <complex451@gmail.com>

* update package info

Signed-off-by: ericharper <complex451@gmail.com>

* remove duplicate code

Signed-off-by: ericharper <complex451@gmail.com>

* remove comment

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Jason <jasoli@nvidia.com>
Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: PeganovAnton <peganoff2@mail.ru>
Co-authored-by: Madhukar K <26607911+madhukarkm@users.noreply.github.com>
Co-authored-by: madhukar <madhukar@penguin>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-10-30T02:15:37Z,"Merge r1.5.0 bugfixes and doc updates to main (#3093)

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* Fix quantization bug in Asr (#3062)

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update reinstall and cherry-pick bignlp commits (#3065)

* add ptl install to reinstall and update jenkins install

Signed-off-by: ericharper <complex451@gmail.com>

* Add a stateless timer to specify max_time per run instead of global m‚Ä¶ (#3056)

* Add a stateless timer to specify max_time per run instead of global max_time across runs

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* (1) reduce the validation loss within a epoch, (2) convert global-batch-based iteartion counts to micro-batch-based (#3055)

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>

* Timer class monitors total time (train + validation + testing) to monitor when to end training (#3061)

* Check total time in train/validation to exit

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>

* Add PUBLICATIONS.md (#3051)

* Add PUBLICATIONS.md

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add NLP

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update PUBLICATIONS.md

* Update PUBLICATIONS.md

* Fix links

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Eric Harper <complex451@gmail.com>

* fix uninstall

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>

* fix File Load Error (#3069)

Signed-off-by: fayejf <fayejf07@gmail.com>

Co-authored-by: Eric Harper <complex451@gmail.com>

* Update hyper parameter saving (#3058)

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Eric Harper <complex451@gmail.com>

* Exp manager small refactor (#3067)

* Exp manager small refactor

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* move super() call earlier in the function

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>

* Fix FastPitch Pitch Duration Notebook (#3068)

* bugfix

Signed-off-by: Jason <jasoli@nvidia.com>

* bugfix2

Signed-off-by: Jason <jasoli@nvidia.com>

* better check

Signed-off-by: Jason <jasoli@nvidia.com>

* confusionmatrix (#3085)

Signed-off-by: fayejf <fayejf07@gmail.com>

* typo and fix link (#3086)

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* inf cross-checking across tensor-parallel ranks (#3088)

* inf cross-checking across tensor-parallel ranks

* sylte

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Eric Harper <complex451@gmail.com>

* Fix save top k (#3075)

* inject mp_rank for checkpoint paths in NLPDDPPlugin

Signed-off-by: ericharper <complex451@gmail.com>

* == instead of i

Signed-off-by: ericharper <complex451@gmail.com>

* when checking previous run account for mp

Signed-off-by: ericharper <complex451@gmail.com>

* uninject mp ranks when needed

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>
Co-authored-by: Jason <jasoli@nvidia.com>
Co-authored-by: Evelina <10428420+ekmb@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-10-27T18:13:29Z,"Merge r1.5.0 bugfixes and doc updates to main (#3048)

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* Always save last checkpoint on train end even if folder does not exist (#2976)

* add fix for no checkpoint folder when training ends

Signed-off-by: Jason <jasoli@nvidia.com>

* update

Signed-off-by: Jason <jasoli@nvidia.com>

* fix test

Signed-off-by: Jason <jasoli@nvidia.com>

* fixes

Signed-off-by: Jason <jasoli@nvidia.com>

* typo

Signed-off-by: Jason <jasoli@nvidia.com>

* change check

Signed-off-by: Jason <jasoli@nvidia.com>

* [NLP] Add Apex import guard (#3041)

* add apex import guard

Signed-off-by: ericharper <complex451@gmail.com>

* add apex import guard

Signed-off-by: ericharper <complex451@gmail.com>

* add apex import guard

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* remove from init add logging to constructor

Signed-off-by: ericharper <complex451@gmail.com>

* remove from init add logging to constructor

Signed-off-by: ericharper <complex451@gmail.com>

* remove import from init

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert encoder logic from NLPModel

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert from init

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert from init

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert from init

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert from init

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert from init

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert from init

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Jason <jasoli@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-10-21T03:06:37Z,"[BigNLP] Merge Megatron GPT to main (#2975)

* fix gpu init after removing debug print in mpu

Signed-off-by: ericharper <complex451@gmail.com>

* add fused_adam

Signed-off-by: ericharper <complex451@gmail.com>

* check ds is not none before logging len

Signed-off-by: ericharper <complex451@gmail.com>

* set fp16 arg to true and fix enum conflict

Signed-off-by: ericharper <complex451@gmail.com>

* make fp16 arg configurable

Signed-off-by: ericharper <complex451@gmail.com>

* add grad clip from megatron

Signed-off-by: ericharper <complex451@gmail.com>

* Linear warmup with cosine annealing and constant holding (#2846)

* Testing cosine schedule

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* More fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* update config for constant steps in schedule

Signed-off-by: ericharper <complex451@gmail.com>

* temporarily import enum from megatron

Signed-off-by: ericharper <complex451@gmail.com>

* add grad clip for fp32

Signed-off-by: ericharper <complex451@gmail.com>

* update check for _del_model_without_trainer

Signed-off-by: ericharper <complex451@gmail.com>

* updating restore for model parallel

Signed-off-by: ericharper <complex451@gmail.com>

* add predict script

Signed-off-by: ericharper <complex451@gmail.com>

* update test iters

Signed-off-by: ericharper <complex451@gmail.com>

* add barrier

Signed-off-by: ericharper <complex451@gmail.com>

* return if clip_val is 0 or None

Signed-off-by: ericharper <complex451@gmail.com>

* when using amp clip grads after they are unscaled

Signed-off-by: ericharper <complex451@gmail.com>

* make native amp scaler hyperparams configurable

Signed-off-by: ericharper <complex451@gmail.com>

* (1) nvfuser, (2) amp-casting decoration (#2894)

* (1) nvfuser, (2) amp-casting decoration

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* support bf16

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* update package info

Signed-off-by: ericharper <complex451@gmail.com>

* add set device to constructor

Signed-off-by: ericharper <complex451@gmail.com>

* set_device in constructor

Signed-off-by: ericharper <complex451@gmail.com>

* [BigNLP] Remove megatron-lm dependency. (#2910)

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* add load_fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* add load_fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* update megatron_init

Signed-off-by: ericharper <complex451@gmail.com>

* add fused kernels

Signed-off-by: ericharper <complex451@gmail.com>

* add fused kernels

Signed-off-by: ericharper <complex451@gmail.com>

* update process batch

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* add megatron clip_grad

Signed-off-by: ericharper <complex451@gmail.com>

* trying to resolve circular import error

Signed-off-by: ericharper <complex451@gmail.com>

* rename file

Signed-off-by: ericharper <complex451@gmail.com>

* remove non-gpt models and datasets from __init__ files

Signed-off-by: ericharper <complex451@gmail.com>

* set device in constructorfor gpu init

Signed-off-by: ericharper <complex451@gmail.com>

* set device in constructorfor gpu init

Signed-off-by: ericharper <complex451@gmail.com>

* set_device in constructor

Signed-off-by: ericharper <complex451@gmail.com>

* clean config

Signed-off-by: ericharper <complex451@gmail.com>

* update MegatronDataset

Signed-off-by: ericharper <complex451@gmail.com>

* clean up MegatronModule

Signed-off-by: ericharper <complex451@gmail.com>

* clean up MegatronModule

Signed-off-by: ericharper <complex451@gmail.com>

* rename fp16 and bf16 flags to fused_softmax_input_in_fp16/bf16

Signed-off-by: ericharper <complex451@gmail.com>

* rename to fused_fp16

Signed-off-by: ericharper <complex451@gmail.com>

* add fused_fp16 arg to LayerNorm calls

Signed-off-by: ericharper <complex451@gmail.com>

* fix arg name

Signed-off-by: ericharper <complex451@gmail.com>

* fix arg name

Signed-off-by: ericharper <complex451@gmail.com>

* fix import

Signed-off-by: ericharper <complex451@gmail.com>

* update arg

Signed-off-by: ericharper <complex451@gmail.com>

* skip warmup default to True

Signed-off-by: ericharper <complex451@gmail.com>

* skip warmup default to True

Signed-off-by: ericharper <complex451@gmail.com>

* Adding complete method to MegatronGPTModel (#2935)

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* make ffn_hidden_size mandatory

Signed-off-by: ericharper <complex451@gmail.com>

* Manually migrating timing of step into branch (#2937)

* 1. Manually migrating timing of step into branch.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Updated file name and content.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Updated to latest code.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

Co-authored-by: Micha Livne <mlivne@nvidia.com>

* remove unused imports

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* check fused_fp16 and fused_bf16 are not both True

Signed-off-by: ericharper <complex451@gmail.com>

* update predict script for model parallel .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>
Co-authored-by: Micha Livne <mlivne@nvidia.com>

* NVfuser (#2943)

* activation checkpoint recompute

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* selective nvfuser setup

* Megatron gpt bfloat support (#2926)

* Save/restore fix

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Another merge

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Bf16 args in init

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Set precision

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove debug stuff

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* add bf16 casting decorator

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* Bfloat layernorm propagation

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* activation checkpoint recompute

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* selective nvfuser setup

* More arg removal

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove BERTDataset

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* update to latest apex and patch transformer autocast

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: ericharper <complex451@gmail.com>

* don't set jit for bf16

Signed-off-by: ericharper <complex451@gmail.com>

* replace apex.mpu

Signed-off-by: ericharper <complex451@gmail.com>

* fix grad clip

Signed-off-by: ericharper <complex451@gmail.com>

* NVFuser fixes (#2951)

* Fuser fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove dummy handler

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove PTL plugin based logic for fusion

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* remove duplicated file

Signed-off-by: ericharper <complex451@gmail.com>

* typo (#2960)

Signed-off-by: ericharper <complex451@gmail.com>

* [BigNLP] Script to convert GPT checkpoint to .nemo (#2958)

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* add load_fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* add load_fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* update megatron_init

Signed-off-by: ericharper <complex451@gmail.com>

* add fused kernels

Signed-off-by: ericharper <complex451@gmail.com>

* add fused kernels

Signed-off-by: ericharper <complex451@gmail.com>

* update process batch

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* add megatron clip_grad

Signed-off-by: ericharper <complex451@gmail.com>

* trying to resolve circular import error

Signed-off-by: ericharper <complex451@gmail.com>

* rename file

Signed-off-by: ericharper <complex451@gmail.com>

* remove non-gpt models and datasets from __init__ files

Signed-off-by: ericharper <complex451@gmail.com>

* set device in constructorfor gpu init

Signed-off-by: ericharper <complex451@gmail.com>

* set device in constructorfor gpu init

Signed-off-by: ericharper <complex451@gmail.com>

* set_device in constructor

Signed-off-by: ericharper <complex451@gmail.com>

* clean config

Signed-off-by: ericharper <complex451@gmail.com>

* update MegatronDataset

Signed-off-by: ericharper <complex451@gmail.com>

* clean up MegatronModule

Signed-off-by: ericharper <complex451@gmail.com>

* clean up MegatronModule

Signed-off-by: ericharper <complex451@gmail.com>

* rename fp16 and bf16 flags to fused_softmax_input_in_fp16/bf16

Signed-off-by: ericharper <complex451@gmail.com>

* rename to fused_fp16

Signed-off-by: ericharper <complex451@gmail.com>

* add fused_fp16 arg to LayerNorm calls

Signed-off-by: ericharper <complex451@gmail.com>

* fix arg name

Signed-off-by: ericharper <complex451@gmail.com>

* fix arg name

Signed-off-by: ericharper <complex451@gmail.com>

* fix import

Signed-off-by: ericharper <complex451@gmail.com>

* update arg

Signed-off-by: ericharper <complex451@gmail.com>

* skip warmup default to True

Signed-off-by: ericharper <complex451@gmail.com>

* skip warmup default to True

Signed-off-by: ericharper <complex451@gmail.com>

* Adding complete method to MegatronGPTModel (#2935)

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* make ffn_hidden_size mandatory

Signed-off-by: ericharper <complex451@gmail.com>

* Manually migrating timing of step into branch (#2937)

* 1. Manually migrating timing of step into branch.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Updated file name and content.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Updated to latest code.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

Co-authored-by: Micha Livne <mlivne@nvidia.com>

* remove unused imports

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* check fused_fp16 and fused_bf16 are not both True

Signed-off-by: ericharper <complex451@gmail.com>

* update predict script for model parallel .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* add script to convert .ckpt to .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* in progress

Signed-off-by: ericharper <complex451@gmail.com>

* update

Signed-off-by: ericharper <complex451@gmail.com>

* convert mp checkpoints to nemo

Signed-off-by: ericharper <complex451@gmail.com>

* update help

Signed-off-by: ericharper <complex451@gmail.com>

* add safeguard for model parallel save_to

Signed-off-by: ericharper <complex451@gmail.com>

* adjust NLPModel save_to to be safer for model parallel

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>
Co-authored-by: Micha Livne <mlivne@nvidia.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* [BigNLP] Update GPT evaluation to work with tensor model parallel  (#2959)

* in progress

Signed-off-by: ericharper <complex451@gmail.com>

* update args

Signed-off-by: ericharper <complex451@gmail.com>

* add request dataset

Signed-off-by: ericharper <complex451@gmail.com>

* tokenize request

Signed-off-by: ericharper <complex451@gmail.com>

* in progress

Signed-off-by: ericharper <complex451@gmail.com>

* able to run

Signed-off-by: ericharper <complex451@gmail.com>

* reduce logits

Signed-off-by: ericharper <complex451@gmail.com>

* capture response

Signed-off-by: ericharper <complex451@gmail.com>

* squeeze and unsqueeze

Signed-off-by: ericharper <complex451@gmail.com>

* handle non model parallel case

Signed-off-by: ericharper <complex451@gmail.com>

* clean imports

Signed-off-by: ericharper <complex451@gmail.com>

* add file

Signed-off-by: ericharper <complex451@gmail.com>

* convert logits to log_probs

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* rename logits to log_probs

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* add megatron gpt pretraining

Signed-off-by: ericharper <complex451@gmail.com>

* add megatron gpt pretraining

Signed-off-by: ericharper <complex451@gmail.com>

* add megatron gpt pretraining

Signed-off-by: ericharper <complex451@gmail.com>

* updating to work with latest megatron

Signed-off-by: ericharper <complex451@gmail.com>

* updating to work with latest megatron

Signed-off-by: ericharper <complex451@gmail.com>

* update _del_model

Signed-off-by: ericharper <complex451@gmail.com>

* adding gpt model

Signed-off-by: ericharper <complex451@gmail.com>

* adding gpt model

Signed-off-by: ericharper <complex451@gmail.com>

* adding gpt model

Signed-off-by: ericharper <complex451@gmail.com>

* instantiate GPTmodel

Signed-off-by: ericharper <complex451@gmail.com>

* adding build dataset

Signed-off-by: ericharper <complex451@gmail.com>

* build megatron dataset in .setup

Signed-off-by: ericharper <complex451@gmail.com>

* setup dataloader

Signed-off-by: ericharper <complex451@gmail.com>

* add vocab_file and merge_file to megatron init

Signed-off-by: ericharper <complex451@gmail.com>

* add forward

Signed-off-by: ericharper <complex451@gmail.com>

* add train loss

Signed-off-by: ericharper <complex451@gmail.com>

* add optimizer

Signed-off-by: ericharper <complex451@gmail.com>

* add exp_manager

Signed-off-by: ericharper <complex451@gmail.com>

* multi-gpu is working

Signed-off-by: ericharper <complex451@gmail.com>

* adding val loop

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* adding val loop

Signed-off-by: ericharper <complex451@gmail.com>

* fix ranks

Signed-off-by: ericharper <complex451@gmail.com>

* fix model parallel checkpoint saving

Signed-off-by: ericharper <complex451@gmail.com>

* fix _del_model

Signed-off-by: ericharper <complex451@gmail.com>

* added megatron batch sampler

Signed-off-by: ericharper <complex451@gmail.com>

* try to fix num steps

Signed-off-by: ericharper <complex451@gmail.com>

* add wandb to config

Signed-off-by: ericharper <complex451@gmail.com>

* log lr

Signed-off-by: ericharper <complex451@gmail.com>

* add warmup ratio to config

Signed-off-by: ericharper <complex451@gmail.com>

* update configs

Signed-off-by: ericharper <complex451@gmail.com>

* update configs

Signed-off-by: ericharper <complex451@gmail.com>

* add cpu init to args

Signed-off-by: ericharper <complex451@gmail.com>

* update config

Signed-off-by: ericharper <complex451@gmail.com>

* update config

Signed-off-by: ericharper <complex451@gmail.com>

* Initial megatron dataset port

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix merge conflicts

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* License fixes and megatron model porting

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* More fixes to import from nemo rather than megatron

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix circular imports

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Revert config file

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Restructure further to avoid circular imports

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* add Makefile

Signed-off-by: ericharper <complex451@gmail.com>

* Add megatron modules

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* add license

Signed-off-by: ericharper <complex451@gmail.com>

* Port from latest megatron

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* update cfg

Signed-off-by: ericharper <complex451@gmail.com>

* update config

Signed-off-by: ericharper <complex451@gmail.com>

* add _del_model_without_trainer

Signed-off-by: ericharper <complex451@gmail.com>

* add data preprocessing script

Signed-off-by: ericharper <complex451@gmail.com>

* update config

Signed-off-by: ericharper <complex451@gmail.com>

* use apex mpu

Signed-off-by: ericharper <complex451@gmail.com>

* replace print_rank_0 with nemo utils logging

Signed-off-by: ericharper <complex451@gmail.com>

* use apex mpu

Signed-off-by: ericharper <complex451@gmail.com>

* use apex mpu

Signed-off-by: ericharper <complex451@gmail.com>

* add use_cpu_initialization

Signed-off-by: ericharper <complex451@gmail.com>

* fixing autoresume in progress

Signed-off-by: ericharper <complex451@gmail.com>

* properly removing last checkpoint

Signed-off-by: ericharper <complex451@gmail.com>

* log consumed samples

Signed-off-by: ericharper <complex451@gmail.com>

* fix mp autoresume

Signed-off-by: ericharper <complex451@gmail.com>

* add NLPSaveRestoreConnector

Signed-off-by: ericharper <complex451@gmail.com>

* Megatron GPT training with NeMo tokenizers (#2818)

* Update files from megatron repo

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove non NLP data related files from megatron

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Merge megatron and nemo tokenizers

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove get_tokenizer() calls from gpt model

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Update tokenizer yaml config

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* add todo

Signed-off-by: ericharper <complex451@gmail.com>

* update config

Signed-off-by: ericharper <complex451@gmail.com>

* make init_method_std configurable

Signed-off-by: ericharper <complex451@gmail.com>

* make gpu init work by setting random seed earlier

Signed-off-by: ericharper <complex451@gmail.com>

* fix gpu init after removing debug print in mpu

Signed-off-by: ericharper <complex451@gmail.com>

* add fused_adam

Signed-off-by: ericharper <complex451@gmail.com>

* check ds is not none before logging len

Signed-off-by: ericharper <complex451@gmail.com>

* set fp16 arg to true and fix enum conflict

Signed-off-by: ericharper <complex451@gmail.com>

* make fp16 arg configurable

Signed-off-by: ericharper <complex451@gmail.com>

* add grad clip from megatron

Signed-off-by: ericharper <complex451@gmail.com>

* Linear warmup with cosine annealing and constant holding (#2846)

* Testing cosine schedule

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* More fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* update config for constant steps in schedule

Signed-off-by: ericharper <complex451@gmail.com>

* temporarily import enum from megatron

Signed-off-by: ericharper <complex451@gmail.com>

* add grad clip for fp32

Signed-off-by: ericharper <complex451@gmail.com>

* update check for _del_model_without_trainer

Signed-off-by: ericharper <complex451@gmail.com>

* updating restore for model parallel

Signed-off-by: ericharper <complex451@gmail.com>

* add predict script

Signed-off-by: ericharper <complex451@gmail.com>

* update test iters

Signed-off-by: ericharper <complex451@gmail.com>

* add barrier

Signed-off-by: ericharper <complex451@gmail.com>

* return if clip_val is 0 or None

Signed-off-by: ericharper <complex451@gmail.com>

* when using amp clip grads after they are unscaled

Signed-off-by: ericharper <complex451@gmail.com>

* make native amp scaler hyperparams configurable

Signed-off-by: ericharper <complex451@gmail.com>

* (1) nvfuser, (2) amp-casting decoration (#2894)

* (1) nvfuser, (2) amp-casting decoration

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* support bf16

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* update package info

Signed-off-by: ericharper <complex451@gmail.com>

* add set device to constructor

Signed-off-by: ericharper <complex451@gmail.com>

* set_device in constructor

Signed-off-by: ericharper <complex451@gmail.com>

* [BigNLP] Remove megatron-lm dependency. (#2910)

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* add load_fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* add load_fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* update megatron_init

Signed-off-by: ericharper <complex451@gmail.com>

* add fused kernels

Signed-off-by: ericharper <complex451@gmail.com>

* add fused kernels

Signed-off-by: ericharper <complex451@gmail.com>

* update process batch

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* add megatron clip_grad

Signed-off-by: ericharper <complex451@gmail.com>

* trying to resolve circular import error

Signed-off-by: ericharper <complex451@gmail.com>

* rename file

Signed-off-by: ericharper <complex451@gmail.com>

* remove non-gpt models and datasets from __init__ files

Signed-off-by: ericharper <complex451@gmail.com>

* set device in constructorfor gpu init

Signed-off-by: ericharper <complex451@gmail.com>

* set device in constructorfor gpu init

Signed-off-by: ericharper <complex451@gmail.com>

* set_device in constructor

Signed-off-by: ericharper <complex451@gmail.com>

* clean config

Signed-off-by: ericharper <complex451@gmail.com>

* update MegatronDataset

Signed-off-by: ericharper <complex451@gmail.com>

* clean up MegatronModule

Signed-off-by: ericharper <complex451@gmail.com>

* clean up MegatronModule

Signed-off-by: ericharper <complex451@gmail.com>

* rename fp16 and bf16 flags to fused_softmax_input_in_fp16/bf16

Signed-off-by: ericharper <complex451@gmail.com>

* rename to fused_fp16

Signed-off-by: ericharper <complex451@gmail.com>

* add fused_fp16 arg to LayerNorm calls

Signed-off-by: ericharper <complex451@gmail.com>

* fix arg name

Signed-off-by: ericharper <complex451@gmail.com>

* fix arg name

Signed-off-by: ericharper <complex451@gmail.com>

* fix import

Signed-off-by: ericharper <complex451@gmail.com>

* update arg

Signed-off-by: ericharper <complex451@gmail.com>

* skip warmup default to True

Signed-off-by: ericharper <complex451@gmail.com>

* skip warmup default to True

Signed-off-by: ericharper <complex451@gmail.com>

* Adding complete method to MegatronGPTModel (#2935)

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* make ffn_hidden_size mandatory

Signed-off-by: ericharper <complex451@gmail.com>

* Manually migrating timing of step into branch (#2937)

* 1. Manually migrating timing of step into branch.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Updated file name and content.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Updated to latest code.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

Co-authored-by: Micha Livne <mlivne@nvidia.com>

* remove unused imports

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* check fused_fp16 and fused_bf16 are not both True

Signed-off-by: ericharper <complex451@gmail.com>

* update predict script for model parallel .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>
Co-authored-by: Micha Livne <mlivne@nvidia.com>

* NVfuser (#2943)

* activation checkpoint recompute

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* selective nvfuser setup

* Megatron gpt bfloat support (#2926)

* Save/restore fix

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Another merge

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Bf16 args in init

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Set precision

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove debug stuff

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* add bf16 casting decorator

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* Bfloat layernorm propagation

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* activation checkpoint recompute

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* selective nvfuser setup

* More arg removal

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove BERTDataset

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* update to latest apex and patch transformer autocast

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: ericharper <complex451@gmail.com>

* don't set jit for bf16

Signed-off-by: ericharper <complex451@gmail.com>

* replace apex.mpu

Signed-off-by: ericharper <complex451@gmail.com>

* fix grad clip

Signed-off-by: ericharper <complex451@gmail.com>

* NVFuser fixes (#2951)

* Fuser fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove dummy handler

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove PTL plugin based logic for fusion

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* remove duplicated file

Signed-off-by: ericharper <complex451@gmail.com>

* typo (#2960)

Signed-off-by: ericharper <complex451@gmail.com>

* [BigNLP] Script to convert GPT checkpoint to .nemo (#2958)

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* add load_fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* add load_fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* update megatron_init

Signed-off-by: ericharper <complex451@gmail.com>

* add fused kernels

Signed-off-by: ericharper <complex451@gmail.com>

* add fused kernels

Signed-off-by: ericharper <complex451@gmail.com>

* update process batch

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* add megatron clip_grad

Signed-off-by: ericharper <complex451@gmail.com>

* trying to resolve circular import error

Signed-off-by: ericharper <complex451@gmail.com>

* rename file

Signed-off-by: ericharper <complex451@gmail.com>

* remove non-gpt models and datasets from __init__ files

Signed-off-by: ericharper <complex451@gmail.com>

* set device in constructorfor gpu init

Signed-off-by: ericharper <complex451@gmail.com>

* set device in constructorfor gpu init

Signed-off-by: ericharper <complex451@gmail.com>

* set_device in constructor

Signed-off-by: ericharper <complex451@gmail.com>

* clean config

Signed-off-by: ericharper <complex451@gmail.com>

* update MegatronDataset

Signed-off-by: ericharper <complex451@gmail.com>

* clean up MegatronModule

Signed-off-by: ericharper <complex451@gmail.com>

* clean up MegatronModule

Signed-off-by: ericharper <complex451@gmail.com>

* rename fp16 and bf16 flags to fused_softmax_input_in_fp16/bf16

Signed-off-by: ericharper <complex451@gmail.com>

* rename to fused_fp16

Signed-off-by: ericharper <complex451@gmail.com>

* add fused_fp16 arg to LayerNorm calls

Signed-off-by: ericharper <complex451@gmail.com>

* fix arg name

Signed-off-by: ericharper <complex451@gmail.com>

* fix arg name

Signed-off-by: ericharper <complex451@gmail.com>

* fix import

Signed-off-by: ericharper <complex451@gmail.com>

* update arg

Signed-off-by: ericharper <complex451@gmail.com>

* skip warmup default to True

Signed-off-by: ericharper <complex451@gmail.com>

* skip warmup default to True

Signed-off-by: ericharper <complex451@gmail.com>

* Adding complete method to MegatronGPTModel (#2935)

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* make ffn_hidden_size mandatory

Signed-off-by: ericharper <complex451@gmail.com>

* Manually migrating timing of step into branch (#2937)

* 1. Manually migrating timing of step into branch.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Updated file name and content.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Updated to latest code.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

Co-authored-by: Micha Livne <mlivne@nvidia.com>

* remove unused imports

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* check fused_fp16 and fused_bf16 are not both True

Signed-off-by: ericharper <complex451@gmail.com>

* update predict script for model parallel .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* add script to convert .ckpt to .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* in progress

Signed-off-by: ericharper <complex451@gmail.com>

* update

Signed-off-by: ericharper <complex451@gmail.com>

* convert mp checkpoints to nemo

Signed-off-by: ericharper <complex451@gmail.com>

* update help

Signed-off-by: ericharper <complex451@gmail.com>

* add safeguard for model parallel save_to

Signed-off-by: ericharper <complex451@gmail.com>

* adjust NLPModel save_to to be safer for model parallel

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>
Co-authored-by: Micha Livne <mlivne@nvidia.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* [BigNLP] Update GPT evaluation to work with tensor model parallel  (#2959)

* in progress

Signed-off-by: ericharper <complex451@gmail.com>

* update args

Signed-off-by: ericharper <complex451@gmail.com>

* add request dataset

Signed-off-by: ericharper <complex451@gmail.com>

* tokenize request

Signed-off-by: ericharper <complex451@gmail.com>

* in progress

Signed-off-by: ericharper <complex451@gmail.com>

* able to run

Signed-off-by: ericharper <complex451@gmail.com>

* reduce logits

Signed-off-by: ericharper <complex451@gmail.com>

* capture response

Signed-off-by: ericharper <complex451@gmail.com>

* squeeze and unsqueeze

Signed-off-by: ericharper <complex451@gmail.com>

* handle non model parallel case

Signed-off-by: ericharper <complex451@gmail.com>

* clean imports

Signed-off-by: ericharper <complex451@gmail.com>

* add file

Signed-off-by: ericharper <complex451@gmail.com>

* convert logits to log_probs

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* rename logits to log_probs

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* fix copyright headers

Signed-off-by: ericharper <complex451@gmail.com>

* fix copyright headers

Signed-off-by: ericharper <complex451@gmail.com>

* remove old TimingCallback

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* update jenkins to use latest apex and sandeep's fork

Signed-off-by: ericharper <complex451@gmail.com>

* update jenkins

Signed-off-by: ericharper <complex451@gmail.com>

* update jenkins

Signed-off-by: ericharper <complex451@gmail.com>

* update jenkins

Signed-off-by: ericharper <complex451@gmail.com>

* update jenkins

Signed-off-by: ericharper <complex451@gmail.com>

* try 2109 container

Signed-off-by: ericharper <complex451@gmail.com>

* try cuda container

Signed-off-by: ericharper <complex451@gmail.com>

* use internal container

Signed-off-by: ericharper <complex451@gmail.com>

* update checkpoint tests

Signed-off-by: ericharper <complex451@gmail.com>

* fix scheduler args

Signed-off-by: ericharper <complex451@gmail.com>

* update eval

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* update jenkins to use ptl 1.5 rc

Signed-off-by: ericharper <complex451@gmail.com>

* add import guard to jenkins

Signed-off-by: ericharper <complex451@gmail.com>

* add import guard to jenkins

Signed-off-by: ericharper <complex451@gmail.com>

* remove deterministic

Signed-off-by: ericharper <complex451@gmail.com>

* install numba .53

Signed-off-by: ericharper <complex451@gmail.com>

* allow for more variance

Signed-off-by: ericharper <complex451@gmail.com>

* update trainer config dataclass

Signed-off-by: ericharper <complex451@gmail.com>

* test_get_optimizer on gpu

Signed-off-by: ericharper <complex451@gmail.com>

* revert comment

Signed-off-by: ericharper <complex451@gmail.com>

* change trainer config default to 32

Signed-off-by: ericharper <complex451@gmail.com>

* [BigNLP] Remove fused kernel code instead use Apex (#2984)

* remove fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* remove fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* remove fused layer norm and fused softmax and use apex instead

Signed-off-by: ericharper <complex451@gmail.com>

* update imports

Signed-off-by: ericharper <complex451@gmail.com>

* remove comment

Signed-off-by: ericharper <complex451@gmail.com>

* use apex enums

Signed-off-by: ericharper <complex451@gmail.com>

* use apex enums

Signed-off-by: ericharper <complex451@gmail.com>

* add tab

Signed-off-by: ericharper <complex451@gmail.com>

* Timer with sliding window (#3002)

Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>

* revert tab

Signed-off-by: ericharper <complex451@gmail.com>

* check for rank zero

Signed-off-by: ericharper <complex451@gmail.com>

* check for rank zero

Signed-off-by: ericharper <complex451@gmail.com>

* try explicit log dir

Signed-off-by: ericharper <complex451@gmail.com>

* add +

Signed-off-by: ericharper <complex451@gmail.com>

* don't rm

Signed-off-by: ericharper <complex451@gmail.com>

* make dir if it doesn't exist

Signed-off-by: ericharper <complex451@gmail.com>

* create mp nemo file in temp directory

Signed-off-by: ericharper <complex451@gmail.com>

* simplify mp save_to

Signed-off-by: ericharper <complex451@gmail.com>

* handle mp 1 case

Signed-off-by: ericharper <complex451@gmail.com>

* style fix

Signed-off-by: ericharper <complex451@gmail.com>

* remove files

Signed-off-by: ericharper <complex451@gmail.com>

* fix consumed_samples when resuming

Signed-off-by: ericharper <complex451@gmail.com>

* fix reinstall.sh

Signed-off-by: ericharper <complex451@gmail.com>

* update req

Signed-off-by: ericharper <complex451@gmail.com>

* add more detailed log for dataloaders

Signed-off-by: ericharper <complex451@gmail.com>

* check if cuda is available before using fused_adam

Signed-off-by: ericharper <complex451@gmail.com>

* revert comment

Signed-off-by: ericharper <complex451@gmail.com>

* update eval script to use model.freeze

Signed-off-by: ericharper <complex451@gmail.com>

* log train loss averaged over gradient accumulation steps

Signed-off-by: ericharper <complex451@gmail.com>

* check copyright earlier

Signed-off-by: ericharper <complex451@gmail.com>

* todo

Signed-off-by: ericharper <complex451@gmail.com>

* override SaveRestoreConnector in NLPModel init

Signed-off-by: ericharper <complex451@gmail.com>

* move to scripts

Signed-off-by: ericharper <complex451@gmail.com>

* remove star import

Signed-off-by: ericharper <complex451@gmail.com>

* remove comments

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused dataset

Signed-off-by: ericharper <complex451@gmail.com>

* removed barrier

Signed-off-by: ericharper <complex451@gmail.com>

* check cfg

Signed-off-by: ericharper <complex451@gmail.com>

* remove logging

Signed-off-by: ericharper <complex451@gmail.com>

* freeze, unfreeze

Signed-off-by: ericharper <complex451@gmail.com>

* return None

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused imports

Signed-off-by: ericharper <complex451@gmail.com>

* add TODO

Signed-off-by: ericharper <complex451@gmail.com>

* typecheck

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* todo

Signed-off-by: ericharper <complex451@gmail.com>

* add common native plugin

Signed-off-by: ericharper <complex451@gmail.com>

* restore with trainer

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* deprecate megatron-lm bert

Signed-off-by: ericharper <complex451@gmail.com>

* deprecate megatron-lm bert

Signed-off-by: ericharper <complex451@gmail.com>

* compile helpers ont he fly

Signed-off-by: ericharper <complex451@gmail.com>

* remove amp_level

Signed-off-by: ericharper <complex451@gmail.com>

* remove amp_level from configs

Signed-off-by: ericharper <complex451@gmail.com>

* add missing import

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* remove amp_level

Signed-off-by: ericharper <complex451@gmail.com>

* use fast huggingface tokenizers by default

Signed-off-by: ericharper <complex451@gmail.com>

* deal with huggingface tokenizer positional args

Signed-off-by: ericharper <complex451@gmail.com>

* deal with huggingface tokenizer positional args

Signed-off-by: ericharper <complex451@gmail.com>

* deal with huggingface tokenizer positional args

Signed-off-by: ericharper <complex451@gmail.com>

* revert use_fast default to False

Signed-off-by: ericharper <complex451@gmail.com>

* return super training_epoch_end

Signed-off-by: ericharper <complex451@gmail.com>

* remove optimizer_idx arg from training_step

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused arg from on_train_epoch_end

Signed-off-by: ericharper <complex451@gmail.com>

* add restore_from_path to nemo config

Signed-off-by: ericharper <complex451@gmail.com>

* add comment

Signed-off-by: ericharper <complex451@gmail.com>

* revert

Signed-off-by: ericharper <complex451@gmail.com>

* override connector if not subclassing NLPSaveRestoreConnector for model parallel save

Signed-off-by: ericharper <complex451@gmail.com>

* update test optimizer

Signed-off-by: ericharper <complex451@gmail.com>

* clean up

Signed-off-by: ericharper <complex451@gmail.com>

* clean up

Signed-off-by: ericharper <complex451@gmail.com>

* clean up

Signed-off-by: ericharper <complex451@gmail.com>

* clean up

Signed-off-by: ericharper <complex451@gmail.com>

* make data_prefix mandatory in config

Signed-off-by: ericharper <complex451@gmail.com>

* update installation instructions on readme

Signed-off-by: ericharper <complex451@gmail.com>

* update dockerfile

Signed-off-by: ericharper <complex451@gmail.com>

* add todo

Signed-off-by: ericharper <complex451@gmail.com>

* raise error if trying to use always_save_nemo with model parallel model

Signed-off-by: ericharper <complex451@gmail.com>

* remove comment

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>
Co-authored-by: Micha Livne <mlivne@nvidia.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-08-17T14:55:43Z,"Refactor and Minimize Dependencies (#2643)

* squash

Signed-off-by: Jason <jasoli@nvidia.com>

* add comments

Signed-off-by: Jason <jasoli@nvidia.com>

* style and cleanup

Signed-off-by: Jason <jasoli@nvidia.com>

* cleanup

Signed-off-by: Jason <jasoli@nvidia.com>

* add new test file

Signed-off-by: Jason <jasoli@nvidia.com>

* syntax

Signed-off-by: Jason <jasoli@nvidia.com>

* style

Signed-off-by: Jason <jasoli@nvidia.com>

* typo

Signed-off-by: Jason <jasoli@nvidia.com>

* update

Signed-off-by: Jason <jasoli@nvidia.com>

* update

Signed-off-by: Jason <jasoli@nvidia.com>

* update

Signed-off-by: Jason <jasoli@nvidia.com>

* try again

Signed-off-by: Jason <jasoli@nvidia.com>

* wip

Signed-off-by: Jason <jasoli@nvidia.com>

* style; ci should fail

Signed-off-by: Jason <jasoli@nvidia.com>

* final

Signed-off-by: Jason <jasoli@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-08-10T19:50:22Z,"Remove PTL 1.4 upper bound (#2600)

* update exp_manager

Signed-off-by: ericharper <complex451@gmail.com>

* update ptl trainer dataclass

Signed-off-by: ericharper <complex451@gmail.com>

* update check for ranks

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* remove returned from val epoch end

Signed-off-by: ericharper <complex451@gmail.com>

* revert

Signed-off-by: ericharper <complex451@gmail.com>

* Try to fix multi validation logging

Signed-off-by: smajumdar <titu1994@gmail.com>

* Try to fix multi validation logging

Signed-off-by: smajumdar <titu1994@gmail.com>

* Try to fix multi validation logging

Signed-off-by: smajumdar <titu1994@gmail.com>

* update for glue with PTL1.4 (#2634)

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* add rank_zero_only to self.log

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: Evelina <10428420+ekmb@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-08-06T22:28:24Z,"Fix return config path for new SaveRestoreConnector (#2626)

* Fix return config path for new SaveRestore connector

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix return config path for new SaveRestore connector

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-08-06T00:45:57Z,"Add save restore connector to ModelPT (#2592)

* add save restore connector

Signed-off-by: ericharper <complex451@gmail.com>

* add save restore connector

Signed-off-by: ericharper <complex451@gmail.com>

* add save restore connector property

Signed-off-by: ericharper <complex451@gmail.com>

* add _default_save_to

Signed-off-by: ericharper <complex451@gmail.com>

* moving globals to app_state

Signed-off-by: ericharper <complex451@gmail.com>

* moving globals to app_state

Signed-off-by: ericharper <complex451@gmail.com>

* add model attribute to connector

Signed-off-by: ericharper <complex451@gmail.com>

* add model attribute to connector

Signed-off-by: ericharper <complex451@gmail.com>

* fix tabs

Signed-off-by: ericharper <complex451@gmail.com>

* fix tabs

Signed-off-by: ericharper <complex451@gmail.com>

* remove ModelPT import

Signed-off-by: ericharper <complex451@gmail.com>

* add default restore

Signed-off-by: ericharper <complex451@gmail.com>

* add default restore

Signed-off-by: ericharper <complex451@gmail.com>

* remove eff globals

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* fix tabs

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* update globals, remove save restore property

Signed-off-by: ericharper <complex451@gmail.com>

* fix typo

Signed-off-by: ericharper <complex451@gmail.com>

* add setter

Signed-off-by: ericharper <complex451@gmail.com>

* add setter

Signed-off-by: ericharper <complex451@gmail.com>

* fix app_state restore flag

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* update paths

Signed-off-by: ericharper <complex451@gmail.com>

* add connector arg to from_pretrained

Signed-off-by: ericharper <complex451@gmail.com>

* update save restore connector after instantiating

Signed-off-by: ericharper <complex451@gmail.com>

* use connector

Signed-off-by: ericharper <complex451@gmail.com>

* get class from config in .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* add TODO

Signed-off-by: ericharper <complex451@gmail.com>

* move extract_state_dict to connector

Signed-off-by: ericharper <complex451@gmail.com>

* add methods for toch save and torch load

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* update mock model conf

Signed-off-by: ericharper <complex451@gmail.com>

* revert

Signed-off-by: ericharper <complex451@gmail.com>

* move mock model to common collection

Signed-off-by: ericharper <complex451@gmail.com>

* update NLPModel

Signed-off-by: ericharper <complex451@gmail.com>

* update test to use connector

Signed-off-by: ericharper <complex451@gmail.com>

* move artifacts to save restore connector

Signed-off-by: ericharper <complex451@gmail.com>

* add save_restore_connector arg to register_artifact

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* default save_restore_connector arg to None

Signed-off-by: ericharper <complex451@gmail.com>

* default save_restore_connector arg to None

Signed-off-by: ericharper <complex451@gmail.com>

* clean commented line

Signed-off-by: ericharper <complex451@gmail.com>

* default save_restore_connector arg to None

Signed-off-by: ericharper <complex451@gmail.com>

* move MockModel

Signed-off-by: ericharper <complex451@gmail.com>

* fix docstrings, remove underscores, default from connector

Signed-off-by: ericharper <complex451@gmail.com>

* update docstring

Signed-off-by: ericharper <complex451@gmail.com>

* update docstring

Signed-off-by: ericharper <complex451@gmail.com>

* change name to is_model_being_restored

Signed-off-by: ericharper <complex451@gmail.com>

* move constants from AppState to SaveRestoreConnector

Signed-off-by: ericharper <complex451@gmail.com>

* encapsulate logic for model parallel checkpoint

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* update mock config

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* add init_subclass, remove connector arg from register_artifact, move MockModel to tests

Signed-off-by: ericharper <complex451@gmail.com>

* remove old import

Signed-off-by: ericharper <complex451@gmail.com>

* Add tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Finalize tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Finalize tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* fixing lgtm

Signed-off-by: ericharper <complex451@gmail.com>

* fix lgtm

Signed-off-by: ericharper <complex451@gmail.com>

* update NLPModel.restore_from

Signed-off-by: ericharper <complex451@gmail.com>

* Fix classpath resolution

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-07-14T21:19:06Z,"fix squad for chinese (#2484)

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-06-12T02:28:32Z,"Update ranges for omegaconf and hydra (#2336)

* Update ranges

Signed-off-by: smajumdar <titu1994@gmail.com>

* Updates for Hydra and OmegaConf updates

Signed-off-by: smajumdar <titu1994@gmail.com>

* Style fixes

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct tests and revert patch for model utils

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct docstring

Signed-off-by: smajumdar <titu1994@gmail.com>

* Revert unnecessary change

Signed-off-by: smajumdar <titu1994@gmail.com>

* Revert unnecessary change

Signed-off-by: smajumdar <titu1994@gmail.com>

* Guard scheduler for None

Signed-off-by: smajumdar <titu1994@gmail.com>

* default to 0.0 if bpe_dropout is None

Signed-off-by: ericharper <complex451@gmail.com>

* Correctly log class that was restored

Signed-off-by: smajumdar <titu1994@gmail.com>

* Root patch *bpe_dropout

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: ericharper <complex451@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-06-08T17:40:35Z,"[BUGFIX] OmegaConf forward compatibility (#2319)

* Update OmegaConf compatibility

Signed-off-by: smajumdar <titu1994@gmail.com>
Signed-off-by: ericharper <complex451@gmail.com>

* Correct OmegaConf.pretty()

Signed-off-by: smajumdar <titu1994@gmail.com>
Signed-off-by: ericharper <complex451@gmail.com>

* upper bound omegaconf

Signed-off-by: ericharper <complex451@gmail.com>

* add if,else back

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-06-03T22:49:50Z,"Merge tag 'v1.0.0' into main

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-06-02T20:02:40Z,"ASR improvements (#2293)

* Update numba messages and citrinet configs

Signed-off-by: smajumdar <titu1994@gmail.com>

* Remove support for weight init scale and hidden hidden bias scale for layer normalized lstm

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add support for multiple filetypes in tarred datasets, correct rnn LN-lstm inputs, fix OmegaConf compat issue

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-06-02T00:02:13Z,"Adjust warning messages (#2294)

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-06-01T23:39:00Z,"Revert ""Adjust warning messages""

This reverts commit df046ec55754d0136a2a28451435068f32409f30."
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-06-01T23:37:20Z,"Adjust warning messages

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-05-26T22:07:02Z,"ASR Refactoring (#2240)

* Refactor out the preprocessing from ASR into common

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct nltk issue with vocabs.py for clusters

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add typing information to SpecAugment and SpecCutout

Signed-off-by: smajumdar <titu1994@gmail.com>

* Reorganize parts directory

Signed-off-by: smajumdar <titu1994@gmail.com>

* Refactor parts submodules, add __init__ to few important parts

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update docs for new path to parts

Signed-off-by: smajumdar <titu1994@gmail.com>

* Cherry pick PR https://github.com/NVIDIA/NeMo/pull/2219

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add header for preprocessing commons

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix style of tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add forced update of configs for train-val-test ds to new labels tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update path to FilterbankFeatures for TTS

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add an alias file for backward compatibility

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add an alias file for backward compatibility

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update training scripts of ASR to support finetuning

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update Finetuning step to be ModelPT level

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update docs for finetuning for ASR

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix style

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update docs and scripts with fine-tuning info

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update docs and scripts with fine-tuning info

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix style

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update scripts

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add comment for weight initialization

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-05-26T06:32:31Z,"Support multiple models being instantiated in same execution scope (#2245)

* Support multiple models being instantiated in same execution scope

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add locks to methods in appstate

Signed-off-by: smajumdar <titu1994@gmail.com>

* Perform locks only on write operations

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct deadlock issue

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add more tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add test for multi save and remove patch to change save type

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update app state to preserve gidx of previous token

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct restoration logic for tarfiles

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-05-25T16:44:20Z,Merge branch 'v1.0.0' into main
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-05-21T23:16:33Z,"[BUGFIX] Only process tarfile artifacts when model was restored from tarfile (#2250)

* process tarfile artifacts only if model is being restored

Signed-off-by: ericharper <complex451@gmail.com>

* process tarfile artifacts only if model was restored from a tarfile

Signed-off-by: ericharper <complex451@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-05-17T23:30:01Z,"Merge branch 'v1.0.0' into main

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-05-17T23:16:29Z,"Set strict=True everywhere by default. (#2225)

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-05-14T06:55:42Z,"ASR patches for v1.0.0 (#2207)

* Multiple updates to RNNT add initialization

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct name of initilization

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update dockerignore

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix RNNT WER calculation

Signed-off-by: smajumdar <titu1994@gmail.com>

* Address comments

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-05-10T17:47:17Z,"Merge branch 'v1.0.0' into main

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-05-10T17:37:26Z,"Update ptl to 1.3 on main branch (#2178)

* Update PTL

Signed-off-by: smajumdar <titu1994@gmail.com>

* Begin update to Pytorch Lightning 1.3.x

Signed-off-by: smajumdar <titu1994@gmail.com>

* Formatting

Signed-off-by: smajumdar <titu1994@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* Formatting

Signed-off-by: smajumdar <titu1994@gmail.com>

* minor fix

Signed-off-by: Jason <jasoli@nvidia.com>

* minor fix

Signed-off-by: Jason <jasoli@nvidia.com>

* get testing attribute from trainer

Signed-off-by: ericharper <complex451@gmail.com>

* update init_ddp_connection override

Signed-off-by: ericharper <complex451@gmail.com>

* update attribute

Signed-off-by: ericharper <complex451@gmail.com>

* add barrier after load checkpoint in megatron

Signed-off-by: ericharper <complex451@gmail.com>

* remove barrier

Signed-off-by: ericharper <complex451@gmail.com>

* update last naming

Signed-off-by: Jason <jasoli@nvidia.com>

Co-authored-by: ericharper <complex451@gmail.com>
Co-authored-by: Jason <jasoli@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-05-07T17:45:12Z,"Fix to always_save_nemo (#2174)

* Initial attempt at always_save_nemo fix

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* updated path before saving in exp manager, fixed bug when handling tarfile artifacts

Signed-off-by: ericharper <complex451@gmail.com>

* Add test with always_save_nemo to exp_manager

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* update jenkins branch

Signed-off-by: ericharper <complex451@gmail.com>

* check for nemo:

Signed-off-by: ericharper <complex451@gmail.com>

* check for nemo:

Signed-off-by: ericharper <complex451@gmail.com>

* check for nemo:

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: ericharper <complex451@gmail.com>
Co-authored-by: Jason <jasoli@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-05-04T20:55:35Z,"Update how artifacts work (#2138)

* Update how artifacts work

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* fixing some tests

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* fix more tests

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* add __init__ to tests to make them discoverable

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* empty src support

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* updates plust unittest

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* add copyright check

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* copyright header

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* fix style

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* handle hashed megatron checkpoint version in nlp restore_from

Signed-off-by: ericharper <complex451@gmail.com>

* add _MODEL_RESTORE_PATH to AppState

Signed-off-by: ericharper <complex451@gmail.com>

* get rid of global folder caching

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* double register - warning instead of exception

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* Add asr spe tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Pop out asr wpe pre-registered value

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct ASR tests and paths

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct tokenizer saving

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct ASR tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct ASR bpe mixin

Signed-off-by: smajumdar <titu1994@gmail.com>

* Patch up backward compatibility

Signed-off-by: smajumdar <titu1994@gmail.com>

* update register_bert_model

Signed-off-by: ericharper <complex451@gmail.com>

* update all get_lm_model calls

Signed-off-by: ericharper <complex451@gmail.com>

* return None if src not found

Signed-off-by: ericharper <complex451@gmail.com>

* handle case with no tokenizer

Signed-off-by: ericharper <complex451@gmail.com>

* do not add another hash is using tarfile_artifacts

Signed-off-by: ericharper <complex451@gmail.com>

* add return_none flag, update doc string

Signed-off-by: ericharper <complex451@gmail.com>

* update default behavior of register_artifact for NLPModel

Signed-off-by: ericharper <complex451@gmail.com>

* change kwarg name to verify_src_exists

Signed-off-by: ericharper <complex451@gmail.com>

* use cfg instead of _cfg

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* some cleanups

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

Co-authored-by: ericharper <complex451@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-04-21T19:26:40Z,"[NLP] Updating model parallel  (#2015)

* create nlp ddp plugin

Signed-off-by: ericharper <complex451@gmail.com>

* call NLPDDPPlugin from example script

Signed-off-by: ericharper <complex451@gmail.com>

* training is working but autoresume is not

Signed-off-by: ericharper <complex451@gmail.com>

* training working again

Signed-off-by: ericharper <complex451@gmail.com>

* autoresume working again

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* bug fix

Signed-off-by: ericharper <complex451@gmail.com>

* automatically use nlp ddp

Signed-off-by: ericharper <complex451@gmail.com>

* automatically use nlp ddp

Signed-off-by: ericharper <complex451@gmail.com>

* automatically use nlp ddp

Signed-off-by: ericharper <complex451@gmail.com>

* automatically use nlp ddp

Signed-off-by: ericharper <complex451@gmail.com>

* automatically use nlp ddp

Signed-off-by: ericharper <complex451@gmail.com>

* automatically use nlp ddp

Signed-off-by: ericharper <complex451@gmail.com>

* automatically use nlp ddp

Signed-off-by: ericharper <complex451@gmail.com>

* upgrade container version

Signed-off-by: ericharper <complex451@gmail.com>

* convert config to dict

Signed-off-by: ericharper <complex451@gmail.com>

* convert config to dict

Signed-off-by: ericharper <complex451@gmail.com>

* save nemo model for each mp rank

Signed-off-by: ericharper <complex451@gmail.com>

* restore from mp .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* refactoring model parallel to nlpddpplugin

Signed-off-by: ericharper <complex451@gmail.com>

* training working again

Signed-off-by: ericharper <complex451@gmail.com>

* only finish mpu init when not using model parallel

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* cleanup and add trainer to restore_from

Signed-off-by: ericharper <complex451@gmail.com>

* testing working

Signed-off-by: ericharper <complex451@gmail.com>

* clean up

Signed-off-by: ericharper <complex451@gmail.com>

* register megatron checkpoint version

Signed-off-by: ericharper <complex451@gmail.com>

* convert to one .nemo file in progress

Signed-off-by: ericharper <complex451@gmail.com>

* combine .nemo files after saving on each mp rank, in progress

Signed-off-by: ericharper <complex451@gmail.com>

* combine .nemo files after saving on each mp rank, in progress

Signed-off-by: ericharper <complex451@gmail.com>

* combine .nemo files after saving on each mp rank

Signed-off-by: ericharper <complex451@gmail.com>

* combine .nemo files after saving on each mp rank

Signed-off-by: ericharper <complex451@gmail.com>

* .nemo restore working

Signed-off-by: ericharper <complex451@gmail.com>

* clean up unused import

Signed-off-by: ericharper <complex451@gmail.com>

* clean up unused import

Signed-off-by: ericharper <complex451@gmail.com>

* remove comment

Signed-off-by: ericharper <complex451@gmail.com>

* remove comment

Signed-off-by: ericharper <complex451@gmail.com>

* remove comment

Signed-off-by: ericharper <complex451@gmail.com>

* get pl random seed default to 1234

Signed-off-by: ericharper <complex451@gmail.com>

* change check

Signed-off-by: ericharper <complex451@gmail.com>

* super restore_from

Signed-off-by: ericharper <complex451@gmail.com>

* super restore_from

Signed-off-by: ericharper <complex451@gmail.com>

* revert config change

Signed-off-by: ericharper <complex451@gmail.com>

* move override after checking for mp

Signed-off-by: ericharper <complex451@gmail.com>

* add jenkins training test

Signed-off-by: ericharper <complex451@gmail.com>

* fix bug when training from .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* add jenkins test for train from .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* added jenkins test for evaluation from .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* add model_restore_path to AppState

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* update cfg if using vocab file in .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* clean up

Signed-off-by: ericharper <complex451@gmail.com>

* detect if model parallel already known from .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* update header

Signed-off-by: ericharper <complex451@gmail.com>

* check for mp_rank in directory

Signed-off-by: ericharper <complex451@gmail.com>

* move NLPDDPPLUgin

Signed-off-by: ericharper <complex451@gmail.com>

* move NLPCheckpointConnector

Signed-off-by: ericharper <complex451@gmail.com>

* clean up

Signed-off-by: ericharper <complex451@gmail.com>

* document PTL hooks

Signed-off-by: ericharper <complex451@gmail.com>

* remove todo

Signed-off-by: ericharper <complex451@gmail.com>

* update docstring for nlp

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* fix import

Signed-off-by: ericharper <complex451@gmail.com>

* update path

Signed-off-by: ericharper <complex451@gmail.com>

* check if checkpoint version already set

Signed-off-by: ericharper <complex451@gmail.com>

* add autoresume test

Signed-off-by: ericharper <complex451@gmail.com>

* super static methods

Signed-off-by: ericharper <complex451@gmail.com>

* detect model parallel without unpacking tarfile

Signed-off-by: ericharper <complex451@gmail.com>

* Update nemo/collections/nlp/models/nlp_model.py

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Signed-off-by: ericharper <complex451@gmail.com>

* add comment

Signed-off-by: ericharper <complex451@gmail.com>

* add comment

Signed-off-by: ericharper <complex451@gmail.com>

* raise exception if trainer.local_rank is None

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-04-20T22:56:53Z,"Update container to 21.04 (#2079)

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-04-06T18:09:51Z,"Talknet bug fix & TalkNet Training tutorial  (#2008)

* has_audio to speech_collate

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* log warning to setup_optimization, fix bugs in talknet

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* fix formatting

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* fix import order

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* add talknet training tutorial

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-03-25T23:48:11Z,Merge branch 'r1.0.0rc1' into main
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-03-25T05:05:48Z,"Correct hyper parameter saving for correct logging (#1965)

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-03-11T22:53:30Z,"NMT export, _prepare_for_export refactor (#1866)

* Added citrinet export test

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* NMT export

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* More realistic citrinet test

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* DIsabling tril() mask for inference

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Removing experimental TRTorch code

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Moved export test into proper separate methods

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Format fix

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Removing unused Exportale APIs from NMT model

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Skipping jit.trace before ONNX export, splitting TS and ONNX tests

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* re-enabling Megatron export test

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixed style

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-03-05T20:49:28Z,"Cleanup save/restore (#1851)

* Cleanup save/restore

* Remove EFF save/restore routes
* Once we can take EFF dependency we will use EFF.Archive directly

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* fix copyright headers

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-02-10T00:39:06Z,"Merge branch 'r1.0.0b4' into main

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-02-09T03:18:51Z,"Patch for NMT accuracy regression from PTL 1.1.3 to PTL 1.1.4+ (#1669)

* set find_unused_parameters to True

Signed-off-by: ericharper <complex451@gmail.com>

* remove comment

Signed-off-by: ericharper <complex451@gmail.com>

* revert requirements

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* lifting ddp override to ModelPT

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* remove from ModelPT

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* remove from ModelPT

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* remove from nlp model

Signed-off-by: ericharper <complex451@gmail.com>

* limit ptl to 1.1.5

Signed-off-by: ericharper <complex451@gmail.com>

* limit ptl to 1.1.5

Signed-off-by: ericharper <complex451@gmail.com>

* remove comments

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* add config for find_unused_parameters

Signed-off-by: ericharper <complex451@gmail.com>

* remove neuralmodule subclassing from beam search

Signed-off-by: ericharper <complex451@gmail.com>

* testing beam search

Signed-off-by: ericharper <complex451@gmail.com>

* in progress

Signed-off-by: ericharper <complex451@gmail.com>

* beam search to constructor

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* revert change from merge

Signed-off-by: ericharper <complex451@gmail.com>

* revert change from merge

Signed-off-by: ericharper <complex451@gmail.com>

* revert change from merge

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* add as_frozen to embedding and decoder

Signed-off-by: ericharper <complex451@gmail.com>

* freeze unfreeze manually

Signed-off-by: ericharper <complex451@gmail.com>

* clean up comments

Signed-off-by: ericharper <complex451@gmail.com>

* default find_unused_parameters to True

Signed-off-by: ericharper <complex451@gmail.com>

* clean up unused imports

Signed-off-by: ericharper <complex451@gmail.com>

* add context manager to beam search

Signed-off-by: ericharper <complex451@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-02-06T00:23:15Z,"Update NeMo model filepath (#1687)

* update

Signed-off-by: Jason <jasoli@nvidia.com>

* update

Signed-off-by: Jason <jasoli@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-02-03T02:21:22Z,"Update ASR RTD Documentation (r1.4) (#1695)

* Cleanup warnings

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update ModelPT docs

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update ASR Docs

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update ASR Docs

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add regex based updation of requirements

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct indentation

Signed-off-by: smajumdar <titu1994@gmail.com>

* catch torch.hub import for nlp docs

Signed-off-by: ericharper <complex451@gmail.com>

* fix nlp docs

Signed-off-by: ericharper <complex451@gmail.com>

* force torch_home to be a string

Signed-off-by: ericharper <complex451@gmail.com>

* adding sacremoses to nlp_requirements

Signed-off-by: ericharper <complex451@gmail.com>
Signed-off-by: smajumdar <titu1994@gmail.com>

* Begin adding benchmarks

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update scores

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update scores

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update benchmarks

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update language benchmarks

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add placeholder for spanish model

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix formatting issues

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add Speech datasets docs

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: ericharper <complex451@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-01-29T18:42:14Z,"add new quantization nodes in QuartzNet (#1678)

* add new quantization nodes in QuartzNet

Signed-off-by: Vincent Huang <vincenth@nvidia.com>

* Refactor quantization support to be optional and localized

Signed-off-by: smajumdar <titu1994@gmail.com>

* Guard last few self.quantize

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update docstring for override_config_path

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix conf assignment

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct amp via flag

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add missing self.quantize

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2021-01-25T22:07:47Z,"Nmt Model (#1574)

* Machine translation modules ported from release 0.10.1_p_c (#1275)

* Fix parameter name

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* remove vocab parameter from encoder constructor call

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix model params

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Test connection to github

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove excess parameter from decoder constructor call

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* remove foo

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove excess parameter from beam search constructor

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix parameter names in beam search constructor call

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix parameter name in function call

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Change gpu settings

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix forward function of mt model

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* backup

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Return to logits type

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* backup

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* backup

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add pl callback

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix docstring

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Replace logger.info with print

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* backou

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove debug prints and fix several bugs

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Detach beam results earlier

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove debug prints and fix several bugs

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* merge

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add ngc config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add NeMo to PYTHONPATH in ngc command

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve the way nemo path is set

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add debug print to ngc command

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Reduce batch size in ngc model

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Reduce validation batch size

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Set result path and reduce batch size

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Update hp

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Update hp

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Reduce batch size in ngc command

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Rename job

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* add de en config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Move logging to model, improve config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Disable progress bar

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove mt callback from trainer

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Update 2 gpu config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix test dataset in ngc command

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* backup

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Update configs

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Update ngc debug commands

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add model converting to float

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Separate train and test

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add de->en config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Reduce seq len

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Reduce batch size

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix bash syntax error and reduce batch

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix Error  in config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Increase num steps

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Migrate to pl 1.0.2

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix bash syntax error

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Update OmegaConf & Hydra (#1303)

* update req

Signed-off-by: Jason <jasoli@nvidia.com>

* update jenkins

Signed-off-by: Jason <jasoli@nvidia.com>

* update

Signed-off-by: Jason <jasoli@nvidia.com>

* update

Signed-off-by: Jason <jasoli@nvidia.com>

* update

Signed-off-by: Jason <jasoli@nvidia.com>

* remove print

Signed-off-by: Jason <jasoli@nvidia.com>
Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix file name

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add debug print into ngc command

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Replace exp dir with best ckpt path

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* disable test (#1310)

Signed-off-by: Jason <jasoli@nvidia.com>
Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix checkpoint path saving

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* TTS Collection Cleanup (#1304)

* update yamls

Signed-off-by: Jason <jasoli@nvidia.com>

* OperationMode change

Signed-off-by: Jason <jasoli@nvidia.com>

* drop experimental and unneeded functions, fix dataclass

Signed-off-by: Jason <jasoli@nvidia.com>

* update notebook

Signed-off-by: Jason <jasoli@nvidia.com>

* update docs; ipython notebook; fixes

Signed-off-by: Jason <jasoli@nvidia.com>

* update

Signed-off-by: Jason <jasoli@nvidia.com>

* unused import

Signed-off-by: Jason <jasoli@nvidia.com>

* style

Signed-off-by: Jason <jasoli@nvidia.com>

* update pad_value

Signed-off-by: Jason <jasoli@nvidia.com>

* style

Signed-off-by: Jason <jasoli@nvidia.com>

* fix Preprocessor dataclass

Signed-off-by: Jason <jasoli@nvidia.com>

* update tacotron2 labels

Signed-off-by: Jason <jasoli@nvidia.com>

* move labels

Signed-off-by: Jason <jasoli@nvidia.com>

* typo

Signed-off-by: Jason <jasoli@nvidia.com>

* typos

Signed-off-by: Jason <jasoli@nvidia.com>

* update taco2

Signed-off-by: Jason <jasoli@nvidia.com>
Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* backup

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* backup

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix yaml configs

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* minor changes to PTL 1.0 support (#1312)

* PTL 1.0 support

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* ipdb removal

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* fix to unequal length of correct counts

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* fixed test

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: ericharper <complex451@gmail.com>
Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Close opened file to prevent IOError (#1308)

* Close opened file to prevent IOError

Close an opened text file when done, in order to prevent reaching the number of opened files.

Signed-off-by: Mpho Mphego <mpho112@gmail.com>

* style

Signed-off-by: Jason <jasoli@nvidia.com>

Co-authored-by: Jason <jasoli@nvidia.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* moved all tensorboard logs to self.log (#1317)

* moved all tensorboard logs to self.log

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* PTL upgrade change for classification models

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>
Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Patch QA training_step (#1316)

* fix training_step

Signed-off-by: ericharper <complex451@gmail.com>

* limit jenkins run with train_ds.num_samples rather than trainer.max_steps

Signed-off-by: ericharper <complex451@gmail.com>

* limit jenkins run with train_ds.num_samples rather than trainer.max_steps

Signed-off-by: ericharper <complex451@gmail.com>

* limit jenkins run with train_ds.num_samples rather than trainer.max_steps

Signed-off-by: ericharper <complex451@gmail.com>

* limit jenkins run with train_ds.num_samples rather than trainer.max_steps

Signed-off-by: ericharper <complex451@gmail.com>
Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix config names

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* upgrade to PTL 1.0.2 (#1318)

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>
Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix config names

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix bug in squeezing excess dimension

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix num tokens in batch

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix num steps

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix Licence section

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove unused import

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove unused local variables

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* init

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove vim swp files

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* back up draft of nmt model

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add tokenizer model

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Expand user in tokenizer model paths

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Expand user in tokenizer model paths

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix typo

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add paths to data to config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Expand user

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Expand user

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Update gitignore

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove excess newlines

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Replace distributed sampler with random sampler

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Make mt train on test

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Make mt train on test

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Make mt train on test

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix shuffle param

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix batch size parameter

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix typo

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix retrieving params from config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix parameter name

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove embedding dropout from encoder parameters

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix parameter name

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* remove vocab parameter from encoder constructor call

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix model params

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Test connection to github

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove excess parameter from decoder constructor call

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* remove foo

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove excess parameter from beam search constructor

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix parameter names in beam search constructor call

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix parameter name in function call

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Change gpu settings

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix forward function of mt model

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* backup

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Return to logits type

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* backup

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* backup

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add pl callback

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix docstring

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Replace logger.info with print

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* backou

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove debug prints and fix several bugs

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Detach beam results earlier

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove debug prints and fix several bugs

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* merge

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add ngc config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add NeMo to PYTHONPATH in ngc command

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve the way nemo path is set

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add debug print to ngc command

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Reduce batch size in ngc model

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Reduce validation batch size

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Set result path and reduce batch size

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Update hp

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Update hp

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Reduce batch size in ngc command

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Rename job

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* add de en config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Move logging to model, improve config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Disable progress bar

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove mt callback from trainer

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Update 2 gpu config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix test dataset in ngc command

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* backup

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Update configs

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Update ngc debug commands

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add model converting to float

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Separate train and test

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add de->en config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Reduce seq len

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Reduce batch size

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix bash syntax error and reduce batch

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix Error  in config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Increase num steps

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Replace logger.info with print

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove debug prints and fix several bugs

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* init

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove vim swp files

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix typo

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add paths to data to config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Update gitignore

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove excess newlines

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Make mt train on test

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Make mt train on test

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix batch size parameter

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix typo

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Test connection to github

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* remove foo

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Change gpu settings

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* backou

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove debug prints and fix several bugs

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Detach beam results earlier

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove debug prints and fix several bugs

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* merge

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add ngc config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add NeMo to PYTHONPATH in ngc command

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve the way nemo path is set

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add debug print to ngc command

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Reduce batch size in ngc model

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Reduce validation batch size

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Set result path and reduce batch size

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Update hp

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Update hp

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Reduce batch size in ngc command

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Rename job

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* add de en config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Move logging to model, improve config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Disable progress bar

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Update 2 gpu config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix test dataset in ngc command

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Reduce seq len

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix Error  in config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Increase num steps

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix bug in squeezing excess dimension

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix num steps

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix Licence section

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove unused local variables

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix problems after rebase

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add configs with large batch size

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix rebase consequences

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Rebalance emb and W, rm excess conf, inc vocab sz

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix en de vocab size

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Replace py scalar with tensor, add fp32 confs

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Adjust parameters

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Make minor fixes

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix jobs names

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix bug in beam search len penalty

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Reduce test and validation batch size

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix PEP 8

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Try to bypass illegal mem access error

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Reduce eval batch size

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Replace byte with bool and fix indents

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Refactor future mask computation

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove dev code

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add debug print

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add debug print

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Reduce batch size

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Reduce init range

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Reduce init range

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix init range

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Reduce init range

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Move embeddings factor to initialization

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add params stats collection script

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix bugs in stats collection

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add step logging when collecting stats

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix code style

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Adjust initialization according to paper

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Restore typecheck command

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix arg name

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix arg name

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix init range

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Update docker image

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix argument passing

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove weights logging

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix init range

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Reduce batch size

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Replace init scaling with multiplication

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add draft of circe command

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Update circe command

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Reduce batch size

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

Co-authored-by: Jason <jasoli@nvidia.com>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Co-authored-by: ericharper <complex451@gmail.com>
Co-authored-by: Mpho Mphego <mmphego@ska.ac.za>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Aleksey Grinchuk (Oleksii Hrinchuk) <grinchuk.alexey@gmail.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* undo ASR example

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* clean up some unnecessary changes

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* Fix default config names

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* updates

Signed-off-by: Oleksii Kuchaiev <kuchaev@gmail.com>

* style fix

Signed-off-by: Oleksii Kuchaiev <kuchaev@gmail.com>

* Change inheritance to NeMo

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add Facebook copyright to fairseq tokenizer

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Switch to sacrebleu lib, remove callback

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Pring examples in debug config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Migrate to sacrebleu package

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove ngc and circe commands

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove eda

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix minor bug and create ckpt to best model

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add configs

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* working copy and train-on-test examples

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* Fix code style

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* minor fixes

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* fix dataloader param reading

Signed-off-by: Oleksii Kuchaiev <kuchaev@gmail.com>

* add 4 gpu example

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* Add perplexity to transformer mt model

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix code style

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Minor fix

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove non existent tokens from beam results

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix best ckpt link creation

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix bugs in id filtering

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove non existent tokens from beam results

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix bleu logging

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Adding nmt_transformer_infer.py script
Fixing bugs in .translate method

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* Comment out scaling in translate method

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Update configs

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix device placement bug

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove hacky hack

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* initial service commit

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* updates

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* disable grads in beam search

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* undo hack

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* style fix

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* Update logging procedure

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Optimize beam search

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix code style

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* magin memory fix

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* delete quant reqs

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* Add fast inference instruments

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix code style

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Change logging message a little

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Encoder-Decoder NMT Refactor (#1471)

* add EncDecNLPModel and refactor nmt

Signed-off-by: ericharper <complex451@gmail.com>

* was refactoring the wrong model

Signed-off-by: ericharper <complex451@gmail.com>

* add encoder-decoder nlp base model

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* adding enc dec tokenizers

Signed-off-by: ericharper <complex451@gmail.com>

* adding enc dec tokenizers

Signed-off-by: ericharper <complex451@gmail.com>

* add vocab_divisible_by_eight config

Signed-off-by: ericharper <complex451@gmail.com>

* add enc_dec_nmt example script

Signed-off-by: ericharper <complex451@gmail.com>

* addded enc dec example

Signed-off-by: ericharper <complex451@gmail.com>

* adding embeddings

Signed-off-by: ericharper <complex451@gmail.com>

* can instantiate embedding layers

Signed-off-by: ericharper <complex451@gmail.com>

* can instantiate embedding layers

Signed-off-by: ericharper <complex451@gmail.com>

* adding transformer encoder

Signed-off-by: ericharper <complex451@gmail.com>

* adding transformer encoder and decoder

Signed-off-by: ericharper <complex451@gmail.com>

* instaniate encoder and decoder

Signed-off-by: ericharper <complex451@gmail.com>

* instantiate head

Signed-off-by: ericharper <complex451@gmail.com>

* instantiate beam search

Signed-off-by: ericharper <complex451@gmail.com>

* instantiate loss

Signed-off-by: ericharper <complex451@gmail.com>

* setup optimizer

Signed-off-by: ericharper <complex451@gmail.com>

* delete unnec comment

Signed-off-by: ericharper <complex451@gmail.com>

* add datasets

Signed-off-by: ericharper <complex451@gmail.com>

* add datasets

Signed-off-by: ericharper <complex451@gmail.com>

* add cache_ids argument to translation dataset

Signed-off-by: ericharper <complex451@gmail.com>

* cache dataset ids per node

Signed-off-by: ericharper <complex451@gmail.com>

* add caching per node argument

Signed-off-by: ericharper <complex451@gmail.com>

* cache dataset ids per node

Signed-off-by: ericharper <complex451@gmail.com>

* cache dataset ids per node

Signed-off-by: ericharper <complex451@gmail.com>

* bug

Signed-off-by: ericharper <complex451@gmail.com>

* style fixes so that CI can proceed

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* fixing optim

Signed-off-by: ericharper <complex451@gmail.com>

* add Optional where needed in dataclasses

Signed-off-by: ericharper <complex451@gmail.com>

* move dataclass

Signed-off-by: ericharper <complex451@gmail.com>

* training is working

Signed-off-by: ericharper <complex451@gmail.com>

* refactor from structured config to hybrid config

Signed-off-by: ericharper <complex451@gmail.com>

* clean

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* transformers version upper bound

Signed-off-by: ericharper <complex451@gmail.com>

* remove num_samples from config

Signed-off-by: ericharper <complex451@gmail.com>

* add exp_manager

Signed-off-by: ericharper <complex451@gmail.com>

* lower default max_generation_delta

Signed-off-by: ericharper <complex451@gmail.com>

* clean up

Signed-off-by: ericharper <complex451@gmail.com>

* change val_check_interval default

Signed-off-by: ericharper <complex451@gmail.com>

* clean up imports

Signed-off-by: ericharper <complex451@gmail.com>

* in progress refactor

Signed-off-by: ericharper <complex451@gmail.com>

* in progress refactor

Signed-off-by: ericharper <complex451@gmail.com>

* refactor

Signed-off-by: ericharper <complex451@gmail.com>

* fixes

Signed-off-by: ericharper <complex451@gmail.com>

* add yaml

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* add jenkins test

Signed-off-by: ericharper <complex451@gmail.com>

* add jenkins test

Signed-off-by: ericharper <complex451@gmail.com>

* remove old megabert test

Signed-off-by: ericharper <complex451@gmail.com>

* add jenkins test

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* remove vocab size setter

Signed-off-by: ericharper <complex451@gmail.com>

* remove vocab size setter

Signed-off-by: ericharper <complex451@gmail.com>

* remove vocab size setter

Signed-off-by: ericharper <complex451@gmail.com>

* remove vocab size setter

Signed-off-by: ericharper <complex451@gmail.com>

* move dataclass to config.py

Signed-off-by: ericharper <complex451@gmail.com>

* update docstring

Signed-off-by: ericharper <complex451@gmail.com>

* remove vocab divis by 8 cfg

Signed-off-by: ericharper <complex451@gmail.com>

* remove unnec comments

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* Bug fixes to EncDec MT model (#1525)

* bugfixes

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* update jenkins test

Signed-off-by: ericharper <complex451@gmail.com>

* update jenkins test

Signed-off-by: ericharper <complex451@gmail.com>

* small fixes

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* config fixes

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* remove pplx

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

Co-authored-by: ericharper <complex451@gmail.com>

* minor fixes

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* Properly register tokenizers inside EncDecNLPModel

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* add pre_ln option

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* cleanups

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* batch size change

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* Add entire dataset caching via a preproc script, bpe dropout and deto‚Ä¶ (#1601)

* Add entire dataset caching via a preproc script, bpe dropout and detokenization

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Removed unused code and allow reverse lang direction in dataset

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix unused imports

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* fix style

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* bugfix

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* bugfix to bugfix

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* get rid of ModelPTConfig

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* fixing a test fail

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* unittest fix

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* set logger as False by default in TrainerConfig

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* exp manager logger fix

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* address some review feedback, bugfixes

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* add tensorboard logger back

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* NMT combine embedding plus encoder/decoder into one neural module. (#1611)

* adding transformer encoder and ecoder neural modules

Signed-off-by: ericharper <complex451@gmail.com>

* encoder decoder neural modules

Signed-off-by: ericharper <complex451@gmail.com>

* update configs and forward

Signed-off-by: ericharper <complex451@gmail.com>

* fix import

Signed-off-by: ericharper <complex451@gmail.com>

* fix import

Signed-off-by: ericharper <complex451@gmail.com>

* forgot subclass

Signed-off-by: ericharper <complex451@gmail.com>

* change comment

Signed-off-by: ericharper <complex451@gmail.com>

* interpolate hidden size and add todo comment

Signed-off-by: ericharper <complex451@gmail.com>

* remove instantiate

Signed-off-by: ericharper <complex451@gmail.com>

* add EncoderModule and DecoderModule to typing

Signed-off-by: ericharper <complex451@gmail.com>

* cleaning configs

Signed-off-by: ericharper <complex451@gmail.com>

* using named arguments

Signed-off-by: ericharper <complex451@gmail.com>

* using named arguments

Signed-off-by: ericharper <complex451@gmail.com>

* bug fix

Signed-off-by: ericharper <complex451@gmail.com>

* bug fix

Signed-off-by: ericharper <complex451@gmail.com>

* bug fix

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* schema not allowing +

Signed-off-by: ericharper <complex451@gmail.com>

* merge configs with update_model_config

Signed-off-by: ericharper <complex451@gmail.com>

* small fixes to Transformer neural modules

Signed-off-by: ericharper <complex451@gmail.com>

* small fixes to Transformer neural modules

Signed-off-by: ericharper <complex451@gmail.com>

* add abstract properties to encoder and decoder nms

Signed-off-by: ericharper <complex451@gmail.com>

* small bugfix

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* check if bert_model is there

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* Add load cached dataset and reverse lang direction to translation data config class

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Add source lang tokenization and punct normalization to model translate function

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* nmt webservice

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* style fix

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* fix bug in configs

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* updates to nmt service

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* add index.html

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* some webapp ui improvements

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* try-catch in webservice

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* Set beam size and len pen during inference

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* average eval loss

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix mnist test

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* sgdqa fix

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* sgdqa fix attempt 2

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* fixing sgdqa

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* jenkinstest to main

Signed-off-by: ericharper <complex451@gmail.com>

* remove comment

Signed-off-by: ericharper <complex451@gmail.com>

* add types to args

Signed-off-by: ericharper <complex451@gmail.com>

* add types to args

Signed-off-by: ericharper <complex451@gmail.com>

* add todo

Signed-off-by: ericharper <complex451@gmail.com>

* remove comments

Signed-off-by: ericharper <complex451@gmail.com>

* remove docstring for forward

Signed-off-by: ericharper <complex451@gmail.com>

* remove comment

Signed-off-by: ericharper <complex451@gmail.com>

* remove comment

Signed-off-by: ericharper <complex451@gmail.com>

* remove comment

Signed-off-by: ericharper <complex451@gmail.com>

* add typecheck

Signed-off-by: ericharper <complex451@gmail.com>

* remove kwargs

Signed-off-by: ericharper <complex451@gmail.com>

* use ptl defaults

Signed-off-by: ericharper <complex451@gmail.com>

* upper bound ptl to 1.13

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* fix jenkins test

Signed-off-by: ericharper <complex451@gmail.com>

* fix jenkins test

Signed-off-by: ericharper <complex451@gmail.com>

* revert typecheck

Signed-off-by: ericharper <complex451@gmail.com>

* adding enable_pl_optimizer to trainer config

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: PeganovAnton <peganoff2@mail.ru>
Co-authored-by: Jason <jasoli@nvidia.com>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Co-authored-by: ericharper <complex451@gmail.com>
Co-authored-by: Mpho Mphego <mmphego@ska.ac.za>
Co-authored-by: Aleksey Grinchuk (Oleksii Hrinchuk) <grinchuk.alexey@gmail.com>
Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2020-12-17T22:14:59Z,"Correct ASR issues + Patch for Pytorch 1.8 (#1565)

* Trim silence default to False

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update stft and torch.fft.ifft for Pytorch 1.8

Signed-off-by: smajumdar <titu1994@gmail.com>

* Style fixes

Signed-off-by: smajumdar <titu1994@gmail.com>

* Clear up old code

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2020-12-16T21:59:14Z,Adding the feature of variational noise to the RNNT-based models (#1563)
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2020-12-06T18:01:43Z,"fix for modelpt config.optim (#1524)

Signed-off-by: ekmb <ebakhturina@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2020-12-04T06:33:51Z,"Update every ASR config to new Hydra format (#1516)

* Update every ASR config to new format

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update notebook tutorials

Signed-off-by: smajumdar <titu1994@gmail.com>

* Reorder cfg

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2020-12-03T22:36:50Z,"Update model config automatically inside ModelPT (#1511)

* Add ModelPT level config update to remove `params`

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update docstring

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update imports

Signed-off-by: smajumdar <titu1994@gmail.com>

* Refactor model cfg initialization

Signed-off-by: smajumdar <titu1994@gmail.com>

* Code foratting

Signed-off-by: smajumdar <titu1994@gmail.com>

* Code formatting

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add compat with RNNT BPE models

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct optimizer and schedulers for new configs

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct return value

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correctly support new configs for ASR

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update every ASR config

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add comment tASR BPE models

Signed-off-by: smajumdar <titu1994@gmail.com>

* Reduce batch size from 64 to 32

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update name of methods

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2020-12-01T23:04:51Z,"Preserve absolute path of model restore file (#1509)

* Preserve absolute path of model restore file

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correctly expand ~

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2020-11-24T21:34:31Z,"Update ModelPT level save and restore (#1491)

* Update ModelPT level save and restore

Signed-off-by: smajumdar <titu1994@gmail.com>

* Remove logging

Signed-off-by: smajumdar <titu1994@gmail.com>

* polish style a little

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* Correct model restoration from .nemo vs .pt checkpoint

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct model restoration from .nemo vs .pt checkpoint

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2020-11-24T01:13:18Z,"Fixes to IntentSlots init logic making all TLT scripts operational (#1477)

* Vlads fix to intent test dataloader setup

Signed-off-by: Tomasz Kornuta <tkornuta@nvidia.com>

* intent model conditional initialization - depending whether user passes data_dir

Signed-off-by: Tomasz Kornuta <tkornuta@nvidia.com>

* intent: train and inference working

Signed-off-by: Tomasz Kornuta <tkornuta@nvidia.com>

* basic training operational

Signed-off-by: Tomasz Kornuta <tkornuta@nvidia.com>

* micro polish of update_data_dir_for_training

Signed-off-by: Tomasz Kornuta <tkornuta@nvidia.com>

* Intent: all TLT scripts operational, spec files with desired structure

Signed-off-by: Tomasz Kornuta <tkornuta@nvidia.com>

* Polishing the logic of Intent&Slots model, fix to the way paths are handled in ModelPT artifacts

Signed-off-by: Tomasz Kornuta <tkornuta@nvidia.com>

* skipping IntentModel exportable test to see other results

Signed-off-by: Tomasz Kornuta <tkornuta@nvidia.com>

* skipping IntentModel exportable test to see other results

Signed-off-by: Tomasz Kornuta <tkornuta@nvidia.com>

* fixes: moved tokenizer back before model init (as used in setup_*_), additional condition to cfg.data_desc

Signed-off-by: Tomasz Kornuta <tkornuta@nvidia.com>

* reverted test skip

Signed-off-by: Tomasz Kornuta <tkornuta@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2020-11-24T00:13:00Z,"Add support for model level dataclass/config class (#1328)

* Add support for model level dataclass

Signed-off-by: smajumdar <titu1994@gmail.com>

* Remove unnecessary code

Signed-off-by: smajumdar <titu1994@gmail.com>

* Reviewer comments, cleanup etc

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update support for configs

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add examples under structured dir

Signed-off-by: smajumdar <titu1994@gmail.com>

* Rebase

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add docstring

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update failing test

Signed-off-by: smajumdar <titu1994@gmail.com>

* Revert update_model_dataclass

Signed-off-by: smajumdar <titu1994@gmail.com>

* reorganize configs

Signed-off-by: smajumdar <titu1994@gmail.com>

* Move /structured under /experimental/structured and move out dataset config from ModelPT dataset config

Signed-off-by: smajumdar <titu1994@gmail.com>

* Remove hardcoded paths to datasets

Signed-off-by: smajumdar <titu1994@gmail.com>

* Make all 3 datasets optional by default

Signed-off-by: smajumdar <titu1994@gmail.com>

* Remove unnecessary imports

Signed-off-by: smajumdar <titu1994@gmail.com>

* Reduce formatting lines

Signed-off-by: smajumdar <titu1994@gmail.com>

* Formatng

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add note about parameter injection not working with data classes

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update tests

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2020-11-17T22:27:05Z,"[Bug] Ensure lr scheduler uses limit_train_batches when working out max steps (#1464)

* Ensure lr scheduler uses limit_train_batches when working out max steps

Signed-off-by: SeanNaren <sean@grid.ai>

* Fix leftover debugging error

Signed-off-by: SeanNaren <sean@grid.ai>

* Reformat

Signed-off-by: SeanNaren <sean@grid.ai>

* Add random option to use float in test

Signed-off-by: SeanNaren <sean@grid.ai>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2020-11-17T00:44:47Z,"Merge branch 'v1.0.0b2' into main

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2020-11-13T00:35:16Z,"minor fix (#1447)

Signed-off-by: Jason <jasoli@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/classes/modelPT.py,2020-11-12T21:42:43Z,"refactored EFF save/restore - NeMoCookbook (#1438)

Signed-off-by: Tomasz Kornuta <tkornuta@nvidia.com>"
github.com/NVIDIA/NeMo,tools/nmt_webapp/nmt_service.py,2021-08-09T22:31:41Z,"Add basic grpc MT server (#1807)

* Add basic grpc MT server

Add readme, server updates

Signed-off-by: Ryan Leary <rleary@nvidia.com>

* style fix

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* fixing license headers

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* Add punctuation model into NMT service

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix merge conflicts

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* style fixes to unblock CI

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* Add a Jarvis ASR + NeMo NMT client

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* style fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Refactor gRPC service

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Update license headers

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Update one more license header

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Whitepsace in header

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix grpc requirement

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Update license headers

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Add option to specify src/tgt lang and import fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix unused imports

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Renaming variables

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

Co-authored-by: Ryan Leary <rleary@nvidia.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>
Co-authored-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>"
github.com/NVIDIA/NeMo,nemo/collections/tts/models/mixer_tts.py,2023-05-30T18:14:09Z,"[TTS] corrected misleading deprecation warnings. (#6702)

* [TTS] corrected misleading deprecation warnings.
* deprecation warning is only triggered when old models applied old g2p.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

---------

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/tts/models/mixer_tts.py,2023-05-16T16:48:01Z,"Merge r1.18.0 bugfixes and doc updates to main (#6655)

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* Remove from jenkins (#6641)

* add megatron_core to requirements

Signed-off-by: ericharper <complex451@gmail.com>

* remove from jenkins

Signed-off-by: ericharper <complex451@gmail.com>

---------

Signed-off-by: ericharper <complex451@gmail.com>

* remove dup

Signed-off-by: ericharper <complex451@gmail.com>

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* [TTS] reformat NeMo versions in the tts logging messages to avoid batch process them when upgrading NeMo versions.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

---------

Signed-off-by: ericharper <complex451@gmail.com>
Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/tts/models/mixer_tts.py,2023-02-24T21:20:48Z,"[TTS][refactor] Part 2 - nemo.colletions.tts.parts (#6105)

* [TTS][refactor] Part 2 - nemo.colletions.tts.parts
* aggregate imports
* added __init__.py
---------
Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/tts/models/mixer_tts.py,2023-02-15T22:22:50Z,"[TTS/TN/G2P] Remove Text Processing from NeMo, move G2P to TTS (#5982)

* remove TN

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix imports

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix import

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add missing init

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix import

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* rename unit test

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix import

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix modules test

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix imports

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* remove whitelist from config

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* delete wordid file

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* remove pynini_install from tutorials

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* update requirements

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add support warning

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* review

Signed-off-by: ekmb <ebakhturina@nvidia.com>

---------

Signed-off-by: ekmb <ebakhturina@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/tts/models/mixer_tts.py,2022-10-27T21:18:31Z,"[TTS] Fastpitch energy condition and refactoring (#5218)

* Incorporating Energy conditioning in FastPitch

Signed-off-by: subhankar-ghosh <subhankar2321@gmail.com>

* Minor fixes in Energy conditioning in FastPitch

Signed-off-by: subhankar-ghosh <subhankar2321@gmail.com>

* Add Energy conditioning in FastPitch to infer method

Signed-off-by: subhankar-ghosh <subhankar2321@gmail.com>

* adding fn to function names

Signed-off-by: subhankar-ghosh <subhankar2321@gmail.com>

* Incorporating Energy conditioning in FastPitch

Signed-off-by: subhankar-ghosh <subhankar2321@gmail.com>

* Minor fixes in Energy conditioning in FastPitch

Signed-off-by: subhankar-ghosh <subhankar2321@gmail.com>

* Add Energy conditioning in FastPitch to infer method

Signed-off-by: subhankar-ghosh <subhankar2321@gmail.com>

* adding fn to function names

Signed-off-by: subhankar-ghosh <subhankar2321@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* remove ifelse from batching, minor refactoring changes in energy code

Signed-off-by: subhankar-ghosh <subhankar2321@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Refactor based on PR comments.

Signed-off-by: subhankar-ghosh <subhankar2321@gmail.com>

* Added support for not learning alignment in energy

Signed-off-by: subhankar-ghosh <subhankar2321@gmail.com>

* Fix typo in assert statemetn

Signed-off-by: subhankar-ghosh <subhankar2321@gmail.com>

* Renaming average_pitch to average_features

Signed-off-by: subhankar-ghosh <subhankar2321@gmail.com>

* Renaming len variable name as it is a keyword

Signed-off-by: subhankar-ghosh <subhankar2321@gmail.com>

* Renaming len variable name as it is a keyword

Signed-off-by: subhankar-ghosh <subhankar2321@gmail.com>

Signed-off-by: subhankar-ghosh <subhankar2321@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/tts/models/mixer_tts.py,2022-08-23T19:56:07Z,"TTS tokenizers moved to collections.common.tokenizers (#4690)

* fixed branch in IR tutorial

Signed-off-by: AlexGrinch <grinchuk.alexey@gmail.com>

* ddp translate GPU allocation fix

Signed-off-by: AlexGrinch <grinchuk.alexey@gmail.com>

* map_location instead of set_device

Signed-off-by: AlexGrinch <grinchuk.alexey@gmail.com>

* tts tokenizers moved to collections.common.tokenizers

Signed-off-by: AlexGrinch <grinchuk.alexey@gmail.com>

* imports and style fixes

Signed-off-by: AlexGrinch <grinchuk.alexey@gmail.com>

* copyright fix

Signed-off-by: AlexGrinch <grinchuk.alexey@gmail.com>

* two last paths refactor

Signed-off-by: AlexGrinch <grinchuk.alexey@gmail.com>

* ugly backward compatibility fix

Signed-off-by: AlexGrinch <grinchuk.alexey@gmail.com>

* codestyle fix

Signed-off-by: AlexGrinch <grinchuk.alexey@gmail.com>

* g2ps moved to nemo_text_processing

Signed-off-by: AlexGrinch <grinchuk.alexey@gmail.com>

* nemo_text_processing import path fix

Signed-off-by: AlexGrinch <grinchuk.alexey@gmail.com>

* nemo_text_processing import path fix

Signed-off-by: AlexGrinch <grinchuk.alexey@gmail.com>

* style fix

Signed-off-by: AlexGrinch <grinchuk.alexey@gmail.com>

* style fix

Signed-off-by: AlexGrinch <grinchuk.alexey@gmail.com>

Signed-off-by: AlexGrinch <grinchuk.alexey@gmail.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>"
github.com/NVIDIA/NeMo,nemo/collections/tts/models/mixer_tts.py,2022-08-04T17:03:39Z,"Pynini dependency fix (#4674)

* nlp guard, logging removed

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* pynini conda install for mac, add import guard to TN tests

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* tts pynini dependency

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* add try/except block to tts models

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* add missing file

Signed-off-by: ekmb <ebakhturina@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/tts/models/mixer_tts.py,2022-03-01T00:43:59Z,"Merge r1.7.0 main (#3773)

* Tn bug 1.7.0 (#3730)

* fix es and fr bug

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* add file

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* [TTS] Fix bugs in E2E TTS, Mixer-TTS and FastPitch (#3740)

* fix bugs

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* fix bug in e2e tts and mixer tts

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* Mirror AN4 data while servers are down (#3743)

Signed-off-by: smajumdar <titu1994@gmail.com>

* Bugfix for GPT eval  (#3744)

* use tokens_cut not tokens

Signed-off-by: ericharper <complex451@gmail.com>

* remove precision conversion and comment jit for bias gelu

Signed-off-by: ericharper <complex451@gmail.com>

* revert comment update mbs in config

Signed-off-by: ericharper <complex451@gmail.com>

* calculate micro_batch_size during complete and compute_logprobs

Signed-off-by: ericharper <complex451@gmail.com>

* ASR SSL update (#3746)

* ssl update

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* tutorial update

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* Fix SSL configs for 1.7 (#3748)

* ssl update

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* tutorial update

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* revert configs

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* revert configs

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* punct process bug fix (#3747)

Signed-off-by: ekmb <ebakhturina@nvidia.com>

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>

* updated conformer models. (#3741)

Signed-off-by: Vahid <vnoroozi@nvidia.com>

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>

* Yuya/megatron t5 glue eval (#3751)

* Add megatron t5 glue eval-only script

Signed-off-by: Yu Yao <yuya@nvidia.com>

* Update megatron t5 glue eval default configs

Signed-off-by: Yu Yao <yuya@nvidia.com>

* Update megatron t5 glue eval configs

Signed-off-by: Yu Yao <yuya@nvidia.com>

* Update config comments

Signed-off-by: Yu Yao <yuya@nvidia.com>

Co-authored-by: Yu Yao <yuya@nvidia.com>

* Specify gpus in SSL notebook (#3753)

* ssl update

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* tutorial update

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* revert configs

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* revert configs

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* specify gpus

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* Duplex model inference fix, money encoder fix (#3754)

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* Update docs for RNNT and overriding fused batch size (#3755)

Signed-off-by: smajumdar <titu1994@gmail.com>

* fix consumed samples calculation + PTune Model bugs (#3738)

* fix the way computing consumed samples

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fixed ptune model

Signed-off-by: Yi Dong <yidong@nvidia.com>

* make sure notebook is working

Signed-off-by: Yi Dong <yidong@nvidia.com>

* added try-catch

Signed-off-by: Yi Dong <yidong@nvidia.com>

Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* fix directories in ssl notebook (#3758)

* ssl update

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* tutorial update

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* revert configs

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* revert configs

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* specify gpus

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* update dirs

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* TN docs update (#3735)

* TN docs update: audio based docs added, quick start, ref fixed, etc

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* add deployment script dir and Sp TN

Signed-off-by: ekmb <ebakhturina@nvidia.com>

Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>

* Update Tacotron2_Training.ipynb (#3769)

Signed-off-by: Jason <jasoli@nvidia.com>

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* update requirements and package info

Signed-off-by: ericharper <complex451@gmail.com>

* revert

Signed-off-by: ericharper <complex451@gmail.com>

* revert

Signed-off-by: ericharper <complex451@gmail.com>

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>
Co-authored-by: Oktai Tatanov <oktai.tatanov@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: Samuel Kriman <samuelkriman@gmail.com>
Co-authored-by: Evelina <10428420+ekmb@users.noreply.github.com>
Co-authored-by: Vahid Noroozi <VahidooX@users.noreply.github.com>
Co-authored-by: yaoyu-33 <54727607+yaoyu-33@users.noreply.github.com>
Co-authored-by: Yu Yao <yuya@nvidia.com>
Co-authored-by: Yi Dong <43824965+yidong72@users.noreply.github.com>
Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>
Co-authored-by: Jason <jasoli@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/tts/models/mixer_tts.py,2022-02-11T22:26:51Z,"[TTS] Add Mixed Representation Training (#3473)

* Update CMUdict with ADLR version pronunciations

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* minor updates for finetuning

Signed-off-by: Jason <jasoli@nvidia.com>

* update conf

Signed-off-by: Jason <jasoli@nvidia.com>

* merge

Signed-off-by: Jason <jasoli@nvidia.com>

* update

Signed-off-by: Jason <jasoli@nvidia.com>

* update

Signed-off-by: Jason <jasoli@nvidia.com>

* bug fixes

Signed-off-by: Jason <jasoli@nvidia.com>

* update config

Signed-off-by: Jason <jasoli@nvidia.com>

* bf16 support

Signed-off-by: Jason <jasoli@nvidia.com>

* bf16 support

Signed-off-by: Jason <jasoli@nvidia.com>

* bugfix

Signed-off-by: Jason <jasoli@nvidia.com>

* update

Signed-off-by: Jason <jasoli@nvidia.com>

* finalize changes

Signed-off-by: Jason <jasoli@nvidia.com>

* undo notebook 1.6.0 pins

Signed-off-by: Jason <jasoli@nvidia.com>

* more 1.6.0 undos

Signed-off-by: Jason <jasoli@nvidia.com>

* wip

Signed-off-by: Jason <jasoli@nvidia.com>

* update num_workers

Signed-off-by: Jason <jasoli@nvidia.com>

* update hypers

Signed-off-by: Jason <jasoli@nvidia.com>

* revert to main _align yamls

Signed-off-by: Jason <jasoli@nvidia.com>

* update yamls

Signed-off-by: Jason <jasoli@nvidia.com>

* cleanup

Signed-off-by: Jason <jasoli@nvidia.com>

* remove unnecessary line

Signed-off-by: Jason <jasoli@nvidia.com>

* address comments

Signed-off-by: Jason <jasoli@nvidia.com>

* update vocoder mel uploading; add contextmanager to mixed g2p

Signed-off-by: Jason <jasoli@nvidia.com>

* update comments; make prob required argument

Signed-off-by: Jason <jasoli@nvidia.com>

* added val check

Signed-off-by: Jason <jasoli@nvidia.com>

* update message

Signed-off-by: Jason <jasoli@nvidia.com>

* update

Signed-off-by: Jason <jasoli@nvidia.com>

* revert num workers

Signed-off-by: Jason <jasoli@nvidia.com>

Co-authored-by: Jocelyn Huang <jocelynh@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/tts/models/mixer_tts.py,2022-02-02T17:22:50Z,"[TTS] Update UnivNet, HiFi-GAN and WaveGlow, small fixes in Mixer-TTS, FastPitch and Exportable (#3585)

* update hifigan and mixer tts

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* remove unused import

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* fix bug with sched_config

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* fix bug with set_struct

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* update hifigan and univnet

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* update univnet

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* update waveglow

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* fix bug

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* fix bug

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* remove unnecessary header from configs

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* fix comments

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>"
github.com/NVIDIA/NeMo,nemo/collections/tts/models/mixer_tts.py,2022-02-02T07:18:45Z,"[TTS] Fix bugs in HiFi-GAN (scheduler, optimizers) and add input_example() in Mixer-TTS/Mixer-TTS-X (#3564)

* update hifigan and mixer tts

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* remove unused import

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* fix bug with sched_config

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* fix bug with set_struct

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

Co-authored-by: Jason <jasoli@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/tts/models/mixer_tts.py,2022-01-11T16:34:16Z,"Update Mixer-TTS, FastPitch and TTSDataset (#3366)

* update tts dataset, fastpitch and mixer tts

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* fix style and notebooks

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* update notebooks

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* update mixer-tts, mixer-tts-x and fastpitch configs

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* update notebooks and configs

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* update configs

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* add links, update README, fix tutorials

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* fix style

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* remove unnecessary code from fastpitch model

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* update jenkinsfile and fastpitch typo fix

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* fix configs

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* revert jenkinsfile

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>"
github.com/NVIDIA/NeMo,nemo/collections/tts/models/mixer_tts.py,2021-12-15T09:38:06Z,"Fix MixerTTS types and dimensions (#3330)

* Fix MixerTTS types and dimensions

Signed-off-by: Markus Toman <m.toman@neuratec.com>

* fix style

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

Co-authored-by: Markus Toman <m.toman@neuratec.com>"
github.com/NVIDIA/NeMo,nemo/collections/tts/models/mixer_tts.py,2021-12-02T15:10:12Z,"Normalizer to TTS models, TTS tokenizer updates, AxisKind updates (#3271)

* update tts tokenizer, mixer tts and add normalizer to some models

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* fix bugs in mixer tts, update types to aligner loss

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* fix bugs, update fastpitch types

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* fix style

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* update german dataset, update fastpitch

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* update types, fix comments

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* add cmudict and heteronyms paths

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>"
github.com/NVIDIA/NeMo,nemo/collections/tts/models/mixer_tts.py,2021-11-01T15:56:14Z,"add export methods for mixer-tts (#3082)

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

Co-authored-by: Jason <jasoli@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/tts/models/mixer_tts.py,2021-10-25T14:50:48Z,"MixerTTS, MixerTTSDataset and small updates in tts tokenizers  (#2859)

* new vocabs and g2ps for tts

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* fix style

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* update tts torch data

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* update g2p modules, data and add example for tts vocabs

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* fix style

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* update tts dataset

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* add tokens field to tts dataset

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* update tts dataset

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* add TTSDataset and docs for all of them

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* fix paths in yaml

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* update test for tts dataset

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* add heteronyms-030921 file to scripts folder

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* change requirements_torch_tts.txt

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* add tts_data_types.py

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* fix style tts_data_types.py

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* update yaml and comments

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* add mixer tts dataset

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* add example.py to tts torch

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* update helpers.py

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* add mixer tts model and ds

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* update mixer tts dataset

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* update tokenizers

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* update tts tokenizer

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* update tts dataset and mixer tts model

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* update tts_dataset.yaml

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* add copyright header to mixer_tts file

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* update import in tts/torch/data.py

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* add fastpitch in mixer tts code

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* add raw version of benchmark

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* remove without matching mode

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* remove unnecessary flags in model

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* refactor mixer tts

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* remove unnecessary blocks in mixer tts model

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* update tts_tokenizers.py

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* add mixer tts x config and run script

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* fix elif and unnecessary file

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* add types for mixer tts methods

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

Co-authored-by: Jason <jasoli@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/core/config/templates/model_card.py,2024-02-06T00:26:43Z,"Support uploading NeMo models to HF via `push_to_hf_hub()` (#8263)

* Initial support for saving unpacked nemo file directly and support for uploading NeMo models to Huggingface Hub

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add support to restore nemo models from HF in unpacked format

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Correct input types for model card

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update License Section to template

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add unit test to restore via unpacked checkpoint

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix typo

Signed-off-by: smajumdar <titu1994@gmail.com>

* Address comments

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Address comments

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add docs and address comments

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/modules/common/lm_utils.py,2023-01-28T14:45:44Z,"RETRO model finetuning (#5800)

* add save and load dynmaic index

Signed-off-by: Yi Dong <yidong@nvidia.com>

* add chunk stride feature

Signed-off-by: Yi Dong <yidong@nvidia.com>

* add chunk stride feature

Signed-off-by: Yi Dong <yidong@nvidia.com>

* add no pq index

Signed-off-by: Yi Dong <yidong@nvidia.com>

* added megatron lm compatible mode

Signed-off-by: Yi Dong <yidong@nvidia.com>

* addd config

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix position embedding

Signed-off-by: Yi Dong <yidong@nvidia.com>

* added index factory

Signed-off-by: Yi Dong <yidong@nvidia.com>

* share neighbors and weights amoung strategies

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix bug

Signed-off-by: Yi Dong <yidong@nvidia.com>

* added metric tto faiss index

Signed-off-by: Yi Dong <yidong@nvidia.com>

* set default to inner product

Signed-off-by: Yi Dong <yidong@nvidia.com>

* added qa fine tuen dataset

Signed-off-by: Yi Dong <yidong@nvidia.com>

* added fine tuning code

Signed-off-by: Yi Dong <yidong@nvidia.com>

* trim it

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix data issue

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix style

Signed-off-by: Yi Dong <yidong@nvidia.com>

* added version

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix key error

Signed-off-by: Yi Dong <yidong@nvidia.com>

* make sure to overwrite the cfg

Signed-off-by: Yi Dong <yidong@nvidia.com>

* make multiple sentence bert available

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix the document

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix the table

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix transformer

Signed-off-by: Yi Dong <yidong@nvidia.com>

* make sure to turn off the rope in chunked cross attention layer

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix the security issue

Signed-off-by: Yi Dong <yidong@nvidia.com>

* style fix

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix codeql issues

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix

Signed-off-by: Yi Dong <yidong@nvidia.com>

* use -1

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix empty index

Signed-off-by: Yi Dong <yidong@nvidia.com>

* clean up

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix the lower bound for repetition penalty

Signed-off-by: Yi Dong <yidong@nvidia.com>

* add retro qa inference strategy

Signed-off-by: Yi Dong <yidong@nvidia.com>

* added new inference logic

Signed-off-by: Yi Dong <yidong@nvidia.com>

* working inference

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix TP inference

Signed-off-by: Yi Dong <yidong@nvidia.com>

* revert requirement

Signed-off-by: Yi Dong <yidong@nvidia.com>

* added file inference

Signed-off-by: Yi Dong <yidong@nvidia.com>

* use string to prevent collison

Signed-off-by: Yi Dong <yidong@nvidia.com>

* use NQ test

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix prompt

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix inference

Signed-off-by: Yi Dong <yidong@nvidia.com>

* set good defaults for demo

Signed-off-by: Yi Dong <yidong@nvidia.com>

* replicate adlr

Signed-off-by: Yi Dong <yidong@nvidia.com>

* make sure to turn off attention reset for megatron lm compatible model

Signed-off-by: Yi Dong <yidong@nvidia.com>

* style fix

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix typo

Signed-off-by: Yi Dong <yidong@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix inference error

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix logging

Signed-off-by: Yi Dong <yidong@nvidia.com>

* address comments

Signed-off-by: Yi Dong <yidong@nvidia.com>

---------

Signed-off-by: Yi Dong <yidong@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/modules/common/lm_utils.py,2023-01-06T04:47:58Z,"Support non-standard padding token id (#5543)

* Support non-standard padding token id

Read the id of the padding token from the tokenizer when creating the
embedding, rather than always defaulting to 0. This allows use of
(admittedly bizarre) non-standard tokenizer models that don't give the
padding token <PAD> the id 0.

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Co-authored-by: Numeri <kaden.uhlig@lilt.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/modules/common/lm_utils.py,2022-09-08T20:41:11Z,"removed unused imports for all domains. (#4901)


Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/modules/common/lm_utils.py,2022-04-09T04:46:15Z,"Merge r1.8 main (#3959)

* update version

Signed-off-by: ericharper <complex451@gmail.com>

* Stateless timer fix for PTL 1.6 (#3925)

* Stateless timer fix for PTL 1.6

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Stateless timer PTL test

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix year

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove unused imports

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* GPU test

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* clean import

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: ericharper <complex451@gmail.com>

* fix save_best missing chpt bug, update for setup_tokenizer() changes (#3932)

* fix save_best missing chpt bug, update for setup_tokenizer() changes

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* style fix

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* Fix divide by world size (#3941)

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* remove old doc (#3946)

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* Fix issues with librosa deprecations (#3950)

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix issue with Segfault in ASR models (#3956)

* Fix issue with Segfault in ASR models

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add docstring

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix notebook bugs for branch r1.8.0 (#3948)

* load the model from ngc

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix all biomegatron notebook

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix the typos

Signed-off-by: Yi Dong <doyend@gmail.com>

* remove output

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix isort

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix merge error

Signed-off-by: Yi Dong <doyend@gmail.com>

* change ntpath for isort workaround

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix unit test

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix ci

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix ci bert pretraining

Signed-off-by: Yi Dong <doyend@gmail.com>

* make it compatible with main

Signed-off-by: Yi Dong <doyend@gmail.com>

* add the teste for biomegatron ner

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix argument

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix usablity issue

Signed-off-by: Yi Dong <doyend@gmail.com>

* work around

Signed-off-by: Yi Dong <doyend@gmail.com>

Co-authored-by: Yi Dong <doyend@gmail.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* Fix global batch fit loop (#3936)

* add lightning module hooks for global batch

Signed-off-by: ericharper <complex451@gmail.com>

* clean scripts

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* DP=1 fix

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* set num dataset workers to 2

Signed-off-by: ericharper <complex451@gmail.com>

* update validation_loop with GlobalDataFetcher

Signed-off-by: ericharper <complex451@gmail.com>

* add test global data fetcher

Signed-off-by: ericharper <complex451@gmail.com>

* Drop last for test ds

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix test epoch end

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix eval

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix reconfigure microbatch in the complete method

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* add comments

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Set init consumed samples

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* fix shuffle

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* add save_restore_connector arg

Signed-off-by: ericharper <complex451@gmail.com>

* Fix padding for labels and loss mask

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* GLUE/XNLI CI tests

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* limit val batches in hydra fix

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Restart CI

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix unittest

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

Co-authored-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Update max_epochs on megatron configs (#3958)

* update config

Signed-off-by: ericharper <complex451@gmail.com>

* update config

Signed-off-by: ericharper <complex451@gmail.com>

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* update version

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Evelina <10428420+ekmb@users.noreply.github.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: Yi Dong <43824965+yidong72@users.noreply.github.com>
Co-authored-by: Yi Dong <doyend@gmail.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/modules/common/lm_utils.py,2022-04-04T19:12:30Z,"Megatron legacy conversion support (#3919)

* Fix merge from main

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Rollback scripts/export.py changes and create a new file for converting legacy MegatronBert NLP models to current version

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Cleanup legacy conversion code

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Move state dict mapping of legacy Megatron to NLPSaveRestoreConnector; Do transpose op in Self attention if it's a legacy checkpoint; Add a example notebook for exporting NLP Bert models

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Style fix

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

Co-authored-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/modules/common/lm_utils.py,2022-03-30T01:20:18Z,"Megatron support (#3893)

* Add MegatronBert training support for more NLP models; Replace check for nemo_file existence to decide if it's megatron training or not; Add support for finetuning a downstream NLP task model on a different downstream dataset using MegatronBert; Add skelton support for ONNX export of MegatronBert

* Remove duplicate lm_checkpoint key in config

* Adding new Apex classes for export replacement and default trainer support

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixing order of exported inputs in NLP models

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixed typo

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixed ORT check

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Refactor of NLP models initialization

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixed style

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixed runtime check

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Code style fixes

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Code style fixes

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Code style fixes

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Check for existence of downstream key before setting it

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Fix Huggingface download unit tests based on get_lm_model refactor

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Add register artifact for MegatronBert models; Remove unused import

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Fixed style

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixed duplex decoder init

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* moved set_world_size up

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixing _trainer initialization

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixed GPT tokenizer init

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fix Token classification and Q&A forward flag checks for MegatronBert

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Fix world_size init

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Skip GPT eval test

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Fix get_lm_model function calls based on recent refactoring

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Check for presence of keys in the Dict before trying to access them in BERTLMModel

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Check if tokenizer is present before accesing it in lm_utils

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Check for presence of keys in the Dict before trying to register them as artifacts

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Bypass NLP init statements pertaining to MegatronBert when using Ptune downstream task

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Revert NLPModel modification

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Skip the Jenkins Test Megatron P-Tuning GPT LM

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Merge branch 'main' into megatron_support

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

Co-authored-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>
Co-authored-by: Boris Fomitchev <bfomitchev@nvidia.com>
Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Co-authored-by: Boris Fomitchev <borisfom@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/modules/common/lm_utils.py,2022-01-28T19:48:04Z,"More fixes to SentencePiece for T5 (#3515)

* More fixes to spm for T5

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/modules/common/lm_utils.py,2022-01-15T03:09:00Z,"BioMegatron token classification tutorial fix to be compatible with current Megatron BERT (#3435)

* fixed the tokenizer

Signed-off-by: Yi Dong <yidong@nvidia.com>

* training is working

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fixed text

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fixed text

Signed-off-by: Yi Dong <yidong@nvidia.com>

* working notebook

Signed-off-by: Yi Dong <yidong@nvidia.com>

* style fix

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fixed text

Signed-off-by: Yi Dong <yidong@nvidia.com>

* handles the different megatron-lm checkpoint versions

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fixed the text classification notebook

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fixed key error

Signed-off-by: Yi Dong <yidong@nvidia.com>

* more key error

Signed-off-by: Yi Dong <yidong@nvidia.com>

* replace the old notebooks

Signed-off-by: Yi Dong <yidong@nvidia.com>

* register vocab to nemo file

Signed-off-by: Yi Dong <yidong@nvidia.com>

* added the missing notebook

Signed-off-by: Yi Dong <yidong@nvidia.com>

Co-authored-by: Eric Harper <complex451@gmail.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/modules/common/lm_utils.py,2021-10-27T18:13:29Z,"Merge r1.5.0 bugfixes and doc updates to main (#3048)

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* Always save last checkpoint on train end even if folder does not exist (#2976)

* add fix for no checkpoint folder when training ends

Signed-off-by: Jason <jasoli@nvidia.com>

* update

Signed-off-by: Jason <jasoli@nvidia.com>

* fix test

Signed-off-by: Jason <jasoli@nvidia.com>

* fixes

Signed-off-by: Jason <jasoli@nvidia.com>

* typo

Signed-off-by: Jason <jasoli@nvidia.com>

* change check

Signed-off-by: Jason <jasoli@nvidia.com>

* [NLP] Add Apex import guard (#3041)

* add apex import guard

Signed-off-by: ericharper <complex451@gmail.com>

* add apex import guard

Signed-off-by: ericharper <complex451@gmail.com>

* add apex import guard

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* remove from init add logging to constructor

Signed-off-by: ericharper <complex451@gmail.com>

* remove from init add logging to constructor

Signed-off-by: ericharper <complex451@gmail.com>

* remove import from init

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert encoder logic from NLPModel

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert from init

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert from init

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert from init

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert from init

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert from init

Signed-off-by: ericharper <complex451@gmail.com>

* remove megatron bert from init

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Jason <jasoli@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/modules/common/lm_utils.py,2021-10-21T03:06:37Z,"[BigNLP] Merge Megatron GPT to main (#2975)

* fix gpu init after removing debug print in mpu

Signed-off-by: ericharper <complex451@gmail.com>

* add fused_adam

Signed-off-by: ericharper <complex451@gmail.com>

* check ds is not none before logging len

Signed-off-by: ericharper <complex451@gmail.com>

* set fp16 arg to true and fix enum conflict

Signed-off-by: ericharper <complex451@gmail.com>

* make fp16 arg configurable

Signed-off-by: ericharper <complex451@gmail.com>

* add grad clip from megatron

Signed-off-by: ericharper <complex451@gmail.com>

* Linear warmup with cosine annealing and constant holding (#2846)

* Testing cosine schedule

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* More fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* update config for constant steps in schedule

Signed-off-by: ericharper <complex451@gmail.com>

* temporarily import enum from megatron

Signed-off-by: ericharper <complex451@gmail.com>

* add grad clip for fp32

Signed-off-by: ericharper <complex451@gmail.com>

* update check for _del_model_without_trainer

Signed-off-by: ericharper <complex451@gmail.com>

* updating restore for model parallel

Signed-off-by: ericharper <complex451@gmail.com>

* add predict script

Signed-off-by: ericharper <complex451@gmail.com>

* update test iters

Signed-off-by: ericharper <complex451@gmail.com>

* add barrier

Signed-off-by: ericharper <complex451@gmail.com>

* return if clip_val is 0 or None

Signed-off-by: ericharper <complex451@gmail.com>

* when using amp clip grads after they are unscaled

Signed-off-by: ericharper <complex451@gmail.com>

* make native amp scaler hyperparams configurable

Signed-off-by: ericharper <complex451@gmail.com>

* (1) nvfuser, (2) amp-casting decoration (#2894)

* (1) nvfuser, (2) amp-casting decoration

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* support bf16

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* update package info

Signed-off-by: ericharper <complex451@gmail.com>

* add set device to constructor

Signed-off-by: ericharper <complex451@gmail.com>

* set_device in constructor

Signed-off-by: ericharper <complex451@gmail.com>

* [BigNLP] Remove megatron-lm dependency. (#2910)

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* add load_fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* add load_fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* update megatron_init

Signed-off-by: ericharper <complex451@gmail.com>

* add fused kernels

Signed-off-by: ericharper <complex451@gmail.com>

* add fused kernels

Signed-off-by: ericharper <complex451@gmail.com>

* update process batch

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* add megatron clip_grad

Signed-off-by: ericharper <complex451@gmail.com>

* trying to resolve circular import error

Signed-off-by: ericharper <complex451@gmail.com>

* rename file

Signed-off-by: ericharper <complex451@gmail.com>

* remove non-gpt models and datasets from __init__ files

Signed-off-by: ericharper <complex451@gmail.com>

* set device in constructorfor gpu init

Signed-off-by: ericharper <complex451@gmail.com>

* set device in constructorfor gpu init

Signed-off-by: ericharper <complex451@gmail.com>

* set_device in constructor

Signed-off-by: ericharper <complex451@gmail.com>

* clean config

Signed-off-by: ericharper <complex451@gmail.com>

* update MegatronDataset

Signed-off-by: ericharper <complex451@gmail.com>

* clean up MegatronModule

Signed-off-by: ericharper <complex451@gmail.com>

* clean up MegatronModule

Signed-off-by: ericharper <complex451@gmail.com>

* rename fp16 and bf16 flags to fused_softmax_input_in_fp16/bf16

Signed-off-by: ericharper <complex451@gmail.com>

* rename to fused_fp16

Signed-off-by: ericharper <complex451@gmail.com>

* add fused_fp16 arg to LayerNorm calls

Signed-off-by: ericharper <complex451@gmail.com>

* fix arg name

Signed-off-by: ericharper <complex451@gmail.com>

* fix arg name

Signed-off-by: ericharper <complex451@gmail.com>

* fix import

Signed-off-by: ericharper <complex451@gmail.com>

* update arg

Signed-off-by: ericharper <complex451@gmail.com>

* skip warmup default to True

Signed-off-by: ericharper <complex451@gmail.com>

* skip warmup default to True

Signed-off-by: ericharper <complex451@gmail.com>

* Adding complete method to MegatronGPTModel (#2935)

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* make ffn_hidden_size mandatory

Signed-off-by: ericharper <complex451@gmail.com>

* Manually migrating timing of step into branch (#2937)

* 1. Manually migrating timing of step into branch.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Updated file name and content.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Updated to latest code.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

Co-authored-by: Micha Livne <mlivne@nvidia.com>

* remove unused imports

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* check fused_fp16 and fused_bf16 are not both True

Signed-off-by: ericharper <complex451@gmail.com>

* update predict script for model parallel .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>
Co-authored-by: Micha Livne <mlivne@nvidia.com>

* NVfuser (#2943)

* activation checkpoint recompute

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* selective nvfuser setup

* Megatron gpt bfloat support (#2926)

* Save/restore fix

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Another merge

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Bf16 args in init

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Set precision

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove debug stuff

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* add bf16 casting decorator

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* Bfloat layernorm propagation

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* activation checkpoint recompute

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* selective nvfuser setup

* More arg removal

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove BERTDataset

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* update to latest apex and patch transformer autocast

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: ericharper <complex451@gmail.com>

* don't set jit for bf16

Signed-off-by: ericharper <complex451@gmail.com>

* replace apex.mpu

Signed-off-by: ericharper <complex451@gmail.com>

* fix grad clip

Signed-off-by: ericharper <complex451@gmail.com>

* NVFuser fixes (#2951)

* Fuser fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove dummy handler

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove PTL plugin based logic for fusion

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* remove duplicated file

Signed-off-by: ericharper <complex451@gmail.com>

* typo (#2960)

Signed-off-by: ericharper <complex451@gmail.com>

* [BigNLP] Script to convert GPT checkpoint to .nemo (#2958)

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* add load_fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* add load_fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* update megatron_init

Signed-off-by: ericharper <complex451@gmail.com>

* add fused kernels

Signed-off-by: ericharper <complex451@gmail.com>

* add fused kernels

Signed-off-by: ericharper <complex451@gmail.com>

* update process batch

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* add megatron clip_grad

Signed-off-by: ericharper <complex451@gmail.com>

* trying to resolve circular import error

Signed-off-by: ericharper <complex451@gmail.com>

* rename file

Signed-off-by: ericharper <complex451@gmail.com>

* remove non-gpt models and datasets from __init__ files

Signed-off-by: ericharper <complex451@gmail.com>

* set device in constructorfor gpu init

Signed-off-by: ericharper <complex451@gmail.com>

* set device in constructorfor gpu init

Signed-off-by: ericharper <complex451@gmail.com>

* set_device in constructor

Signed-off-by: ericharper <complex451@gmail.com>

* clean config

Signed-off-by: ericharper <complex451@gmail.com>

* update MegatronDataset

Signed-off-by: ericharper <complex451@gmail.com>

* clean up MegatronModule

Signed-off-by: ericharper <complex451@gmail.com>

* clean up MegatronModule

Signed-off-by: ericharper <complex451@gmail.com>

* rename fp16 and bf16 flags to fused_softmax_input_in_fp16/bf16

Signed-off-by: ericharper <complex451@gmail.com>

* rename to fused_fp16

Signed-off-by: ericharper <complex451@gmail.com>

* add fused_fp16 arg to LayerNorm calls

Signed-off-by: ericharper <complex451@gmail.com>

* fix arg name

Signed-off-by: ericharper <complex451@gmail.com>

* fix arg name

Signed-off-by: ericharper <complex451@gmail.com>

* fix import

Signed-off-by: ericharper <complex451@gmail.com>

* update arg

Signed-off-by: ericharper <complex451@gmail.com>

* skip warmup default to True

Signed-off-by: ericharper <complex451@gmail.com>

* skip warmup default to True

Signed-off-by: ericharper <complex451@gmail.com>

* Adding complete method to MegatronGPTModel (#2935)

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* make ffn_hidden_size mandatory

Signed-off-by: ericharper <complex451@gmail.com>

* Manually migrating timing of step into branch (#2937)

* 1. Manually migrating timing of step into branch.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Updated file name and content.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Updated to latest code.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

Co-authored-by: Micha Livne <mlivne@nvidia.com>

* remove unused imports

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* check fused_fp16 and fused_bf16 are not both True

Signed-off-by: ericharper <complex451@gmail.com>

* update predict script for model parallel .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* add script to convert .ckpt to .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* in progress

Signed-off-by: ericharper <complex451@gmail.com>

* update

Signed-off-by: ericharper <complex451@gmail.com>

* convert mp checkpoints to nemo

Signed-off-by: ericharper <complex451@gmail.com>

* update help

Signed-off-by: ericharper <complex451@gmail.com>

* add safeguard for model parallel save_to

Signed-off-by: ericharper <complex451@gmail.com>

* adjust NLPModel save_to to be safer for model parallel

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>
Co-authored-by: Micha Livne <mlivne@nvidia.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* [BigNLP] Update GPT evaluation to work with tensor model parallel  (#2959)

* in progress

Signed-off-by: ericharper <complex451@gmail.com>

* update args

Signed-off-by: ericharper <complex451@gmail.com>

* add request dataset

Signed-off-by: ericharper <complex451@gmail.com>

* tokenize request

Signed-off-by: ericharper <complex451@gmail.com>

* in progress

Signed-off-by: ericharper <complex451@gmail.com>

* able to run

Signed-off-by: ericharper <complex451@gmail.com>

* reduce logits

Signed-off-by: ericharper <complex451@gmail.com>

* capture response

Signed-off-by: ericharper <complex451@gmail.com>

* squeeze and unsqueeze

Signed-off-by: ericharper <complex451@gmail.com>

* handle non model parallel case

Signed-off-by: ericharper <complex451@gmail.com>

* clean imports

Signed-off-by: ericharper <complex451@gmail.com>

* add file

Signed-off-by: ericharper <complex451@gmail.com>

* convert logits to log_probs

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* rename logits to log_probs

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* add megatron gpt pretraining

Signed-off-by: ericharper <complex451@gmail.com>

* add megatron gpt pretraining

Signed-off-by: ericharper <complex451@gmail.com>

* add megatron gpt pretraining

Signed-off-by: ericharper <complex451@gmail.com>

* updating to work with latest megatron

Signed-off-by: ericharper <complex451@gmail.com>

* updating to work with latest megatron

Signed-off-by: ericharper <complex451@gmail.com>

* update _del_model

Signed-off-by: ericharper <complex451@gmail.com>

* adding gpt model

Signed-off-by: ericharper <complex451@gmail.com>

* adding gpt model

Signed-off-by: ericharper <complex451@gmail.com>

* adding gpt model

Signed-off-by: ericharper <complex451@gmail.com>

* instantiate GPTmodel

Signed-off-by: ericharper <complex451@gmail.com>

* adding build dataset

Signed-off-by: ericharper <complex451@gmail.com>

* build megatron dataset in .setup

Signed-off-by: ericharper <complex451@gmail.com>

* setup dataloader

Signed-off-by: ericharper <complex451@gmail.com>

* add vocab_file and merge_file to megatron init

Signed-off-by: ericharper <complex451@gmail.com>

* add forward

Signed-off-by: ericharper <complex451@gmail.com>

* add train loss

Signed-off-by: ericharper <complex451@gmail.com>

* add optimizer

Signed-off-by: ericharper <complex451@gmail.com>

* add exp_manager

Signed-off-by: ericharper <complex451@gmail.com>

* multi-gpu is working

Signed-off-by: ericharper <complex451@gmail.com>

* adding val loop

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* adding val loop

Signed-off-by: ericharper <complex451@gmail.com>

* fix ranks

Signed-off-by: ericharper <complex451@gmail.com>

* fix model parallel checkpoint saving

Signed-off-by: ericharper <complex451@gmail.com>

* fix _del_model

Signed-off-by: ericharper <complex451@gmail.com>

* added megatron batch sampler

Signed-off-by: ericharper <complex451@gmail.com>

* try to fix num steps

Signed-off-by: ericharper <complex451@gmail.com>

* add wandb to config

Signed-off-by: ericharper <complex451@gmail.com>

* log lr

Signed-off-by: ericharper <complex451@gmail.com>

* add warmup ratio to config

Signed-off-by: ericharper <complex451@gmail.com>

* update configs

Signed-off-by: ericharper <complex451@gmail.com>

* update configs

Signed-off-by: ericharper <complex451@gmail.com>

* add cpu init to args

Signed-off-by: ericharper <complex451@gmail.com>

* update config

Signed-off-by: ericharper <complex451@gmail.com>

* update config

Signed-off-by: ericharper <complex451@gmail.com>

* Initial megatron dataset port

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix merge conflicts

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* License fixes and megatron model porting

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* More fixes to import from nemo rather than megatron

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix circular imports

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Revert config file

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Restructure further to avoid circular imports

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* add Makefile

Signed-off-by: ericharper <complex451@gmail.com>

* Add megatron modules

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* add license

Signed-off-by: ericharper <complex451@gmail.com>

* Port from latest megatron

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* update cfg

Signed-off-by: ericharper <complex451@gmail.com>

* update config

Signed-off-by: ericharper <complex451@gmail.com>

* add _del_model_without_trainer

Signed-off-by: ericharper <complex451@gmail.com>

* add data preprocessing script

Signed-off-by: ericharper <complex451@gmail.com>

* update config

Signed-off-by: ericharper <complex451@gmail.com>

* use apex mpu

Signed-off-by: ericharper <complex451@gmail.com>

* replace print_rank_0 with nemo utils logging

Signed-off-by: ericharper <complex451@gmail.com>

* use apex mpu

Signed-off-by: ericharper <complex451@gmail.com>

* use apex mpu

Signed-off-by: ericharper <complex451@gmail.com>

* add use_cpu_initialization

Signed-off-by: ericharper <complex451@gmail.com>

* fixing autoresume in progress

Signed-off-by: ericharper <complex451@gmail.com>

* properly removing last checkpoint

Signed-off-by: ericharper <complex451@gmail.com>

* log consumed samples

Signed-off-by: ericharper <complex451@gmail.com>

* fix mp autoresume

Signed-off-by: ericharper <complex451@gmail.com>

* add NLPSaveRestoreConnector

Signed-off-by: ericharper <complex451@gmail.com>

* Megatron GPT training with NeMo tokenizers (#2818)

* Update files from megatron repo

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove non NLP data related files from megatron

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Merge megatron and nemo tokenizers

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove get_tokenizer() calls from gpt model

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Update tokenizer yaml config

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* add todo

Signed-off-by: ericharper <complex451@gmail.com>

* update config

Signed-off-by: ericharper <complex451@gmail.com>

* make init_method_std configurable

Signed-off-by: ericharper <complex451@gmail.com>

* make gpu init work by setting random seed earlier

Signed-off-by: ericharper <complex451@gmail.com>

* fix gpu init after removing debug print in mpu

Signed-off-by: ericharper <complex451@gmail.com>

* add fused_adam

Signed-off-by: ericharper <complex451@gmail.com>

* check ds is not none before logging len

Signed-off-by: ericharper <complex451@gmail.com>

* set fp16 arg to true and fix enum conflict

Signed-off-by: ericharper <complex451@gmail.com>

* make fp16 arg configurable

Signed-off-by: ericharper <complex451@gmail.com>

* add grad clip from megatron

Signed-off-by: ericharper <complex451@gmail.com>

* Linear warmup with cosine annealing and constant holding (#2846)

* Testing cosine schedule

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* More fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* update config for constant steps in schedule

Signed-off-by: ericharper <complex451@gmail.com>

* temporarily import enum from megatron

Signed-off-by: ericharper <complex451@gmail.com>

* add grad clip for fp32

Signed-off-by: ericharper <complex451@gmail.com>

* update check for _del_model_without_trainer

Signed-off-by: ericharper <complex451@gmail.com>

* updating restore for model parallel

Signed-off-by: ericharper <complex451@gmail.com>

* add predict script

Signed-off-by: ericharper <complex451@gmail.com>

* update test iters

Signed-off-by: ericharper <complex451@gmail.com>

* add barrier

Signed-off-by: ericharper <complex451@gmail.com>

* return if clip_val is 0 or None

Signed-off-by: ericharper <complex451@gmail.com>

* when using amp clip grads after they are unscaled

Signed-off-by: ericharper <complex451@gmail.com>

* make native amp scaler hyperparams configurable

Signed-off-by: ericharper <complex451@gmail.com>

* (1) nvfuser, (2) amp-casting decoration (#2894)

* (1) nvfuser, (2) amp-casting decoration

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* support bf16

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* update package info

Signed-off-by: ericharper <complex451@gmail.com>

* add set device to constructor

Signed-off-by: ericharper <complex451@gmail.com>

* set_device in constructor

Signed-off-by: ericharper <complex451@gmail.com>

* [BigNLP] Remove megatron-lm dependency. (#2910)

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* add load_fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* add load_fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* update megatron_init

Signed-off-by: ericharper <complex451@gmail.com>

* add fused kernels

Signed-off-by: ericharper <complex451@gmail.com>

* add fused kernels

Signed-off-by: ericharper <complex451@gmail.com>

* update process batch

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* add megatron clip_grad

Signed-off-by: ericharper <complex451@gmail.com>

* trying to resolve circular import error

Signed-off-by: ericharper <complex451@gmail.com>

* rename file

Signed-off-by: ericharper <complex451@gmail.com>

* remove non-gpt models and datasets from __init__ files

Signed-off-by: ericharper <complex451@gmail.com>

* set device in constructorfor gpu init

Signed-off-by: ericharper <complex451@gmail.com>

* set device in constructorfor gpu init

Signed-off-by: ericharper <complex451@gmail.com>

* set_device in constructor

Signed-off-by: ericharper <complex451@gmail.com>

* clean config

Signed-off-by: ericharper <complex451@gmail.com>

* update MegatronDataset

Signed-off-by: ericharper <complex451@gmail.com>

* clean up MegatronModule

Signed-off-by: ericharper <complex451@gmail.com>

* clean up MegatronModule

Signed-off-by: ericharper <complex451@gmail.com>

* rename fp16 and bf16 flags to fused_softmax_input_in_fp16/bf16

Signed-off-by: ericharper <complex451@gmail.com>

* rename to fused_fp16

Signed-off-by: ericharper <complex451@gmail.com>

* add fused_fp16 arg to LayerNorm calls

Signed-off-by: ericharper <complex451@gmail.com>

* fix arg name

Signed-off-by: ericharper <complex451@gmail.com>

* fix arg name

Signed-off-by: ericharper <complex451@gmail.com>

* fix import

Signed-off-by: ericharper <complex451@gmail.com>

* update arg

Signed-off-by: ericharper <complex451@gmail.com>

* skip warmup default to True

Signed-off-by: ericharper <complex451@gmail.com>

* skip warmup default to True

Signed-off-by: ericharper <complex451@gmail.com>

* Adding complete method to MegatronGPTModel (#2935)

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* make ffn_hidden_size mandatory

Signed-off-by: ericharper <complex451@gmail.com>

* Manually migrating timing of step into branch (#2937)

* 1. Manually migrating timing of step into branch.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Updated file name and content.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Updated to latest code.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

Co-authored-by: Micha Livne <mlivne@nvidia.com>

* remove unused imports

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* check fused_fp16 and fused_bf16 are not both True

Signed-off-by: ericharper <complex451@gmail.com>

* update predict script for model parallel .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>
Co-authored-by: Micha Livne <mlivne@nvidia.com>

* NVfuser (#2943)

* activation checkpoint recompute

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* selective nvfuser setup

* Megatron gpt bfloat support (#2926)

* Save/restore fix

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Another merge

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Bf16 args in init

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Set precision

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove debug stuff

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* add bf16 casting decorator

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* Bfloat layernorm propagation

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* activation checkpoint recompute

Signed-off-by: Sangkug Lym <slym@nvidia.com>

* selective nvfuser setup

* More arg removal

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove BERTDataset

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* update to latest apex and patch transformer autocast

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: ericharper <complex451@gmail.com>

* don't set jit for bf16

Signed-off-by: ericharper <complex451@gmail.com>

* replace apex.mpu

Signed-off-by: ericharper <complex451@gmail.com>

* fix grad clip

Signed-off-by: ericharper <complex451@gmail.com>

* NVFuser fixes (#2951)

* Fuser fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove dummy handler

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove PTL plugin based logic for fusion

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* remove duplicated file

Signed-off-by: ericharper <complex451@gmail.com>

* typo (#2960)

Signed-off-by: ericharper <complex451@gmail.com>

* [BigNLP] Script to convert GPT checkpoint to .nemo (#2958)

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* remove args in progress

Signed-off-by: ericharper <complex451@gmail.com>

* add load_fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* add load_fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* update megatron_init

Signed-off-by: ericharper <complex451@gmail.com>

* add fused kernels

Signed-off-by: ericharper <complex451@gmail.com>

* add fused kernels

Signed-off-by: ericharper <complex451@gmail.com>

* update process batch

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* remove erroneous import

Signed-off-by: ericharper <complex451@gmail.com>

* add megatron clip_grad

Signed-off-by: ericharper <complex451@gmail.com>

* trying to resolve circular import error

Signed-off-by: ericharper <complex451@gmail.com>

* rename file

Signed-off-by: ericharper <complex451@gmail.com>

* remove non-gpt models and datasets from __init__ files

Signed-off-by: ericharper <complex451@gmail.com>

* set device in constructorfor gpu init

Signed-off-by: ericharper <complex451@gmail.com>

* set device in constructorfor gpu init

Signed-off-by: ericharper <complex451@gmail.com>

* set_device in constructor

Signed-off-by: ericharper <complex451@gmail.com>

* clean config

Signed-off-by: ericharper <complex451@gmail.com>

* update MegatronDataset

Signed-off-by: ericharper <complex451@gmail.com>

* clean up MegatronModule

Signed-off-by: ericharper <complex451@gmail.com>

* clean up MegatronModule

Signed-off-by: ericharper <complex451@gmail.com>

* rename fp16 and bf16 flags to fused_softmax_input_in_fp16/bf16

Signed-off-by: ericharper <complex451@gmail.com>

* rename to fused_fp16

Signed-off-by: ericharper <complex451@gmail.com>

* add fused_fp16 arg to LayerNorm calls

Signed-off-by: ericharper <complex451@gmail.com>

* fix arg name

Signed-off-by: ericharper <complex451@gmail.com>

* fix arg name

Signed-off-by: ericharper <complex451@gmail.com>

* fix import

Signed-off-by: ericharper <complex451@gmail.com>

* update arg

Signed-off-by: ericharper <complex451@gmail.com>

* skip warmup default to True

Signed-off-by: ericharper <complex451@gmail.com>

* skip warmup default to True

Signed-off-by: ericharper <complex451@gmail.com>

* Adding complete method to MegatronGPTModel (#2935)

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* make ffn_hidden_size mandatory

Signed-off-by: ericharper <complex451@gmail.com>

* Manually migrating timing of step into branch (#2937)

* 1. Manually migrating timing of step into branch.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Updated file name and content.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Updated to latest code.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

Co-authored-by: Micha Livne <mlivne@nvidia.com>

* remove unused imports

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* check fused_fp16 and fused_bf16 are not both True

Signed-off-by: ericharper <complex451@gmail.com>

* update predict script for model parallel .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* add script to convert .ckpt to .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* in progress

Signed-off-by: ericharper <complex451@gmail.com>

* update

Signed-off-by: ericharper <complex451@gmail.com>

* convert mp checkpoints to nemo

Signed-off-by: ericharper <complex451@gmail.com>

* update help

Signed-off-by: ericharper <complex451@gmail.com>

* add safeguard for model parallel save_to

Signed-off-by: ericharper <complex451@gmail.com>

* adjust NLPModel save_to to be safer for model parallel

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>
Co-authored-by: Micha Livne <mlivne@nvidia.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* [BigNLP] Update GPT evaluation to work with tensor model parallel  (#2959)

* in progress

Signed-off-by: ericharper <complex451@gmail.com>

* update args

Signed-off-by: ericharper <complex451@gmail.com>

* add request dataset

Signed-off-by: ericharper <complex451@gmail.com>

* tokenize request

Signed-off-by: ericharper <complex451@gmail.com>

* in progress

Signed-off-by: ericharper <complex451@gmail.com>

* able to run

Signed-off-by: ericharper <complex451@gmail.com>

* reduce logits

Signed-off-by: ericharper <complex451@gmail.com>

* capture response

Signed-off-by: ericharper <complex451@gmail.com>

* squeeze and unsqueeze

Signed-off-by: ericharper <complex451@gmail.com>

* handle non model parallel case

Signed-off-by: ericharper <complex451@gmail.com>

* clean imports

Signed-off-by: ericharper <complex451@gmail.com>

* add file

Signed-off-by: ericharper <complex451@gmail.com>

* convert logits to log_probs

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* rename logits to log_probs

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* fix copyright headers

Signed-off-by: ericharper <complex451@gmail.com>

* fix copyright headers

Signed-off-by: ericharper <complex451@gmail.com>

* remove old TimingCallback

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* update jenkins to use latest apex and sandeep's fork

Signed-off-by: ericharper <complex451@gmail.com>

* update jenkins

Signed-off-by: ericharper <complex451@gmail.com>

* update jenkins

Signed-off-by: ericharper <complex451@gmail.com>

* update jenkins

Signed-off-by: ericharper <complex451@gmail.com>

* update jenkins

Signed-off-by: ericharper <complex451@gmail.com>

* try 2109 container

Signed-off-by: ericharper <complex451@gmail.com>

* try cuda container

Signed-off-by: ericharper <complex451@gmail.com>

* use internal container

Signed-off-by: ericharper <complex451@gmail.com>

* update checkpoint tests

Signed-off-by: ericharper <complex451@gmail.com>

* fix scheduler args

Signed-off-by: ericharper <complex451@gmail.com>

* update eval

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* update jenkins to use ptl 1.5 rc

Signed-off-by: ericharper <complex451@gmail.com>

* add import guard to jenkins

Signed-off-by: ericharper <complex451@gmail.com>

* add import guard to jenkins

Signed-off-by: ericharper <complex451@gmail.com>

* remove deterministic

Signed-off-by: ericharper <complex451@gmail.com>

* install numba .53

Signed-off-by: ericharper <complex451@gmail.com>

* allow for more variance

Signed-off-by: ericharper <complex451@gmail.com>

* update trainer config dataclass

Signed-off-by: ericharper <complex451@gmail.com>

* test_get_optimizer on gpu

Signed-off-by: ericharper <complex451@gmail.com>

* revert comment

Signed-off-by: ericharper <complex451@gmail.com>

* change trainer config default to 32

Signed-off-by: ericharper <complex451@gmail.com>

* [BigNLP] Remove fused kernel code instead use Apex (#2984)

* remove fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* remove fused_kernels

Signed-off-by: ericharper <complex451@gmail.com>

* remove fused layer norm and fused softmax and use apex instead

Signed-off-by: ericharper <complex451@gmail.com>

* update imports

Signed-off-by: ericharper <complex451@gmail.com>

* remove comment

Signed-off-by: ericharper <complex451@gmail.com>

* use apex enums

Signed-off-by: ericharper <complex451@gmail.com>

* use apex enums

Signed-off-by: ericharper <complex451@gmail.com>

* add tab

Signed-off-by: ericharper <complex451@gmail.com>

* Timer with sliding window (#3002)

Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>

* revert tab

Signed-off-by: ericharper <complex451@gmail.com>

* check for rank zero

Signed-off-by: ericharper <complex451@gmail.com>

* check for rank zero

Signed-off-by: ericharper <complex451@gmail.com>

* try explicit log dir

Signed-off-by: ericharper <complex451@gmail.com>

* add +

Signed-off-by: ericharper <complex451@gmail.com>

* don't rm

Signed-off-by: ericharper <complex451@gmail.com>

* make dir if it doesn't exist

Signed-off-by: ericharper <complex451@gmail.com>

* create mp nemo file in temp directory

Signed-off-by: ericharper <complex451@gmail.com>

* simplify mp save_to

Signed-off-by: ericharper <complex451@gmail.com>

* handle mp 1 case

Signed-off-by: ericharper <complex451@gmail.com>

* style fix

Signed-off-by: ericharper <complex451@gmail.com>

* remove files

Signed-off-by: ericharper <complex451@gmail.com>

* fix consumed_samples when resuming

Signed-off-by: ericharper <complex451@gmail.com>

* fix reinstall.sh

Signed-off-by: ericharper <complex451@gmail.com>

* update req

Signed-off-by: ericharper <complex451@gmail.com>

* add more detailed log for dataloaders

Signed-off-by: ericharper <complex451@gmail.com>

* check if cuda is available before using fused_adam

Signed-off-by: ericharper <complex451@gmail.com>

* revert comment

Signed-off-by: ericharper <complex451@gmail.com>

* update eval script to use model.freeze

Signed-off-by: ericharper <complex451@gmail.com>

* log train loss averaged over gradient accumulation steps

Signed-off-by: ericharper <complex451@gmail.com>

* check copyright earlier

Signed-off-by: ericharper <complex451@gmail.com>

* todo

Signed-off-by: ericharper <complex451@gmail.com>

* override SaveRestoreConnector in NLPModel init

Signed-off-by: ericharper <complex451@gmail.com>

* move to scripts

Signed-off-by: ericharper <complex451@gmail.com>

* remove star import

Signed-off-by: ericharper <complex451@gmail.com>

* remove comments

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused dataset

Signed-off-by: ericharper <complex451@gmail.com>

* removed barrier

Signed-off-by: ericharper <complex451@gmail.com>

* check cfg

Signed-off-by: ericharper <complex451@gmail.com>

* remove logging

Signed-off-by: ericharper <complex451@gmail.com>

* freeze, unfreeze

Signed-off-by: ericharper <complex451@gmail.com>

* return None

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused imports

Signed-off-by: ericharper <complex451@gmail.com>

* add TODO

Signed-off-by: ericharper <complex451@gmail.com>

* typecheck

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* todo

Signed-off-by: ericharper <complex451@gmail.com>

* add common native plugin

Signed-off-by: ericharper <complex451@gmail.com>

* restore with trainer

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* deprecate megatron-lm bert

Signed-off-by: ericharper <complex451@gmail.com>

* deprecate megatron-lm bert

Signed-off-by: ericharper <complex451@gmail.com>

* compile helpers ont he fly

Signed-off-by: ericharper <complex451@gmail.com>

* remove amp_level

Signed-off-by: ericharper <complex451@gmail.com>

* remove amp_level from configs

Signed-off-by: ericharper <complex451@gmail.com>

* add missing import

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* remove amp_level

Signed-off-by: ericharper <complex451@gmail.com>

* use fast huggingface tokenizers by default

Signed-off-by: ericharper <complex451@gmail.com>

* deal with huggingface tokenizer positional args

Signed-off-by: ericharper <complex451@gmail.com>

* deal with huggingface tokenizer positional args

Signed-off-by: ericharper <complex451@gmail.com>

* deal with huggingface tokenizer positional args

Signed-off-by: ericharper <complex451@gmail.com>

* revert use_fast default to False

Signed-off-by: ericharper <complex451@gmail.com>

* return super training_epoch_end

Signed-off-by: ericharper <complex451@gmail.com>

* remove optimizer_idx arg from training_step

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused arg from on_train_epoch_end

Signed-off-by: ericharper <complex451@gmail.com>

* add restore_from_path to nemo config

Signed-off-by: ericharper <complex451@gmail.com>

* add comment

Signed-off-by: ericharper <complex451@gmail.com>

* revert

Signed-off-by: ericharper <complex451@gmail.com>

* override connector if not subclassing NLPSaveRestoreConnector for model parallel save

Signed-off-by: ericharper <complex451@gmail.com>

* update test optimizer

Signed-off-by: ericharper <complex451@gmail.com>

* clean up

Signed-off-by: ericharper <complex451@gmail.com>

* clean up

Signed-off-by: ericharper <complex451@gmail.com>

* clean up

Signed-off-by: ericharper <complex451@gmail.com>

* clean up

Signed-off-by: ericharper <complex451@gmail.com>

* make data_prefix mandatory in config

Signed-off-by: ericharper <complex451@gmail.com>

* update installation instructions on readme

Signed-off-by: ericharper <complex451@gmail.com>

* update dockerfile

Signed-off-by: ericharper <complex451@gmail.com>

* add todo

Signed-off-by: ericharper <complex451@gmail.com>

* raise error if trying to use always_save_nemo with model parallel model

Signed-off-by: ericharper <complex451@gmail.com>

* remove comment

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>
Co-authored-by: Micha Livne <mlivne@nvidia.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/modules/common/lm_utils.py,2021-08-24T22:21:59Z,"Merge 1.3 bugfixes into main (#2715)

* update jenkins branch

Signed-off-by: ericharper <complex451@gmail.com>

* update notebooks branch

Signed-off-by: ericharper <complex451@gmail.com>

* update package info

Signed-off-by: ericharper <complex451@gmail.com>

* update readme

Signed-off-by: ericharper <complex451@gmail.com>

* update nemo version for Dockerfile

Signed-off-by: ericharper <complex451@gmail.com>

* update notebook branch

Signed-off-by: ericharper <complex451@gmail.com>

* Update colab links to Transducer notebooks (#2654)

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix nmt grpc server, concatdataset for raw text files (#2656)

* Fix nmt grpc server and concatdataset for raw text files

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Check if lang direction is provided correctly

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* add missing init (#2662)

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix qa inference for single example (#2668)

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* Fix max symbol per step updating for RNNT (#2672)

* Fix max symbol per step updating for RNNT

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix notebooks

Signed-off-by: smajumdar <titu1994@gmail.com>

* Replaced unfold() with split_view() (#2671)

* Replaced unfold() with split_view()

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* fixed typo

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>

* Correct voice app demo (#2682)

Signed-off-by: smajumdar <titu1994@gmail.com>

* Import guard (#2692)

* add asr and pynini import guard

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* remove asrmodel type

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* remove asrmodel type

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fixing branch (#2695)

Signed-off-by: Ghasem Pasandi <gpasandi@nvidia.com>

Co-authored-by: Ghasem Pasandi <gpasandi@nvidia.com>

* fix for emojis (#2675)

* fix for emojis

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* remove redundant line

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* raise error

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* use app_state

Signed-off-by: ekmb <ebakhturina@nvidia.com>

Co-authored-by: Eric Harper <complex451@gmail.com>

* Fix issues with ASR notebooks (#2698)

Signed-off-by: smajumdar <titu1994@gmail.com>

* Allow non divisible split_size (#2699)

* bugfix

Signed-off-by: Jason <jasoli@nvidia.com>

* bugfix

Signed-off-by: Jason <jasoli@nvidia.com>

* TN fix for corner cases (#2689)

* serial added, weights to common defaults, decimal bug fix

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* one failing

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* all tests pass

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* remove redundant file

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix telephone, add test cases

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* money fix

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* clean format

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix edge case of greedy decoding for greedy_batch mode (#2701)

Signed-off-by: smajumdar <titu1994@gmail.com>

* Remove time macro (#2703)

Signed-off-by: smajumdar <titu1994@gmail.com>

* Minor FastPitch Fixes (#2697)

* fixes

Signed-off-by: Jason <jasoli@nvidia.com>

* update CI

Signed-off-by: Jason <jasoli@nvidia.com>

* refix

Signed-off-by: Jason <jasoli@nvidia.com>

* Fix ddp error. (#2678)

To avoid ""MisconfigurationException: Selected distributed backend ddp is not compatible with an interactive environment."" error.

Co-authored-by: ekmb <ebakhturina@nvidia.com>

* update jenkins

Signed-off-by: ericharper <complex451@gmail.com>

* update notebooks

Signed-off-by: ericharper <complex451@gmail.com>

* add split_view back

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Evelina <10428420+ekmb@users.noreply.github.com>
Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>
Co-authored-by: Boris Fomitchev <borisfom@users.noreply.github.com>
Co-authored-by: Ghasem <35242805+pasandi20@users.noreply.github.com>
Co-authored-by: Ghasem Pasandi <gpasandi@nvidia.com>
Co-authored-by: Jason <jasoli@nvidia.com>
Co-authored-by: khcs <khcs@users.noreply.github.com>
Co-authored-by: ekmb <ebakhturina@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/modules/common/lm_utils.py,2021-06-17T02:32:33Z,"[NMT] Model Parallel Megatron Encoders (#2238)

* add megatron encoder

Signed-off-by: ericharper <complex451@gmail.com>

* added megatron to get_nmt_tokenizer

Signed-off-by: ericharper <complex451@gmail.com>

* add vocab_size and hidden_size to megatron bert

Signed-off-by: ericharper <complex451@gmail.com>

* add megatron encoder module

Signed-off-by: ericharper <complex451@gmail.com>

* fixed horrible typo

Signed-off-by: ericharper <complex451@gmail.com>

* fix typo and add default

Signed-off-by: ericharper <complex451@gmail.com>

* updating nlp overrides for mp nmt

Signed-off-by: ericharper <complex451@gmail.com>

* move some logic back to nlpmodel from overrides

Signed-off-by: ericharper <complex451@gmail.com>

* add checkpoint_file property

Signed-off-by: ericharper <complex451@gmail.com>

* fix property

Signed-off-by: ericharper <complex451@gmail.com>

* num_tokentypes=0

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* find_unused_parameters=True

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* get instead of pop

Signed-off-by: ericharper <complex451@gmail.com>

* remove token type ids from megatron input example

Signed-off-by: ericharper <complex451@gmail.com>

* pop vocab_size

Signed-off-by: ericharper <complex451@gmail.com>

* fix checkpointing for model parallel

Signed-off-by: ericharper <complex451@gmail.com>

* fix bug in non model parallel

Signed-off-by: ericharper <complex451@gmail.com>

* convert cfg.trainer to dict

Signed-off-by: ericharper <complex451@gmail.com>

* make num_tokentypes configurable for nmt

Signed-off-by: ericharper <complex451@gmail.com>

* update checkpoint_file when using named megatron model in nemo

Signed-off-by: ericharper <complex451@gmail.com>

* make vocab_file configurable

Signed-off-by: ericharper <complex451@gmail.com>

* dataclass can't have mutable default

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* unused imports

Signed-off-by: ericharper <complex451@gmail.com>

* revert input example

Signed-off-by: ericharper <complex451@gmail.com>

* check that checkpoint version is not None

Signed-off-by: ericharper <complex451@gmail.com>

* add mp jenkins test

Signed-off-by: ericharper <complex451@gmail.com>

* update docstring

Signed-off-by: ericharper <complex451@gmail.com>

* add docs for pretrained encoders with nemo nmt

Signed-off-by: ericharper <complex451@gmail.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/modules/common/lm_utils.py,2021-05-18T17:07:23Z,"Transformer final norm preln (#2197)

* fix pre_ln final norm

Signed-off-by: fayejf <fayejf07@gmail.com>

* style fix

Signed-off-by: fayejf <fayejf07@gmail.com>

* bug fixed

Signed-off-by: fayejf <fayejf07@gmail.com>

* bugfix post_ln

Signed-off-by: fayejf <fayejf07@gmail.com>

* update and add pre_ln_final_norm

Signed-off-by: fayejf <fayejf07@gmail.com>

* style fix

Signed-off-by: fayejf <fayejf07@gmail.com>

* fix for unit test

Signed-off-by: fayejf <fayejf07@gmail.com>

* rename final_norm to final_layer_norm

Signed-off-by: fayejf <fayejf07@gmail.com>

* bug fix

Signed-off-by: fayejf <fayejf07@gmail.com>

* tiny fix

Signed-off-by: fayejf <fayejf07@gmail.com>

* fix and improve

Signed-off-by: fayejf <fayejf07@gmail.com>

* tiny fix

Signed-off-by: fayejf <fayejf07@gmail.com>

* Patch for NMT to allow loading old modlels trained with pre-LN

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/modules/common/lm_utils.py,2021-03-12T00:29:33Z,"NMT adding pretrained HF encoders (#1871)

* add get_transformer and get_nemo_transformer

Signed-off-by: ericharper <complex451@gmail.com>

* add get_transformer and get_nemo_transformer

Signed-off-by: ericharper <complex451@gmail.com>

* in progress

Signed-off-by: ericharper <complex451@gmail.com>

* updating get_nemo_transformer

Signed-off-by: ericharper <complex451@gmail.com>

* updating get_nemo_transformer

Signed-off-by: ericharper <complex451@gmail.com>

* aayn base working with get_transformer

Signed-off-by: ericharper <complex451@gmail.com>

* add get_nmt_tokenizer

Signed-off-by: ericharper <complex451@gmail.com>

* in progress of adding HF encoder

Signed-off-by: ericharper <complex451@gmail.com>

* updating yaml and args

Signed-off-by: ericharper <complex451@gmail.com>

* fixing typos and bugs

Signed-off-by: ericharper <complex451@gmail.com>

* fixing typos and bugs

Signed-off-by: ericharper <complex451@gmail.com>

* fixing typos and bugs

Signed-off-by: ericharper <complex451@gmail.com>

* add huggingface encoder neural module

Signed-off-by: ericharper <complex451@gmail.com>

* add hf encoder decoder neural module files

Signed-off-by: ericharper <complex451@gmail.com>

* update weight init for pretrained encoder and decoder

Signed-off-by: ericharper <complex451@gmail.com>

* add huggingface config file

Signed-off-by: ericharper <complex451@gmail.com>

* add todo

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* add docstring and dataclass for get_transformer

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* refactoring encoder configs

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* refactoring encoder configs

Signed-off-by: ericharper <complex451@gmail.com>

* refactoring encoder configs

Signed-off-by: ericharper <complex451@gmail.com>

* refactor decoder config

Signed-off-by: ericharper <complex451@gmail.com>

* revert arg name change

Signed-off-by: ericharper <complex451@gmail.com>

* refactor encoder/decoder configs again

Signed-off-by: ericharper <complex451@gmail.com>

* refactor encoder/decoder configs again

Signed-off-by: ericharper <complex451@gmail.com>

* previous configs working

Signed-off-by: ericharper <complex451@gmail.com>

* refactor config

Signed-off-by: ericharper <complex451@gmail.com>

* clean up config

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* check for attribute

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* use detach

Signed-off-by: ericharper <complex451@gmail.com>

* use detach

Signed-off-by: ericharper <complex451@gmail.com>

* if using pretrained, check config_dict is empty

Signed-off-by: ericharper <complex451@gmail.com>

* change pretrained default to false in hf config

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused imports

Signed-off-by: ericharper <complex451@gmail.com>

* add jenkins tests

Signed-off-by: ericharper <complex451@gmail.com>

* fix jenkins test for custom hf

Signed-off-by: ericharper <complex451@gmail.com>

* change jenkins to bert-base-cased

Signed-off-by: ericharper <complex451@gmail.com>

* fix header and add doc string

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused neural type

Signed-off-by: ericharper <complex451@gmail.com>

* add exception if tokenizer not supported

Signed-off-by: ericharper <complex451@gmail.com>

* add typecheck to huggingface encoder

Signed-off-by: ericharper <complex451@gmail.com>

* add typecheck to nemo decoder

Signed-off-by: ericharper <complex451@gmail.com>

* add typecheck to nemo encoder

Signed-off-by: ericharper <complex451@gmail.com>

* revert change from merge

Signed-off-by: ericharper <complex451@gmail.com>

* revert change from merge

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/modules/common/lm_utils.py,2021-02-25T23:46:56Z,"Fix megatron vocab file (#1803)

* use user specified vocab_file with megatron

Signed-off-by: ericharper <complex451@gmail.com>

* updating all examples

Signed-off-by: ericharper <complex451@gmail.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/modules/common/lm_utils.py,2020-09-23T23:20:55Z,"Aws2ngc (#1212)

* change AWS links with NGC links

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* remove some experimental

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* bugfix

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* bugfix

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* bugfixes

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* fix test

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* attempted bugfix

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* bugfixes

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* stylefix

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* file rename

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* download test refactor

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/modules/common/lm_utils.py,2020-09-09T02:04:18Z,"Support for automodel hf (#1132)

* biomegatron added

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* expose get_megatron_models_list method

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* nlp LM notebook added, nlp notebooks renamed to follow asr naming schema

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* adding renamed notebooks

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* revert notebooks renaming

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* docstring fix

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* branch name changed, get_lm_model added

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* autoencoder added

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* jenkins tests added for HF models withou a wrapper class + mismatched name

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* revert changes in the notebooks

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* clean up

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* clean up

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* get_huggingface_pretrained_lm_models_list doent include all HF supported automodels

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* ooo  fix

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* update auto tokenizer to preserve default special tokens

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* check added, review feedback

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* tests renamed

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* glue tests changed from bert to albert and roberta, fix in tokenizer_type

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix squad test

Signed-off-by: ekmb <ebakhturina@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/modules/common/lm_utils.py,2020-09-05T02:10:33Z,"Tokenizer and get_lm refactoring (#1114)

* Tokenizer refactoring

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2024-02-09T03:44:47Z,"Add longform infer for MultitaskAED models (#8355)

* add multitaskAED longform infer

Signed-off-by: stevehuang52 <heh@nvidia.com>

* refactor and changed default beam=1 and len_pen=0

Signed-off-by: stevehuang52 <heh@nvidia.com>

* revert default len_pen=1.0

Signed-off-by: stevehuang52 <heh@nvidia.com>

* refactor

Signed-off-by: stevehuang52 <heh@nvidia.com>

* refactor

Signed-off-by: stevehuang52 <heh@nvidia.com>

* update doc

Signed-off-by: stevehuang52 <heh@nvidia.com>

* fix autocast

Signed-off-by: stevehuang52 <heh@nvidia.com>

* fix typo in docstring

Signed-off-by: stevehuang52 <heh@nvidia.com>

---------

Signed-off-by: stevehuang52 <heh@nvidia.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2024-02-03T05:35:28Z," Attention encoder-decoder models for multiple speech-to-text tasks  (#8242)

* Rebasing canary changes at current main

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* Move the changes from asr transformer to nlp transformer as originally intended

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* update eval to strip spaces before punctuations

Signed-off-by: stevehuang52 <heh@nvidia.com>

* update pc strip

Signed-off-by: stevehuang52 <heh@nvidia.com>

* [canary] Refactor: `PromptedAudioToTextLhotseDataset` and `EncDecMultiTaskModel` (#8247)

* Create a separate CanaryDataset and use it inside `transformer_bpe_models.py`. Ditches `token_sequence_format`.

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* [canary] Refactor: move changes in transformer_bpe_models.py to Canar‚Ä¶ (#8252)

* [canary] Refactor: move changes in transformer_bpe_models.py to CanaryModel

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* Rename `CanaryModel` to `EncDecMultiTaskModel` and remove inheritance from `EncDecTransfModelBPE`; add a separate config for this model

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

---------

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* Rename `CanaryDataset` to `PromptedAudioToTextLhotseDataset`; add `prompt_format_fn` argument; clean-up the `_canary_prompt_format` function a bit

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* Move tokenization into `prompt_format_fn`, fix usage, add docs

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* Backward-compatible utterance validation

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* Improve type annotations

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* config and prompt_fn registration changes from review

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

---------

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* fix transcribe config

Signed-off-by: stevehuang52 <heh@nvidia.com>

* Refactor Canary to follow schema of remaining ASR models (#8260)

* Initial draft of multi task beam decoding strategy

Signed-off-by: smajumdar <titu1994@gmail.com>

* Stabilize inference

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update AED Multi Task model to mostly conform to Archetype-Type format. Update config

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add change decoding strategy

Signed-off-by: smajumdar <titu1994@gmail.com>

* Remove redundant imports

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Cleanup

Signed-off-by: smajumdar <titu1994@gmail.com>

* Cleanup

Signed-off-by: smajumdar <titu1994@gmail.com>

* remove asr transformer dependency on nlp

Signed-off-by: stevehuang52 <heh@nvidia.com>

* clean up

Signed-off-by: stevehuang52 <heh@nvidia.com>

* copy token_classifier from nlp to asr

Signed-off-by: stevehuang52 <heh@nvidia.com>

* Address comments

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add typing to beam decoding

Signed-off-by: smajumdar <titu1994@gmail.com>

* Make prompt format configurable

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* drop asr dependency on nlp

Signed-off-by: stevehuang52 <heh@nvidia.com>

---------

Signed-off-by: smajumdar <titu1994@gmail.com>
Signed-off-by: stevehuang52 <heh@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: stevehuang52 <heh@nvidia.com>

* fix transcribe, update asr evaluator

Signed-off-by: stevehuang52 <heh@nvidia.com>

* Extend the docs for the canary prompt_fn

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* Incorporate changes from Nithin's code review

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* training bug fix and adding launch script for speech_multitask (#8270)

* bug fix and adding launch script for speech_multitask

Signed-off-by: Krishna Puvvada <kpuvvada@nvidia.com>

* update launch script example in speech_to_text_aed.py

Signed-off-by: Krishna Puvvada <kpuvvada@nvidia.com>

---------

Signed-off-by: Krishna Puvvada <kpuvvada@nvidia.com>
Co-authored-by: Krishna Puvvada <kpuvvada@nvidia.com>

* Fix: drop_last must be true in validation/test otherwise the training will hang

Signed-off-by: Piotr ≈ªelasko <pzelasko@nvidia.com>

* revert to current transcribe API

Signed-off-by: stevehuang52 <heh@nvidia.com>

* revert changes to NLP, update docs

Signed-off-by: stevehuang52 <heh@nvidia.com>

* update eval utils

Signed-off-by: stevehuang52 <heh@nvidia.com>

* update docs

Signed-off-by: stevehuang52 <heh@nvidia.com>

* Remove DALI; rename compute_audio_loss to compute_loss

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* set default use_model_transcribe=False

Signed-off-by: stevehuang52 <heh@nvidia.com>

* change os.path.dirname to pathlib

Signed-off-by: stevehuang52 <heh@nvidia.com>

* [canary] Test for CanaryTokenizer + refactoring (#8285)

* Test for CanaryTokenizer

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* Attempt at refactor...

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

---------

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* Update config for AED models (#8294)

Signed-off-by: smajumdar <titu1994@gmail.com>

* set default calculate_wer=False in transcribe_speech.py

Signed-off-by: stevehuang52 <heh@nvidia.com>

* Attention encoder-decoder models for multiple speech-to-text tasks

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* Apply suggestions from code review, part 1

Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* Apply suggestions from code review, part 2

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* Document compute_loss

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

* update transcribe_speech.py

Signed-off-by: stevehuang52 <heh@nvidia.com>

* add docstring

Signed-off-by: stevehuang52 <heh@nvidia.com>

* Attention encoder-decoder models for multiple speech-to-text tasks

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>

---------

Signed-off-by: Piotr ≈ªelasko <petezor@gmail.com>
Signed-off-by: stevehuang52 <heh@nvidia.com>
Signed-off-by: smajumdar <titu1994@gmail.com>
Signed-off-by: Krishna Puvvada <kpuvvada@nvidia.com>
Signed-off-by: Piotr ≈ªelasko <pzelasko@nvidia.com>
Co-authored-by: stevehuang52 <heh@nvidia.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Krishna Puvvada <93558329+krishnacpuvvada@users.noreply.github.com>
Co-authored-by: Krishna Puvvada <kpuvvada@nvidia.com>
Co-authored-by: He Huang (Steve) <105218074+stevehuang52@users.noreply.github.com>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2023-12-29T15:34:44Z,"Add text metrics to asr eval (#8087)

* add text metrics to asr eval

Signed-off-by: stevehuang52 <heh@nvidia.com>

* temporary fix for EncDecTransfModelBPE

Signed-off-by: stevehuang52 <heh@nvidia.com>

---------

Signed-off-by: stevehuang52 <heh@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2023-10-07T01:51:11Z,"Create per.py (#7538)

* Move model precision copy (#7336)

* move cfg precision set to megatron base model

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* remove copy from other models

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* modify attribute not arg

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* fix gpt model test for ptl 2.0

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* rename function and add docstring

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* replace precision to dtype conditionals with func call

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* unnecessary function and cfg reset

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* set default value

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* fix precision lookup in a few more places

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* rename mapping function

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* ununsed import

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* save torch datatype to model

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* set weights precision wrt amp o2

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* Revert ""set weights precision wrt amp o2""

This reverts commit 313a4bfe5eb69d771a6d2433898c0685836aef5c.

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* revert half precision at inference attempt

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* move autocast dtype to base model

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* move params dtype to base model, enable fp16 O2 inf

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* unused imports

Signed-off-by: Maanu Grover <maanug@nvidia.com>

---------

Signed-off-by: Maanu Grover <maanug@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix PEFT checkpoint loading (#7388)

* Fix PEFT checkpoint loading

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Jason Wang <jasonwan@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Use distributed optimizer support for multiple dtypes (#7359)

* Update distopt wrapper with multiple dtype support

Remove manual handling of separate FP32 optimizer.

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Use distopt support for contiguous buffers with multiple dtypes

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Fix typo

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Separate distopt buckets for first GPT layer and non-overlapped params

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Add distopt logic for int dtypes

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Update Apex commit

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Remove unused variables

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Update Apex commit in README and Jenkensfile

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Debug Dockerfile and Jenkinsfile

Signed-off-by: Tim Moon <tmoon@nvidia.com>

---------

Signed-off-by: Tim Moon <tmoon@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* minor fix for llama ckpt conversion script (#7387)

* minor fix for llama ckpt conversion script

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* Update Jenkinsfile

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* remove fast_swiglu configuration

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

---------

Signed-off-by: Jason Wang <jasonwan@nvidia.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix wrong calling of librosa.get_duration() in notebook (#7376)

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [PATCH] PEFT import mcore (#7393)

* [PATCH] PEFT import mcore

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Jason Wang <jasonwan@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Create per.py

Script for calculation Punctuation Error Rate and related rates (correct rate, deletions rate, etc.)

Signed-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [TTS] Added a callback for logging initial data (#7384)

Signed-off-by: Ante JukicÃÅ <ajukic@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Update Core Commit (#7402)

* Update Core Commit

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* update commit

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

---------

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Use cfg attribute in bert (#7394)

* use cfg attribute instead of arg

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* use torch_dtype in place of cfg.precision

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* move precision copy before super constructor

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* use trainer arg

Signed-off-by: Maanu Grover <maanug@nvidia.com>

---------

Signed-off-by: Maanu Grover <maanug@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add support for bias conversion in Swiglu models (#7386)

* Add support for bias conversion in Swiglu models

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add support for auto extracting tokenizer model

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add support for auto extracting tokenizer model

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix issue with missing tokenizer

Signed-off-by: smajumdar <titu1994@gmail.com>

* Refactor

Signed-off-by: smajumdar <titu1994@gmail.com>

* Refactor

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Update save_to and restore_from for dist checkpointing (#7343)

* add dist ckpt to save to, in progress

Signed-off-by: eharper <eharper@nvidia.com>

* move dist ckpt

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* clean up

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update restore from, need to figure out how to initialize distributed

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* launch distrib if needed when restoring dist ckpt

Signed-off-by: eharper <eharper@nvidia.com>

* when using mcore we can change tp pp on the fly

Signed-off-by: eharper <eharper@nvidia.com>

* add load_from_checkpoint support for dist ckpt

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update llama convert script to save dist .nemo

Signed-off-by: eharper <eharper@nvidia.com>

* fix load dist ckpt

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* setup TE TP groups if needed

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* setup te tp groups if needed

Signed-off-by: eharper <eharper@nvidia.com>

* remove import

Signed-off-by: eharper <eharper@nvidia.com>

---------

Signed-off-by: eharper <eharper@nvidia.com>
Signed-off-by: jasonwan <jasonwan@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: jasonwan <jasonwan@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* fix forward for with mcore=false (#7403)

Signed-off-by: Jimmy Zhang <jiemingz@nvidia.com>
Co-authored-by: Jimmy Zhang <jiemingz@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix logging to remove 's/it' from progress bar in Megatron models and add train_step_timing (#7374)

* Add CustomProgressBar class to exp_manager and trainer callbacks

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix the progress bar to reflect total microbatch cnt

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Modify CustomProgressBar class

1) Modify CustomProgressBar class to update progress bar per global_step instead of per microbatch
2) Add the callback to other megatron training/finetuning files that are not using MegatronTrainerBuilder

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add CustomProgressBar callback to tuning files

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Set Activation Checkpointing Defaults (#7404)

* Set Activation Checkpointing Defaults

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* check for None

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* make loss mask default to false (#7407)

Signed-off-by: eharper <eharper@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add dummy userbuffer config files (#7408)

Signed-off-by: Sangkug Lym <slym@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* add missing ubconf files (#7412)

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* New tutorial on Speech Data Explorer (#7405)

* Added Google Colab based tutorial on Speech Data Explorer

Signed-off-by: George Zelenfroynd <gzelenfroind@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Update ptl training ckpt conversion script to work with dist ckpt (#7416)

* update ptl convert script

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* don't break legacy

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: eharper <eharper@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Allow disabling sanity checking when num_sanity_val_steps=0 (#7413)

* Allow disabling sanity checking when num_sanity_val_steps=0

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Update num_sanity_val_steps to be a multiple of num_microbatches

Signed-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Signed-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add comprehensive error messages (#7261)

Signed-off-by: Anton Peganov <apeganov@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* check NEMO_PATH (#7418)

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* layer selection for ia3 (#7417)

* layer selection for ia3

Signed-off-by: arendu <adithyare@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: arendu <adithyare@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix missing pip package 'einops' (#7397)

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix failure of pyaudio in Google Colab (#7396)

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Update README.md: output_path --> output_manifest_filepath (#7442)

Signed-off-by: Samuele Cornell <cornellsamuele@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add rope dynamic linear scaling (#7437)

* Add dynamic linear scaling

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

---------

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix None dataloader issue in PTL2.0 (#7455)

* Fix None dataloader issue in PTL2.0

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* updating values of self._validation_dl and self._test_dl as well

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* updating values of self._validation_dl and self._test_dl as well

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [ASR] Confidence measure -> method renames (#7434)

* measure -> method

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add steps for document of getting dataset 'SF Bilingual Speech' (#7378)

* Add steps for document of getting dataset 'SF Bilingual Speech'

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Update datasets.rst

added a link from a tutorial demonstrating detailed data prep steps.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* RNN-T confidence and alignment bugfix (#7381)

* new frame_confidence and alignments lists are now always created after the while loop

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

* tests added

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

---------

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix resume from checkpoint in exp_manager (#7424) (#7426)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix checking of cuda/cpu device for inputs of Decoder (#7444)

* Fix checking of cuda/cpu device for inputs of Decoder

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Update tacotron2.py

Signed-off-by: Jason <jasoli@nvidia.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Jason <jasoli@nvidia.com>
Co-authored-by: Jason <jasoli@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix failure of ljspeech's get_data.py (#7430)

* Fix failure of ljspeech's get_data.py

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [TTS] Fix audio codec type checks (#7373)

* [TTS] Fix audio codec type checks

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Fix audio codec tests

Signed-off-by: Ryan <rlangman@nvidia.com>

---------

Signed-off-by: Ryan <rlangman@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [TTS] Add dataset to path of logged artifacts (#7462)

* [TTS] Add dataset to path of logged artifacts

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Revert axis name back to Audio Frames

Signed-off-by: Ryan <rlangman@nvidia.com>

---------

Signed-off-by: Ryan <rlangman@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix sft dataset truncation (#7464)

* Add fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

---------

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Automatic Lip Reading Recognition (ALR) - ASR/CV (Visual ASR) (#7330)

* striding_conv1d_k5 and dw_striding_conv1d_k5 subsampling

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* transpose conv1d inputs

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* Update subsampling.py

change striding_conv1d_k5 to striding_conv1d

Signed-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>

* cv branch

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* video manifest

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* add collection classes

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add test_step_outputs

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* correct manifest bug when having only audio or only videos

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* correct manifest bug when having only audio or only videos

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* clean references

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* freeze unfreeze transcribe cv models

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* correct manifest get_full_path bug

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* update for PR

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* guard torchvision

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Update nemo/collections/cv/data/video_to_text_dataset.py

Co-authored-by: Igor Gitman <igor.a.gitman@gmail.com>
Signed-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>

* _video_speech_collate_fn in cv/data/video_to_text.py

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* add self.out = None to asr subsampling

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* Update nemo/collections/cv/data/video_to_text_dataset.py

Co-authored-by: Igor Gitman <igor.a.gitman@gmail.com>
Signed-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>

* cv -> multimodal/speech_cv branch

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: mburchi <maxime.burchi@gmail.com>
Signed-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Igor Gitman <igor.a.gitman@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* HF StarCoder to NeMo conversion script (#7421)

* Script to convert HF StarCoder checkpoint to NeMo

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* StarCoder conversion test

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Fix test

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Catch up with save_to changes

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Don't abbreviate args for clarity

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Configurable precision: BF16 vs FP32

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* fix bug when loading dist ckpt in peft (#7452)

Signed-off-by: Hongbin Liu <hongbinl@nvidia.com>
Co-authored-by: Hongbin Liu <hongbinl@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix adding positional embeddings in-place in transformer module (#7440)

Signed-off-by: Tamerlan Tabolov <tktabolov@gmail.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix (#7478)

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* add sleep (#7498) (#7499)

* add sleep

* add sleep onto config instead

* add comment

---------

Signed-off-by: Gerald Shen <geshen@nvidia.com>
Co-authored-by: Gerald Shen <119401249+gshennvm@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix exp manager check for sleep (#7503) (#7504)

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* bugfix: trainer.accelerator=auto from None. (#7492) (#7493)

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [doc] fix broken link (#7481)

Signed-off-by: Stas Bekman <stas00@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [TTS] Read audio as int32 to avoid flac read errors (#7477)

* [TTS] Read audio as int32 to avoid flac read errors

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Add comment about read failures

Signed-off-by: Ryan <rlangman@nvidia.com>

---------

Signed-off-by: Ryan <rlangman@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add dataset 'AISHELL-3' from OpenSLR for training mandarin TTS (#7409)

* Add dataset 'AISHELL-3' from OpenSLR for training mandarin TTS
* Train 'AISHELL-3' dataset with multi-speakers

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Update get_data.py

update copyright header

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* Update get_data.py

added a disclaimer

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add new configuration file for AISHELL3 with multispeaker of fastpitch

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* dllogger - log on rank 0 only (#7513)

Signed-off-by: Stas Bekman <stas00@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix TTS FastPitch tutorial (#7494) (#7516)

* Fix

---------

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix get_dist() tensor dimension (#7506) (#7515)

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>
Co-authored-by: Jocelyn <jocelynh@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* bugfix: specify trainer.strategy=auto when devices=1 (#7509) (#7512)

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* fix (#7511)

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [TTS] Fix FastPitch data prep tutorial (#7524)

Signed-off-by: Ryan <rlangman@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* add italian tokenization (#7486)

* add italian tokenization

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add more ipa lexicon it

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix error deletion

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* add test

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Replace None strategy with auto in tutorial notebooks (#7521) (#7527)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* unpin setuptools (#7534) (#7535)

Signed-off-by: fayejf <36722593+fayejf@users.noreply.github.com>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Update per.py

- if __name__ == ""__main__"" removed (now metric can be imported);
- removed excessive classes (like ""Sample"" and ""Statistics"");
- transition from pandas df to dict of dicts;
- removed unnecessary ""return"";
- notation fixing;
- reduced calculation time

Signed-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Create punctuation_rates.py

Signed-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Format fixing

Signed-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* added nemo.logging, header, docstrings, how to use

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Added asserions to rate_punctuation.py

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* fix typo

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* added function for import and call, docstrings

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* remove auto generated examples (#7510)

* explicitly remove autogenerated examples for data parallel evaluation

Signed-off-by: arendu <adithyare@nvidia.com>

* mark autogenrated and remove it for test

Signed-off-by: arendu <adithyare@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: arendu <adithyare@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add the `strategy` argument to `MegatronGPTModel.generate()` (#7264)

It is passed as an explicit argument rather than through
`**strategy_args` so as to ensure someone cannot accidentally pass other
arguments that would end up being ignored.

It is a keyword-only argument to ensure that if in the future we want to
update the signature to `**strategy_args`, we can do it without breaking
code.

Signed-off-by: Olivier Delalleau <507137+odelalleau@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix PTL2.0 related ASR bugs in r1.21.0: Val metrics logging, None dataloader issue (#7531) (#7533)

* fix none dataloader issue ptl2

* ptl2.0 logging fixes for rnnt_models

---------

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>
Co-authored-by: Kunal Dhawan <kunaldhawan97@gmail.com>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* gpus -> devices (#7542) (#7545)

Signed-off-by: Nithin Rao Koluguri <nithinraok>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Update FFMPEG version to fix issue with torchaudio (#7551) (#7553)

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* PEFT GPT & T5 Refactor (#7308)

* initial implementation of add_adapters API

* correct type hint

* Add config in add_adapters for save and load (@author bobchen)

* Remove AdapterConfig to avoid import error

* Add AdaterConfig back and move adaptermixin to sft model

* Add NLPSaveRestoreConnector as default in NLPModel.restore_from

* Add restore_from_nemo_with_adapter and test script

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* rename t5 file and classes to be consistent with GPT

* add t5 sft dataset

* add support for single-file format with T5SFTDataset

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Various small changes to make T5 SFT work like GPT SFT

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add adapter evaluation test script

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add MultiAdaterConfig for ia3 and fix builder issue

* Make ptuning for T5SFTModel work using mixin

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add IA3_Adapter for AdapterName

* Add adapter name for ptuning and attention adapter

* Make test script GPT/T5 agnostic

* Add layer selection feature

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Integrate adapter name and config

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update gpt peft tuning script to new API

* add t5 peft tuning script with new API

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix IA3 layer selection issue

* Override state_dict on SFT model instead of mixin

* Add load adapter by adapter config

* move peft config map away from example script

* auto get config from nemo adapter

* Move PEFTConfig to new file

* fix ckpt save/load for t5

* name change: add_adapters -> add_adapter

* variable name change

* update t5 script

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix t5 issues

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add weight tying

* update gpt tuning script

* PEFT-API proposal

* Fix according to comments

* update tuning scripts

* move merge_cfg_with to mixin class since it applies to both gpt and t5 and requires the model class for restore

* Add mcore_gpt support for NLPAdapterMixin

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix typo

* variable name change to distinguish ""peft"" and ""adapter""

* override `load_adapters` to support `add_adapter` name change

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update tuning and eval script for adapter save/load

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add Ptuning on first stage only

* add lora tutorial for review

* Fix layer selection for mcore

* add landing page

* fix resume training

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add mcore condition in sharded_state_dict to make sft work

* Update lora_tutorial.md

First edit of this file for PEFT documentation for NeMO

Signed-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>

* rename Adapter to AttentionAdapter to avoid confusion in doc

* Change load_adapters to load .nemo

* add quick start guide

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add load_adapters with .ckpt

* Remove setup_complete changes in load_adapters

* update landing page

* remove typo

* Updated quick_start.md per Chen Cui

Signed-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>

* Add inference config merger and tutorial

* Add doc string for NLPAdapterModelMixin and deprecated warning on MegatronGPTPEFTModel

* add supported_methods.md and update other documentations

* Update supported_methods.md

minor updates.

Signed-off-by: Adi Renduchintala <adithyare@nvidia.com>

* Update landing_page.md

minor update.

Signed-off-by: Adi Renduchintala <adithyare@nvidia.com>

* Modify doc string for NLPAdapterModelMixin

* Add doc string add_adapters in NLPAdapterModelMixin

* rename canonical adapters

* remove mcore hard dependency

* [PATCH] move microbatch calculator to nemo from apex

* remove apex dependency in gpt and t5 sft models

* remove apex dependency in gpt model

* render doc strings

* fix

* Add missing virtual_tokens on ptuning

* fix docstrings

* update gpt-style model coverage in docs

* update docstring

* Remove pdb

* add lightning_fabric to make docstring rendering work

* Add Ptuning missing key

* try docstring rendering

* Fix ptuning issue

* update gpt t5 peft tuning and eval scripts

* typos

* update eval config

* fix bug relating to apex dependency removal

* typo

* make predict step behave the same as test step

* make lora tutorial work in notebook

* cosmetics

* update yaml scripts

* mcore_gpt attribute optional

* typo

* update eval scripts and fix T5 eval bugs

* add NLPDDPStrategyNotebook and trainer builder logic to use it

* update lora notebook to use new trainer builder

* fix microbatch calculator bug for inference after training

* Convert markdown files to RST and incorporate with doc

* typo

* revise language

* remove extra cell

* remove unnecessary inheritance

* remove old tests

* move layer selection default so logging messages make sense

* remove `save_adapters` as adapter weights are saved automatically during training

* initialize weights from a checkpoint instead of randomly

* multiple fields can form a context (#7147)

* list of context fields and flexible prompt template

Signed-off-by: arendu <adithya.r@gmail.com>

* list of fields for context

Signed-off-by: arendu <adithya.r@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Add multiple truncation fields and middle truncation

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Compatible to old ckpt

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix tokenize detokenize issue

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Remove detokenization, add truncation augmentation

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Resolve comments

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Remove unused import

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* revert eos

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Add tokenizer space_sensitive attribute

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix error

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Fix erorr and use re

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Change assert logic

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Follow adi suggestion

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Remove merge function

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add example and comment

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Remove context_key and add comment

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Remove random truncation

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix template none

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

---------

Signed-off-by: arendu <adithya.r@gmail.com>
Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Signed-off-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>

* revert config changes

* remove accidental breakpoint

* support TP>1 loading

* infer adapter type from checkpoint in during eval

* breakup add adapter

* enable interpolation of train_ds and validation_ds

* update metric calc script to conform to single-file eval format

* remove extraneous print

* update lora notebook for updated merge_inference_cfg

* Update nlp_adapter_mixins.py

variable name change

Signed-off-by: Chen Cui <chcui@nvidia.com>

* turn off grad scaler for PP to match old scripts

* remove PEFTSaveRestoreConnector since functionality all covered by the new mixin class

* remove resume_from_checkpoint check since covered in #7335

* revert changes made in eval config interpolation

* more interpolation

* typo

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* remove dup line

Signed-off-by: Chen Cui <chcui@nvidia.com>

* code style warnings

Signed-off-by: Chen Cui <chcui@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix config mistake

Signed-off-by: Chen Cui <chcui@nvidia.com>

* add copyright header

Signed-off-by: Chen Cui <chcui@nvidia.com>

* fix code check warnings

Signed-off-by: Chen Cui <chcui@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* revert changes to remove apex dependency (mixed apex+nemo microbatch calculator broke some CI tests)

Signed-off-by: Chen Cui <chcui@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add more deprecation notices

Signed-off-by: Chen Cui <chcui@nvidia.com>

* update deprecation notices

Signed-off-by: Chen Cui <chcui@nvidia.com>

* update deprecation notices

Signed-off-by: Chen Cui <chcui@nvidia.com>

* consolidate peft and sft scripts

Signed-off-by: Chen Cui <chcui@nvidia.com>

* update CI tests

Signed-off-by: Chen Cui <chcui@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* notebook branch points to main to prepare for merge

Signed-off-by: Chen Cui <chcui@nvidia.com>

* fix gpt and t5 validation with any metric other than loss

Signed-off-by: Chen Cui <chcui@nvidia.com>

* support pre-extracted checkpoints

Signed-off-by: Chen Cui <chcui@nvidia.com>

---------

Signed-off-by: jasonwan <jasonwan@nvidia.com>
Signed-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>
Signed-off-by: Adi Renduchintala <adithyare@nvidia.com>
Signed-off-by: arendu <adithya.r@gmail.com>
Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Signed-off-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>
Signed-off-by: Chen Cui <chcui@nvidia.com>
Co-authored-by: Chen Cui <chcui@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Marc Romeyn <marcromeyn@gmail.com>
Co-authored-by: jasonwan <jasonwan@nvidia.com>
Co-authored-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>
Co-authored-by: Adi Renduchintala <adithyare@nvidia.com>
Co-authored-by: Yuanzhe Dong <yudong@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* fix a typo (#7496)

Signed-off-by: BestJuly <chntaoli@163.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [TTS] remove curly braces from ${BRANCH} in jupyer notebook cell. (#7554) (#7560)

* remove curly braces.
* remove installation of pynini.
---------

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* add youtube embed url (#7570)

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Remap speakers to continuous range of speaker_id for dataset AISHELL3 (#7536)

* Remap speakers to continuous range of speaker_id for dataset AISHELL3
* Add new key/value pair to record raw speaker for AISHELL3 dataset

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* fix validation_step_outputs initialization for multi-dataloader (#7546) (#7572)

* added correct validation_step_outputs initialization for mutli-dataloader

* changed kernel for display

* Update logic for validation and test step outputs

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* revert multidataloader changes in multilang ASR notebook

---------

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>
Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: Kunal Dhawan <kunaldhawan97@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Append output of val step to self.validation_step_outputs (#7530) (#7532)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [TTS] fixed trainer's accelerator and strategy. (#7569) (#7574)

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Append val/test output to instance variable in EncDecSpeakerLabelModel (#7562) (#7573)

* Append val/test output to the instance variable in EncDecSpeakerLabelModel

* Handle test case in evaluation_step

* Replace type with isinstance

---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix CustomProgressBar for resume (#7427) (#7522)

* Fix CustomProgress Bar for resume and multiple epochs

* Edit num_training_batches

* Use max_steps as total for progress bar for resume

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* fix typos in nfa and speech enhancement tutorials (#7580) (#7583)

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>
Co-authored-by: Elena Rastorgueva <80532067+erastorgueva-nv@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add strategy as ddp_find_unused_parameters_true for glue_benchmark.py (#7454) (#7461)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* update strategy (#7577) (#7578)

Signed-off-by: Nithin Rao Koluguri <nithinraok>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix typos (#7581)

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Change hifigan finetune strategy to ddp_find_unused_parameters_true (#7579) (#7584)

* Change strategy to auto

---------

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [BugFix] Add missing quotes for auto strategy in tutorial notebooks (#7541) (#7548)

* Add missing quotes for auto strategy

* Revert trainer.gpus to trainer.devices in Self_Supervised_Pre_Training.ipynb

---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Signed-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* added per tests

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [PATCH] PEFT import mcore (#7393)

* [PATCH] PEFT import mcore

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Jason Wang <jasonwan@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* add build os key (#7596) (#7599)

* add build os key

* add tools

* update to stable version

---------

Signed-off-by: Nithin Rao Koluguri <nithinraok>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* StarCoder SFT test + bump PyT NGC image to 23.09 (#7540)

* Add SFT StarCoder test

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Remove _modify_config call as it is covered in load_from_nemo just below

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Test with pyt:23.09 container

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

---------

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* defaults changed (#7600)

* defaults changed

Signed-off-by: arendu <adithyare@nvidia.com>

* typo

Signed-off-by: arendu <adithyare@nvidia.com>

* update

Signed-off-by: arendu <adithyare@nvidia.com>

---------

Signed-off-by: arendu <adithyare@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* add ItalianPhonemesTokenizer (#7587)

* add ItalianPhonemesTokenizer

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix Italian phonemes

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add test

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

---------

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* best ckpt fix (#7564) (#7588)

Signed-off-by: dimapihtar <dpihtar@gmail.com>
Co-authored-by: Dmytro Pykhtar <37850217+dimapihtar@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* rate_punctuation.py

Fixed output manifest saving

Signed-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix tests

Signed-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add files via upload (#7598)

specifies the branch

Signed-off-by: George <37293288+Jorjeous@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix validation in G2PModel and ThutmoseTaggerModel (#7597) (#7606)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain (#7576) (#7586)

* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain
* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Safeguard nemo_text_processing installation on ARM (#7485)

* safeguard nemo_text_processing installing

Signed-off-by: Jason <jasoli@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update check

Signed-off-by: Jason <jasoli@nvidia.com>

---------

Signed-off-by: Jason <jasoli@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Function name fixing

Signed-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Moving PER to speech_to_text_eval.py

Added:
- ""use_per"": PER metric computing;
- ""scores_per_sample"": metrics computation sample by sample for wer/cer/punctuation rates;
- ""output_with_scores_filename"": saving manifest with metrics

Signed-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Update test_metrics.py

Updated ""punctuation_error_rate"" function name

Signed-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Added use_per description

Signed-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* guard extra dependencies

Signed-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Write metrics to ""output_filename"" if ""scores_per_sample=True""

Signed-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* scores_per_sample description

Signed-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix import guards

Signed-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Stats printing when HAVE_TABLUATE_AND_PANDAS=False

Signed-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Bound transformers version in requirements (#7620)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* fix llama2 70b lora tuning bug (#7622)

* fix llama2 70b lora tuning bug

Signed-off-by: Chen Cui <chcui@nvidia.com>

* Update peft_config.py

brackets

Signed-off-by: Adi Renduchintala <adithyare@nvidia.com>

---------

Signed-off-by: Chen Cui <chcui@nvidia.com>
Signed-off-by: Adi Renduchintala <adithyare@nvidia.com>
Co-authored-by: Adi Renduchintala <adithyare@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix import error no module name model_utils (#7629)

Signed-off-by: Mehadi Hasan Menon <mehadihasan80@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Delete examples/asr/rate_punctuation.py

Signed-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Added use_per description

Signed-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* metric and variables name fixing

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add else samples = None

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* add fc large ls models (#7641)

Signed-off-by: Nithin Rao Koluguri <nithinraok>
Co-authored-by: Nithin Rao Koluguri <nithinraok>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* bugfix: trainer.gpus, trainer.strategy, trainer.accelerator (#7621) (#7642)

* [TTS] bugfix for Tacotron2 tutorial due to PTL 2.0
* trainer.gpus -> trainer.devices
* fixed related tutorial bugs
---------
Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* fix ssl models ptl monitor val through logging (#7608) (#7614)

Signed-off-by: Nithin Rao Koluguri <nithinraok>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix metrics for SE tutorial (#7604) (#7612)

Signed-off-by: Ante JukicÃÅ <ajukic@nvidia.com>
Co-authored-by: anteju <108555623+anteju@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add ddp_find_unused_parameters=True and change accelerator to auto (#7623) (#7644)

* Add ddp_find_unused_parameters=True and change acclerator to auto

* Add ddp_find_unused_parameters True for normalization_as_tagging_train.py

---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix py3.11 dataclasses issue  (#7616)

* Fix py3.11 dataclasses issue  (#7582)

* Update ASR configs to support Python 3.11

Signe‚Ä¶"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2023-08-22T18:38:37Z,"fix partial transcribe (#7284)

* fix partial transcribe

Signed-off-by: stevehuang52 <heh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* refactor

Signed-off-by: stevehuang52 <heh@nvidia.com>

---------

Signed-off-by: stevehuang52 <heh@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2023-08-17T19:31:33Z,"[ASR] Fix GPU memory leak in transcribe_speech.py (#7249)

Signed-off-by: Ryan <rlangman@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2023-06-26T16:23:58Z,"Move model change out of if-branch (#6908)

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2023-06-21T21:41:31Z,"Fix transcribe_utils.py for hybrid models in partial transcribe mode (#6899)

* Fix transcribe_utils.py for hybrid models in partial transcribe mode

Signed-off-by: He Huang (Steve) <105218074+stevehuang52@users.noreply.github.com>

* Update transcribe_utils.py

Signed-off-by: He Huang (Steve) <105218074+stevehuang52@users.noreply.github.com>

---------

Signed-off-by: He Huang (Steve) <105218074+stevehuang52@users.noreply.github.com>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2023-06-15T05:57:56Z,"Update transcribe_utils.py (#6865)

fix ctc decoding for hybrid model in partial transcribe

Signed-off-by: He Huang (Steve) <105218074+stevehuang52@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2023-05-16T22:42:08Z,"Add support for RNNT/hybrid models to partial transcribe (#6609)

* Add support for RNNT/hybrid models to partial transcribe

Signed-off-by: He Huang (Steve) <105218074+stevehuang52@users.noreply.github.com>

* Update transcribe_utils.py

Signed-off-by: He Huang (Steve) <105218074+stevehuang52@users.noreply.github.com>

* Update transcribe_speech.py

Signed-off-by: He Huang (Steve) <105218074+stevehuang52@users.noreply.github.com>

* Update transcribe_utils.py

Signed-off-by: He Huang (Steve) <105218074+stevehuang52@users.noreply.github.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: He Huang (Steve) <105218074+stevehuang52@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2023-05-11T01:31:03Z,"Offline and streaming inference support for hybrid model (#6570)

* streaming buffered for hybrid + ctc

Signed-off-by: fayejf <fayejf07@gmail.com>

* change default model_stride in eval.yaml

Signed-off-by: fayejf <fayejf07@gmail.com>

* add fc model_stride

Signed-off-by: fayejf <fayejf07@gmail.com>

* small fix

Signed-off-by: fayejf <fayejf07@gmail.com>

* check whether model and decoding match

Signed-off-by: fayejf <fayejf07@gmail.com>

* small fix

Signed-off-by: fayejf <fayejf07@gmail.com>

* streaming buffered for hybrid + rnnt

Signed-off-by: fayejf <fayejf07@gmail.com>

* style fix

Signed-off-by: fayejf <fayejf07@gmail.com>

* fix yaml

Signed-off-by: fayejf <fayejf07@gmail.com>

* reflect comment wip

Signed-off-by: fayejf <fayejf07@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix

Signed-off-by: fayejf <fayejf07@gmail.com>

* refactor and verified

Signed-off-by: fayejf <fayejf07@gmail.com>

* add get_full_path to buffered

Signed-off-by: fayejf <fayejf07@gmail.com>

* small fix

Signed-off-by: fayejf <fayejf07@gmail.com>

* add RNNTDecodingConfig

Signed-off-by: fayejf <fayejf07@gmail.com>

* model name & instruction of changing decoding

Signed-off-by: fayejf <fayejf07@gmail.com>

---------

Signed-off-by: fayejf <fayejf07@gmail.com>
Signed-off-by: fayejf <36722593+fayejf@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2023-05-10T15:14:38Z,"Cherry pick commits in #6601 to main (#6611)

* fix write

Signed-off-by: fayejf <fayejf07@gmail.com>

* decoding ctc

Signed-off-by: fayejf <fayejf07@gmail.com>

* temp set rnnt decoding return_best_hypothesis to true

Signed-off-by: fayejf <fayejf07@gmail.com>

* add wer cal back to transcribe_speech as requested

Signed-off-by: fayejf <fayejf07@gmail.com>

* add wer cal back to speech_to_text_buffered_infer_rnnt  as requested

Signed-off-by: fayejf <fayejf07@gmail.com>

* add wer cal back to speech_to_text_buffered_infer_ctc as requested

Signed-off-by: fayejf <fayejf07@gmail.com>

* style fix

Signed-off-by: fayejf <fayejf07@gmail.com>

* reflect change in asr_evaluator

Signed-off-by: fayejf <fayejf07@gmail.com>

* reflect som and vahid comment

Signed-off-by: fayejf <fayejf07@gmail.com>

* remove return_best_hy=true in transcribe_speech

Signed-off-by: fayejf <fayejf07@gmail.com>

* no text skip

Signed-off-by: fayejf <fayejf07@gmail.com>

* revert partial

Signed-off-by: fayejf <fayejf07@gmail.com>

---------

Signed-off-by: fayejf <fayejf07@gmail.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2023-05-09T19:04:46Z,"Bug/typo fixes (#6599)

Signed-off-by: Igor Gitman <igitman@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2023-05-05T18:17:02Z,"whitespace (#6574)

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2023-05-04T16:57:47Z,"Make KenLM with PC for AggregateTokenizer and merge it (#6081)

* do_lowercase, rm_punctuation

Signed-off-by: Nikolay Karpov <nkarpov@nvidia.com>

* support beam_strategy = beam

Signed-off-by: Nikolay Karpov <nkarpov@nvidia.com>

* black

Signed-off-by: Nikolay Karpov <nkarpov@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix config and^Cunctuation capitalization

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* rm math

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* update kenlm

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* black

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add opengrm

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* mv install_beamsearch_decoders

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* punctuation_to_preserve

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Only tikenizer opion

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* Black

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* DEFAULT_TOKEN_OFFSET

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* aggregate_tokenizer

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* install kenlm with more than 5gram

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* install_beamsearch_decoders

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* ngram_bin_path kenlm_bin_path

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* black

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* fix greedy PC bug

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* move global params

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* fix description and perplexity

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* fix description

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* NEMO_PATH

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* nemo:23.01

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* License

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* description

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* isinstance

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* refactor kenlm stdin

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* black

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* add cmd arg

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* use new iter_files

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* EncDecHybridRNNTCTCModel

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* punctuation

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* train_kenlm args

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* add docstrings

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add ngram_merge docs

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* ngram_prune

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* rename to ngram_merge

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* rename to ngram

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* add comments

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* Ngram

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* nemo_model_file

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* install_opengrm_ngram

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* install opengrm

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* rename to install_opengrm.sh

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* rm extra import

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* train_paths

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* text_processing

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* fix ngram_bin_path

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* DECODERS_PATH

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* farcompile

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* rm text processing

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* text_processing

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* AggregateTokenizer.DummyTokenizer

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* comments

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* TextProcessingConfig

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* typo

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* doc

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* types

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* nemo_model_file

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* rm assert

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* import kenlm_utils

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* return None

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* Copyright

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* 2022

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* 2023

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

---------

Signed-off-by: Nikolay Karpov <nkarpov@nvidia.com>
Signed-off-by: Nikolay Karpov <karpnv@gmail.com>
Co-authored-by: Nikolay Karpov <nkarpov@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2023-05-04T09:36:27Z,"Patch transcribe and support offline transcribe for hybrid model (#6550) (#6559)

Signed-off-by: fayejf <fayejf07@gmail.com>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2023-03-20T19:12:10Z,"[ASR] Support audio_filepath as a list of single-channel files (#6132)

* Support audio_filepath as a list of single-channel files

Signed-off-by: Ante JukicÃÅ <ajukic@nvidia.com>

---------

Signed-off-by: Ante JukicÃÅ <ajukic@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2023-03-01T18:12:26Z,"Ngram (#6063)

* do_lowercase, rm_punctuation

Signed-off-by: Nikolay Karpov <nkarpov@nvidia.com>

* support beam_strategy = beam

Signed-off-by: Nikolay Karpov <nkarpov@nvidia.com>

* black

Signed-off-by: Nikolay Karpov <nkarpov@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix config and^Cunctuation capitalization

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* rm math

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* else TypeError

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

---------

Signed-off-by: Nikolay Karpov <nkarpov@nvidia.com>
Signed-off-by: Nikolay Karpov <karpnv@gmail.com>
Co-authored-by: Nikolay Karpov <nkarpov@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2023-01-14T01:35:05Z,"Flashlight Decoder for Nemo (#5790)

* Added initial flashlight decoding files

Signed-off-by: Daniel Egert <degert@nvidia.com>

* Fixed some minor bugs

Signed-off-by: Daniel Egert <degert@nvidia.com>

* Added fixes from auto style thingamajig

Signed-off-by: Daniel Egert <degert@nvidia.com>

* Ran pre-commit and fixed script file formatting

Signed-off-by: Daniel Egert <degert@nvidia.com>

* Minor fixes brought up during PR review

Signed-off-by: Daniel Egert <degert@nvidia.com>

* Added Windows-compatible code to eval_beamsearch_ngram.py

Signed-off-by: Daniel Egert <degert@nvidia.com>

* Added initial flashlight decoding files

Signed-off-by: Daniel Egert <degert@nvidia.com>

* Fixed some minor bugs

Signed-off-by: Daniel Egert <degert@nvidia.com>

* Added fixes from auto style thingamajig

Signed-off-by: Daniel Egert <degert@nvidia.com>

* Ran pre-commit and fixed script file formatting

Signed-off-by: Daniel Egert <degert@nvidia.com>

* Minor fixes brought up during PR review

Signed-off-by: Daniel Egert <degert@nvidia.com>

* Added Windows-compatible code to eval_beamsearch_ngram.py

Signed-off-by: Daniel Egert <degert@nvidia.com>

Signed-off-by: Daniel Egert <degert@nvidia.com>
Co-authored-by: Daniel Egert <degert@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2023-01-11T16:57:25Z,"ASR evaluator (#5728)

* backbone

Signed-off-by: fayejf <fayejf07@gmail.com>

* engineer and analyzer

Signed-off-by: fayejf <fayejf07@gmail.com>

* offline_by_chunked

Signed-off-by: fayejf <fayejf07@gmail.com>

* test_ds wip

Signed-off-by: fayejf <fayejf07@gmail.com>

* temp remove inference

Signed-off-by: fayejf <fayejf07@gmail.com>

* mandarin yaml

Signed-off-by: fayejf <fayejf07@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* augmentor and a few updates

Signed-off-by: fayejf <fayejf07@gmail.com>

* address alerts and revert unnecessary changes

Signed-off-by: fayejf <fayejf07@gmail.com>

* Add readme

Signed-off-by: fayejf <fayejf07@gmail.com>

* rename

Signed-off-by: fayejf <fayejf07@gmail.com>

* typo fix

Signed-off-by: fayejf <fayejf07@gmail.com>

* small fix

Signed-off-by: fayejf <fayejf07@gmail.com>

* add missing header

Signed-off-by: fayejf <fayejf07@gmail.com>

* rename augmentor_config to augmentor

Signed-off-by: fayejf <fayejf07@gmail.com>

* raname inference_mode to inference

Signed-off-by: fayejf <fayejf07@gmail.com>

* move utils.py

Signed-off-by: fayejf <fayejf07@gmail.com>

* update temp file

Signed-off-by: fayejf <fayejf07@gmail.com>

* make wer cer clear

Signed-off-by: fayejf <fayejf07@gmail.com>

* seed_everything

Signed-off-by: fayejf <fayejf07@gmail.com>

* fix missing rn augmentor_config in rnnt

Signed-off-by: fayejf <fayejf07@gmail.com>

* fix rnnt transcribe

Signed-off-by: fayejf <fayejf07@gmail.com>

* add more docstring and style fix

Signed-off-by: fayejf <fayejf07@gmail.com>

* address codeQL

Signed-off-by: fayejf <fayejf07@gmail.com>

* reflect comments

Signed-off-by: fayejf <fayejf07@gmail.com>

* update readme

Signed-off-by: fayejf <fayejf07@gmail.com>

* clearer

Signed-off-by: fayejf <fayejf07@gmail.com>

Signed-off-by: fayejf <fayejf07@gmail.com>
Signed-off-by: fayejf <36722593+fayejf@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: He Huang (Steve) <105218074+stevehuang52@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2022-12-15T11:16:03Z,"Conformer local attention (#5525)

* local attn and merge

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* optional

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* override

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* incorporate comments

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* update

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* fix

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* comment

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* changes, test

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* changes

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* check att context

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* readme link

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* utils

Signed-off-by: sam1373 <samuelkriman@gmail.com>

* update

Signed-off-by: sam1373 <samuelkriman@gmail.com>

Signed-off-by: sam1373 <samuelkriman@gmail.com>
Signed-off-by: Samuel Kriman <samuelkriman@gmail.com>
Co-authored-by: Vahid Noroozi <VahidooX@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2022-12-07T19:16:48Z,"Finalize (#5568)

Signed-off-by: smajumdar <titu1994@gmail.com>

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: Vahid Noroozi <VahidooX@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2022-11-23T10:22:41Z,"Add missing import (#5487)

Signed-off-by: Jonghwan Hyeon <hyeon0145@gmail.com>

Signed-off-by: Jonghwan Hyeon <hyeon0145@gmail.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2022-11-22T22:13:31Z,"Transcribe for multi-channel signals (#5479)

Transcribe for multi-channel signals (#5479)

Signed-off-by: Ante JukicÃÅ <ajukic@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2022-11-19T00:19:57Z,"Refactor/unify ASR offline and buffered inference (#5440)

* refactor/unify offline and buffered

Signed-off-by: fayejf <fayejf07@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* docstring and type

Signed-off-by: fayejf <fayejf07@gmail.com>

* style fix

Signed-off-by: fayejf <fayejf07@gmail.com>

* audio_dir for buffered

Signed-off-by: fayejf <fayejf07@gmail.com>

* fix

Signed-off-by: fayejf <fayejf07@gmail.com>

* reflect comments

Signed-off-by: fayejf <fayejf07@gmail.com>

* revert setup_gpu

Signed-off-by: fayejf <fayejf07@gmail.com>

* fix

Signed-off-by: fayejf <fayejf07@gmail.com>

Signed-off-by: fayejf <fayejf07@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2022-11-10T19:43:34Z,"Fix Python type hints according to Python Docs (#5370)

* Remove duplicated type annotations

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

* Fix tuple annotations in function return types

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

* Add necessary imports

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

* Add necessary imports

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

* Fix types in obvious places

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

* Fix types in obvious places

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

* Fix unused import (avoid quotes in type annotations)

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

* Revert ""Fix unused import (avoid quotes in type annotations)""

This reverts commit ea433efcd9916abf8944879e791484a0a1437f83.

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

* Remove problematic import

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

* Fix list_available_models method type

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

* Revert some changes

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

* Revert quotes in list_available_models

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2022-11-04T22:03:05Z,"small bugfix for r1.13.0 (#5310) (#5325)

* typo fix
* udpate transcribe

Signed-off-by: fayejf <fayejf07@gmail.com>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2022-08-24T20:18:02Z,"Merge r1.11.0 main (#4787)

* NeMo Megatron doc updates

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* update package info and dockerfile

Signed-off-by: ericharper <complex451@gmail.com>

* fix fastpitch export (#4676)

Signed-off-by: Jason <jasoli@nvidia.com>

* [TTS] fixed wrong pronunciations for r1.11. (#4677)

* [TTS] fixed wrong pronunciations.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* incremented the version number to 22.08 as @blisc suggested.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* correct cmudict versions in world-wide places.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* Fix for incorrect batch size issue while decoding (#4675)

Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* [TTS] incremented the version number to 22.08 in tutorials. (#4684)

* [TTS] incremented the version number to 22.08 in tutorials.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* Megatron encode function with RPE fix (#4692)

* Fix for RPE

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* fix to fetch config file (#4699)

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* Fix notebook for buffered inference (#4703)

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Prompt Learning Notebook Bug Fix (#4689)

* Added back dataset class list of dict input for generation in tutorial notebook

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* updated argument name for build dataset

Signed-off-by: Virginia Adams <vadams@nvidia.com>

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* add psutils to mock imports (#4728)

Signed-off-by: ericharper <complex451@gmail.com>

Signed-off-by: ericharper <complex451@gmail.com>

* Update Aligner model and tutorial to add NGC checkpoint loading (#4714)

* Update Aligner model and tutorial to add NGC checkpoint loading

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Fix pynini install for Aligner notebook, minor formatting fix for model

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Aligner notebook formatting consistency

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* [TTS] bugfix for missing configs. (#4725)

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* docs typo fix

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* Fix pynini install in TTS tutorials (#4729)

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Fix ASR notebooks (#4738)

Signed-off-by: smajumdar <smajumdar@nvidia.com>

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Multilingual VAD model (#4734)

* add ngc link

Signed-off-by: fayejf <fayejf07@gmail.com>

* add tuned VAD config on ASR data

Signed-off-by: fayejf <fayejf07@gmail.com>

* yaml note

Signed-off-by: fayejf <fayejf07@gmail.com>

* update vad asr notebook with mVAD

Signed-off-by: fayejf <fayejf07@gmail.com>

* update vad infer config comment

Signed-off-by: fayejf <fayejf07@gmail.com>

* fix

Signed-off-by: fayejf <fayejf07@gmail.com>

* mvad sd config for ch109

Signed-off-by: fayejf <fayejf07@gmail.com>

* update sd readme

Signed-off-by: fayejf <fayejf07@gmail.com>

* add new mVAD model to doc

Signed-off-by: fayejf <fayejf07@gmail.com>

* style fix

Signed-off-by: fayejf <fayejf07@gmail.com>

* update sd tutorial with mVAD

Signed-off-by: fayejf <fayejf07@gmail.com>

* typo fix

Signed-off-by: fayejf <fayejf07@gmail.com>

Signed-off-by: fayejf <fayejf07@gmail.com>

* publish pretrained itn t5 model for English (#4748)

Signed-off-by: Alexandra Antonova <aleksandraa@nvidia.com>

Signed-off-by: Alexandra Antonova <aleksandraa@nvidia.com>
Co-authored-by: Alexandra Antonova <aleksandraa@nvidia.com>

* Updated docs and doc paths (#4754)

* Updated docs and doc paths

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Update Multitask_Prompt_and_PTuning.ipynb

* Update README.rst

* Changed branch name to use single quotes

Signed-off-by: Virginia Adams <vadams@nvidia.com>

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* fix bug relating to ddp strategy in joint intent slot classification tutorial (#4762)

* [TTS] updated config with a German IPA phoneme tokenizer (#4756)

* [TTS] added a German IPA phoneme tokenizer
* [TTS][ASR] enabled customized arguments for trimming the leading and trailing silence.
* [TTS] disabled spline interpolation for beta-binomial distribution. Let it generate align prior and save to disks. Use a new phoneme tokenizer.
* [TTS] use consistent spline interpolation with fastpitch checkpoint when generating mel-spectrograms for hifigan finetune.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* Update r1.11 to new heteronyms list (#4745)

* Update configs to new heteronyms list
* Remove old heteronyms list, add alt 'merchandise' pron to CMUdict
* Update remaining references to old heteronyms list

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* [TTS] Add multi-speaker German FastPitch and HiFiGAN NGC checkpoints (#4763)

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* [TTS] Add single male speaker German FastPitch and HiFiGAN NGC checkpoints (#4770)

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* Update CMUdict with more recent 0.7b entries (#4768)

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* Install pynini in docker container (#4733)

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* Fix tutorial formatting (#4778)

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* [TTS] deprecated old scripts for ljspeech. (#4780)

* deprecated old scripts for ljspeech.
* removed relevent function calls in TTS docs.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* update package info and requirements

Signed-off-by: ericharper <complex451@gmail.com>

* update container

Signed-off-by: ericharper <complex451@gmail.com>

* Update stragglers to new cmudict and heteronyms paths

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>
Signed-off-by: ericharper <complex451@gmail.com>
Signed-off-by: Jason <jasoli@nvidia.com>
Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>
Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>
Signed-off-by: smajumdar <smajumdar@nvidia.com>
Signed-off-by: Virginia Adams <vadams@nvidia.com>
Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>
Signed-off-by: fayejf <fayejf07@gmail.com>
Signed-off-by: Alexandra Antonova <aleksandraa@nvidia.com>
Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>
Co-authored-by: Jason <jasoli@nvidia.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Rajesh Ilango <rilango@gmail.com>
Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>
Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: Virginia Adams <78445382+vadam5@users.noreply.github.com>
Co-authored-by: Jocelyn <jocelynh@nvidia.com>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>
Co-authored-by: bene-ges <61418381+bene-ges@users.noreply.github.com>
Co-authored-by: Alexandra Antonova <aleksandraa@nvidia.com>
Co-authored-by: Zhilin Wang <wangzhilin12061996@hotmail.com>
Co-authored-by: Vladimir Bataev <vbataev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2022-06-22T02:52:18Z,"Fixing bugs in calling method ctc_decoder_predictions_tensor. (#4414)

* updated ctc decoding calls.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed the ones for timestamp_utils.py

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed the ones for timestamp_utils.py

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed the ones for timestamp_utils.py

Signed-off-by: Vahid <vnoroozi@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/transcribe_utils.py,2022-03-17T01:43:26Z,"Offline VAD+ASR tutorial (#3828)

* merge updated part in vad_utils

Signed-off-by: fayejf <fayejf07@gmail.com>

* path librosa and add int

Signed-off-by: fayejf <fayejf07@gmail.com>

* manifest input for transcribe

Signed-off-by: fayejf <fayejf07@gmail.com>

* stitch support

Signed-off-by: fayejf <fayejf07@gmail.com>

* Offline_VAD_ASR tutorial

Signed-off-by: fayejf <fayejf07@gmail.com>

* full manifest warning

Signed-off-by: fayejf <fayejf07@gmail.com>

* style fix

Signed-off-by: fayejf <fayejf07@gmail.com>

* handle the pure noise edge case

Signed-off-by: fayejf <fayejf07@gmail.com>

* merge vad_util in other branch

Signed-off-by: fayejf <fayejf07@gmail.com>

* revert parts that will be convered in https://github.com/NVIDIA/NeMo/pull/3463

Signed-off-by: fayejf <fayejf07@gmail.com>

* remove comment

Signed-off-by: fayejf <fayejf07@gmail.com>

* style fix

Signed-off-by: fayejf <fayejf07@gmail.com>

* add readme and doc

Signed-off-by: fayejf <fayejf07@gmail.com>

* typo and fix lgtm

Signed-off-by: fayejf <fayejf07@gmail.com>

* revert due to abstraction level

Signed-off-by: fayejf <fayejf07@gmail.com>

* update per request

Signed-off-by: fayejf <fayejf07@gmail.com>

* remove num_files

Signed-off-by: fayejf <fayejf07@gmail.com>

* update for feedback

Signed-off-by: fayejf <fayejf07@gmail.com>

* add missing files

Signed-off-by: fayejf <fayejf07@gmail.com>

* rename notebook

Signed-off-by: fayejf <fayejf07@gmail.com>

* update for feedback and modify for colab

Signed-off-by: fayejf <fayejf07@gmail.com>

* update per request

Signed-off-by: fayejf <fayejf07@gmail.com>"
github.com/NVIDIA/NeMo,nemo/collections/multimodal/data/neva/neva_dataset.py,2024-02-21T22:56:21Z,"Add Neva Template for NV-DPO Models  (#8358)

* add/rename from nvgpt to nv_steerlm, add nv_dpo template

Signed-off-by: HuiyingLi <willwin.lee@gmail.com>

* add nv_dpo conversation to accomendate empty system message

Signed-off-by: HuiyingLi <willwin.lee@gmail.com>

* handle nv_dpo template text generation

Signed-off-by: HuiyingLi <willwin.lee@gmail.com>

* add prompt string to nvgpt

Signed-off-by: HuiyingLi <willwin.lee@gmail.com>

* bugfix for inference prompt template

Signed-off-by: HuiyingLi <willwin.lee@gmail.com>

* bug fix for grabbing clean text

Signed-off-by: Huiying Li <willwin.lee@gmail.com>

* fix code format

Signed-off-by: Huiying Li <willwin.lee@gmail.com>

---------

Signed-off-by: HuiyingLi <willwin.lee@gmail.com>
Signed-off-by: Huiying Li <willwin.lee@gmail.com>"
github.com/NVIDIA/NeMo,nemo/collections/multimodal/data/neva/neva_dataset.py,2024-02-16T17:16:16Z,"Multimodal r1.23.0 bug fix  (#8315) (#8339)

* Rename quick-gelu



* ddpm config guard



* Fix ddpm edit api



* Fix insert_image_token cfg issue



* neva updates



* reformat



* Add back jenkins



* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix jenkins



* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bugs



* Update default neva template



---------

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>
Co-authored-by: yaoyu-33 <54727607+yaoyu-33@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/multimodal/data/neva/neva_dataset.py,2024-01-20T01:59:23Z,"Final multimodal PR with our recent developments on MM side (#8127)

* Hotfix (#7501) (#7568)

Signed-off-by: Jan Baczek <jbaczek@nvidia.com>
Co-authored-by: jbaczek <45043825+jbaczek@users.noreply.github.com>

* Avoid duplicated checkpoint save (#7555) (#7566)

Signed-off-by: Miko≈Çaj B≈Ça≈º <mblaz@nvidia.com>
Co-authored-by: mikolajblaz <mikolajblaz@users.noreply.github.com>

* Cache FP8 weight and transpose only at the first micro-batch in each validation and test routine (#7470) (#7483)

* Cache weight and transpose only in the first batch in all training, val, and test runs



* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Add an option to disable manual GC in validation (#7467) (#7476)

Signed-off-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: Sangkug Lym <slym@nvidia.com>

* Remove PUBLICATIONS.md, point to github.io NeMo page instead (#7694) (#7695)

* update publications section to point to blog website page



* add hyphen



* use double backquotes for code formatting



---------

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>
Signed-off-by: Elena Rastorgueva <80532067+erastorgueva-nv@users.noreply.github.com>
Co-authored-by: Elena Rastorgueva <80532067+erastorgueva-nv@users.noreply.github.com>

* Fix multi rank finetune for ASR (#7684) (#7699)

* Fix multi rank finetune for ASR



* Actually add time



* Actually add time



---------

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>

* Update docs: readme, getting started, ASR intro (#7679)

* [TTS] Add dataset to path of logged artifacts (#7462)

* [TTS] Add dataset to path of logged artifacts

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Revert axis name back to Audio Frames

Signed-off-by: Ryan <rlangman@nvidia.com>

---------

Signed-off-by: Ryan <rlangman@nvidia.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* move install info to INSTALLATION.md

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* tidy up links

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix sft dataset truncation (#7464)

* Add fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

---------

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Automatic Lip Reading Recognition (ALR) - ASR/CV (Visual ASR) (#7330)

* striding_conv1d_k5 and dw_striding_conv1d_k5 subsampling

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* transpose conv1d inputs

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* Update subsampling.py

change striding_conv1d_k5 to striding_conv1d

Signed-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>

* cv branch

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* video manifest

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* add collection classes

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add test_step_outputs

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* correct manifest bug when having only audio or only videos

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* correct manifest bug when having only audio or only videos

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* clean references

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* freeze unfreeze transcribe cv models

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* correct manifest get_full_path bug

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* update for PR

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* guard torchvision

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Update nemo/collections/cv/data/video_to_text_dataset.py

Co-authored-by: Igor Gitman <igor.a.gitman@gmail.com>
Signed-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>

* _video_speech_collate_fn in cv/data/video_to_text.py

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* add self.out = None to asr subsampling

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* Update nemo/collections/cv/data/video_to_text_dataset.py

Co-authored-by: Igor Gitman <igor.a.gitman@gmail.com>
Signed-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>

* cv -> multimodal/speech_cv branch

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: mburchi <maxime.burchi@gmail.com>
Signed-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Igor Gitman <igor.a.gitman@gmail.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* HF StarCoder to NeMo conversion script (#7421)

* Script to convert HF StarCoder checkpoint to NeMo

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* StarCoder conversion test

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Fix test

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Catch up with save_to changes

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Don't abbreviate args for clarity

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Configurable precision: BF16 vs FP32

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* fix bug when loading dist ckpt in peft (#7452)

Signed-off-by: Hongbin Liu <hongbinl@nvidia.com>
Co-authored-by: Hongbin Liu <hongbinl@nvidia.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix adding positional embeddings in-place in transformer module (#7440)

Signed-off-by: Tamerlan Tabolov <tktabolov@gmail.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix (#7478)

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* add sleep (#7498) (#7499)

* add sleep



* add sleep onto config instead



* add comment



---------

Signed-off-by: Gerald Shen <geshen@nvidia.com>
Co-authored-by: Gerald Shen <119401249+gshennvm@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix exp manager check for sleep (#7503) (#7504)

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* bugfix: trainer.accelerator=auto from None. (#7492) (#7493)

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* [doc] fix broken link (#7481)

Signed-off-by: Stas Bekman <stas00@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* [TTS] Read audio as int32 to avoid flac read errors (#7477)

* [TTS] Read audio as int32 to avoid flac read errors

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Add comment about read failures

Signed-off-by: Ryan <rlangman@nvidia.com>

---------

Signed-off-by: Ryan <rlangman@nvidia.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Add dataset 'AISHELL-3' from OpenSLR for training mandarin TTS (#7409)

* Add dataset 'AISHELL-3' from OpenSLR for training mandarin TTS
* Train 'AISHELL-3' dataset with multi-speakers

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Update get_data.py

update copyright header

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* Update get_data.py

added a disclaimer

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add new configuration file for AISHELL3 with multispeaker of fastpitch

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* dllogger - log on rank 0 only (#7513)

Signed-off-by: Stas Bekman <stas00@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix TTS FastPitch tutorial (#7494) (#7516)

* Fix

---------

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix get_dist() tensor dimension (#7506) (#7515)

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>
Co-authored-by: Jocelyn <jocelynh@nvidia.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* bugfix: specify trainer.strategy=auto when devices=1 (#7509) (#7512)

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* fix (#7511)

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* [TTS] Fix FastPitch data prep tutorial (#7524)

Signed-off-by: Ryan <rlangman@nvidia.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* add italian tokenization (#7486)

* add italian tokenization

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add more ipa lexicon it

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix error deletion

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* add test

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Replace None strategy with auto in tutorial notebooks (#7521) (#7527)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* unpin setuptools (#7534) (#7535)

Signed-off-by: fayejf <36722593+fayejf@users.noreply.github.com>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* remove auto generated examples (#7510)

* explicitly remove autogenerated examples for data parallel evaluation

Signed-off-by: arendu <adithyare@nvidia.com>

* mark autogenrated and remove it for test

Signed-off-by: arendu <adithyare@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: arendu <adithyare@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Add the `strategy` argument to `MegatronGPTModel.generate()` (#7264)

It is passed as an explicit argument rather than through
`**strategy_args` so as to ensure someone cannot accidentally pass other
arguments that would end up being ignored.

It is a keyword-only argument to ensure that if in the future we want to
update the signature to `**strategy_args`, we can do it without breaking
code.

Signed-off-by: Olivier Delalleau <507137+odelalleau@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix PTL2.0 related ASR bugs in r1.21.0: Val metrics logging, None dataloader issue (#7531) (#7533)

* fix none dataloader issue ptl2



* ptl2.0 logging fixes for rnnt_models



---------

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>
Co-authored-by: Kunal Dhawan <kunaldhawan97@gmail.com>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* gpus -> devices (#7542) (#7545)

Signed-off-by: Nithin Rao Koluguri <nithinraok>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Update FFMPEG version to fix issue with torchaudio (#7551) (#7553)

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* PEFT GPT & T5 Refactor (#7308)

* initial implementation of add_adapters API

* correct type hint

* Add config in add_adapters for save and load (@author bobchen)

* Remove AdapterConfig to avoid import error

* Add AdaterConfig back and move adaptermixin to sft model

* Add NLPSaveRestoreConnector as default in NLPModel.restore_from

* Add restore_from_nemo_with_adapter and test script

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* rename t5 file and classes to be consistent with GPT

* add t5 sft dataset

* add support for single-file format with T5SFTDataset

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Various small changes to make T5 SFT work like GPT SFT

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add adapter evaluation test script

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add MultiAdaterConfig for ia3 and fix builder issue

* Make ptuning for T5SFTModel work using mixin

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add IA3_Adapter for AdapterName

* Add adapter name for ptuning and attention adapter

* Make test script GPT/T5 agnostic

* Add layer selection feature

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Integrate adapter name and config

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update gpt peft tuning script to new API

* add t5 peft tuning script with new API

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix IA3 layer selection issue

* Override state_dict on SFT model instead of mixin

* Add load adapter by adapter config

* move peft config map away from example script

* auto get config from nemo adapter

* Move PEFTConfig to new file

* fix ckpt save/load for t5

* name change: add_adapters -> add_adapter

* variable name change

* update t5 script

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix t5 issues

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add weight tying

* update gpt tuning script

* PEFT-API proposal

* Fix according to comments

* update tuning scripts

* move merge_cfg_with to mixin class since it applies to both gpt and t5 and requires the model class for restore

* Add mcore_gpt support for NLPAdapterMixin

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix typo

* variable name change to distinguish ""peft"" and ""adapter""

* override `load_adapters` to support `add_adapter` name change

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update tuning and eval script for adapter save/load

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add Ptuning on first stage only

* add lora tutorial for review

* Fix layer selection for mcore

* add landing page

* fix resume training

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add mcore condition in sharded_state_dict to make sft work

* Update lora_tutorial.md

First edit of this file for PEFT documentation for NeMO

Signed-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>

* rename Adapter to AttentionAdapter to avoid confusion in doc

* Change load_adapters to load .nemo

* add quick start guide

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add load_adapters with .ckpt

* Remove setup_complete changes in load_adapters

* update landing page

* remove typo

* Updated quick_start.md per Chen Cui

Signed-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>

* Add inference config merger and tutorial

* Add doc string for NLPAdapterModelMixin and deprecated warning on MegatronGPTPEFTModel

* add supported_methods.md and update other documentations

* Update supported_methods.md

minor updates.

Signed-off-by: Adi Renduchintala <adithyare@nvidia.com>

* Update landing_page.md

minor update.

Signed-off-by: Adi Renduchintala <adithyare@nvidia.com>

* Modify doc string for NLPAdapterModelMixin

* Add doc string add_adapters in NLPAdapterModelMixin

* rename canonical adapters

* remove mcore hard dependency

* [PATCH] move microbatch calculator to nemo from apex

* remove apex dependency in gpt and t5 sft models

* remove apex dependency in gpt model

* render doc strings

* fix

* Add missing virtual_tokens on ptuning

* fix docstrings

* update gpt-style model coverage in docs

* update docstring

* Remove pdb

* add lightning_fabric to make docstring rendering work

* Add Ptuning missing key

* try docstring rendering

* Fix ptuning issue

* update gpt t5 peft tuning and eval scripts

* typos

* update eval config

* fix bug relating to apex dependency removal

* typo

* make predict step behave the same as test step

* make lora tutorial work in notebook

* cosmetics

* update yaml scripts

* mcore_gpt attribute optional

* typo

* update eval scripts and fix T5 eval bugs

* add NLPDDPStrategyNotebook and trainer builder logic to use it

* update lora notebook to use new trainer builder

* fix microbatch calculator bug for inference after training

* Convert markdown files to RST and incorporate with doc

* typo

* revise language

* remove extra cell

* remove unnecessary inheritance

* remove old tests

* move layer selection default so logging messages make sense

* remove `save_adapters` as adapter weights are saved automatically during training

* initialize weights from a checkpoint instead of randomly

* multiple fields can form a context (#7147)

* list of context fields and flexible prompt template

Signed-off-by: arendu <adithya.r@gmail.com>

* list of fields for context

Signed-off-by: arendu <adithya.r@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Add multiple truncation fields and middle truncation

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Compatible to old ckpt

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix tokenize detokenize issue

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Remove detokenization, add truncation augmentation

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Resolve comments

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Remove unused import

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* revert eos

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Add tokenizer space_sensitive attribute

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix error

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Fix erorr and use re

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Change assert logic

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Follow adi suggestion

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Remove merge function

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add example and comment

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Remove context_key and add comment

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Remove random truncation

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix template none

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

---------

Signed-off-by: arendu <adithya.r@gmail.com>
Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Signed-off-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>

* revert config changes

* remove accidental breakpoint

* support TP>1 loading

* infer adapter type from checkpoint in during eval

* breakup add adapter

* enable interpolation of train_ds and validation_ds

* update metric calc script to conform to single-file eval format

* remove extraneous print

* update lora notebook for updated merge_inference_cfg

* Update nlp_adapter_mixins.py

variable name change

Signed-off-by: Chen Cui <chcui@nvidia.com>

* turn off grad scaler for PP to match old scripts

* remove PEFTSaveRestoreConnector since functionality all covered by the new mixin class

* remove resume_from_checkpoint check since covered in #7335

* revert changes made in eval config interpolation

* more interpolation

* typo

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* remove dup line

Signed-off-by: Chen Cui <chcui@nvidia.com>

* code style warnings

Signed-off-by: Chen Cui <chcui@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix config mistake

Signed-off-by: Chen Cui <chcui@nvidia.com>

* add copyright header

Signed-off-by: Chen Cui <chcui@nvidia.com>

* fix code check warnings

Signed-off-by: Chen Cui <chcui@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* revert changes to remove apex dependency (mixed apex+nemo microbatch calculator broke some CI tests)

Signed-off-by: Chen Cui <chcui@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add more deprecation notices

Signed-off-by: Chen Cui <chcui@nvidia.com>

* update deprecation notices

Signed-off-by: Chen Cui <chcui@nvidia.com>

* update deprecation notices

Signed-off-by: Chen Cui <chcui@nvidia.com>

* consolidate peft and sft scripts

Signed-off-by: Chen Cui <chcui@nvidia.com>

* update CI tests

Signed-off-by: Chen Cui <chcui@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* notebook branch points to main to prepare for merge

Signed-off-by: Chen Cui <chcui@nvidia.com>

* fix gpt and t5 validation with any metric other than loss

Signed-off-by: Chen Cui <chcui@nvidia.com>

* support pre-extracted checkpoints

Signed-off-by: Chen Cui <chcui@nvidia.com>

---------

Signed-off-by: jasonwan <jasonwan@nvidia.com>
Signed-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>
Signed-off-by: Adi Renduchintala <adithyare@nvidia.com>
Signed-off-by: arendu <adithya.r@gmail.com>
Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Signed-off-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>
Signed-off-by: Chen Cui <chcui@nvidia.com>
Co-authored-by: Chen Cui <chcui@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Marc Romeyn <marcromeyn@gmail.com>
Co-authored-by: jasonwan <jasonwan@nvidia.com>
Co-authored-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>
Co-authored-by: Adi Renduchintala <adithyare@nvidia.com>
Co-authored-by: Yuanzhe Dong <yudong@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* fix a typo (#7496)

Signed-off-by: BestJuly <chntaoli@163.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* [TTS] remove curly braces from ${BRANCH} in jupyer notebook cell. (#7554) (#7560)

* remove curly braces.
* remove installation of pynini.
---------

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* add youtube embed url (#7570)

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Remap speakers to continuous range of speaker_id for dataset AISHELL3 (#7536)

* Remap speakers to continuous range of speaker_id for dataset AISHELL3
* Add new key/value pair to record raw speaker for AISHELL3 dataset

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* fix validation_step_outputs initialization for multi-dataloader (#7546) (#7572)

* added correct validation_step_outputs initialization for mutli-dataloader



* changed kernel for display



* Update logic for validation and test step outputs



* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* revert multidataloader changes in multilang ASR notebook



---------

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>
Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: Kunal Dhawan <kunaldhawan97@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Append output of val step to self.validation_step_outputs (#7530) (#7532)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* [TTS] fixed trainer's accelerator and strategy. (#7569) (#7574)

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Append val/test output to instance variable in EncDecSpeakerLabelModel (#7562) (#7573)

* Append val/test output to the instance variable in EncDecSpeakerLabelModel



* Handle test case in evaluation_step



* Replace type with isinstance



---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix CustomProgressBar for resume (#7427) (#7522)

* Fix CustomProgress Bar for resume and multiple epochs



* Edit num_training_batches



* Use max_steps as total for progress bar for resume



* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* fix typos in nfa and speech enhancement tutorials (#7580) (#7583)

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>
Co-authored-by: Elena Rastorgueva <80532067+erastorgueva-nv@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Add strategy as ddp_find_unused_parameters_true for glue_benchmark.py (#7454) (#7461)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* update strategy (#7577) (#7578)

Signed-off-by: Nithin Rao Koluguri <nithinraok>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix typos (#7581)

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Change hifigan finetune strategy to ddp_find_unused_parameters_true (#7579) (#7584)

* Change strategy to auto



---------

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* [BugFix] Add missing quotes for auto strategy in tutorial notebooks (#7541) (#7548)

* Add missing quotes for auto strategy



* Revert trainer.gpus to trainer.devices in Self_Supervised_Pre_Training.ipynb



---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Signed-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* add build os key (#7596) (#7599)

* add build os key



* add tools



* update to stable version



---------

Signed-off-by: Nithin Rao Koluguri <nithinraok>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* StarCoder SFT test + bump PyT NGC image to 23.09 (#7540)

* Add SFT StarCoder test

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Remove _modify_config call as it is covered in load_from_nemo just below

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Test with pyt:23.09 container

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

---------

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* defaults changed (#7600)

* defaults changed

Signed-off-by: arendu <adithyare@nvidia.com>

* typo

Signed-off-by: arendu <adithyare@nvidia.com>

* update

Signed-off-by: arendu <adithyare@nvidia.com>

---------

Signed-off-by: arendu <adithyare@nvidia.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* add ItalianPhonemesTokenizer (#7587)

* add ItalianPhonemesTokenizer

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix Italian phonemes

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add test

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

---------

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* best ckpt fix (#7564) (#7588)

Signed-off-by: dimapihtar <dpihtar@gmail.com>
Co-authored-by: Dmytro Pykhtar <37850217+dimapihtar@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Add files via upload (#7598)

specifies the branch

Signed-off-by: George <37293288+Jorjeous@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix validation in G2PModel and ThutmoseTaggerModel (#7597) (#7606)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain (#7576) (#7586)

* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain
* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Safeguard nemo_text_processing installation on ARM (#7485)

* safeguard nemo_text_processing installing

Signed-off-by: Jason <jasoli@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update check

Signed-off-by: Jason <jasoli@nvidia.com>

---------

Signed-off-by: Jason <jasoli@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Bound transformers version in requirements (#7620)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* fix llama2 70b lora tuning bug (#7622)

* fix llama2 70b lora tuning bug

Signed-off-by: Chen Cui <chcui@nvidia.com>

* Update peft_config.py

brackets

Signed-off-by: Adi Renduchintala <adithyare@nvidia.com>

---------

Signed-off-by: Chen Cui <chcui@nvidia.com>
Signed-off-by: Adi Renduchintala <adithyare@nvidia.com>
Co-authored-by: Adi Renduchintala <adithyare@nvidia.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix import error no module name model_utils (#7629)

Signed-off-by: Mehadi Hasan Menon <mehadihasan80@gmail.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* add fc large ls models (#7641)

Signed-off-by: Nithin Rao Koluguri <nithinraok>
Co-authored-by: Nithin Rao Koluguri <nithinraok>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* bugfix: trainer.gpus, trainer.strategy, trainer.accelerator (#7621) (#7642)

* [TTS] bugfix for Tacotron2 tutorial due to PTL 2.0
* trainer.gpus -> trainer.devices
* fixed related tutorial bugs
---------
Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* fix ssl models ptl monitor val through logging (#7608) (#7614)

Signed-off-by: Nithin Rao Koluguri <nithinraok>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix metrics for SE tutorial (#7604) (#7612)

Signed-off-by: Ante JukicÃÅ <ajukic@nvidia.com>
Co-authored-by: anteju <108555623+anteju@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Add ddp_find_unused_parameters=True and change accelerator to auto (#7623) (#7644)

* Add ddp_find_unused_parameters=True and change acclerator to auto



* Add ddp_find_unused_parameters True for normalization_as_tagging_train.py



---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix py3.11 dataclasses issue  (#7616)

* Fix py3.11 dataclasses issue  (#7582)

* Update ASR configs to support Python 3.11

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update TTS configs to support Python 3.11

Signed-off-by: smajumdar <titu1994@gmail.com>

* Guard MeCab and Ipadic

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix remaining ASR dataclasses

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix remaining ASR dataclasses

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix scripts

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Update name to ConfidenceMethodConfig

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain (#7576) (#7586)

* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain
* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Safeguard nemo_text_processing installation on ARM (#7485)

* safeguard nemo_text_processing installing

Signed-off-by: Jason <jasoli@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update check

Signed-off-by: Jason <jasoli@nvidia.com>

---------

Signed-off-by: Jason <jasoli@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Fix changes to confidence measure

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: smajumdar <titu1994@gmail.com>
Signed-off-by: Sangkug Lym <slym@nvidia.com>
Signed-off-by: Jason <jasoli@nvidia.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>
Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: Jason <jasoli@nvidia.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix issues with Dockerfile (#7650) (#7652)

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* [ASR] RNN-T greedy decoding max_frames fix for alignment and confidence (#7635)

* decoding and test fix

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* [ASR] Fix type error in jasper (#7636) (#7653)

Signed-off-by: Ryan <rlangman@nvidia.com>
Co-authored-by: Ryan Langman <rlangman@nvidia.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* [TTS] Add STFT and SI-SDR loss to audio codec recipe (#7468)

* [TTS] Add STFT and SI-SDR loss to audio codec recipe

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Fix STFT resolution

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Fix training metric logging

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Add docstring to mel and stft losses

Signed-off-by: Ryan <rlangman@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Ryan <rlangman@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* add outline of asr quickstart info to asr/intro.rst

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* add CLI, LM and real-time transcription sections

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Create per.py (#7538)

* Move model precision copy (#7336)

* move cfg precision set to megatron base model

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* remove copy from other models

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* modify attribute not arg

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* fix gpt model test for ptl 2.0

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* rename function and add docstring

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* replace precision to dtype conditionals with func call

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* unnecessary function and cfg reset

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* set default value

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* fix precision lookup in a few more places

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* rename mapping function

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* ununsed import

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* save torch datatype to model

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* set weights precision wrt amp o2

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* Revert ""set weights precision wrt amp o2""

This reverts commit 313a4bfe5eb69d771a6d2433898c0685836aef5c.

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* revert half precision at inference attempt

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* move autocast dtype to base model

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* move params dtype to base model, enable fp16 O2 inf

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* unused imports

Signed-off-by: Maanu Grover <maanug@nvidia.com>

---------

Signed-off-by: Maanu Grover <maanug@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix PEFT checkpoint loading (#7388)

* Fix PEFT checkpoint loading

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Jason Wang <jasonwan@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Use distributed optimizer support for multiple dtypes (#7359)

* Update distopt wrapper with multiple dtype support

Remove manual handling of separate FP32 optimizer.

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Use distopt support for contiguous buffers with multiple dtypes

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Fix typo

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Separate distopt buckets for first GPT layer and non-overlapped params

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Add distopt logic for int dtypes

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Update Apex commit

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Remove unused variables

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Update Apex commit in README and Jenkensfile

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Debug Dockerfile and Jenkinsfile

Signed-off-by: Tim Moon <tmoon@nvidia.com>

---------

Signed-off-by: Tim Moon <tmoon@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* minor fix for llama ckpt conversion script (#7387)

* minor fix for llama ckpt conversion script

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* Update Jenkinsfile

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* remove fast_swiglu configuration

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

---------

Signed-off-by: Jason Wang <jasonwan@nvidia.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix wrong calling of librosa.get_duration() in notebook (#7376)

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [PATCH] PEFT import mcore (#7393)

* [PATCH] PEFT import mcore

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Jason Wang <jasonwan@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Create per.py

Script for calculation Punctuation Error Rate and related rates (correct rate, deletions rate, etc.)

Signed-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [TTS] Added a callback for logging initial data (#7384)

Signed-off-by: Ante JukicÃÅ <ajukic@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Update Core Commit (#7402)

* Update Core Commit

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* update commit

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

---------

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Use cfg attribute in bert (#7394)

* use cfg attribute instead of arg

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* use torch_dtype in place of cfg.precision

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* move precision copy before super constructor

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* use trainer arg

Signed-off-by: Maanu Grover <maanug@nvidia.com>

---------

Signed-off-by: Maanu Grover <maanug@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add support for bias conversion in Swiglu models (#7386)

* Add support for bias conversion in Swiglu models

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add support for auto extracting tokenizer model

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add support for auto extracting tokenizer model

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix issue with missing tokenizer

Signed-off-by: smajumdar <titu1994@gmail.com>

* Refactor

Signed-off-by: smajumdar <titu1994@gmail.com>

* Refactor

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Update save_to and restore_from for dist checkpointing (#7343)

* add dist ckpt to save to, in progress

Signed-off-by: eharper <eharper@nvidia.com>

* move dist ckpt

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* clean up

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update restore from, need to figure out how to initialize distributed

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* launch distrib if needed when restoring dist ckpt

Signed-off-by: eharper <eharper@nvidia.com>

* when using mcore we can change tp pp on the fly

Signed-off-by: eharper <eharper@nvidia.com>

* add load_from_checkpoint support for dist ckpt

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update llama convert script to save dist .nemo

Signed-off-by: eharper <eharper@nvidia.com>

* fix load dist ckpt

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* setup TE TP groups if needed

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* setup te tp groups if needed

Signed-off-by: eharper <eharper@nvidia.com>

* remove import

Signed-off-by: eharper <eharper@nvidia.com>

---------

Signed-off-by: eharper <eharper@nvidia.com>
Signed-off-by: jasonwan <jasonwan@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: jasonwan <jasonwan@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* fix forward for with mcore=false (#7403)

Signed-off-by: Jimmy Zhang <jiemingz@nvidia.com>
Co-authored-by: Jimmy Zhang <jiemingz@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix logging to remove 's/it' from progress bar in Megatron models and add train_step_timing (#7374)

* Add CustomProgressBar class to exp_manager and trainer callbacks

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix the progress bar to reflect total microbatch cnt

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Modify CustomProgressBar class

1) Modify CustomProgressBar class to update progress bar per global_step instead of per microbatch
2) Add the callback to other megatron training/finetuning files that are not using MegatronTrainerBuilder

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add CustomProgressBar callback to tuning files

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Set Activation Checkpointing Defaults (#7404)

* Set Activation Checkpointing Defaults

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* check for None

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* make loss mask default to false (#7407)

Signed-off-by: eharper <eharper@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add dummy userbuffer config files (#7408)

Signed-off-by: Sangkug Lym <slym@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* add missing ubconf files (#7412)

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* New tutorial on Speech Data Explorer (#7405)

* Added Google Colab based tutorial on Speech Data Explorer

Signed-off-by: George Zelenfroynd <gzelenfroind@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Update ptl training ckpt conversion script to work with dist ckpt (#7416)

* update ptl convert script

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* don't break legacy

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: eharper <eharper@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Allow disabling sanity checking when num_sanity_val_steps=0 (#7413)

* Allow disabling sanity checking when num_sanity_val_steps=0

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Update num_sanity_val_steps to be a multiple of num_microbatches

Signed-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Signed-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add comprehensive error messages (#7261)

Signed-off-by: Anton Peganov <apeganov@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* check NEMO_PATH (#7418)

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* layer selection for ia3 (#7417)

* layer selection for ia3

Signed-off-by: arendu <adithyare@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: arendu <adithyare@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix missing pip package 'einops' (#7397)

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix failure of pyaudio in Google Colab (#7396)

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Update README.md: output_path --> output_manifest_filepath (#7442)

Signed-off-by: Samuele Cornell <cornellsamuele@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add rope dynamic linear scaling (#7437)

* Add dynamic linear scaling

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

---------

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix None dataloader issue in PTL2.0 (#7455)

* Fix None dataloader issue in PTL2.0

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* updating values of self._validation_dl and self._test_dl as well

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* updating values of self._validation_dl and self._test_dl as well

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [ASR] Confidence measure -> method renames (#7434)

* measure -> method

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add steps for document of getting dataset 'SF Bilingual Speech' (#7378)

* Add steps for document of getting dataset 'SF Bilingual Speech'

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Update datasets.rst

added a link from a tutorial demonstrating detailed data prep steps.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* RNN-T confidence and alignment bugfix (#7381)

* new frame_confidence and alignments lists are now always created after the while loop

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

* tests added

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

---------

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix resume from checkpoint in exp_manager (#7424) (#7426)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix checking of cuda/cpu device for inputs of Decoder (#7444)

* Fix checking of cuda/cpu device for inputs of Decoder

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Update tacotron2.py

Signed-off-by: Jason <jasoli@nvidia.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Jason <jasoli@nvidia.com>
Co-authored-by: Jason <jasoli@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix failure of ljspeech's get_data.py (#7430)

* Fix failure of ljspeech's get_data.py

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [TTS] Fix audio codec type checks (#7373)

* [TTS] Fix audio codec type checks

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Fix audio codec tests

Signed-off-by: Ryan <rlangman@nvidia.com>

---------

Signed-off-by: Ryan <rlangman@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [‚Ä¶"
github.com/NVIDIA/NeMo,nemo/collections/multimodal/data/neva/neva_dataset.py,2024-01-19T16:11:01Z,"Multimodal required NLP base model changes (#8188)

* Base model changes

* Revert ""Base model changes""

This reverts commit 8d7fd0e86b73e2cc706d46c128896c4443f63fae.

* Base model changes

* Update nvgpt template

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Co-authored-by: Eric Harper <complex451@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/multimodal/data/neva/neva_dataset.py,2023-12-13T02:12:55Z,"Add All Multimodal Source Code (#7791)

* Add comprehensive error messages (#7261)

Signed-off-by: Anton Peganov <apeganov@nvidia.com>

* check NEMO_PATH (#7418)

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* layer selection for ia3 (#7417)

* layer selection for ia3

Signed-off-by: arendu <adithyare@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: arendu <adithyare@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Fix missing pip package 'einops' (#7397)

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Fix failure of pyaudio in Google Colab (#7396)

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Update README.md: output_path --> output_manifest_filepath (#7442)

Signed-off-by: Samuele Cornell <cornellsamuele@gmail.com>

* Updating FlashAttention API to match FlashAttentionV2

* Multiple fixes for mm

* Fix CI inductor issue and update to torch compile

* Remove suppress error

* Fix when conversion config uses fp16 and it complains about precision plugin

* Fixing FAv2 API usage

* Initial release of content filtering model

* Added synthetic dataloader for precached and online mode

* Mingyuanm/dreambooth opt

* Add llama2 support in neva training

* Fix sampler length

* Fix all precision issues in nemo multimodal

* Add rope dynamic linear scaling (#7437)

* Add dynamic linear scaling

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

---------

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>

* Fix None dataloader issue in PTL2.0 (#7455)

* Fix None dataloader issue in PTL2.0

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* updating values of self._validation_dl and self._test_dl as well

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* updating values of self._validation_dl and self._test_dl as well

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* [ASR] Confidence measure -> method renames (#7434)

* measure -> method

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Add steps for document of getting dataset 'SF Bilingual Speech' (#7378)

* Add steps for document of getting dataset 'SF Bilingual Speech'

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Update datasets.rst

added a link from a tutorial demonstrating detailed data prep steps.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* RNN-T confidence and alignment bugfix (#7381)

* new frame_confidence and alignments lists are now always created after the while loop

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

* tests added

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

---------

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

* Fix resume from checkpoint in exp_manager (#7424) (#7426)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* Fix checking of cuda/cpu device for inputs of Decoder (#7444)

* Fix checking of cuda/cpu device for inputs of Decoder

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Update tacotron2.py

Signed-off-by: Jason <jasoli@nvidia.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Jason <jasoli@nvidia.com>
Co-authored-by: Jason <jasoli@nvidia.com>

* Fix failure of ljspeech's get_data.py (#7430)

* Fix failure of ljspeech's get_data.py

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* [TTS] Fix audio codec type checks (#7373)

* [TTS] Fix audio codec type checks

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Fix audio codec tests

Signed-off-by: Ryan <rlangman@nvidia.com>

---------

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Add dataset to path of logged artifacts (#7462)

* [TTS] Add dataset to path of logged artifacts

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Revert axis name back to Audio Frames

Signed-off-by: Ryan <rlangman@nvidia.com>

---------

Signed-off-by: Ryan <rlangman@nvidia.com>

* Fix sft dataset truncation (#7464)

* Add fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

---------

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Automatic Lip Reading Recognition (ALR) - ASR/CV (Visual ASR) (#7330)

* striding_conv1d_k5 and dw_striding_conv1d_k5 subsampling

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* transpose conv1d inputs

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* Update subsampling.py

change striding_conv1d_k5 to striding_conv1d

Signed-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>

* cv branch

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* video manifest

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* add collection classes

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add test_step_outputs

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* correct manifest bug when having only audio or only videos

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* correct manifest bug when having only audio or only videos

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* clean references

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* freeze unfreeze transcribe cv models

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* correct manifest get_full_path bug

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* update for PR

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* guard torchvision

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Update nemo/collections/cv/data/video_to_text_dataset.py

Co-authored-by: Igor Gitman <igor.a.gitman@gmail.com>
Signed-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>

* _video_speech_collate_fn in cv/data/video_to_text.py

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* add self.out = None to asr subsampling

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* Update nemo/collections/cv/data/video_to_text_dataset.py

Co-authored-by: Igor Gitman <igor.a.gitman@gmail.com>
Signed-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>

* cv -> multimodal/speech_cv branch

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: mburchi <maxime.burchi@gmail.com>
Signed-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Igor Gitman <igor.a.gitman@gmail.com>

* HF StarCoder to NeMo conversion script (#7421)

* Script to convert HF StarCoder checkpoint to NeMo

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* StarCoder conversion test

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Fix test

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Catch up with save_to changes

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Don't abbreviate args for clarity

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Configurable precision: BF16 vs FP32

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* fix bug when loading dist ckpt in peft (#7452)

Signed-off-by: Hongbin Liu <hongbinl@nvidia.com>
Co-authored-by: Hongbin Liu <hongbinl@nvidia.com>

* Fix adding positional embeddings in-place in transformer module (#7440)

Signed-off-by: Tamerlan Tabolov <tktabolov@gmail.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>

* Fix (#7478)

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* add sleep (#7498) (#7499)

* add sleep



* add sleep onto config instead



* add comment



---------

Signed-off-by: Gerald Shen <geshen@nvidia.com>
Co-authored-by: Gerald Shen <119401249+gshennvm@users.noreply.github.com>

* Fix exp manager check for sleep (#7503) (#7504)

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>

* bugfix: trainer.accelerator=auto from None. (#7492) (#7493)

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>

* [doc] fix broken link (#7481)

Signed-off-by: Stas Bekman <stas00@users.noreply.github.com>

* [TTS] Read audio as int32 to avoid flac read errors (#7477)

* [TTS] Read audio as int32 to avoid flac read errors

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Add comment about read failures

Signed-off-by: Ryan <rlangman@nvidia.com>

---------

Signed-off-by: Ryan <rlangman@nvidia.com>

* Add dataset 'AISHELL-3' from OpenSLR for training mandarin TTS (#7409)

* Add dataset 'AISHELL-3' from OpenSLR for training mandarin TTS
* Train 'AISHELL-3' dataset with multi-speakers

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Update get_data.py

update copyright header

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* Update get_data.py

added a disclaimer

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add new configuration file for AISHELL3 with multispeaker of fastpitch

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* dllogger - log on rank 0 only (#7513)

Signed-off-by: Stas Bekman <stas00@users.noreply.github.com>

* Fix TTS FastPitch tutorial (#7494) (#7516)

* Fix

---------

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>

* Fix get_dist() tensor dimension (#7506) (#7515)

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>
Co-authored-by: Jocelyn <jocelynh@nvidia.com>

* bugfix: specify trainer.strategy=auto when devices=1 (#7509) (#7512)

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>

* fix (#7511)

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* [TTS] Fix FastPitch data prep tutorial (#7524)

Signed-off-by: Ryan <rlangman@nvidia.com>

* add italian tokenization (#7486)

* add italian tokenization

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add more ipa lexicon it

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix error deletion

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* add test

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Replace None strategy with auto in tutorial notebooks (#7521) (#7527)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

* unpin setuptools (#7534) (#7535)

Signed-off-by: fayejf <36722593+fayejf@users.noreply.github.com>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>

* remove auto generated examples (#7510)

* explicitly remove autogenerated examples for data parallel evaluation

Signed-off-by: arendu <adithyare@nvidia.com>

* mark autogenrated and remove it for test

Signed-off-by: arendu <adithyare@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: arendu <adithyare@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Add the `strategy` argument to `MegatronGPTModel.generate()` (#7264)

It is passed as an explicit argument rather than through
`**strategy_args` so as to ensure someone cannot accidentally pass other
arguments that would end up being ignored.

It is a keyword-only argument to ensure that if in the future we want to
update the signature to `**strategy_args`, we can do it without breaking
code.

Signed-off-by: Olivier Delalleau <507137+odelalleau@users.noreply.github.com>

* Fix PTL2.0 related ASR bugs in r1.21.0: Val metrics logging, None dataloader issue (#7531) (#7533)

* fix none dataloader issue ptl2



* ptl2.0 logging fixes for rnnt_models



---------

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>
Co-authored-by: Kunal Dhawan <kunaldhawan97@gmail.com>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>

* gpus -> devices (#7542) (#7545)

Signed-off-by: Nithin Rao Koluguri <nithinraok>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>

* Update FFMPEG version to fix issue with torchaudio (#7551) (#7553)

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>

* PEFT GPT & T5 Refactor (#7308)

* initial implementation of add_adapters API

* correct type hint

* Add config in add_adapters for save and load (@author bobchen)

* Remove AdapterConfig to avoid import error

* Add AdaterConfig back and move adaptermixin to sft model

* Add NLPSaveRestoreConnector as default in NLPModel.restore_from

* Add restore_from_nemo_with_adapter and test script

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* rename t5 file and classes to be consistent with GPT

* add t5 sft dataset

* add support for single-file format with T5SFTDataset

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Various small changes to make T5 SFT work like GPT SFT

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add adapter evaluation test script

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add MultiAdaterConfig for ia3 and fix builder issue

* Make ptuning for T5SFTModel work using mixin

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add IA3_Adapter for AdapterName

* Add adapter name for ptuning and attention adapter

* Make test script GPT/T5 agnostic

* Add layer selection feature

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Integrate adapter name and config

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update gpt peft tuning script to new API

* add t5 peft tuning script with new API

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix IA3 layer selection issue

* Override state_dict on SFT model instead of mixin

* Add load adapter by adapter config

* move peft config map away from example script

* auto get config from nemo adapter

* Move PEFTConfig to new file

* fix ckpt save/load for t5

* name change: add_adapters -> add_adapter

* variable name change

* update t5 script

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix t5 issues

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add weight tying

* update gpt tuning script

* PEFT-API proposal

* Fix according to comments

* update tuning scripts

* move merge_cfg_with to mixin class since it applies to both gpt and t5 and requires the model class for restore

* Add mcore_gpt support for NLPAdapterMixin

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix typo

* variable name change to distinguish ""peft"" and ""adapter""

* override `load_adapters` to support `add_adapter` name change

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update tuning and eval script for adapter save/load

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add Ptuning on first stage only

* add lora tutorial for review

* Fix layer selection for mcore

* add landing page

* fix resume training

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add mcore condition in sharded_state_dict to make sft work

* Update lora_tutorial.md

First edit of this file for PEFT documentation for NeMO

Signed-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>

* rename Adapter to AttentionAdapter to avoid confusion in doc

* Change load_adapters to load .nemo

* add quick start guide

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add load_adapters with .ckpt

* Remove setup_complete changes in load_adapters

* update landing page

* remove typo

* Updated quick_start.md per Chen Cui

Signed-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>

* Add inference config merger and tutorial

* Add doc string for NLPAdapterModelMixin and deprecated warning on MegatronGPTPEFTModel

* add supported_methods.md and update other documentations

* Update supported_methods.md

minor updates.

Signed-off-by: Adi Renduchintala <adithyare@nvidia.com>

* Update landing_page.md

minor update.

Signed-off-by: Adi Renduchintala <adithyare@nvidia.com>

* Modify doc string for NLPAdapterModelMixin

* Add doc string add_adapters in NLPAdapterModelMixin

* rename canonical adapters

* remove mcore hard dependency

* [PATCH] move microbatch calculator to nemo from apex

* remove apex dependency in gpt and t5 sft models

* remove apex dependency in gpt model

* render doc strings

* fix

* Add missing virtual_tokens on ptuning

* fix docstrings

* update gpt-style model coverage in docs

* update docstring

* Remove pdb

* add lightning_fabric to make docstring rendering work

* Add Ptuning missing key

* try docstring rendering

* Fix ptuning issue

* update gpt t5 peft tuning and eval scripts

* typos

* update eval config

* fix bug relating to apex dependency removal

* typo

* make predict step behave the same as test step

* make lora tutorial work in notebook

* cosmetics

* update yaml scripts

* mcore_gpt attribute optional

* typo

* update eval scripts and fix T5 eval bugs

* add NLPDDPStrategyNotebook and trainer builder logic to use it

* update lora notebook to use new trainer builder

* fix microbatch calculator bug for inference after training

* Convert markdown files to RST and incorporate with doc

* typo

* revise language

* remove extra cell

* remove unnecessary inheritance

* remove old tests

* move layer selection default so logging messages make sense

* remove `save_adapters` as adapter weights are saved automatically during training

* initialize weights from a checkpoint instead of randomly

* multiple fields can form a context (#7147)

* list of context fields and flexible prompt template

Signed-off-by: arendu <adithya.r@gmail.com>

* list of fields for context

Signed-off-by: arendu <adithya.r@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Add multiple truncation fields and middle truncation

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Compatible to old ckpt

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix tokenize detokenize issue

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Remove detokenization, add truncation augmentation

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Resolve comments

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Remove unused import

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* revert eos

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Add tokenizer space_sensitive attribute

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix error

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Fix erorr and use re

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Change assert logic

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Follow adi suggestion

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Remove merge function

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add example and comment

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Remove context_key and add comment

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Remove random truncation

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix template none

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

---------

Signed-off-by: arendu <adithya.r@gmail.com>
Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Signed-off-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>

* revert config changes

* remove accidental breakpoint

* support TP>1 loading

* infer adapter type from checkpoint in during eval

* breakup add adapter

* enable interpolation of train_ds and validation_ds

* update metric calc script to conform to single-file eval format

* remove extraneous print

* update lora notebook for updated merge_inference_cfg

* Update nlp_adapter_mixins.py

variable name change

Signed-off-by: Chen Cui <chcui@nvidia.com>

* turn off grad scaler for PP to match old scripts

* remove PEFTSaveRestoreConnector since functionality all covered by the new mixin class

* remove resume_from_checkpoint check since covered in #7335

* revert changes made in eval config interpolation

* more interpolation

* typo

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* remove dup line

Signed-off-by: Chen Cui <chcui@nvidia.com>

* code style warnings

Signed-off-by: Chen Cui <chcui@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix config mistake

Signed-off-by: Chen Cui <chcui@nvidia.com>

* add copyright header

Signed-off-by: Chen Cui <chcui@nvidia.com>

* fix code check warnings

Signed-off-by: Chen Cui <chcui@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* revert changes to remove apex dependency (mixed apex+nemo microbatch calculator broke some CI tests)

Signed-off-by: Chen Cui <chcui@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add more deprecation notices

Signed-off-by: Chen Cui <chcui@nvidia.com>

* update deprecation notices

Signed-off-by: Chen Cui <chcui@nvidia.com>

* update deprecation notices

Signed-off-by: Chen Cui <chcui@nvidia.com>

* consolidate peft and sft scripts

Signed-off-by: Chen Cui <chcui@nvidia.com>

* update CI tests

Signed-off-by: Chen Cui <chcui@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* notebook branch points to main to prepare for merge

Signed-off-by: Chen Cui <chcui@nvidia.com>

* fix gpt and t5 validation with any metric other than loss

Signed-off-by: Chen Cui <chcui@nvidia.com>

* support pre-extracted checkpoints

Signed-off-by: Chen Cui <chcui@nvidia.com>

---------

Signed-off-by: jasonwan <jasonwan@nvidia.com>
Signed-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>
Signed-off-by: Adi Renduchintala <adithyare@nvidia.com>
Signed-off-by: arendu <adithya.r@gmail.com>
Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Signed-off-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>
Signed-off-by: Chen Cui <chcui@nvidia.com>
Co-authored-by: Chen Cui <chcui@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Marc Romeyn <marcromeyn@gmail.com>
Co-authored-by: jasonwan <jasonwan@nvidia.com>
Co-authored-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>
Co-authored-by: Adi Renduchintala <adithyare@nvidia.com>
Co-authored-by: Yuanzhe Dong <yudong@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>

* fix a typo (#7496)

Signed-off-by: BestJuly <chntaoli@163.com>

* [TTS] remove curly braces from ${BRANCH} in jupyer notebook cell. (#7554) (#7560)

* remove curly braces.
* remove installation of pynini.
---------

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>

* add youtube embed url (#7570)

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>

* Remap speakers to continuous range of speaker_id for dataset AISHELL3 (#7536)

* Remap speakers to continuous range of speaker_id for dataset AISHELL3
* Add new key/value pair to record raw speaker for AISHELL3 dataset

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* fix validation_step_outputs initialization for multi-dataloader (#7546) (#7572)

* added correct validation_step_outputs initialization for mutli-dataloader



* changed kernel for display



* Update logic for validation and test step outputs



* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* revert multidataloader changes in multilang ASR notebook



---------

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>
Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: Kunal Dhawan <kunaldhawan97@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Append output of val step to self.validation_step_outputs (#7530) (#7532)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

* [TTS] fixed trainer's accelerator and strategy. (#7569) (#7574)

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>

* Append val/test output to instance variable in EncDecSpeakerLabelModel (#7562) (#7573)

* Append val/test output to the instance variable in EncDecSpeakerLabelModel



* Handle test case in evaluation_step



* Replace type with isinstance



---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

* Fix CustomProgressBar for resume (#7427) (#7522)

* Fix CustomProgress Bar for resume and multiple epochs



* Edit num_training_batches



* Use max_steps as total for progress bar for resume



* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* fix typos in nfa and speech enhancement tutorials (#7580) (#7583)

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>
Co-authored-by: Elena Rastorgueva <80532067+erastorgueva-nv@users.noreply.github.com>

* Add strategy as ddp_find_unused_parameters_true for glue_benchmark.py (#7454) (#7461)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

* update strategy (#7577) (#7578)

Signed-off-by: Nithin Rao Koluguri <nithinraok>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>

* Fix typos (#7581)

* Change hifigan finetune strategy to ddp_find_unused_parameters_true (#7579) (#7584)

* Change strategy to auto



---------

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>

* [BugFix] Add missing quotes for auto strategy in tutorial notebooks (#7541) (#7548)

* Add missing quotes for auto strategy



* Revert trainer.gpus to trainer.devices in Self_Supervised_Pre_Training.ipynb



---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Signed-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

* add build os key (#7596) (#7599)

* add build os key



* add tools



* update to stable version



---------

Signed-off-by: Nithin Rao Koluguri <nithinraok>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>

* StarCoder SFT test + bump PyT NGC image to 23.09 (#7540)

* Add SFT StarCoder test

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Remove _modify_config call as it is covered in load_from_nemo just below

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Test with pyt:23.09 container

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

---------

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* defaults changed (#7600)

* defaults changed

Signed-off-by: arendu <adithyare@nvidia.com>

* typo

Signed-off-by: arendu <adithyare@nvidia.com>

* update

Signed-off-by: arendu <adithyare@nvidia.com>

---------

Signed-off-by: arendu <adithyare@nvidia.com>

* add ItalianPhonemesTokenizer (#7587)

* add ItalianPhonemesTokenizer

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix Italian phonemes

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add test

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

---------

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* best ckpt fix (#7564) (#7588)

Signed-off-by: dimapihtar <dpihtar@gmail.com>
Co-authored-by: Dmytro Pykhtar <37850217+dimapihtar@users.noreply.github.com>

* Add files via upload (#7598)

specifies the branch

Signed-off-by: George <37293288+Jorjeous@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* Fix validation in G2PModel and ThutmoseTaggerModel (#7597) (#7606)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain (#7576) (#7586)

* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain
* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Safeguard nemo_text_processing installation on ARM (#7485)

* safeguard nemo_text_processing installing

Signed-off-by: Jason <jasoli@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update check

Signed-off-by: Jason <jasoli@nvidia.com>

---------

Signed-off-by: Jason <jasoli@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Bound transformers version in requirements (#7620)

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* fix llama2 70b lora tuning bug (#7622)

* fix llama2 70b lora tuning bug

Signed-off-by: Chen Cui <chcui@nvidia.com>

* Update peft_config.py

brackets

Signed-off-by: Adi Renduchintala <adithyare@nvidia.com>

---------

Signed-off-by: Chen Cui <chcui@nvidia.com>
Signed-off-by: Adi Renduchintala <adithyare@nvidia.com>
Co-authored-by: Adi Renduchintala <adithyare@nvidia.com>

* Fix import error no module name model_utils (#7629)

Signed-off-by: Mehadi Hasan Menon <mehadihasan80@gmail.com>

* add fc large ls models (#7641)

Signed-off-by: Nithin Rao Koluguri <nithinraok>
Co-authored-by: Nithin Rao Koluguri <nithinraok>

* bugfix: trainer.gpus, trainer.strategy, trainer.accelerator (#7621) (#7642)

* [TTS] bugfix for Tacotron2 tutorial due to PTL 2.0
* trainer.gpus -> trainer.devices
* fixed related tutorial bugs
---------
Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* fix ssl models ptl monitor val through logging (#7608) (#7614)

Signed-off-by: Nithin Rao Koluguri <nithinraok>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* Fix metrics for SE tutorial (#7604) (#7612)

Signed-off-by: Ante JukicÃÅ <ajukic@nvidia.com>
Co-authored-by: anteju <108555623+anteju@users.noreply.github.com>

* Add ddp_find_unused_parameters=True and change accelerator to auto (#7623) (#7644)

* Add ddp_find_unused_parameters=True and change acclerator to auto



* Add ddp_find_unused_parameters True for normalization_as_tagging_train.py



---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

* Fix py3.11 dataclasses issue  (#7616)

* Fix py3.11 dataclasses issue  (#7582)

* Update ASR configs to support Python 3.11

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update TTS configs to support Python 3.11

Signed-off-by: smajumdar <titu1994@gmail.com>

* Guard MeCab and Ipadic

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix remaining ASR dataclasses

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix remaining ASR dataclasses

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix scripts

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Update name to ConfidenceMethodConfig

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain (#7576) (#7586)

* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain
* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Safeguard nemo_text_processing installation on ARM (#7485)

* safeguard nemo_text_processing installing

Signed-off-by: Jason <jasoli@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update check

Signed-off-by: Jason <jasoli@nvidia.com>

---------

Signed-off-by: Jason <jasoli@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Fix changes to confidence measure

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: smajumdar <titu1994@gmail.com>
Signed-off-by: Sangkug Lym <slym@nvidia.com>
Signed-off-by: Jason <jasoli@nvidia.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>
Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: Jason <jasoli@nvidia.com>

* [Stable Diffusion/ControlNet] Enable O2 training for SD and Fix ControlNet CI failure

* Mingyuanm/dreambooth fix

* Fix NeMo CI Infer Issue

* DreamFusion

* Move neva export changes

* Add Imagen Synthetic Dataloader

* Add VITWrapper and export stuff to wrapper

* Update neva with megatron-core support

* Fix issues with Dockerfile (#7650) (#7652)

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>

* [ASR] RNN-T greedy decoding max_frames fix for alignment and confidence (#7635)

* decoding and test fix

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* [ASR] Fix type error in jasper (#7636) (#7653)

Signed-off-by: Ryan <rlangman@nvidia.com>
Co-authored-by: Ryan Langman <rlangman@nvidia.com>

* [TTS] Add STFT and SI-SDR loss to audio codec recipe (#7468)

* [TTS] Add STFT and SI-SDR loss to audio codec recipe

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Fix STFT resolution

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Fix training metric logging

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Add docstring to mel and stft losses

Signed-off-by: Ryan <rlangman@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Ryan <rlangman@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Create per.py (#7538)

* Move model precision copy (#7336)

* move cfg precision set to megatron base model

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* remove copy from other models

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* modify attribute not arg

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* fix gpt model test for ptl 2.0

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* rename function and add docstring

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* replace precision to dtype conditionals with func call

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* unnecessary function and cfg reset

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* set default value

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* fix precision lookup in a few more places

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* rename mapping function

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* ununsed import

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* save torch datatype to model

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* set weights precision wrt amp o2

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* Revert ""set weights precision wrt amp o2""

This reverts commit 313a4bfe5eb69d771a6d2433898c0685836aef5c.

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* revert half precision at inference attempt

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* move autocast dtype to base model

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* move params dtype to base model, enable fp16 O2 inf

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* unused imports

Signed-off-by: Maanu Grover <maanug@nvidia.com>

---------

Signed-off-by: Maanu Grover <maanug@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix PEFT checkpoint loading (#7388)

* Fix PEFT checkpoint loading

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Jason Wang <jasonwan@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Use distributed optimizer support for multiple dtypes (#7359)

* Update distopt wrapper with multiple dtype support

Remove manual handling of separate FP32 optimizer.

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Use distopt support for contiguous buffers with multiple dtypes

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Fix typo

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Separate distopt buckets for first GPT layer and non-overlapped params

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Add distopt logic for int dtypes

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Update Apex commit

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Remove unused variables

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Update Apex commit in README and Jenkensfile

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Debug Dockerfile and Jenkinsfile

Signed-off-by: Tim Moon <tmoon@nvidia.com>

---------

Signed-off-by: Tim Moon <tmoon@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* minor fix for llama ckpt conversion script (#7387)

* minor fix for llama ckpt conversion script

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* Update Jenkinsfile

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* remove fast_swiglu configuration

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

---------

Signed-off-by: Jason Wang <jasonwan@nvidia.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix wrong calling of librosa.get_duration() in notebook (#7376)

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [PATCH] PEFT import mcore (#7393)

* [PATCH] PEFT import mcore

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Jason Wang <jasonwan@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Create per.py

Script for calculation Punctuation Error Rate and related rates (correct rate, deletions rate, etc.)

Signed-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [TTS] Added a callback for logging initial data (#7384)

Signed-off-by: Ante JukicÃÅ <ajukic@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Update Core Commit (#7402)

* Update Core Commit

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* update commit

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

---------

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Use cfg attribute in bert (#7394)

* use cfg attribute instead of arg

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* use torch_dtype in place of cfg.precision

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* move precision copy before super constructor

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* use trainer arg

Signed-off-by: Maanu Grover <maanug@nvidia.com>

---------

Signed-off-by: Maanu Grover <maanug@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add support for bias conversion in Swiglu models (#7386)

* Add support for bias conversion in Swiglu models

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add support for auto extracting tokenizer model

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add support for auto extracting tokenizer model

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix issue with missing tokenizer

Signed-off-by: smajumdar <titu1994@gmail.com>

* Refactor

Signed-off-by: smajumdar <titu1994@gmail.com>

* Refactor

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Update save_to and restore_from for dist checkpointing (#7343)

* add dist ckpt to save to, in progress

Signed-off-by: eharper <eharper@nvidia.com>

* move dist ckpt

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* clean up

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update restore from, need to figure out how to initialize distributed

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* launch distrib if needed when restoring dist ckpt

Signed-off-by: eharper <eharper@nvidia.com>

* when using mcore we can change tp pp on the fly

Signed-off-by: eharper <eharper@nvidia.com>

* add load_from_checkpoint support for dist ckpt

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update llama convert script to save dist .nemo

Signed-off-by: eharper <eharper@nvidia.com>

* fix load dist ckpt

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* setup TE TP groups if needed

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* setup te tp groups if needed

Signed-off-by: eharper <eharper@nvidia.com>

* remove import

Signed-off-by: eharper <eharper@nvidia.com>

---------

Signed-off-by: eharper <eharper@nvidia.com>
Signed-off-by: jasonwan <jasonwan@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: jasonwan <jasonwan@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* fix forward for with mcore=false (#7403)

Signed-off-by: Jimmy Zhang <jiemingz@nvidia.com>
Co-authored-by: Jimmy Zhang <jiemingz@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix logging to remove 's/it' from progress bar in Megatron models and add train_step_timing (#7374)

* Add CustomProgressBar class to exp_manager and trainer callbacks

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix the progress bar to reflect total microbatch cnt

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Modify CustomProgressBar class

1) Modify CustomProgressBar class to update progress bar per global_step instead of per microbatch
2) Add the callback to other megatron training/finetuning files that are not using MegatronTrainerBuilder

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add CustomProgressBar callback to tuning files

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Set Activation Checkpointing Defaults (#7404)

* Set Activation Checkpointing Defaults

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* check for None

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* make loss mask default to false (#7407)

Signed-off-by: eharper <eharper@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add dummy userbuffer config files (#7408)

Signed-off-by: Sangkug Lym <slym@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* add missing ubconf files (#7412)

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* New tutorial on Speech Data Explorer (#7405)

* Added Google Colab based tutorial on Speech Data Explorer

Signed-off-by: George Zelenfroynd <gzelenfroind@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Update ptl training ckpt conversion script to work with dist ckpt (#7416)

* update ptl convert script

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* don't break legacy

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: eharper <eharper@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Allow disabling sanity checking when num_sanity_val_steps=0 (#7413)

* Allow disabling sanity checking when num_sanity_val_steps=0

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Update num_sanity_val_steps to be a multiple of num_microbatches

Signed-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Signed-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add comprehensive error messages (#7261)

Signed-off-by: Anton Peganov <apeganov@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* check NEMO_PATH (#7418)

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* layer selection for ia3 (#7417)

* layer selection for ia3

Signed-off-by: arendu <adithyare@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: arendu <adithyare@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix missing pip package 'einops' (#7397)

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix failure of pyaudio in Google Colab (#7396)

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Update README.md: output_path --> output_manifest_filepath (#7442)

Signed-off-by: Samuele Cornell <cornellsamuele@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add rope dynamic linear scaling (#7437)

* Add dynamic linear scaling

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

---------

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix None dataloader issue in PTL2.0 (#7455)

* Fix None dataloader issue in PTL2.0

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* updating values of self._validation_dl and self._test_dl as well

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* updating values of self._validation_dl and self._test_dl as well

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [ASR] Confidence measure -> method renames (#7434)

* measure -> method

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add steps for document of getting dataset 'SF Bilingual Speech' (#7378)

* Add steps for document of getting dataset 'SF Bilingual Speech'

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Update datasets.rst

added a link from a tutorial demonstrating detailed data prep steps.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* RNN-T confidence and alignment bugfix (#7381)

* new frame_confidence and alignments lists are now always created after the while loop

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

* tests added

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

---------

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix resume from checkpoint in exp_manager (#7424) (#7426)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix checking of cuda/cpu device for inputs of Decoder (#7444)

* Fix checking of cuda/cpu device for inputs of Decoder

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Update tacotron2.py

Signed-off-by: Jason <jasoli@nvidia.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Jason <jasoli@nvidia.com>
Co-authored-by: Jason <jasoli@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix failure of ljspeech's get_data.py (#7430)

* Fix failure of ljspeech's get_data.py

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [TTS] Fix audio codec type checks (#7373)

* [TTS] Fix audio codec type checks

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Fix audio codec tests

Signed-off-by: Ryan <rlangman@nvidia.com>

---------

Signed-off-by: Ryan <rlangman@nvidia.com>
Signe‚Ä¶"
github.com/NVIDIA/NeMo,scripts/asr_language_modeling/ngram_lm/ngram_merge.py,2023-07-14T09:24:14Z,"rnnt and char utils (#6971)

* rnnt_ngram_merge

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* char level bug

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,scripts/asr_language_modeling/ngram_lm/ngram_merge.py,2023-05-04T16:57:47Z,"Make KenLM with PC for AggregateTokenizer and merge it (#6081)

* do_lowercase, rm_punctuation

Signed-off-by: Nikolay Karpov <nkarpov@nvidia.com>

* support beam_strategy = beam

Signed-off-by: Nikolay Karpov <nkarpov@nvidia.com>

* black

Signed-off-by: Nikolay Karpov <nkarpov@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix config and^Cunctuation capitalization

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* rm math

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* update kenlm

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* black

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add opengrm

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* mv install_beamsearch_decoders

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* punctuation_to_preserve

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Only tikenizer opion

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* Black

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* DEFAULT_TOKEN_OFFSET

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* aggregate_tokenizer

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* install kenlm with more than 5gram

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* install_beamsearch_decoders

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* ngram_bin_path kenlm_bin_path

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* black

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* fix greedy PC bug

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* move global params

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* fix description and perplexity

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* fix description

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* NEMO_PATH

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* nemo:23.01

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* License

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* description

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* isinstance

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* refactor kenlm stdin

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* black

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* add cmd arg

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* use new iter_files

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* EncDecHybridRNNTCTCModel

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* punctuation

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* train_kenlm args

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* add docstrings

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add ngram_merge docs

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* ngram_prune

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* rename to ngram_merge

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* rename to ngram

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* add comments

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* Ngram

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* nemo_model_file

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* install_opengrm_ngram

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* install opengrm

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* rename to install_opengrm.sh

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* rm extra import

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* train_paths

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* text_processing

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* fix ngram_bin_path

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* DECODERS_PATH

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* farcompile

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* rm text processing

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* text_processing

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* AggregateTokenizer.DummyTokenizer

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* comments

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* TextProcessingConfig

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* typo

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* doc

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* types

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* nemo_model_file

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* rm assert

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* import kenlm_utils

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* return None

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* Copyright

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* 2022

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* 2023

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

---------

Signed-off-by: Nikolay Karpov <nkarpov@nvidia.com>
Signed-off-by: Nikolay Karpov <karpnv@gmail.com>
Co-authored-by: Nikolay Karpov <nkarpov@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,scripts/asr_language_modeling/ngram_lm/kenlm_utils.py,2023-07-14T09:24:14Z,"rnnt and char utils (#6971)

* rnnt_ngram_merge

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* char level bug

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,scripts/asr_language_modeling/ngram_lm/kenlm_utils.py,2023-05-04T16:57:47Z,"Make KenLM with PC for AggregateTokenizer and merge it (#6081)

* do_lowercase, rm_punctuation

Signed-off-by: Nikolay Karpov <nkarpov@nvidia.com>

* support beam_strategy = beam

Signed-off-by: Nikolay Karpov <nkarpov@nvidia.com>

* black

Signed-off-by: Nikolay Karpov <nkarpov@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix config and^Cunctuation capitalization

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* rm math

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* update kenlm

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* black

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add opengrm

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* mv install_beamsearch_decoders

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* punctuation_to_preserve

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Only tikenizer opion

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* Black

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* DEFAULT_TOKEN_OFFSET

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* aggregate_tokenizer

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* install kenlm with more than 5gram

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* install_beamsearch_decoders

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* ngram_bin_path kenlm_bin_path

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* black

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* fix greedy PC bug

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* move global params

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* fix description and perplexity

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* fix description

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* NEMO_PATH

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* nemo:23.01

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* License

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* description

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* isinstance

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* refactor kenlm stdin

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* black

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* add cmd arg

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* use new iter_files

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* EncDecHybridRNNTCTCModel

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* punctuation

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* train_kenlm args

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* add docstrings

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add ngram_merge docs

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* ngram_prune

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* rename to ngram_merge

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* rename to ngram

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* add comments

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* Ngram

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* nemo_model_file

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* install_opengrm_ngram

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* install opengrm

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* rename to install_opengrm.sh

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* rm extra import

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* train_paths

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* text_processing

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* fix ngram_bin_path

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* DECODERS_PATH

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* farcompile

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* rm text processing

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* text_processing

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* AggregateTokenizer.DummyTokenizer

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* comments

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* TextProcessingConfig

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* typo

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* doc

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* types

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* nemo_model_file

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* rm assert

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* import kenlm_utils

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* return None

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* Copyright

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* 2022

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* 2023

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

---------

Signed-off-by: Nikolay Karpov <nkarpov@nvidia.com>
Signed-off-by: Nikolay Karpov <karpnv@gmail.com>
Co-authored-by: Nikolay Karpov <nkarpov@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,scripts/asr_language_modeling/ngram_lm/kenlm_utils.py,2023-03-14T16:26:08Z,"Ngram lm fusion for RNNT maes decoding (#6118)

* add parameters for ngram_lm

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add parameters for ngram lm

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add RNNT model types for kenlm training

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add ngram lm fusion to maes decoding mode

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add a script for the rnnt beam search decoding with a ngram lm fusion for maes

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* minor fixes

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* minor fixes

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix autocast

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* typing fix

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* minor fixes

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add set_decoding_type function

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* remove tokens_type

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* remove tokens_type from config

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* import token_offset from train_kenlm

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add DEFAULT_TOKEN_OFFSET variable

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix applying token_offset for char models

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* fixe copyright year

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* leave DEFAULT_TOKEN_OFFSET only

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

---------

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,scripts/asr_language_modeling/ngram_lm/kenlm_utils.py,2022-01-31T22:20:29Z,"Final merge r1.6.0 main (#3570)

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* Fix the tutorial notebooks bug (#3465)

* fix checkpoint loading and model config file

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix style

Signed-off-by: Yi Dong <yidong@nvidia.com>

* Fix checkpoint converter in O2 style (#3486)

* Fix checkpoint converter in O2 style

Signed-off-by: Yu Yao <yuya@nvidia.com>

* Fix style

Signed-off-by: Yu Yao <yuya@nvidia.com>

Co-authored-by: Yu Yao <yuya@nvidia.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* Remove pickled features from tarred dataset (#3491)

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

Co-authored-by: ekmb <ebakhturina@nvidia.com>

* adding missing init files (#3505)

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* typos (#3504)

* typos

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* link fix

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* update titanet conf (#3507)

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* Fix link to NGC page for ASR (#3512)

Signed-off-by: smajumdar <titu1994@gmail.com>

* vad typo fix (#3490)

* remove always broken ptl link

Signed-off-by: fayejf <fayejf07@gmail.com>

* fix typo

Signed-off-by: fayejf <fayejf07@gmail.com>

* Add verification helper function and update docs (#3514)

* Add verification helper function and update docs

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* typo fixes

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* fixed the num_classes bug of conv decoder. (#3525)

* fixed the num_classes bug.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* added logging info.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* Enforce utf-8 on all file r/w (#3520)

* Update paths to subtask

Signed-off-by: smajumdar <titu1994@gmail.com>

* Enforce utf-8 on all file r/w

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Eric Harper <complex451@gmail.com>

* Fixed section typo (#3522)

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Pushing updated WFST Tutorial to r1.6.0 (#3521)

Signed-off-by: tbartley94 <tbartley@nvidia.com>

Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>

* Fixed duplicate cell bug (#3518)

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* WFST tutorial update (#3531)

* Pushing updated WFST Tutorial to r1.6.0

Signed-off-by: tbartley94 <tbartley@nvidia.com>

* Hopefully final corrections to WFST tutorials.

Signed-off-by: tbartley94 <tbartley@nvidia.com>

* [TTS] Fix bug in inference tts notebook (#3532)

* fix bug in inference tts notebook

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* Update Inference_ModelSelect.ipynb

* fix space

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* remove space

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* Prompt tuning documentation (#3541)

* Started prompt tuning doc

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Update prompt_tuning.rst

* Update models.rst

* Update models.rst

* Update and rename megatron_finetuning.rst to megatron_downstream_tasks.rst

* Update intro.rst

* Update intro.rst

* Update and rename megatron_downstream_tasks.rst to megatron_finetuning.rst

* Update megatron_finetuning.rst

* Delete prompt_tuning.rst

* Update README.rst

* Update docs/source/nlp/megatron_finetuning.rst

Co-authored-by: Eric Harper <complex451@gmail.com>

* Fix nmt resume (#3539)

* check for model attr

Signed-off-by: ericharper <complex451@gmail.com>

* update jenkins test

Signed-off-by: ericharper <complex451@gmail.com>

* TN bug fix (#3538)

* ve and cel fixes

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* ve and cel fixes

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* add w to single digit roman and cardinal single digit graph (non det)

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* isn't fix

Signed-off-by: ekmb <ebakhturina@nvidia.com>

Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* fix bug in tutorial (#3546)

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* Update nvidia container check (#3535)

* update nvidia container check

Signed-off-by: ericharper <complex451@gmail.com>

* update minor version

Signed-off-by: ericharper <complex451@gmail.com>

* add check to T5

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* update bert

Signed-off-by: ericharper <complex451@gmail.com>

* forgot import

Signed-off-by: ericharper <complex451@gmail.com>

* remove import

Signed-off-by: ericharper <complex451@gmail.com>

* Fix an issue with wandb not displaying updated config changes (#3552)

Signed-off-by: smajumdar <titu1994@gmail.com>

* remove extra instance (#3551)

Signed-off-by: ericharper <complex451@gmail.com>

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* update package info

Signed-off-by: ericharper <complex451@gmail.com>

* revert

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Yi Dong <43824965+yidong72@users.noreply.github.com>
Co-authored-by: yaoyu-33 <54727607+yaoyu-33@users.noreply.github.com>
Co-authored-by: Yu Yao <yuya@nvidia.com>
Co-authored-by: PeganovAnton <peganoff2@mail.ru>
Co-authored-by: ekmb <ebakhturina@nvidia.com>
Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>
Co-authored-by: Evelina <10428420+ekmb@users.noreply.github.com>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>
Co-authored-by: Vahid Noroozi <VahidooX@users.noreply.github.com>
Co-authored-by: Virginia Adams <78445382+vadam5@users.noreply.github.com>
Co-authored-by: tbartley94 <90423858+tbartley94@users.noreply.github.com>
Co-authored-by: Oktai Tatanov <oktai.tatanov@gmail.com>"
github.com/NVIDIA/NeMo,scripts/asr_language_modeling/ngram_lm/kenlm_utils.py,2021-04-23T01:51:48Z,"Fixing the docs of LM for ASR models. (#2096)

* fixed the doc.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed the doc.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed the doc.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed the doc.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed the doc.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed the doc.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* changed modelling to modeling.

Signed-off-by: Vahid <vnoroozi@nvidia.com>"
github.com/NVIDIA/NeMo,tools/ctc_segmentation/scripts/run_ctc_segmentation.py,2024-02-06T21:12:13Z,"ASR Transcription Refactor (#8167)

* Temp commit

Signed-off-by: smajumdar <titu1994@gmail.com>

* Temp transcription api file

Signed-off-by: smajumdar <titu1994@gmail.com>

* Initial draft of transcribable API

Signed-off-by: smajumdar <titu1994@gmail.com>

* Udpate ASR nd Classiication model transcription functions

Signed-off-by: smajumdar <titu1994@gmail.com>

* Commit draft - works on CTC/RNNT/Hybrid/SpeechClassification

Signed-off-by: smajumdar <titu1994@gmail.com>

* Upate transcription tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix minor typos

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Update tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Try reverting numba fix

Signed-off-by: smajumdar <titu1994@gmail.com>

* Revert patch

Signed-off-by: smajumdar <titu1994@gmail.com>

* Formatting

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add full support for tensor input to model.transcribe()

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Make BS 2 for mixed tensor inference

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update return type signature and add return type tests. Fix decoder_timestamps_utils.py

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix cast to numpy

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update transcribe speech script

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix signature to transcribe()

Signed-off-by: smajumdar <titu1994@gmail.com>

* Address comments, fix issues with paths2audio_files

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix typo

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix logprobs signature for ctc segmentation

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Address comments

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Address comments and add docs

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Remove old logprobs

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix issues with regression model and add regression config. Remove `return_generator` arg

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add AED tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add support for Canary models

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Refactor prompt format for lhotse

Signed-off-by: smajumdar <titu1994@gmail.com>

---------

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,tools/ctc_segmentation/scripts/run_ctc_segmentation.py,2022-09-30T20:06:11Z,"Bug fixes for parallel mp3 to wav conversion, PC notebook, update Readme for TN requirements (#5047) (#5053)

* bug fixes segmenation, pc

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* bug fixes segmenation, pc

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* install pynini

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* add requirements install back

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* update sox requirements

Signed-off-by: ekmb <ebakhturina@nvidia.com>

Signed-off-by: ekmb <ebakhturina@nvidia.com>

Signed-off-by: ekmb <ebakhturina@nvidia.com>
Co-authored-by: Evelina <10428420+ekmb@users.noreply.github.com>"
github.com/NVIDIA/NeMo,tools/ctc_segmentation/scripts/run_ctc_segmentation.py,2022-01-06T21:03:39Z,"CTC Segmentation-Citrinet support (#3279)

* segmentation package update

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* update logs

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* tutorial

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* tutorial update

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* update tutorial

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* header

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* jenkins

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* lgtm

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* jenkins reqs installed

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* review

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* add num_workers to transcribe, fix oov pre-processing

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* num_workers default

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* docstrings

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* review

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* lgmt

Signed-off-by: ekmb <ebakhturina@nvidia.com>"
github.com/NVIDIA/NeMo,tools/ctc_segmentation/scripts/run_ctc_segmentation.py,2021-04-22T21:55:01Z,Adding N-gram LM for ASR Models (#2066)
github.com/NVIDIA/NeMo,tools/ctc_segmentation/scripts/run_ctc_segmentation.py,2020-12-11T22:48:59Z,"segmentation (#1529)

* expanded normalization helpers

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* wip

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* wip

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* wip

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* wip

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* wip

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* additional split symbols exposed

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* split condition fix

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix in add split symbols

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* jenkins test added

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* text update

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* notebook jupyter upgrade cmd added

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* jenkins

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* jenkins

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* jenkins

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* jenkins

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* install requirements

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* jenkins

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* test

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* ffmped install

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* rearrange steps

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* rearrange steps

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* test

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* test

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* test

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* restart ci

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* restart ci

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* restart ci

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* file name update

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* prefix=0

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* separator

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* refactor

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* notebook reqs

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* replace

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* restart ci

Signed-off-by: ekmb <ebakhturina@nvidia.com>"
github.com/NVIDIA/NeMo,tools/ctc_segmentation/scripts/run_ctc_segmentation.py,2020-11-13T22:55:22Z,"Dataset creation tool based on CTC-segmentation (#1450)

Dataset creation tool based on CTC-segmentation
Signed-off-by: ekmb <ebakhturina@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/decoder_timestamps_utils.py,2024-02-06T21:12:13Z,"ASR Transcription Refactor (#8167)

* Temp commit

Signed-off-by: smajumdar <titu1994@gmail.com>

* Temp transcription api file

Signed-off-by: smajumdar <titu1994@gmail.com>

* Initial draft of transcribable API

Signed-off-by: smajumdar <titu1994@gmail.com>

* Udpate ASR nd Classiication model transcription functions

Signed-off-by: smajumdar <titu1994@gmail.com>

* Commit draft - works on CTC/RNNT/Hybrid/SpeechClassification

Signed-off-by: smajumdar <titu1994@gmail.com>

* Upate transcription tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix minor typos

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Update tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Try reverting numba fix

Signed-off-by: smajumdar <titu1994@gmail.com>

* Revert patch

Signed-off-by: smajumdar <titu1994@gmail.com>

* Formatting

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add full support for tensor input to model.transcribe()

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Make BS 2 for mixed tensor inference

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update return type signature and add return type tests. Fix decoder_timestamps_utils.py

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix cast to numpy

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update transcribe speech script

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix signature to transcribe()

Signed-off-by: smajumdar <titu1994@gmail.com>

* Address comments, fix issues with paths2audio_files

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix typo

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix logprobs signature for ctc segmentation

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Address comments

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Address comments and add docs

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Remove old logprobs

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix issues with regression model and add regression config. Remove `return_generator` arg

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add AED tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add support for Canary models

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Refactor prompt format for lhotse

Signed-off-by: smajumdar <titu1994@gmail.com>

---------

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/decoder_timestamps_utils.py,2024-01-05T23:45:52Z,"Wer fix (#8047)

* WER metric reformatting + AST transformer fix.

Signed-off-by: Travis Bartley <tbartley@nvidia.com>

* fixing tests for new WER

Signed-off-by: Travis Bartley <tbartley@nvidia.com>

* Adding getter and fixing tutorial formats

Signed-off-by: Travis Bartley <tbartley@nvidia.com>

* Adding in fold_consecutive flag for WER, also adding check for proper decoding types

Signed-off-by: Travis Bartley <tbartley@nvidia.com>

* Catching some jenkins bugs

Signed-off-by: Travis Bartley <tbartley@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fixing ctc_decoding variable assignment

Signed-off-by: Travis Bartley <tbartley@nvidia.com>

* Broke a test, fixing

Signed-off-by: Travis Bartley <tbartley@nvidia.com>

* variable fix

Signed-off-by: Travis Bartley <tbartley@nvidia.com>

* Found missed ctc import for diarization_with_asr

Signed-off-by: Travis Bartley <tbartley@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Got too excited, forgot to style check

Signed-off-by: Travis Bartley <tbartley@nvidia.com>

* I have found yet more references to old ctc_decoding

Signed-off-by: Travis Bartley <tbartley@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Variable fixes for tutorials, tts, and speech_cv modules

Signed-off-by: Travis Bartley <tbartley@nvidia.com>

---------

Signed-off-by: Travis Bartley <tbartley@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/decoder_timestamps_utils.py,2023-04-28T16:39:24Z,"[BugFix] Force _get_batch_preds() to keep logits in decoder timestamps generator (#6499)

* [BugFix] _get_batch_preds() is forced to keep logits in  decoder timestamps generators

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Ingnore keep_logits boolean in FrameASRBatchLogits

Signed-off-by: Taejin Park <tango4j@gmail.com>

---------

Signed-off-by: Taejin Park <tango4j@gmail.com>
Co-authored-by: Jagadeesh Balam <4916480+jbalam-nv@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/decoder_timestamps_utils.py,2023-01-19T14:45:15Z,"[BugFix] decoder timestamp count has a mismatch when <unk> is decoded (#5825)

* [BugFix] decoder timestamp count has a mismatch when <unk> is decoded

Signed-off-by: Taejin Park <tango4j@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Style fix

Signed-off-by: Taejin Park <tango4j@gmail.com>

Signed-off-by: Taejin Park <tango4j@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/decoder_timestamps_utils.py,2022-11-29T04:24:23Z,"[BugFix] Removing <unk> tokens from decoding timestamp (#5481)

* Removing <unk> tokens from timestamp decoding

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Fixed notebook bug

Signed-off-by: Taejin Park <tango4j@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Taejin Park <tango4j@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/decoder_timestamps_utils.py,2022-11-12T01:33:06Z,"Add cpWER for evaluation of ASR with diarization (#5279)

* Add cpWER calculation feature

Signed-off-by: Taejin Park <tango4j@gmail.com>

* added notebook

Signed-off-by: Taejin Park <tango4j@gmail.com>

* updated notebook and diarization_utils

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Minor update on tutorial notebook

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Style fix

Signed-off-by: Taejin Park <tango4j@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Update on missing docstrings

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Fixed an unfinished docstring

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Removed unused variables

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Fixed dict input to list input

Signed-off-by: Taejin Park <tango4j@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Style fix

Signed-off-by: Taejin Park <tango4j@gmail.com>

* fixed LGTM issues

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Fixed error in cpWER cal

Signed-off-by: Taejin Park <tango4j@gmail.com>

* fixed docstrings

Signed-off-by: Taejin Park <tango4j@gmail.com>

* fixed docstrings

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Fix some of the typing issues, lower case names

Signed-off-by: SeanNaren <snarenthiran@nvidia.com>

* Replaced bruteforce with LSA alg for cpWER

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Reflected PR comments

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Cleaned notebook

Signed-off-by: Taejin Park <tango4j@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* updated notebook

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Fixed LGTM warnings

Signed-off-by: Taejin Park <tango4j@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* added test_diar_metrics.py

Signed-off-by: Taejin Park <tango4j@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fixed typos

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Fixed wrong type annotations

Signed-off-by: Taejin Park <tango4j@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Added bruteforce mode and its unit-test

Signed-off-by: Taejin Park <tango4j@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* LGTM issues fixed

Signed-off-by: Taejin Park <tango4j@gmail.com>

* reolve LGTM issues

Signed-off-by: Taejin Park <tango4j@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* unified speaker key in trans_dict

Signed-off-by: Taejin Park <tango4j@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Removed unused variable and imports

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Update nemo/collections/asr/parts/utils/diarization_utils.py

Co-authored-by: Sean Naren <snarenthiran@nvidia.com>
Signed-off-by: Taejin Park <tango4j@gmail.com>

* Update nemo/collections/asr/parts/utils/diarization_utils.py

Co-authored-by: Sean Naren <snarenthiran@nvidia.com>
Signed-off-by: Taejin Park <tango4j@gmail.com>

* moved all the diarization eval to der.py

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Update tests/collections/asr/test_diar_metrics.py

Co-authored-by: Sean Naren <snarenthiran@nvidia.com>
Signed-off-by: Taejin Park <tango4j@gmail.com>

* der.py update on tests

Signed-off-by: Taejin Park <tango4j@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* unused imports and style fix

Signed-off-by: Taejin Park <tango4j@gmail.com>

* style fix

Signed-off-by: Taejin Park <tango4j@gmail.com>

* unused import

Signed-off-by: Taejin Park <tango4j@gmail.com>

* reflected review comments

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Fixed an import bug in tutorial notebook

Signed-off-by: Taejin Park <tango4j@gmail.com>

Signed-off-by: Taejin Park <tango4j@gmail.com>
Signed-off-by: SeanNaren <snarenthiran@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: SeanNaren <snarenthiran@nvidia.com>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/decoder_timestamps_utils.py,2022-10-16T15:52:38Z,"Fix get_samples function  (#5141)

* Fixed get_samples in utils and tutorial

Signed-off-by: Taejin Park <tango4j@gmail.com>

* tutorial update

Signed-off-by: Taejin Park <tango4j@gmail.com>

* LGTM fix

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Reflected comments for PR. moved get_samples to audio_utils

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Style fix

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Updated get_samples function in tutorials

Signed-off-by: Taejin Park <tango4j@gmail.com>

Signed-off-by: Taejin Park <tango4j@gmail.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/decoder_timestamps_utils.py,2022-09-21T02:35:05Z,"reorder model check (#4959) (#4967)

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/decoder_timestamps_utils.py,2022-08-26T10:39:20Z,"Bug fix for Issue #4059, word time stamps for single word cases (#4095)"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/decoder_timestamps_utils.py,2022-06-14T17:26:26Z,"Add ASR CTC Decoding module (#4342)

* Initial commit

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Full support for decoding strategy

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Temp

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Fix labels of y_sequence

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Set support for sentencepiece subword merging

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Fix char and word based token merge alignment

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Revert incorrect change

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Update docstring

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Improve compatibility with greedy tokens and log probs

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Update scripts to use decoding strategy

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Add tests and docs

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Add tests and docs

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Fix speaker decoder timestamps

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Fix speaker decoder timestamps

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Fix decoding of ctc models

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Address reviewer comments

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Address reviewer comments

Signed-off-by: smajumdar <smajumdar@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/decoder_timestamps_utils.py,2022-04-14T01:33:57Z,"Torch conversion for VAD-Diarization pipeline (#3930)

* Initial file update

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Fixed torch jit related error

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Style fix update

Signed-off-by: Taejin Park <tango4j@gmail.com>

* revived torch.jit deco on NMESC class

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Fixed sentence output error in diar utils

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Fix code formatting error

Signed-off-by: Taejin Park <tango4j@gmail.com>

* encoding

Signed-off-by: fayejf <fayejf07@gmail.com>

* update and style fix

Signed-off-by: fayejf <fayejf07@gmail.com>

* Switched npath import position for CI pass

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Fixed conformer_ctc bug for diar_with_asr

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Fix errors in docstrings

Signed-off-by: Taejin Park <tango4j@gmail.com>

* typo fix and reflect TJ's comment

Signed-off-by: fayejf <fayejf07@gmail.com>

* style fix

Signed-off-by: fayejf <fayejf07@gmail.com>

* reflect nithin's comment

Signed-off-by: fayejf <fayejf07@gmail.com>

* Reflect review comments

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Minor fixes on docstrings

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Reduced unnecessary code lines

Signed-off-by: Taejin Park <tango4j@gmail.com>

* grammar

Signed-off-by: fayejf <fayejf07@gmail.com>

* reflected comments

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Filled in the missing docsterings and random trials

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Reflected Jagadesh's comments

Signed-off-by: Taejin Park <tango4j@gmail.com>

* style fix

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Added docstrings to specClus

Signed-off-by: Taejin Park <tango4j@gmail.com>

Co-authored-by: fayejf <fayejf07@gmail.com>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/decoder_timestamps_utils.py,2022-01-24T05:46:55Z,"Merge r1.6.0 main (#3500)

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* minor updates for finetuning (#3455)

Signed-off-by: Jason <jasoli@nvidia.com>

* Fix hysterisis loading (#3460)

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix the tutorial notebooks bug (#3465)

* fix checkpoint loading and model config file

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix style

Signed-off-by: Yi Dong <yidong@nvidia.com>

* Test only if the model was trained on single GPU for accurate results. (#3470)

* Test only if the model was trained on single GPU for accurate results.

Signed-off-by: smajumdar <titu1994@gmail.com>

* Test only if the model was trained on single GPU for accurate results.

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix the errors/bugs in ASR with diarization tutorial (#3461)

* Initial commit

Signed-off-by: Taejin Park <tango4j@gmail.com>

* fixed missing docstring

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Fixed main to r1.6.0

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Fixed pip install issues

Signed-off-by: Taejin Park <tango4j@gmail.com>

* NMT documentation for bottleneck architecture (#3464)

* 1. Updated NMT doc to include bottleneck architecture.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

Co-authored-by: Micha Livne <mlivne@nvidia.com>

* WFST Punct post fix + punct tutorial fixes (#3469)

* punct tutorial and wfst_post_process firx

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* format

Signed-off-by: ekmb <ebakhturina@nvidia.com>

Co-authored-by: PeganovAnton <peganoff2@mail.ru>
Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>

* Process correctly label ids dataset parameter + standardize type of label ids model attribute + minor changes (error messages, typing) (#3471)

* Fix label ids dictionary type

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix to_container method usage

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

Co-authored-by: ekmb <ebakhturina@nvidia.com>

* file name fix - Segmentation tutorial (#3474)

* update file name

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* update file name

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* Patch fix for the multiple last checkpoints issue (#3468)

* fix line

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* add TODO comment

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* upper bound ptl, lower bound numpy (#3466)

Signed-off-by: ericharper <complex451@gmail.com>

* fix bug with arguments for preprocessor (#3481)

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* Fix `punctuation_capitalization_train_evaluate.py` description (#3482)

* fix run script documentation

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add missing parameters to examples in documentation

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Standardize format of paths and file names in docs examples

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove wordtokenizer example from NLP tokenizer notebook (#3477)

* nb fix

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* keep token change for later

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* fix

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* Properly support -1 for labels in ctc char models (#3487)

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Vahid Noroozi <VahidooX@users.noreply.github.com>

* typo fix in diarization notebooks (#3480)

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* Add Apex import guard (#3467)

* add apex guard

Signed-off-by: ericharper <complex451@gmail.com>

* add import guard

Signed-off-by: ericharper <complex451@gmail.com>

* remove MegatronBertEncoder class

Signed-off-by: ericharper <complex451@gmail.com>

* update warning message when apex not found

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* remove import from init

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* Fix checkpoint converter in O2 style (#3486)

* Fix checkpoint converter in O2 style

Signed-off-by: Yu Yao <yuya@nvidia.com>

* Fix style

Signed-off-by: Yu Yao <yuya@nvidia.com>

Co-authored-by: Yu Yao <yuya@nvidia.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Jason <jasoli@nvidia.com>
Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Yi Dong <43824965+yidong72@users.noreply.github.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: Taejin Park <tango4j@gmail.com>
Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>
Co-authored-by: Micha Livne <mlivne@nvidia.com>
Co-authored-by: Evelina <10428420+ekmb@users.noreply.github.com>
Co-authored-by: PeganovAnton <peganoff2@mail.ru>
Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>
Co-authored-by: ekmb <ebakhturina@nvidia.com>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Co-authored-by: Oktai Tatanov <oktai.tatanov@gmail.com>
Co-authored-by: Abhinav Khattar <aklife97@gmail.com>
Co-authored-by: Vahid Noroozi <VahidooX@users.noreply.github.com>
Co-authored-by: yaoyu-33 <54727607+yaoyu-33@users.noreply.github.com>
Co-authored-by: Yu Yao <yuya@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/decoder_timestamps_utils.py,2022-01-13T22:11:33Z,"Update speaker diarization docs (#3419)

* Initial commit

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Fixed minor mistakes

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Some changes regarding diarization utils

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Fixed minor typos

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Reflected PR comments

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Reflected PR comments

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Reflected addtional comments

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Changed pics and refined text

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Minor typos

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Minor change on dataset

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Minor change on dataset 2

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Changed manifest input to yaml format

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Capitalization of titles

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Last commit

Signed-off-by: Taejin Park <tango4j@gmail.com>

Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>"
github.com/NVIDIA/NeMo,nemo/collections/asr/parts/utils/decoder_timestamps_utils.py,2022-01-10T20:30:26Z,"Updates on ASR with diarization util files (#3359)

* Initial commit

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Update LM part and multiscale part in README.

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Removed redundant parts

Signed-off-by: Taejin Park <tango4j@gmail.com>

* modified example script

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Revised doc strings

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Changed paths_to_manifest.py script

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Reflected PR comments and revised tutorials

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Added ASR models and kenlm installation 

Signed-off-by: tango4j@gmail.com

* Added ASR models and kenlm installation 

Signed-off-by: tango4j@gmail.com
Signed-off-by: Taejin Park <tango4j@gmail.com>

* Changed docstrings and style fix

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Fixed unused import and vars

Signed-off-by: Taejin Park <tango4j@gmail.com>

* Added LM part in ASR_diar tutorial.

Signed-off-by: Taejin Park <tango4j@gmail.com>

Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>"
github.com/NVIDIA/NeMo,scripts/asr_language_modeling/ngram_lm/eval_beamsearch_ngram_ctc.py,2023-10-19T17:04:06Z,"fix hybrid eval (#7759) (#7760)

* fix



* rename



* docs



* warning



* if



---------

Signed-off-by: Nikolay Karpov <nkarpov@nvidia.com>
Co-authored-by: Nikolay Karpov <karpnv@gmail.com>
Co-authored-by: Nikolay Karpov <nkarpov@nvidia.com>"
github.com/NVIDIA/NeMo,examples/nlp/duplex_text_normalization/data/create_tarred_dataset.py,2023-01-17T18:38:43Z,"add constraint info on batch size for tar dataset (#5812)

* add constraint info on batch size for tar dataset

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* style fix

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,examples/nlp/duplex_text_normalization/data/create_tarred_dataset.py,2023-01-11T21:12:36Z,"adding back tar script for decoder dataset for duplex (#5773)

* adding back tar script for decoder dataset for duplex

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2023-02-15T22:22:50Z,"[TTS/TN/G2P] Remove Text Processing from NeMo, move G2P to TTS (#5982)

* remove TN

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix imports

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix import

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add missing init

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix import

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* rename unit test

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix import

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix modules test

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix imports

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* remove whitelist from config

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* delete wordid file

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* remove pynini_install from tutorials

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* update requirements

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add support warning

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* review

Signed-off-by: ekmb <ebakhturina@nvidia.com>

---------

Signed-off-by: ekmb <ebakhturina@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2022-08-24T20:18:02Z,"Merge r1.11.0 main (#4787)

* NeMo Megatron doc updates

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* update package info and dockerfile

Signed-off-by: ericharper <complex451@gmail.com>

* fix fastpitch export (#4676)

Signed-off-by: Jason <jasoli@nvidia.com>

* [TTS] fixed wrong pronunciations for r1.11. (#4677)

* [TTS] fixed wrong pronunciations.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* incremented the version number to 22.08 as @blisc suggested.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* correct cmudict versions in world-wide places.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* Fix for incorrect batch size issue while decoding (#4675)

Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* [TTS] incremented the version number to 22.08 in tutorials. (#4684)

* [TTS] incremented the version number to 22.08 in tutorials.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* Megatron encode function with RPE fix (#4692)

* Fix for RPE

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* fix to fetch config file (#4699)

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* Fix notebook for buffered inference (#4703)

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Prompt Learning Notebook Bug Fix (#4689)

* Added back dataset class list of dict input for generation in tutorial notebook

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* updated argument name for build dataset

Signed-off-by: Virginia Adams <vadams@nvidia.com>

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* add psutils to mock imports (#4728)

Signed-off-by: ericharper <complex451@gmail.com>

Signed-off-by: ericharper <complex451@gmail.com>

* Update Aligner model and tutorial to add NGC checkpoint loading (#4714)

* Update Aligner model and tutorial to add NGC checkpoint loading

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Fix pynini install for Aligner notebook, minor formatting fix for model

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Aligner notebook formatting consistency

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* [TTS] bugfix for missing configs. (#4725)

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* docs typo fix

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* Fix pynini install in TTS tutorials (#4729)

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Fix ASR notebooks (#4738)

Signed-off-by: smajumdar <smajumdar@nvidia.com>

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Multilingual VAD model (#4734)

* add ngc link

Signed-off-by: fayejf <fayejf07@gmail.com>

* add tuned VAD config on ASR data

Signed-off-by: fayejf <fayejf07@gmail.com>

* yaml note

Signed-off-by: fayejf <fayejf07@gmail.com>

* update vad asr notebook with mVAD

Signed-off-by: fayejf <fayejf07@gmail.com>

* update vad infer config comment

Signed-off-by: fayejf <fayejf07@gmail.com>

* fix

Signed-off-by: fayejf <fayejf07@gmail.com>

* mvad sd config for ch109

Signed-off-by: fayejf <fayejf07@gmail.com>

* update sd readme

Signed-off-by: fayejf <fayejf07@gmail.com>

* add new mVAD model to doc

Signed-off-by: fayejf <fayejf07@gmail.com>

* style fix

Signed-off-by: fayejf <fayejf07@gmail.com>

* update sd tutorial with mVAD

Signed-off-by: fayejf <fayejf07@gmail.com>

* typo fix

Signed-off-by: fayejf <fayejf07@gmail.com>

Signed-off-by: fayejf <fayejf07@gmail.com>

* publish pretrained itn t5 model for English (#4748)

Signed-off-by: Alexandra Antonova <aleksandraa@nvidia.com>

Signed-off-by: Alexandra Antonova <aleksandraa@nvidia.com>
Co-authored-by: Alexandra Antonova <aleksandraa@nvidia.com>

* Updated docs and doc paths (#4754)

* Updated docs and doc paths

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Update Multitask_Prompt_and_PTuning.ipynb

* Update README.rst

* Changed branch name to use single quotes

Signed-off-by: Virginia Adams <vadams@nvidia.com>

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* fix bug relating to ddp strategy in joint intent slot classification tutorial (#4762)

* [TTS] updated config with a German IPA phoneme tokenizer (#4756)

* [TTS] added a German IPA phoneme tokenizer
* [TTS][ASR] enabled customized arguments for trimming the leading and trailing silence.
* [TTS] disabled spline interpolation for beta-binomial distribution. Let it generate align prior and save to disks. Use a new phoneme tokenizer.
* [TTS] use consistent spline interpolation with fastpitch checkpoint when generating mel-spectrograms for hifigan finetune.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* Update r1.11 to new heteronyms list (#4745)

* Update configs to new heteronyms list
* Remove old heteronyms list, add alt 'merchandise' pron to CMUdict
* Update remaining references to old heteronyms list

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* [TTS] Add multi-speaker German FastPitch and HiFiGAN NGC checkpoints (#4763)

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* [TTS] Add single male speaker German FastPitch and HiFiGAN NGC checkpoints (#4770)

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* Update CMUdict with more recent 0.7b entries (#4768)

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* Install pynini in docker container (#4733)

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* Fix tutorial formatting (#4778)

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* [TTS] deprecated old scripts for ljspeech. (#4780)

* deprecated old scripts for ljspeech.
* removed relevent function calls in TTS docs.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* update package info and requirements

Signed-off-by: ericharper <complex451@gmail.com>

* update container

Signed-off-by: ericharper <complex451@gmail.com>

* Update stragglers to new cmudict and heteronyms paths

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>
Signed-off-by: ericharper <complex451@gmail.com>
Signed-off-by: Jason <jasoli@nvidia.com>
Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>
Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>
Signed-off-by: smajumdar <smajumdar@nvidia.com>
Signed-off-by: Virginia Adams <vadams@nvidia.com>
Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>
Signed-off-by: fayejf <fayejf07@gmail.com>
Signed-off-by: Alexandra Antonova <aleksandraa@nvidia.com>
Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>
Co-authored-by: Jason <jasoli@nvidia.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Rajesh Ilango <rilango@gmail.com>
Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>
Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: Virginia Adams <78445382+vadam5@users.noreply.github.com>
Co-authored-by: Jocelyn <jocelynh@nvidia.com>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>
Co-authored-by: bene-ges <61418381+bene-ges@users.noreply.github.com>
Co-authored-by: Alexandra Antonova <aleksandraa@nvidia.com>
Co-authored-by: Zhilin Wang <wangzhilin12061996@hotmail.com>
Co-authored-by: Vladimir Bataev <vbataev@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2022-08-04T17:03:39Z,"Pynini dependency fix (#4674)

* nlp guard, logging removed

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* pynini conda install for mac, add import guard to TN tests

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* tts pynini dependency

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* add try/except block to tts models

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* add missing file

Signed-off-by: ekmb <ebakhturina@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2022-07-26T13:51:16Z,"fix tarred dataset len when num shards is not divisible by workers (#4553)

* fix tarred dataset len when num shards is not divisible by workers

Signed-off-by: Iztok Lebar Bajec <ilb@fri.uni-lj.si>

* update error reporting on invalid `shard_strategy`

* update NLP/PC tarred dataset docstring

* add `shard_strategy` to NLP/PC `@dataclass`

* update NLP/PC tarred dataset docstring

* add `shard_strategy` to NLP/PC docs

* revert test with Dataloader retruning the actual data length

* make dataloader return actual num of samples, set `limit_train_baches` on `setup_*`

* update `shard_strategy` docstrings

Signed-off-by: Iztok Lebar Bajec <ilb@fri.uni-lj.si>

* update `tarred_dataset` documentation

Signed-off-by: Iztok Lebar Bajec <ilb@fri.uni-lj.si>

* fix style

* update documentation

Signed-off-by: Iztok Lebar Bajec <ilb@fri.uni-lj.si>

* updated docstrings

Signed-off-by: Iztok Lebar Bajec <ilb@fri.uni-lj.si>

Co-authored-by: PeganovAnton <peganoff2@mail.ru>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2022-07-15T19:00:50Z,"added MLM Scoring (#4476)

* added MLM Scoring

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* fix header

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* refactor

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* fix bug that made normalization options set

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* fix style

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* fix discrepancy of space versus no space to previous version e.g. < sixteen > and <sixteen>

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* remove and from cardinal when lm is used to reduce number of options

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* fix grammar

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* fix masked input for [MASK] token before mlm scoring

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* mask out everything apart from one semiotic token

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* reverted masking change and added roman to lm

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* fix slash, expand measure

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix masked scoring

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* audio based set fix for --lm

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix bug

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* fix

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* added jenkins test

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* update jenkins

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* fix header

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* fix lgtm

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* add dependency

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* moved mlmscore file

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* moved hybrid to nemo_text_processing folder

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* update jenkins

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* fix path

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* fix test

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* fix dataset license

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

Co-authored-by: ekmb <ebakhturina@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2022-06-27T17:50:35Z,"Merge r1.10.0 main (#4448)

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* Fix ASR Typos in tutorials (#4384)

* Fix typos

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Quick wav2vec fix. In-place operation adding convolutional positions to encoder was overwriting leaf history. Wasn't caught on previous torch versions. (#4383)

Signed-off-by: tbartley94 <tbartley@nvidia.com>

Co-authored-by: tbartley94 <tbartley@nvidia.com>
(cherry picked from commit 0322b158f26a0b690edca7a84714e33752283923)

Co-authored-by: Travis Bartley <Travismbartley@gmail.com>

* Punctuation and capitalization tests race condition (#4399)

* Add draft of race condition fixes

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Minor improvements

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* More race condition fixes

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve error message

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve error message

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve error message

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix tutorial typos and docs (#4415)

* Fix typos

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Fix typos

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Add reconfigure on validation epoch start (#4393)

* Add reconfigure on validation epoch start

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove pdb

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* switch branch (#4424)

Signed-off-by: fayejf <fayejf07@gmail.com>

* Add ASR Scores to Docs (#4412)

* Fix link

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Correct model card

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Add ASR Results to Docs

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Update info

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Update info

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Re-apply fixes from r1.9.0 (#4425)

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Replace all with /content/ (#4427)

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Fix hanging issue by multiprocessing in SD tutorial and add ETA for VAD processing (#4405)

* cherry-pick pr 4317 and avoid signoff issue

Signed-off-by: fayejf <fayejf07@gmail.com>

* workaround for mp nb issue

Signed-off-by: fayejf <fayejf07@gmail.com>

* tdqm for mp functions in vad_utils

Signed-off-by: fayejf <fayejf07@gmail.com>

* style fix

Signed-off-by: fayejf <fayejf07@gmail.com>

* reflect comment

Signed-off-by: fayejf <fayejf07@gmail.com>

* remove

Signed-off-by: fayejf <fayejf07@gmail.com>

* [NLP] P&C Fix multi node cache issue, add pynini guard (#4410)

* add sleep to fix multi node cache issue, add pynini guard

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix lgtm

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* add tempfile

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* savfe tmp file to the same dir

Signed-off-by: ekmb <ebakhturina@nvidia.com>

Co-authored-by: PeganovAnton <peganoff2@mail.ru>

* fix the notebook (#4438)

Signed-off-by: Yi Dong <yidong@nvidia.com>

* update nemo version dialogue tutorial (#4437)

* docs: add table overflow handling for nested sections (#4441)

Co-authored-by: Nick Goncharenko <ngoncharenko@nvidia.com>

* Docs: Decrease Font Size on Tables  (#4444)

* docs: add table overflow handling for nested sections

* docs: set table font-size to small

Co-authored-by: Nick Goncharenko <ngoncharenko@nvidia.com>

* unify intent slot dataset util functions in tutorials (#4445)

* Notebook bug fix: add subfolder (#4442)

* add subfolder

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* exp_dir update

Signed-off-by: ekmb <ebakhturina@nvidia.com>

Co-authored-by: Eric Harper <complex451@gmail.com>

* Fix typo in HiFi-GAN config's max steps (#4446)

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

Co-authored-by: Eric Harper <complex451@gmail.com>

* Updated notebook to fix batch configuration and precision bugs (#4447)

* Updated notebook to fix batch configuration and precision bugs

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Deleted cell outputs

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Set datasets back to full dataset

Signed-off-by: Virginia Adams <vadams@nvidia.com>

Co-authored-by: Eric Harper <complex451@gmail.com>

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: Travis Bartley <Travismbartley@gmail.com>
Co-authored-by: PeganovAnton <peganoff2@mail.ru>
Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>
Co-authored-by: Jocelyn <jocelynh@nvidia.com>
Co-authored-by: Evelina <10428420+ekmb@users.noreply.github.com>
Co-authored-by: Yi Dong <43824965+yidong72@users.noreply.github.com>
Co-authored-by: Zhilin Wang <wangzhilin12061996@hotmail.com>
Co-authored-by: Nick Goncharenko <8766167+nickolyamba@users.noreply.github.com>
Co-authored-by: Nick Goncharenko <ngoncharenko@nvidia.com>
Co-authored-by: Virginia Adams <78445382+vadam5@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2022-06-08T17:41:12Z,"Tn install (#4055)

* remove conda pynini requirement

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* remove remnants

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* merge with main

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* removing nlp collection dependency from text processing and thus breaking cyclyc imports

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* fix

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* fix wrong requirement

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* fix bug in vi

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* update jenkins folders

Signed-off-by: ekmb <ebakhturina@nvidia.com>

Co-authored-by: Eric Harper <complex451@gmail.com>
Co-authored-by: ekmb <ebakhturina@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2022-04-20T22:46:30Z,"Merge r1.8.0 main (#4036)

* update version

Signed-off-by: ericharper <complex451@gmail.com>

* Stateless timer fix for PTL 1.6 (#3925)

* Stateless timer fix for PTL 1.6

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Stateless timer PTL test

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix year

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Remove unused imports

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* GPU test

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* clean import

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: ericharper <complex451@gmail.com>

* Fix issues with librosa deprecations (#3950)

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix notebook bugs for branch r1.8.0 (#3948)

* load the model from ngc

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix all biomegatron notebook

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix the typos

Signed-off-by: Yi Dong <doyend@gmail.com>

* remove output

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix isort

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix merge error

Signed-off-by: Yi Dong <doyend@gmail.com>

* change ntpath for isort workaround

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix unit test

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix ci

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix ci bert pretraining

Signed-off-by: Yi Dong <doyend@gmail.com>

* make it compatible with main

Signed-off-by: Yi Dong <doyend@gmail.com>

* add the teste for biomegatron ner

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix argument

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix usablity issue

Signed-off-by: Yi Dong <doyend@gmail.com>

* work around

Signed-off-by: Yi Dong <doyend@gmail.com>

Co-authored-by: Yi Dong <doyend@gmail.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* Fix global batch fit loop (#3936)

* add lightning module hooks for global batch

Signed-off-by: ericharper <complex451@gmail.com>

* clean scripts

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* DP=1 fix

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* set num dataset workers to 2

Signed-off-by: ericharper <complex451@gmail.com>

* update validation_loop with GlobalDataFetcher

Signed-off-by: ericharper <complex451@gmail.com>

* add test global data fetcher

Signed-off-by: ericharper <complex451@gmail.com>

* Drop last for test ds

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix test epoch end

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix eval

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix reconfigure microbatch in the complete method

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* add comments

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Set init consumed samples

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* fix shuffle

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* add save_restore_connector arg

Signed-off-by: ericharper <complex451@gmail.com>

* Fix padding for labels and loss mask

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* GLUE/XNLI CI tests

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* limit val batches in hydra fix

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Restart CI

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix unittest

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

Co-authored-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Exports 22.03 war (#3957)

* Fixed fastpitch for 22.03

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* cleanup

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Restored mask expansion; added WAR for test container images

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* style

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Refactor restorefrom (#3927)

* update package info (#3926)

Signed-off-by: ericharper <complex451@gmail.com>

* Refactor restore_from

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Move export related python files to scripts/export/

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Return state dict after modification function

* Remove Megatron legacy parameter in common.py restore_from function

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* ability to set log_predictions to false (#3929)

* Bumping Python version

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* fixing style

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* load the model from ngc

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix all biomegatron notebook

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix the typos

Signed-off-by: Yi Dong <doyend@gmail.com>

* remove output

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix isort

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix merge error

Signed-off-by: Yi Dong <doyend@gmail.com>

* change ntpath for isort workaround

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix unit test

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix ci

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix ci bert pretraining

Signed-off-by: Yi Dong <doyend@gmail.com>

* Rearrage export files; Style fix; Extend legacy MegatronBert conversion to NLP models nemo version updation

* Glu activation variants (#3951)

* Temp

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Add reglu and swiglu activations

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style on unrelated file

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* CI changes to test activations

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix unused import

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style fix beacuse of merge from main

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* make it compatible with main

Signed-off-by: Yi Dong <doyend@gmail.com>

* add the teste for biomegatron ner

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix argument

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix usablity issue

Signed-off-by: Yi Dong <doyend@gmail.com>

* FastPitch FT notebook - Improving Speech Quality clarifications (#3954)

* FastPitch FT notebook - Improving Speech Quality clarifications

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Add pynini dependency install to FastPitch FT notebook

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Pin pynini install for FastPitch FT tutorial

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* work around

Signed-off-by: Yi Dong <doyend@gmail.com>

Co-authored-by: Eric Harper <complex451@gmail.com>
Co-authored-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>
Co-authored-by: Dima Rekesh <bmwshop@gmail.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>
Co-authored-by: Yi Dong <doyend@gmail.com>
Co-authored-by: Yi Dong <43824965+yidong72@users.noreply.github.com>
Co-authored-by: Boris Fomitchev <borisfom@users.noreply.github.com>
Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Jocelyn <jocelynh@nvidia.com>

* Bump TTS deprecation version to 1.9 (#3955)

* bump deprecation version

Signed-off-by: Jason <jasoli@nvidia.com>

* update talknet depre

Signed-off-by: Jason <jasoli@nvidia.com>

* added conformer for zh. (#3970)

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* Add pinned pynini and scipy installs to TTS training tutorial (#3967)

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Fix variable name and move models to CPU in Change partition (#3972)

* fixes

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* add CI

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>

* fix misconfiguration (#3975)

Signed-off-by: Yi Dong <doyend@gmail.com>

Co-authored-by: Yi Dong <doyend@gmail.com>

* Fix NMT variable passing bug (#3985)

* fix

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* stylefix

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* Compatability override to load_state_dict for old TTS checkpoints (#3978)

* Compatability override to load_state_dict for old TTS checkpoints

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Tacotron2 training notebook fix - add GPU argument

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Add hann window override warning for old model loading

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Notebook Bug Fixes for r1.8.0 (#3989)

* Made config related bug fixes

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Fixed cfg.get syntax

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Fix compat override for TalkNet Aligner (#3993)

* Fix compatibility override for TalkNet Aligner

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* Remove extraneous logging import

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>

* docs fixes (#3987)

* docs fixes

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* rename files in docs

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* docs improvement

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* arg renamed

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* Fix nemo megatron restore with artifacts (#3997)

* update config_path in register_artifact

Signed-off-by: ericharper <complex451@gmail.com>

* fix register_artifact calls

Signed-off-by: ericharper <complex451@gmail.com>

* fix register_artifact calls

Signed-off-by: ericharper <complex451@gmail.com>

* update log messages to include merges file

Signed-off-by: ericharper <complex451@gmail.com>

* add default prompts to config

Signed-off-by: ericharper <complex451@gmail.com>

* Fixes val_check_interval, skip loading train data during eval (#3968)

* Change stage check

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix bugs in megatron t5 glue eval scripts

Signed-off-by: Yu Yao <yuya@nvidia.com>

* Fix reconfigure

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Change check

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix hasattr

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix typo in cfg structure

Signed-off-by: Yu Yao <yuya@nvidia.com>

* Update megatron t5 glue eval config file

Signed-off-by: Yu Yao <yuya@nvidia.com>

* Reconfigure to avoid drop last

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix for train step reconfigure as well

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Update megatron t5 glue eval config file drop_last to False

Signed-off-by: Yu Yao <yuya@nvidia.com>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* limit test batches

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

Co-authored-by: Yu Yao <54727607+yaoyu-33@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* LogProb calculation performance fix (#3984)

* performance fix for logprob computation

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix redandant assign

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix bug to add gather from TP workers

Signed-off-by: Yi Dong <doyend@gmail.com>

Co-authored-by: Yi Dong <doyend@gmail.com>

* Fix link issues in export example notebook and fix pretrained model info for MegatronBert (#4004)

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

Co-authored-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Fix single GPU training issue + change deprecated Lightning args (#4010)

* change vars

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* style fix

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* Fix P-Tune T5 model (#4001)

* fix ptune t5

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix ci test

Signed-off-by: Yi Dong <doyend@gmail.com>

* fix the ci fail because of the order problem

Signed-off-by: Yi Dong <doyend@gmail.com>

Co-authored-by: Yi Dong <doyend@gmail.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* Megatron work-arounds (#3998)

* WAR around Apex issue, and making sure output is FP32

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixing merge issues; moving dummy Trainer; adding float() casts

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixing ColumnParallelLinear call

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Cleanup

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Cleanup#2

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* fix the broadcast shape mismatch (#4017)

Signed-off-by: Yi Dong <doyend@gmail.com>

Co-authored-by: Yi Dong <doyend@gmail.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* add known issues (#4024)

Signed-off-by: ericharper <complex451@gmail.com>

* update readme with conda env setup instructions

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* update package info

Signed-off-by: ericharper <complex451@gmail.com>

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* update package info

Signed-off-by: ericharper <complex451@gmail.com>

* revert apex guard removal

Signed-off-by: ericharper <complex451@gmail.com>

* revert --language to --lang

Signed-off-by: ericharper <complex451@gmail.com>

* fix apex guard

Signed-off-by: ericharper <complex451@gmail.com>

* remove set_trace

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* fix apex guard

Signed-off-by: ericharper <complex451@gmail.com>

* remove unreachable statement

Signed-off-by: ericharper <complex451@gmail.com>

* remove duplicate lines

Signed-off-by: ericharper <complex451@gmail.com>

* remove duplicate lines

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: Yi Dong <43824965+yidong72@users.noreply.github.com>
Co-authored-by: Yi Dong <doyend@gmail.com>
Co-authored-by: Boris Fomitchev <borisfom@users.noreply.github.com>
Co-authored-by: Ramanathan Arunachalam <ramanathan.arun@rutgers.edu>
Co-authored-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>
Co-authored-by: Dima Rekesh <bmwshop@gmail.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>
Co-authored-by: Jocelyn <jocelynh@nvidia.com>
Co-authored-by: Jason <jasoli@nvidia.com>
Co-authored-by: Vahid Noroozi <VahidooX@users.noreply.github.com>
Co-authored-by: Abhinav Khattar <aklife97@gmail.com>
Co-authored-by: Virginia Adams <78445382+vadam5@users.noreply.github.com>
Co-authored-by: Evelina <10428420+ekmb@users.noreply.github.com>
Co-authored-by: Yu Yao <54727607+yaoyu-33@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2022-03-30T01:20:18Z,"Megatron support (#3893)

* Add MegatronBert training support for more NLP models; Replace check for nemo_file existence to decide if it's megatron training or not; Add support for finetuning a downstream NLP task model on a different downstream dataset using MegatronBert; Add skelton support for ONNX export of MegatronBert

* Remove duplicate lm_checkpoint key in config

* Adding new Apex classes for export replacement and default trainer support

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixing order of exported inputs in NLP models

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixed typo

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixed ORT check

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Refactor of NLP models initialization

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixed style

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixed runtime check

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Code style fixes

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Code style fixes

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Code style fixes

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Check for existence of downstream key before setting it

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Fix Huggingface download unit tests based on get_lm_model refactor

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Add register artifact for MegatronBert models; Remove unused import

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Fixed style

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixed duplex decoder init

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* moved set_world_size up

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixing _trainer initialization

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixed GPT tokenizer init

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fix Token classification and Q&A forward flag checks for MegatronBert

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Fix world_size init

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Skip GPT eval test

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Fix get_lm_model function calls based on recent refactoring

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Check for presence of keys in the Dict before trying to access them in BERTLMModel

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Check if tokenizer is present before accesing it in lm_utils

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Check for presence of keys in the Dict before trying to register them as artifacts

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Bypass NLP init statements pertaining to MegatronBert when using Ptune downstream task

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Revert NLPModel modification

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Skip the Jenkins Test Megatron P-Tuning GPT LM

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

* Merge branch 'main' into megatron_support

Signed-off-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>

Co-authored-by: Ramanathan Arunachalam <rarunachalam@nvidia.com>
Co-authored-by: Boris Fomitchev <bfomitchev@nvidia.com>
Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Co-authored-by: Boris Fomitchev <borisfom@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2021-11-05T18:24:47Z,"update english tn ckpt (#3143)

* update english tn ckpt

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* remove ununsed import

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2021-11-04T18:48:26Z,"Tn add nn wfst and doc (#3135)

* made tagger exportable

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* added whitelist wfst for nn

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* updated documentation

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* remove experimental

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* updated doc

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* made tagger exportable

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* added whitelist wfst for nn

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* updated documentation

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* remove experimental

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* updated doc

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* preserve punct after nn wfst

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2021-11-02T17:26:17Z,"Tn clean upsample (#3024)

* init

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* renamed file

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* adding all cleaning scripts

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* skip sentence if error

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* remove I-SAME

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* fix tyle

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* remove I the first from training

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* remove DM and Da from upsampling

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* remove I -> one/first, also add space around dash for alphanumerical context, remove rare currency from being upsampled

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* remove dalton and DM from being verbalized

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* remove Da and DM sentences competely

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* addressed review feedback, added data folder in examples

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* refactored code, added data utils functions

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* fix lgtm

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* fix lgtm

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* added electronic wfst for english neural TN

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* header and lgtm

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2021-10-20T21:51:23Z,"delete nltk download for TN (#3028)

* delete nltk

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* style fix

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* remove unused import

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2021-10-11T22:56:25Z,"TN updates (#2983)

* moses added

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* updates to make eval with moses work

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* clean up

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* clean up

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix init

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* review

Signed-off-by: ekmb <ebakhturina@nvidia.com>

Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2021-10-07T18:30:27Z,"reading max_sequence_len parameter from config fixed (#2961)

Signed-off-by: Fedor Streltsov <sfeaal@gmail.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2021-09-30T22:41:04Z,"TN infer  (#2929)

* en_small grammars added

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* infer fix

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* add whitelist arg

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* add input fall back

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* docstring

Signed-off-by: ekmb <ebakhturina@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2021-09-22T20:44:24Z,"TN/ITN update (#2854)

* from file added for all modes

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* directions map

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* decoder eval

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* separate eval and inference added

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* handle unk tokens and proper pre-post processing

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* clean up

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* review feedback

Signed-off-by: ekmb <ebakhturina@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2021-09-16T17:33:56Z,"tar dataset for TN/ITN (#2826)

* tar dataset added

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* typo and ci test

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* jenkins format

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* jenkins

Signed-off-by: ekmb <ebakhturina@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2021-08-30T16:58:16Z,"num_workers set to 3 (#2748)

Signed-off-by: ekmb <ebakhturina@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2021-08-25T00:17:21Z,"TN Duplex model update (#2704)

* wip

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* wip

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* wip

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* wip

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* evaluation during training added

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* evaluation during training added

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* add multiclass eval and avg metric

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* make compatible with old checkpoints

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* clean up

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix format

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix format

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* update

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* add avg to log

Signed-off-by: ekmb <ebakhturina@nvidia.com>

Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2021-08-24T22:21:59Z,"Merge 1.3 bugfixes into main (#2715)

* update jenkins branch

Signed-off-by: ericharper <complex451@gmail.com>

* update notebooks branch

Signed-off-by: ericharper <complex451@gmail.com>

* update package info

Signed-off-by: ericharper <complex451@gmail.com>

* update readme

Signed-off-by: ericharper <complex451@gmail.com>

* update nemo version for Dockerfile

Signed-off-by: ericharper <complex451@gmail.com>

* update notebook branch

Signed-off-by: ericharper <complex451@gmail.com>

* Update colab links to Transducer notebooks (#2654)

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix nmt grpc server, concatdataset for raw text files (#2656)

* Fix nmt grpc server and concatdataset for raw text files

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Check if lang direction is provided correctly

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style fixes

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* add missing init (#2662)

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix qa inference for single example (#2668)

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* Fix max symbol per step updating for RNNT (#2672)

* Fix max symbol per step updating for RNNT

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix notebooks

Signed-off-by: smajumdar <titu1994@gmail.com>

* Replaced unfold() with split_view() (#2671)

* Replaced unfold() with split_view()

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* fixed typo

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>

* Correct voice app demo (#2682)

Signed-off-by: smajumdar <titu1994@gmail.com>

* Import guard (#2692)

* add asr and pynini import guard

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* remove asrmodel type

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* remove asrmodel type

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fixing branch (#2695)

Signed-off-by: Ghasem Pasandi <gpasandi@nvidia.com>

Co-authored-by: Ghasem Pasandi <gpasandi@nvidia.com>

* fix for emojis (#2675)

* fix for emojis

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* remove redundant line

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* raise error

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* use app_state

Signed-off-by: ekmb <ebakhturina@nvidia.com>

Co-authored-by: Eric Harper <complex451@gmail.com>

* Fix issues with ASR notebooks (#2698)

Signed-off-by: smajumdar <titu1994@gmail.com>

* Allow non divisible split_size (#2699)

* bugfix

Signed-off-by: Jason <jasoli@nvidia.com>

* bugfix

Signed-off-by: Jason <jasoli@nvidia.com>

* TN fix for corner cases (#2689)

* serial added, weights to common defaults, decimal bug fix

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* one failing

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* all tests pass

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* remove redundant file

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix telephone, add test cases

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* money fix

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* clean format

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix edge case of greedy decoding for greedy_batch mode (#2701)

Signed-off-by: smajumdar <titu1994@gmail.com>

* Remove time macro (#2703)

Signed-off-by: smajumdar <titu1994@gmail.com>

* Minor FastPitch Fixes (#2697)

* fixes

Signed-off-by: Jason <jasoli@nvidia.com>

* update CI

Signed-off-by: Jason <jasoli@nvidia.com>

* refix

Signed-off-by: Jason <jasoli@nvidia.com>

* Fix ddp error. (#2678)

To avoid ""MisconfigurationException: Selected distributed backend ddp is not compatible with an interactive environment."" error.

Co-authored-by: ekmb <ebakhturina@nvidia.com>

* update jenkins

Signed-off-by: ericharper <complex451@gmail.com>

* update notebooks

Signed-off-by: ericharper <complex451@gmail.com>

* add split_view back

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Evelina <10428420+ekmb@users.noreply.github.com>
Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>
Co-authored-by: Boris Fomitchev <borisfom@users.noreply.github.com>
Co-authored-by: Ghasem <35242805+pasandi20@users.noreply.github.com>
Co-authored-by: Ghasem Pasandi <gpasandi@nvidia.com>
Co-authored-by: Jason <jasoli@nvidia.com>
Co-authored-by: khcs <khcs@users.noreply.github.com>
Co-authored-by: ekmb <ebakhturina@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2021-08-07T23:01:32Z,"Evaluate the performance of the decoder for each semiotic class (#2625)

* Add class_based_decoding_evaluation.py
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Remove unused imports
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Add evaluation for tagger to class_based_eval
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2021-08-07T22:19:57Z,"Allow using covering grammars for neural English TN model (#2602)

* Compute probability of each sequence
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Use CGs when the model is not confident
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Add script for visualization
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Add comments on how to generate visualizations
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Remove spaces in URLs when using CGs
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Allow setting n_tagged
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Add docstring
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* if there is any exception, fall back to the input
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Minor changes to URL processing
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Style fix
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Add docstrings and comments
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* PYNINI_AVAILABLE check
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2021-08-06T19:34:52Z,"Allowed setting the train set size of duplex TN training (#2605)

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2021-08-03T19:03:14Z,"import fix (#2597)

* import fix

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* remove unused import

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* example import fix

Signed-off-by: ekmb <ebakhturina@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2021-07-30T21:52:58Z,"Fixes for neural TN (#2581)

* Preprocessed Google TN data not need basic tokenization
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Minor Fix
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Error fix
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Added test after train
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Minor Fix
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Allow data caching for Tagger Dataset
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Allow data caching for Decoder dataset
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Minor fixes
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2021-07-21T16:22:53Z,"Extending the neural TN/ITN models for other languages (#2497)

* extending the neural TN/ITN model to handle RU

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Support German
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Catch AttributeError instead of BaseException
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Style fix
Signed-off-by: Tuan Lai <tuanl@nvidia.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2021-07-08T05:26:37Z,"Transformer-based Text Normalization Models (#2415)

* Add notebook with recommendations for 8 kHz speech (#2326)

* Added a notebook with best practices for telephony speech

* Added datasets detaiils

* Added training recommendations

* Emptied out cells with results

* Added tutorial to docs

Signed-off-by: jbalam <jbalam@nvidia.com>

* Addressed review comments

Signed-off-by: jbalam <jbalam@nvidia.com>

* Added a line to note original sampling rate of an4

Signed-off-by: jbalam <jbalam@nvidia.com>

* Made changes suggested in review

Signed-off-by: jbalam <jbalam@nvidia.com>
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Add FastEmit support for RNNT Losses (#2374)

* Temp commit

Signed-off-by: smajumdar <titu1994@gmail.com>

* Initial code for fastemit forward pass

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct return reg value

Signed-off-by: smajumdar <titu1994@gmail.com>

* Initial cpu impl

Signed-off-by: smajumdar <titu1994@gmail.com>

* Try gpu impl

Signed-off-by: smajumdar <titu1994@gmail.com>

* Try gpu impl

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct few impl

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update fastemit scaling

Signed-off-by: smajumdar <titu1994@gmail.com>

* Cleanup fastemit

Signed-off-by: smajumdar <titu1994@gmail.com>

* Finalize FastEmit regularization PR

Signed-off-by: smajumdar <titu1994@gmail.com>

* Refactor code to support fastemit regularization

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Samuel Kriman <samuelkriman@gmail.com>
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Implement inference functions of TN models

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Minor Fix

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* fix bugs in hifigan code (#2392)

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Update setup.py (#2394)

Signed-off-by: Jason <jasoli@nvidia.com>
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* update checkpointing (#2396)

Signed-off-by: Jason <jasoli@nvidia.com>
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* byt5 unicode implementation (#2365)

* Audio Norm (#2285)

* add jenkins test, refactoring

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* update test

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix new test

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* add serial to the default normalizer, add tests

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* manifest test added

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* expose more params, new test cases

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix jenkins, serial clean, exclude range from cardinal

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* jenkins

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* jenkins dollar sign format

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* jenkins

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* jenkins dollar sign format

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* addressed review comments

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix decimal in measure

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* move serial in cardinal

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* clean up

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* update for SH zero -> oh

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* change n_tagger default

Signed-off-by: ekmb <ebakhturina@nvidia.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* bumping version to 1.0.1

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* Add check for numba regardless of device

Signed-off-by: smajumdar <titu1994@gmail.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* upper bound for webdataset

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* Correct Dockerfile

Signed-off-by: smajumdar <titu1994@gmail.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* update readmes

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* update README (#2332)

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* ddp translate GPU allocation fix (#2312)

* fixed branch in IR tutorial

Signed-off-by: AlexGrinch <grinchuk.alexey@gmail.com>

* ddp translate GPU allocation fix

Signed-off-by: AlexGrinch <grinchuk.alexey@gmail.com>

* map_location instead of set_device

Signed-off-by: AlexGrinch <grinchuk.alexey@gmail.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* Shallow fusion (#2315)

* fixed branch in IR tutorial

Signed-off-by: AlexGrinch <grinchuk.alexey@gmail.com>

* shallow fusion init commit

Signed-off-by: AlexGrinch <grinchuk.alexey@gmail.com>

* debug info removed

Signed-off-by: AlexGrinch <grinchuk.alexey@gmail.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* [BUGFIX] Add upper bound to hydra for 1.0.x (#2337)

* upper bound hydra

Signed-off-by: ericharper <complex451@gmail.com>

* upper bound hydra

Signed-off-by: ericharper <complex451@gmail.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* update version number

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* update package version

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* sparrowhawk tests + punctuation post processing for pynini TN (#2320)

* add jenkins test, refactoring

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* update test

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix new test

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* add serial to the default normalizer, add tests

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* manifest test added

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* expose more params, new test cases

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix jenkins, serial clean, exclude range from cardinal

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* jenkins

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* jenkins dollar sign format

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* jenkins

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* jenkins dollar sign format

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* addressed review comments

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix decimal in measure

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* move serial in cardinal

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* sh tests init

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* sparrowhawk container tests support added

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* add post process to normalize.py, update tests

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* remove duplication

Signed-off-by: ekmb <ebakhturina@nvidia.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* Update notebooks to 1.0.2 release (#2338)

Signed-off-by: smajumdar <titu1994@gmail.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* Update ranges for omegaconf and hydra (#2336)

* Update ranges

Signed-off-by: smajumdar <titu1994@gmail.com>

* Updates for Hydra and OmegaConf updates

Signed-off-by: smajumdar <titu1994@gmail.com>

* Style fixes

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct tests and revert patch for model utils

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct docstring

Signed-off-by: smajumdar <titu1994@gmail.com>

* Revert unnecessary change

Signed-off-by: smajumdar <titu1994@gmail.com>

* Revert unnecessary change

Signed-off-by: smajumdar <titu1994@gmail.com>

* Guard scheduler for None

Signed-off-by: smajumdar <titu1994@gmail.com>

* default to 0.0 if bpe_dropout is None

Signed-off-by: ericharper <complex451@gmail.com>

* Correctly log class that was restored

Signed-off-by: smajumdar <titu1994@gmail.com>

* Root patch *bpe_dropout

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: ericharper <complex451@gmail.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* Update FastPitch Export (#2355)

Signed-off-by: Jason <jasoli@nvidia.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* byt5 unicode implementation, first cut

Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* add bytelevel tokenizer

Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* update out_dir to not collide (#2358)

Signed-off-by: ericharper <complex451@gmail.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* Update container version to 21.05 (#2309)

* Update container version

Signed-off-by: smajumdar <titu1994@gmail.com>

* Temporarily change export format of waveglow

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add conda update for numba

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update numba compat via global flag for strictness level `--relax_numba_compat`, remove pytorchlightning.metrics, refactor out numba utils to core, update tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct order of numba minimum verion, remove wrong flag from test

Signed-off-by: smajumdar <titu1994@gmail.com>

* Double test of cuda numba

Signed-off-by: smajumdar <titu1994@gmail.com>

* Double test of cuda numba

Signed-off-by: smajumdar <titu1994@gmail.com>

* Enable RNNT tests

Signed-off-by: smajumdar <titu1994@gmail.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* Text Normalization Update (#2356)

* upper cased date support

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* update whitelist, change roman weights

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* docstrings, space fix, init file

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* lgtm

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fraction with measure class

Signed-off-by: ekmb <ebakhturina@nvidia.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* address comment

Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* Add ASR CTC tutorial on fine-tuning on another language (#2346)

* Add ASR CTC Language finetuning notebook

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add to documentation

Signed-off-by: smajumdar <titu1994@gmail.com>

* Improve documentation

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct name of the dataset

Signed-off-by: smajumdar <titu1994@gmail.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* Correct colab link to notebook (#2366)

Signed-off-by: smajumdar <titu1994@gmail.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* sgdqa update data directories for testing (#2323)

* sgdqa update data directories for testing

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* fix syntax

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* check if data dir exists

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* fix

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* adding pretrained model

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* Added documentation for export() (#2330)

* Added export document

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Addressed review comments

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

Co-authored-by: Eric Harper <complex451@gmail.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* Update Citrinet model card info (#2369)

* Update model card info

Signed-off-by: smajumdar <titu1994@gmail.com>

* Cleanup Docs

Signed-off-by: smajumdar <titu1994@gmail.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* [NMT] Model Parallel Megatron Encoders (#2238)

* add megatron encoder

Signed-off-by: ericharper <complex451@gmail.com>

* added megatron to get_nmt_tokenizer

Signed-off-by: ericharper <complex451@gmail.com>

* add vocab_size and hidden_size to megatron bert

Signed-off-by: ericharper <complex451@gmail.com>

* add megatron encoder module

Signed-off-by: ericharper <complex451@gmail.com>

* fixed horrible typo

Signed-off-by: ericharper <complex451@gmail.com>

* fix typo and add default

Signed-off-by: ericharper <complex451@gmail.com>

* updating nlp overrides for mp nmt

Signed-off-by: ericharper <complex451@gmail.com>

* move some logic back to nlpmodel from overrides

Signed-off-by: ericharper <complex451@gmail.com>

* add checkpoint_file property

Signed-off-by: ericharper <complex451@gmail.com>

* fix property

Signed-off-by: ericharper <complex451@gmail.com>

* num_tokentypes=0

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* find_unused_parameters=True

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* get instead of pop

Signed-off-by: ericharper <complex451@gmail.com>

* remove token type ids from megatron input example

Signed-off-by: ericharper <complex451@gmail.com>

* pop vocab_size

Signed-off-by: ericharper <complex451@gmail.com>

* fix checkpointing for model parallel

Signed-off-by: ericharper <complex451@gmail.com>

* fix bug in non model parallel

Signed-off-by: ericharper <complex451@gmail.com>

* convert cfg.trainer to dict

Signed-off-by: ericharper <complex451@gmail.com>

* make num_tokentypes configurable for nmt

Signed-off-by: ericharper <complex451@gmail.com>

* update checkpoint_file when using named megatron model in nemo

Signed-off-by: ericharper <complex451@gmail.com>

* make vocab_file configurable

Signed-off-by: ericharper <complex451@gmail.com>

* dataclass can't have mutable default

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* unused imports

Signed-off-by: ericharper <complex451@gmail.com>

* revert input example

Signed-off-by: ericharper <complex451@gmail.com>

* check that checkpoint version is not None

Signed-off-by: ericharper <complex451@gmail.com>

* add mp jenkins test

Signed-off-by: ericharper <complex451@gmail.com>

* update docstring

Signed-off-by: ericharper <complex451@gmail.com>

* add docs for pretrained encoders with nemo nmt

Signed-off-by: ericharper <complex451@gmail.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* Add notebook with recommendations for 8 kHz speech (#2326)

* Added a notebook with best practices for telephony speech

* Added datasets detaiils

* Added training recommendations

* Emptied out cells with results

* Added tutorial to docs

Signed-off-by: jbalam <jbalam@nvidia.com>

* Addressed review comments

Signed-off-by: jbalam <jbalam@nvidia.com>

* Added a line to note original sampling rate of an4

Signed-off-by: jbalam <jbalam@nvidia.com>

* Made changes suggested in review

Signed-off-by: jbalam <jbalam@nvidia.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* Add FastEmit support for RNNT Losses (#2374)

* Temp commit

Signed-off-by: smajumdar <titu1994@gmail.com>

* Initial code for fastemit forward pass

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct return reg value

Signed-off-by: smajumdar <titu1994@gmail.com>

* Initial cpu impl

Signed-off-by: smajumdar <titu1994@gmail.com>

* Try gpu impl

Signed-off-by: smajumdar <titu1994@gmail.com>

* Try gpu impl

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct few impl

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update fastemit scaling

Signed-off-by: smajumdar <titu1994@gmail.com>

* Cleanup fastemit

Signed-off-by: smajumdar <titu1994@gmail.com>

* Finalize FastEmit regularization PR

Signed-off-by: smajumdar <titu1994@gmail.com>

* Refactor code to support fastemit regularization

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Samuel Kriman <samuelkriman@gmail.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* byt5 unicode implementation, first cut

Signed-off-by: Mike Chrzanowski <mchrzanowski@nvidia.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* add bytelevel tokenizer

Signed-off-by: Mike Chrzanowski <mchrzanowski@nvidia.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* update styling

Signed-off-by: Mike Chrzanowski <mchrzanowski@nvidia.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* avoid circular import

Signed-off-by: Mike Chrzanowski <mchrzanowski@nvidia.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* fix bugs in hifigan code (#2392)

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* Update setup.py (#2394)

Signed-off-by: Jason <jasoli@nvidia.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* Update bytelevel_tokenizer.py

Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* Update bytelevel_tokenizer.py

Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* typo

Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* missed one

Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* bug fixes

Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* style fix

Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* bytelevelprocessor is now generic.

Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* style fix

Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* update checkpointing (#2396)

Signed-off-by: Jason <jasoli@nvidia.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>
Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* woops, didnt merge jenkinsfile the right way

* add newline

Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* undo changes to enja processor

Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* processor selection decision fix

Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

* newline fix

Signed-off-by: mchrzanowski <mchrzanowski@nvidia.com>

Co-authored-by: Evelina <10428420+ekmb@users.noreply.github.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Aleksey Grinchuk (Oleksii Hrinchuk) <grinchuk.alexey@gmail.com>
Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Eric Harper <complex451@gmail.com>
Co-authored-by: Jason <jasoli@nvidia.com>
Co-authored-by: mchrzanowski <mchrzanowski@nvidia.com>
Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>
Co-authored-by: Boris Fomitchev <borisfom@users.noreply.github.com>
Co-authored-by: Jagadeesh Balam <4916480+jbalam-nv@users.noreply.github.com>
Co-authored-by: Samuel Kriman <samuelkriman@gmail.com>
Co-authored-by: Oktai Tatanov <oktai.tatanov@gmail.com>
Co-authored-by: root <root@dgx0026.nsv.rno1.nvmetal.net>
Co-authored-by: root <root@dgx0079.nsv.rno1.nvmetal.net>
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Minor Fix

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Minor Fixes

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Add TextNormalizationTestDataset and testing/evaluation code

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Add TextNormalizationTaggerDataset and training code for tagger

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Restore from local nemo ckpts

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Add TextNormalizationDecoderDataset

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Add interactive mode for neural_text_normalization_test.py

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Add options to do training or not for tagger/decoder

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Renamed

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Implemented setup dataloader for decoder

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Implemented training and validation for decoder

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Data augmentation for decoder training

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Config change

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* add blossom-ci.yml (#2401)

Signed-off-by: ericharper <complex451@gmail.com>
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Merge r1.1 bugfixes into main (#2407)

* Update notebook branch and Jenkinsfile for 1.1.0 testing (#2378)

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* update jenkinsfile

Signed-off-by: ericharper <complex451@gmail.com>

* [BUGFIX] NMT Multi-node was incorrectly computing num_replicas (#2380)

* fix property when not using model parallel

Signed-off-by: ericharper <complex451@gmail.com>

* fix property when not using model parallel

Signed-off-by: ericharper <complex451@gmail.com>

* add debug statement

Signed-off-by: ericharper <complex451@gmail.com>

* add debug statement

Signed-off-by: ericharper <complex451@gmail.com>

* instantiate with NLPDDPPlugin with num_nodes from trainer config

Signed-off-by: ericharper <complex451@gmail.com>

* Update ASR scripts for tokenizer building and tarred dataset building (#2381)

* Update ASR scripts for tokenizer building and tarred dataset building

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update container

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add STT Zh Citrinet 1024 Gamma 0.25 model

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update notebook (#2391)

Signed-off-by: smajumdar <titu1994@gmail.com>

* ASR Notebooks fix for 1.1.0 (#2395)

* nb fix for spring clean

Signed-off-by: fayejf <fayejf07@gmail.com>

* remove outdated instruction

Signed-off-by: fayejf <fayejf07@gmail.com>

* Mean normalization (#2397)

* norm embeddings

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* move to utils

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* Bugfix adaptive spec augment time masking (#2398)

* bugfix adaptive spec augment

Signed-off-by: smajumdar <titu1994@gmail.com>

* Revert freq mask guard

Signed-off-by: smajumdar <titu1994@gmail.com>

* Revert freq mask guard

Signed-off-by: smajumdar <titu1994@gmail.com>

* Remove static time width clamping

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct typos and issues with notebooks (#2402)

* Fix Primer notebook

Signed-off-by: smajumdar <titu1994@gmail.com>

* Typo

Signed-off-by: smajumdar <titu1994@gmail.com>

* remove accelerator=DDP in tutorial notebooks to avoid errors. (#2403)

Signed-off-by: Hoo Chang Shin <hshin@nvidia.com>

Co-authored-by: Hoo Chang Shin <hshin@nvidia.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* update jenkins branch

Signed-off-by: ericharper <complex451@gmail.com>

* update notebook branch to main

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Co-authored-by: khcs <khcs@users.noreply.github.com>
Co-authored-by: Hoo Chang Shin <hshin@nvidia.com>
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Remove unused imports

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Add initial doc for text_normalization

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Fixed imports warnings

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Minor Fix

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Renamed

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Allowed duplex modes

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Minor Fix

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Add docs for duplex_text_normalization_train and duplex_text_normalization_test

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* docstrings for model codes + minor fix

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Add more comments and doc strings

Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Add doc for datasets + Use time.perf_counter()
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Add code for preprocessing Google TN data
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Add more docs and comments + Minor Fixes
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Add more licenses + Fixed comments + Minors
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Moved evaluation logic to DuplexTextNormalizationModel
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Add logging errors
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Updated validation code of tagger + Minors
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Also write tag preds to log file
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Add data augmentation for tagger dataset
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Added experimental decorators
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Updated docs
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Updated duplex_tn_config.yaml
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Compute token precision of tagger using NeMo metrics
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Fixed saving issue when using ddp accelerator
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Refactoring
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Add option to keep punctuations in TextNormalizationTestDataset
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Changes to input preprocessing + decoder's postprocessing
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Fixed styles + Add references
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

* Renamed examples/nlp/duplex_text_normalization/utils.py to helpers.py
Signed-off-by: Tuan Lai <tuanl@nvidia.com>

Co-authored-by: Jagadeesh Balam <4916480+jbalam-nv@users.noreply.github.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: Samuel Kriman <samuelkriman@gmail.com>
Co-authored-by: Oktai Tatanov <oktai.tatanov@gmail.com>
Co-authored-by: Jason <jasoli@nvidia.com>
Co-authored-by: Mike Chrzanowski <mike.chrzanowski0@gmail.com>
Co-authored-by: Evelina <10428420+ekmb@users.noreply.github.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Aleksey Grinchuk (Oleksii Hrinchuk) <grinchuk.alexey@gmail.com>
Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Eric Harper <complex451@gmail.com>
Co-authored-by: mchrzanowski <mchrzanowski@nvidia.com>
Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>
Co-authored-by: Boris Fomitchev <borisfom@users.noreply.github.com>
Co-authored-by: root <root@dgx0026.nsv.rno1.nvmetal.net>
Co-authored-by: root <root@dgx0079.nsv.rno1.nvmetal.net>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Co-authored-by: khcs <khcs@users.noreply.github.com>
Co-authored-by: Hoo Chang Shin <hshin@nvidia.com>"
github.com/NVIDIA/NeMo,scripts/asr_context_biasing/eval_greedy_decoding_with_context_biasing.py,2024-02-13T12:05:08Z,"Context-biasing by CTC-based Word Spotter (CTC-WS) (#8223)

* initial commit

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* some fixes

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* fix blank_idx slow down

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add new non-blank pruning

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* some fixes

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* some fixes

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* descriptions fix

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* description fixes

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add ctc only model

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* move scripts to nemo asr parts

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* remove scripts from scripts dir

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add first test

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add some tests

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add test

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* some fixes

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* fixes

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fixes

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix circular import

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix preds_output_folder

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* set loop_lables=True

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add .json to output manifest name

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* fix rnnt wer degradation

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add round(score) for test

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix bow token

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* review fixes

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* review fixes

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add epsilon shift in alignment

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* minor fix

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* fix transcribe modification

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix autocast

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

---------

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,scripts/asr_language_modeling/ngram_lm/eval_beamsearch_ngram_transducer.py,2023-10-05T17:18:32Z,"Fix py3.11 dataclasses issue  (#7616)

* Fix py3.11 dataclasses issue  (#7582)

* Update ASR configs to support Python 3.11

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update TTS configs to support Python 3.11

Signed-off-by: smajumdar <titu1994@gmail.com>

* Guard MeCab and Ipadic

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix remaining ASR dataclasses

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix remaining ASR dataclasses

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix scripts

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Update name to ConfidenceMethodConfig

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain (#7576) (#7586)

* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain
* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Safeguard nemo_text_processing installation on ARM (#7485)

* safeguard nemo_text_processing installing

Signed-off-by: Jason <jasoli@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update check

Signed-off-by: Jason <jasoli@nvidia.com>

---------

Signed-off-by: Jason <jasoli@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Fix changes to confidence measure

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: smajumdar <titu1994@gmail.com>
Signed-off-by: Sangkug Lym <slym@nvidia.com>
Signed-off-by: Jason <jasoli@nvidia.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>
Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: Jason <jasoli@nvidia.com>"
github.com/NVIDIA/NeMo,scripts/asr_language_modeling/ngram_lm/eval_beamsearch_ngram_transducer.py,2023-04-21T21:44:03Z,"Update script for ngram rnnt and hat beam search decoding (#6370)

* add rnnt ngram beamsearch script

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add return encoding embedding option

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* update script

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add rnnt and hat ngram decoding script

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add some parameters

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add return_encoder_embeddings parameter to RNNTDecodingConfig

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* replace return_encoder_embeddings parameter

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* generalization of scipt behavior

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* remove return_encoder_embeddings parameter

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* remove return_encoder_embeddings parameter

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add manual encoder_embeddings calculation

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix beam_width value to 8

Signed-off-by: Andrei Andrusenko <52885736+andrusenkoau@users.noreply.github.com>

* fix rescoring description

Signed-off-by: Andrei Andrusenko <52885736+andrusenkoau@users.noreply.github.com>

---------

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>
Signed-off-by: Andrei Andrusenko <52885736+andrusenkoau@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,scripts/asr_language_modeling/ngram_lm/eval_beamsearch_ngram_transducer.py,2023-03-24T07:55:57Z,"Hybrid Autoregressive Transducer (HAT) (#6260)

* add hat joint network

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add HATJoint module

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add hat script

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add hat decoding option

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add hat related parameters to maes decoding

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* minor fixes

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add hat decoding option

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* minor fixes

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add hat related parameters

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* minor fixes

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add hat to all rnnt decoding types

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add test for hatjoint

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* combine hatjoint with all rnntjoint tests

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* minor fixes

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* rename hat file

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* fix hat double output

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* fix hat double output

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* fix hat double output

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* nemo/collections/asr/parts/submodules/rnnt_greedy_decoding.py

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* minor fixes

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add return_hat_ilm property

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add HATJointOutput dataclass

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add resolve_joint_output function

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add local return_hat_ilm_default variable

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* minor fixes

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

---------

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,scripts/asr_language_modeling/ngram_lm/eval_beamsearch_ngram_transducer.py,2023-03-14T16:26:08Z,"Ngram lm fusion for RNNT maes decoding (#6118)

* add parameters for ngram_lm

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add parameters for ngram lm

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add RNNT model types for kenlm training

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add ngram lm fusion to maes decoding mode

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add a script for the rnnt beam search decoding with a ngram lm fusion for maes

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* minor fixes

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* minor fixes

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix autocast

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* typing fix

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* minor fixes

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* add set_decoding_type function

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* remove tokens_type

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* remove tokens_type from config

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* import token_offset from train_kenlm

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add DEFAULT_TOKEN_OFFSET variable

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix applying token_offset for char models

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* fixe copyright year

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

* leave DEFAULT_TOKEN_OFFSET only

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>

---------

Signed-off-by: andrusenkoau <andrusenkoau@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,examples/asr/asr_cache_aware_streaming/speech_to_text_cache_aware_streaming_infer.py,2023-07-22T07:12:01Z,Adding docs and models for multiple lookahead cache-aware ASR (#7067) (#7094)
github.com/NVIDIA/NeMo,examples/asr/asr_cache_aware_streaming/speech_to_text_cache_aware_streaming_infer.py,2023-04-25T19:42:29Z,Fix cache aware hybrid bugs (#6466) (#6484)
github.com/NVIDIA/NeMo,examples/asr/asr_cache_aware_streaming/speech_to_text_cache_aware_streaming_infer.py,2023-04-03T08:47:09Z,"chore: minor cleanup (#6311)

* chore: minor cleanup

Signed-off-by: Greg Clark <grclark@nvidia.com>

* Adding docs and missing param

Signed-off-by: Greg Clark <grclark@nvidia.com>

* Clip cache_keep_size in causal_convs

Signed-off-by: Greg Clark <grclark@nvidia.com>

---------

Signed-off-by: Greg Clark <grclark@nvidia.com>"
github.com/NVIDIA/NeMo,examples/asr/asr_cache_aware_streaming/speech_to_text_cache_aware_streaming_infer.py,2023-03-14T02:11:32Z,"Streaming conformer CTC export (#5837)

* cache-aware streaming export

Test onnx streaming conformer ctc WER

Constant att cache width with len param

Remove some extra functions in cache_aware runner

transpose cache so that batch is first for trt

Signed-off-by: Greg Clark <grclark@nvidia.com>

* fix export for full-context conformer

* WIP trying to improve onnx perf

Signed-off-by: Greg Clark <grclark@nvidia.com>

* Adding test scripts

Signed-off-by: Greg Clark <grclark@nvidia.com>

* More perf testing script

Signed-off-by: Greg Clark <grclark@nvidia.com>

* Updates for jit torch_tensorrt tracing

Signed-off-by: Greg Clark <grclark@nvidia.com>

* Fixed trace warnings

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Rearranging tests

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixing non-caching case

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* testing

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixed channel cache length issue

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* cache-aware streaming export

Test onnx streaming conformer ctc WER

Constant att cache width with len param

Remove some extra functions in cache_aware runner

transpose cache so that batch is first for trt

Signed-off-by: Greg Clark <grclark@nvidia.com>

* fix export for full-context conformer

* WIP trying to improve onnx perf

Signed-off-by: Greg Clark <grclark@nvidia.com>

* Adding test scripts

Signed-off-by: Greg Clark <grclark@nvidia.com>

* More perf testing script

Signed-off-by: Greg Clark <grclark@nvidia.com>

* Updates for jit torch_tensorrt tracing

Signed-off-by: Greg Clark <grclark@nvidia.com>

* stash

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Reverting non-essential changes

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Offset=None case

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Remove test scripts

Signed-off-by: Greg Clark <grclark@nvidia.com>

* Clean up speech_to_text_cache_aware_streaming_infer

Signed-off-by: Greg Clark <grclark@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Revert pad -> constant_pad_nd

Signed-off-by: Greg Clark <grclark@nvidia.com>

* conformer-encoder set window_size from streaming_cfg

Signed-off-by: Greg Clark <grclark@nvidia.com>

* Fixes for working export(), using more constants

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Optional rand init for cahce

Signed-off-by: Greg Clark <grclark@nvidia.com>

* Folding update_cache with constants

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* More folding

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Reducing diff #1

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Reducing diff #2

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Reducing diff #3

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Fixed unit tests, more reverts

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Export fixes

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Reverted slice changes that ruined ONNX perf

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

* Adding back keep_all_outputs and drop_extra_preencoded

Signed-off-by: Greg Clark <grclark@nvidia.com>

* Fix export

Signed-off-by: Greg Clark <grclark@nvidia.com>

---------

Signed-off-by: Greg Clark <grclark@nvidia.com>
Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>
Co-authored-by: Boris Fomitchev <bfomitchev@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Vahid Noroozi <VahidooX@users.noreply.github.com>"
github.com/NVIDIA/NeMo,examples/asr/asr_cache_aware_streaming/speech_to_text_cache_aware_streaming_infer.py,2022-12-06T05:52:20Z,"Adding Hybrid RNNT-CTC model (#5364)

* added initial code.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* added the confs.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* added the confs.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* changed name from joint to hybrid.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fixed format.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed format.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed bug.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fixed bug.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* addressed comments.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* addressed comments.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* added docs.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* added docs.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* added docs.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* added docs.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed bug.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fixed bug.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed bug.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed bug.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed bug.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fixed bug.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed bug.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed bug.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* addec CI test.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* addec CI test.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed bugs in change_vocabs.

Signed-off-by: vahidoox <vnoroozi@nvidia.com>

* fixed bugs in change_vocabs.

Signed-off-by: vahidoox <vnoroozi@nvidia.com>

* fixed style.

Signed-off-by: vahidoox <vnoroozi@nvidia.com>

* fixed style.

Signed-off-by: vahidoox <vnoroozi@nvidia.com>

* fixed style.

Signed-off-by: vahidoox <vnoroozi@nvidia.com>

* raise error for aux_ctc.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* raise error for aux_ctc.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* raise error for aux_ctc.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* raise error for aux_ctc.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* updated the streaming names.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* added unittests.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* added unittests.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* added unittests.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed tests.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed tests.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed tests.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed tests.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fixed tests.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fixed tests.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* added methods.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* added decoding.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* fxied the tests.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

Signed-off-by: Vahid <vnoroozi@nvidia.com>
Signed-off-by: vahidoox <vnoroozi@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/token_classification/punctuation_capitalization_lexical_audio_model.py,2023-01-23T20:55:58Z,"Add SSL import functionality for Audio Lexical PNC Models (#5834)

* Initial commit of changes for SSL functionality

Signed-off-by: Daniel Egert <degert@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Deleted comment lines as per request

Signed-off-by: Daniel Egert <degert@nvidia.com>

Signed-off-by: Daniel Egert <degert@nvidia.com>
Co-authored-by: Daniel Egert <degert@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Matvei Novikov <mattyson.so@gmail.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/token_classification/punctuation_capitalization_lexical_audio_model.py,2022-10-20T18:54:51Z,"P&C tutorial, inference script (#5195)

* Added tutorial, infrerence script

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>

* Style fix

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>

* Added missing import

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>

* Reformat

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/token_classification/punctuation_capitalization_lexical_audio_model.py,2022-09-08T20:41:11Z,"removed unused imports for all domains. (#4901)


Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/nlp/models/token_classification/punctuation_capitalization_lexical_audio_model.py,2022-08-31T15:13:43Z,"Added P&C lexical audio model (#4802)

* Added P&C lexical audio model

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>

* Added copyright header

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>

* Default config fix

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>

* Multiple dev/valid dataloaders fix

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>

* Updated copyright header

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>

* Updated config

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>

* Added help to new arguments.

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>

* Fixed description

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>

* Refactor

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>

* Added fusion related args to default config

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>

* Config refactor

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>

* Config refactor

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>

* Removed redundant typings

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>

* Updated config structure

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>

* Added compatibility with other ASR models

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>

* Added import guard for ASR collection

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>

* Fixed type in `use_audio` param

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>

* Added comments on `freeze` section in default config

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>

* Added comments and default values for `adapter.config` config

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>

* Fixed wrong class usage in train/evaluate script

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>

* Import guard corrected

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>

* Check if path exists added for `restore_lexical_encoder_from`

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>

Signed-off-by: Matvei Novikov <mattyson.so@gmail.com>"
github.com/NVIDIA/NeMo,tests/core/test_save_restore.py,2024-02-06T00:26:43Z,"Support uploading NeMo models to HF via `push_to_hf_hub()` (#8263)

* Initial support for saving unpacked nemo file directly and support for uploading NeMo models to Huggingface Hub

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add support to restore nemo models from HF in unpacked format

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Correct input types for model card

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update License Section to template

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add unit test to restore via unpacked checkpoint

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix typo

Signed-off-by: smajumdar <titu1994@gmail.com>

* Address comments

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Address comments

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add docs and address comments

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,tests/core/test_save_restore.py,2023-11-15T03:16:59Z,"Add back import guard (#7882)

* add back import guard

Signed-off-by: Chen Cui <chcui@nvidia.com>

* skip failing unit test

Signed-off-by: Chen Cui <chcui@nvidia.com>

---------

Signed-off-by: Chen Cui <chcui@nvidia.com>
Co-authored-by: Eric Harper <complex451@gmail.com>"
github.com/NVIDIA/NeMo,tests/core/test_save_restore.py,2023-04-02T04:58:37Z,"[Core] return_config=True now extracts just config, not full tarfile (#6346)

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,tests/core/test_save_restore.py,2023-01-23T19:04:09Z,"Support nested NeMo models (#5671)

Nested NeMo models support

Signed-off-by: Vladimir Bataev <vbataev@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Sean Naren <sean.narenthiran@gmail.com>"
github.com/NVIDIA/NeMo,tests/core/test_save_restore.py,2022-10-15T00:33:55Z,"Support TorchScript export for Squeezeformer (#5164)

* Support TorchScript export for Squeezeformer

Signed-off-by: smajumdar <titu1994@gmail.com>

* Remove prints

Signed-off-by: smajumdar <titu1994@gmail.com>

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,tests/core/test_save_restore.py,2022-07-29T05:50:03Z,"Fix HF check for model card info (#4628)

Signed-off-by: smajumdar <smajumdar@nvidia.com>"
github.com/NVIDIA/NeMo,tests/core/test_save_restore.py,2022-07-29T00:34:44Z,"r1.10.0 MegaMolBART Compatibility (#4603)

* 1. Added vocab_size property to RegExTokenizer.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Fixed passing hiddens directly.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Fixed style.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Added support in encoder outputs.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Added comments.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Added automatic mapping of kwargs to args in forward.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Added encode function.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Fixed style.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Fixed style.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. Fixed style.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@nvidia.com>

* 1. PP and TP works (but not together)

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Separated get_forward_output_only_func_encode and get_forward_output_only_func_decode.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* Set headscale false (#4364)

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Add wandb as dependency (#4365)

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Raise trainer error (#4356)

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>

* Set headscale false (#4364) (#4366)

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>
Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Finetuning changes for BART (#4003)

* Temp

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Checkpoint converter to nemo for bart

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>

* Make position embedding expansion specific to a batch to avoid checkpoint size mismatches (#4357)

* Style

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

* Fix logging warning

Signed-off-by: MaximumEntropy <sandeep.subramanian.1@umontreal.ca>

Co-authored-by: Micha Livne <michalivne@users.noreply.github.com>

* 1. Added return logits to validation.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Fixed unkown token during sampling.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Fixed RegExTokenizer loading.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Fixed ckpt file with samples int(0).

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Fixed regex tokenizer.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Fixed allowing enc_tokens to be None.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Added ability to ignore tokens by id during decode.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Fixed regex tokenizer .nemo loading issue.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Fixed style.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Fixed style.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Fixed RegEx test.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* r1.10.0 untie embeddings weights (#4519)

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Added independent decoder embeddings, and independent decoder token_head.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Added support in yaml config.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Fixed initialization.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Added tests for untied embeddings and decoder token head.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Updated share_word_embeddings to share_token_embeddings.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Fixed style.
Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Fixed error in __del__ when TextMemMapDataset fails to build.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Fixed comments.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1.Made method private.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Fixed config names.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Fixed alerts and style.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Fixed PP, TP, PP+TP still fails.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

* 1. Debugging.

Signed-off-by: Micha Livne <mlivne@cs.toronto.edu>

Co-authored-by: Micha Livne <mlivne@nvidia.com>
Co-authored-by: ericharper <complex451@gmail.com>
Co-authored-by: Sandeep Subramanian <sandeep.subramanian.1@umontreal.ca>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,tests/core/test_save_restore.py,2022-07-27T22:12:53Z,"Support listing Hugging Face model info (#4619)

* Support listing Hugging Face model info

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Add documentation about usage

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Add documentation about usage

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Update name of method, support list of model filters

Signed-off-by: smajumdar <smajumdar@nvidia.com>

* Improve docstring

Signed-off-by: smajumdar <smajumdar@nvidia.com>"
github.com/NVIDIA/NeMo,tests/core/test_save_restore.py,2022-04-28T00:39:51Z,"[Core] Support pre-extracted nemo checkpoint for restoration (#4061)

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>"
github.com/NVIDIA/NeMo,tests/core/test_save_restore.py,2022-04-25T19:21:58Z,"Cherry pick HF integration and bug fixes from 1.8.1 (#4052)

* Patch commons.py (#4039)

* revert export.py changes

Signed-off-by: ericharper <complex451@gmail.com>

* revert hack

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused imports

Signed-off-by: ericharper <complex451@gmail.com>

* Add support for Huggingface Hub to NeMo `from_pretrained()` (#4030)

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Eric Harper <complex451@gmail.com>

* Fixing pretrained name (#4022)

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>

* Add back Citrinet zh (#4040)

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>

* cherry pick

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: Boris Fomitchev <borisfom@users.noreply.github.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Subhankar Ghosh <subhankar2321@gmail.com>"
github.com/NVIDIA/NeMo,tests/core/test_save_restore.py,2022-01-31T22:20:29Z,"Final merge r1.6.0 main (#3570)

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* Fix the tutorial notebooks bug (#3465)

* fix checkpoint loading and model config file

Signed-off-by: Yi Dong <yidong@nvidia.com>

* fix style

Signed-off-by: Yi Dong <yidong@nvidia.com>

* Fix checkpoint converter in O2 style (#3486)

* Fix checkpoint converter in O2 style

Signed-off-by: Yu Yao <yuya@nvidia.com>

* Fix style

Signed-off-by: Yu Yao <yuya@nvidia.com>

Co-authored-by: Yu Yao <yuya@nvidia.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* Remove pickled features from tarred dataset (#3491)

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

Co-authored-by: ekmb <ebakhturina@nvidia.com>

* adding missing init files (#3505)

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* typos (#3504)

* typos

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* link fix

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* update titanet conf (#3507)

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* Fix link to NGC page for ASR (#3512)

Signed-off-by: smajumdar <titu1994@gmail.com>

* vad typo fix (#3490)

* remove always broken ptl link

Signed-off-by: fayejf <fayejf07@gmail.com>

* fix typo

Signed-off-by: fayejf <fayejf07@gmail.com>

* Add verification helper function and update docs (#3514)

* Add verification helper function and update docs

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* typo fixes

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* fixed the num_classes bug of conv decoder. (#3525)

* fixed the num_classes bug.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* added logging info.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* Enforce utf-8 on all file r/w (#3520)

* Update paths to subtask

Signed-off-by: smajumdar <titu1994@gmail.com>

* Enforce utf-8 on all file r/w

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Eric Harper <complex451@gmail.com>

* Fixed section typo (#3522)

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Pushing updated WFST Tutorial to r1.6.0 (#3521)

Signed-off-by: tbartley94 <tbartley@nvidia.com>

Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>

* Fixed duplicate cell bug (#3518)

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* WFST tutorial update (#3531)

* Pushing updated WFST Tutorial to r1.6.0

Signed-off-by: tbartley94 <tbartley@nvidia.com>

* Hopefully final corrections to WFST tutorials.

Signed-off-by: tbartley94 <tbartley@nvidia.com>

* [TTS] Fix bug in inference tts notebook (#3532)

* fix bug in inference tts notebook

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* Update Inference_ModelSelect.ipynb

* fix space

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* remove space

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* Prompt tuning documentation (#3541)

* Started prompt tuning doc

Signed-off-by: Virginia Adams <vadams@nvidia.com>

* Update prompt_tuning.rst

* Update models.rst

* Update models.rst

* Update and rename megatron_finetuning.rst to megatron_downstream_tasks.rst

* Update intro.rst

* Update intro.rst

* Update and rename megatron_downstream_tasks.rst to megatron_finetuning.rst

* Update megatron_finetuning.rst

* Delete prompt_tuning.rst

* Update README.rst

* Update docs/source/nlp/megatron_finetuning.rst

Co-authored-by: Eric Harper <complex451@gmail.com>

* Fix nmt resume (#3539)

* check for model attr

Signed-off-by: ericharper <complex451@gmail.com>

* update jenkins test

Signed-off-by: ericharper <complex451@gmail.com>

* TN bug fix (#3538)

* ve and cel fixes

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* ve and cel fixes

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* add w to single digit roman and cardinal single digit graph (non det)

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* isn't fix

Signed-off-by: ekmb <ebakhturina@nvidia.com>

Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* fix bug in tutorial (#3546)

Signed-off-by: Oktai Tatanov <oktai.tatanov@gmail.com>

* Update nvidia container check (#3535)

* update nvidia container check

Signed-off-by: ericharper <complex451@gmail.com>

* update minor version

Signed-off-by: ericharper <complex451@gmail.com>

* add check to T5

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* update bert

Signed-off-by: ericharper <complex451@gmail.com>

* forgot import

Signed-off-by: ericharper <complex451@gmail.com>

* remove import

Signed-off-by: ericharper <complex451@gmail.com>

* Fix an issue with wandb not displaying updated config changes (#3552)

Signed-off-by: smajumdar <titu1994@gmail.com>

* remove extra instance (#3551)

Signed-off-by: ericharper <complex451@gmail.com>

* update branch

Signed-off-by: ericharper <complex451@gmail.com>

* update package info

Signed-off-by: ericharper <complex451@gmail.com>

* revert

Signed-off-by: ericharper <complex451@gmail.com>

Co-authored-by: Yi Dong <43824965+yidong72@users.noreply.github.com>
Co-authored-by: yaoyu-33 <54727607+yaoyu-33@users.noreply.github.com>
Co-authored-by: Yu Yao <yuya@nvidia.com>
Co-authored-by: PeganovAnton <peganoff2@mail.ru>
Co-authored-by: ekmb <ebakhturina@nvidia.com>
Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>
Co-authored-by: Evelina <10428420+ekmb@users.noreply.github.com>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>
Co-authored-by: Vahid Noroozi <VahidooX@users.noreply.github.com>
Co-authored-by: Virginia Adams <78445382+vadam5@users.noreply.github.com>
Co-authored-by: tbartley94 <90423858+tbartley94@users.noreply.github.com>
Co-authored-by: Oktai Tatanov <oktai.tatanov@gmail.com>"
github.com/NVIDIA/NeMo,tests/core/test_save_restore.py,2022-01-07T20:27:12Z,"TalkNet Fix (#3092)

* Fix 'model' key and object collisionn.

Signed-off-by: Stanislav Beliaev <stasbelyaev96@gmail.com>

* Rename model config node to encoder.

Signed-off-by: Stanislav Beliaev <stasbelyaev96@gmail.com>

* Remove line adding.

Signed-off-by: Stanislav Beliaev <stasbelyaev96@gmail.com>

* Return TalkNet to README.

Signed-off-by: Stanislav Beliaev <stasbelyaev96@gmail.com>

* Add model.model test and remove some stuff.

Signed-off-by: Stanislav Beliaev <stasbelyaev96@gmail.com>

* Add moAdd typing to TalkNet.

Signed-off-by: Stanislav Beliaev <stasbelyaev96@gmail.com>

* TalkNet Training notebook fixed.

Signed-off-by: Stanislav Beliaev <stasbelyaev96@gmail.com>

Co-authored-by: Oktai Tatanov <oktai.tatanov@gmail.com>"
github.com/NVIDIA/NeMo,tests/core/test_save_restore.py,2021-08-06T00:45:57Z,"Add save restore connector to ModelPT (#2592)

* add save restore connector

Signed-off-by: ericharper <complex451@gmail.com>

* add save restore connector

Signed-off-by: ericharper <complex451@gmail.com>

* add save restore connector property

Signed-off-by: ericharper <complex451@gmail.com>

* add _default_save_to

Signed-off-by: ericharper <complex451@gmail.com>

* moving globals to app_state

Signed-off-by: ericharper <complex451@gmail.com>

* moving globals to app_state

Signed-off-by: ericharper <complex451@gmail.com>

* add model attribute to connector

Signed-off-by: ericharper <complex451@gmail.com>

* add model attribute to connector

Signed-off-by: ericharper <complex451@gmail.com>

* fix tabs

Signed-off-by: ericharper <complex451@gmail.com>

* fix tabs

Signed-off-by: ericharper <complex451@gmail.com>

* remove ModelPT import

Signed-off-by: ericharper <complex451@gmail.com>

* add default restore

Signed-off-by: ericharper <complex451@gmail.com>

* add default restore

Signed-off-by: ericharper <complex451@gmail.com>

* remove eff globals

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* fix tabs

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* update globals, remove save restore property

Signed-off-by: ericharper <complex451@gmail.com>

* fix typo

Signed-off-by: ericharper <complex451@gmail.com>

* add setter

Signed-off-by: ericharper <complex451@gmail.com>

* add setter

Signed-off-by: ericharper <complex451@gmail.com>

* fix app_state restore flag

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* update paths

Signed-off-by: ericharper <complex451@gmail.com>

* add connector arg to from_pretrained

Signed-off-by: ericharper <complex451@gmail.com>

* update save restore connector after instantiating

Signed-off-by: ericharper <complex451@gmail.com>

* use connector

Signed-off-by: ericharper <complex451@gmail.com>

* get class from config in .nemo

Signed-off-by: ericharper <complex451@gmail.com>

* add TODO

Signed-off-by: ericharper <complex451@gmail.com>

* move extract_state_dict to connector

Signed-off-by: ericharper <complex451@gmail.com>

* add methods for toch save and torch load

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* typo

Signed-off-by: ericharper <complex451@gmail.com>

* update mock model conf

Signed-off-by: ericharper <complex451@gmail.com>

* revert

Signed-off-by: ericharper <complex451@gmail.com>

* move mock model to common collection

Signed-off-by: ericharper <complex451@gmail.com>

* update NLPModel

Signed-off-by: ericharper <complex451@gmail.com>

* update test to use connector

Signed-off-by: ericharper <complex451@gmail.com>

* move artifacts to save restore connector

Signed-off-by: ericharper <complex451@gmail.com>

* add save_restore_connector arg to register_artifact

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* default save_restore_connector arg to None

Signed-off-by: ericharper <complex451@gmail.com>

* default save_restore_connector arg to None

Signed-off-by: ericharper <complex451@gmail.com>

* clean commented line

Signed-off-by: ericharper <complex451@gmail.com>

* default save_restore_connector arg to None

Signed-off-by: ericharper <complex451@gmail.com>

* move MockModel

Signed-off-by: ericharper <complex451@gmail.com>

* fix docstrings, remove underscores, default from connector

Signed-off-by: ericharper <complex451@gmail.com>

* update docstring

Signed-off-by: ericharper <complex451@gmail.com>

* update docstring

Signed-off-by: ericharper <complex451@gmail.com>

* change name to is_model_being_restored

Signed-off-by: ericharper <complex451@gmail.com>

* move constants from AppState to SaveRestoreConnector

Signed-off-by: ericharper <complex451@gmail.com>

* encapsulate logic for model parallel checkpoint

Signed-off-by: ericharper <complex451@gmail.com>

* style

Signed-off-by: ericharper <complex451@gmail.com>

* update mock config

Signed-off-by: ericharper <complex451@gmail.com>

* remove unused import

Signed-off-by: ericharper <complex451@gmail.com>

* add init_subclass, remove connector arg from register_artifact, move MockModel to tests

Signed-off-by: ericharper <complex451@gmail.com>

* remove old import

Signed-off-by: ericharper <complex451@gmail.com>

* Add tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Finalize tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Finalize tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* fixing lgtm

Signed-off-by: ericharper <complex451@gmail.com>

* fix lgtm

Signed-off-by: ericharper <complex451@gmail.com>

* update NLPModel.restore_from

Signed-off-by: ericharper <complex451@gmail.com>

* Fix classpath resolution

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,tests/core/test_save_restore.py,2021-06-03T22:49:50Z,"Merge tag 'v1.0.0' into main

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,tests/core/test_save_restore.py,2021-06-02T04:49:50Z,"Adding new Models releases on NGC. (#2295)

* added new models.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* added tests for asr lm.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* added tests for asr lm.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* dropped the test.

Signed-off-by: Vahid <vnoroozi@nvidia.com>"
github.com/NVIDIA/NeMo,tests/core/test_save_restore.py,2021-05-26T06:32:31Z,"Support multiple models being instantiated in same execution scope (#2245)

* Support multiple models being instantiated in same execution scope

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add locks to methods in appstate

Signed-off-by: smajumdar <titu1994@gmail.com>

* Perform locks only on write operations

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct deadlock issue

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add more tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add test for multi save and remove patch to change save type

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update app state to preserve gidx of previous token

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct restoration logic for tarfiles

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,tests/core/test_save_restore.py,2021-05-04T20:55:35Z,"Update how artifacts work (#2138)

* Update how artifacts work

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* fixing some tests

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* fix more tests

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* add __init__ to tests to make them discoverable

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* empty src support

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* updates plust unittest

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* add copyright check

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* copyright header

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* fix style

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* handle hashed megatron checkpoint version in nlp restore_from

Signed-off-by: ericharper <complex451@gmail.com>

* add _MODEL_RESTORE_PATH to AppState

Signed-off-by: ericharper <complex451@gmail.com>

* get rid of global folder caching

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* double register - warning instead of exception

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* Add asr spe tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Pop out asr wpe pre-registered value

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct ASR tests and paths

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct tokenizer saving

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct ASR tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct ASR bpe mixin

Signed-off-by: smajumdar <titu1994@gmail.com>

* Patch up backward compatibility

Signed-off-by: smajumdar <titu1994@gmail.com>

* update register_bert_model

Signed-off-by: ericharper <complex451@gmail.com>

* update all get_lm_model calls

Signed-off-by: ericharper <complex451@gmail.com>

* return None if src not found

Signed-off-by: ericharper <complex451@gmail.com>

* handle case with no tokenizer

Signed-off-by: ericharper <complex451@gmail.com>

* do not add another hash is using tarfile_artifacts

Signed-off-by: ericharper <complex451@gmail.com>

* add return_none flag, update doc string

Signed-off-by: ericharper <complex451@gmail.com>

* update default behavior of register_artifact for NLPModel

Signed-off-by: ericharper <complex451@gmail.com>

* change kwarg name to verify_src_exists

Signed-off-by: ericharper <complex451@gmail.com>

* use cfg instead of _cfg

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* some cleanups

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

Co-authored-by: ericharper <complex451@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,tests/core/test_save_restore.py,2021-03-30T06:15:43Z,"update to unittesting (#1983)

* update to unittesting

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* expanding unittesting

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* Update test_megatron.py

Signed-off-by: ericharper <complex451@gmail.com>

* unskip export test

Signed-off-by: ericharper <complex451@gmail.com>

* try get test

Signed-off-by: ericharper <complex451@gmail.com>

* add check to megatron test to make sure it is in
ourt CI environment

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

Co-authored-by: Eric Harper <complex451@gmail.com>"
github.com/NVIDIA/NeMo,tests/core/test_save_restore.py,2021-03-30T02:51:15Z,"Support target classpath resolution for all ModelPT subclasses (#1982)

* Support target classpath resolution for all ModelPT subclasses

Signed-off-by: smajumdar <titu1994@gmail.com>

* Support target classpath resolution for all ModelPT subclasses

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>"
github.com/NVIDIA/NeMo,tests/core/test_save_restore.py,2021-03-25T04:10:48Z,"Add conformer docs. (#1966)

* added tests, initial docs on conformer.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* Added conformer encoder doc strings.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* Added conformer encoder doc strings.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* added text classification model.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* added config table.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* addressed comments.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* addressed comments.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* addressed comments.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* addressed comments.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* addressed comments.

Signed-off-by: Vahid <vnoroozi@nvidia.com>"
github.com/NVIDIA/NeMo,tests/core/test_save_restore.py,2021-03-15T21:54:53Z,"NLP, Megatron and Tools docs (#1739)

* wip

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* docs for nlp and tools init

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* tools docs

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* added script to convert raw data into nemo format

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* placeholders added

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* punctuation model docs updated

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* Speech Regression Support (#1707)

* Speech Regression support

Signed-off-by: diego-fustes <diegofustesfic@gmail.com>

* Speech Regression Support

Signed-off-by: diego-fustes <diegofustesfic@gmail.com>

* Speech Regression Support

Signed-off-by: diego-fustes <diegofustesfic@gmail.com>

* Refactoring after review

Signed-off-by: diego-fustes <diegofustesfic@gmail.com>

* Refactorings after review, fixes

Signed-off-by: diego-fustes <diegofustesfic@gmail.com>

* Refactorings after review, fixes

Signed-off-by: diego-fustes <diegofustesfic@gmail.com>

* Refactorings after review, fixes

Signed-off-by: diego-fustes <diegofustesfic@gmail.com>

* Refactorings after review, fixes

Signed-off-by: diego-fustes <diegofustesfic@gmail.com>

* Refactorings after review, fixes

Signed-off-by: diego-fustes <diegofustesfic@gmail.com>

* Refactorings after review, fixes

Signed-off-by: diego-fustes <diegofustesfic@gmail.com>

* Refactorings after review, fixes

Signed-off-by: diego-fustes <diegofustesfic@gmail.com>

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>

* update max seq len

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* tc_update

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* docs update

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* remove untouched files

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* review feedback

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* new ngc model names

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* text norm doc (#1893)

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* model name updated

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* jenkins rename model

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* quick start added, model_nlp added

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* commnet changed

Signed-off-by: ekmb <ebakhturina@nvidia.com>

Co-authored-by: Diego Fustes Villad√≥niga <diegofustesfic@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>"
github.com/NVIDIA/NeMo,tests/core/test_save_restore.py,2021-03-12T19:09:44Z,"dropped conformer checkpoints. (#1897)

* dropped conformer checkpoints.

Signed-off-by: Vahid <vnoroozi@nvidia.com>

* dropped from unittest.

Signed-off-by: Vahid <vnoroozi@nvidia.com>"
github.com/NVIDIA/NeMo,tests/core/test_save_restore.py,2021-03-11T22:12:21Z,"Update ASR pretrained models (#1888)

* Update ASR pretrained models

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct names for tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct Jenkinsfile

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add back BPE model to tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add back BPE model to tests

Signed-off-by: smajumdar <titu1994@gmail.com>

* Convert all QuartzNet15x5Base-En to stt_en_quartznet15x5

Signed-off-by: smajumdar <titu1994@gmail.com>

* Revert ""Convert all QuartzNet15x5Base-En to stt_en_quartznet15x5""

This reverts commit 03424b32

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add back QuartzNet15x5Base-En

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,tests/core/test_save_restore.py,2021-02-05T00:49:31Z,"Add save restore tests (#1717)

Signed-off-by: smajumdar <titu1994@gmail.com>"
github.com/NVIDIA/NeMo,tests/core/test_save_restore.py,2020-11-24T21:34:31Z,"Update ModelPT level save and restore (#1491)

* Update ModelPT level save and restore

Signed-off-by: smajumdar <titu1994@gmail.com>

* Remove logging

Signed-off-by: smajumdar <titu1994@gmail.com>

* polish style a little

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* Correct model restoration from .nemo vs .pt checkpoint

Signed-off-by: smajumdar <titu1994@gmail.com>

* Correct model restoration from .nemo vs .pt checkpoint

Signed-off-by: smajumdar <titu1994@gmail.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,tests/manualtest_model_downloads.py,2021-09-20T01:58:04Z,"Update model names (#2845)

* updated speaker model names

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* update tutorial model names

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>"
github.com/NVIDIA/NeMo,tests/manualtest_model_downloads.py,2021-03-11T16:59:24Z,"Renamed pretrained names (#1882)

* Renamed pretrained names

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* Updated pretrained description format and updated README tutorials table with Speaker Diarization tutorials

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Jason <jasoli@nvidia.com>"
github.com/NVIDIA/NeMo,tests/manualtest_model_downloads.py,2020-09-23T23:20:55Z,"Aws2ngc (#1212)

* change AWS links with NGC links

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* remove some experimental

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* bugfix

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* bugfix

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* bugfixes

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* fix test

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* attempted bugfix

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* bugfixes

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* stylefix

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* file rename

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* download test refactor

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,tests/collections/nlp/test_pretrained_models_performance.py,2022-09-26T01:39:59Z,"clean warnings from tests and CI runs, and prepare for upgrade to PTL 1.8 (#4830)

* remove with_downloads marker warning from pytest

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* regex escape sequence and np float deprecation

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* improve speed of torchmetrics

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* fix python __int__ deprecation

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* multi binary accuracy

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* update topkaccuracy metric

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* trainer, text norm, qr -> linalg.qr

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* remove weights save path arg to trainer

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* ptl core warnings fix

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* on_pretrain_routine_start -> on_fit_start

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* add weights_save_path to deprecated args

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* style fix

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* Remove reference

Signed-off-by: SeanNaren <snarenthiran@nvidia.com>

* revert torch.long change

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>
Signed-off-by: SeanNaren <snarenthiran@nvidia.com>
Co-authored-by: SeanNaren <snarenthiran@nvidia.com>"
github.com/NVIDIA/NeMo,tests/collections/nlp/test_pretrained_models_performance.py,2022-02-23T21:40:46Z,"upgrade PTL trainer flags (#3589)

* upgrade PTL trainer flags

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* remove nlp override changes

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* fix test and update gpus -> devices

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* setting devices=1 for cpu case

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* update jenkins for gpus -> devices

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* single quote to double quote jenkins fix

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* single quote to double quote jenkins fix

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* devices in docs

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* update devices in tutorials :sigh:

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* checkpointing_callback -> enable_checkpointing

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* text normalization decoder trainer name fix

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* enable checkpoint callback name fix

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* NLPDDPPlugin overrides strategy ddp

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* more strategy cleaning for NLPDDPPlugin

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* now DDPPlugin for strategy removal

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* typo fix

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* NLPDDPPlugin more fixes

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* devices require accelerator as mandatory argument

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* moving megatron accelerator from gpu to cpu

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* accelerator null to strategy null fix

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* distrib_type taken from training_type_plugin

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* check for distributed type is removed

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* revert training type

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* revert transcribe_speech

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* style fix

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* typo fix

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* revert distributed backend type

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* remove flush_logs_every_n_steps trainer flag

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* rebase main

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* style fix

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* SGD gen update

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* NLPDDPPlugin change

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* remove strategy from dialogue conf

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* reverting find unused params

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* update jenkins

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* more nlp fixes

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>

* num_nodes removal from NLPDDPPlugin

Signed-off-by: nithinraok <nithinrao.koluguri@gmail.com>"
github.com/NVIDIA/NeMo,tests/collections/nlp/test_pretrained_models_performance.py,2021-11-30T15:55:49Z,"Improve data pipeline for punctuation capitalization model and make other useful changes (#3159)

* Fix: inference on short sequences problem

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add draft of new punctuation and capitalization model

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix debug config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add parameter check

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Update punctuation training script

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix head config parameter names

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix ds_item and class_label parameters in config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix dataloader shuffling for tarred dataset

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Reduce validation batch

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add debug print

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix metrics initialization

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix minor bug

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix device problem

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add debug print

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Register metrics properly

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Put metrics setup after module init

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Reduce model size

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add wandb logging

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Change wandb name

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix logging names for metrics

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add debug print

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add returning from eval steps

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add second dev dataset

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Move config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix path to dataset""

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add more tokenizer parameters

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add debug script for more tokenizer in creating tarred dataset

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Update output path in debug script

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix minor bug in typing

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix bug in parsing arguments

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Do not pass tokenizer through queue

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Set hf tokenizer in debug script

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Try char vocabulary

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix typo

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve error message

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix OOV problem

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add label ids creation and getting

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: add missing parameter

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve error message for label ids building

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add short tar files repacking

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix minor bug and add more security

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix minor bug

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: replace Path with str

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: iter datasets

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve logging

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Turn off repacking

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Turn off repacking

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Turn on repacking

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Turn off repacking

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add debug print

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve unexpected removal

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Turn on repacking

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: remove repacked files

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add default config for testing

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve code style in evaluate script

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add docstrings

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove debug config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove commented code

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix code style in doc string

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix usage of parser.error function

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve working with config and fix restoring of old checkpoints

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Do not demand cfg as dataclass

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add backward compatibility for absense of use_tarred_dataset

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fight for backwards compatibility

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add tokens_in_batch backward compatibility

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Undo unintentional changes in tutorial

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Do not allow more workers than queries

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix metric names in tests

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix metric location

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix metric location

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Require ds_item or data_dir

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Disable multiprocessing data preparation by default

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Disable multiprocessing data preparation by default

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Disable multiprocessing data preparation by default

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Make minor improvements in docstrings and typing

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix finetuning code

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix shuffle train dataset config parameter

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix evaluation script

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Update test

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add new test and make minor changes

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix repacked file names

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add assertion error

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix minor bug in regex

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve Jenkins command

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix code style

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: add name to Jenkins stage

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: add steps block to Jenkins stage

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: move nemo_experiments removal to post section

Previously I encoutered a weird error

+ rm -rf nemo_experiments
rm: cannot remove 'nemo_experiments': Directory not empty
script returned exit code 1

And suspect that this could be because to parallel stages try to
remove same directory simultaneously.

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Turn off cache usage in Jenkins for token classification models

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Stop pickling features

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Reference webdataset in docs

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Make multiple minor improvements

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add parameters tokens_in_batch, repack to documentation

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Refactoring and improving readability

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Make tar_shuffle_n optional parameter

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix path to label vocab files

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix metadata label vocab key

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Create for_nemo directory

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix tar_shuffle_n default value

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* First round of review fixes

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Return tokens_in_batch default value

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove duplicate parameters in `CommonDatasetParameters`

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove duplicate parameters in config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Refactor user interface

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: add missing parameter in calling setting dataloader up

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: replace data config with model config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: typo in config parameter name

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: location of label ids parameters in config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: transforming not first legacy data config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: num_samples can be negative

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: create directory for nemo ids files

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: remove unremoved with_label

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: features contain ids if loaded from pickle

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix kwargs parameters

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add label setting for testing case

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix: change parameter location in config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix: transform legacy config in init

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix: make minor improvement in checking config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: check label ids for None before checking pad label id

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: set labels when restoring

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: place where label ids are taken

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix minor bug

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: register artifacts in set_label_ids

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: perform checking only if label ids are not set

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: set label_ids_are_set

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix using of dataset in create tarred dataset

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: manipulate label ids if fragment_idx is zero

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: remove directory correctly

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: vocab file names

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: vocab file names

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add debug print

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add directories for cache and label info

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Minor fixes

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Minor fix

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Minor fix

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve debug config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Create missing directories

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve feature pkl file name

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* WORKING VERSION OF VOCAB CONFIG

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve vocab file extraction

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve vocab file extraction

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix register artifact calls

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: add class_labels to legacy fixing

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: add missing method

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add support for checkpoints without class labels artifact

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: add missing return values to function

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix saving label ids in creation of tarred dataset

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: adjust tarred dataset consistency check

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: consistency check call

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Try checking labels every time dataloader is set

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: add another label consistency check

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add more parameters to update_config function

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Small docstrings and refactoring

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Try to use ds_item in get_metrics test func

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Try new format get_metrics func

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* update data configs

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix merging data configs

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix test parameters

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix code style

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix config parameter names

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve docstrings

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve docstrings and make small refactoring

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Minor fix in rst API

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add apex to autodoc_mock_imports

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix megatron func location

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Update rst files

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: some fixes in docs

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add configs to toctree

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Try 2 fixes in docs

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: not f docstrings

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Try another small fix

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Try another small fix

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Make multiple type improvements

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add draft of correct doc strings

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix minor bug in head init and code style

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix code style

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add and improve docs

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix docs

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Make minor fix in overriding config parameters

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: restoring from checkpoint

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix minor bug

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix minor bug in command

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix minor mistake in Jenkins command

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* set use_tarred_dataset in metrics test func

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix paths to tokenizer files

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve error messages

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix: consider case when cache is ready and ids are provided

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix: improve update_config method behaviour

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Update tutorial

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Undo unintentional changes in tutorial

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Make many minor fixes

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* fix notebook

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove empty sell in the end of tutorial

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Make n_jobs parameter optional

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove unused imports

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix minor bug

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* change config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve updating of optim config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add debug print

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Extend debug print

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Extend debug print

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add more debug prints

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add more debug prints

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Replace accelerator with strategy

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add use_cache check

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix code style

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Make minor fixes

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix func name in tutorial

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add punctuation inference test

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix script name

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove unused import

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix links documentation

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix links documentation

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix device setting

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve script help

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add comment for `tokens_in_batch` in config

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Reduce batch size in test

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>"
github.com/NVIDIA/NeMo,tests/collections/nlp/test_pretrained_models_performance.py,2021-09-21T13:31:24Z,"Feat/punctuation capitalization/long queries signoff (#2683)

* Move files from long_queries branch

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* remove sys.path modification

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix code style

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix code style

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix code style

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Update tests

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix code style

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Remove unused imports

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Move all code to punctuate_capitalize.py

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix minor bug

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve help message

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Improve help message

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix code style

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add docstrings and typing

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix code style

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Add remark about default parameter values

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* refactor

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Refactor

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Code style

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix typo

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix typo

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix code style

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix script name

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix script name

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix code style

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

* Fix code style

Signed-off-by: PeganovAnton <peganoff2@mail.ru>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>"
github.com/NVIDIA/NeMo,tests/collections/nlp/test_pretrained_models_performance.py,2021-03-31T05:10:04Z,"Merge branch 'main' of https://github.com/NVIDIA/NeMo into main

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,tests/collections/nlp/test_pretrained_models_performance.py,2021-03-31T01:38:57Z,"Merge branch 'r1.0.0rc1' into main

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,tests/collections/nlp/test_pretrained_models_performance.py,2021-03-31T00:06:16Z,"Autouse cleanup_local_folder fixture (#1990)

* force cleanup

Signed-off-by: Jason <jasoli@nvidia.com>

* fix

Signed-off-by: Jason <jasoli@nvidia.com>

* fix

Signed-off-by: Jason <jasoli@nvidia.com>

* fix

Signed-off-by: Jason <jasoli@nvidia.com>

Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>"
github.com/NVIDIA/NeMo,tests/collections/nlp/test_pretrained_models_performance.py,2021-03-30T06:15:43Z,"update to unittesting (#1983)

* update to unittesting

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* expanding unittesting

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

* Update test_megatron.py

Signed-off-by: ericharper <complex451@gmail.com>

* unskip export test

Signed-off-by: ericharper <complex451@gmail.com>

* try get test

Signed-off-by: ericharper <complex451@gmail.com>

* add check to megatron test to make sure it is in
ourt CI environment

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>

Co-authored-by: Eric Harper <complex451@gmail.com>"
github.com/NVIDIA/NeMo,tests/collections/nlp/test_pretrained_models_performance.py,2021-03-16T06:38:44Z,"Merge branch 'r1.0.0rc1' into main

Signed-off-by: Oleksii Kuchaiev <okuchaiev@nvidia.com>"
github.com/NVIDIA/NeMo,tests/collections/nlp/test_pretrained_models_performance.py,2021-03-15T21:54:53Z,"NLP, Megatron and Tools docs (#1739)

* wip

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* docs for nlp and tools init

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* tools docs

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* added script to convert raw data into nemo format

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* placeholders added

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* punctuation model docs updated

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* fix

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* Speech Regression Support (#1707)

* Speech Regression support

Signed-off-by: diego-fustes <diegofustesfic@gmail.com>

* Speech Regression Support

Signed-off-by: diego-fustes <diegofustesfic@gmail.com>

* Speech Regression Support

Signed-off-by: diego-fustes <diegofustesfic@gmail.com>

* Refactoring after review

Signed-off-by: diego-fustes <diegofustesfic@gmail.com>

* Refactorings after review, fixes

Signed-off-by: diego-fustes <diegofustesfic@gmail.com>

* Refactorings after review, fixes

Signed-off-by: diego-fustes <diegofustesfic@gmail.com>

* Refactorings after review, fixes

Signed-off-by: diego-fustes <diegofustesfic@gmail.com>

* Refactorings after review, fixes

Signed-off-by: diego-fustes <diegofustesfic@gmail.com>

* Refactorings after review, fixes

Signed-off-by: diego-fustes <diegofustesfic@gmail.com>

* Refactorings after review, fixes

Signed-off-by: diego-fustes <diegofustesfic@gmail.com>

* Refactorings after review, fixes

Signed-off-by: diego-fustes <diegofustesfic@gmail.com>

Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>

* update max seq len

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* tc_update

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* docs update

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* remove untouched files

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* review feedback

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* new ngc model names

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* text norm doc (#1893)

Signed-off-by: Yang Zhang <yangzhang@nvidia.com>

* model name updated

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* jenkins rename model

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* quick start added, model_nlp added

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* commnet changed

Signed-off-by: ekmb <ebakhturina@nvidia.com>

Co-authored-by: Diego Fustes Villad√≥niga <diegofustesfic@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>
Co-authored-by: Oleksii Kuchaiev <okuchaiev@users.noreply.github.com>
Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>"
github.com/NVIDIA/NeMo,tests/collections/nlp/test_pretrained_models_performance.py,2021-03-04T19:50:30Z,"CI Fixes for Lightning 1.2.1 (#1839)

* updates

Signed-off-by: Jason <jasoli@nvidia.com>

* add back pleasefixme

Signed-off-by: Jason <jasoli@nvidia.com>

* rmtree

Signed-off-by: Jason <jasoli@nvidia.com>

* add cleanup_local_folder fixtures instead of rmtree

Signed-off-by: Jason <jasoli@nvidia.com>

* bugfix?

Signed-off-by: Jason <jasoli@nvidia.com>

* add back pleasefixme

Signed-off-by: Jason <jasoli@nvidia.com>

* typo

Signed-off-by: Jason <jasoli@nvidia.com>

* set melGAN to find_unused = True

Signed-off-by: Jason <jasoli@nvidia.com>

* force deletion

Signed-off-by: Jason <jasoli@nvidia.com>"
github.com/NVIDIA/NeMo,tests/collections/nlp/test_pretrained_models_performance.py,2021-02-26T18:47:00Z,"set max seq length for inference (#1809)

* update inference

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* update

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* make params explicit

Signed-off-by: ekmb <ebakhturina@nvidia.com>"
github.com/NVIDIA/NeMo,tests/collections/nlp/test_pretrained_models_performance.py,2021-01-22T21:22:55Z,"pre-trained models performance test added and masking fix (#1656)

* performance tests added

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* distilbert mertics updated

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* data_dir path updated to jenkins

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* header fixed

Signed-off-by: ekmb <ebakhturina@nvidia.com>

* run on gpu only

Signed-off-by: ekmb <ebakhturina@nvidia.com>"
github.com/HumanSignal/label-studio-ml-backend,label_studio_ml/examples/nemo/asr.py,2023-07-19T16:20:52Z,"fix: LSDV-2775: v2 and ML backend /versions (#286)

* fix: LSDV-2775: Switching to v2 version, change API for /versions, fix fit() function in ML backends

* Remove key

* Change README

* Loosen requirements.txt version requirements to avoid conflict

* Update simplest backend dockerfile to simplest setup which works, enable autoupdate feature

* Simplify fit function to conform to latest data schema

* Make OpenAIPredictor more generic

* Fix log level

* Fix prompt and output payload

* Add file prompt

* Fix the simplest ml backend

* Revert package updates for testing

* Add back in label studio ml requirement, removing it did not fix the tests

* Fix train_output persistence

* Remove broken dirs

* Update to use current version

* Try clearing unneeded space to avoid out of memory error

* Remove test change

---------

Co-authored-by: nik <nik@heartex.net>
Co-authored-by: andreasdivaris <dredivaris@gmail.com>"
github.com/HumanSignal/label-studio-ml-backend,label_studio_ml/examples/nemo/asr.py,2021-10-14T15:23:30Z,[fix] Delete label studio dependency in label-studio-ml
github.com/HumanSignal/label-studio-ml-backend,label_studio_ml/examples/nemo/asr.py,2021-05-31T04:51:03Z,Add undefined field in other ml backends
github.com/HumanSignal/label-studio-ml-backend,label_studio_ml/examples/nemo/asr.py,2021-04-30T18:03:13Z,Update asr.py
github.com/HumanSignal/label-studio-ml-backend,label_studio_ml/examples/nemo/asr.py,2021-03-30T14:02:02Z,Make examples compatible with LS 1.0
github.com/HumanSignal/label-studio-ml-backend,label_studio_ml/examples/nemo/asr.py,2021-03-15T15:04:28Z,Add ML backend SDK code from label-studio
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2023-11-06T17:39:31Z,"Fix stall when pynini is imported before this library (#33)

* Fix stall when pynini is imported before this library

Previously, doing this:

import pynini
import riva.asrlib.decoder

would end in a stall. This was because the FlagRegister template class
had a static variable Mutex, that ended up being shared by default in
the global symbol namespace. Because the openfst versions are
different between the two libraries (I think pynini's openfst 1.8.2
uses absl::Mutex), the mutex layouts are different, probably causing
the stall.

We don't want the symbols to be shared in this case, so we disable
unique symbols via ""-fno-gnu-unique"".

* Increase error margin.

Helps with flaky CI failures."
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2023-09-07T18:57:15Z,"Add improved flashlight tests.

This reads data from memory rather than from disk for a more
apples-to-apples comparison."
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2023-05-22T23:12:56Z,"0.4.0 release

3.10 test does not work for bizarre reasons (see file changes)"
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2023-05-22T23:12:56Z,"redo partial paths.

See kaldi commit for more information."
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2023-05-22T23:12:56Z,"Redo online decoding.

Choose whether to depend on final state probabilities in lattice
generation.

Add python interface to online decoder.

Unit test that best path callbacks are working. Sometimes, they vary
from the lattice-based ""offline"" decoder results, but this is rare. It
is good enough for now."
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2023-05-22T23:12:56Z,Add support for passing a list of tensors instead of a padded tensor
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2023-04-03T21:46:00Z,"Version 0.3.2

Actual hotfix.

Version 0.3.1 got yanked because I misspecified the version of
nanobind in pyproject.toml"
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2023-04-03T21:46:00Z,0.3.0 release
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2023-04-03T21:46:00Z,"Add back Conformer CTC Large unit tests.

They are working! I just thought there was an error because of an
erroneous error check."
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2023-04-03T21:46:00Z,Add support for returning ilabels from nbest decoding.
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2023-03-23T19:24:26Z,Remove useless commented out test.
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2023-03-23T19:24:26Z,"clang-format, black format, isort"
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2023-03-23T19:24:26Z,"Add throughput tests.

We don't check that RTFx is the same right now. It might be too
flakey.

Recorded throughput results here: https://github.com/nvidia-riva/riva-asrlib-decoder/issues/19"
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2023-03-23T19:24:26Z,"Add WER unit tests for alternate topologies other than Eessen.

Minor cleanup."
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2023-03-23T19:24:26Z,"Fix memory leak by using ""class scope"" fixture.

Previously, 5 GiB were being created for every test because the
librispeech test and dev sets were being loaded every time."
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2023-03-23T19:24:26Z,"Change to nanobind.

Fix GPU memory leak. My pybind11 DLPack integration was incorrect."
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2023-03-23T19:24:26Z,Save work before trying nanobind.
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2023-03-23T19:24:26Z,Add unit test for comparing NeMo and flashlight.
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2023-03-23T19:24:26Z,Test throughput performance
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2023-01-06T16:59:07Z,"Fix bug where allocating a decoder, deallocating it, and then reallocating a new one crashed.

Add a sweep over the entirety of librispeech in the nemo unit test."
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2023-01-05T00:53:30Z,"Migrate to pytest, from unittest

pytest fixtures are helpful for setting things up.

And unittest could never detect my test cases from within cibuildwheel
for some reason.

Add a half precision test for NeMo.

Tbhere is a crash when I create more than one cuda decoder object for
some reason. Will investigate later."
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2023-01-05T00:53:30Z,"Add NeMo end-to-end test

The WER is lower with beam search than with greedy decoding, which is good."
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2022-12-13T23:32:22Z,"cibuildwheel support

Commit before trying to make a namespace package."
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2022-12-13T23:31:53Z,"WIP: Make lattice postprocessor optional.

This is to debug some WER regressions comapred to CPU-based decoding."
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2022-10-21T22:22:23Z,"Fix #3 (#4)

See https://github.com/kaldi-asr/kaldi/pull/4802"
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2022-07-20T18:33:01Z,Fix bug
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2022-07-20T17:19:46Z,"Fix creation of T.fst.

Previously, the units were sorted alphabetically. Fix this by
providing the ""--units"" option explicitly. Probably should make this a
required argument."
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2022-06-24T06:18:44Z,Formatting.
github.com/nvidia-riva/riva-asrlib-decoder,src/riva/asrlib/decoder/test_graph_construction.py,2022-06-21T22:55:15Z,Open source commit.
github.com/dusty-nv/jetson-voice,scripts/nemo_export_onnx.py,2021-06-07T18:00:31Z,added NeMo scripts
github.com/k2-fsa/sherpa-onnx,scripts/nemo/speaker-verification/export-onnx.py,2024-01-13T11:49:45Z,Export speaker verification models from NeMo to ONNX (#526)
github.com/wefantasy/label-studio-demo,backend/examples/nemo/asr.py,2022-11-24T09:16:57Z,Âü∫Êú¨ÂÆåÊàê
github.com/audio-df-ucb/ClonedVoiceDetection,src/packages/ExperimentPipeline.py,2023-07-14T06:01:49Z,src folder with relevant pipeline code
github.com/dusty-nv/jetson-containers,packages/nemo/test_qa.py,2023-08-21T18:03:04Z,added apex to nemo container
github.com/jsvir/vad,nemo/core/classes/common.py,2023-03-08T13:31:39Z,.
github.com/jsvir/vad,nemo/collections/nlp/modules/common/lm_utils.py,2023-03-08T13:31:39Z,.
github.com/jsvir/vad,nemo/collections/asr/parts/utils/decoder_timestamps_utils.py,2023-03-08T13:31:39Z,.
github.com/jsvir/vad,nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2023-03-08T13:31:39Z,.
github.com/daddydrac/NVIDIA-Rapids-NeMo-PyTorch-Tensorboard,examples/nlp/intent_detection_slot_tagging/joint_intent_slot_infer.py,2020-03-17T03:29:14Z,Added mounted volumes
github.com/daddydrac/NVIDIA-Rapids-NeMo-PyTorch-Tensorboard,examples/nlp/intent_detection_slot_tagging/joint_intent_slot_infer_b1.py,2020-03-17T03:29:14Z,Added mounted volumes
github.com/daddydrac/NVIDIA-Rapids-NeMo-PyTorch-Tensorboard,examples/nlp/intent_detection_slot_tagging/joint_intent_slot_with_bert.py,2020-03-17T03:29:14Z,Added mounted volumes
github.com/webaverse/tiktalknet,core/extract.py,2022-01-08T14:20:33Z,Reverted auto-tune change
github.com/webaverse/tiktalknet,core/extract.py,2021-12-28T18:50:39Z,Reconstruction fixes
github.com/webaverse/tiktalknet,core/extract.py,2021-12-28T05:22:41Z,Added spectrogram reconstruction
github.com/webaverse/tiktalknet,core/extract.py,2021-12-26T03:38:21Z,Improved pitch detection
github.com/webaverse/tiktalknet,core/extract.py,2021-12-25T23:59:40Z,Refactor and license update
github.com/Eric3911/OpenAGI,NeMo-master/nemo/core/classes/common.py,2023-05-06T06:44:54Z,"Ê®°ÂûãÊõ¥Êñ∞

update"
github.com/Eric3911/OpenAGI,NeMo-master/nemo/core/classes/modelPT.py,2023-05-06T06:44:54Z,"Ê®°ÂûãÊõ¥Êñ∞

update"
github.com/Eric3911/OpenAGI,NeMo-master/tools/asr_webapp/model_api.py,2023-05-06T06:44:54Z,"Ê®°ÂûãÊõ¥Êñ∞

update"
github.com/Eric3911/OpenAGI,NeMo-master/tools/nmt_webapp/nmt_service.py,2023-05-06T06:44:54Z,"Ê®°ÂûãÊõ¥Êñ∞

update"
github.com/Eric3911/OpenAGI,NeMo-master/nemo/collections/tts/models/mixer_tts.py,2023-05-06T06:44:54Z,"Ê®°ÂûãÊõ¥Êñ∞

update"
github.com/Eric3911/OpenAGI,NeMo-master/nemo/collections/nlp/modules/common/lm_utils.py,2023-05-06T06:44:54Z,"Ê®°ÂûãÊõ¥Êñ∞

update"
github.com/Eric3911/OpenAGI,NeMo-master/nemo/collections/asr/parts/utils/transcribe_utils.py,2023-05-06T06:44:54Z,"Ê®°ÂûãÊõ¥Êñ∞

update"
github.com/Eric3911/OpenAGI,NeMo-master/scripts/asr_language_modeling/ngram_lm/train_kenlm.py,2023-05-06T06:44:54Z,"Ê®°ÂûãÊõ¥Êñ∞

update"
github.com/Eric3911/OpenAGI,NeMo-master/tools/ctc_segmentation/scripts/run_ctc_segmentation.py,2023-05-06T06:44:54Z,"Ê®°ÂûãÊõ¥Êñ∞

update"
github.com/Eric3911/OpenAGI,NeMo-master/nemo/collections/asr/parts/utils/decoder_timestamps_utils.py,2023-05-06T06:44:54Z,"Ê®°ÂûãÊõ¥Êñ∞

update"
github.com/Eric3911/OpenAGI,NeMo-master/scripts/asr_language_modeling/ngram_lm/eval_beamsearch_ngram.py,2023-05-06T06:44:54Z,"Ê®°ÂûãÊõ¥Êñ∞

update"
github.com/Eric3911/OpenAGI,NeMo-master/examples/nlp/duplex_text_normalization/data/create_tarred_dataset.py,2023-05-06T06:44:54Z,"Ê®°ÂûãÊõ¥Êñ∞

update"
github.com/Eric3911/OpenAGI,NeMo-master/nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py,2023-05-06T06:44:54Z,"Ê®°ÂûãÊõ¥Êñ∞

update"
github.com/Eric3911/OpenAGI,NeMo-master/scripts/asr_language_modeling/ngram_lm/eval_beamsearch_ngram_transducer.py,2023-05-06T06:44:54Z,"Ê®°ÂûãÊõ¥Êñ∞

update"
github.com/Eric3911/OpenAGI,NeMo-master/examples/asr/asr_cache_aware_streaming/speech_to_text_cache_aware_streaming_infer.py,2023-05-06T06:44:54Z,"Ê®°ÂûãÊõ¥Êñ∞

update"
github.com/Eric3911/OpenAGI,NeMo-master/nemo/collections/nlp/models/token_classification/punctuation_capitalization_lexical_audio_model.py,2023-05-06T06:44:54Z,"Ê®°ÂûãÊõ¥Êñ∞

update"
github.com/Eric3911/OpenAGI,NeMo-master/tests/core/test_save_restore.py,2023-05-06T06:44:54Z,"Ê®°ÂûãÊõ¥Êñ∞

update"
github.com/Eric3911/OpenAGI,NeMo-master/tests/manualtest_model_downloads.py,2023-05-06T06:44:54Z,"Ê®°ÂûãÊõ¥Êñ∞

update"
github.com/Eric3911/OpenAGI,NeMo-master/tests/collections/nlp/test_pretrained_models_performance.py,2023-05-06T06:44:54Z,"Ê®°ÂûãÊõ¥Êñ∞

update"
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2024-02-27T11:11:47Z,"[Update] An example for Gemma model was initially added.
[Update] An example for Transformer Reinforcement Learning (TRL) library to train small LLMs (sLLMs) was initially added."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2024-02-21T12:45:57Z,[Update] Examples for DINO and DPT models were implemented in hugging_face_transformers_test.py.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2024-02-16T06:16:19Z,[Update] Placeholders for examples of Hugging Face TRL library were added.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2024-02-08T03:52:31Z,"[Update] Several examples for stable diffusion models of Stability AI were implemented.
[Update] Information about Transformer Reinforcement Learning (TRL) library was moved from hugging_face_test.py to hugging_face_transformers_test.py."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2024-01-31T08:31:27Z,[Update] A test was added to fine-tune a ResNet model using Dog/Cat dataset in pytorch_transfer_learning.py.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2024-01-28T13:07:24Z,"[Update] An example of Zephyr-7B was added in hugging_face_transformers_test.py, but was not tested.
[Update] The information about Transformer Reinforcement Learning (TRL) was described in hugging_face_test.py."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2024-01-11T04:04:23Z,"[Update] An example for retrieval-augmented generation (RAG) was added in hugging_face_transformers_test.py.
[Update] Information about TableMASTER-mmocr was removed from mmocr_usage_guide.txt."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-12-24T02:40:32Z,"[Update] A few examples for Mistral-7B and Mixtral-8x7B models were added.
[Update] The installation of TensorFlow 2 were updated."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-12-23T12:07:02Z,"[Update] An example of ORCA-2 model was added.
[Update] The examples for ViT, ViLT, BEiT, LayoutLM, and Donut models were merged respectively.
[Update] The information about data preparation, training, evaluation, visualization, and model export were supplemented in paddle_ocr_usage_guide.txt."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-11-24T08:49:41Z,"[Update] A customized version of ViT model was implemented and tested, which doesn't have classification token and head."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-11-20T04:48:59Z,"[Update] A few examples for Falcon model were initially implemented.
[Update] A few examples for StarCoder and Replit models were implemented, but yet tested."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-11-10T01:27:50Z,[Update] LLMs Yi-6B & Yi-34B were tested.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-11-06T12:24:53Z,"[Update] The forecasts of transformers.TimeSeriesTransformerForPrediction were evaluated by MASE, MAPE, sMAPE.
[Update] Metrics for evaluating ML models' performance in evaluate library were tested."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-11-03T10:46:04Z,"[Update] Two examples for phi-1 & phi-1.5 models were initially added.
[Update] A few examples for Kosmos-2 model were initially added."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-10-12T12:13:43Z,[Update] A few examples for Probabilistic time series transformer model were added.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-10-07T08:51:44Z,[Update] Several examples for Perceiver IO model were initially committed.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-09-26T10:56:44Z,"[Update] A simple example for Code Llama model was initially added.
[Update] A few commands were explained to profile Python scripts."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-09-21T12:31:58Z,[Update] A simple example for OpenLLaMA models was implemented.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-09-04T03:59:25Z,"[New] A few examples for PID, MPC, LQR using python-control library was initially added.
[Update] A simple example for trajectory transformer models was added in hugging_face_transformers_test.py."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-09-02T12:30:02Z,"[New] An example for model predictive control (MPC) was initially committed.
[Update] A test for CodeParrot model was initially implemented in hugging_face_transformers_test.py."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-08-31T06:01:24Z,"[Update] Memory footprint and computing performance (FLOPS) Hugging Face transformers models was measured.
[Update] A simple test for Hugging Face datasets was added."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-08-04T08:13:21Z,[Update] An example for OpenFlamingo library was added.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-07-25T12:07:07Z,"[Update] A few examples for Llama 2 model were added in hugging_face_transformers_test.py.
[Update] Model parallelism was tested based on Hugging Face Accelerate library in hugging_face_transformers_test.py.
[Update] Information about Hugging Face Accelerate library was reinforced."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-07-22T06:48:12Z,"[Update] Cross references about transformer and ViT models were reinforced.
[Chore] vit_test.py was moved from sw_dev/python/rnd/test/machine_learning/vit_test.py to sw_dev/python/rnd/test/machine_vision/vit_test.py.
[Update] The installation of node.js was explained."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-07-17T01:20:26Z,"[Update] Several examples for CodeBERT, CodeBERTa, CodeT5+, CodeGen2, & CodeGen2.5 models were added.
[Update] An example of SpeechT5 model was divied into 3 examples: ASR, TTS, & speech-to-speech."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-07-12T07:54:25Z,"[Update] An example for Decision Transformer was initially committed.
[Update] A few examples for NVIDIA Megatron-LM, ASR, TTS models were implemented.
[Update] An example for SegFormer model was initially added."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-06-30T14:20:26Z,"[Update] The method, chain of thoughts (CoT) was tested on two LLMs, LLaMA & MPT."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-06-27T04:15:37Z,"[Update] A few examples for MPT & TVLT models were implemented in hugging_face_transformers_test.py.
[Chore] table_generation_usage_guide.txt was moved to SWLP repository."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-06-22T14:49:33Z,"[Update] Three language models were tested: LLaMA, Galactica, & OPT models."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-06-15T07:51:37Z,[Update] The example of LLaMA model was reinforced.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-05-26T18:49:16Z,[Update] A test for data parallelism was implemented in PyTorch library.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-05-05T03:23:29Z,"[Update] Two simple tests for two Facebook's language models, OPT and Galactica were initially implemented."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-04-30T14:34:59Z,[New] A simple tutorial for MMSegmentation library was initially committed.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-04-06T03:27:45Z,"[Update] A couple of simple examples for LLaMa model were implemented, but they were not tested due to library version issue."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-04-04T11:52:41Z,"[Update] Several examples for CodeT5 and CodeGen models were implemented to generate code.
[Update] Several examples for GIT and BLIP models for vision-and-language modeling models were implemented.
[Update] A few examples for TaPEx model were implemented to understand tables."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-04-01T14:10:48Z,"[New] A couple of examples for GPT4All models were added, but they were not correctly working."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-04-01T06:26:08Z,"[New] Two simple examples for PaLM and PaLM+RLHF models were implemented, but the models were not trained in the examples."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-30T08:06:47Z,"[New] Several examples for table processing (Table Transformer, TATR), OCR (TrOCR), speech processing (SpeechT5) were initially added to hugging_face_transformers_test.py."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-29T05:25:41Z,[Update] Several examples for speech recognition (whisper) & synthesis (tacotron2 & fastspeech2) were newly implemented in hugging_face_transformers_test.py.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-26T07:02:13Z,"[New] A few examples for ALIGN model were implemented in hugging_face_transformers_test.py.
[Update] A couple of examples for CLIP model were added in hugging_face_transformers_test.py.
[Update] Useful information about Transformer architectures was described."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-24T06:24:31Z,[Update] A simple example about dataclass in Python was added.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-22T14:48:31Z,"[Update] A couple of examples of seq-to-seq models for LoRA & Prefix Tuning were implemented in hugging_face_test.py.
[Update] An example for CLIP model was implemented in hugging_face_transformers_test.py.
[Update] A few examples for Whisper model were implemented in hugging_face_transformers_test.py.
[New] A usage guide for Hugging Face library was added."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-21T07:10:38Z,"[Update] A simple example of Parameter-Efficient Fine-Tuning (PEFT) library was added to hugging_face_test.py.
[Update] A simple test of tokenizers was added to hugging_face_transformers_test.py."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-18T11:41:29Z,[Update] A couple of examples for diffusion models of StabilityAI and CompVis were added.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-18T05:09:16Z,[Update] A few examples for Flan-T5 model were reinforced in hugging_face_transformers_test.py.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-16T11:52:13Z,[Update] Information about Hugging Face models was supplemented.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-15T12:08:24Z,[Update] An example for question answering using GPT-neo was added.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-14T02:43:49Z,[Update] A few examples for BLOOM and Flan-T5 models were implemented.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-09T01:51:43Z,[Update] A test for KLUE BERT models was implemented.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-09T01:49:32Z,"[Update] A few examples of text summarization for Korean & English were implemented, but their results were not good.
[Update] A few examples for T5 model were added."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-07T17:44:21Z,[Update] A few tests were added for GPT & BERT models.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-01T14:37:43Z,[New] A few guides were initially committed for Hugging Face Hub library.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-01-19T10:11:50Z,"[Update] Several examples for vision, vision and language models were added in HuggingFace library."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-01-19T08:06:44Z,[Update] Several examples for LayoutLM & Donut models were implemented in HugggingFace library.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2022-04-01T07:00:58Z,[Chore] transformers_test.py was renamed to hugging_face_transformers_test.py.
github.com/nvidia-riva/nemo2riva,tests/test_cli.py,2022-12-19T21:29:15Z,"reorganizing for release

Signed-off-by: Boris Fomitchev <bfomitchev@nvidia.com>"
