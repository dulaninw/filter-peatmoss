repo_url,filepath,commit_date,message
github.com/huggingface/transformers,src/transformers/trainer.py,2024-03-01T17:00:29Z,"Fix deprecated arg issue (#29372)

* Fix deprecated arg issue

* Trainer check too

* Check for dict or dataclass

* Simplify, make config always AcceleratorConfig

* Upstream to Trainer"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-02-26T10:35:37Z,"Add feature extraction mapping for automatic metadata update (#28944)

* add feature extraction mapping

* added prefix

* ruff check

* minor fix

* Update modeling_auto.py

* fix typo

* remove prefix to make variable public/importable

* Update src/transformers/models/auto/modeling_auto.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* fixes

* addressed comments

* nit

* fix-copies

* remove from tests

* this should fix

* Update tests/models/convnextv2/test_modeling_convnextv2.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* nits

---------

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-02-20T11:45:08Z,"FIX [`PEFT` / `Trainer` ] Handle better peft + quantized compiled models (#29055)

* handle peft + compiled models

* add tests

* fixup

* adapt from suggestions

* clarify comment"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-02-20T01:43:02Z,"FEAT [`Trainer` / `bnb`]: Add RMSProp from `bitsandbytes` to HF `Trainer` (#29082)

* add RMSProp to Trainer

* revert some change

* Update src/transformers/trainer.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

---------

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-02-19T19:07:41Z,"storing & logging gradient norm in trainer (#27326)

* report grad_norm during training

* support getting grad_norm from deepspeed"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-02-16T12:41:09Z,"`auto_find_batch_size` isn't yet supported with DeepSpeed/FSDP. Raise error accrodingly. (#29058)

Update trainer.py"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-02-14T22:56:35Z,"FIX [`Trainer` / tags]: Fix trainer + tags when users do not pass `""tags""` to `trainer.push_to_hub()` (#29009)

* fix trainer tags

* add test"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-02-14T21:44:49Z,"[TPU] Support PyTorch/XLA FSDP via SPMD (#28949)

* Initial commit

* Add guards for the global mesh

* Address more comments

* Move the dataloader into integrations/tpu.py

* Fix linters

* Make karg more explicitly

* Remove the move device logic

* Fix the CI

* Fix linters

* Re-enable checkpointing"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-02-14T15:18:09Z,"Introduce AcceleratorConfig dataclass (#28664)

* Introduce acceleratorconfig dataclass

* Extra second warn

* Move import

* Try moving import under is_accelerate_available

* Quality

* Apply suggestions from code review

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Clean

* Remove to_kwargs

* Change version

* Improve tests by including dispatch and split batches

* Improve reliability

* Update tests/trainer/test_trainer.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Fixup tests and review nits

* Make tests pass

* protect import

* Protect import

* Empty-Commit

* Make training_args.to_dict handle the AcceleratorConfig

---------

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-02-14T00:30:23Z,"ENH [`AutoQuantizer`]: enhance trainer + not supported quant methods (#28991)

* enhance trainer + not support quant methods

* remove all old logic

* add version"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-02-12T15:47:21Z,"Clean up staging tmp checkpoint directory (#28848)

clean up remaining tmp checkpoint dir

Signed-off-by: woshiyyya <xiaoyunxuan1998@gmail.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-02-06T05:55:44Z,"Raise error when using `save_only_model` with `load_best_model_at_end` for DeepSpeed/FSDP (#28866)

* Raise error when using `save_only_model` with `load_best_model_at_end` for DeepSpeed/FSDP

* Update trainer.py"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-02-06T02:21:50Z,"Do not use mtime for checkpoint rotation. (#28862)

Resolve https://github.com/huggingface/transformers/issues/26961"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-02-02T15:48:01Z,"Reduce GPU memory usage when using FSDP+PEFT (#28830)

support FSDP+PEFT"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-01-31T12:58:26Z,"Resolve DeepSpeed cannot resume training with PeftModel (#28746)

* fix: resolve deepspeed resume peft model issues

* chore: update something

* chore: update model instance pass into is peft model checks

* chore: remove hard code value to tests

* fix: format code"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-01-29T17:10:15Z,"Support saving only PEFT adapter in checkpoints when using PEFT + FSDP (#28297)

* Update trainer.py

* Revert ""Update trainer.py""

This reverts commit 0557e2cc9effa3a41304322032239a3874b948a7.

* Make trainer.py use adapter_only=True when using FSDP + PEFT

* Support load_best_model with adapter_only=True

* Ruff format

* Inspect function args for save_ load_ fsdp utility functions and only pass adapter_only=True if they support it"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-01-26T12:00:49Z,"Fix `weights_only` (#28725)

fix

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-01-26T11:05:01Z,"support PeftMixedModel signature inspect (#28321)

* support PeftMixedModel signature inspect

* import PeftMixedModel only peft>=0.7.0

* Update src/transformers/trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update src/transformers/trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update src/transformers/trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update src/transformers/trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update src/transformers/trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update src/transformers/trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* fix styling

* Update src/transformers/trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update src/transformers/trainer.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* style fixup

* fix note

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>
Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-01-25T09:34:52Z,"[`chore`] Add missing space in warning (#28695)

Add missing space in warning"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-01-24T11:57:45Z,"Use save_safetensor to disable safe serialization for XLA (#28669)

* Use save_safetensor to disable safe serialization for XLA

https://github.com/huggingface/transformers/issues/28438

* Style fixup"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-01-23T15:08:18Z,"add dataloader prefetch factor in training args and trainer (#28498)

* add dataloader prefetch factor in training args and trainer

* remove trailing spaces

* prevent dataloader_num_workers == 0 and dataloader_prefetch_factor != None

dataloader_prefetch_factor works only when data is loaded in a different process as the main one. This commit adds the necessary checks to avoid having prefetch_factor set when there is no such process.

* Remove whitespaces in empty line

* Update src/transformers/training_args.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/training_args.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/training_args.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/training_args.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

---------

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-01-23T13:30:36Z,"Fix windows err with checkpoint race conditions (#28637)

Fix windows err"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-01-18T10:55:29Z,"Use `weights_only` only if torch >= 1.13 (#28506)

* fix

* fix

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-01-15T13:48:07Z,"[`core`/ FEAT] Add the possibility to push custom tags using `PreTrainedModel` itself (#28405)

* v1 tags

* remove unneeded conversion

* v2

* rm unneeded warning

* add more utility methods

* Update src/transformers/utils/hub.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/utils/hub.py

Co-authored-by: Lucain <lucainp@gmail.com>

* Update src/transformers/utils/hub.py

Co-authored-by: Lucain <lucainp@gmail.com>

* more enhancements

* oops

* merge tags

* clean up

* revert unneeded change

* add extensive docs

* more docs

* more kwargs

* add test

* oops

* fix test

* Update src/transformers/modeling_utils.py

Co-authored-by: Omar Sanseviero <osanseviero@gmail.com>

* Update src/transformers/utils/hub.py

Co-authored-by: Lucain <lucainp@gmail.com>

* Update src/transformers/modeling_utils.py

* Update src/transformers/trainer.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/modeling_utils.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* add more conditions

* more logic

---------

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>
Co-authored-by: Lucain <lucainp@gmail.com>
Co-authored-by: Omar Sanseviero <osanseviero@gmail.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-01-11T15:18:27Z,"Byebye torch 1.10 (#28207)

* fix

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-01-10T15:55:42Z,"Fix for checkpoint rename race condition (#28364)

* Changed logic for renaming staging directory when saving checkpoint to only operate with the main process.
Added fsync functionality to attempt to flush the write changes in case os.rename is not atomic.

* Updated styling using make fixup

* Updated check for main process to use built-in versions from trainer

Co-authored-by: Zach Mueller <muellerzr@gmail.com>

* Fixed incorrect usage of trainer main process checks
Added with open usage to ensure better file closing as suggested from PR
Added rotate_checkpoints into main process logic

* Removed ""with open"" due to not working with directory. os.open seems to work for directories.

---------

Co-authored-by: Zach Mueller <muellerzr@gmail.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-01-10T11:03:13Z,"Support `DeepSpeed` when using auto find batch size (#28088)

Fixup test"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-01-02T14:19:42Z,"fix bug:divide by zero in _maybe_log_save_evaluate() (#28251)

Co-authored-by: liujizhong1 <liujizhong1@xiaomi.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2024-01-02T12:58:29Z,"Fix trainer saving safetensors: metadata is None (#28219)

* Update trainer.py

* format"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-12-20T09:55:56Z,"move code to Trainer.evaluate to enable use of that function with multiple datasets (#27844)

* move code to Trainer.evaluate to enable use of that function with multiple datasets

* test

* update doc string

* and a tip

* forgot the type

---------

Co-authored-by: Prof. Peter Schneider-Kamp <jps@ordbogen.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-12-19T10:08:51Z,"When save a model on TPU, make a copy to be moved to CPU (#27993)

* When save a model, make a copy to be moved to CPU, dont move the original
model

* make deepcopy inside of _save_tpu

* Move to tpu without copy"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-12-18T14:27:05Z,"in peft finetune, only the trainable parameters need to be saved (#27825)

to reduce the storage size and also save the time of checkpoint saving while using deepspeed for training

Signed-off-by: Wang, Yi <yi.a.wang@intel.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-12-16T14:11:43Z,"fix resuming from ckpt when using FSDP with FULL_STATE_DICT (#27891)

* fix resuming from ckpt when suing FSDP with FULL_STATE_DICT

* update tests

* fix tests"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-12-15T16:18:56Z,"Fix bug for checkpoint saving on multi node training setting (#28078)

* add multi-node traning setting

* fix style"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-12-15T15:01:18Z,"make torch.load a bit safer (#27282)

* make torch.load a bit safer

* Fixes

---------

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-12-13T17:17:30Z,"Fix bug with rotating checkpoints (#28009)

* Fix bug

* Write test

* Keep back old modification for grad accum steps

* Whitespace...

* Whitespace again

* Race condition

* Wait for everyone"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-12-11T19:30:11Z,"Support PeftModel signature inspect (#27865)

* Support PeftModel signature inspect

* Use get_base_model() to get the base model

---------

Co-authored-by: shujunhua1 <shujunhua1@jd.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-12-09T10:04:13Z,"[integration] Update Ray Tune integration for Ray 2.7 (#26499)

* fix tune integration for ray 2.7+

Signed-off-by: Justin Yu <justinvyu@anyscale.com>

* add version check for ray tune backend availability

Signed-off-by: Justin Yu <justinvyu@anyscale.com>

* missing import

Signed-off-by: Justin Yu <justinvyu@anyscale.com>

* pin min version instead

Signed-off-by: Justin Yu <justinvyu@anyscale.com>

* address comments

Signed-off-by: Justin Yu <justinvyu@anyscale.com>

* some fixes

Signed-off-by: Justin Yu <justinvyu@anyscale.com>

* fix unnecessary final checkpoint

Signed-off-by: Justin Yu <justinvyu@anyscale.com>

* fix lint

Signed-off-by: Justin Yu <justinvyu@anyscale.com>

* dep table fix

Signed-off-by: Justin Yu <justinvyu@anyscale.com>

* fix lint

Signed-off-by: Justin Yu <justinvyu@anyscale.com>

---------

Signed-off-by: Justin Yu <justinvyu@anyscale.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-12-08T16:51:02Z,"Allow `resume_from_checkpoint` to handle `auto_find_batch_size` (#27568)

* Fuffill request

* Add test

* Better test

* Apply suggestions from code review

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>
Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Better test

* Better test

* MOre comments

---------

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>
Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-12-08T13:08:54Z,fix: non-atomic checkpoint save (#27820)
github.com/huggingface/transformers,src/transformers/trainer.py,2023-12-07T12:06:02Z,"update `create_model_card` to properly save peft details when using Trainer with PEFT (#27754)

* update `create_model_card` to properly save peft details when using Trainer with PEFT

* nit

* Apply suggestions from code review

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-12-04T13:56:00Z,"[Hot-Fix][XLA] Re-enable broken _tpu_save for XLATensors (#27799)

* [XLA] Re-enable broken _tpu_save for XLATensors, by explicitly moving to cpu

* linter-fix"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-12-04T06:43:32Z,"Add `persistent_workers` parameter to `TrainingArguments` (#27189)

added param

Co-authored-by: Ilya Fedorov <ilyaf@nvidia.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-11-28T16:40:44Z,"Docs: Fix broken cross-references, i.e. `~transformer.` -> `~transformers.` (#27740)

~transformer. -> ~transformers."
github.com/huggingface/transformers,src/transformers/trainer.py,2023-11-28T07:33:45Z,"Fixed passing scheduler-specific kwargs via TrainingArguments lr_scheduler_kwargs (#27595)

* Fix passing scheduler-specific kwargs through TrainingArguments `lr_scheduler_kwargs`

* Added test for lr_scheduler_kwargs"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-11-27T16:26:33Z,"docs: replace torch.distributed.run by torchrun (#27528)

* docs: replace torch.distributed.run by torchrun

 `transformers` now officially support pytorch >= 1.10.
 The entrypoint `torchrun`` is present from 1.10 onwards.

Signed-off-by: Peter Pan <Peter.Pan@daocloud.io>

* Update src/transformers/trainer.py

with @ArthurZucker's suggestion

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

---------

Signed-off-by: Peter Pan <Peter.Pan@daocloud.io>
Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-11-24T06:10:52Z,"Refactoring Trainer, adds `save_only_model` arg and simplifying FSDP integration (#27652)

* add code changes

1. Refactor FSDP
2. Add `--save_only_model` option: When checkpointing, whether to only save the model, or also the optimizer, scheduler & rng state.
3. Bump up the minimum `accelerate` version to `0.21.0`

* quality

* fix quality?

* Revert ""fix quality?""

This reverts commit 149330a6abc078827be274db84c8a2d26a76eba1.

* fix fsdp doc strings

* fix quality

* Update src/transformers/training_args.py

Co-authored-by: Zach Mueller <muellerzr@gmail.com>

* please fix the quality issue 😅

* Apply suggestions from code review

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* address comment

* simplify conditional check as per the comment

* update documentation

---------

Co-authored-by: Zach Mueller <muellerzr@gmail.com>
Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-11-21T16:09:35Z,"remove the deprecated method `init_git_repo` (#27617)

* remove deprecated method `init_git_repo`

* make style"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-11-14T20:31:04Z,"Track the number of tokens seen to metrics (#27274)

* Add tokens seen

* Address comments, add to TrainingArgs

* Update log

* Apply suggestions from code review

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Use self.args

* Fix docstring

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

---------

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-11-14T19:54:44Z,"Have seq2seq just use gather (#27025)

* Have seq2seq just use gather

* Change

* Reset after

* Make slow

* Apply suggestions from code review

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Clean

* Simplify and just use gather

* Update tests/trainer/test_trainer_seq2seq.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* gather always for seq2seq

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>
Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-11-07T21:40:00Z,"Allow scheduler parameters (#26480)

* Allow for scheduler kwargs

* Formatting

* Arguments checks, passing the tests

* Black failed somehow

---------

Co-authored-by: Pierre <pierre@avatarin.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-11-02T20:08:03Z,"Fixed base model class name extraction from PeftModels (#27162)

* Fixed base model class name extraction from PeftModels

* Changes to first unwrap the model then extract the base model name

* Changed base_model to base_model.model to stay consistent with peft model abstractions"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-11-02T10:27:13Z,"Reproducible checkpoint for npu (#27208)

* save NPU's RNG states when saving a checkpoint and set after all the
data skip phase when resuming training.

* re-trigger ci

* re-trigger ci"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-11-01T18:42:38Z,"Enable split_batches through TrainingArguments (#26798)

* Enable split_batches through TrainingArguments

* Extra dispatch_batches

* Keep as default false

* Add to docstring

* Add to docstring

* Remove the capturewarnings change

* Comma"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-10-31T15:03:59Z,"[FEAT] Add Neftune into transformers Trainer (#27141)

* add v1 neftune

* use `unwrap_model` instead

* add test + docs

* Apply suggestions from code review

Co-authored-by: Zach Mueller <muellerzr@gmail.com>

* more details

* fixup

* Update docs/source/en/main_classes/trainer.md

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* refactor a bit

* more elaborated test

* fix unwrap issue

---------

Co-authored-by: Zach Mueller <muellerzr@gmail.com>
Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-10-30T11:55:03Z,"remove the obsolete code related to fairscale FSDP (#26651)

* remove the obsolete code related to fairscale FSDP

* apple review suggestion"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-10-30T11:41:48Z,"[`Trainer` / `GC`] Add `gradient_checkpointing_kwargs` in trainer and training arguments (#27068)

* add `gradient_checkpointing_kwargs` in trainer and training arguments

* add comment

* add test - currently failing

* now tests pass"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-10-26T16:13:19Z,"Save TB logs as part of push_to_hub (#27022)

* Support runs/

* Upload runs folder as part of push to hub

* Add a test

* Add to test deps

* Update with proposed solution from Slack

* Ensure that repo gets deleted in tests"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-10-26T15:46:17Z,"Correct docstrings and a typo in comments (#27047)

* docs(training_args): correct docstrings

Correct docstrings of these methods in `TrainingArguments`:

- `set_save`
- `set_logging`

* docs(training_args): adjust words in docstrings

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* docs(trainer): correct a typo in comments

---------

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-10-26T09:20:11Z,"Bring back `set_epoch` for Accelerate-based dataloaders (#26850)

* Working tests!

* Fix sampler

* Fix

* Update src/transformers/trainer.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Fix check

* Clean

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-10-16T13:29:47Z,"fix resume_from_checkpoint bug (#26739)

* fix resume_from_checkpoint bug

* update code"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-10-12T08:28:40Z,"Add many missing spaces in adjacent strings (#26751)

Add missing spaces in adjacent strings"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-10-06T14:03:11Z,"remove SharedDDP as it is deprecated (#25702)

* remove SharedDDP as it was drepracated

* apply review suggestion

* make style

* Oops,forgot to remove the compute_loss context manager in Seq2SeqTrainer.

* remove the unnecessary conditional statement

* keep the logic of IPEX

* clean code

* mix precision setup & make fixup

---------

Co-authored-by: statelesshz <jihuazhong1@huawei.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-10-04T13:13:37Z,"Docstring check (#26052)

* Fix number of minimal calls to the Hub with peft integration

* Alternate design

* And this way?

* Revert

* Nits to fix

* Add util

* Print when changes are made

* Add list to ignore

* Add more rules

* Manual fixes

* deal with kwargs

* deal with enum defaults

* avoid many digits for floats

* Manual fixes

* Fix regex

* Fix regex

* Auto fix

* Style

* Apply script

* Add ignored list

* Add check that templates are filled

* Adding to CI checks

* Add back semi-fix

* Ignore more objects

* More auto-fixes

* Ignore missing objects

* Remove temp semi-fix

* Fixes

* Update src/transformers/models/pvt/configuration_pvt.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update utils/check_docstrings.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update src/transformers/utils/quantization_config.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Deal with float defaults

* Fix small defaults

* Address review comment

* Treat

* Post-rebase cleanup

* Address review comment

* Update src/transformers/models/deprecated/mctct/configuration_mctct.py

Co-authored-by: Lysandre Debut <lysandre.debut@reseau.eseo.fr>

* Address review comment

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre.debut@reseau.eseo.fr>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-10-04T12:57:11Z,Extend Trainer to enable Ascend NPU to use the fused Adamw optimizer when training (#26194)
github.com/huggingface/transformers,src/transformers/trainer.py,2023-09-26T17:27:09Z,"Add torch `RMSProp` optimizer (#26425)

add rmsprop"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-09-22T01:33:29Z,"[QUICK FIX LINK] Update trainer.py (#26293)

* Update trainer.py

Fix link

* Update src/transformers/trainer.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update trainer.py

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-09-20T15:38:59Z,"[`Trainer`] Refactor trainer + bnb logic (#26248)

* refactor trainer + bnb logic

* remove logger.info

* oops"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-09-20T04:56:16Z,"FSDP tests and checkpointing fixes (#26180)

* add fsdp tests

* Update test_fsdp.py

* Update test_fsdp.py

* fixes

* checks

* Update trainer.py

* fix

* fixes for saving/resuming checkpoints

* fixes

* add tests and delete debug statements

* fixing tests

* Update test_fsdp.py

* fix tests

* fix tests

* minor nits

* fix code style and quality

* refactor and modularize test code

* reduce the time of tests

* reduce the test time

* fix test

* reduce test time

* reduce test time

* fix failing tests

* fix

* Apply suggestions from code review

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* resolve comments

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-09-18T15:40:11Z,refactor decay_parameters production into its own function (#26152)
github.com/huggingface/transformers,src/transformers/trainer.py,2023-09-14T09:57:47Z,"Fix eval accumulation when `accelerate` > 0.20.3 (#26060)

As mentioned in: https://github.com/huggingface/transformers/issues/25641

Eval accumulation will never happen with `accelerate > 0.20.3`, so this change ensures that `sync_gradients` is ignored if accelerate is > 0.20.3"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-09-12T17:01:22Z,"enable optuna multi-objectives feature (#25969)

* enable optuna multi-objectives feature

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

* Apply suggestions from code review

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* update hpo doc

* update docstring

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

* extend direction to List[str] type

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

* Update src/transformers/integrations/integration_utils.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

---------

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>
Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>
Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-09-11T11:56:36Z,"only main process should call _save on deepspeed zero3 (#25959)

only main process should call _save when deepspeed zero3"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-09-07T19:00:22Z,"Try to fix training Loss inconsistent after resume from old checkpoint (#25872)

* fix loss inconsistent after resume  #25340

* fix typo

* clean code

* reformatted code

* adjust code according to comments

* adjust check_dataloader_randomsampler location

* return sampler only

* handle sampler is None

* Update src/transformers/trainer_pt_utils.py

thanks @amyeroberts

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

---------

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-09-07T16:17:30Z,"Add `tgs` speed metrics (#25858)

* Add tgs metrics

* bugfix and black formatting

* workaround for tokens counting

* formating and bugfix

* Fix

* Add opt-in for tgs metrics

* make style and fix error

* Fix doc

* fix docbuild

* hf-doc-build

* fix

* test

* Update src/transformers/training_args.py

renaming

Co-authored-by: Zach Mueller <muellerzr@gmail.com>

* Update src/transformers/training_args.py

renaming

Co-authored-by: Zach Mueller <muellerzr@gmail.com>

* Fix some symbol

* test

* Update src/transformers/trainer_utils.py

match nameing patterns

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/training_args.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/trainer.py

nice

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Fix reviews

* Fix

* Fix black

---------

Co-authored-by: Zach Mueller <muellerzr@gmail.com>
Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-09-07T04:22:53Z,"Fix err with FSDP (#25991)

* Fix err

* Use version check"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-09-05T17:01:20Z,"deepspeed resume from ckpt fixes and adding support for deepspeed optimizer and HF scheduler (#25863)

* Add support for deepspeed optimizer and HF scheduler

* fix bug

* fix the import

* fix issue with deepspeed scheduler saving for hf optim + hf scheduler scenario

* fix loading of hf scheduler when loading deepspeed checkpoint

* fix import of `DeepSpeedSchedulerWrapper`

* add tests

* add the comment and skip the failing tests

* address comment"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-09-01T15:24:12Z,"Revert frozen training arguments (#25903)

* Revert frozen training arguments

* TODO"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-09-01T09:50:42Z,"fix FSDP model resume optimizer & scheduler (#25852)

* fix FSDP resume optimizer & scheduler

* improve trainer code quality

---------

Co-authored-by: machi04 <machi04@meituan.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-08-31T09:47:53Z,"fix ds z3 checkpointing when  `stage3_gather_16bit_weights_on_model_save=False` (#25817)

* fix ds z3 checkpointing when  `stage3_gather_16bit_weights_on_model_save=False`

* refactoring"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-08-29T14:06:41Z,"Error with checking args.eval_accumulation_steps to gather tensors (#25819)

* Update trainer.py (error with checking steps in args.eval_accumulation_steps to gather tensors)

While the deprecated code has the correct check (line 3772): 
""if args.eval_accumulation_steps is not None and (step + 1) % args.eval_accumulation_steps == 0:""

The current code does not (line 3196):
""if args.eval_accumulation_steps is not None and self.accelerator.sync_gradients:""

We need to check ""(step + 1) % args.eval_accumulation_steps == 0"". Hence, the line 3196 should be modified to:
""if args.eval_accumulation_steps is not None and (step + 1) % args.eval_accumulation_steps == 0 and self.accelerator.sync_gradients:""

* Fix error with checking args.eval_accumulation_steps to gather tensors"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-08-29T07:22:14Z,"Arde/fsdp activation checkpointing (#25771)

* add FSDP config option to enable activation-checkpointing

* update docs

* add checks and remove redundant code

* fix formatting error"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-08-25T15:13:34Z,"🚨🚨🚨 [`Refactor`] Move third-party related utility files into `integrations/` folder 🚨🚨🚨 (#25599)

* move deepspeed to `lib_integrations.deepspeed`

* more refactor

* oops

* fix slow tests

* Fix docs

* fix docs

* addess feedback

* address feedback

* final modifs for PEFT

* fixup

* ok now

* trigger CI

* trigger CI again

* Update docs/source/en/main_classes/deepspeed.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* import from `integrations`

* address feedback

* revert removal of `deepspeed` module

* revert removal of `deepspeed` module

* fix conflicts

* ooops

* oops

* add deprecation warning

* place it on the top

* put `FutureWarning`

* fix conflicts with not_doctested.txt

* add back `bitsandbytes` module with a depr warning

* fix

* fix

* fixup

* oops

* fix doctests

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-08-18T17:08:03Z,"[`PEFT`] Peft integration alternative design  (#25077)

* a draft version

* v2 integration

* fix

* make it more generic and works for IA3

* add set adapter and multiple adapters support

* fixup

* adapt a bit

* oops

* oops

* oops

* adapt more

* fix

* add more refactor

* now works with model class

* change it to instance method as it causes issues with `jit`.

* add CR

* change method name

* add `add_adapter` method

* clean up

* Update src/transformers/adapters/peft_mixin.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* add moe utils

* fixup

* Update src/transformers/adapters/peft_mixin.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* adapt

* oops

* fixup

* add is_peft_available

* remove `requires_backend`

* trainer compatibility

* fixup + docstring

* more details

* trigger CI

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/modeling_utils.py

* fixup + is_main_process

* added `save_peft_format` in save_pretrained

* up

* fix nits here and there

* nits here and there.

* docs

* revert `encoding=""utf-8""`

* comment

* added slow tests before the PEFT release.

* fixup and nits

* let's be on the safe zone

* added more comments

* v1 docs

* add remaining docs

* Apply suggestions from code review

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* move to `lib_integrations`

* fixup

* this time fixup

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* address final comments

* refactor to use `token`

* add PEFT to DockerFile for slow tests.

* added pipeline support.

---------

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-08-17T18:48:58Z,"add warning for 8bit optimizers (#25575)

* add warning for 8bit optimizers

* protect import"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-08-17T16:23:34Z,"add util for ram efficient loading of model when using fsdp (#25107)

* add util for ram efficient loading of model when using fsdp

* make fix-copies

* fixes 😅

* docs

* making it further easier to use

* rename the function

* refactor to handle fsdp ram efficiency in `from_pretrained`

* fixes

* fixes

* fixes

* update

* fixes

* revert `load_pretrained_model_only_on_rank0`

* resolve `load_from_checkpoint`"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-08-17T15:44:01Z,"Revert ""change version (#25387)"" (#25573)

This reverts commit 3a05e010e0c7e8abd3e5357dd4e89e28cc69003e."
github.com/huggingface/transformers,src/transformers/trainer.py,2023-08-17T06:10:33Z,Update trainer.py (#25553)
github.com/huggingface/transformers,src/transformers/trainer.py,2023-08-16T16:19:51Z,More frozen args (#25540)
github.com/huggingface/transformers,src/transformers/trainer.py,2023-08-10T20:06:29Z,"GPTQ integration (#25062)

* GTPQ integration

* Add tests for gptq

* support for more quantization model

* fix style

* typo

* fix method

* Update src/transformers/modeling_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* add dataclass and fix quantization_method

* fix doc

* Update tests/quantization/gptq/test_gptq.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* modify dataclass

* add gtpqconfig import

* fix typo

* fix tests

* remove dataset as req arg

* remove tokenizer import

* add offload cpu quantization test

* fix check dataset

* modify dockerfile

* protect trainer

* style

* test for config

* add more log

* overwrite torch_dtype

* draft doc

* modify quantization_config docstring

* fix class name in docstring

* Apply suggestions from code review

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* more warning

* fix 8bit kwargs tests

* peft compatibility

* remove var

* fix is_gptq_quantized

* remove is_gptq_quantized

* fix wrap

* Update src/transformers/modeling_utils.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* add exllama

* skip test

* overwrite float16

* style

* fix skip test

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* fix docsting formatting

* add doc

* better test

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-08-10T15:07:32Z,"Fix issue with ratio evaluation steps and auto find batch size (#25436)

* Fully rebased solution

* 500"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-08-09T07:31:24Z,rm useless condition since the previous condition contains it. (#25403)
github.com/huggingface/transformers,src/transformers/trainer.py,2023-08-08T17:05:41Z,change version (#25387)
github.com/huggingface/transformers,src/transformers/trainer.py,2023-08-07T15:47:22Z,"Migrate Trainer from `Repository` to `upload_folder` (#25095)

* First draft

* Deal with progress bars

* Update src/transformers/utils/hub.py

Co-authored-by: Lucain <lucainp@gmail.com>

* Address review comments

* Forgot one

* Pin hf_hub

* Add argument for push all and fix tests

* Fix tests

* Address review comments

---------

Co-authored-by: Lucain <lucainp@gmail.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-08-02T07:29:00Z,Fix set of model parallel in the Trainer when no GPUs are available (#25239)
github.com/huggingface/transformers,src/transformers/trainer.py,2023-07-28T09:40:08Z,"Fix `.push_to_hub` and cleanup `get_full_repo_name` usage (#25120)

* Fix .push_to_hub and cleanup get_full_repo_name usage

* Do not rely on Python bool conversion magic

* request changes"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-07-27T12:34:02Z,fix delete all checkpoints when save_total_limit is set to 1 (#25136)
github.com/huggingface/transformers,src/transformers/trainer.py,2023-07-27T01:41:43Z,fix deepspeed load best model at end when the model gets sharded (#25057)
github.com/huggingface/transformers,src/transformers/trainer.py,2023-07-24T14:53:10Z,"compute_loss in trainer failing to label shift for PEFT model when label smoothing enabled. (#25044)

* added PeftModelForCausalLM to MODEL_FOR_CAUSAL_LM_MAPPING_NAMES dict

* check for PEFT model in compute_loss section

---------

Co-authored-by: Nathan Brake <nbrake3@mmm.com>"
github.com/huggingface/transformers,src/transformers/trainer.py,2023-07-24T13:27:19Z,"Add dispatch_batches to training arguments (#25038)

* Dispatch batches

* Copy items"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-03-01T01:12:51Z,"Expose `offload_buffers` parameter of `accelerate` to `PreTrainedModel.from_pretrained` method (#28755)

Expose offload_buffers parameter to from_pretrained method"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-02-27T09:43:01Z,"Fix `attn_implementation` documentation (#29295)

fix"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-02-20T01:23:25Z,"[`gradient_checkpointing`] default to use it for torch 2.3 (#28538)

* default to use it

* style"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-02-16T07:16:58Z,"Update all references to canonical models (#29001)

* Script & Manual edition

* Update"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-02-15T14:33:26Z,"FIX: Fix error with `logger.warning` + inline with recent refactor (#29039)

Update modeling_utils.py"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-02-14T00:30:23Z,"ENH [`AutoQuantizer`]: enhance trainer + not supported quant methods (#28991)

* enhance trainer + not support quant methods

* remove all old logic

* add version"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-02-12T15:47:08Z,"Always initialize tied output_embeddings if it has a bias term (#28947)

Continue to initialize tied output_embeddings if it has a bias term

The bias term is not tied, and so will need to be initialized accordingly."
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-02-06T16:18:30Z,"Revert ""[WIP] Hard error when ignoring tensors."" (#28898)

Revert ""[WIP] Hard error when ignoring tensors. (#27484)""

This reverts commit 2da28c4b41bba23969a8afe97c3dfdcbc47a57dc."
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-02-05T08:17:24Z,"[WIP] Hard error when ignoring tensors. (#27484)

* [WIP] Hard error when ignoring tensors.

* Better selection/error when saving a checkpoint.

- Find all names we should normally drop (those are in the transformers
  config)
- Find all disjoint tensors (for those we can safely trigger a copy to
  get rid of the sharing before saving)
- Clone those disjoint tensors getting rid of the issue
- Find all identical names (those should be declared in the config
  but we try to find them all anyway.)
- For all identical names:
  - If they are in the config, just ignore them everything is fine
  - If they are not, warn about them.
- For all remainder tensors which are shared yet neither identical NOR
  disjoint. raise a hard error.

* Adding a failing test on `main` that passes here.

* We don't need to keep the subfolder logic in this test.

* Apply suggestions from code review

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-02-02T08:34:12Z,"Add missing None check for hf_quantizer (#28804)

* Add missing None check for hf_quantizer

* Add test, fix logic.

* make style

* Switch test model to Mistral

* Comment

* Update tests/test_modeling_utils.py

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-02-02T07:45:00Z,"[Docs] Fix spelling and grammar mistakes (#28825)

* Fix typos and grammar mistakes in docs and examples

* Fix typos in docstrings and comments

* Fix spelling of `tokenizer` in model tests

* Remove erroneous spaces in decorators

* Remove extra spaces in Markdown link texts"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-01-31T01:19:18Z,"don't initialize the output embeddings if we're going to tie them to input embeddings (#28192)

* test that tied output embeddings aren't initialized on load

* don't initialize the output embeddings if we're going to tie them to the input embeddings"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-01-30T01:48:25Z,"`HfQuantizer` class for quantization-related stuff in `modeling_utils.py` (#26610)

* squashed earlier commits for easier rebase

* rm rebase leftovers

* 4bit save enabled @quantizers

* TMP gptq test use exllama

* fix AwqConfigTest::test_wrong_backend for A100

* quantizers AWQ fixes

* _load_pretrained_model low_cpu_mem_usage branch

* quantizers style

* remove require_low_cpu_mem_usage attr

* rm dtype arg from process_model_before_weight_loading

* rm config_origin from Q-config

* rm inspect from q_config

* fixed docstrings in QuantizationConfigParser

* logger.warning fix

* mv is_loaded_in_4(8)bit to BnbHFQuantizer

* is_accelerate_available error msg fix in quantizer

* split is_model_trainable in bnb quantizer class

* rm llm_int8_skip_modules as separate var in Q

* Q rm todo

* fwd ref to HFQuantizer in type hint

* rm note re optimum.gptq.GPTQQuantizer

* quantization_config in __init__ simplified

* replaced NonImplemented with  create_quantized_param

* rm load_in_4/8_bit deprecation warning

* QuantizationConfigParser refactoring

* awq-related minor changes

* awq-related changes

* awq config.modules_to_not_convert

* raise error if no q-method in q-config in args

* minor cleanup

* awq quantizer docstring

* combine common parts in bnb process_model_before_weight_loading

* revert test_gptq

* .process_model_ cleanup

* restore dict config warning

* removed typevars in quantizers.py

* cleanup post-rebase 16 jan

* QuantizationConfigParser classmethod refactor

* rework of handling of unexpected aux elements of bnb weights

* moved q-related stuff from save_pretrained to quantizers

* refactor v1

* more changes

* fix some tests

* remove it from main init

* ooops

* Apply suggestions from code review

Co-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>

* fix awq issues

* fix

* fix

* fix

* fix

* fix

* fix

* add docs

* Apply suggestions from code review

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>
Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update docs/source/en/hf_quantizer.md

* address comments

* fix

* fixup

* Update src/transformers/modeling_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update src/transformers/modeling_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* address final comment

* update

* Update src/transformers/quantizers/base.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update src/transformers/quantizers/auto.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* fix

* add kwargs update

* fixup

* add `optimum_quantizer` attribute

* oops

* rm unneeded file

* fix doctests

---------

Co-authored-by: younesbelkada <younesbelkada@gmail.com>
Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>
Co-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>
Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>
Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-01-26T16:25:08Z,"fix: suppress `GatedRepoError` to use cache file (fix #28558). (#28566)

* fix: suppress `GatedRepoError` to use cache file (fix #28558).

* move condition_to_return parameter back to outside."
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-01-26T12:00:49Z,"Fix `weights_only` (#28725)

fix

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-01-26T11:52:53Z,fix: corrected misleading log message in save_pretrained function (#28699)
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-01-26T08:37:04Z,"Fix duplicate & unnecessary flash attention warnings (#28557)

* fix duplicate & unnecessary flash warnings

* trigger ci

* warning_once

* if/else order

---------

Co-authored-by: Your Name <you@example.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-01-18T10:55:29Z,"Use `weights_only` only if torch >= 1.13 (#28506)

* fix

* fix

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-01-16T18:31:01Z,Config: warning when saving generation kwargs in the model config (#28514)
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-01-16T16:10:44Z,"Clearer error for SDPA when explicitely requested (#28006)

* clearer error for sdpa

* better message"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-01-16T13:29:51Z,"Fix mismatching loading in from_pretrained with/without accelerate (#28414)

* fix mismatching behavior in from_pretrained with/without accelerate

* meaningful refactor

* remove added space

* add test

* fix model on the hub

* comment

* use tiny model

* style"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-01-15T13:48:07Z,"[`core`/ FEAT] Add the possibility to push custom tags using `PreTrainedModel` itself (#28405)

* v1 tags

* remove unneeded conversion

* v2

* rm unneeded warning

* add more utility methods

* Update src/transformers/utils/hub.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/utils/hub.py

Co-authored-by: Lucain <lucainp@gmail.com>

* Update src/transformers/utils/hub.py

Co-authored-by: Lucain <lucainp@gmail.com>

* more enhancements

* oops

* merge tags

* clean up

* revert unneeded change

* add extensive docs

* more docs

* more kwargs

* add test

* oops

* fix test

* Update src/transformers/modeling_utils.py

Co-authored-by: Omar Sanseviero <osanseviero@gmail.com>

* Update src/transformers/utils/hub.py

Co-authored-by: Lucain <lucainp@gmail.com>

* Update src/transformers/modeling_utils.py

* Update src/transformers/trainer.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/modeling_utils.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* add more conditions

* more logic

---------

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>
Co-authored-by: Lucain <lucainp@gmail.com>
Co-authored-by: Omar Sanseviero <osanseviero@gmail.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-01-12T13:29:35Z,"[`Mixtral` / `Awq`] Add mixtral fused modules for Awq  (#28240)

* add mixtral fused modules

* add changes from modeling utils

* add test

* fix test + rope theta issue

* Update src/transformers/modeling_utils.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* add tests

---------

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-01-12T05:55:54Z,"[`Awq`] Add llava fused modules support (#28239)

* add llava + fused modules

* Update src/transformers/models/llava/modeling_llava.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-01-11T15:18:27Z,"Byebye torch 1.10 (#28207)

* fix

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-01-10T08:57:30Z,"Use mmap option to load_state_dict (#28331)

Use mmap option to load_state_dict (#28331)"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2024-01-09T14:58:21Z,"Fix initialization for missing parameters in `from_pretrained` under ZeRO-3 (#28245)

* Fix initialization for missing parameters in `from_pretrained` under ZeRO-3

* Test initialization for missing parameters under ZeRO-3

* Add more tests

* Only enable deepspeed context for per-module level parameters

* Enable deepspeed context only once

* Move class definition inside test case body"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-12-26T20:52:10Z,"small typo (#28229)

Update modeling_utils.py"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-12-25T10:06:56Z,"[`Awq`] Enable the possibility to skip quantization for some target modules (#27950)

* v1

* add docstring

* add tests

* add awq 0.1.8

* oops

* fix test"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-12-22T15:05:10Z,"update the logger message with accordant weights_file_name (#28181)

Co-authored-by: yudong.lin <yudong.lin@funplus.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-12-21T10:54:44Z,"[bnb] Let's make serialization of 4bit models possible  (#26037)

* updated bitsandbytes.py

* rm test_raise_* from test_4bit.py

* add test_4bit_serialization.py

* modeling_utils bulk edits

* bnb_ver 0.41.3 in integrations/bitsandbytes.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* @slow reinstated

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* bnb ver 0.41.3 in  src/transformers/modeling_utils.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* rm bnb version todo in  integrations/bitsandbytes.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* moved 4b serialization tests to test_4bit

* tests upd for opt

* to torch_device

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* ruff fixes to tests

* rm redundant bnb version check in mod_utils

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* restore _hf_peft_config_loaded  modeling_utils.py::2188

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* restore _hf_peft_config_loaded  test in modeling_utils.py::2199

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* fixed NOT getattr(self, ""is_8bit_serializable"")

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* setting model.is_4bit_serializable

* rm separate fp16_statistics arg from set_module...

* rm else branch in integrations::bnb::set_module

* bnb 4bit dtype check

* upd comment on 4bit weights

* upd tests for FP4 safe

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-12-20T16:52:16Z,"Update FA2 exception msg to point to hub discussions (#28161)

* Update FA2 exception msg to point to hub discussions

* Use path for hub url"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-12-20T13:20:02Z,"Fix weights not properly initialized due to shape mismatch (#28122)

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-12-20T08:55:07Z,"Fix FA2 integration (#28142)

* fix fa2

* fix FA2 for popular models

* improve warning and add Younes as co-author

Co-Authored-By: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update src/transformers/modeling_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* fix the warning

* Add Tip

* typo fix

* nit

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>
Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-12-19T17:07:57Z,"Update modeling_utils.py (#28127)

In docstring for PreTrainedModel.resize_token_embeddings, correct definition of new_num_tokens parameter to read ""the new number of tokens"" (meaning the new size of the vocab) rather than ""the number of new tokens"" (number of newly added tokens only)."
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-12-15T17:03:41Z,"Fix `low_cpu_mem_usage` Flag Conflict with DeepSpeed Zero 3 in `from_pretrained` for Models with `keep_in_fp32_modules`"" (#27762)

Fix `from_pretrained` Logic
for `low_cpu_mem_usage` with DeepSpeed Zero3"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-12-15T15:01:18Z,"make torch.load a bit safer (#27282)

* make torch.load a bit safer

* Fixes

---------

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-12-15T10:08:27Z,"[`FA-2`] Fix fa-2 issue when passing `config` to `from_pretrained` (#28043)

* fix fa-2 issue

* fix test

* Update src/transformers/modeling_utils.py

Co-authored-by: fxmarty <9808326+fxmarty@users.noreply.github.com>

* clenaer fix

* up

* add more robust tests

* Update src/transformers/modeling_utils.py

Co-authored-by: fxmarty <9808326+fxmarty@users.noreply.github.com>

* fixup

* Update src/transformers/modeling_utils.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* pop

* add test

---------

Co-authored-by: fxmarty <9808326+fxmarty@users.noreply.github.com>
Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-12-11T11:38:17Z,"[`from_pretrained`] Make from_pretrained fast again (#27709)

* Skip nn.Module.reset_parameters

* Actually skip

* Check quality

* Maybe change all inits

* Fix init issues: only modify public functions

* Add a small test for now

* Style

* test updates

* style

* nice tes

* style

* make it even faster

* one more second

* remove fx icompatible

* Update tests/test_modeling_common.py

Co-authored-by: Lysandre Debut <hi@lysand.re>

* Update tests/test_modeling_common.py

Co-authored-by: Lysandre Debut <hi@lysand.re>

* skip

* fix quality

* protect the import

---------

Co-authored-by: Lysandre Debut <hi@lysand.re>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-12-11T09:56:38Z,"Fix SDPA dispatch & make SDPA CI compatible with torch<2.1.1 (#27940)

fix sdpa dispatch"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-12-08T20:38:14Z,"F.scaled_dot_product_attention support (#26572)

* add sdpa

* wip

* cleaning

* add ref

* yet more cleaning

* and more :)

* wip llama

* working llama

* add output_attentions=True support

* bigcode sdpa support

* fixes

* gpt-bigcode support, require torch>=2.1.1

* add falcon support

* fix conflicts falcon

* style

* fix attention_mask definition

* remove output_attentions from attnmaskconverter

* support whisper without removing any Copied from statement

* fix mbart default to eager renaming

* fix typo in falcon

* fix is_causal in SDPA

* check is_flash_attn_2_available in the models init as well in case the model is not initialized through from_pretrained

* add warnings when falling back on the manual implementation

* precise doc

* wip replace _flash_attn_enabled by config.attn_implementation

* fix typo

* add tests

* style

* add a copy.deepcopy on the config in from_pretrained, as we do not want to modify it inplace

* obey to config.attn_implementation if a config is passed in from_pretrained

* fix is_torch_sdpa_available when torch is not installed

* remove dead code

* Update src/transformers/modeling_attn_mask_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update src/transformers/modeling_attn_mask_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update src/transformers/modeling_attn_mask_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update src/transformers/modeling_attn_mask_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update src/transformers/modeling_attn_mask_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update src/transformers/models/bart/modeling_bart.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* remove duplicate pretraining_tp code

* add dropout in llama

* precise comment on attn_mask

* add fmt: off for _unmask_unattended docstring

* precise num_masks comment

* nuke pretraining_tp in LlamaSDPAAttention following Arthur's suggestion

* cleanup modeling_utils

* backward compatibility

* fix style as requested

* style

* improve documentation

* test pass

* style

* add _unmask_unattended tests

* skip meaningless tests for idefics

* hard_check SDPA requirements when specifically requested

* standardize the use if XXX_ATTENTION_CLASSES

* fix SDPA bug with mem-efficient backend on CUDA when using fp32

* fix test

* rely on SDPA is_causal parameter to handle the causal mask in some cases

* fix FALCON_ATTENTION_CLASSES

* remove _flash_attn_2_enabled occurences

* fix test

* add OPT to the list of supported flash models

* improve test

* properly test on different SDPA backends, on different dtypes & properly handle separately the pad tokens in the test

* remove remaining _flash_attn_2_enabled occurence

* Update src/transformers/modeling_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update src/transformers/modeling_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update src/transformers/modeling_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update src/transformers/modeling_attn_mask_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update docs/source/en/perf_infer_gpu_one.md

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* remove use_attn_implementation

* fix docstring & slight bug

* make attn_implementation internal (_attn_implementation)

* typos

* fix tests

* deprecate use_flash_attention_2=True

* fix test

* add back llama that was removed by mistake

* fix tests

* remove _flash_attn_2_enabled occurences bis

* add check & test that passed attn_implementation is valid

* fix falcon torchscript export

* fix device of mask in tests

* add tip about torch.jit.trace and move bt doc below sdpa

* fix parameterized.expand order

* move tests from test_modeling_attn_mask_utils to test_modeling_utils as a relevant test class is already there

* update sdpaattention class with the new cache

* Update src/transformers/configuration_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update src/transformers/models/bark/modeling_bark.py

* address review comments

* WIP torch.jit.trace fix. left: test both eager & sdpa

* add test for torch.jit.trace for both eager/sdpa

* fix falcon with torch==2.0 that needs to use sdpa

* fix doc

* hopefully last fix

* fix key_value_length that has no default now in mask converter

* is it flacky?

* fix speculative decoding bug

* tests do pass

* fix following #27907

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-12-08T08:00:17Z,"Generate: New `Cache` abstraction and Attention Sinks support (#26681)

* Draft version of new KV Caching

This should allow Attention Sinks (https://github.com/tomaarsen/attention_sinks)
/ StreamingLLM (https://arxiv.org/abs/2309.17453) to be easily implemented
in a third-party or in transformers directly

* Address numerous PR suggestions

1. Move layer_idx from cache to ...Attention. Removes confusing set_layer_idx magic.
2. Always convert past_key_values to Cache instance at the start of ...Attention, removes all other isinstance calls.
3. Remove __bool__ and __getitem__ magic as they're confusing.
4. past_key_values.update(key, value, idx) now returns key, value.
5. Add use_legacy_cache flag, defaults to None, i.e. Falsey. This breaks generate for now, until 1) the cache is used is generate() or 2) use_legacy_cache is defaulted to True in generate() until we change it in another PR.
6. Separate key_cache and value_cache.

Some work is still needed to see if the SinkCache can conveniently be implemented with just one update method.

* Implement the SinkCache through backward+forward rotations

* Integrate (Sink)Cache with Llama FA2

* Set use_legacy_cache=True as default, allows for test passes

* Move from/to_legacy_cache to ...Model class

* Undo unnecessary newline change

* Remove copy utility from deprecated OpenLlama

* Match import style

* manual rebase with main

* Cache class working with generate (#1)

* Draft version of new KV Caching

This should allow Attention Sinks (https://github.com/tomaarsen/attention_sinks)
/ StreamingLLM (https://arxiv.org/abs/2309.17453) to be easily implemented
in a third-party or in transformers directly

* Address numerous PR suggestions

1. Move layer_idx from cache to ...Attention. Removes confusing set_layer_idx magic.
2. Always convert past_key_values to Cache instance at the start of ...Attention, removes all other isinstance calls.
3. Remove __bool__ and __getitem__ magic as they're confusing.
4. past_key_values.update(key, value, idx) now returns key, value.
5. Add use_legacy_cache flag, defaults to None, i.e. Falsey. This breaks generate for now, until 1) the cache is used is generate() or 2) use_legacy_cache is defaulted to True in generate() until we change it in another PR.
6. Separate key_cache and value_cache.

Some work is still needed to see if the SinkCache can conveniently be implemented with just one update method.

* Integrate (Sink)Cache with Llama FA2

* Move from/to_legacy_cache to ...Model class

* Undo unnecessary newline change

* Match import style

* working generate

* Add tests; Simplify code; Apply changes to Mistral and Persimmon

* fix rebase mess

* a few more manual fixes

* last manual fix

* propagate changes to phi

* upgrade test

* add use_legacy_cache docstring; beef up tests

* reintroduce unwanted deletes

---------

Co-authored-by: Tom Aarsen <Cubiegamedev@gmail.com>

* move import

* add default to model_kwargs.get('use_legacy_cache')

* correct failing test

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* apply PR suggestions

* fix failing test

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Tom Aarsen <37621491+tomaarsen@users.noreply.github.com>

* PR comments

* tmp commit

* add docstrings

* more tests, more docstrings, add to docs

* derp

* tmp commit

* tmp dbg

* more dbg

* fix beam search bug

* cache can be a list of tuples in some models

* fix group beam search

* all but sinkcache integration tests

* fix sink cache and add hard integration test

* now also compatible with input_embeds input

* PR comments

* add Cache support to Phi+FA2

* make fixup

---------

Co-authored-by: Joao Gante <joao@huggingface.co>
Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-12-06T16:19:44Z,"Avoid class attribute `_keep_in_fp32_modules` being modified (#27867)

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-12-05T11:14:45Z,"Faster generation using AWQ + Fused modules (#27411)

* v1 fusing modules

* add fused mlp support

* up

* fix CI

* block save_pretrained

* fixup

* small fix

* add new condition

* add v1 docs

* add some comments

* style

* fix nit

* adapt from suggestion

* add check

* change arg names

* change variables name

* Update src/transformers/integrations/awq.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* style

* split up into 3 different private methods

* more conditions

* more checks

* add fused tests for custom models

* fix

* fix tests

* final update docs

* final fixes

* fix importlib metadata

* Update src/transformers/utils/quantization_config.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* change it to `do_fuse`

* nit

* Update src/transformers/utils/quantization_config.py

Co-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>

* Update src/transformers/utils/quantization_config.py

Co-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>

* Update src/transformers/utils/quantization_config.py

Co-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>

* few fixes

* revert

* fix test

* fix copies

* raise error if model is not quantized

* add test

* use quantization_config.config when fusing

* Update src/transformers/modeling_utils.py

---------

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>
Co-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-12-04T12:52:17Z,"Flash Attention 2 support for RoCm (#27611)

* support FA2

* fix typo

* fix broken tests

* fix more test errors

* left/right

* fix bug

* more test

* typo

* fix layout flash attention falcon

* do not support this case

* use allclose instead of equal

* fix various bugs with flash attention

* bump

* fix test

* fix mistral

* use skiptest instead of return that may be misleading

* add fix causal arg flash attention

* fix copies

* more explicit comment

* still use self.is_causal

* fix causal argument

* comment

* fixes

* update documentation

* add link

* wrong test

* simplify FA2 RoCm requirements

* update opt

* make flash_attn_uses_top_left_mask attribute private and precise comment

* better error handling

* fix copy & mistral

* Update src/transformers/modeling_utils.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/modeling_utils.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/modeling_utils.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/utils/import_utils.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* use is_flash_attn_greater_or_equal_2_10 instead of is_flash_attn_greater_or_equal_210

* fix merge

* simplify

* inline args

---------

Co-authored-by: Felix Marty <felix@hf.co>
Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-12-01T16:59:14Z,"Better error message for bitsandbytes import  (#27764)

* better error message

* fix logic

* fix log"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-12-01T14:51:10Z,"Make using safetensors files automated. (#27571)

* [WIP] Make using safetensors files automated.

If `use_safetensors=True` is used, and it doesn't exist:

- Don't crash just yet
- Lookup for an open PR containing it.
- If yes, use that instead
- If not, touch the space to convert, wait for conversion to be finished
  and the PR to be opened
- Use that new PR
- Profit.

* Remove the token.

* [Auto Safetensors] Websocket -> SSE (#27656)

* Websocket -> SSE

* Support sharded + tests +cleanup

a

* env var

* Apply suggestions from code review

* Thanks Simon

* Thanks Wauplin

Co-authored-by: Wauplin <lucainp@gmail.com>

* Cleanup

* Update tests

* Tests should pass

* Apply to other tests

* Extend extension

* relax requirement on latest hfh

* Revert

* Correct private handling & debug statements

* Skip gated repos as of now

* Address review comments

Co-authored-by: ArthurZucker <arthur.zucker@gmail.com>

---------

Co-authored-by: Lysandre Debut <hi@lysand.re>
Co-authored-by: Lysandre <lysandre@huggingface.co>
Co-authored-by: Wauplin <lucainp@gmail.com>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
Co-authored-by: ArthurZucker <arthur.zucker@gmail.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-11-24T06:10:52Z,"Refactoring Trainer, adds `save_only_model` arg and simplifying FSDP integration (#27652)

* add code changes

1. Refactor FSDP
2. Add `--save_only_model` option: When checkpointing, whether to only save the model, or also the optimizer, scheduler & rng state.
3. Bump up the minimum `accelerate` version to `0.21.0`

* quality

* fix quality?

* Revert ""fix quality?""

This reverts commit 149330a6abc078827be274db84c8a2d26a76eba1.

* fix fsdp doc strings

* fix quality

* Update src/transformers/training_args.py

Co-authored-by: Zach Mueller <muellerzr@gmail.com>

* please fix the quality issue 😅

* Apply suggestions from code review

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* address comment

* simplify conditional check as per the comment

* update documentation

---------

Co-authored-by: Zach Mueller <muellerzr@gmail.com>
Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-11-21T17:51:48Z,"Fix `resize_token_embeddings` (#26861) (#26865)

* Fix `resize_token_embeddings` about `requires_grad`

The method `resize_token_embeddings` should keep `requires_grad`
unchanged for all parameters in embeddings.

Previously, `resize_token_embeddings` always set `requires_grad`
to `True`. After fixed, `resize_token_embeddings` copy the
`requires_grad` attribute in the old embeddings."
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-11-21T10:03:30Z,"[`core` / `gradient_checkpointing`] add support for old GC method (#27610)

* add support for old GC method

* add also disable

* up

* oops"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-11-20T15:45:55Z,"[`FA-2`] Add fa2 support for `from_config` (#26914)

* add fa2 support for from_config

* Update test_modeling_common.py"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-11-16T16:43:19Z,"[`Styling`] stylify using ruff (#27144)

* try to stylify using ruff

* might need to remove these changes?

* use ruf format andruff check

* use isinstance instead of type comparision

* use # fmt: skip

* use # fmt: skip

* nits

* soem styling changes

* update ci job

* nits isinstance

* more files update

* nits

* more nits

* small nits

* check and format

* revert wrong changes

* actually use formatter instead of checker

* nits

* well docbuilder is overwriting this commit

* revert notebook changes

* try to nuke docbuilder

* style

* fix feature exrtaction test

* remve `indent-width = 4`

* fixup

* more nits

* update the ruff version that we use

* style

* nuke docbuilder styling

* leve the print for detected changes

* nits

* Remove file I/O

Co-authored-by: charliermarsh
 <charlie.r.marsh@gmail.com>

* style

* nits

* revert notebook changes

* Add # fmt skip when possible

* Add # fmt skip when possible

* Fix

* More `  # fmt: skip` usage

* More `  # fmt: skip` usage

* More `  # fmt: skip` usage

* NIts

* more fixes

* fix tapas

* Another way to skip

* Recommended way

* Fix two more fiels

* Remove asynch
Remove asynch

---------

Co-authored-by: charliermarsh <charlie.r.marsh@gmail.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-11-16T15:35:40Z,"Raise error when quantizing a quantized model (#27500)

add error msg"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-11-15T19:58:08Z,"Fix offload disk for loading derivated model checkpoint into base model (#27253)

* fix

* style

* add test"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-11-02T11:03:51Z,"[`core` / `Quantization`] Fix for 8bit serialization tests (#27234)

* fix for 8bit serialization

* added regression tests.

* fixup"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-11-01T18:25:23Z,"Fix CPU offload + disk offload tests (#27204)

Fix disk offload tests + weight sharing issues"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-11-01T17:09:21Z,"Add exllamav2 better (#27111)

* add_ xllamav2 arg

* add test

* style

* add check

* add doc

* replace by use_exllama_v2

* fix tests

* fix doc

* style

* better condition

* fix logic

* add deprecate msg

* deprecate exllama

* remove disable_exllama from the linter

* remove

* fix warning

* Revert the commits deprecating exllama

* deprecate disable_exllama for use_exllama

* fix

* fix loading attribute

* better handling of args

* remove disable_exllama from init and linter

* Apply suggestions from code review

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* better arg

* fix warning

* Apply suggestions from code review

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* switch to dict

* Apply suggestions from code review

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* style

* nits

* style

* better tests

* style

---------

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-11-01T08:06:31Z,"[`core` / `Quantization` ] AWQ integration (#27045)

* working v1

* oops

* Update src/transformers/modeling_utils.py

Co-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>

* fixup

* oops

* push

* more changes

* add docs

* some fixes

* fix copies

* add v1 doc

* added installation guide

* relax constraints

* revert

* attempt llm-awq

* oops

* oops

* fixup

* raise error when incorrect cuda compute capability

* nit

* add instructions for llm-awq

* fixup

* fix copies

* fixup and docs

* change

* few changes + add demo

* add v1 tests

* add autoawq in dockerfile

* finalize

* Update tests/quantization/autoawq/test_awq.py

* fix test

* fix

* fix issue

* Update src/transformers/integrations/awq.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update docs/source/en/main_classes/quantization.md

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update docs/source/en/main_classes/quantization.md

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update src/transformers/integrations/awq.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update src/transformers/integrations/awq.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* add link to example script

* Update docs/source/en/main_classes/quantization.md

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* add more content

* add more details

* add link to quantization docs

* camel case + change backend class name

* change to string

* fixup

* raise errors if libs not installed

* change to `bits` and `group_size`

* nit

* nit

* Apply suggestions from code review

Co-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>

* disable training

* address some comments and fix nits

* fix

* final nits and fix tests

* adapt to our new runners

* make fix-copies

* Update src/transformers/utils/quantization_config.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/utils/quantization_config.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/integrations/awq.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/integrations/awq.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* move to top

* add conversion test

* final nit

* add more elaborated test

---------

Co-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>
Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>
Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-10-31T18:16:49Z,"Safetensors serialization by default (#27064)

* Safetensors serialization by default

* First pass on the tests

* Second pass on the tests

* Third pass on the tests

* Fix TF weight loading from TF-format safetensors

* Specific encoder-decoder fixes for weight crossloading

* Add VisionEncoderDecoder fixes for TF too

* Change filename test for pt-to-tf

* One missing fix for TFVisionEncoderDecoder

* Fix the other crossload test

* Support for flax + updated tests

* Apply suggestions from code review

Co-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>

* Sanchit's comments

* Sanchit's comments 2

* Nico's comments

* Fix tests

* cleanup

* Apply suggestions from code review

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

---------

Co-authored-by: Matt <rocketknight1@gmail.com>
Co-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>
Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-10-31T13:45:23Z,"Add support for loading GPTQ models on CPU (#26719)

* Add support for loading GPTQ models on CPU

Right now, we can only load the GPTQ Quantized model on the CUDA
device. The attribute `gptq_supports_cpu` checks if the current
auto_gptq version is the one which has the cpu support for the
model or not.
The larger variants of the model are hard to load/run/trace on
the GPU and that's the rationale behind adding this attribute.

Signed-Off By: Vivek Khandelwal <vivek@nod-labs.com>

* Update quantization.md

* Update quantization.md

* Update quantization.md"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-10-30T20:08:29Z,"Fix import of torch.utils.checkpoint (#27155)

* Fix import

* Apply suggestions from code review

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-10-27T14:15:22Z,"[`core`/ `gradient_checkpointing`] Refactor GC - part 2 (#27073)

* fix

* more fixes

* fix other models

* fix long t5

* use `gradient_checkpointing_func` instead

* fix copies

* set `gradient_checkpointing_func` as a private attribute and retrieve previous behaviour

* Update src/transformers/modeling_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* replace it with `is_gradient_checkpointing_set`

* remove default

* Update src/transformers/modeling_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* fixup

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-10-27T13:49:20Z,"Fix no split modules underlying modules (#27090)

* fix no split

* style

* remove comm

* Update src/transformers/modeling_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* rename modules

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-10-27T12:32:54Z,Provide alternative when warning on use_auth_token (#27105)
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-10-27T09:23:06Z,"Revert ""add exllamav2 arg"" (#27102)

Revert ""add exllamav2 arg (#26437)""

This reverts commit 8214d6e7b1d6ac25859ad745ccebdf73434e166d."
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-10-26T14:15:05Z,"add exllamav2 arg (#26437)

* add_ xllamav2 arg

* add test

* style

* add check

* add doc

* replace by use_exllama_v2

* fix tests

* fix doc

* style

* better condition

* fix logic

* add deprecate msg"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-10-26T09:21:04Z,"Bump`flash_attn` version to `2.1` (#27079)

* pin FA-2 to `2.1`

* fix on modeling"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-10-25T10:16:15Z,"[`core`] Refactor of `gradient_checkpointing` (#27020)

* v1

* fix

* remove `create_custom_forward`

* fixup

* fixup

* add test and fix all failing GC tests

* remove all remaining `create_custom_forward` methods

* fix idefics bug

* fixup

* replace with `__call__`

* add comment

* quality"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-10-24T17:05:37Z,"Fix config silent copy in from_pretrained (#27043)

* Fix config modeling utils

* fix more

* fix attn mask bug

* Update src/transformers/modeling_utils.py"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-10-24T13:10:23Z,"Add fuyu device map (#26949)

* add _no_split_modules

* style

* fix _no_split_modules

* add doc"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-10-23T12:25:48Z,"Change default `max_shard_size` to smaller value (#26942)

* Update modeling_utils.py

* fixup

* let's change it to 5GB

* fix"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-10-16T17:56:53Z,"🚨🚨🚨 [`Quantization`] Store the original dtype in the config as a private attribute 🚨🚨🚨 (#26761)

* First step

* fix

* add adjustements for gptq

* change to `_pre_quantization_dtype`

* Update src/transformers/modeling_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* fix serialization

* Apply suggestions from code review

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* fixup

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-10-16T13:29:01Z,"Make fsdp ram efficient loading optional (#26631)

make fsdp ram efficient loading optional"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-10-13T10:56:50Z,"[`core`] Fix fa-2 import (#26785)

* fix fa-2 import

* nit"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-10-12T08:28:40Z,"Add many missing spaces in adjacent strings (#26751)

Add missing spaces in adjacent strings"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-10-05T12:44:31Z,"[`core`] fix silent bug `keep_in_fp32` modules (#26589)

* fix silent bug `keep_in_fp32` modules

* final fix

* added a common test.

* Trigger CI

* revert"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-10-03T12:53:09Z,"[`PEFT`] Final fixes (#26559)

* fix issues with PEFT

* logger warning futurewarning issues

* fixup

* adapt from suggestions

* oops

* rm test"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-10-03T06:55:39Z,"[RFC, Logging] Change warning to info (#26545)

[Logging] Change warning to info"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-10-02T12:59:24Z,"[`PEFT`] Protect `adapter_kwargs` check (#26537)

Update modeling_utils.py"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-10-02T09:23:03Z,"[`PEFT`] Pass token when calling `find_adapter_config` (#26488)

* try

* nit

* nits"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-09-28T09:13:03Z,"[`PEFT`] introducing `adapter_kwargs` for loading adapters from different Hub location (`subfolder`, `revision`) than the base model (#26270)

* make use of adapter_revision

* v1 adapter kwargs

* fix CI

* fix CI

* fix CI

* fixup

* add BC

* Update src/transformers/integrations/peft.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* fixup

* change it to error

* Update src/transformers/modeling_utils.py

* Update src/transformers/modeling_utils.py

* fixup

* change

* Update src/transformers/integrations/peft.py

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-09-27T14:45:31Z,"[`PEFT`] Fix PEFT multi adapters support (#26407)

* fix PEFT multi adapters support

* refactor a bit

* save pretrained + BC + added tests

* Update src/transformers/integrations/peft.py

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* add more tests

* add suggestion

* final changes

* adapt a bit

* fixup

* Update src/transformers/integrations/peft.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* adapt from suggestions

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-09-22T15:42:10Z,"[`core` ] Integrate Flash attention 2 in most used models (#25598)

* v1

* oops

* working v1

* fixup

* add some TODOs

* fixup

* padding support + try with module replacement

* nit

* alternative design

* oops

* add `use_cache` support for llama

* v1 falcon

* nit

* a bit of refactor

* nit

* nits nits

* add v1 padding support falcon (even though it seemed to work before)

* nit

* falcon works

* fixup

* v1 tests

* nit

* fix generation llama flash

* update tests

* fix tests + nits

* fix copies

* fix nit

* test- padding mask

* stype

* add more mem efficient support

* Update src/transformers/modeling_utils.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* fixup

* nit

* fixup

* remove it from config when saving

* fixup

* revert docstring

* add more checks

* use values

* oops

* new version

* fixup

* add same trick for falcon

* nit

* add another test

* change tests

* fix issues with GC and also falcon

* fixup

* oops

* Update src/transformers/models/falcon/modeling_falcon.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* add init_rope

* updates

* fix copies

* fixup

* fixup

* more clarification

* fixup

* right padding tests

* add docs

* add FA in docker image

* more clarifications

* add some figures

* add todo

* rectify comment

* Change to FA2

* Update docs/source/en/perf_infer_gpu_one.md

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* split in two lines

* change test name

* add more tests

* some clean up

* remove `rearrange` deps

* add more docs

* revert changes on dockerfile

* Revert ""revert changes on dockerfile""

This reverts commit 8d72a66b4b9b771abc3f15a9b9506b4246d62d8e.

* revert changes on dockerfile

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <hi@lysand.re>

* address some comments

* docs

* use inheritance

* Update src/transformers/testing_utils.py

Co-authored-by: Lysandre Debut <hi@lysand.re>

* fixup

* Apply suggestions from code review

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update src/transformers/modeling_utils.py

* final comments

* clean up

* style

* add cast + warning for PEFT models

* fixup

---------

Co-authored-by: Felix Marty <9808326+fxmarty@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>
Co-authored-by: Lysandre Debut <hi@lysand.re>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-09-21T10:00:03Z,"Keep relevant weights in fp32 when `model._keep_in_fp32_modules` is set even when `accelerate` is not installed (#26225)

* fix bug where weight would not be kept in fp32

* nit

* address review comments

* fix test"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-09-20T04:56:16Z,"FSDP tests and checkpointing fixes (#26180)

* add fsdp tests

* Update test_fsdp.py

* Update test_fsdp.py

* fixes

* checks

* Update trainer.py

* fix

* fixes for saving/resuming checkpoints

* fixes

* add tests and delete debug statements

* fixing tests

* Update test_fsdp.py

* fix tests

* fix tests

* minor nits

* fix code style and quality

* refactor and modularize test code

* reduce the time of tests

* reduce the test time

* fix test

* reduce test time

* reduce test time

* fix failing tests

* fix

* Apply suggestions from code review

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* resolve comments

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-09-19T19:44:41Z,"[FIX] resize_token_embeddings (#26102)

* fix roundup command

* add test for resize_token_embeddings

* Update tests/test_modeling_common.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* style

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-09-19T19:04:56Z,"DeepSpeed ZeRO-3 handling when resizing embedding layers (#26259)

* fix failing deepspeed slow tests

* fixes"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-09-15T15:53:39Z,"Fix pad to multiple of (#25732)

* nits

* update the test

* nits

* update

* fix bark

* fix bark tests and allow padding to multiple of without new tokens"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-09-14T11:01:58Z,"[`PEFT`] Fix PEFT + gradient checkpointing (#25846)

* fix PEFT + gradient checkpointing

* add disable RG

* polish tests

* fix comment

* Revert ""fix comment""

This reverts commit b85386f50d2b104bac522e823c47b7e232116a47.

* final explanations and tests"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-09-13T12:12:35Z,"[`core`] fix 4bit `num_parameters` (#26132)

* fix 4bit `num_parameters`

* stronger check"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-09-13T04:56:37Z,safeguard torch distributed check (#26056)
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-09-08T19:13:33Z,"Skip warning if tracing with dynamo (#25581)

* Ignore warning if tracing with dynamo

* fix import error

* separate to function

* add test"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-09-07T09:10:40Z,fix _resize_token_embeddings will set lm head size to 0 when enabled deepspeed zero3 (#26024)
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-09-06T15:45:47Z,"modify context length for GPTQ + version bump (#25899)

* add new arg for gptq

* add tests

* add min version autogptq

* fix order

* skip test

* fix

* Update src/transformers/modeling_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* fix style

* change model path

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-09-05T09:37:54Z,"nn.Identity is not required to be compatible with PyTorch < 1.1.0 as the minimum PyTorch version we currently support is 1.10.0 (#25974)

nn.Identity is not required to be compatible with PyTorch < 1.1.0 as the
minimum PyTorch version we currently support is 1.10.0"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-08-31T21:38:14Z,"remove torch_dtype override (#25894)

* remove torch_dtype override

* style

* Update src/transformers/modeling_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-08-30T15:00:36Z,fix max_memory for bnb (#25842)
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-08-29T19:10:46Z,Generate: models with custom `generate()` return `True` in `can_generate()` (#25838)
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-08-29T11:32:19Z,"Resolving Attribute error when using the FSDP ram efficient feature (#25820)

fix bug"
github.com/huggingface/transformers,src/transformers/modeling_utils.py,2023-08-25T15:46:56Z,"fix a typo in docsting (#25759)

* fix a typo in docsting

* Update src/transformers/modeling_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

---------

Co-authored-by: statelesshz <jihuazhong1@huawei.com>
Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/integrations/peft.py,2023-11-14T09:32:57Z,"[`Peft`] `modules_to_save` support for peft integration (#27466)

* `modules_to_save` support for peft integration

* Update docs/source/en/peft.md

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* slightly elaborate test

---------

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/integrations/peft.py,2023-11-13T13:20:54Z,"Remove-auth-token (#27060)

* don't use `use_auth_token`internally

* let's use token everywhere

* fixup"
github.com/huggingface/transformers,src/transformers/integrations/peft.py,2023-10-03T12:53:09Z,"[`PEFT`] Final fixes (#26559)

* fix issues with PEFT

* logger warning futurewarning issues

* fixup

* adapt from suggestions

* oops

* rm test"
github.com/huggingface/transformers,src/transformers/integrations/peft.py,2023-09-28T09:13:03Z,"[`PEFT`] introducing `adapter_kwargs` for loading adapters from different Hub location (`subfolder`, `revision`) than the base model (#26270)

* make use of adapter_revision

* v1 adapter kwargs

* fix CI

* fix CI

* fix CI

* fixup

* add BC

* Update src/transformers/integrations/peft.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* fixup

* change it to error

* Update src/transformers/modeling_utils.py

* Update src/transformers/modeling_utils.py

* fixup

* change

* Update src/transformers/integrations/peft.py

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/integrations/peft.py,2023-09-27T14:45:31Z,"[`PEFT`] Fix PEFT multi adapters support (#26407)

* fix PEFT multi adapters support

* refactor a bit

* save pretrained + BC + added tests

* Update src/transformers/integrations/peft.py

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* add more tests

* add suggestion

* final changes

* adapt a bit

* fixup

* Update src/transformers/integrations/peft.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* adapt from suggestions

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
github.com/huggingface/transformers,src/transformers/integrations/peft.py,2023-09-15T16:22:01Z,"[PEFT] Allow PEFT model dict to be loaded (#25721)

* Allow PEFT model dict to be loaded

* make style

* make style

* Apply suggestions from code review

* address comments

* fixup

* final change

* added tests

* fix test

* better logic for handling if adapter has been loaded

* Update tests/peft_integration/test_peft_integration.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

---------

Co-authored-by: younesbelkada <younesbelkada@gmail.com>
Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>
Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/huggingface/transformers,src/transformers/integrations/peft.py,2023-08-30T10:56:05Z,"minor typo fix in PeftAdapterMixin docs (#25829)

fix minor documentation typo"
github.com/huggingface/transformers,src/transformers/integrations/peft.py,2023-08-25T15:13:34Z,"🚨🚨🚨 [`Refactor`] Move third-party related utility files into `integrations/` folder 🚨🚨🚨 (#25599)

* move deepspeed to `lib_integrations.deepspeed`

* more refactor

* oops

* fix slow tests

* Fix docs

* fix docs

* addess feedback

* address feedback

* final modifs for PEFT

* fixup

* ok now

* trigger CI

* trigger CI again

* Update docs/source/en/main_classes/deepspeed.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* import from `integrations`

* address feedback

* revert removal of `deepspeed` module

* revert removal of `deepspeed` module

* fix conflicts

* ooops

* oops

* add deprecation warning

* place it on the top

* put `FutureWarning`

* fix conflicts with not_doctested.txt

* add back `bitsandbytes` module with a depr warning

* fix

* fix

* fixup

* oops

* fix doctests

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>"
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2024-02-06T14:21:36Z,"Revert ""Remove non-HF ExLlamaV2 loader (#5431)""

This reverts commit cde000d47801fa13c5a88f9e435da64132bd96bc."
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2024-02-04T04:40:10Z,Lint
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2024-02-04T04:15:51Z,Remove non-HF ExLlamaV2 loader (#5431)
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-12-31T04:57:06Z,Remove exllamav1 loaders (#5128)
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-12-20T16:29:19Z,let exllama v1 models load safetensor loras (#4854)
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-12-20T04:54:32Z,Improve several log messages
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-11-19T15:59:29Z,Minor LoRA bug fix
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-11-19T15:55:25Z,Fix PEFT LoRA unloading
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-10-27T02:39:51Z,Intel Gpu support initialization (#4340)
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-10-23T20:07:17Z,Organize
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-10-22T19:06:22Z,transformers loader: multi-LoRAs support (#3120)
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-10-14T19:12:41Z,"Exllamav2 lora support (#4229)


---------

Co-authored-by: oobabooga <112222186+oobabooga@users.noreply.github.com>"
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-08-10T17:03:12Z,Allow --lora to use an absolute path
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-07-18T00:27:18Z,Use 'torch.backends.mps.is_available' to check if mps is supported (#3164)
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-07-12T18:33:25Z,lint
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-07-09T04:03:43Z,Lora fixes for AutoGPTQ (#2818)
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-07-07T05:24:07Z, Fixed the param name when loading a LoRA using a model loaded in 4 or 8 bits (#3036)
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-07-03T20:40:22Z,Update LoRA.py - avoid potential error (#2953)
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-06-26T03:10:33Z,Add LoRA support to ExLlama_HF
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-06-24T23:24:17Z,Use pre-compiled python module for ExLlama (#2770)
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-06-19T15:31:24Z,Add ExLlama+LoRA support (#2756)
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-06-16T22:00:37Z,Reorganize model loading UI completely (#2720)
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-06-14T21:29:42Z,"Add LORA name instead of ""default"" in PeftModel (#2689)"
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-06-06T16:05:05Z,Handle the case of older autogptq install
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-06-06T02:32:57Z,Add AutoGPTQ LoRA support
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-05-22T01:42:34Z,Prevent unwanted log messages from modules
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-05-08T19:21:55Z,fixed LoRA loading issue (#1865)
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-05-04T00:43:17Z,Better warning messages
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-04-26T01:58:48Z,"Load more than one LoRA with --lora, fix a bug"
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-04-26T00:20:26Z,Monkey patch fixes
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-04-17T02:26:52Z,Add 4-bit LoRA support (#1200)
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-04-14T17:52:06Z,"initial multi-lora support (#1103)

---------

Co-authored-by: oobabooga <112222186+oobabooga@users.noreply.github.com>"
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-04-08T00:36:04Z,"SD Api Pics extension, v.1.1 (#596)"
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-04-07T03:15:45Z,Make the code more like PEP8 for readability (#862)
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-03-30T01:50:58Z,Move an import
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-03-28T02:16:44Z,Merge branch 'main' into catalpaaa-lora-and-model-dir
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-03-27T03:04:43Z,Handle unloading LoRA from dropdown menu icon
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-03-25T08:28:33Z,Merge branch 'oobabooga:main' into lora-and-model-dir
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-03-25T04:18:32Z,Fix LoRA on mps
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-03-25T00:30:18Z,"lora-dir, model-dir and login auth

Added lora-dir, model-dir, and a login auth arguments that points to a file contains usernames and passwords in the format of ""u:pw,u:pw,..."""
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-03-24T01:02:09Z,Update LoRA.py
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-03-24T00:56:26Z,Clear cache while switching LoRAs
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-03-23T19:49:41Z,Fix LoRA device map (attempt)
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-03-23T04:05:13Z,Fix LoRA in CPU mode
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-03-23T03:55:33Z,Make LoRAs work in 16-bit mode
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-03-19T15:11:35Z,Make custom LoRAs work by default #385
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-03-18T13:55:24Z,Don't include PeftModel every time
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-03-17T20:45:28Z,Add some LoRA params
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-03-17T16:07:17Z,Add a comment
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-03-17T14:43:11Z,Remove unused import
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-03-17T14:39:48Z,"Remove LoRA tab, move it into the Parameters menu"
github.com/oobabooga/text-generation-webui,modules/LoRA.py,2023-03-17T00:35:53Z,Add files
github.com/nomic-ai/gpt4all,gpt4all-training/generate.py,2023-10-24T13:28:21Z,make scripts executable (#1555)
github.com/nomic-ai/gpt4all,gpt4all-training/generate.py,2023-05-01T19:45:23Z,mono repo structure
github.com/TabbyML/tabby,python/tabby/trainer.py,2023-06-13T19:48:27Z,feat: cleanup trainer with new data format
github.com/mlflow/mlflow,mlflow/transformers/peft.py,2024-02-29T00:24:11Z,"Merge PEFT feature branch (#11240)

Signed-off-by: B-Step62 <yuki.watanabe@databricks.com>
Signed-off-by: Yuki Watanabe <31463517+B-Step62@users.noreply.github.com>
Co-authored-by: Ben Wilson <39283302+BenWilson2@users.noreply.github.com>"
github.com/lm-sys/FastChat,fastchat/model/apply_lora.py,2023-06-14T09:07:44Z,Add support for QLoRA (#1676)
github.com/lm-sys/FastChat,fastchat/model/apply_lora.py,2023-04-24T10:02:08Z,Add support for Baize and LoRA models (#553)
github.com/huggingface/peft,src/peft/auto.py,2024-02-22T04:08:19Z,"Allow trust_remote_code for tokenizers when loading AutoPeftModels (#1477)

* feat: Allow tokenizer remote code when loading AutoPeftModels

* style: Merge arguments into one line"
github.com/huggingface/peft,src/peft/auto.py,2024-02-12T13:41:35Z,"FIX Honor HF_HUB_OFFLINE mode if set by user (#1454)

Resolves #1452

If users enable offline mode, don't perform checks for files on HF Hub,
as they would fail."
github.com/huggingface/peft,src/peft/auto.py,2024-02-09T10:00:03Z,"FIX Loading with AutoPeftModel.from_pretrained (#1449)

Fixes #1430

When Using AutoPeftModel.from_pretrained, there is a check to see if a
tokenizer can be found. This check will include a search for the
tokenizer on HF Hub. However, when the model is stored locally, the path
may not be a valid HF Hub repo ID. In that case, an error is raised by
huggingface_hub.

This PR consists of catching that error, and assuming that if the error
occurs, the tokenizer does not exist. This resolves the issue."
github.com/huggingface/peft,src/peft/auto.py,2024-02-07T11:52:35Z,MNT Move code quality fully to ruff (#1421)
github.com/huggingface/peft,src/peft/auto.py,2024-01-17T15:32:16Z,"Handle resizing of embedding layers for AutoPeftModel (#1367)

* handle resizing of embedding layers for AutoPeftModel

* fixes

* add test"
github.com/huggingface/peft,src/peft/auto.py,2023-08-07T14:34:54Z,"[`core`] PEFT refactor + introducing `inject_adapter_in_model` public method (#749)

Refactors a bit the internals of some PEFT models and introduces a new
method inject_adapter_in_model for users that want to pass a bare model
and a peft config to inject adapters in-place into the model. These
changes are totally BC with the previous PEFT versions.

This PR makes things easier for the PEFT integration in transformers
huggingface/transformers#25077

The main goal of the PR is to expose a new API for advanced users that
want to integrate PEFT method without the need to use the PeftModel
wrapper. A simple use case could be someone that wants to inject adapters
into a model and wants to keep the original class of the model without
having to offload that to peft that will create a PeftModel. I have
faced this issue in huggingface/transformers#25077 Among other things,
this PR refactors some internals of PEFT library, while keeping it fully
backward compatible.

To tackle the main motivation I propose to differentiate things between
two type of adapters

1- adapters that are injectable (LoRA, AdaLoRA, IA3)
2- adapters that are not injectable (the rest)

As a first iteration this API would be supported only for the scenario
1- / therefore I decided to create 2 abstract classes to make things
easy to be able to determine if the adapter layer (e.g. LoraLayer) /
adapter module (e.g. LoraModel) does follow the minimal
requirement (i.e. needed attributes, etc.)

Other related changes:

1- Creates a new property method is_prompt_learning to avoid importing
   PromptLearningConfig all the way down
2- Introduces a new object TUNERS_MAPPING, which is a mapping of
   supported pluggable adapters
3- Creates two abstract classes
3.1- BaseTunerLayer: a mixin to check for minimal required attributes
     that a tuner layer should have active_adapter / _is_plugable
3.2- BaseTuner: a higher level module mixin that should be used for any
     injectable adapters in the future.

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
github.com/huggingface/peft,src/peft/auto.py,2023-07-15T12:18:34Z,"[`Auto`] Support `AutoPeftModel` for custom HF models (#707)

* support `AutoPeftModel` for custom HF models

* added documentation."
github.com/huggingface/peft,src/peft/auto.py,2023-07-14T09:07:09Z,"Introducing `AutoPeftModelForxxx` (#694)

* working v1 for LMs

* added tests.

* added documentation.

* fixed ruff issues.

* added `AutoPeftModelForFeatureExtraction` .

* replace with `TypeError`

* address last comments

* added comment."
github.com/huggingface/peft,src/peft/config.py,2024-02-07T11:52:35Z,MNT Move code quality fully to ruff (#1421)
github.com/huggingface/peft,src/peft/config.py,2024-01-29T06:25:01Z,add peft type constructor (#1398)
github.com/huggingface/peft,src/peft/config.py,2023-12-07T22:22:26Z,"[docs] PeftConfig and PeftModel (#1211)

* rough draft

* feedback

* feedback"
github.com/huggingface/peft,src/peft/config.py,2023-11-22T19:52:26Z,"(minor) correct type annotation (#1166)

* add correct type annotation

* make style"
github.com/huggingface/peft,src/peft/config.py,2023-10-24T10:06:01Z,"Fix target_modules type in config.from_pretrained (#1046)

Fixes #1045, supersedes #1041

Description

When loading a config from a file, we currently set the loaded
attributes on the config directly. However, this sidesteps the
__post_init__ call, which is required to convert the target_modules to a
set. This PR fixes this by avoiding to set attributes on the config
class directly, instead of going through __init__.

Other changes

While working on this, I did a slight refactor of the config tests.

1. All config classes are included now (some where missing before).
2. Use parameterized instead of looping through the classes.
3. Added a unit test for the aforementioned bug."
github.com/huggingface/peft,src/peft/config.py,2023-10-05T07:57:49Z,"Fix lora creation (#993)

* reducing the time for inject lora modules

* fix bugs

* fix bug

* fixes

* Revert ""fixes""

This reverts commit c7f30627c1798db11be8a5da8f3c801f9469a5e3.

* refactor

* fix failing tests

* fix tests

* fix tests

* fix tests

* fix tests

* Apply suggestions from code review

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* address comments

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/config.py,2023-09-25T14:42:51Z,"feat: add type hints (#858)

* feat: add type hints

* build: trigger ci"
github.com/huggingface/peft,src/peft/config.py,2023-08-07T14:34:54Z,"[`core`] PEFT refactor + introducing `inject_adapter_in_model` public method (#749)

Refactors a bit the internals of some PEFT models and introduces a new
method inject_adapter_in_model for users that want to pass a bare model
and a peft config to inject adapters in-place into the model. These
changes are totally BC with the previous PEFT versions.

This PR makes things easier for the PEFT integration in transformers
huggingface/transformers#25077

The main goal of the PR is to expose a new API for advanced users that
want to integrate PEFT method without the need to use the PeftModel
wrapper. A simple use case could be someone that wants to inject adapters
into a model and wants to keep the original class of the model without
having to offload that to peft that will create a PeftModel. I have
faced this issue in huggingface/transformers#25077 Among other things,
this PR refactors some internals of PEFT library, while keeping it fully
backward compatible.

To tackle the main motivation I propose to differentiate things between
two type of adapters

1- adapters that are injectable (LoRA, AdaLoRA, IA3)
2- adapters that are not injectable (the rest)

As a first iteration this API would be supported only for the scenario
1- / therefore I decided to create 2 abstract classes to make things
easy to be able to determine if the adapter layer (e.g. LoraLayer) /
adapter module (e.g. LoraModel) does follow the minimal
requirement (i.e. needed attributes, etc.)

Other related changes:

1- Creates a new property method is_prompt_learning to avoid importing
   PromptLearningConfig all the way down
2- Introduces a new object TUNERS_MAPPING, which is a mapping of
   supported pluggable adapters
3- Creates two abstract classes
3.1- BaseTunerLayer: a mixin to check for minimal required attributes
     that a tuner layer should have active_adapter / _is_plugable
3.2- BaseTuner: a higher level module mixin that should be used for any
     injectable adapters in the future.

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
github.com/huggingface/peft,src/peft/helpers.py,2023-09-25T14:42:51Z,"feat: add type hints (#858)

* feat: add type hints

* build: trigger ci"
github.com/huggingface/peft,src/peft/helpers.py,2023-08-10T10:14:40Z,"Helper function to update model signature (#784)

Provides helper functions in peft.helpers to update the signature of the
forward or generate method of a PeftModel (or subclass). This can be
useful because the wrapping class may override the docstring and type
annotations of the underlying base model. Applying the helper functions
will restore those, leading to better tab completion, help text, etc.

For the time being, these helper functions are purely optional to use.
At a later stage, we may consider applying them automatically, but that
would require testing to ensure that nothing breaks."
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2024-02-28T19:26:39Z,"refactor: move model helper function in pipeline to a mixin class (#6571)

* move model helper function in pipeline to EfficiencyMixin

---------

Co-authored-by: YiYi Xu <yixu310@gmail.com>
Co-authored-by: Sayak Paul <spsayakpaul@gmail.com>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2024-02-27T03:22:38Z,"[Core] pass revision in the loading_kwargs. (#7019)

* pass revision in the loading_kwarhs.

* remove revision from load_sub_model."
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2024-02-23T17:24:51Z,Fix typos (#7068)
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2024-02-12T18:38:13Z,"[docs] Community pipelines (#6929)

fix"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2024-02-12T08:12:32Z,Pass device to enable_model_cpu_offload in maybe_free_model_hooks (#6937)
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2024-02-08T18:19:31Z,"change to 2024 in the license (#6902)

change to 2024"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2024-02-08T04:08:57Z,"Remove `torch_dtype` in to() to end deprecation (#6886)

* remove torch_dtype from to()

* remove torch_dtype from usage scripts.

* remove old lora backend

* Revert ""remove old lora backend""

This reverts commit adcddf6ba421f847e7da2a0ce57b9456cae43356."
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2024-01-26T17:31:48Z,"[Hub] feat: explicitly tag to diffusers when using push_to_hub (#6678)

* feat: explicitly tag to diffusers when using push_to_hub

* remove tags.

* reset repo.

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* fix: tests

* fix: push_to_hub behaviour for tagging from save_pretrained

* Apply suggestions from code review

Co-authored-by: Lucain <lucainp@gmail.com>

* Apply suggestions from code review

Co-authored-by: Lucain <lucainp@gmail.com>

* import fixes.

* add library name to existing model card.

* add: standalone test for generate_model_card

* fix tests for standalone method

* moved library_name to a better place.

* merge create_model_card and generate_model_card.

* fix test

* address lucain's comments

* fix return identation

* Apply suggestions from code review

Co-authored-by: Lucain <lucainp@gmail.com>

* address further comments.

* Update src/diffusers/pipelines/pipeline_utils.py

Co-authored-by: Lucain <lucainp@gmail.com>

---------

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Lucain <lucainp@gmail.com>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2024-01-23T09:12:03Z,"[Refactor] Update from single file (#6428)

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update'

* update

* update

* update

* update

* update

* update

* up

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* up

* update

* update

* update

* update

* update'

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* clean

* update

* update

* clean up

* clean up

* update

* clean

* clean

* update

* updaet

* clean up

* fix docs

* update

* update

* Revert ""update""

This reverts commit dbfb8f1ea9c61a2b4e02f926245be2b3d387e577.

* update

* update

* update

* update

* fix controlnet

* fix scheduler

* fix controlnet tests"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2024-01-05T12:32:09Z,"Correctly handle creating model index json files when setting compiled modules in pipelines.  (#6436)

update"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2024-01-04T16:05:55Z,"Respect offline mode when loading pipeline (#6456)

* Respect offline mode when loading model

* default to local entry if connectionerror"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-12-06T21:22:31Z,"Harmonize HF environment variables + deprecate use_auth_token (#6066)

* Harmonize HF environment variables + deprecate use_auth_token

* fix import

* fix"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-12-01T17:43:44Z,"Post Release: v0.24.0 (#5985)

* Post Release: v0.24.0

* post pone deprecation

* post pone deprecation

* Add model_index.json"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-11-29T15:33:04Z,"Fixed custom module importing on Windows (#5891)

* Fixed custom module importing on Windows

Windows use back slash and `os.path.join()` follows that convention.

* Apply suggestions from code review

Co-authored-by: Lucain <lucainp@gmail.com>

* Update pipeline_utils.py

---------

Co-authored-by: Sayak Paul <spsayakpaul@gmail.com>
Co-authored-by: Lucain <lucainp@gmail.com>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-11-27T13:35:19Z,[From_pretrained] Fix warning (#5948)
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-11-21T17:34:30Z,"[feat] IP Adapters (author @okotaku ) (#5713)

* add ip-adapter


---------

Co-authored-by: okotaku <to78314910@gmail.com>
Co-authored-by: sayakpaul <spsayakpaul@gmail.com>
Co-authored-by: yiyixuxu <yixu310@gmail,com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-11-20T10:48:34Z,"[Styling] stylify using ruff (#5841)

* ruff format

* not need to use doc-builder's black styling as the doc is styled in ruff

* make fix-copies

* comment

* use run_ruff"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-11-14T11:08:03Z,"Unwrap models everywhere (#5789)

more debug"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-11-14T10:40:35Z,"[Enhacne] Support maybe_raise_or_warn for peft (#5653)

* Support maybe_raise_or_warn for peft

* fix by comment

* unwrap function"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-11-08T10:51:15Z,"Fixed is_safetensors_compatible() handling of windows path separators (#5650)

Closes #4665"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-11-06T14:11:48Z,"[Custom Pipelines] Make sure that community pipelines can use repo revision (#5659)

fix custom pipelines"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-10-30T18:32:11Z,"Fix incorrect loading of custom pipeline (#5568)

* update

* update

* update

* update"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-10-26T15:11:49Z,"[Remote code] Add functionality to run remote models, schedulers, pipelines (#5472)

* upload custom remote poc

* up

* make style

* finish

* better name

* Apply suggestions from code review

* Update tests/pipelines/test_pipelines.py

* more fixes

* remove ipdb

* more fixes

* fix more

* finish tests

---------

Co-authored-by: Sayak Paul <spsayakpaul@gmail.com>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-09-27T12:04:57Z,"Fix memory issues in tests (#5183)

* fix memory issues

* set _offload_gpu_id

* set gpu offload id"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-09-25T18:24:03Z,make style
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-09-25T18:22:41Z,"[pipeline utils] sanitize pretrained_model_name_or_path (#5173)

Make sure the repo_id is valid before sending it to huggingface_hub to get a more understandable error message.

Re #5110

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-09-25T17:26:39Z,"Fix type annotation (#5146)

* Fix type annotation on Scheduler.from_pretrained

* Fix type annotation on PIL.Image"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-09-25T12:10:18Z,"[Core] Improve `.to(...)` method, fix offloads multi-gpu, add docstring, add dtype (#5132)

* fix cpu offload

* fix

* fix

* Update src/diffusers/pipelines/pipeline_utils.py

* make style

* Apply suggestions from code review

Co-authored-by: YiYi Xu <yixu310@gmail.com>
Co-authored-by: Pedro Cuenca <pedro@huggingface.co>

* fix more

* fix more

---------

Co-authored-by: YiYi Xu <yixu310@gmail.com>
Co-authored-by: Pedro Cuenca <pedro@huggingface.co>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-09-14T16:28:57Z,"[Release 0.21] Bump version (#5018)

* [Release 0.21] Bump version

* fix & remove

* fix more

* fix all, upload"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-09-14T14:36:19Z,Allow disabling `from_pretrained` tqdm (#5007)
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-09-14T09:02:06Z,"Fix model offload bug when key isn't present (#5030)

* fix model offload bug when key isn't present

* make style"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-09-13T09:29:13Z,"[Flax->PT] Fix flaky testing (#5011)

fix flaky flax class name"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-09-12T15:58:47Z,"[docs] Fix DiffusionPipeline.enable_sequential_cpu_offload docstring (#4952)

* Fix an unmatched backtick and make description more general for DiffusionPipeline.enable_sequential_cpu_offload.

* make style

* _exclude_from_cpu_offload -> self._exclude_from_cpu_offload

* make style

* apply suggestions from review

* make style"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-09-11T17:39:26Z,"Refactor model offload (#4514)

* [Draft] Refactor model offload

* [Draft] Refactor model offload

* Apply suggestions from code review

* cpu offlaod updates

* remove model cpu offload from individual pipelines

* add hook to offload models to cpu

* clean up

* model offload

* add model cpu offload string

* make style

* clean up

* fixes for offload issues

* fix tests issues

* resolve merge conflicts

* update src/diffusers/pipelines/pipeline_utils.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* make style

* Update src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py

---------

Co-authored-by: Dhruv Nair <dhruv.nair@gmail.com>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-09-11T10:03:49Z,"Make sure Flax pipelines can be loaded into PyTorch (#4971)

* Make sure Flax pipelines can be loaded into PyTorch

* add test

* Update src/diffusers/pipelines/pipeline_utils.py"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-09-11T07:56:22Z,"Lazy Import for Diffusers (#4829)

* initial commit

* move modules to import struct

* add dummy objects and _LazyModule

* add lazy import to schedulers

* clean up unused imports

* lazy import on models module

* lazy import for schedulers module

* add lazy import to pipelines module

* lazy import altdiffusion

* lazy import audio diffusion

* lazy import audioldm

* lazy import consistency model

* lazy import controlnet

* lazy import dance diffusion ddim ddpm

* lazy import deepfloyd

* lazy import kandinksy

* lazy imports

* lazy import semantic diffusion

* lazy imports

* lazy import stable diffusion

* move sd output to its own module

* clean up

* lazy import t2iadapter

* lazy import unclip

* lazy import versatile and vq diffsuion

* lazy import vq diffusion

* helper to fetch objects from modules

* lazy import sdxl

* lazy import txt2vid

* lazy import stochastic karras

* fix model imports

* fix bug

* lazy import

* clean up

* clean up

* fixes for tests

* fixes for tests

* clean up

* remove import of torch_utils from utils module

* clean up

* clean up

* fix mistake import statement

* dedicated modules for exporting and loading

* remove testing utils from utils module

* fixes from  merge conflicts

* Update src/diffusers/pipelines/kandinsky2_2/__init__.py

* fix docs

* fix alt diffusion copied from

* fix check dummies

* fix more docs

* remove accelerate import from utils module

* add type checking

* make style

* fix check dummies

* remove torch import from xformers check

* clean up error message

* fixes after upstream merges

* dummy objects fix

* fix tests

* remove unused module import

---------

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-09-04T16:21:36Z,"allow passing components to connected pipelines when use the combined pipeline (#4883)

* fix

* add test

---------

Co-authored-by: yiyixuxu <yixu310@gmail,com>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-08-25T16:25:48Z,"fix a bug in `from_pretrained` when load optional components (#4745)

* fix
---------

Co-authored-by: yiyixuxu <yixu310@gmail,com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-08-25T09:13:32Z,Torch device (#4755)
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-08-17T06:19:32Z,"Fix `use_onnx` parameter usage in `from_pretrained` func and update `test_download_no_onnx_by_default` test (#4508)

* add missing use_onnx in from_pretrained func

* fix test_download_no_onnx_by_default test func

* address comments

* split test cases"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-08-17T05:24:28Z,"[Safetensors] Make safetensors the default way of saving weights (#4235)

* make safetensors default

* set default save method as safetensors

* update tests

* update to support saving safetensors

* update test to account for safetensors default

* update example tests to use safetensors

* update example to support safetensors

* update unet tests for safetensors

* fix failing loader tests

* fix qc issues

* fix pipeline tests

* fix example test

---------

Co-authored-by: Dhruv Nair <dhruv.nair@gmail.com>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-08-15T02:09:22Z,"[Pipeline utils] feat: implement push_to_hub for standalone models, schedulers as well as pipelines (#4128)

* feat: implement push_to_hub for standalone models.

* address PR feedback.

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* remove max_shard_size.

* add: support for scheduler push_to_hub

* enable push_to_hub support for flax schedulers.

* enable push_to_hub for pipelines.

* Apply suggestions from code review

Co-authored-by: Lucain <lucainp@gmail.com>

* reflect pr feedback.

* address another round of deedback.

* better handling of kwargs.

* add: tests

* Apply suggestions from code review

Co-authored-by: Lucain <lucainp@gmail.com>

* setting hub staging to False for now.

* incorporate staging test as a separate job.

Co-authored-by: ydshieh <2521628+ydshieh@users.noreply.github.com>

* fix: tokenizer loading.

* fix: json dumping.

* move is_staging_test to a better location.

* better treatment to tokens.

* define repo_id to better handle concurrency

* style

* explicitly set token

* Empty-Commit

* move SUER, TOKEN to test

* collate org_repo_id

* delete repo

---------

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Lucain <lucainp@gmail.com>
Co-authored-by: ydshieh <2521628+ydshieh@users.noreply.github.com>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-08-11T05:35:22Z,"Remove code snippets containing `is_safetensors_available()` (#4521)

* [WIP] Remove code snippets containing `is_safetensors_available()`

* Modifying `import_utils.py`

* update pipeline tests for safetensor default

* fix test related to cached requests

* address import nits

---------

Co-authored-by: Dhruv Nair <dhruv.nair@gmail.com>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-08-10T18:00:03Z,"[docs] Remove attention slicing (#4518)

* remove attention slicing

* apply feedback"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-07-28T12:02:48Z,"[ONNX] Don't download ONNX model by default (#4338)

* [Download] Don't download ONNX weights by default

* [Download] Don't download ONNX weights by default

* [Download] Don't download ONNX weights by default

* fix more

* finish

* finish

* finish"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-07-27T14:16:46Z,"[Local loading] Correct bug with local files only (#4318)

* [Local loading] Correct bug with local files only

* file not found error

* fix

* finish"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-07-26T21:06:18Z,"0.20.0dev0 (#4299)

* 0.20.0dev0

* make style"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-07-26T15:13:55Z,"[Kandinsky] Add combined pipelines / Fix cpu model offload / Fix inpainting (#4207)

* Add combined pipeline

* Download readme

* Upload

* up

* up

* fix final

* Add enable model cpu offload kandinsky

* finish

* finish

* Fix

* fix more

* make style

* fix kandinsky mask

* fix inpainting test

* add callbacks

* add tests

* fix tests

* Apply suggestions from code review

Co-authored-by: YiYi Xu <yixu310@gmail.com>

* docs

* docs

* correct docs

* fix tests

* add warning

* correct docs

---------

Co-authored-by: YiYi Xu <yixu310@gmail.com>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-07-24T18:24:29Z,"[Docs] Fix from pretrained docs (#4240)

* [Docs] Fix from pretrained docs

* [Docs] Fix from pretrained docs"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-07-24T13:35:16Z,"Raise initial HTTPError if pipeline is not cached locally (#4230)

* Raise initial HTTPError if pipeline is not cached locally

* make style"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-07-21T18:01:34Z,"[docs] Clean up pipeline apis (#3905)

* start with stable diffusion

* fix

* finish stable diffusion pipelines

* fix path to pipeline output

* fix flax paths

* fix copies

* add up to score sde ve

* finish first pass of pipelines

* fix copies

* second review

* align doc titles

* more review fixes

* final review"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-07-21T11:45:09Z,"make enable_sequential_cpu_offload more generic for third-party devices (#4191)

* make enable_sequential_cpu_offload more generic for third-party devices

* make style"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-07-18T09:04:40Z,"Refactor execution device & cpu offload (#4114)

* create general cpu offload & execution device

* Remove boiler plate

* finish

* kp

* Correct offload more pipelines

* up

* Update src/diffusers/pipelines/pipeline_utils.py

* make style

* up"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-07-11T10:20:00Z,"FIX `force_download` in download utility (#4036)

FIX force_download in download utils"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-07-10T16:05:40Z,"Improve loading pipe (#4009)

* improve loading subcomponents

* Add test for logging

* improve loading subcomponents

* make style

* make style

* fix

* finish"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-07-10T10:12:29Z,"[DiffusionPipeline] Deprecate not throwing error when loading non-existant variant (#4011)

* Deprecate variant nicely

* make style

* Apply suggestions from code review

Co-authored-by: Sayak Paul <spsayakpaul@gmail.com>

* Apply suggestions from code review

Co-authored-by: Pedro Cuenca <pedro@huggingface.co>

---------

Co-authored-by: Sayak Paul <spsayakpaul@gmail.com>
Co-authored-by: Pedro Cuenca <pedro@huggingface.co>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-07-07T07:33:51Z,"typo in safetensors (safetenstors) (#3976)

* Update pipeline_utils.py

typo in safetensors (safetenstors)

* Update loaders.py

typo in safetensors (safetenstors)

* Update modeling_utils.py

typo in safetensors (safetenstors)"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-06-21T18:07:23Z,"[docs] More API stuff (#3835)

* clean up loaders

* clean up rest of main class apis

* apply feedback"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-06-08T16:08:49Z,"Post 0.17.0 release (#3721)

* Post release

* Post release"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-06-05T16:47:26Z,"[docs] More API fixes (#3640)

* part 2 of api fixes

* move randn_tensor

* add to toctree

* apply feedback

* more feedback"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-06-02T17:26:41Z,"set config from original module but set compiled module on class (#3650)

* set config from original module but set compiled module on class

* add test"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-05-23T13:22:43Z,"Make sure Diffusers works even if Hub is down (#3447)

* Make sure Diffusers works even if Hub is down

* Make sure hub down is well tested"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-05-23T12:20:55Z,Allow custom pipeline loading (#3504)
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-05-22T15:11:08Z,"feat: allow disk offload for diffuser models (#3285)

* allow disk offload for diffuser models

* sort import

* add max_memory argument

* Changed sample[0] to images[0] (#3304)

A pipeline object stores the results in `images` not in `sample`.
Current code blocks don't work.

* Typo in tutorial (#3295)

* Torch compile graph fix (#3286)

* fix more

* Fix more

* fix more

* Apply suggestions from code review

* fix

* make style

* make fix-copies

* fix

* make sure torch compile

* Clean

* fix test

* Postprocessing refactor img2img (#3268)

* refactor img2img VaeImageProcessor.postprocess

* remove copy from for init, run_safety_checker, decode_latents

Co-authored-by: Sayak Paul <spsayakpaul@gmail.com>

---------

Co-authored-by: yiyixuxu <yixu@yis-macbook-pro.lan>
Co-authored-by: Sayak Paul <spsayakpaul@gmail.com>

* [Torch 2.0 compile] Fix more torch compile breaks (#3313)

* Fix more torch compile breaks

* add tests

* Fix all

* fix controlnet

* fix more

* Add Horace He as co-author.
>
>
Co-authored-by: Horace He <horacehe2007@yahoo.com>

* Add Horace He as co-author.

Co-authored-by: Horace He <horacehe2007@yahoo.com>

---------

Co-authored-by: Horace He <horacehe2007@yahoo.com>

* fix: scale_lr and sync example readme and docs. (#3299)

* fix: scale_lr and sync example readme and docs.

* fix doc link.

* Update stable_diffusion.mdx (#3310)

fixed import statement

* Fix missing variable assign in DeepFloyd-IF-II (#3315)

Fix missing variable assign

lol

* Correct doc build for patch releases (#3316)

Update build_documentation.yml

* Add Stable Diffusion RePaint to community pipelines (#3320)

* Add Stable Diffsuion RePaint to community pipelines

- Adds Stable Diffsuion RePaint to community pipelines
- Add Readme enty for pipeline

* Fix: Remove wrong import

- Remove wrong import
- Minor change in comments

* Fix: Code formatting of stable_diffusion_repaint

* Fix: ruff errors in stable_diffusion_repaint

* Fix multistep dpmsolver for cosine schedule (suitable for deepfloyd-if) (#3314)

* fix multistep dpmsolver for cosine schedule (deepfloy-if)

* fix a typo

* Update src/diffusers/schedulers/scheduling_dpmsolver_multistep.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/diffusers/schedulers/scheduling_dpmsolver_multistep.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/diffusers/schedulers/scheduling_dpmsolver_multistep.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/diffusers/schedulers/scheduling_dpmsolver_multistep.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/diffusers/schedulers/scheduling_dpmsolver_multistep.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* update all dpmsolver (singlestep, multistep, dpm, dpm++) for cosine noise schedule

* add test, fix style

---------

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* [docs] Improve LoRA docs (#3311)

* update docs

* add to toctree

* apply feedback

* Added input pretubation (#3292)

* Added input pretubation

* Fixed spelling

* Update write_own_pipeline.mdx (#3323)

* update controlling generation doc with latest goodies. (#3321)

* [Quality] Make style (#3341)

* Fix config dpm (#3343)

* Add the SDE variant of DPM-Solver and DPM-Solver++ (#3344)

* add SDE variant of DPM-Solver and DPM-Solver++

* add test

* fix typo

* fix typo

* Add upsample_size to AttnUpBlock2D, AttnDownBlock2D (#3275)

The argument `upsample_size` needs to be added to these modules to allow compatibility with other blocks that require this argument.

* Rename --only_save_embeds to --save_as_full_pipeline (#3206)

* Set --only_save_embeds to False by default

Due to how the option is named, it makes more sense to behave like this.

* Refactor only_save_embeds to save_as_full_pipeline

* [AudioLDM] Generalise conversion script (#3328)

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Fix TypeError when using prompt_embeds and negative_prompt (#2982)

* test: Added test case

* fix: fixed type checking issue on _encode_prompt

* fix: fixed copies consistency

* fix: one copy was not sufficient

* Fix pipeline class on README (#3345)

Update README.md

* Inpainting: typo in docs (#3331)

Typo in docs

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Add `use_Karras_sigmas` to LMSDiscreteScheduler (#3351)

* add karras sigma to lms discrete scheduler

* add test for lms_scheduler karras

* reformat test lms

* Batched load of textual inversions (#3277)

* Batched load of textual inversions

- Only call resize_token_embeddings once per batch as it is the most expensive operation
- Allow pretrained_model_name_or_path and token to be an optional list
- Remove Dict from type annotation pretrained_model_name_or_path as it was not supported in this function
- Add comment that single files (e.g. .pt/.safetensors) are supported
- Add comment for token parameter
- Convert token override log message from warning to info

* Update src/diffusers/loaders.py

Check for duplicate tokens

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update condition for None tokens

---------

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* make fix-copies

* [docs] Fix docstring (#3334)

fix docstring

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* if dreambooth lora (#3360)

* update IF stage I pipelines

add fixed variance schedulers and lora loading

* added kv lora attn processor

* allow loading into alternative lora attn processor

* make vae optional

* throw away predicted variance

* allow loading into added kv lora layer

* allow load T5

* allow pre compute text embeddings

* set new variance type in schedulers

* fix copies

* refactor all prompt embedding code

class prompts are now included in pre-encoding code
max tokenizer length is now configurable
embedding attention mask is now configurable

* fix for when variance type is not defined on scheduler

* do not pre compute validation prompt if not present

* add example test for if lora dreambooth

* add check for train text encoder and pre compute text embeddings

* Postprocessing refactor all others (#3337)

* add text2img

* fix-copies

* add

* add all other pipelines

* add

* add

* add

* add

* add

* make style

* style + fix copies

---------

Co-authored-by: yiyixuxu <yixu310@gmail,com>

* [docs] Improve safetensors docstring (#3368)

* clarify safetensor docstring

* fix typo

* apply feedback

* add: a warning message when using xformers in a PT 2.0 env. (#3365)

* add: a warning message when using xformers in a PT 2.0 env.

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

---------

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* StableDiffusionInpaintingPipeline - resize image w.r.t height and width (#3322)

* StableDiffusionInpaintingPipeline now resizes input images and masks w.r.t to passed input height and width. Default is already set to 512. This addresses the common tensor mismatch error. Also moved type check into relevant funciton to keep main pipeline body tidy.

* Fixed StableDiffusionInpaintingPrepareMaskAndMaskedImageTests

Due to previous commit these tests were failing as height and width need to be passed into the prepare_mask_and_masked_image function, I have updated the code and added a height/width variable per unit test as it seemed more appropriate than the current hard coded solution

* Added a resolution test to StableDiffusionInpaintPipelineSlowTests

this unit test simply gets the input and resizes it into some that would fail (e.g. would throw a tensor mismatch error/not a mult of 8). Then passes it through the pipeline and verifies it produces output with correct dims w.r.t the passed height and width

---------

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* make style

* [docs] Adapt a model (#3326)

* first draft

* apply feedback

* conv_in.weight thrown away

* [docs] Load safetensors (#3333)

* safetensors

* apply feedback

* apply feedback

* Apply suggestions from code review

---------

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* make style

* [Docs] Fix stable_diffusion.mdx typo (#3398)

Fix typo in last code block. Correct ""prommpts"" to ""prompt""

* Support ControlNet v1.1 shuffle properly (#3340)

* add inferring_controlnet_cond_batch

* Revert ""add inferring_controlnet_cond_batch""

This reverts commit abe8d6311d4b7f5b9409ca709c7fabf80d06c1a9.

* set guess_mode to True
whenever global_pool_conditions is True

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* nit

* add integration test

---------

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* [Tests] better determinism (#3374)

* enable deterministic pytorch and cuda operations.

* disable manual seeding.

* make style && make quality for unet_2d tests.

* enable determinism for the unet2dconditional model.

* add CUBLAS_WORKSPACE_CONFIG for better reproducibility.

* relax tolerance (very weird issue, though).

* revert to torch manual_seed() where needed.

* relax more tolerance.

* better placement of the cuda variable and relax more tolerance.

* enable determinism for 3d condition model.

* relax tolerance.

* add: determinism to alt_diffusion.

* relax tolerance for alt diffusion.

* dance diffusion.

* dance diffusion is flaky.

* test_dict_tuple_outputs_equivalent edit.

* fix two more tests.

* fix more ddim tests.

* fix: argument.

* change to diff in place of difference.

* fix: test_save_load call.

* test_save_load_float16 call.

* fix: expected_max_diff

* fix: paint by example.

* relax tolerance.

* add determinism to 1d unet model.

* torch 2.0 regressions seem to be brutal

* determinism to vae.

* add reason to skipping.

* up tolerance.

* determinism to vq.

* determinism to cuda.

* determinism to the generic test pipeline file.

* refactor general pipelines testing a bit.

* determinism to alt diffusion i2i

* up tolerance for alt diff i2i and audio diff

* up tolerance.

* determinism to audioldm

* increase tolerance for audioldm lms.

* increase tolerance for paint by paint.

* increase tolerance for repaint.

* determinism to cycle diffusion and sd 1.

* relax tol for cycle diffusion 🚲

* relax tol for sd 1.0

* relax tol for controlnet.

* determinism to img var.

* relax tol for img variation.

* tolerance to i2i sd

* make style

* determinism to inpaint.

* relax tolerance for inpaiting.

* determinism for inpainting legacy

* relax tolerance.

* determinism to instruct pix2pix

* determinism to model editing.

* model editing tolerance.

* panorama determinism

* determinism to pix2pix zero.

* determinism to sag.

* sd 2. determinism

* sd. tolerance

* disallow tf32 matmul.

* relax tolerance is all you need.

* make style and determinism to sd 2 depth

* relax tolerance for depth.

* tolerance to diffedit.

* tolerance to sd 2 inpaint.

* up tolerance.

* determinism in upscaling.

* tolerance in upscaler.

* more tolerance relaxation.

* determinism to v pred.

* up tol for v_pred

* unclip determinism

* determinism to unclip img2img

* determinism to text to video.

* determinism to last set of tests

* up tol.

* vq cumsum doesn't have a deterministic kernel

* relax tol

* relax tol

* [docs] Add transformers to install (#3388)

add transformers to install

* [deepspeed] partial ZeRO-3 support (#3076)

* [deepspeed] partial ZeRO-3 support

* cleanup

* improve deepspeed fixes

* Improve

* make style

---------

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Add omegaconf for tests (#3400)

Add omegaconfg

* Fix various bugs with LoRA Dreambooth and Dreambooth script (#3353)

* Improve checkpointing lora

* fix more

* Improve doc string

* Update src/diffusers/loaders.py

* make stytle

* Apply suggestions from code review

* Update src/diffusers/loaders.py

* Apply suggestions from code review

* Apply suggestions from code review

* better

* Fix all

* Fix multi-GPU dreambooth

* Apply suggestions from code review

Co-authored-by: Pedro Cuenca <pedro@huggingface.co>

* Fix all

* make style

* make style

---------

Co-authored-by: Pedro Cuenca <pedro@huggingface.co>

* Fix docker file (#3402)

* up

* up

* fix: deepseepd_plugin retrieval from accelerate state (#3410)

* [Docs] Add `sigmoid` beta_scheduler to docstrings of relevant Schedulers (#3399)

* Add `sigmoid` beta scheduler to `DDPMScheduler` docstring

* Add `sigmoid` beta scheduler to `RePaintScheduler` docstring

---------

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Don't install accelerate and transformers from source (#3415)

* Don't install transformers and accelerate from source (#3414)

* Improve fast tests (#3416)

Update pr_tests.yml

* attention refactor: the trilogy  (#3387)

* Replace `AttentionBlock` with `Attention`

* use _from_deprecated_attn_block check re: @patrickvonplaten

* [Docs] update the PT 2.0 optimization doc with latest findings (#3370)

* add: benchmarking stats for A100 and V100.

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* address patrick's comments.

* add: rtx 4090 stats

* ⚔ benchmark reports done

* Apply suggestions from code review

Co-authored-by: Pedro Cuenca <pedro@huggingface.co>

* 3313 pr link.

* add: plots.

Co-authored-by: Pedro <pedro@huggingface.co>

* fix formattimg

* update number percent.

---------

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Pedro Cuenca <pedro@huggingface.co>

* Fix style rendering (#3433)

* Fix style rendering.

* Fix typo

* unCLIP scheduler do not use note (#3417)

* Replace deprecated command with environment file (#3409)

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* fix warning message pipeline loading (#3446)

* add stable diffusion tensorrt img2img pipeline (#3419)

* add stable diffusion tensorrt img2img pipeline

Signed-off-by: Asfiya Baig <asfiyab@nvidia.com>

* update docstrings

Signed-off-by: Asfiya Baig <asfiyab@nvidia.com>

---------

Signed-off-by: Asfiya Baig <asfiyab@nvidia.com>

* Refactor controlnet and add img2img and inpaint (#3386)

* refactor controlnet and add img2img and inpaint

* First draft to get pipelines to work

* make style

* Fix more

* Fix more

* More tests

* Fix more

* Make inpainting work

* make style and more tests

* Apply suggestions from code review

* up

* make style

* Fix imports

* Fix more

* Fix more

* Improve examples

* add test

* Make sure import is correctly deprecated

* Make sure everything works in compile mode

* make sure authorship is correctly attributed

* [Scheduler] DPM-Solver (++) Inverse Scheduler (#3335)

* Add DPM-Solver Multistep Inverse Scheduler

* Add draft tests for DiffEdit

* Add inverse sde-dpmsolver steps to tune image diversity from inverted latents

* Fix tests

---------

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* [Docs] Fix incomplete docstring for resnet.py (#3438)

Fix incomplete docstrings for resnet.py

* fix tiled vae blend extent range (#3384)

fix tiled vae bleand extent range

* Small update to ""Next steps"" section (#3443)

Small update to ""Next steps"" section:

- PyTorch 2 is recommended.
- Updated improvement figures.

* Allow arbitrary aspect ratio in IFSuperResolutionPipeline (#3298)

* Update pipeline_if_superresolution.py

Allow arbitrary aspect ratio in IFSuperResolutionPipeline by using the input image shape

* IFSuperResolutionPipeline: allow the user to override the height and width through the arguments

* update IFSuperResolutionPipeline width/height doc string to match StableDiffusionInpaintPipeline conventions

---------

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Adding 'strength' parameter to StableDiffusionInpaintingPipeline  (#3424)

* Added explanation of 'strength' parameter

* Added get_timesteps function which relies on new strength parameter

* Added `strength` parameter which defaults to 1.

* Swapped ordering so `noise_timestep` can be calculated before masking the image

this is required when you aren't applying 100% noise to the masked region, e.g. strength < 1.

* Added strength to check_inputs, throws error if out of range

* Changed `prepare_latents` to initialise latents w.r.t strength

inspired from the stable diffusion img2img pipeline, init latents are initialised by converting the init image into a VAE latent and adding noise (based upon the strength parameter passed in), e.g. random when strength = 1, or the init image at strength = 0.

* WIP: Added a unit test for the new strength parameter in the StableDiffusionInpaintingPipeline

still need to add correct regression values

* Created a is_strength_max to initialise from pure random noise

* Updated unit tests w.r.t new strength parameter + fixed new strength unit test

* renamed parameter to avoid confusion with variable of same name

* Updated regression values for new strength test - now passes

* removed 'copied from' comment as this method is now different and divergent from the cpy

* Update src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Ensure backwards compatibility for prepare_mask_and_masked_image

created a return_image boolean and initialised to false

* Ensure backwards compatibility for prepare_latents

* Fixed copy check typo

* Fixes w.r.t backward compibility changes

* make style

* keep function argument ordering same for backwards compatibility in callees with copied from statements

* make fix-copies

---------

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: William Berman <WLBberman@gmail.com>

* [WIP] Bugfix - Pipeline.from_pretrained is broken when the pipeline is partially downloaded (#3448)

Added bugfix using f strings.

* Fix gradient checkpointing bugs in freezing part of models (requires_grad=False) (#3404)

* gradient checkpointing bug fix

* bug fix; changes for reviews

* reformat

* reformat

---------

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Make dreambooth lora more robust to orig unet (#3462)

* Make dreambooth lora more robust to orig unet

* up

* Reduce peak VRAM by releasing large attention tensors (as soon as they're unnecessary) (#3463)

Release large tensors in attention (as soon as they're no longer required). Reduces peak VRAM by nearly 2 GB for 1024x1024 (even after slicing), and the savings scale up with image size.

* Add min snr to text2img lora training script (#3459)

add min snr to text2img lora training script

* Add inpaint lora scale support (#3460)

* add inpaint lora scale support

* add inpaint lora scale test

---------

Co-authored-by: yueyang.hyy <yueyang.hyy@alibaba-inc.com>

* [From ckpt] Fix from_ckpt (#3466)

* Correct from_ckpt

* make style

* Update full dreambooth script to work with IF (#3425)

* Add IF dreambooth docs (#3470)

* parameterize pass single args through tuple (#3477)

* attend and excite tests disable determinism on the class level (#3478)

* dreambooth docs torch.compile note (#3471)

* dreambooth docs torch.compile note

* Update examples/dreambooth/README.md

Co-authored-by: Sayak Paul <spsayakpaul@gmail.com>

* Update examples/dreambooth/README.md

Co-authored-by: Pedro Cuenca <pedro@huggingface.co>

---------

Co-authored-by: Sayak Paul <spsayakpaul@gmail.com>
Co-authored-by: Pedro Cuenca <pedro@huggingface.co>

* add: if entry in the dreambooth training docs. (#3472)

* [docs] Textual inversion inference (#3473)

* add textual inversion inference to docs

* add to toctree

---------

Co-authored-by: Sayak Paul <spsayakpaul@gmail.com>

* [docs] Distributed inference (#3376)

* distributed inference

* move to inference section

* apply feedback

* update with split_between_processes

* apply feedback

* [{Up,Down}sample1d] explicit view kernel size as number elements in flattened indices (#3479)

explicit view kernel size as number elements in flattened indices

* mps & onnx tests rework (#3449)

* Remove ONNX tests from PR.

They are already a part of push_tests.yml.

* Remove mps tests from PRs.

They are already performed on push.

* Fix workflow name for fast push tests.

* Extract mps tests to a workflow.

For better control/filtering.

* Remove --extra-index-url from mps tests

* Increase tolerance of mps test

This test passes in my Mac (Ventura 13.3) but fails in the CI hardware
(Ventura 13.2). I ran the local tests following the same steps that
exist in the CI workflow.

* Temporarily run mps tests on pr

So we can test.

* Revert ""Temporarily run mps tests on pr""

Tests passed, go back to running on push.

---------

Signed-off-by: Asfiya Baig <asfiyab@nvidia.com>
Co-authored-by: Ilia Larchenko <41329713+IliaLarchenko@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: YiYi Xu <yixu310@gmail.com>
Co-authored-by: yiyixuxu <yixu@yis-macbook-pro.lan>
Co-authored-by: Sayak Paul <spsayakpaul@gmail.com>
Co-authored-by: Horace He <horacehe2007@yahoo.com>
Co-authored-by: Umar <55330742+mu94-csl@users.noreply.github.com>
Co-authored-by: Mylo <36931363+gitmylo@users.noreply.github.com>
Co-authored-by: Markus Pobitzer <markuspobitzer@gmail.com>
Co-authored-by: Cheng Lu <lucheng.lc15@gmail.com>
Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>
Co-authored-by: Isamu Isozaki <isamu.website@gmail.com>
Co-authored-by: Cesar Aybar <csaybar@gmail.com>
Co-authored-by: Will Rice <will@spokestack.io>
Co-authored-by: Adrià Arrufat <1671644+arrufat@users.noreply.github.com>
Co-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>
Co-authored-by: At-sushi <dkahw210@kyoto.zaq.ne.jp>
Co-authored-by: Lucca Zenóbio <luccazen@gmail.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Isotr0py <41363108+Isotr0py@users.noreply.github.com>
Co-authored-by: pdoane <pdoane2@gmail.com>
Co-authored-by: Will Berman <wlbberman@gmail.com>
Co-authored-by: yiyixuxu <yixu310@gmail,com>
Co-authored-by: Rupert Menneer <71332436+rupertmenneer@users.noreply.github.com>
Co-authored-by: sudowind <wfpkueecs@163.com>
Co-authored-by: Takuma Mori <takuma104@gmail.com>
Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: Pedro Cuenca <pedro@huggingface.co>
Co-authored-by: Laureηt <laurentfainsin@protonmail.com>
Co-authored-by: Jongwoo Han <jongwooo.han@gmail.com>
Co-authored-by: asfiyab-nvidia <117682710+asfiyab-nvidia@users.noreply.github.com>
Co-authored-by: clarencechen <clarencechenct@gmail.com>
Co-authored-by: Laureηt <laurent@fainsin.bzh>
Co-authored-by: superlabs-dev <133080491+superlabs-dev@users.noreply.github.com>
Co-authored-by: Dev Aggarwal <devxpy@gmail.com>
Co-authored-by: Vimarsh Chaturvedi <vimarsh.c@gmail.com>
Co-authored-by: 7eu7d7 <31194890+7eu7d7@users.noreply.github.com>
Co-authored-by: cmdr2 <shashank.shekhar.global@gmail.com>
Co-authored-by: wfng92 <43742196+wfng92@users.noreply.github.com>
Co-authored-by: Glaceon-Hyy <ffheyy0017@gmail.com>
Co-authored-by: yueyang.hyy <yueyang.hyy@alibaba-inc.com>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-05-17T10:05:33Z,"[WIP] Bugfix - Pipeline.from_pretrained is broken when the pipeline is partially downloaded (#3448)

Added bugfix using f strings."
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-05-16T11:58:24Z,fix warning message pipeline loading (#3446)
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-05-09T23:15:05Z,"[docs] Improve safetensors docstring (#3368)

* clarify safetensor docstring

* fix typo

* apply feedback"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-05-08T19:08:23Z,"[docs] Fix docstring (#3334)

fix docstring

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-04-26T16:43:09Z,"Post release for 0.16.0 (#3244)

* Post release

* fix more"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-04-25T21:20:43Z,"add model (#3230)

* add

* clean

* up

* clean up more

* fix more tests

* Improve docs further

* improve

* more fixes docs

* Improve docs more

* Update src/diffusers/models/unet_2d_condition.py

* fix

* up

* update doc links

* make fix-copies

* add safety checker and watermarker to stage 3 doc page code snippets

* speed optimizations docs

* memory optimization docs

* make style

* add watermarking snippets to doc string examples

* make style

* use pt_to_pil helper functions in doc strings

* skip mps tests

* Improve safety

* make style

* new logic

* fix

* fix bad onnx design

* make new stable diffusion upscale pipeline model arguments optional

* define has_nsfw_concept when non-pil output type

* lowercase linked to notebook name

---------

Co-authored-by: William Berman <WLBberman@gmail.com>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-04-17T16:16:28Z,"Fix config deprecation (#3129)

* Better deprecation message

* Better deprecation message

* Better doc string

* Fixes

* fix more

* fix more

* Improve __getattr__

* correct more

* fix more

* fix

* Improve more

* more improvements

* fix more

* Apply suggestions from code review

Co-authored-by: Pedro Cuenca <pedro@huggingface.co>

* make style

* Fix all rest & add tests & remove old deprecation fns

---------

Co-authored-by: Pedro Cuenca <pedro@huggingface.co>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-04-17T15:19:11Z,Improve deprecation warnings (#3131)
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-04-13T12:33:11Z,"Throw deprecation warning for return_cached_folder (#3092)

Throw deprecation warning"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-04-12T22:25:10Z,[Pipelines] Make sure that None functions are correctly not saved (#3080)
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-04-12T11:11:09Z,"fix pipeline __setattr__ value == None (#3063)

* fix pipeline __setattr__

* add test

---------

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-04-11T11:35:42Z,"Fix config prints and save, load of pipelines (#2849)

* [Config] Fix config prints and save, load

* Only use potential nn.Modules for dtype and device

* Correct vae image processor

* make sure in_channels is not accessed directly

* make sure in channels is only accessed via config

* Make sure schedulers only access config attributes

* Make sure to access config in SAG

* Fix vae processor and make style

* add tests

* uP

* make style

* Fix more naming issues

* Final fix with vae config

* change more"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-04-05T23:31:09Z,"[Pipeline download] Improve pipeline download for index and passed co… (#2980)

* [Pipeline download] Improve pipeline download for index and passed components

* correct

* add more tests

* up"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-03-28T07:03:21Z,"Make dynamo wrapped modules work with save_pretrained  (#2726)

* Workaround for saving dynamo-wrapped models.

* Accept suggestion from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Apply workaround when overriding pipeline components.

* Ensure the correct config.json is saved to disk.

Instead of the dynamo class.

* Save correct module (not compiled one)

* Add test

* style

* fix docstrings

* Go back to using string comparisons.

PyTorch CPU does not have _dynamo.

* Simple test for save_pretrained of compiled models.

* Helper function to test whether module is compiled.

---------

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-03-27T14:18:57Z,"Ruff: apply same rules as in transformers (#2827)

* Apply same ruff settings as in transformers

See https://github.com/huggingface/transformers/blob/main/pyproject.toml
Co-authored-by: Aaron Gokaslan <aaronGokaslan@gmail.com>

* Apply new style rules

* Style

Co-authored-by: Aaron Gokaslan <aaronGokaslan@gmail.com>

* style

* remove list, ruff wouldn't auto fix.

---------

Co-authored-by: Aaron Gokaslan <aaronGokaslan@gmail.com>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-03-21T14:21:23Z,"Add option to set dtype in pipeline.to() method (#2317)

add test_to_dtype to check pipe.to(fp16)"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-03-21T12:45:04Z,"Fix typos (#2715)

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-03-16T14:57:43Z,"Adding `use_safetensors` argument to give more control to users (#2123)

* Adding `use_safetensors` argument to give more control to users

about which weights they use.

* Doc style.

* Rebased (not functional).

* Rebased and functional with tests.

* Style.

* Apply suggestions from code review

* Style.

* Addressing comments.

* Update tests/test_pipelines.py

Co-authored-by: Will Berman <wlbberman@gmail.com>

* Black ???

---------

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Will Berman <wlbberman@gmail.com>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-03-10T20:01:59Z,"[Pipeline loading] Remove send_telemetry (#2640)

* [Pipeline loading]

* up"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-03-10T10:56:10Z,"[From pretrained] Speed-up loading from cache (#2515)

* [From pretrained] Speed-up loading from cache

* up

* Fix more

* fix one more bug

* make style

* bigger refactor

* factor out function

* Improve more

* better

* deprecate return cache folder

* clean up

* improve tests

* up

* upload

* add nice tests

* simplify

* finish

* correct

* fix version

* rename

* Apply suggestions from code review

Co-authored-by: Lucain <lucainp@gmail.com>

* rename

* correct doc string

* correct more

* Apply suggestions from code review

Co-authored-by: Pedro Cuenca <pedro@huggingface.co>

* apply code suggestions

* finish

---------

Co-authored-by: Lucain <lucainp@gmail.com>
Co-authored-by: Pedro Cuenca <pedro@huggingface.co>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-03-09T15:13:55Z,"Up vesion at which we deprecate ""revision='fp16'"" since `transformers` is not released yet (#2623)

* improve error message

* upload"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-03-09T13:00:36Z,"Add cache_dir to docs (#2624)

Improve docs"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-03-03T15:44:10Z,"[Model offload] Add nice warning (#2543)

* [Model offload] Add nice warning

* Treat sequential and model offload differently.

Sequential raises an error because the operation would fail with a
cryptic warning later.

* Forcibly move to cpu when offloading.

* make style

* one more fix

* make fix-copies

* up

---------

Co-authored-by: Pedro Cuenca <pedro@huggingface.co>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-03-03T15:08:56Z,"Fix ONNX checkpoint loading (#2544)

* Revert ""Disable ONNX tests (#2509)""

This reverts commit a0549fea4469251a8021d20d6550cf061a1cdb84.

* add external weights

* + pb

* style"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-03-01T09:31:00Z,[Copyright] 2023 (#2524)
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-03-01T04:37:42Z,"is_safetensors_compatible refactor (#2499)

* is_safetensors_compatible refactor

* files list comma"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-02-20T07:38:30Z,fix transformers naming (#2430)
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-02-20T07:34:30Z,make style
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-02-20T07:34:13Z,Update pipeline_utils.py (#2415)
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-02-19T22:13:03Z,"Fix deprecation warning (#2426)

Deprecation warning should only hit at version 0.15"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-02-17T15:27:51Z,"Add ddim inversion pix2pix (#2397)

* add

* finish

* add tests

* add tests

* up

* up

* pull from main

* uP

* Apply suggestions from code review

* finish

* Update docs/source/en/_toctree.yml

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* finish

* clean docs

* next

* next

* Apply suggestions from code review

Co-authored-by: Pedro Cuenca <pedro@huggingface.co>

* up

* up

---------

Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Pedro Cuenca <pedro@huggingface.co>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-02-16T10:02:58Z,"[Variant] Add ""variant"" as input kwarg so to have better UX when downloading no_ema or fp16 weights (#2305)

* [Variant] Add variant loading mechanism

* clean

* improve further

* up

* add tests

* add some first tests

* up

* up

* use path splittetx

* add deprecate

* deprecation warnings

* improve docs

* up

* up

* up

* fix tests

* Apply suggestions from code review

Co-authored-by: Pedro Cuenca <pedro@huggingface.co>

* Apply suggestions from code review

Co-authored-by: Pedro Cuenca <pedro@huggingface.co>

* correct code format

* fix warning

* finish

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Update docs/source/en/using-diffusers/loading.mdx

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Apply suggestions from code review

Co-authored-by: Will Berman <wlbberman@gmail.com>
Co-authored-by: Suraj Patil <surajp815@gmail.com>

* correct loading docs

* finish

---------

Co-authored-by: Pedro Cuenca <pedro@huggingface.co>
Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Will Berman <wlbberman@gmail.com>"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-02-07T22:46:23Z,"Replace flake8 with ruff and update black (#2279)

* before running make style

* remove left overs from flake8

* finish

* make fix-copies

* final fix

* more fixes"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-02-04T19:55:11Z,"Show error when loading safety_checker `from_flax` (#2187)

* Show error when loading safety_checker `from_flax`

* fix style"
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-01-27T16:23:55Z,[from_pretrained] only load config one time (#2131)
github.com/huggingface/diffusers,src/diffusers/pipelines/pipeline_utils.py,2023-01-27T07:42:33Z,"Don't call the Hub if `local_files_only` is specifiied (#2119)

Don't call the Hub if"
github.com/tloen/alpaca-lora,generate.py,2023-04-09T21:07:59Z,"Update export_hf_checkpoint.py (#302)

* Update export_hf_checkpoint.py

* Update finetune.py

New tokenizer base model for the current dev branch of transformers

* Update generate.py

* Update export_state_dict_checkpoint.py

* Update export_hf_checkpoint.py"
github.com/tloen/alpaca-lora,generate.py,2023-04-04T15:05:20Z,Support streaming output on generate (#263)
github.com/tloen/alpaca-lora,generate.py,2023-03-30T15:57:40Z,Fix server_name
github.com/tloen/alpaca-lora,generate.py,2023-03-30T15:57:40Z,"Added Dockerfile and docker-compose.yml (#207)

* Added Dockerfile for inference

* Added instructions for Dockerfile

* Update README.md

* Update README.md

* Update README.md

* Pass env through Dockerfile

* Added docker compose setup and instructions

* Added more environment options

* Set a safer default mount point

* add docker-compose changes

* Added Dockerfile for inference

* Added instructions for Dockerfile

* Update README.md

* Update README.md

* Update README.md

* Pass env through Dockerfile

* Added docker compose setup and instructions

* Added more environment options

* Set a safer default mount point

* add to gitignore, update to new generate.py

* add docker ignore, simplify docker compose file

* add back missing requirements

* Adjustments to compose and generate.py, added Docker to README.md

* Linting adjust to Black

* Adjusting import linting

* Update README.md

* Update README.md

* Removed comment by original Dockerfile creator.

Comment not necessary.

* cleanup README

Co-authored-by: Francesco Saverio Zuppichini <zuppif@usi.ch>

---------

Co-authored-by: Francesco Saverio Zuppichini <zuppif@usi.ch>
Co-authored-by: Chris Alexiuk <c.s.alexiuk@gmail.com>
Co-authored-by: ElRoberto538 <>
Co-authored-by: Sam Sipe <samsipe@gmail.com>
Co-authored-by: Eric J. Wang <eric.james.wang@gmail.com>"
github.com/tloen/alpaca-lora,generate.py,2023-03-29T23:36:04Z,"Templated prompter (#184)

* Templated prompter

* fix dup import

* Set Verbose False by default

I forgot to disable after testing.

* Fix imports order

* Use Black Formatting

* lint

* Re-introduce lost line

* Cleanup

* template default

* isort

---------

Co-authored-by: Eric Wang <eric.james.wang@gmail.com>"
github.com/tloen/alpaca-lora,generate.py,2023-03-28T16:43:29Z,"Add option to share Gradio demo publicly (#189)

* Add option to share Gradio demo publicly

* gradio_share -> share_gradio

---------

Co-authored-by: Eric Wang <eric.james.wang@gmail.com>"
github.com/tloen/alpaca-lora,generate.py,2023-03-28T15:33:47Z,remove asserts
github.com/tloen/alpaca-lora,generate.py,2023-03-27T17:31:44Z,"Add HF dataset loading, add linters, pyproject.toml (#175)

* add HF dataset loading, add linters, pyproject.toml

- applied markdownlint
- add black, black[jupyter], isort
- fix noqa codes
- add .github workflow linting
- update README.md

* restore default settings

* resume_from_checkpoint

Co-authored-by: AngainorDev <54739135+AngainorDev@users.noreply.github.com>

* Print warning on checkpoint not found

* add HF dataset loading, add linters, pyproject.toml

- applied markdownlint
- add black, black[jupyter], isort
- fix noqa codes
- add .github workflow linting
- update README.md

* Default to local copy and update it

* Typo

* Remove duplicate code block

---------

Co-authored-by: Eric Wang <eric.james.wang@gmail.com>
Co-authored-by: AngainorDev <54739135+AngainorDev@users.noreply.github.com>"
github.com/tloen/alpaca-lora,generate.py,2023-03-24T21:18:42Z,"Use CLI arguments (#159)

* CLI args for finetune

* Update README

* CLI args for generate.py

* reqs.txt

* reorder hyperparams

* lora_target_modules

* cleanup"
github.com/tloen/alpaca-lora,generate.py,2023-03-23T20:54:39Z,"Remove LLaMA download code, as a precaution"
github.com/tloen/alpaca-lora,generate.py,2023-03-23T20:44:45Z,"bos, eos in generate.py"
github.com/tloen/alpaca-lora,generate.py,2023-03-21T21:31:30Z,fix fp16 inference
github.com/tloen/alpaca-lora,generate.py,2023-03-19T22:53:21Z,slider for tokens generated
github.com/tloen/alpaca-lora,generate.py,2023-03-19T18:22:02Z,Remove messy test code
github.com/tloen/alpaca-lora,generate.py,2023-03-19T06:00:18Z,generate.py tweaks
github.com/tloen/alpaca-lora,generate.py,2023-03-18T23:43:53Z,don't share publicly
github.com/tloen/alpaca-lora,generate.py,2023-03-17T22:07:08Z,min beams = 1
github.com/tloen/alpaca-lora,generate.py,2023-03-17T20:53:21Z,Enable inference on CPU and Mac GPU using pytorch support for MPS (#48)
github.com/tloen/alpaca-lora,generate.py,2023-03-17T02:30:27Z,"Update generate.py

Adapting to the input function, a text box for inputting content has been added."
github.com/tloen/alpaca-lora,generate.py,2023-03-16T23:04:06Z,add Gradio interface to generate.py
github.com/tloen/alpaca-lora,generate.py,2023-03-16T19:11:47Z,Catch outdated installs
github.com/tloen/alpaca-lora,generate.py,2023-03-16T19:11:29Z,Update alpaca-lora to use transformers main branch
github.com/tloen/alpaca-lora,generate.py,2023-03-16T16:59:10Z,Expand sampling in generate.py for new test
github.com/tloen/alpaca-lora,generate.py,2023-03-16T07:05:32Z,Add counting test
github.com/tloen/alpaca-lora,generate.py,2023-03-16T00:22:22Z,"generate.py memory, perf updates"
github.com/tloen/alpaca-lora,generate.py,2023-03-15T18:11:26Z,torch.no_grad
github.com/tloen/alpaca-lora,generate.py,2023-03-15T04:41:02Z,add text-davinci-003 to comparisons
github.com/tloen/alpaca-lora,generate.py,2023-03-15T04:33:12Z,Update README.md with new checkpoint details
github.com/tloen/alpaca-lora,generate.py,2023-03-14T22:10:33Z,Ready to go
github.com/tloen/alpaca-lora,generate.py,2023-03-14T00:23:29Z,decapoda
github.com/tloen/alpaca-lora,generate.py,2023-03-13T22:00:05Z,Licenses and whatnot
github.com/huggingface/peft,src/peft/peft_model.py,2024-02-23T09:54:10Z,"FIX Bug in prompt learning after disabling adapter (#1502)

There was a big that after using the disable_adapter context, the
prepare method was not correctly restored, meaning that generations were
incorrect once the context was exited. This is now fixed."
github.com/huggingface/peft,src/peft/peft_model.py,2024-02-19T12:53:39Z,"FIX: Multitask prompt tuning with other tuning init (#1144)

Resolves #1082.

Also, adding tests for prompt_tuning_init != RANDOM.

---------

Co-authored-by: Mayank Mishra <32954280+mayank31398@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2024-02-19T12:33:24Z,"FIX [`PromptTuning`] Simple fix for transformers >= 4.38 (#1484)

* fix for transformers >= 4.38

* style"
github.com/huggingface/peft,src/peft/peft_model.py,2024-02-08T13:39:46Z,DOC How to freeze adapter after set_adapter call (#1447)
github.com/huggingface/peft,src/peft/peft_model.py,2024-02-07T11:52:35Z,MNT Move code quality fully to ruff (#1421)
github.com/huggingface/peft,src/peft/peft_model.py,2024-02-06T15:33:15Z,FIX Saving models that don't have _name_or_path in config (#1440)
github.com/huggingface/peft,src/peft/peft_model.py,2024-02-06T00:54:06Z,Fix typos (#1435)
github.com/huggingface/peft,src/peft/peft_model.py,2024-01-30T11:32:39Z,"Add positional args to PeftModelForCausalLM.generate (#1393)

* add positional args

* update tests"
github.com/huggingface/peft,src/peft/peft_model.py,2024-01-12T16:19:12Z,"FEAT Add Poly Adapter (#1129)

Implement the Poly (Polytropon) adapter.

Papers:

- https://arxiv.org/abs/2202.13914
- https://arxiv.org/abs/2211.03831

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2024-01-12T10:54:53Z,"New transformers caching ETA now v4.38 (#1348)

See #1252 and #1352 for more context.

The initial idea was for transformers 4.37 to add the new caching to all
architectures, but this was postponed to 4.38. The code needs to be
adapted for prompt tuning not to break when transformers 4.37 is
released."
github.com/huggingface/peft,src/peft/peft_model.py,2024-01-12T10:48:42Z,"fix `prepare_inputs_for_generation` logic for Prompt Learning methods (#1352)

* fix `prepare_inputs_for_generation` logic for Prompt Learning methods

* 😅"
github.com/huggingface/peft,src/peft/peft_model.py,2023-12-12T14:16:00Z,"FIX Issues with transformers 4.36 (#1252)

Adjust for different type of past_key_values when using caching.

Also: Fix some seeds for flaky tests.

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>
Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2023-12-07T09:56:21Z,"Don't set config attribute on custom models (#1200)

Initially, we had the issue that it was sometimes assumed that models
had a config attribute, as is given for transformers models. This made
PEFT fail with custom models, so we made a change to set a dummy config
on those.

However, this can lead to issues down the line. For example, when users
use the Trainer class from transformers, they can stumble upon lines
like this:

https://github.com/huggingface/transformers/blob/62ab32b2997846526cdffd629c72c47bc7b4f215/src/transformers/integrations/integration_utils.py#L636-L637

https://github.com/huggingface/transformers/blob/62ab32b2997846526cdffd629c72c47bc7b4f215/src/transformers/integrations/integration_utils.py#L729-L730

Here transformers assumes that if config attribute exists on the model,
it must have a to_json_string method or a to_dict method (as it assumes
the config to be a PretrainedConfig instance). Therefore, in order not
to trip up transformers, it is best not to set any config at all.

Alternative

Alternatively, transformers could be changed to check each time when the
config attributes exists, if it is a PretrainedConfig instance, but that
would be a much larger change (albeit a cleaner one)."
github.com/huggingface/peft,src/peft/peft_model.py,2023-12-04T11:22:03Z,"DOC: Update & improve docstrings and type annotations for common methods and classes (#1201)

The docstrings of the most user-exposed methods and classes have been
updated, or added if not already present. Furthermore, type annotations
have been updated or added for those methods and classes.

---------

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>
Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2023-11-30T15:58:42Z,"[Feature] Support OFT (#1160)

* Support OFT

* add test

* Update README

* fix code quality

* fix test

* Skip 1 test

* fix eps rule and add more test

* feat: added examples to new OFT method

* fix: removed wrong arguments from model example

* fix: changed name of inference file

* fix: changed prompt variable

* fix docs

* fix: dreambooth inference revision based on feedback

* fix: review from BenjaminBossan

* apply safe merge

* del partially

* refactor oft

* refactor oft

* del unused line

* del unused line

* fix skip in windows

* skip test

* Add comments about bias added place

* rename orig_weights to new_weights

* use inverse instead of linalg.inv

* delete alpha and scaling

---------

Co-authored-by: Lukas Kuhn <lukaskuhn.lku@gmail.com>
Co-authored-by: Lukas Kuhn <lukas.kuhn@deutschebahn.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2023-11-29T13:58:41Z,"Training PEFT models with new tokens being added to the embedding layers and tokenizer (#1147)

* add support for saving base layers weights along with adapter weights

* Update save_and_load.py

* Add an example showing the usage of the added feature

* refactor the functionality

* fix

* refactoring code

1. Add `is_embedding_layer_resized` parameter to `save_pretrained`
2. Fix the deduplication in README when adding PEFT details.
3. `save_pretrained` should only save the model when `is_main_process=True` which is one of the parameters of `save_pretrained`.

* update example

* fix the model card

* fix model card

* 😅

* fix model card

* automate setting `is_embedding_layer_resized`

* nits

* Update peft_lora_clm_with_additional_tokens.ipynb

* add test

* fix tests

* maybe fixes the issue?

* address comments

Co-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2023-11-28T13:17:25Z,FIX Pass HF token when calling PeftModel.from_pretrained (#1076)
github.com/huggingface/peft,src/peft/peft_model.py,2023-11-15T10:21:23Z,"FEAT: Make safe serialization the default one (#1088)

* make safe serialization the default one

* adapt tests

* fix final tests'

* adapt from suggestion"
github.com/huggingface/peft,src/peft/peft_model.py,2023-11-14T09:30:52Z,"FIX: Adding 2 adapters when target_modules is a str fails (#1111)

* Fix adding 2 adapters when target_modules is a str

Problem description

Adding two adapters (e.g. LoRA) when using a list for `target_mdules`
works but passing a str fails. The issue is that for str, we do a
`re.fullmatch`, whereas for list, we just check `endswith`. After adding
the first adapter, though, the naming pattern of the modules changes. In
the example above, the name for the linear layer changes from `""lin0""`
to `""base_model.model.lin0""`, which is why the `fullmatch` fails but the
`endswith` still works.

Reproduction

from peft import LoraConfig, get_peft_model
from torch import nn

class MLP(nn.Module):
    def __init__(self, bias=True):
        super().__init__()
        self.lin0 = nn.Linear(10, 20, bias=bias)

def test_target_modules_list():
    config = LoraConfig(target_modules=[""lin0""])
    test_it(config)
    print(""Adding two adapters with target_module being a list works"")

def test_target_modules_str():
    config = LoraConfig(target_modules=""lin0"")
    test_it(config)

def test_it(config):
    model = MLP()
    model = get_peft_model(model, config, ""adapter0"")
    model.add_adapter(""adapter1"", config)
    print(""Adding two adapters with target_module being a str works"")

if __name__ == ""__main__"":
    # works
    test_target_modules_list()
    # ValueError: Target modules lin0 not found in the base model
    test_target_modules_str()

I think that most users would be surprised that:

1. Adding the first adapter works but adding the second fails, even
   though they use the same config.
2. Using `target_modules=[""lin0""]` works but `target_modules=""lin0""`
   fails for the 2nd adapter.

Solution

We could change the logic of not using `re.fullmatch` for str, but I
think that could be tricky to achieve without breaking BC. Instead, I
chose to change the inject_adapter call in add_adapter to pass the base
model, not the whole peft model. This way, the naming pattern is
preserved.

Tests

I haven't added extra tests for this. The script above could serve as a
test. However, it will be sufficient to remove the guard added in #1105:

    if isinstance(config.target_str, modules):
        # TODO this should be doable
        self.skipTest(""Multiple adapters cannot currently be added when target_modules is a string."")

as that will test exactly this behavior and was how the bug was
originally uncovered. Depending on what PR lands first, the guard has to
removed in this PR or in #1105.

* Enable tests for adding 2 adapters with str"
github.com/huggingface/peft,src/peft/peft_model.py,2023-11-10T17:37:38Z,fix import issue transformers (#1116)
github.com/huggingface/peft,src/peft/peft_model.py,2023-11-09T13:50:35Z,"[`core`] Fix safetensors serialization for shared tensors (#1101)

* fix st serialization

* add test

* add CI test

* add comment"
github.com/huggingface/peft,src/peft/peft_model.py,2023-10-30T14:36:41Z,"Add implementation of LyCORIS LoKr for SD&SDXL models (#978)

KronA-like adapter"
github.com/huggingface/peft,src/peft/peft_model.py,2023-10-25T12:53:45Z,"FIX setting active adapter correctly (#1051)

Currently, when calling set_adapter, the active adapter is not updated.
Tests have been added to trigger the bug and the method updated to fix
it.

Moreover, I created an active_adapters property on the PeftModel class
so that it behaves consistently with the underlying models like
LoraModel."
github.com/huggingface/peft,src/peft/peft_model.py,2023-10-11T08:34:28Z,FIX Don't assume model_config contains model_type (#1012)
github.com/huggingface/peft,src/peft/peft_model.py,2023-10-09T12:25:07Z,"Fix word_embeddings match for deepspeed wrapped model (#1000)

* vocab size prompt vocab fix

* add comments

* Update src/peft/peft_model.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2023-10-04T07:44:10Z,"Add base model metadata to model card (#975)

Resolves #938

This PR adds the base model metadata, if present, to the model card.

On top of this, the code for creating the model card has been refactored
to use the huggingface_hub classes instead of doing ad hoc parsing and
writing.
---------

Co-authored-by: Lucain <lucainp@gmail.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2023-10-02T08:44:51Z,"FEAT Add LyCORIS LoHa for SD&SDXL models (#956)

https://arxiv.org/abs/2108.06098"
github.com/huggingface/peft,src/peft/peft_model.py,2023-09-12T09:12:40Z,"Make base_model.peft_config single source of truth (#921)

Resolves #802, #923

For the problem description, please check the first issue.

I went with solution 2, i.e. making the base_model.peft_config the
""single source of truth"" for the PEFT configuration. That way, we
minimize the risk of diverging configurations.

This does not apply for prompt learning, where we don't have a
peft_config on the base model (which is just the normal model, not a
PEFT class).

I added a setter for peft_config but from my testing, it isn't being
used. It's only there for completeness."
github.com/huggingface/peft,src/peft/peft_model.py,2023-08-30T15:16:22Z,DOC: PeftModel save_pretrained docstring (#881) (#888)
github.com/huggingface/peft,src/peft/peft_model.py,2023-08-29T08:53:14Z,FIX: seq2seq prompt tuning (#439) (#809)
github.com/huggingface/peft,src/peft/peft_model.py,2023-08-25T06:12:11Z,"🎉 Add Multitask Prompt Tuning (#400)

* mpt

* fix save

* fix save

* add jupyter notebook

* add jupyter notebook

* add jupyter notebook

* drop shuffling

* drop classify_dataset

* drop classify_dataset

* fix keys

* fix keys

* add comments

* use EXACT_SOURCE_TASK in the example

* formatting

* Fix dict index in embedding retrieval

* run style and quality

* run style and quality

* run style and quality

* style

* final fix

* style

* comment out failing tests

* fix generation tests

* fix style and save test

* all testcases

* fix import

* add license header

* reformat

* fix encoder-decoder models

* fix tests running multiple times

* fix paper name for IA3 and add MPT paper

* Trigger CI

* address the recommended changes

* reformat

* address suggestions

* address suggestions

* revert reformatting

* revert reformatting

---------

Co-authored-by: Alex-Brooks <Alex.Brooks@ibm.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2023-08-11T21:31:17Z,"GPTQ Integration (#771)

* add gptq lora

* fix peft gptq

* fix condition

* fix test

* remove unused weights

* check type

* style

* change attribute

* remove print

* add exllama

* make style

* refactor + fix tests

* remove print

* remove dep on transformers"
github.com/huggingface/peft,src/peft/peft_model.py,2023-08-08T12:38:23Z,"Update docstring of PeftModel.from_pretrained (#799)

1. Addresses
https://github.com/huggingface/peft/issues/430#issuecomment-1666312815
2. Reword docstring to not be LoRA-specific"
github.com/huggingface/peft,src/peft/peft_model.py,2023-08-08T12:35:19Z,"Add adapter error handling (#800)

When a user tries to add a 2nd adapter, Lora and AdaLora make some checks to
ensure the new adapter is compatible with existing adapters. Currently, that
check is performed halfway through the method. This means that if the check
fails, the new adapter is partially applied, leaving the model in a bad state.
The main purpose of this PR is to ensure that the model state is correct after
such a failure is encountered.

Tests were added to catch this potential bug.

While working on this, I also did some related, but not strictly necessary
changes to the add_adapter methods:

- Previously, the peft_config from the PeftModel was passed to the base
  model. This meant that sometimes, the base model would hold a reference
  to PeftModel.peft_config, but not always, as some base models would
  create new dicts. This is problematic, because some code would rely on
  the objects being the same. Now, they are never the same, leading to
  more consistency.
- I think that the check if multiple adapters have biases (which is not
  supported) was accidentally removed by #749. It is added back in.
- Add some type annotations
- Extend docstrings to contain adapter_name"
github.com/huggingface/peft,src/peft/peft_model.py,2023-08-07T14:34:54Z,"[`core`] PEFT refactor + introducing `inject_adapter_in_model` public method (#749)

Refactors a bit the internals of some PEFT models and introduces a new
method inject_adapter_in_model for users that want to pass a bare model
and a peft config to inject adapters in-place into the model. These
changes are totally BC with the previous PEFT versions.

This PR makes things easier for the PEFT integration in transformers
huggingface/transformers#25077

The main goal of the PR is to expose a new API for advanced users that
want to integrate PEFT method without the need to use the PeftModel
wrapper. A simple use case could be someone that wants to inject adapters
into a model and wants to keep the original class of the model without
having to offload that to peft that will create a PeftModel. I have
faced this issue in huggingface/transformers#25077 Among other things,
this PR refactors some internals of PEFT library, while keeping it fully
backward compatible.

To tackle the main motivation I propose to differentiate things between
two type of adapters

1- adapters that are injectable (LoRA, AdaLoRA, IA3)
2- adapters that are not injectable (the rest)

As a first iteration this API would be supported only for the scenario
1- / therefore I decided to create 2 abstract classes to make things
easy to be able to determine if the adapter layer (e.g. LoraLayer) /
adapter module (e.g. LoraModel) does follow the minimal
requirement (i.e. needed attributes, etc.)

Other related changes:

1- Creates a new property method is_prompt_learning to avoid importing
   PromptLearningConfig all the way down
2- Introduces a new object TUNERS_MAPPING, which is a mapping of
   supported pluggable adapters
3- Creates two abstract classes
3.1- BaseTunerLayer: a mixin to check for minimal required attributes
     that a tuner layer should have active_adapter / _is_plugable
3.2- BaseTuner: a higher level module mixin that should be used for any
     injectable adapters in the future.

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2023-08-02T14:59:11Z,"Allow passing inputs_embeds instead of input_ids (#757)

Resolves #727

Right now, there is an issue with a few PeftModelForXxx classes when
users pass only inputs_embeds but not input_ids. First of all, the batch
size was being derived on input_ids, now it is derived from
inputs_embeds instead if input_ids is None. Furthermore, a few forward
calls to the base model were not passing the inputs_embeds along, which
resulted in errors down the line. These issues have been fixed now."
github.com/huggingface/peft,src/peft/peft_model.py,2023-08-01T13:46:18Z,Support XPU adapter loading   (#737)
github.com/huggingface/peft,src/peft/peft_model.py,2023-07-19T12:57:14Z,"[`Patch`] patch trainable params for 4bit layers (#733)

* patch trainable params for 4bit layers

* revert

* added tests.

* added comments.

* addressed final comments"
github.com/huggingface/peft,src/peft/peft_model.py,2023-07-19T12:29:36Z,"[`Llama2`] Add disabling TP behavior (#728)

* add disabling TP behavior

* add comments

* adapt from new changes of transformers PR"
github.com/huggingface/peft,src/peft/peft_model.py,2023-07-19T08:59:55Z,revert change (#731)
github.com/huggingface/peft,src/peft/peft_model.py,2023-07-19T07:52:25Z,fix the param count when using 4-bit bnb
github.com/huggingface/peft,src/peft/peft_model.py,2023-07-19T05:47:15Z,"Fix subfolder issue (#721)

* fix subfolder issue

* added tests"
github.com/huggingface/peft,src/peft/peft_model.py,2023-07-17T09:58:57Z,better hub kwargs management (#712)
github.com/huggingface/peft,src/peft/peft_model.py,2023-07-17T08:02:30Z,"FEAT: Make LoRA work with custom models (#676)

Enable custom models to work with LoRA

This PR enables custom models to work with LoRA in peft by performing a few
changes required for non-transformers models. New tests for linear,
transformers conv1d, and conv2d layers were added.

Not yet contained in this PR:

- support for AdaLoRA and IA³
- documentation
- examples

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2023-07-15T12:18:34Z,"[`Auto`] Support `AutoPeftModel` for custom HF models (#707)

* support `AutoPeftModel` for custom HF models

* added documentation."
github.com/huggingface/peft,src/peft/peft_model.py,2023-07-14T14:28:03Z,"[`Feature`] Save only selected adapters for LoRA (#705)

* v1 working for LoRA

* more checks

* fix prompt learning issues

* fix failing test

* Apply suggestions from code review

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* fixed indentation

* move the check above

* added tests for adaption prompt, enc-dec and feature extraction

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2023-07-14T14:14:51Z,"[Core] Enhancements and refactoring of LoRA method (#695)

* refactor lora and add utils

1. Refactor LoRA code
2. Add method to delete LoRA adapters
3. Add method to unload the PEFT LoRA model.
4. Add `svd` weighted adapter support.
5. minor fixes

* fixes

* fixes

* Update lora.py

* fixes

* Update lora.py

* docstrings for the added public APIs

* docs

* Update src/peft/tuners/lora.py

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* resolve comments, refactoring and adding tests

* fix the remaining failing tests

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2023-07-14T12:33:33Z,"[WIP] FIX for disabling adapter, adding tests (#683)

This PR deals with some issues with disabling adapter:

- typo in active.adapter
- prompt encoder could be on wrong device
- when using prompt learning + generate, disabling did not work

For the last point, there is a somewhat ugly fix in place for now,
pending a more comprehensive refactor (a comment was added to that
effect).

Comprehensive tests were added to check that everything works now.

The following tests still not working:

- adaption prompt
- seq2seq with prompt tuning/prompt encoding
- stable diffusion is a little bit flaky but test is hopefully robust enough

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2023-07-13T12:34:28Z,"add support for Feature Extraction using PEFT (#647)

* add support for embedding with peft

* add example and resolve code quality issues

* update notebook example post fixing the loss

* adding full example with inference notebook

* quality ✨

* add tests, docs, guide and rename task_type to be inline with Hub

* fixes

* fixes

* Apply suggestions from code review

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update peft_model.py

* fixes

* final fixes

* Update _toctree.yml

* fixes and make style and make quality

* deberta exception with checkpointing

* Update docs/source/task_guides/semantic-similarity-lora.md

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update docs/source/task_guides/semantic-similarity-lora.md

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* resolve comments

* testing prompt learning methods

* Update testing_common.py

* fix the tests

---------

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>
Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>
Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2023-07-13T09:12:40Z,FIX: base_model_torch_dtype when using model.half() after init (#688)
github.com/huggingface/peft,src/peft/peft_model.py,2023-07-13T07:45:50Z,"Add functionality to support IA3 (#578)

* Added initial ia3 code

* Implemented ia3 correctly for feedforward layers; Fixed regex matching

* Fixed module mapping for mt5

* Merged changes from huggingface:main

* Merged changes

* Fixed lora merge conflicts

* Different bloom config

* Added save option for ia3

* Added loading code for ia3

* Added feedforward implementation in utils and seq cls example

* Added feedforward implementation in utils and seq cls example

* Implemented merge, unmerge, enable/disable adapters functionality

* Fixed feedforward during merge

* Debugging Merge

* Removing debug messages

* Cleaned up repo

* Removed non-IA3 changes

* Refactor save and load

* Added support to all models in tests; Added IA3Config for common tests

* Added half-precision support and test for gradient checkpointing; Formatted jupyter notebooks

* Added target modules for new models GPTBigCode and LLama

* Cleaned up code

* Cleaned up code

* Cleaned up example notebook

* Cleaned up  seq2seq notebook

* Corrected function docstrings; refactored find_and_replace

* Corrected function docstrings; refactored find_and_replace

* Added basic docs for IA3

* Added new conceptual guide in source tree for documentation

* Minor fix to documentation

* Minor fixes to docstrings; Added error handling for 4bit quantization; Cleaned unused merge/unmerge methods

* styling changes after merge from main

* Update src/peft/tuners/ia3.py

Remove unused attribute merge_weights

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>

---------

Co-authored-by: Abhishek2304 <abhishekgupta2304@gmail.com>
Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2023-06-28T07:03:16Z,"style: tentatively add hints for some public function (#614)

* style: tentatively add hints for some public function

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* fix: import annotations to evaluate to str

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* fix: style

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2023-06-27T21:41:51Z,Update peft_model.py (#644)
github.com/huggingface/peft,src/peft/peft_model.py,2023-06-27T12:57:57Z,"feat(model): Allow from_pretrained to accept PeftConfig class (#612)

* feat(model): Allow from_pretrained to accept PeftConfig class

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* tests: add test cases for config construction

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: address comments and run tools

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* fix: style

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2023-06-27T11:26:47Z,"fix ptun and prompt tuning generation issue (#543)

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2023-06-27T08:27:21Z,"fix Prefix-tuning error in clm Float16 evaluation (#520)

Signed-off-by: Wang, Yi <yi.a.wang@intel.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2023-06-27T06:15:49Z,"Add seq2seq prompt tuning support (#519)

* Added prompt tuning for seq2seq and corresponding notebook examples

* Added prompt tuning for seq2seq and corresponding notebook examples

* Added prompt tuning for seq2seq and corresponding notebook examples

* Call encoder with get_encoder() and update notebook example

* Style formatting

* Add seq2seq p-tuning support, and improve seq2seq prompt tuning support, enabling the use of generate()

* Fix imports

* Fix imports

* Add co-author.

Co-authored-by: ZhengxiangShi michaelszx117@gmail.com

* Add co-author.

Co-authored-by: ZhengxiangShi <michaelszx117@gmail.com>

---------

Co-authored-by: Thomas SCHILLACI <tschilla@px101.prod.exalead.com>
Co-authored-by: ZhengxiangShi <michaelszx117@gmail.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2023-06-19T08:49:41Z,"Improve the README when using PEFT (#594)

* add logic

* Update peft_model.py

* fix test failures

* fixes

* fix"
github.com/huggingface/peft,src/peft/peft_model.py,2023-06-16T11:23:58Z,"feat: Add PeftModelForQuestionAnswering (#473)

* Added first try of supporting QuestionAnswering

* Updated example to be correct

* Added changes from PR 404

* Added missing mapping for task type

* Remove unrelated code

* Run make style"
github.com/huggingface/peft,src/peft/peft_model.py,2023-06-16T11:04:07Z,"when from_pretrained is called in finetune of lora with flag ""is_trainable"" True, should not call model.eval() (#591)"
github.com/huggingface/peft,src/peft/peft_model.py,2023-06-16T08:58:51Z,"Fix typo at peft_model.py (#588)

Fix typo on description:
- `imputs_embeds` to `inputs_embeds`"
github.com/huggingface/peft,src/peft/peft_model.py,2023-06-15T10:23:05Z,"[`core`] Correctly passing the kwargs all over the place (#575)

* v1 of the fix

* forward contrib credits from discussions

* add tests

---------

Co-authored-by: winglian <winglian@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2023-06-15T07:35:43Z,"enable lora for mpt (#576)

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2023-06-09T10:33:13Z,"[`core`] Add safetensors integration (#553)

* add v1

* clean up

* more improvements

* add device

* final adjustements

* use `EntryNotFoundError`

* better checks

* add tests and final fixes

* make style && make quality

* remove `push_to_hub` because of the release"
github.com/huggingface/peft,src/peft/peft_model.py,2023-06-07T12:39:17Z,add thousands separator in print_trainable_parameters (#443)
github.com/huggingface/peft,src/peft/peft_model.py,2023-06-05T13:14:40Z,add library name to model card (#549)
github.com/huggingface/peft,src/peft/peft_model.py,2023-06-01T09:17:05Z,Fixed problem with duplicate same code. (#517)
github.com/huggingface/peft,src/peft/peft_model.py,2023-06-01T09:16:38Z,return load_result when load_adapter (#481)
github.com/huggingface/peft,src/peft/peft_model.py,2023-06-01T09:09:54Z,"Enable PeftConfig & PeftModel to load from revision (#433)

* Enable PeftConfig to load from revision

* Add revision to PeftModel

* Fix weights download with revision"
github.com/huggingface/peft,src/peft/peft_model.py,2023-05-31T10:14:27Z,"[`core`] Add gradient checkpointing check (#404)

* add automatic input enable gradients when calling `get_peft_model`

* style

* better check

* add 4bit check"
github.com/huggingface/peft,src/peft/peft_model.py,2023-05-31T06:08:12Z,Remove merge_weights (#392)
github.com/huggingface/peft,src/peft/peft_model.py,2023-05-20T15:47:15Z,"4-bit QLoRA via bitsandbytes (4-bit base model + LoRA) (#476)

* 4bit lora

* 4bit test

* fixing 4bits bugs

* fp4 pass variables

* fix inference datatype and generation config

* updating prep for int8 function to work for 4-bit

* Added FP4 LoRA and FP4 fine-tuning example.

* LinearFP4 -> Linear4bit

* fixes

* Fixed 4-bit example.

* Style changes.

* final changes

---------

Co-authored-by: Artidoro Pagnoni <pagnoni.artidoro@gmail.com>
Co-authored-by: younesbelkada <younesbelkada@gmail.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2023-05-10T04:39:28Z,"do not use self.device. In FSDP cpu offload mode. self.device is ""CPU"" instead of ""cuda"" (#352)

and there's error like ""Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:1""

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2023-04-26T12:56:14Z,Use `try` and `finally` in `disable_adapter()` to catch exceptions (#368)
github.com/huggingface/peft,src/peft/peft_model.py,2023-04-25T06:54:18Z,"Implement adaption prompt from Llama-Adapter paper (#268)

* Implement adaption prompt from Llama-Adapter paper

* Support multi-adapters

* Refactor adaption prompt to target attn modules instead of layers

* Refactor adaption prompt to be more generic

* Fix adaption prompt not on right device

* Apply suggestions from code review

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>

* Fix style

* Add support for Llama config use_cache=True

* Fix rebase issues

---------

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2023-04-20T10:46:13Z,"fix lora modules_to_save issue (#343)

* fix lora modules_to_save issue

* fix quality"
github.com/huggingface/peft,src/peft/peft_model.py,2023-04-08T08:27:08Z,"Merge pull request #283 from huggingface/smangrul/multi-lora-support

fix trainable params setting"
github.com/huggingface/peft,src/peft/peft_model.py,2023-04-08T06:15:32Z,Update peft_model.py
github.com/huggingface/peft,src/peft/peft_model.py,2023-04-07T10:48:22Z,add and fix tests
github.com/huggingface/peft,src/peft/peft_model.py,2023-04-07T10:36:58Z,Merge remote-tracking branch 'upstream/main' into fix-half-prec
github.com/huggingface/peft,src/peft/peft_model.py,2023-04-06T22:38:10Z,fixing adalora saving and loading
github.com/huggingface/peft,src/peft/peft_model.py,2023-04-06T14:01:21Z,😅
github.com/huggingface/peft,src/peft/peft_model.py,2023-04-06T13:35:31Z,Merge branch 'main' into smangrul/multi-lora-support
github.com/huggingface/peft,src/peft/peft_model.py,2023-04-06T06:00:30Z,"Merge pull request #233 from QingruZhang/main

The Implementation of AdaLoRA (ICLR 2023)"
github.com/huggingface/peft,src/peft/peft_model.py,2023-04-05T20:52:12Z,Run make style and make quality
github.com/huggingface/peft,src/peft/peft_model.py,2023-04-04T20:33:31Z,fix 🐛
github.com/huggingface/peft,src/peft/peft_model.py,2023-04-04T14:14:58Z,fixing 🐛
github.com/huggingface/peft,src/peft/peft_model.py,2023-04-04T12:14:24Z,fix 🐛
github.com/huggingface/peft,src/peft/peft_model.py,2023-04-04T11:34:32Z,Merge branch 'main' into smangrul/multi-lora-support
github.com/huggingface/peft,src/peft/peft_model.py,2023-04-04T10:01:47Z,fix half precision forward
github.com/huggingface/peft,src/peft/peft_model.py,2023-04-03T16:28:11Z,Fixing a bug where a wrong parameter name is used.
github.com/huggingface/peft,src/peft/peft_model.py,2023-04-01T12:54:46Z,"[`core`] Fix offload issue (#248)

* fix offload dir

* remove offload index

* safety checker

* forward contrib credits from previous PR

---------

Co-authored-by: cosimoiaia <cosimoiaia@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2023-03-31T21:41:14Z,fix kwargs
github.com/huggingface/peft,src/peft/peft_model.py,2023-03-31T21:30:05Z,clean up docstrings
github.com/huggingface/peft,src/peft/peft_model.py,2023-03-30T06:24:45Z,Merge branch 'huggingface:main' into main
github.com/huggingface/peft,src/peft/peft_model.py,2023-03-29T13:20:14Z,"Causal LM generation fix for prefix tuning: GPT2 model (#222)

* expand attention mask after preparing generation inputs for prefix tuning

* reformat

* Update src/peft/peft_model.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* reformat as per black

---------

Co-authored-by: Vineet Kumar <vineeku6@in.ibm.com>
Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/peft_model.py,2023-03-29T03:19:14Z,merge the conflit'
github.com/huggingface/peft,src/peft/peft_model.py,2023-03-28T14:10:06Z,Update peft_model.py
github.com/huggingface/peft,src/peft/peft_model.py,2023-03-28T14:02:21Z,Update peft_model.py
github.com/huggingface/peft,src/peft/peft_model.py,2023-03-28T13:59:24Z,Update peft_model.py
github.com/huggingface/peft,src/peft/mixed_model.py,2024-02-08T13:39:46Z,DOC How to freeze adapter after set_adapter call (#1447)
github.com/huggingface/peft,src/peft/mixed_model.py,2024-02-07T11:52:35Z,MNT Move code quality fully to ruff (#1421)
github.com/huggingface/peft,src/peft/mixed_model.py,2024-02-06T17:01:05Z,"[docs] Doc maintenance (#1394)

* improvements

* fix name

* feedback

* fix typos

* feedback"
github.com/huggingface/peft,src/peft/mixed_model.py,2024-02-06T00:54:06Z,Fix typos (#1435)
github.com/huggingface/peft,src/peft/mixed_model.py,2024-01-17T14:25:49Z,Added missing getattr methods for mixed model (#1365)
github.com/huggingface/peft,src/peft/mixed_model.py,2023-12-04T11:18:49Z,"ENH: Enable OFT adapter for mixed adapter models (#1204)

This PR makes it possible to use the newly added OFT adapter in mixed
adapter type models, similar to LoRA, LoHa, etc.

Notes

Adding the integration was pretty straightforward, which is a good sign.

The difficult part was actually about the tests. This stems from the
fact that OFT is (if my understanding is correct) never commutative.
What I mean is that even if the adapters are applied to the last layer
of a model, it makes a difference whether we apply, say, first LoRA,
then OFT vs first OFT, then LoRA.

This is different for the other adapters that were added so far for
mixed models, as they basically do:

- Xa = X + dXa
- Xab = Xa + dXb = X + dXa + dXb = X + dXb + dXa = Xb + dXa = Xba

This is not true for OFT, so when OFT is used, I had to ensure
that no test was applied that (implicitly) assumes commutativity.

Furthermore, I had to increase the model size, see this comment:

https://github.com/huggingface/peft/pull/1160#issuecomment-1836107235"
github.com/huggingface/peft,src/peft/mixed_model.py,2023-11-30T20:58:16Z,"Mixed adapter models (#1163)

Description

This PR allows to add adapters of different types, e.g. LoRA and LoHa:

base_model = ...
config0 = LoraConfig(...)
peft_model = get_peft_model(base_model, config0, mixed=True)
config1 = LoHaConfig(...)
peft_model.add_adapter(config1, ""other"")
peft_model.set_adapter([""default"", ""other""])
peft_model(x)

At this point, both adapters are active at the same time.

Existing code should not be affected by this change, since users need to
opt into this behavior by setting mixed=True, and a completely different
class is being used (PeftMixedModel).

Also interesting is that this method can be used for a single adapter
type but with very different configs. Right now, we have limited support
for that (e.g. for LoRA, different r values by using rank_pattern), but
with this, we don't need to special case the differing arguments
anymore.

Not implemented

- [ ] I'm not yet sure if the same logic can be applied to IA³ or if it
  may fail because IA³ can apply its scaling to the input, not the output.
- [ ] OFT is not supported yet but should work.
- [ ] It is currently not possible to represent a mixed adapter model as
  a single config. I think we can come up with a solution but I don't
  think it is necessary for a first version of this.
- [ ] Saving and loading is not yet implemented for mixed models.

Those could potentially be added in a future PR.

---------

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>"
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2024-02-25T19:10:02Z,Add Gemma (#3078)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2024-02-11T15:47:44Z,Add llava 34b template (#3034)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2024-02-09T19:03:27Z,Upgrade gradio to 4.17 (#3027)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2024-02-05T23:38:31Z,code update (#2997)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2024-02-01T03:31:33Z,update yuan2.0 generation (#2989)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2024-01-24T10:38:22Z,Fix sglang worker (#2953)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2024-01-24T08:22:52Z,SGLang Worker (#2928)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2024-01-24T08:02:31Z,format code
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2024-01-24T07:58:26Z,Fix the pooling method of BGE embedding model (#2926)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2024-01-24T07:52:05Z,"feat: support Model Yuan2.0, a new generation Fundamental Large Language Model developed by IEIT System (#2936)"
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2024-01-18T22:20:41Z,"fix specify local path issue use model from www.modelscope.cn (#2934)

Co-authored-by: mulin.lyh <mulin.lyh@taobao.com>"
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2024-01-17T07:47:04Z,Bump the version to 0.2.35 (#2927)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2024-01-17T07:28:52Z,nous-hermes-2-mixtral-dpo (#2922)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2024-01-17T06:57:34Z,add support for iei yuan2.0 (https://huggingface.co/IEITYuan) (#2919)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2024-01-17T06:56:20Z,"Add TenyxChat-7B-v1 model (#2901)

Co-authored-by: sarath@L3 <[omitted]>"
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2024-01-17T06:55:47Z,feat: use variables OPENAI_MODEL_LIST (#2907)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2024-01-08T16:27:41Z,Update model_adapter.py (#2895)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2024-01-07T21:04:31Z,"Add `Notus` support (#2813)

Co-authored-by: alvarobartt <alvaro@argilla.io>"
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2024-01-07T16:30:22Z,Fix bug that model doesn't automatically switch peft adapter (#2884)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2024-01-07T16:28:57Z,Add TinyLlama (#2889)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-12-28T07:18:45Z,update a new sota model on MT-Bench which touch an 8.8 scores. (#2864)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-12-28T07:18:26Z,Fix the problem of not using the decoding method corresponding to the base model in peft mode (#2865)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-12-24T22:41:28Z,"Add new models (Perplexity, gemini) & Separate GPT versions (#2856)

Co-authored-by: Wei-Lin Chiang <infwinston@gmail.com>"
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-12-24T06:35:01Z,Format code (#2854)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-12-24T06:01:25Z,Import `accelerate` locally to avoid it as a strong dependency (#2820)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-12-24T05:45:01Z,add bagel model adapter (#2814)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-12-24T05:18:40Z,Fix conv_template of chinese alpaca 2 (#2812)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-12-24T05:11:00Z,"add download models from www.modelscope.cn (#2830)

Co-authored-by: mulin.lyh <mulin.lyh@taobao.com>"
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-12-24T05:08:03Z,Add support for CatPPT (#2840)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-12-17T22:38:16Z,Add SOLAR-10.7b Instruct Model (#2826)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-12-09T21:58:25Z,add dolphin (#2794)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-12-09T16:07:25Z,Support xDAN-L1-Chat Model  (#2732)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-12-09T16:04:44Z,Support MetaMath (#2748)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-12-09T16:02:01Z,Update UI and new models (#2762)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-12-01T03:18:43Z,Add deepseek chat (#2760)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-11-27T05:13:02Z,add starling support (#2738)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-11-26T21:20:23Z,Fix MPS backend 'index out of range' error (#2737)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-11-26T08:05:02Z,Fix YiAdapter (#2730)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-11-26T06:05:58Z,Fix Hermes2Adapter (#2727)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-11-23T11:42:09Z,Add Hermes 2.5 [fixed] (#2725)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-11-23T07:45:31Z,Add Yi support (#2723)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-11-23T00:12:18Z,Exllama cache 8bit (#2719)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-11-23T00:11:43Z,support stable-vicuna model (#2696)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-11-22T08:36:01Z,add support for Chinese-LLaMA-Alpaca (#2700)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-11-22T08:29:16Z,Add Microsoft/Orca-2-7b and update model support docs (#2714)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-11-12T21:09:49Z,Fix gpt template (#2674)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-11-10T18:19:07Z,add chatglm3 conv template support in conversation.py (#2622)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-11-09T11:43:55Z,added support for CodeGeex(2) (#2645)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-11-07T23:44:38Z,Improve Azure OpenAI interface (#2651)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-11-03T23:59:58Z,xFastTransformer framework support (#2615)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-11-03T23:59:00Z,openchat 3.5 model support (#2638)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-11-02T05:40:05Z,fix: Fix for OpenOrcaAdapter to return correct conversation template (#2613)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-11-01T08:14:00Z,feat: Support model AquilaChat2 (#2616)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-10-28T02:53:35Z,Update qwen and add pygmalion (#2607)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-10-24T01:24:29Z,add trust_remote_code=True in BaseModelAdapter (#2583)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-10-24T01:23:52Z,"Add Lemur model (#2584)

Co-authored-by: Roberto Ugolotti <Roberto.UGOLOTTI@ec.europa.eu>"
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-10-21T21:06:08Z,Update README.md  (vicuna-v1.3 -> vicuna-1.5) (#2592)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-10-21T11:06:24Z,docs: bit misspell comments model adapter default template name conversation (#2594)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-10-20T18:25:36Z,Add Mistral-7B-OpenOrca conversation_temmplate (#2585)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-10-18T23:12:36Z,Update vigogne template (#2580)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-10-17T20:47:52Z,feat: add claude-v2 (#2571)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-10-15T19:37:17Z,"Add Xwin-LM V0.1, V0.2 support (#2566)"
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-10-15T19:27:15Z,Add airoboros_v3 chat template (llama-2 format) (#2564)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-10-12T21:54:02Z,"Revert ""Improve Support for Mistral-Instruct"" (#2552)"
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-10-12T18:47:50Z,Improve Support for Mistral-Instruct (#2547)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-10-11T18:08:14Z,Add Zephyr 7B Alpha (#2535)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-10-09T20:06:32Z,Improve docs (#2534)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-10-09T19:51:37Z,Add ExllamaV2 Inference Framework Support. (#2455)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-10-09T19:47:31Z,add Llama2ChangAdapter (#2510)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-10-02T21:13:07Z,Add Mistral AI instruction template (#2483)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-09-18T20:41:57Z,Fix docs
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-09-18T20:18:38Z,"merge google/flan based adapters: T5Adapter, CodeT5pAdapter, FlanAdapter (#2411)"
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-09-18T03:06:14Z,add dtype and seed (#2430)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-09-18T02:33:55Z,Improve docs (#2438)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-09-18T02:01:58Z,Add falcon 180B chat conversation template (#2384)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-09-18T01:58:03Z,"Add support for Phind-CodeLlama models (#2415) (#2416)

Co-authored-by: Lianmin Zheng <lianminzheng@gmail.com>"
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-09-18T01:38:32Z,Add Ascend NPU support (#2422)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-09-13T05:07:34Z,Add support for baichuan2 models (#2408)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-09-12T04:04:46Z,Added google/flan models and fixed AutoModelForSeq2SeqLM when loading T5 compression model (#2402)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-09-11T17:51:51Z,"Spicyboros + airoboros 2.2 template update. (#2392)

Co-authored-by: Jon Durbin <jon.durbin@onna.com>"
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-09-07T06:29:27Z,Make E5 adapter more restrict to reduce mismatch (#2381)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-09-01T01:34:32Z,Remove hardcode flash-attn disable setting (#2342)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-08-28T09:13:29Z,"Add Code Llama Support and Fix empty system prompt for llama 2 (#2326)

Signed-off-by: woshiyyya <xiaoyunxuan1998@gmail.com>"
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-08-27T09:21:41Z,Improve gradio demo (#2323)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-08-24T07:05:34Z,Add new model to the arena (#2296)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-08-21T23:37:12Z,Add conversation support for VMware's OpenLLaMa OpenInstruct models (#2278)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-08-21T06:21:56Z,[Minor] Style clean up & Fix embeding (#2272)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-08-21T04:51:11Z,Update embedding logic (#2244)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-08-21T04:49:01Z,"Add Intel AMX/AVX512 support to accelerate inference (#2247)

Signed-off-by: LeiZhou-97 <lei.zhou@intel.com>"
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-08-21T04:23:16Z,[Minor] Update the warning to follow the new conv_template file (#2248)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-08-16T10:37:27Z,Add support for Vigogne models (#2236)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-08-15T11:26:19Z,Add Llama2-Chinese model support (#2218)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-08-13T00:56:02Z,Improve docs (#2210)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-08-13T00:15:29Z,"Add Support of bge model family for embedding generation (#2203)

Co-authored-by: “Extremys” <“Extremys@email.com”>"
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-08-11T14:56:13Z,"Modify vllm compatible empty special token, and revise qwen (#2200)"
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-08-10T01:11:01Z,revise qwen adapter (#2191)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-08-09T19:43:15Z,feat: support BAAI/AquilaChat-7B. (#2192)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-08-09T05:25:22Z,Format code (#2189)
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-08-09T02:14:20Z,"Add WizardCoder model support (#2166)

Co-authored-by: valentin <valentin.planes@voxaya.com>
Co-authored-by: Valentin PLANES <valentin@valentins-macbook-air.home>"
github.com/lm-sys/FastChat,fastchat/model/model_adapter.py,2023-08-08T13:43:05Z,1. add shell scripts for shutdowning serve; 2. add a feature to launch all serve related to openai-api-server in one cmd; (#2141)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2024-02-27T21:48:50Z,fix: use eos token in target tensor for instruction-tuning (#3945)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2024-01-12T06:10:07Z,Add per-step token utilization to tensorboard and progress tracker. (#3867)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-12-15T00:04:03Z,Add LLM Text Encoder (#3828)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-12-14T23:15:28Z,"hack: workaround torch bug to unblock mixtral fine-tuning (#3830)

Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-12-14T20:58:06Z,Add support for Phi-1 and Phi 1.5 (#3831)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-12-12T20:01:52Z,Remove quantization config before saving dequantized weights to disk (#3825)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-12-12T04:01:34Z,Move reusable LLM model methods to utility functions (#3821)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-12-05T08:22:30Z,Add support for dequantizing 4-bit bitsandbytes base models into fp16 (#3799)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-11-23T00:21:06Z,Add NEFTune implementation for Noised Embedding Instruction Fine-Tuning support (#3744)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-11-20T23:13:17Z,Cleanup: Use existing HFTokenizer to consolidate manual pad token setting. (#3774)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-11-04T02:11:44Z,Pinning Transformers to not include version 4.35.0 because it breaks a method in PEFT.
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-11-04T02:11:44Z,WIP
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-10-28T02:20:41Z,Define initialize_llm() and generate() methods. Remove extra logging in llm.py (#3711)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-10-25T21:00:45Z,Fix: Prevent memory from ballooning during post-training evaluation (#3756)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-10-20T15:45:34Z,Retry LLM model downloads from HF Hub with exponential backoff when there is a Read timeout (#3742)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-10-13T08:17:48Z,"Dynamically set `max_new_tokens` based on output feature length, GMSL and model window size (#3713)"
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-10-11T17:34:55Z,QoL: Only log generation config being used once at inference time (#3715)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-10-05T15:31:36Z,[BUGFIX] Ensure that full base models and not only adapter weights get saved when merge_and_unload is set (#3679)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-10-02T15:35:27Z,"Fix dynamic generation config load during `model.predict` (#3666)

Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-09-29T13:23:46Z,Fix eos_token and pad_token issue (#3667)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-09-28T18:13:13Z,"[MAINTENANCE] Partially reconcile type hints, fix some warnings, and fix comments in parts of the codebase. (#3673)"
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-09-27T19:29:33Z,[FEATURE] Support Merging LoRA Weights Into Base Model (Issue-3603) (#3649)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-09-22T20:25:16Z,Remove unnecessary peft config updating (#3642)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-09-18T17:40:04Z,QoL: Default to using fast tokenizer for Llama models (#3625)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-09-15T04:01:24Z,fix: Load 8-bit quantized models for eval after fine-tuning (#3606)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-09-12T00:36:39Z,Allow user to specify huggingface link or local path to pretrained lora weights (#3572)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-08-28T09:42:25Z,Set default max_sequence_length to None for LLM text input/output features (#3547)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-08-25T20:18:23Z,Refactor evaluation metrics to support decoded generated text metrics like BLEU and ROUGE. (#3539)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-08-15T19:51:46Z,Improve observability during LLM inference  (#3536)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-08-11T21:40:25Z,Move loss metric to same device as inputs (#3522)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-08-11T15:55:47Z,Add mechanic to override default values for generation during model.predict() (#3520)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-07-30T22:49:30Z,"Add RoPE scaling to increase context length up to 8K for training or inference. (#3477)

Co-authored-by: Travis Addair <tgaddair@gmail.com>"
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-07-29T19:44:46Z,"Enable evaluation with LLMs <7B (#3478)

Co-authored-by: Arnav Garg <arnav@predibase.com>"
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-07-29T18:47:29Z,Fixed QLoRA with mutli-gpu and reduce CUDA memory pressure during eval (#3486)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-07-28T16:48:49Z,Exclude frozen weights from checkpoints and fix evaluation using quantized LLMs (#3483)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-07-24T16:18:05Z,Add QLoRA for 4-bit fine-tuning (#3476)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-07-22T08:57:41Z,"Zero copy initialization of models onto training workers for LLMs (#3469)

Co-authored-by: Geoffrey Angus <geoffrey@predibase.com>
Co-authored-by: Travis Addair <tgaddair@gmail.com>"
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-06-29T07:38:49Z,[llm] fix device placement issues when using CPUs and GPUs during LLM fine tuning (#3447)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-06-29T00:25:36Z,"[llm] Replace `model_name` with *required* `base_model`, add preset LLM registry, update internal adapter modules (#3423)

Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Martin Davis <martin@predibase.com>
Co-authored-by: ksbrar <kabir@brar.org>"
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-06-23T18:24:35Z,[LLM] Various fixes for LLM Fine-Tuning issues that caused loss disparity between train and val sets (#3437)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-06-07T07:42:32Z,[LLM] Skip left padding removal when there is no left padding (#3432)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-05-25T01:06:25Z,[llm] Fixed loading when performing full fine-tuning (#3421)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-05-23T06:48:05Z,[llm] Fixed adapter initialization and OOM on checkpoint loading (#3416)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-05-22T16:10:26Z,"[llm] Create separate Predictor for LLMs and enable flash attention on CUDA (#3409)

Co-authored-by: Arnav Garg <arnav@predibase.com>"
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-05-20T03:30:53Z,[LLM] Fix Loss Computation for LLM Fine-tuning using shifted tensors/new loss function (#3408)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-05-16T00:09:06Z,"[LLM] Add Prefix Tuning, PTuning, LoRA, AdaLoRA and Adaption Prompt for LLM fine-tuning (#3386)

Co-authored-by: Arnav Garg <arnav@predibase.com>"
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-05-06T07:38:05Z,[LLM] Fine-Tune LLMs via Prompt tuning (#3359)
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-05-03T18:28:13Z,"[LLM] Few-shot learning via Retrieval-augmented ICL (#3351)

Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/ludwig-ai/ludwig,ludwig/models/llm.py,2023-04-15T00:04:06Z,"Support zero-shot learning and text generation through LLMs (#3335)

Co-authored-by: Travis Addair <tgaddair@gmail.com>"
github.com/nomic-ai/gpt4all,gpt4all-training/eval_self_instruct.py,2023-10-24T13:28:21Z,make scripts executable (#1555)
github.com/nomic-ai/gpt4all,gpt4all-training/eval_self_instruct.py,2023-05-01T19:45:23Z,mono repo structure
github.com/h2oai/h2ogpt,src/gen.py,2024-03-01T21:51:24Z,Fix allowed_paths
github.com/h2oai/h2ogpt,src/gen.py,2024-02-28T02:23:31Z,"Change to default of document choice in sidebar, on average easier even if not cleaner"
github.com/h2oai/h2ogpt,src/gen.py,2024-02-28T02:03:10Z,save liked chatbots
github.com/h2oai/h2ogpt,src/gen.py,2024-02-28T00:05:04Z,"Control visibility of reviews, keep likeable default always on"
github.com/h2oai/h2ogpt,src/gen.py,2024-02-27T21:03:18Z,Update after transformers assertion
github.com/h2oai/h2ogpt,src/gen.py,2024-02-27T02:35:32Z,Cleanup
github.com/h2oai/h2ogpt,src/gen.py,2024-02-26T22:41:17Z,"Fix AWQ, old models are not compatible"
github.com/h2oai/h2ogpt,src/gen.py,2024-02-26T22:32:03Z,"Update OpenAI server and add client docs&tests for other client parameters like langchain, like inspired by https://github.com/h2oai/h2ogpt/pull/1433/.  Fix AWQ."
github.com/h2oai/h2ogpt,src/gen.py,2024-02-26T17:22:42Z,"Ensure don't break h2oGPT when new models come out for openai, anthropic, mistralai, google"
github.com/h2oai/h2ogpt,src/gen.py,2024-02-25T05:51:17Z,Handle case when local server doesn't need to know about Gated repo
github.com/h2oai/h2ogpt,src/gen.py,2024-02-24T19:18:31Z,Handle auth_access=open or closed for OpenAI server
github.com/h2oai/h2ogpt,src/gen.py,2024-02-23T05:40:33Z,"Close responses for streaming in case incomplete yield, e.g. timeout"
github.com/h2oai/h2ogpt,src/gen.py,2024-02-23T02:22:13Z,Update openai client for https://github.com/openai/openai-python/issues/1181
github.com/h2oai/h2ogpt,src/gen.py,2024-02-22T23:11:21Z,Close OpenAI connection once done
github.com/h2oai/h2ogpt,src/gen.py,2024-02-22T04:32:57Z,Groq via OpenAI API
github.com/h2oai/h2ogpt,src/gen.py,2024-02-22T04:30:02Z,groq fix for temperature=0 using openai api
github.com/h2oai/h2ogpt,src/gen.py,2024-02-21T00:23:51Z,Wrap gradio __h2oai__
github.com/h2oai/h2ogpt,src/gen.py,2024-02-20T23:34:18Z,Limit public portal to no more than 4 llms at once
github.com/h2oai/h2ogpt,src/gen.py,2024-02-20T18:31:01Z,Handle soft linked /tmp/gradio
github.com/h2oai/h2ogpt,src/gen.py,2024-02-15T19:41:42Z,Add note about --enable_tts and --enable_stt using more GPU memory
github.com/h2oai/h2ogpt,src/gen.py,2024-02-13T06:52:18Z,"If user goes from not logged in to logged in, persist user id from previous, so can still use dbs."
github.com/h2oai/h2ogpt,src/gen.py,2024-02-12T21:54:02Z,"Disable streaming chunking if not using model lock, for https://github.com/h2oai/h2ogpt/issues/1367#issuecomment-1938913813"
github.com/h2oai/h2ogpt,src/gen.py,2024-02-12T21:44:02Z,Handle url for TheBloke GGUF
github.com/h2oai/h2ogpt,src/gen.py,2024-02-12T08:31:21Z,cosmetics
github.com/h2oai/h2ogpt,src/gen.py,2024-02-12T08:12:37Z,Fixes #1270
github.com/h2oai/h2ogpt,src/gen.py,2024-02-10T09:11:52Z,"Use GradioClient for llava too, so new hash for each user, so image isn't re-used if pass no image"
github.com/h2oai/h2ogpt,src/gen.py,2024-02-10T08:15:56Z,Fix regenerate or not
github.com/h2oai/h2ogpt,src/gen.py,2024-02-10T06:54:26Z,Make llava faster
github.com/h2oai/h2ogpt,src/gen.py,2024-02-10T01:51:49Z,Note about https://github.com/gradio-app/gradio/issues/7379
github.com/h2oai/h2ogpt,src/gen.py,2024-02-08T06:40:10Z,"change in gradio4 for image upload, can't push string"
github.com/h2oai/h2ogpt,src/gen.py,2024-02-07T19:04:55Z,Fixes https://github.com/h2oai/h2ogpt/issues/1378#issuecomment-1932658797
github.com/h2oai/h2ogpt,src/gen.py,2024-02-06T21:46:30Z,Generalize vllm so can pass api_key and url extras too
github.com/h2oai/h2ogpt,src/gen.py,2024-02-06T21:41:06Z,"Fix evaluate use, fix check of img_file if not set, and relax openai model checks a bit"
github.com/h2oai/h2ogpt,src/gen.py,2024-02-06T10:44:41Z,cosmetics
github.com/h2oai/h2ogpt,src/gen.py,2024-02-06T10:37:07Z,Make Vision Q/A work for CLI/eval
github.com/h2oai/h2ogpt,src/gen.py,2024-02-06T09:11:37Z,Fix gradio_server setting so plain doesn't trigger both context and chat_conversation to have conversation history
github.com/h2oai/h2ogpt,src/gen.py,2024-02-06T08:14:58Z,Handle gradio->gradio for Vision Q/A
github.com/h2oai/h2ogpt,src/gen.py,2024-02-06T04:28:39Z,Use llava prompt so can handle chat history
github.com/h2oai/h2ogpt,src/gen.py,2024-02-06T02:57:31Z,"WIP for langchain vision model passthrough.  Handle more general image files, simple gridnumbers.gif fails, still fails on ChatGPT"
github.com/h2oai/h2ogpt,src/gen.py,2024-02-05T22:23:15Z,pass image from UI/API to Vision models
github.com/h2oai/h2ogpt,src/gen.py,2024-02-05T21:43:46Z,Add vision models as llms
github.com/h2oai/h2ogpt,src/gen.py,2024-02-05T00:20:00Z,"Fixes #1361 -- just UI/embed_info issue, was still using OpenAI for embeddings"
github.com/h2oai/h2ogpt,src/gen.py,2024-02-04T02:42:57Z,Resolve
github.com/h2oai/h2ogpt,src/gen.py,2024-02-04T01:31:12Z,protection if user didn't upgrade all packages
github.com/h2oai/h2ogpt,src/gen.py,2024-02-04T01:20:49Z,Faster transfers
github.com/h2oai/h2ogpt,src/gen.py,2024-01-31T19:40:10Z,"Account for beacon memory, don't use for now"
github.com/h2oai/h2ogpt,src/gen.py,2024-01-31T08:44:01Z,Improve conditional
github.com/h2oai/h2ogpt,src/gen.py,2024-01-31T08:42:09Z,Set namespace-Pt/activation-beacon-llama2-7b-chat as llama2 type and try to clear cace for some cache bug
github.com/h2oai/h2ogpt,src/gen.py,2024-01-29T02:03:40Z,Resolve conflict
github.com/h2oai/h2ogpt,src/gen.py,2024-01-26T01:33:24Z,Improve exception handling
github.com/h2oai/h2ogpt,src/gen.py,2024-01-25T23:20:16Z,Ensure use locked and copied version of stream from gradio server
github.com/h2oai/h2ogpt,src/gen.py,2024-01-21T11:14:06Z,"min_max_new_tokens=256 is too strict for more modern LLMs, default to 512 to avoid truncation"
github.com/h2oai/h2ogpt,src/gen.py,2024-01-21T11:00:26Z,"Avoid need for openvllm, just use openai, pass vllm-specific things through extra_body key, i.e. https://github.com/01-ai/Yi/issues/223#issuecomment-1897731789"
github.com/h2oai/h2ogpt,src/gen.py,2024-01-19T08:18:08Z,"Only need requests for HF, gradio client already fails fast if system not up."
github.com/h2oai/h2ogpt,src/gen.py,2024-01-19T02:01:07Z,"Cleanup of new stream function, but still avoid since still hangs in gradio sync code"
github.com/h2oai/h2ogpt,src/gen.py,2024-01-18T22:53:17Z,"Bring back old for now, doesn't hang"
github.com/h2oai/h2ogpt,src/gen.py,2024-01-18T22:11:05Z,"Avoid late import, get clients early just don't store if regenerate_clients=True.  Other clean-up"
github.com/h2oai/h2ogpt,src/gen.py,2024-01-18T05:59:58Z,Separate out GradioClient streaming part with h2oGPT related logic
github.com/h2oai/h2ogpt,src/gen.py,2024-01-17T10:23:28Z,Adjust presence_penalty down a bit
github.com/h2oai/h2ogpt,src/gen.py,2024-01-17T09:56:18Z,Avoid conversation system prompt if empty system prompt
github.com/h2oai/h2ogpt,src/gen.py,2024-01-16T06:47:35Z,Conform more to OpenAI API for proxy server for Issue #1217
github.com/h2oai/h2ogpt,src/gen.py,2024-01-15T06:30:03Z,MistralAI
github.com/h2oai/h2ogpt,src/gen.py,2024-01-14T05:20:29Z,More for https://github.com/h2oai/h2ogpt/issues/1217#issuecomment-1890474230
github.com/h2oai/h2ogpt,src/gen.py,2024-01-14T05:08:27Z,Probably Fixes empty openai_chat/vllm_chat messages as https://github.com/h2oai/h2ogpt/issues/1217#issuecomment-1890474230
github.com/h2oai/h2ogpt,src/gen.py,2024-01-11T23:40:02Z,Avoid plot for string scoring models
github.com/h2oai/h2ogpt,src/gen.py,2024-01-11T23:24:49Z,"Get all models in gen.py, not cli/eval codes duplicated"
github.com/h2oai/h2ogpt,src/gen.py,2024-01-11T18:40:15Z,Merge branch 'main' into verifier
github.com/h2oai/h2ogpt,src/gen.py,2024-01-11T16:42:21Z,WIP generalize scorer to allow for verifier
github.com/h2oai/h2ogpt,src/gen.py,2024-01-11T00:33:06Z,"For Issue #1217 -- force do_sample=False if temp=0, allow temp=0 from UI."
github.com/h2oai/h2ogpt,src/gen.py,2024-01-10T08:01:28Z,Resolve conflict
github.com/h2oai/h2ogpt,src/gen.py,2024-01-10T07:43:26Z,"Stop clearing torch cache periodically too, and no ping gpu if public.  Add memory debug, disabled by default."
github.com/h2oai/h2ogpt,src/gen.py,2024-01-10T03:03:24Z,Fixes #1048
github.com/h2oai/h2ogpt,src/gen.py,2024-01-10T02:50:20Z,"Merge pull request #1272 from h2oai/windows_jan8

Windows update Jan 8, 2024"
github.com/h2oai/h2ogpt,src/gen.py,2024-01-09T20:56:18Z,make windows installer easier for GPU and exlpain how to tweak install and code at runtime
github.com/h2oai/h2ogpt,src/gen.py,2024-01-08T17:27:31Z,cleanup for using transformers for attention sinks
github.com/h2oai/h2ogpt,src/gen.py,2024-01-07T11:13:16Z,For Issue #1248 -- dedup and clean-up a bit model list from disk
github.com/h2oai/h2ogpt,src/gen.py,2024-01-06T08:28:12Z,Add verifier
github.com/h2oai/h2ogpt,src/gen.py,2024-01-06T06:14:57Z,"Show sources in separate chat by default, so easier to copy non-source text from UI"
github.com/h2oai/h2ogpt,src/gen.py,2024-01-06T00:36:28Z,"For Issue #1217 -- decouple prompt_type from instruct or few-shot mode for langchain, so can use h2oGPT as pass through with --prompt_type=plain when using vllm_chat etc. and let inference server handling prompting"
github.com/h2oai/h2ogpt,src/gen.py,2024-01-05T21:39:24Z,Try to work around https://github.com/encode/httpx/discussions/3043
github.com/h2oai/h2ogpt,src/gen.py,2024-01-05T21:19:16Z,"user None removes specific messages already, but parse out if present too"
github.com/h2oai/h2ogpt,src/gen.py,2024-01-05T11:48:34Z,"Show exceptions in chat by default inside accordion that is removed when going to LLM, and add more images/video/audio for document view"
github.com/h2oai/h2ogpt,src/gen.py,2024-01-05T10:11:24Z,"Remove HYDE accordion outputs if present before giving history to LLM, and remove chat=True/False for prompt generation, hold-over and led to bugs in prompting for gradio->gradio"
github.com/h2oai/h2ogpt,src/gen.py,2024-01-05T06:34:22Z,Add note
github.com/h2oai/h2ogpt,src/gen.py,2024-01-05T04:57:55Z,Control llava prompt
github.com/h2oai/h2ogpt,src/gen.py,2024-01-04T23:33:59Z,Stop showing 12b
github.com/h2oai/h2ogpt,src/gen.py,2024-01-04T23:13:54Z,Prompt template info for Issue #1257
github.com/h2oai/h2ogpt,src/gen.py,2024-01-03T00:48:48Z,"Go back to checking system hash since stored in docker image now, even if takes 0.2s, worth it. Could delay checks to every minute or something, but more risky."
github.com/h2oai/h2ogpt,src/gen.py,2024-01-01T21:10:06Z,Fixes for vllm_chat for Issue #1217
github.com/h2oai/h2ogpt,src/gen.py,2023-12-29T02:59:16Z,Fixes #1245
github.com/h2oai/h2ogpt,src/gen.py,2023-12-29T02:17:06Z,Avoid redundant return if not streaming
github.com/h2oai/h2ogpt,src/gen.py,2023-12-27T04:51:40Z,Figure out restriction on visible models
github.com/h2oai/h2ogpt,src/gen.py,2023-12-27T04:28:36Z,max visible models
github.com/h2oai/h2ogpt,src/gen.py,2023-12-24T06:46:17Z,"force openai -> openai_chat if using chat models, so simpler code and correct handling of system prompt counting"
github.com/h2oai/h2ogpt,src/gen.py,2023-12-23T04:22:51Z,"Better auto system prompt, and go to auto as default"
github.com/h2oai/h2ogpt,src/gen.py,2023-12-22T11:17:45Z,Accordion intermediate HYDE results by default.  For https://github.com/h2oai/h2ogpt/issues/1128#issuecomment-1866938571
github.com/h2oai/h2ogpt,src/gen.py,2023-12-22T09:51:03Z,"Update so gradio4 client works with gradio4 server, but gradio4 can't talk to gradio3"
github.com/h2oai/h2ogpt,src/gen.py,2023-12-22T01:57:48Z,Improve testing for OpenAI server and fix key issues with auth etc.
github.com/h2oai/h2ogpt,src/gen.py,2023-12-21T12:41:55Z,OpenAI Proxy Server redirects to Gradio Server
github.com/h2oai/h2ogpt,src/gen.py,2023-12-20T08:12:48Z,"Update tests, and require max_seq_len if using gradio->gradio so first gradio knows that for docQA handling etc., since can't just get tokenizer for llama/gptj, would have to get model too."
github.com/AI4Finance-Foundation/FinGPT,fingpt/FinGPT_Forecaster/app.py,2023-11-08T13:08:09Z,add FinGPT_Forecaster
github.com/haotian-liu/LLaVA,llava/model/builder.py,2024-02-06T16:39:06Z,Fix lora loading #1075
github.com/haotian-liu/LLaVA,llava/model/builder.py,2024-02-03T04:34:33Z,Improve serving.
github.com/haotian-liu/LLaVA,llava/model/builder.py,2024-02-02T22:08:26Z,Fix multiple GPU inference.
github.com/haotian-liu/LLaVA,llava/model/builder.py,2024-01-31T06:04:23Z,Release LLaVA-v1.6
github.com/haotian-liu/LLaVA,llava/model/builder.py,2023-11-24T13:55:43Z,"🩹 make ``load_pretrained_model`` accept kwargs

- this allows it to e.g. take a ``cache_dir`` argument"
github.com/haotian-liu/LLaVA,llava/model/builder.py,2023-10-31T20:09:27Z,Update macOS support.
github.com/haotian-liu/LLaVA,llava/model/builder.py,2023-10-06T23:49:36Z,"For inference in model_worker, allow the device to be specified via a command line parameter.

Right now it has only been tested with Apple Sillicon devices via the mps device."
github.com/haotian-liu/LLaVA,llava/model/builder.py,2023-09-01T16:45:00Z,Update instruction for LoRA
github.com/haotian-liu/LLaVA,llava/model/builder.py,2023-07-29T23:55:59Z,Update docs
github.com/haotian-liu/LLaVA,llava/model/builder.py,2023-07-19T09:08:02Z,Release v1.0.0
github.com/h2oai/h2ogpt,finetune.py,2024-01-05T10:11:24Z,"Remove HYDE accordion outputs if present before giving history to LLM, and remove chat=True/False for prompt generation, hold-over and led to bugs in prompting for gradio->gradio"
github.com/h2oai/h2ogpt,finetune.py,2023-10-24T21:46:27Z,"Avoid some warnings from langchain and transformers, but one transformers warning left that is their issue: https://github.com/huggingface/transformers/issues/27049"
github.com/h2oai/h2ogpt,finetune.py,2023-08-24T16:06:07Z,"Comment out fsdp for now, not yet working."
github.com/h2oai/h2ogpt,finetune.py,2023-08-23T21:55:57Z,Upgrade peft/accelerate/transformers.
github.com/h2oai/h2ogpt,finetune.py,2023-08-23T20:48:21Z,Add comment.
github.com/h2oai/h2ogpt,finetune.py,2023-08-23T20:35:53Z,"Fix fine-tuning of Llama2 7B, needs bf16 instead of fp16."
github.com/h2oai/h2ogpt,finetune.py,2023-08-23T20:22:06Z,Update fsdp
github.com/h2oai/h2ogpt,finetune.py,2023-08-17T17:23:04Z,Fixes #678
github.com/h2oai/h2ogpt,finetune.py,2023-08-09T14:54:58Z,deprecation fix
github.com/h2oai/h2ogpt,finetune.py,2023-08-02T03:30:34Z,Token updates
github.com/h2oai/h2ogpt,finetune.py,2023-07-28T16:41:37Z,"Add env var to other scripts that accepts ""Fire"" arguments"
github.com/h2oai/h2ogpt,finetune.py,2023-07-26T02:38:55Z,Stick to 11.7 so conda install makes sense
github.com/h2oai/h2ogpt,finetune.py,2023-07-21T22:04:22Z,"Fixes #514 -- ensure bot and human type strings filled and account for possible system prompt.  Note that FastSYS versions are probably not right, missing space before end of instruction: [/INST]."
github.com/h2oai/h2ogpt,finetune.py,2023-07-14T20:33:05Z,Fix missing space in docs
github.com/h2oai/h2ogpt,finetune.py,2023-07-11T08:25:19Z,"Simpler use of model loader, and fix instructions to avoid CUDA extension not installed issue"
github.com/h2oai/h2ogpt,finetune.py,2023-07-10T21:22:20Z,Fix imports for fine-tune.
github.com/h2oai/h2ogpt,finetune.py,2023-07-07T22:37:35Z,Adjust generate import for new location
github.com/h2oai/h2ogpt,finetune.py,2023-06-29T06:01:26Z,"If have HF model/tokenizer, use that instead of faketokenizer (tiktoken) since see too large differences and failures even with 250 token buffer, still of by another 350."
github.com/h2oai/h2ogpt,finetune.py,2023-06-25T08:58:45Z,Fix generate_prompt use in fine tune
github.com/h2oai/h2ogpt,finetune.py,2023-06-24T00:08:00Z,Fix prompting
github.com/h2oai/h2ogpt,finetune.py,2023-06-20T22:33:05Z,Better fix for #307
github.com/h2oai/h2ogpt,finetune.py,2023-06-20T22:10:48Z,Fix empty final state.
github.com/h2oai/h2ogpt,finetune.py,2023-06-06T18:32:02Z,Use optional dependencies
github.com/h2oai/h2ogpt,finetune.py,2023-06-06T03:10:57Z,"Make API easier, and add prompt_dict for custom control over prompt as example of new API parameter don't need to pass"
github.com/h2oai/h2ogpt,finetune.py,2023-06-01T15:05:46Z,Add support for Falcon 40b
github.com/h2oai/h2ogpt,finetune.py,2023-05-25T05:55:01Z,Update generation/fine-tuning to handle 4-bit
github.com/h2oai/h2ogpt,finetune.py,2023-05-24T10:33:29Z,"Fix issue with 6.9 being in readme but code having 6_9, so 6.9 would not use correct prompt_type"
github.com/h2oai/h2ogpt,finetune.py,2023-05-23T20:44:36Z,"Move loaders out of finetune, which is only for training, while loader used for generation too"
github.com/h2oai/h2ogpt,finetune.py,2023-05-19T18:30:27Z,"Merge pull request #132 from orellavie1212/patch-1

Update finetune.py"
github.com/h2oai/h2ogpt,finetune.py,2023-05-15T18:40:37Z,"Update finetune.py

FIXED bug model =  set_peft_model_state_dict, as it only set without return value."
github.com/h2oai/h2ogpt,finetune.py,2023-05-14T22:52:48Z,Refactor prompt stuff into single fine instead of finetune
github.com/h2oai/h2ogpt,finetune.py,2023-05-14T22:13:08Z,Resolve conflict
github.com/h2oai/h2ogpt,finetune.py,2023-05-13T18:28:54Z,Add wizard-vicuna
github.com/h2oai/h2ogpt,finetune.py,2023-05-12T04:39:39Z,Add LLama and Vicuna models to eval.
github.com/h2oai/h2ogpt,finetune.py,2023-05-12T01:31:19Z,Add wizardlm prompts.
github.com/h2oai/h2ogpt,finetune.py,2023-05-11T21:15:10Z,"Merge pull request #31 from h2oai/flash-attn

neox Flash attn"
github.com/h2oai/h2ogpt,finetune.py,2023-05-11T21:12:32Z,Revert more changes.
github.com/h2oai/h2ogpt,finetune.py,2023-05-11T21:11:14Z,Revert name change.
github.com/h2oai/h2ogpt,finetune.py,2023-05-11T21:09:54Z,Cleanup.
github.com/h2oai/h2ogpt,finetune.py,2023-05-11T16:06:36Z,Fix model types map.
github.com/h2oai/h2ogpt,finetune.py,2023-05-11T06:49:29Z,Add 2 new gm models.
github.com/h2oai/h2ogpt,finetune.py,2023-05-11T06:06:25Z,"Rebase, rename llama_flash_attn -> flash_attn."
github.com/h2oai/h2ogpt,finetune.py,2023-05-11T04:06:47Z,WIP - nothing working yet. Disable mix_in by default.
github.com/h2oai/h2ogpt,finetune.py,2023-05-10T23:06:05Z,"Allow pipeline to stream, and use for langchain case."
github.com/h2oai/h2ogpt,finetune.py,2023-05-10T18:20:03Z,Add new models to list of human_bot.
github.com/h2oai/h2ogpt,finetune.py,2023-05-08T23:32:29Z,"Use TrainingArguments, more general."
github.com/h2oai/h2ogpt,finetune.py,2023-05-08T19:06:38Z,Undo chat_sep
github.com/h2oai/h2ogpt,finetune.py,2023-05-08T16:40:31Z,End with <human>:\n for human_bot
github.com/h2oai/h2ogpt,finetune.py,2023-05-07T09:57:59Z,"Support OpenAssistant models in basic form, including 30B xor one

Follow: https://huggingface.co/OpenAssistant/oasst-sft-7-llama-30b-xor

source ~/.bashrc.mamba
mamba create -n oasstllamahf
conda activate oasstllamahf
mamba install python==3.10
mkdir oasstllamahf
cd oasstllamahf
git clone https://github.com/huggingface/transformers.git
cd transformers
git checkout d04ec99bec8a0b432fc03ed60cea9a1a20ebaf3c
pip install .
pip install torch==1.13.1 accelerate==0.18.0 sentencepiece==0.1.98 protobuf==3.20.1
pip freeze  # check
python src/transformers/models/llama/convert_llama_weights_to_hf.py --input_dir /data/jon/LLaMA  --output_dir LLaMA30B --model_size 30B
python
> from huggingface_hub import snapshot_download
> snapshot_download(""OpenAssistant/oasst-sft-7-llama-30b-xor"")
> exit()
ls -arlt ~/.cache/huggingface/hub/models--OpenAssistant--oasst-sft-7-llama-30b-xor/snapshots/d31b76426385f683c6efb8d4daea58379fb9bb09/oasst-sft-7-llama-30b-xor/  # check
ls -alrt LLaMA30B/  # check
find LLaMA30B -type f | xargs md5sum  # check
oadir=~/.cache/huggingface/hub/models--OpenAssistant--oasst-sft-7-llama-30b-xor/snapshots/d31b76426385f683c6efb8d4daea58379fb9bb09
python $oadir/xor_codec.py oasst-sft-7-llama-30b/ $oadir/oasst-sft-7-llama-30b-xor/ LLaMA30B
ls -alrt oasst-sft-7-llama-30b/ # check
cd ~/h2ogpt/
ln -s <location of oasst-sft-7-llama-30b> ."
github.com/h2oai/h2ogpt,finetune.py,2023-05-05T23:03:07Z,Specify chat separator
github.com/h2oai/h2ogpt,finetune.py,2023-05-05T19:08:35Z,Default to plain if nothing set in lookup
github.com/h2oai/h2ogpt,finetune.py,2023-05-04T06:46:35Z,Fix extra import
github.com/h2oai/h2ogpt,finetune.py,2023-05-04T06:36:40Z,Work around suspected bug in peft (empty lora weights at end of training).
github.com/h2oai/h2ogpt,finetune.py,2023-05-04T00:19:31Z,"Add h2ogpt-gm models, and prompt_answer prompt_type."
github.com/h2oai/h2ogpt,finetune.py,2023-05-01T09:05:42Z,Give default context to help chatbot
github.com/h2oai/h2ogpt,finetune.py,2023-04-30T20:40:38Z,improve control of devices
github.com/h2oai/h2ogpt,finetune.py,2023-04-27T20:57:19Z,directly control save_total_limit
github.com/h2oai/h2ogpt,finetune.py,2023-04-27T20:55:36Z,Flush log
github.com/h2oai/h2ogpt,finetune.py,2023-04-27T20:23:05Z,"Dup if mixin needs to match training data, and allow prune of token-truncation rows to avoid learning from truncated language"
github.com/h2oai/h2ogpt,finetune.py,2023-04-27T18:49:03Z,Refactor finetune so some of it can be used to check data and its tokenization
github.com/h2oai/h2ogpt,finetune.py,2023-04-26T20:38:51Z,Update peft imports
github.com/h2oai/h2ogpt,finetune.py,2023-04-26T08:24:05Z,Raise if asked for flash attn but don't have it.
github.com/h2oai/h2ogpt,finetune.py,2023-04-26T08:15:34Z,"Refactor a bit to avoid early global imports to control when flash attention is done, so early on.

NOTE: flash attention requires installing cuda 11.7 via https://developer.nvidia.com/cuda-11-7-0-download-archive?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=runfile_local and then when running, to avoid installing driver, docs, samples, just install toolkit.  Then when pip installing flash attention do:

CUDA_HOME=/usr/local/cuda-11.7 pip install flash-attn"
github.com/h2oai/h2ogpt,finetune.py,2023-04-26T07:03:23Z,Add llama flash attention. Add option to disable lora (with lora_r=0)
github.com/h2oai/h2ogpt,finetune.py,2023-04-26T06:30:34Z,Go back to 512 cutoff len.
github.com/h2oai/h2ogpt,finetune.py,2023-04-26T05:41:08Z,"Add distilgpt2 lora target, like gpt2"
github.com/h2oai/h2ogpt,finetune.py,2023-04-26T03:58:13Z,use autocausal for distilgpt2
github.com/h2oai/h2ogpt,finetune.py,2023-04-26T03:33:06Z,"Merge pull request #85 from h2oai/refactor1

Refactor gradio into separate file and isolate it from torch specific stuff"
github.com/h2oai/h2ogpt,finetune.py,2023-04-26T03:00:45Z,Refactor gradio into separate file and isolate it from torch specific stuff
github.com/h2oai/h2ogpt,finetune.py,2023-04-26T02:59:40Z,"Improve error message, pass default dataset."
github.com/h2oai/h2ogpt,finetune.py,2023-04-26T00:22:30Z,"Add option to train with 8-bit, enabled by default."
github.com/h2oai/h2ogpt,finetune.py,2023-04-24T22:29:29Z,"Fix context to have space when using reduce mode, so human-bot consistent in context"
github.com/h2oai/h2ogpt,finetune.py,2023-04-23T17:38:15Z,Updates to model cards and default model selection.
github.com/h2oai/h2ogpt,finetune.py,2023-04-22T09:08:28Z,"Allow system pre-prompt context in non-chat mode, hide context in chat mode since used for history

e.g. could add to chat=False case a context of:

<bot>: I am a helpful assistant that can generate detailed elaborate question from topics such as biology, physics, mathematics, psychology, or computer science.

And then one gives the instruction:

I am a person seeking assistance from a bot.  Please generate an elaborate question.

And that will force the bot to behave a certain way because it has already effectively responded that way."
github.com/h2oai/h2ogpt,finetune.py,2023-04-21T22:00:26Z,Change default model.
github.com/h2oai/h2ogpt,finetune.py,2023-04-21T19:35:02Z,Add some h2oGPT models to finetune/generate examples.
github.com/h2oai/h2ogpt,finetune.py,2023-04-21T03:01:45Z,"Undo change to PreResponse, makes it no longer speak English by default...?"
github.com/h2oai/h2ogpt,finetune.py,2023-04-21T02:44:14Z,"Merge pull request #69 from h2oai/save-output

Add option to save prompt and response as .json."
github.com/h2oai/h2ogpt,finetune.py,2023-04-21T02:01:47Z,Add option to save prompt and response as .json.
github.com/h2oai/h2ogpt,finetune.py,2023-04-21T00:33:05Z,Notes about llama 65B
github.com/h2oai/h2ogpt,finetune.py,2023-04-19T07:16:58Z,Add code to push spaces chatbot
github.com/h2oai/h2ogpt,finetune.py,2023-04-18T05:53:54Z,Update model list.
github.com/h2oai/h2ogpt,finetune.py,2023-04-18T05:51:59Z,Clean up more.
github.com/h2oai/h2ogpt,finetune.py,2023-04-17T17:36:25Z,Add links to HF.
github.com/h2oai/h2ogpt,finetune.py,2023-04-17T17:10:21Z,Add h2ogpt-oig-oasst1-256-12b to list.
github.com/h2oai/h2ogpt,finetune.py,2023-04-17T12:42:22Z,Don't require HF token
github.com/h2oai/h2ogpt,finetune.py,2023-04-17T12:07:37Z,"Clean-up LORA part with controlled visibility, and tell from_pretrained that can use_auth_token=True but no models online yet (even private which would then have worked)"
github.com/h2oai/h2ogpt,finetune.py,2023-04-17T08:43:48Z,"Terminate vicuna output even if keeps generating, until work-around non-uniqueness of its tokens"
github.com/h2oai/h2ogpt,finetune.py,2023-04-17T08:36:23Z,"Work-around gr.State() deepcopy https://github.com/gradio-app/gradio/issues/3558 of model/tokenizer when load model up front from CLI.  But can't seem to get memory off GPU, even .cpu().  Even if references are in another thread, why shouldn't .cpu() + clear cache + collect work?"
github.com/h2oai/h2ogpt,finetune.py,2023-04-17T07:21:10Z,Make human_bot not add the Date/Time.
github.com/h2oai/h2ogpt,finetune.py,2023-04-17T03:22:22Z,"Simplify handling of stopping words since not all extra can be removed, just remove pad if in front.  Add dolly and instruct_with_end."
github.com/h2oai/h2ogpt,finetune.py,2023-04-17T00:41:12Z,"update transformers and remove assert, and fix use of special tokens in stopping for cases when object not str"
github.com/h2oai/h2ogpt,finetune.py,2023-04-16T23:48:35Z,"Merge pull request #40 from h2oai/instruct_vicuna

Another prompt + fix gradio add with state"
github.com/h2oai/h2ogpt,finetune.py,2023-04-16T21:46:59Z,"Add another prompt type, but prompt parsing is prone to mistakes, best to use unique token or stop generation on text if prompts are text-based."
github.com/h2oai/h2ogpt,finetune.py,2023-04-16T18:19:21Z,Add pythia models.
github.com/h2oai/h2ogpt,finetune.py,2023-04-15T20:38:04Z,"Add score model, defaulting to https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2

Still some problems with model/data on different devices for multi-GPU. Infer helps some for non-lora case, since often puts on 1 GPU, even if 20B could have been on multi-GPU spread."
github.com/h2oai/h2ogpt,finetune.py,2023-04-15T10:28:06Z,"If no prompt_type at start, but know about model, choose reasonable one"
github.com/h2oai/h2ogpt,finetune.py,2023-04-15T10:12:21Z,Comment-out WIP
github.com/h2oai/h2ogpt,finetune.py,2023-04-15T10:10:52Z,"Allow fast start without model, show message if user didn't load yet"
github.com/h2oai/h2ogpt,finetune.py,2023-04-15T09:37:05Z,"Support model control at run-time, including loading models/loras, adding new HF models/loras.  Ensure GPU memory cleared between switching, including cache."
github.com/huggingface/peft,src/peft/tuners/ia3/model.py,2024-02-20T14:12:34Z,"FIX Correctly unload double wrapped modules (#1490)

Resolves #1485, but note that some additional solutions are mentioned in
thet issue.

This checks that when unloading a PEFT model, if the
ModulesToSaveWrapper contains a tuner module, it is correctly unloaded.
The unloaded model should not have PEFT layers at the end."
github.com/huggingface/peft,src/peft/tuners/ia3/model.py,2024-02-08T13:39:46Z,DOC How to freeze adapter after set_adapter call (#1447)
github.com/huggingface/peft,src/peft/tuners/ia3/model.py,2024-02-07T11:52:35Z,MNT Move code quality fully to ruff (#1421)
github.com/huggingface/peft,src/peft/tuners/ia3/model.py,2024-01-09T11:18:31Z,"Refactor dispatching logic of LoRA layers (#1319)

This PR's goal is to simplify the logic for deciding which LoRA layer
backend is being used when LoRA is applied to a target layer.

Originally, this refactor was done in #1286 which was about adding the
""fast"" backend for LoRA, but since that PR was closed, I moved the
refactor to this dedicated PR.

Motivation

Right, now, the LoraModel._create_new_module method has become quite
complex and hard to read, spanning >100 lines:

https://github.com/huggingface/peft/blob/8665e2b5719faa4e4b91749ddec09442927b53e0/src/peft/tuners/lora/model.py#L235-L339

The reason for this is that method contains the logic for deciding which
LoRA layer backend to use for all the different types of LoRA layers
that we have, i.e. normal Linear layer, Conv2d layer, bnb layer, gptq,
etc.

Description

To remedy this, I moved the logic for deciding which layer to match to
the respective implementation of the layers. For example, in
lora/layer.py, there is now a function called dispatch_default, whose
responsibility it is to decide if an Embedding layer, Conv2d layer or
Linear layer is the right match. Similarly, in lora/bnb.py, there are
now the two functions dispatch_bnb_8bit and dispatch_bnb_4bit to decide
what/if any bnb 8bit or 4bit layer should be matched.

This way, the logic to decide what layer to match now resides next to
the respective layers. The only thing that LoraModel now needs to do is
to collect all the dispatching methods and use the first layer that
matches.

Note that only LoRA was modified, the other tuners don't have different
backends and thus this approach was not necessary for them. The only
exception is IA³, which has the normal and bnb backend. Since those are
only 2, it's not as complicated as for LoRA, but if this PR is accepted,
I can refactor IA³ in a similar fashion.

Other changes

- Removed the optional_kwargs argument from _create_and_replace, as it
  was an unnecessary indirection.
- Removed the bias argument from kwargs, as it was not used.

Backwards compatibility

This should be fully backwards compatible, as the constructed LoRA model
is 100% the same. If there are users that override _create_new_module,
their code will probably break, but since this is a private method, we
should be fine."
github.com/huggingface/peft,src/peft/tuners/ia3/model.py,2023-12-18T09:59:17Z,"Refactor and a couple of fixes for adapter layer updates (#1268)

* Refactor: Move LoRA update_layer to child classes

For LoRA, so far, we have update_layer for Linear,
update_layer_embedding for Embedding, and update_layer_conv2d for
Conv2d, all defined on LoraLayer.

We can simplify the code by always using the name update_layer, and by
moving the layer-specific methods to the subclasses. So e.g.
update_layer_embedding is moved to the Embedding class and renamed to
update_layer. This way, the caller does not need to differentiate which
type of layer it's calling.

Interestingly, this was already practiced for IA³, so the same change
was not necessary there. But I did find the same method implemented
twice, once on IA3Layer and once on Linear, so I removed one of the
duplicates

* Systematic handling of r (rank) <= 0

Always raise an error when r <= 0, not only for LoRA. Also, removed
later check for r > 0 in LoRA layers, since we already check for r <= 0.

* Fix broken __repr__ method on QuantLinear

Was indented too deep, thus not being applied.

* Fix bug for updating Lora GPTQ and IA3 bnb layers

Before this fix, when adding a 2nd adapter to a model, we did not
correctly check if there was already an adapter layer in the model when
dealing with LoRA GPTQ or IA3 bnb layers. As a consequence, instead of
updating the existing layers, we would create a new layer and the
existing layer would be set as the base_layer of that new layer. Now, we
correctly update the existing layer to add the new adapter.

Note that for this fix to work correctly with LoRA and GPTQ, I had to
add a check for qweight, since we only checked for weight before.

Tests were added to check this. They fail with the current main but are
fixed with this PR.

* Don't match AdaLoraLayer when updating LoraLayers

AdaLoraLayer is a subclass of LoraLayer, so just checking for
isinstance(target, LoraLayer) will match AdaLoraLayer, which we don't
want when it comes to updating a LoraLayer. Now, we explicitly check
that the layer is *not* an instance of AdaLoraLayer."
github.com/huggingface/peft,src/peft/tuners/ia3/model.py,2023-12-12T14:34:45Z,"Fix: Multiple adapters with bnb layers (#1243)

Resolves #1239

Fixes a bug that led to an error when loading multiple adapters into a
peft model that uses bnb layers.

Also: Fix for loading 2nd adapter with AutoGPTQ"
github.com/huggingface/peft,src/peft/tuners/ia3/model.py,2023-12-11T11:35:28Z,"FIX Use model argument consistently (#1198) (#1205)

Some methods were using model and self.model interchangeably. This was
fine, as they were referring to the same object, but is also confusing.
Now model is used consistently."
github.com/huggingface/peft,src/peft/tuners/ia3/model.py,2023-12-07T15:39:08Z,"Lazy import of bitsandbytes (#1230)

Previously, we imported from bitsandbytes eagerly if the package was
installed. This caused two major issues:

- Slow loading time of PEFT (~4 sec)
- Errors with multiprocessing because bnb initializes CUDA

This commit fixes both issues by importing bitsandbytes lazily. PEFT
import time is now reduced to ~2sec.

Notes

Implementation-wise, I use a combination of local imports and
module-level __getattr__. The latter was introduced in Python 3.7 and
should therefore be safe to use."
github.com/huggingface/peft,src/peft/tuners/ia3/model.py,2023-12-06T13:44:58Z,"Raise error when `modules_to_save` is specified and multiple adapters are being unloaded (#1137)

* handle `modules_to_save` when unloading

* address comments

* Apply suggestions from code review

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* quality

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/tuners/ia3/model.py,2023-12-04T11:22:03Z,"DOC: Update & improve docstrings and type annotations for common methods and classes (#1201)

The docstrings of the most user-exposed methods and classes have been
updated, or added if not already present. Furthermore, type annotations
have been updated or added for those methods and classes.

---------

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>
Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/tuners/ia3/model.py,2023-11-20T17:22:52Z,ENH Delete IA3 adapters (#1153)
github.com/huggingface/peft,src/peft/tuners/ia3/model.py,2023-11-16T11:45:12Z,"Refactor base layer pattern (#1106)

Description

Refactor all tuners (where it applies, i.e. not prompt tuning) to use
the ""base layer pattern"". This means that the adapter layer will always
hold a reference to the original layer that it modifies. This pattern is
already partly used (e.g. LoRA bnb, gptq layers), now it is consistently
used everywhere when applicable.

This PR is a companion PR to #1069, where I first added these changes.
They are now extracted to a separate PR to make code review easier and
to advance more quickly.

Implementation

The main change is that the adapter layer wraps the original layer and
calls forward on that layer, instead of doing stuff like this:

F.linear(input, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)

which completely circumvents the call to the target layer's forward
method. With the base layer pattern, we now call the target layer's
forward method. Therefore, if the target layer is another adapter
layer (which will be crucial for mixed adapters), we call its forward
method correctly. Also, this should allow passing extra arguments, like
lora_scale to forward.

This change has the nice side benefit that we no longer need to use
_init_empty_weights -- in fact, we don't initialize any of the target
layer's weights anymore, since we have a reference to it. There is thus
no risk of having slow but superfluous initialization of layers.

Moreover, I could greatly simplify merge_and_unload by just using the
base_layer instead of having to create a completely new layer. For
OPT-350m, this results in a 15x speedup.

Note that same as for the bnb layers, this should be backwards
incompatible, since the adapter weights and their state_dicts are not
affected by this change. I used #1115 for regression testing.

Somewhat unrelated changes

During debugging, I got very annoyed with the fact that the reprs of
adapter layers and normal PyTorch layers are hard to distinguish, e.g.
the type is just ""Linear"". Now, for adapter layers, it is prefixed by
the adapter type, e.g. ""lora.Linear"". This should have no further
implications except for the repr (e.g. state_dict remains unaffected).

For LoHa and LoKr, I had to change the init of weights when using
init_weights=False. This is because of what is discussed in Numerical
instabilities with LoHa #1058.

IA³ now has the unload method too.

LoHa and LoKr now support safe_merge=True when merging layers.

Migration guide

For 99% of users, the code should continue working as ususal, because
the API stays the same. Only low level details have been changed.

Code that relies on isinstance checks on specific PEFT classes may
break. E.g. the LoRA Linear layer no longer inherits from nn.Linear. It
is, however, still a BaseTunerLayer. The same logic applies for other
layer types like Conv2d and for other tuners like IA³.

To retrieve the base layer of an adapter layer, you should now call
module.get_base_layer() if you deal with a BaseTunerLayer. Don't rely on
something like module.weight being present (though it might be)."
github.com/huggingface/peft,src/peft/tuners/ia3/model.py,2023-11-16T11:05:22Z,"FEAT: Merging only specified `adapter_names` when calling `merge` (#1132)

* working v1

* add tests

* remove

* add it also for lokr and loha, left a todo

* Update tests/testing_common.py

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>

* better test

* up

* fix tests

* credits contrib and suggestions from disscussions

* credits contrib and suggestions from disscussions

* address last comments

---------

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>
Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/tuners/ia3/model.py,2023-11-10T12:33:56Z,"Refactor adapter deletion (#1105)

Description

The job of deleting an adapter is now transferred to the adapter layer,
instead of the adapter model. This makes it easier for users or other
libraries who don't use the adapter model to delete adapters.

Implementation

The code should now be more generic, relying less on hard-coded
attributes.

As a precaution, I also changed the type of adapter_layer_names from
list to tuple, as it should not be mutated.

When deleting the active adapter, the logic for choosing the new active
adapter has been changed slightly to ensure consistency across layers.
In practice, this should rarely make a difference. An error is now
raised if the last remaining adapter is deleted.

Test coverage has been increased:

- Deleting adapters is now also tested for custom models.
- It is also tested for LoHa, LoKr, not only LoRA.
- I added a test for deleting the non-active adapter.

Not implemented

I did not add adapter deletion to IA³, since it is included in #980. LMK
if it should be added here instead."
github.com/huggingface/peft,src/peft/tuners/ia3/model.py,2023-11-01T10:39:40Z,"TST test coverage for layer matching (#1031)

Add tests for module name matching using regex and other custom arguments."
github.com/huggingface/peft,src/peft/tuners/ia3/model.py,2023-10-26T13:51:49Z,FIX Conv1D merge error for IA3 (#1014)
github.com/huggingface/peft,src/peft/tuners/ia3/model.py,2023-10-09T16:28:00Z,"FEAT: Add `safe_merge` option in `merge` (#1001)

* add `safe_merge` option in `merge`

* oops

* Apply suggestions from code review

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* address final comments

* Update src/peft/tuners/lora/layer.py

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* Update src/peft/tuners/lora/layer.py

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* add it for ia3

* add it for adalora

* up

* revert for loha

* style

* fix CI

* adapt from suggestions

* add tests

* up

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/tuners/ia3/model.py,2023-10-09T10:20:19Z,"ENH Support Conv2d layers for IA³ (#972)

Adds support for Conv2D layers to the IA³ tuner. Tests are added to
check that they work.

Notes:

Unfortunately, when unmerging the Conv2d IA³ layers, there is quite a
bit of rounding error. I had to increase the tolerances for this
specific test case to make the tests pass. I'm not 100% sure why this
is, but I could imagine that for Conv2d, small errors accumulate because
of the convolution operation.

I also added tests for IA³ Linear layers for the custom models, which
also pass. However, there is an error when using Conv1D. The reason is
that merging fails because there is a shape mismatch when
fan_in_fan_out=True (which is set automatically for Conv1D). This is
left for a future PR."
github.com/huggingface/peft,src/peft/tuners/ia3/model.py,2023-10-05T07:57:49Z,"Fix lora creation (#993)

* reducing the time for inject lora modules

* fix bugs

* fix bug

* fixes

* Revert ""fixes""

This reverts commit c7f30627c1798db11be8a5da8f3c801f9469a5e3.

* refactor

* fix failing tests

* fix tests

* fix tests

* fix tests

* fix tests

* Apply suggestions from code review

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* address comments

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/tuners/ia3/model.py,2023-09-29T04:14:30Z,"[tests] add multiple active adapters tests (#961)

* add tests for multiple active adapters

* add multiple active adapter tests

* fix tests

* fix the device error

* fix typo

* fix the variables

* fix the `adalora` config

* add util function for proper naming of tests

* fix bugs

1. fix `add_weighted_adapter` when working with adapters targeting different layers
2. fix `ia3` model and layer to handle adapters targeting different layers
3. fix the multiple active adapter tests

* fix `ia3` issue

* remove debug statements

* fix test

* fix bug

* address comments

* address comments

Co-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* fix tests

* remove unused code

* Update test_custom_models.py

* increasing tolerance for a test

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/tuners/ia3/model.py,2023-09-26T12:11:32Z,"ENH Add 4-bit support for IA3 (#864)

Notes:

- Add guard to IA³ Linear8bitLt definition (should have already been there).
- Merging not supported (yet)."
github.com/huggingface/peft,src/peft/tuners/ia3/model.py,2023-09-26T07:31:05Z,"FIX: setting requires_grad on adapter layers (#905)

* [WIP] Fix setting requires_grad on adapter layers

This is an alternative to #900, resolves #899.

Description

Currently, we don't handle setting requires_grad on adapter layers
really well. The main issue is that it can be set to True on adapter
parameters that are not being used, e.g. the original_module in
ModulesToSaveWrapper or inactive adapters in LoRA.

Normally, this is not a big issue, except maybe if we want to correctly
count the number of trainable parameters. However, when training with
DistributedDataParallel, this results in errors, as PyTorch thinks that
all parameters with requires_grad=True should participate in the loss
computation, but those mentioned parameters don't. For that reason,
training with DDP currently fails when using modules_to_save or multiple
adapters.

Implementation

This turned out to be more complicated than I initially thought. The
logic for setting requires_grad is all over the place, it was hard to
encapsulate the logic and I only succeeded partially. As is, this PR is
more complex than the one it tries to supersede, #900, but it is also
""more correct"".

Tests were added to check whether requires_grad is set correctly. There
are (so far) no tests for whether DDP indeed works, they could be added
with multi-GPU. I did, however, test an early stage of this PR with DDP
and setting requires_grad correctly will indeed fix the DDP error.

DONE/TODO

- [x] ModulesToSaveWrapper
- [x] LoRA
- [ ] IA³
- [ ] AdaLora

Since some tuners are not implemented yet, tests are expected to fail.
Check the new tests at the bottom of test_custom.py, those should pass.

* Refactor: move more requires_grad machinery to ABC

* [skip ci] [WIP] Add requires_grad logic to IA³

* Add AdaLora

* Fix some minor issues

* Make style"
github.com/huggingface/peft,src/peft/tuners/ia3/model.py,2023-09-22T20:03:44Z,"support multiple ranks and alphas for LoRA (#873)

* support multiple ranks and alphas

* Update lora.py

* Update lora.py

* commit suggestions

Co-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* address comments

Co-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* Fixed multirank + multialpha for sequential LoRAs, added correct support of LoRA-C3Lier conversion (#937)

* Fixed multirank multialpha for sequential loras, added tests, fixed docs

* Refactored kohya_ss conversion script for proper support of LoRA-C3Lier

* Fixed styling

* Removed old comment from docstring

* shift `scale_layer`/`unscale_layer` to `LoraLayer` class to support all the child classes

* support multiple active adapters

* add `active_adapters` property

Co-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* fix bug related to active adapter of `ModulesToSaveWrapper`

* revert the change wrt active_adapter assignment

Co-Authored-By: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* addressing comments

* address comments

* address comment

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>
Co-authored-by: Alexander Kovalchuk <kovalexal@gmail.com>
Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/tuners/ia3/model.py,2023-08-29T09:32:29Z,"MNT: Move tuners to subpackages (#807)

For each tuner, created a sub-module that contains at least:

- config.py for config stuff
- model.py for the actual model/encoder/embedding
- __init__.py so that imports are preserved

Then, when there was a need, further files were created, like layer.py
or utils.py.

Imports were changed to absolute imports everywhere, except for the
sub-packages within a tuner directory, as these packages will always 
stay together in the same place.

For some existing modules, the license comment of the top of the file
was missing, I always added it.

There was a bug in the forward method of 4bit linear lora layers introduced
in #851, for the case that the model is merged AND adapters are disabled.
For that scenario, we need to unmerge first before generating the output,
same as we do for the vanilla Linear layer. This step was missing from the
code previously and is now implemented correctly. Tests were adjusted to
catch that error."
github.com/ludwig-ai/ludwig,ludwig/utils/llm_utils.py,2024-02-27T21:48:50Z,fix: use eos token in target tensor for instruction-tuning (#3945)
github.com/ludwig-ai/ludwig,ludwig/utils/llm_utils.py,2024-01-23T17:52:25Z,Add default LoRA target modules for Phi-2 (#3911)
github.com/ludwig-ai/ludwig,ludwig/utils/llm_utils.py,2024-01-22T19:13:10Z,"Revert ""Revert ""Add support for the official `microsoft/phi-2` model … (#3901)"
github.com/ludwig-ai/ludwig,ludwig/utils/llm_utils.py,2024-01-18T23:02:39Z,"Revert ""Add support for the official `microsoft/phi-2` model (#3880)"" (#3898)"
github.com/ludwig-ai/ludwig,ludwig/utils/llm_utils.py,2024-01-18T18:03:51Z,Add support for the official `microsoft/phi-2` model (#3880)
github.com/ludwig-ai/ludwig,ludwig/utils/llm_utils.py,2024-01-11T21:51:21Z,Add streaming support for zero shot inference (#3878)
github.com/ludwig-ai/ludwig,ludwig/utils/llm_utils.py,2023-12-18T19:08:56Z,Add support for Phi 2 (#3838)
github.com/ludwig-ai/ludwig,ludwig/utils/llm_utils.py,2023-12-15T00:04:03Z,Add LLM Text Encoder (#3828)
github.com/ludwig-ai/ludwig,ludwig/utils/llm_utils.py,2023-12-14T20:58:06Z,Add support for Phi-1 and Phi 1.5 (#3831)
github.com/ludwig-ai/ludwig,ludwig/utils/llm_utils.py,2023-12-12T04:01:34Z,Move reusable LLM model methods to utility functions (#3821)
github.com/ludwig-ai/ludwig,ludwig/utils/llm_utils.py,2023-11-20T23:13:17Z,Cleanup: Use existing HFTokenizer to consolidate manual pad token setting. (#3774)
github.com/ludwig-ai/ludwig,ludwig/utils/llm_utils.py,2023-10-25T21:00:45Z,Fix: Prevent memory from ballooning during post-training evaluation (#3756)
github.com/ludwig-ai/ludwig,ludwig/utils/llm_utils.py,2023-10-13T08:17:48Z,"Dynamically set `max_new_tokens` based on output feature length, GMSL and model window size (#3713)"
github.com/ludwig-ai/ludwig,ludwig/utils/llm_utils.py,2023-09-12T21:07:51Z,Add codellama to tokenizer list for set_pad_token (#3598)
github.com/ludwig-ai/ludwig,ludwig/utils/llm_utils.py,2023-09-08T17:36:19Z,"Add support for Paged Optimizers (Adam, Adamw), 8-bit optimizers, and new optimizers: LARS, LAMB and LION (#3588)

Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/ludwig-ai/ludwig,ludwig/utils/llm_utils.py,2023-08-25T20:18:23Z,Refactor evaluation metrics to support decoded generated text metrics like BLEU and ROUGE. (#3539)
github.com/ludwig-ai/ludwig,ludwig/utils/llm_utils.py,2023-06-29T07:38:49Z,[llm] fix device placement issues when using CPUs and GPUs during LLM fine tuning (#3447)
github.com/ludwig-ai/ludwig,ludwig/utils/llm_utils.py,2023-06-23T18:24:35Z,[LLM] Various fixes for LLM Fine-Tuning issues that caused loss disparity between train and val sets (#3437)
github.com/ludwig-ai/ludwig,ludwig/utils/llm_utils.py,2023-06-07T07:42:32Z,[LLM] Skip left padding removal when there is no left padding (#3432)
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2024-02-27T11:02:11Z,"FEAT Implement DoRA (#1474)

Add DoRA (Weight-Decomposed Low-Rank Adaptation).

https://arxiv.org/abs/2402.09353

To use this with LoRA, add use_dora=True to the LoraConfig.

Currently only supports nn.Linear layers, not other types or
quantized linear layers like bnb."
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2024-02-22T01:31:04Z,"AQLM support for LoRA (#1476)

* aqlm

* Style and copied tests

* aqlm import guadr

* docs

* correct model in tests

* Update docs/source/developer_guides/quantization.md

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* Update docs/source/developer_guides/quantization.md

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* moved aqlm install and added >=

* Removed `quant_linear_module`

* AqlmLoraLinear

* docs update

* transformers version check

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2024-02-20T14:12:34Z,"FIX Correctly unload double wrapped modules (#1490)

Resolves #1485, but note that some additional solutions are mentioned in
thet issue.

This checks that when unloading a PEFT model, if the
ModulesToSaveWrapper contains a tuner module, it is correctly unloaded.
The unloaded model should not have PEFT layers at the end."
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2024-02-19T00:31:21Z,"FEAT: add awq suppot in PEFT (#1399)

* add awq suppot in PEFT

* fix

* fux

* Update src/peft/tuners/lora/awq.py

* style & fix tests

* forward contrib credits from PR14084

* forward contrib credits from autoawq PR

* change name

* fix

* change to peft internal testing

* fix

* fix

* add multi-GPU tests

* add to dockerfile

* fix todo

* raise error only at the dispatch level

* quality

* fix test

* fix dockerfile

* fix

* fix

* update dockerfile and tests

---------

Co-authored-by: s4rduk4r <s4rduk4r@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2024-02-15T12:29:39Z,"add `magnitude_prune` merging method (#1466)

* add `magnitude_prune` merging method

* Update model.py

* 😅"
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2024-02-12T17:31:15Z,"[docs] Docstring typo (#1455)

* fix typo

* fix"
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2024-02-09T06:40:04Z,"Add new merging methods (#1364)

* add code

* update docstring

* quality

* fix test

* fix test

* fix svd embedding layer merging

* fixes

* fixes

* Update model.py

* Add test and example

* quality

* fix tests

* update the example

* Apply suggestions from code review

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* address comments

* address comments and add co-authors

Co-Authored-By: Prateek Yadav <15224633+prateeky2806@users.noreply.github.com>
Co-Authored-By: Yu Le <55241218+yule-buaa@users.noreply.github.com>
Co-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* quality

* Update merge_utils.py

* revert

* address comments

* address comment

---------

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>
Co-authored-by: Prateek Yadav <15224633+prateeky2806@users.noreply.github.com>
Co-authored-by: Yu Le <55241218+yule-buaa@users.noreply.github.com>
Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2024-02-08T13:39:46Z,DOC How to freeze adapter after set_adapter call (#1447)
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2024-02-07T11:52:35Z,MNT Move code quality fully to ruff (#1421)
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2024-02-06T00:54:06Z,Fix typos (#1435)
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2024-01-09T11:18:31Z,"Refactor dispatching logic of LoRA layers (#1319)

This PR's goal is to simplify the logic for deciding which LoRA layer
backend is being used when LoRA is applied to a target layer.

Originally, this refactor was done in #1286 which was about adding the
""fast"" backend for LoRA, but since that PR was closed, I moved the
refactor to this dedicated PR.

Motivation

Right, now, the LoraModel._create_new_module method has become quite
complex and hard to read, spanning >100 lines:

https://github.com/huggingface/peft/blob/8665e2b5719faa4e4b91749ddec09442927b53e0/src/peft/tuners/lora/model.py#L235-L339

The reason for this is that method contains the logic for deciding which
LoRA layer backend to use for all the different types of LoRA layers
that we have, i.e. normal Linear layer, Conv2d layer, bnb layer, gptq,
etc.

Description

To remedy this, I moved the logic for deciding which layer to match to
the respective implementation of the layers. For example, in
lora/layer.py, there is now a function called dispatch_default, whose
responsibility it is to decide if an Embedding layer, Conv2d layer or
Linear layer is the right match. Similarly, in lora/bnb.py, there are
now the two functions dispatch_bnb_8bit and dispatch_bnb_4bit to decide
what/if any bnb 8bit or 4bit layer should be matched.

This way, the logic to decide what layer to match now resides next to
the respective layers. The only thing that LoraModel now needs to do is
to collect all the dispatching methods and use the first layer that
matches.

Note that only LoRA was modified, the other tuners don't have different
backends and thus this approach was not necessary for them. The only
exception is IA³, which has the normal and bnb backend. Since those are
only 2, it's not as complicated as for LoRA, but if this PR is accepted,
I can refactor IA³ in a similar fashion.

Other changes

- Removed the optional_kwargs argument from _create_and_replace, as it
  was an unnecessary indirection.
- Removed the bias argument from kwargs, as it was not used.

Backwards compatibility

This should be fully backwards compatible, as the constructed LoRA model
is 100% the same. If there are users that override _create_new_module,
their code will probably break, but since this is a private method, we
should be fine."
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2024-01-09T05:31:30Z,"Extend merge_and_unload to offloaded models (#1190)

* activated pre-forward

* activated pre-forward hook

* activated pre-forward hook

* activated pre-forward hook

* debugged hook call

* added explicit forwards

* debugged

* debugged

* fixed pre-forward hook call

* fixed pre-forward hook call

* debugged module iteration

* fixed post forward args

* added conditional attr check

* fixed conditional attr check

* memory overflow debug

* memory overflow debug

* added mem trace

* added mem trace

* more memory traces

* debug memory leak

* debug memory leak

* removed replace

* removed device assign during replacement

* no grad during replacement

* new module hook

* to cpu

* to cpu

* removed replace module

* conditional on replace module

* removed traces

* make style

* added back replace_module

* added test and make style

* inline key, module

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* fixed test and make style

* reverted _unload_and_optionally_merge and moved test

* match main

* make style

* reverted model.py

* make style

* reverted merge

* fetched model.py from head

* added onload

* debug

* removed replace module

* removed replace module

* pre forward on target and parent

* removed _replace_module

* reverted

* debugged

* debugged

* traced adapters

* debugged

* added trace on adapter names

* onloaded target

* further traces

* further traces

* further traces

* further traces

* further traces

* onloaded adapters

* onload module

* onload module

* onload module

* debugged

* debugged

* debugged

* removed delta weight onload

* revamped delta weight onload

* revamped delta weight onload

* removed replace module

* added parent and target act

* debugged

* debugged

* added traces

* added traces

* added traces

* init hook

* init hook

* traces

* traces

* specd weights map

* removed traces and offload check

* post forwards on lora

* added post forward for target and parent

* added trace

* removed traces and tp post forwards

* added onloads and offloads to embedding and conv2d

* updated test

* make style

* debugged and make style

* refactored and make style

* cleaned

* refactored and make style

* cleaned

* cleaned

* make style

* make style

* disk offload compatibility

* refactored linear onload via contextmanager

* refactored onloads

* debugged

* tempfile to tempfolder

* changed disk offload to original directory

* refactored for general tuners

* debugged

* explicit base layer

* added base traces

* more traces

* debugged;

* reverted lora layer.py

* removed traces and make style

* cleaned

* removed todo

* fixed test and cleaned

* added suggestions and make style

* onload for unmerge and merge_and_unload

* improved docstring

* onload target only and make style

* Update src/peft/tuners/tuners_utils.py

Co-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>

* revised descriptions

* make style

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>
Co-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2023-12-18T09:59:17Z,"Refactor and a couple of fixes for adapter layer updates (#1268)

* Refactor: Move LoRA update_layer to child classes

For LoRA, so far, we have update_layer for Linear,
update_layer_embedding for Embedding, and update_layer_conv2d for
Conv2d, all defined on LoraLayer.

We can simplify the code by always using the name update_layer, and by
moving the layer-specific methods to the subclasses. So e.g.
update_layer_embedding is moved to the Embedding class and renamed to
update_layer. This way, the caller does not need to differentiate which
type of layer it's calling.

Interestingly, this was already practiced for IA³, so the same change
was not necessary there. But I did find the same method implemented
twice, once on IA3Layer and once on Linear, so I removed one of the
duplicates

* Systematic handling of r (rank) <= 0

Always raise an error when r <= 0, not only for LoRA. Also, removed
later check for r > 0 in LoRA layers, since we already check for r <= 0.

* Fix broken __repr__ method on QuantLinear

Was indented too deep, thus not being applied.

* Fix bug for updating Lora GPTQ and IA3 bnb layers

Before this fix, when adding a 2nd adapter to a model, we did not
correctly check if there was already an adapter layer in the model when
dealing with LoRA GPTQ or IA3 bnb layers. As a consequence, instead of
updating the existing layers, we would create a new layer and the
existing layer would be set as the base_layer of that new layer. Now, we
correctly update the existing layer to add the new adapter.

Note that for this fix to work correctly with LoRA and GPTQ, I had to
add a check for qweight, since we only checked for weight before.

Tests were added to check this. They fail with the current main but are
fixed with this PR.

* Don't match AdaLoraLayer when updating LoraLayers

AdaLoraLayer is a subclass of LoraLayer, so just checking for
isinstance(target, LoraLayer) will match AdaLoraLayer, which we don't
want when it comes to updating a LoraLayer. Now, we explicitly check
that the layer is *not* an instance of AdaLoraLayer."
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2023-12-15T11:16:59Z,"ENH Rank-stabilized LoRA scaling option (#1244)

Add option to scale LoRA weights by alpha/sqrt(r) by passing
LoraConfig(..., use_rslora=True).

https://doi.org/10.48550/arXiv.2312.03732"
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2023-12-12T14:34:45Z,"Fix: Multiple adapters with bnb layers (#1243)

Resolves #1239

Fixes a bug that led to an error when loading multiple adapters into a
peft model that uses bnb layers.

Also: Fix for loading 2nd adapter with AutoGPTQ"
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2023-12-11T11:35:28Z,"FIX Use model argument consistently (#1198) (#1205)

Some methods were using model and self.model interchangeably. This was
fine, as they were referring to the same object, but is also confusing.
Now model is used consistently."
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2023-12-07T15:39:08Z,"Lazy import of bitsandbytes (#1230)

Previously, we imported from bitsandbytes eagerly if the package was
installed. This caused two major issues:

- Slow loading time of PEFT (~4 sec)
- Errors with multiprocessing because bnb initializes CUDA

This commit fixes both issues by importing bitsandbytes lazily. PEFT
import time is now reduced to ~2sec.

Notes

Implementation-wise, I use a combination of local imports and
module-level __getattr__. The latter was introduced in Python 3.7 and
should therefore be safe to use."
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2023-12-06T13:44:58Z,"Raise error when `modules_to_save` is specified and multiple adapters are being unloaded (#1137)

* handle `modules_to_save` when unloading

* address comments

* Apply suggestions from code review

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* quality

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2023-12-04T11:22:03Z,"DOC: Update & improve docstrings and type annotations for common methods and classes (#1201)

The docstrings of the most user-exposed methods and classes have been
updated, or added if not already present. Furthermore, type annotations
have been updated or added for those methods and classes.

---------

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>
Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2023-11-30T15:24:58Z,"Megatron distributed parallel linear LoRA (#1092)

Adds option to use Megatron's ColumnParallelLinear and RowParallelLinear
for LoRA linear layers, leading to improved performance when using LoRA
with Megatron."
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2023-11-29T16:08:17Z,"Add LoftQ initialization method for LoRA (#1150)

---------

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>
Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2023-11-22T12:14:21Z,"fix `add_weighted_adapter` method (#1169)

* fix `add_weighted_adapter` method

Co-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>
Co-Authored-By: jihuishan <151612440+jihuishan@users.noreply.github.com>

* Update testing_common.py

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>
Co-authored-by: jihuishan <151612440+jihuishan@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2023-11-16T11:45:12Z,"Refactor base layer pattern (#1106)

Description

Refactor all tuners (where it applies, i.e. not prompt tuning) to use
the ""base layer pattern"". This means that the adapter layer will always
hold a reference to the original layer that it modifies. This pattern is
already partly used (e.g. LoRA bnb, gptq layers), now it is consistently
used everywhere when applicable.

This PR is a companion PR to #1069, where I first added these changes.
They are now extracted to a separate PR to make code review easier and
to advance more quickly.

Implementation

The main change is that the adapter layer wraps the original layer and
calls forward on that layer, instead of doing stuff like this:

F.linear(input, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)

which completely circumvents the call to the target layer's forward
method. With the base layer pattern, we now call the target layer's
forward method. Therefore, if the target layer is another adapter
layer (which will be crucial for mixed adapters), we call its forward
method correctly. Also, this should allow passing extra arguments, like
lora_scale to forward.

This change has the nice side benefit that we no longer need to use
_init_empty_weights -- in fact, we don't initialize any of the target
layer's weights anymore, since we have a reference to it. There is thus
no risk of having slow but superfluous initialization of layers.

Moreover, I could greatly simplify merge_and_unload by just using the
base_layer instead of having to create a completely new layer. For
OPT-350m, this results in a 15x speedup.

Note that same as for the bnb layers, this should be backwards
incompatible, since the adapter weights and their state_dicts are not
affected by this change. I used #1115 for regression testing.

Somewhat unrelated changes

During debugging, I got very annoyed with the fact that the reprs of
adapter layers and normal PyTorch layers are hard to distinguish, e.g.
the type is just ""Linear"". Now, for adapter layers, it is prefixed by
the adapter type, e.g. ""lora.Linear"". This should have no further
implications except for the repr (e.g. state_dict remains unaffected).

For LoHa and LoKr, I had to change the init of weights when using
init_weights=False. This is because of what is discussed in Numerical
instabilities with LoHa #1058.

IA³ now has the unload method too.

LoHa and LoKr now support safe_merge=True when merging layers.

Migration guide

For 99% of users, the code should continue working as ususal, because
the API stays the same. Only low level details have been changed.

Code that relies on isinstance checks on specific PEFT classes may
break. E.g. the LoRA Linear layer no longer inherits from nn.Linear. It
is, however, still a BaseTunerLayer. The same logic applies for other
layer types like Conv2d and for other tuners like IA³.

To retrieve the base layer of an adapter layer, you should now call
module.get_base_layer() if you deal with a BaseTunerLayer. Don't rely on
something like module.weight being present (though it might be)."
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2023-11-16T11:05:22Z,"FEAT: Merging only specified `adapter_names` when calling `merge` (#1132)

* working v1

* add tests

* remove

* add it also for lokr and loha, left a todo

* Update tests/testing_common.py

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>

* better test

* up

* fix tests

* credits contrib and suggestions from disscussions

* credits contrib and suggestions from disscussions

* address last comments

---------

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>
Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2023-11-10T12:33:56Z,"Refactor adapter deletion (#1105)

Description

The job of deleting an adapter is now transferred to the adapter layer,
instead of the adapter model. This makes it easier for users or other
libraries who don't use the adapter model to delete adapters.

Implementation

The code should now be more generic, relying less on hard-coded
attributes.

As a precaution, I also changed the type of adapter_layer_names from
list to tuple, as it should not be mutated.

When deleting the active adapter, the logic for choosing the new active
adapter has been changed slightly to ensure consistency across layers.
In practice, this should rarely make a difference. An error is now
raised if the last remaining adapter is deleted.

Test coverage has been increased:

- Deleting adapters is now also tested for custom models.
- It is also tested for LoHa, LoKr, not only LoRA.
- I added a test for deleting the non-active adapter.

Not implemented

I did not add adapter deletion to IA³, since it is included in #980. LMK
if it should be added here instead."
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2023-10-25T12:53:45Z,"FIX setting active adapter correctly (#1051)

Currently, when calling set_adapter, the active adapter is not updated.
Tests have been added to trigger the bug and the method updated to fix
it.

Moreover, I created an active_adapters property on the PeftModel class
so that it behaves consistently with the underlying models like
LoraModel."
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2023-10-10T14:47:35Z,"ENH: Refactor LoRA bnb layers for faster initialization (#994)

Partly addresses #896

Description

After speeding up normal LoRA layer initialization, this PR improves
initialization speed of bnb LoRA layers.

The method to achieve this is different from the one used before, namely
this time the base layer is stored as a reference on the LoRA layer.
This allows us to avoid calling __init__ on the bnb layer, which is what
is slow.

Notes

We cannot use the same method as for the normal LoRA layers, (i.e.
calling the super class's __init__ with meta device) because the bnb
layers have extra logic that still creates unnecessary weights.

However, the way used here could also be a solution to the normal
layers, so if we want to have consistency, the normal layers could be
refactored to use the same approach.

Interestingly, even though we now save the base layer as a reference,
which results in a different state_dict, the existing models can still
be loaded successfully. This is because the adapter state_dict is not
affected by the change, so users can still load their existing adapters.

The only problem would occur if users dump the whole model, i.e. base
model and adapter, using torch.save and then trying to load with
torch.load. For those users, we could theoretically provide a script to
convert the state_dict (i.e. renaming some keys).

To ensure that the old adapters can still be loaded successfully, I'm
working at the same time on adding regression tests. I'll create a
separate PR for those to avoid blowing up this one.

Tests

I ran a test on bloomz-1b1 for how long it takes to create the
PeftModel, the results are:

8bit: 1108.34 ms > 26.82 ms
4bit: 1101.96 ms > 23.69 ms"
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2023-10-09T16:28:00Z,"FEAT: Add `safe_merge` option in `merge` (#1001)

* add `safe_merge` option in `merge`

* oops

* Apply suggestions from code review

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* address final comments

* Update src/peft/tuners/lora/layer.py

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* Update src/peft/tuners/lora/layer.py

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* add it for ia3

* add it for adalora

* up

* revert for loha

* style

* fix CI

* adapt from suggestions

* add tests

* up

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2023-10-05T07:57:49Z,"Fix lora creation (#993)

* reducing the time for inject lora modules

* fix bugs

* fix bug

* fixes

* Revert ""fixes""

This reverts commit c7f30627c1798db11be8a5da8f3c801f9469a5e3.

* refactor

* fix failing tests

* fix tests

* fix tests

* fix tests

* fix tests

* Apply suggestions from code review

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* address comments

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2023-10-02T08:44:51Z,"FEAT Add LyCORIS LoHa for SD&SDXL models (#956)

https://arxiv.org/abs/2108.06098"
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2023-09-29T04:14:30Z,"[tests] add multiple active adapters tests (#961)

* add tests for multiple active adapters

* add multiple active adapter tests

* fix tests

* fix the device error

* fix typo

* fix the variables

* fix the `adalora` config

* add util function for proper naming of tests

* fix bugs

1. fix `add_weighted_adapter` when working with adapters targeting different layers
2. fix `ia3` model and layer to handle adapters targeting different layers
3. fix the multiple active adapter tests

* fix `ia3` issue

* remove debug statements

* fix test

* fix bug

* address comments

* address comments

Co-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* fix tests

* remove unused code

* Update test_custom_models.py

* increasing tolerance for a test

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2023-09-26T07:31:05Z,"FIX: setting requires_grad on adapter layers (#905)

* [WIP] Fix setting requires_grad on adapter layers

This is an alternative to #900, resolves #899.

Description

Currently, we don't handle setting requires_grad on adapter layers
really well. The main issue is that it can be set to True on adapter
parameters that are not being used, e.g. the original_module in
ModulesToSaveWrapper or inactive adapters in LoRA.

Normally, this is not a big issue, except maybe if we want to correctly
count the number of trainable parameters. However, when training with
DistributedDataParallel, this results in errors, as PyTorch thinks that
all parameters with requires_grad=True should participate in the loss
computation, but those mentioned parameters don't. For that reason,
training with DDP currently fails when using modules_to_save or multiple
adapters.

Implementation

This turned out to be more complicated than I initially thought. The
logic for setting requires_grad is all over the place, it was hard to
encapsulate the logic and I only succeeded partially. As is, this PR is
more complex than the one it tries to supersede, #900, but it is also
""more correct"".

Tests were added to check whether requires_grad is set correctly. There
are (so far) no tests for whether DDP indeed works, they could be added
with multi-GPU. I did, however, test an early stage of this PR with DDP
and setting requires_grad correctly will indeed fix the DDP error.

DONE/TODO

- [x] ModulesToSaveWrapper
- [x] LoRA
- [ ] IA³
- [ ] AdaLora

Since some tuners are not implemented yet, tests are expected to fail.
Check the new tests at the bottom of test_custom.py, those should pass.

* Refactor: move more requires_grad machinery to ABC

* [skip ci] [WIP] Add requires_grad logic to IA³

* Add AdaLora

* Fix some minor issues

* Make style"
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2023-09-22T20:03:44Z,"support multiple ranks and alphas for LoRA (#873)

* support multiple ranks and alphas

* Update lora.py

* Update lora.py

* commit suggestions

Co-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* address comments

Co-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* Fixed multirank + multialpha for sequential LoRAs, added correct support of LoRA-C3Lier conversion (#937)

* Fixed multirank multialpha for sequential loras, added tests, fixed docs

* Refactored kohya_ss conversion script for proper support of LoRA-C3Lier

* Fixed styling

* Removed old comment from docstring

* shift `scale_layer`/`unscale_layer` to `LoraLayer` class to support all the child classes

* support multiple active adapters

* add `active_adapters` property

Co-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* fix bug related to active adapter of `ModulesToSaveWrapper`

* revert the change wrt active_adapter assignment

Co-Authored-By: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* addressing comments

* address comments

* address comment

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>
Co-authored-by: Alexander Kovalchuk <kovalexal@gmail.com>
Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2023-09-20T09:26:35Z,"ENH error message when choosing wrong bias (#946)

Raise an error with a helpful error message when the user chooses an incorrect
option for the bias argument.

---------

Co-authored-by: datta0 <venkatadattasanimmaturi@gmail.com>"
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2023-09-07T10:14:37Z,"ENH Merge lora module to 8bit model (#875)

Allows merging 8bit weights from bnb.

4bit weight merging was already implemented through the dequantization method
provided by bnb but there is no official dequantization method for 8bit weights.
This PR works by multiplying the weights to an identity matrix using bnb's
quantization aware matmul operation. Empirically, this results in a very small
rounding error."
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2023-09-06T15:31:55Z,"ENH Remove redundant initialization layer calls (#887)

This should lead to a big speedup when initializing LoRA layers.

---------

Co-authored-by: poedator <ruslansv@gmail.com>"
github.com/huggingface/peft,src/peft/tuners/lora/model.py,2023-08-29T09:32:29Z,"MNT: Move tuners to subpackages (#807)

For each tuner, created a sub-module that contains at least:

- config.py for config stuff
- model.py for the actual model/encoder/embedding
- __init__.py so that imports are preserved

Then, when there was a need, further files were created, like layer.py
or utils.py.

Imports were changed to absolute imports everywhere, except for the
sub-packages within a tuner directory, as these packages will always 
stay together in the same place.

For some existing modules, the license comment of the top of the file
was missing, I always added it.

There was a bug in the forward method of 4bit linear lora layers introduced
in #851, for the case that the model is merged AND adapters are disabled.
For that scenario, we need to unmerge first before generating the output,
same as we do for the vanilla Linear layer. This step was missing from the
code previously and is now implemented correctly. Tests were adjusted to
catch that error."
github.com/hpcaitech/ColossalAI,applications/Chat/examples/community/peft/easy_models.py,2023-09-19T06:20:26Z,"[misc] update pre-commit and run all files (#4752)

* [misc] update pre-commit

* [misc] run pre-commit

* [misc] remove useless configuration files

* [misc] ignore cuda for clang-format"
github.com/hpcaitech/ColossalAI,applications/Chat/examples/community/peft/easy_models.py,2023-04-06T07:04:48Z,add community example dictionary (#3465)
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/inference/gradio_demo.py,2023-07-15T06:17:18Z,fix code style
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/inference/gradio_demo.py,2023-07-15T06:10:54Z,update emperical formula
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/inference/gradio_demo.py,2023-07-13T13:11:14Z,refactoring code
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/inference/gradio_demo.py,2023-07-13T04:19:21Z,fix potential inv_freq issue; add alpha argument
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/inference/gradio_demo.py,2023-07-13T01:56:50Z,update max_position_embeddings
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/inference/gradio_demo.py,2023-07-13T00:48:15Z,add pathces for memory_efficient_attention and NTK scaling
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/inference/gradio_demo.py,2023-07-06T02:01:24Z,"update banner path, change default decoding values"
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/inference/gradio_demo.py,2023-07-05T09:26:51Z,"Merge pull request #705 from ymcui/context_extend

Extend context size (8K+) without fine-tuning"
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/inference/gradio_demo.py,2023-07-05T08:39:09Z,replace position interploation with NTK method
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/inference/gradio_demo.py,2023-07-04T01:32:19Z,fix Codacy issues
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/inference/gradio_demo.py,2023-07-04T01:21:34Z,fix Codacy issues
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/inference/gradio_demo.py,2023-07-03T10:13:14Z,fix output speed in gradio demo
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/inference/gradio_demo.py,2023-07-03T01:37:10Z,add Position Interpolation for inference scripts
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/inference/gradio_demo.py,2023-06-28T13:46:37Z,fix: fix English output missing space and tune default option
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/inference/gradio_demo.py,2023-06-26T13:58:28Z,fix: make unused argument max_memory into use
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/inference/gradio_demo.py,2023-06-26T13:31:03Z,refactor: Apply GPT-4 instructions to refactor the code
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/inference/gradio_demo.py,2023-06-18T12:14:59Z,chore: change port
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/inference/gradio_demo.py,2023-06-18T12:12:39Z,feat: support stream output for gradio demo
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/inference/gradio_demo.py,2023-05-31T14:24:03Z,fix banner path
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/inference/gradio_demo.py,2023-05-31T10:05:13Z,Merge remote-tracking branch 'origin/33b' into gradio_notebook
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/inference/gradio_demo.py,2023-05-26T02:24:38Z,"reorganize scripts folder structure

⚠️ note that these changes will take effect in next version."
github.com/stanfordnlp/dspy,dsp/modules/hf.py,2024-03-01T21:18:19Z,Big reformat push
github.com/stanfordnlp/dspy,dsp/modules/hf.py,2024-02-20T11:34:56Z,Propagate device_map to hf model
github.com/stanfordnlp/dspy,dsp/modules/hf.py,2024-02-13T17:33:28Z,Allow HFModel to use CPU (fix)
github.com/stanfordnlp/dspy,dsp/modules/hf.py,2024-02-13T17:31:49Z,Revert the coding style to original
github.com/stanfordnlp/dspy,dsp/modules/hf.py,2024-02-07T10:21:14Z,Allow HFModel to use CPU
github.com/stanfordnlp/dspy,dsp/modules/hf.py,2024-02-05T10:56:30Z,"chore: move torch init to when it is not client in hf module

this make sure that torch does not need to be installed when hf or vllm model is accessed via api"
github.com/stanfordnlp/dspy,dsp/modules/hf.py,2023-10-11T15:50:28Z,Add multihop_finetune initial notebook
github.com/stanfordnlp/dspy,dsp/modules/hf.py,2023-10-05T01:12:06Z,"Update modules. Include generic ReAct, MultiChainComparison, ChainOfThoughtWithHint, improve dspy.majority, tweak teleprompters, update default OpenAI LM to turbo-instruct"
github.com/stanfordnlp/dspy,dsp/modules/hf.py,2023-08-15T17:37:42Z,Readying for release. Working notebook without cache yet
github.com/stanfordnlp/dspy,dsp/modules/hf.py,2023-04-23T15:44:33Z,minor device usage fix
github.com/stanfordnlp/dspy,dsp/modules/hf.py,2023-04-23T14:48:19Z,minor fix
github.com/stanfordnlp/dspy,dsp/modules/hf.py,2023-04-23T14:45:19Z,export HFModel withtout requiring deps
github.com/stanfordnlp/dspy,dsp/modules/hf.py,2023-04-23T14:24:27Z,minor hf usage fix
github.com/stanfordnlp/dspy,dsp/modules/hf.py,2023-04-14T19:47:09Z,Drop the prompt from a Causal LM model's output
github.com/stanfordnlp/dspy,dsp/modules/hf.py,2023-04-14T19:41:30Z,--cruft
github.com/stanfordnlp/dspy,dsp/modules/hf.py,2023-04-11T21:54:31Z,Add support for Causal LMs
github.com/stanfordnlp/dspy,dsp/modules/hf.py,2023-04-04T21:19:35Z,Add Support for Multi-GPU Inference
github.com/stanfordnlp/dspy,dsp/modules/hf.py,2023-03-27T06:59:08Z,fixing conflicting changes
github.com/stanfordnlp/dspy,dsp/modules/hf.py,2023-03-21T03:11:56Z,Add code for supporting arbitrary Hugging Face models
github.com/hpcaitech/ColossalAI,applications/ColossalEval/colossal_eval/models/huggingface.py,2024-02-06T02:53:03Z, [eval] update llama npu eval (#5366)
github.com/hpcaitech/ColossalAI,applications/ColossalEval/colossal_eval/models/huggingface.py,2023-12-15T07:06:06Z,"Fix ColossalEval (#5186)

Co-authored-by: Xu Yuanchen <yuanchen.xu00@gmail.com>"
github.com/hpcaitech/ColossalAI,applications/ColossalEval/colossal_eval/models/huggingface.py,2023-12-12T06:47:35Z,"[ColossalEval] Support GSM, Data Leakage Evaluation and Tensor Parallel (#5169)

* Support GSM, Data Leakage Evaluation and Tensor Parallel

* remove redundant code and update inference.py in examples/gpt_evaluation

---------

Co-authored-by: Xu Yuanchen <yuanchen.xu00@gmail.com>"
github.com/hpcaitech/ColossalAI,applications/ColossalEval/colossal_eval/models/huggingface.py,2023-11-09T05:41:50Z,"Support mtbench (#5025)

Co-authored-by: Xu Yuanchen <yuanchen.xu00@gmail.com>"
github.com/hpcaitech/ColossalAI,applications/ColossalEval/colossal_eval/models/huggingface.py,2023-10-31T02:30:03Z,"fix ColossalEval (#4992)

Co-authored-by: Xu Yuanchen <yuanchen.xu00@gmail.com>"
github.com/hpcaitech/ColossalAI,applications/ColossalEval/colossal_eval/models/huggingface.py,2023-09-24T15:14:11Z,"[feature] ColossalEval: Evaluation Pipeline for LLMs (#4786)

* Add ColossalEval

* Delete evaluate in Chat

---------

Co-authored-by: Xu Yuanchen <yuanchen.xu00@gmail.com>
Co-authored-by: Tong Li <tong.li352711588@gmail.com>"
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2024-02-12T13:41:35Z,"FIX Honor HF_HUB_OFFLINE mode if set by user (#1454)

Resolves #1452

If users enable offline mode, don't perform checks for files on HF Hub,
as they would fail."
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2024-02-07T11:52:35Z,MNT Move code quality fully to ruff (#1421)
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2024-01-31T06:08:13Z,fix: subfolder existence check (#1417)
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2024-01-30T09:43:47Z,"Fix breaking change  (#1414)

* fix

* Update src/peft/utils/save_and_load.py

* Update src/peft/utils/save_and_load.py"
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2024-01-22T14:46:42Z,save the embeddings even when they aren't targetted but resized (#1383)
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2024-01-12T16:19:12Z,"FEAT Add Poly Adapter (#1129)

Implement the Poly (Polytropon) adapter.

Papers:

- https://arxiv.org/abs/2202.13914
- https://arxiv.org/abs/2211.03831

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2024-01-03T14:35:06Z,"fix diffusers tests (#1317)

* fix diffusers tests

* quality"
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2024-01-03T09:52:26Z,"fix the embedding saving for adaption prompt (#1314)

* fix the embedding saving for adaption prompt

* fix

* automate setting `save_embedding_layers` when embedding layer is resized during finetuning

* fix

* address comment

Co-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* oops

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-12-15T12:05:06Z,"feat: add apple silicon GPU acceleration (#1217)

* feat: add apple silicon GPU acceleration

* Fix device compatibility issue in
load_peft_weights function

* Update save_and_load.py

* Update save_and_load.py

* Update save_and_load.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update src/peft/utils/save_and_load.py

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* Fix string formatting in image_classification_timm_peft_lora.ipynb and multilayer_perceptron_lora.ipynb

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>
Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-11-30T15:58:42Z,"[Feature] Support OFT (#1160)

* Support OFT

* add test

* Update README

* fix code quality

* fix test

* Skip 1 test

* fix eps rule and add more test

* feat: added examples to new OFT method

* fix: removed wrong arguments from model example

* fix: changed name of inference file

* fix: changed prompt variable

* fix docs

* fix: dreambooth inference revision based on feedback

* fix: review from BenjaminBossan

* apply safe merge

* del partially

* refactor oft

* refactor oft

* del unused line

* del unused line

* fix skip in windows

* skip test

* Add comments about bias added place

* rename orig_weights to new_weights

* use inverse instead of linalg.inv

* delete alpha and scaling

---------

Co-authored-by: Lukas Kuhn <lukaskuhn.lku@gmail.com>
Co-authored-by: Lukas Kuhn <lukas.kuhn@deutschebahn.com>"
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-11-29T13:58:41Z,"Training PEFT models with new tokens being added to the embedding layers and tokenizer (#1147)

* add support for saving base layers weights along with adapter weights

* Update save_and_load.py

* Add an example showing the usage of the added feature

* refactor the functionality

* fix

* refactoring code

1. Add `is_embedding_layer_resized` parameter to `save_pretrained`
2. Fix the deduplication in README when adding PEFT details.
3. `save_pretrained` should only save the model when `is_main_process=True` which is one of the parameters of `save_pretrained`.

* update example

* fix the model card

* fix model card

* 😅

* fix model card

* automate setting `is_embedding_layer_resized`

* nits

* Update peft_lora_clm_with_additional_tokens.ipynb

* add test

* fix tests

* maybe fixes the issue?

* address comments

Co-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-11-28T13:17:25Z,FIX Pass HF token when calling PeftModel.from_pretrained (#1076)
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-11-17T14:48:02Z,"Use `huggingface_hub.file_exists` instead of custom helper (#1145)

* Use 'huggingface_hub.file_exists' instead of custom helper

* make quality"
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-10-30T14:36:41Z,"Add implementation of LyCORIS LoKr for SD&SDXL models (#978)

KronA-like adapter"
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-10-02T08:44:51Z,"FEAT Add LyCORIS LoHa for SD&SDXL models (#956)

https://arxiv.org/abs/2108.06098"
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-09-21T07:46:28Z,"Fix some tests that would fail with torch.compile (#949)

Some tests would currently fail with torch.compile, not because there is
anything wrong with how PEFT works with compiled models, but simply
because of the way the tests are written. This is because when models
are compiled, the keys of the state dict change. Tests have now been
adapted to unwrap the compiled model first before getting the state
dict.

Note that the mentioned issue does not affect saving and loading,
because save_pretrained is already called on the original module, so
there is no issue with mismatched keys.

Also fixed the docstring of get_peft_model_state_dict."
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-08-25T06:12:11Z,"🎉 Add Multitask Prompt Tuning (#400)

* mpt

* fix save

* fix save

* add jupyter notebook

* add jupyter notebook

* add jupyter notebook

* drop shuffling

* drop classify_dataset

* drop classify_dataset

* fix keys

* fix keys

* add comments

* use EXACT_SOURCE_TASK in the example

* formatting

* Fix dict index in embedding retrieval

* run style and quality

* run style and quality

* run style and quality

* style

* final fix

* style

* comment out failing tests

* fix generation tests

* fix style and save test

* all testcases

* fix import

* add license header

* reformat

* fix encoder-decoder models

* fix tests running multiple times

* fix paper name for IA3 and add MPT paper

* Trigger CI

* address the recommended changes

* reformat

* address suggestions

* address suggestions

* revert reformatting

* revert reformatting

---------

Co-authored-by: Alex-Brooks <Alex.Brooks@ibm.com>"
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-08-07T14:34:54Z,"[`core`] PEFT refactor + introducing `inject_adapter_in_model` public method (#749)

Refactors a bit the internals of some PEFT models and introduces a new
method inject_adapter_in_model for users that want to pass a bare model
and a peft config to inject adapters in-place into the model. These
changes are totally BC with the previous PEFT versions.

This PR makes things easier for the PEFT integration in transformers
huggingface/transformers#25077

The main goal of the PR is to expose a new API for advanced users that
want to integrate PEFT method without the need to use the PeftModel
wrapper. A simple use case could be someone that wants to inject adapters
into a model and wants to keep the original class of the model without
having to offload that to peft that will create a PeftModel. I have
faced this issue in huggingface/transformers#25077 Among other things,
this PR refactors some internals of PEFT library, while keeping it fully
backward compatible.

To tackle the main motivation I propose to differentiate things between
two type of adapters

1- adapters that are injectable (LoRA, AdaLoRA, IA3)
2- adapters that are not injectable (the rest)

As a first iteration this API would be supported only for the scenario
1- / therefore I decided to create 2 abstract classes to make things
easy to be able to determine if the adapter layer (e.g. LoraLayer) /
adapter module (e.g. LoraModel) does follow the minimal
requirement (i.e. needed attributes, etc.)

Other related changes:

1- Creates a new property method is_prompt_learning to avoid importing
   PromptLearningConfig all the way down
2- Introduces a new object TUNERS_MAPPING, which is a mapping of
   supported pluggable adapters
3- Creates two abstract classes
3.1- BaseTunerLayer: a mixin to check for minimal required attributes
     that a tuner layer should have active_adapter / _is_plugable
3.2- BaseTuner: a higher level module mixin that should be used for any
     injectable adapters in the future.

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-07-13T07:45:50Z,"Add functionality to support IA3 (#578)

* Added initial ia3 code

* Implemented ia3 correctly for feedforward layers; Fixed regex matching

* Fixed module mapping for mt5

* Merged changes from huggingface:main

* Merged changes

* Fixed lora merge conflicts

* Different bloom config

* Added save option for ia3

* Added loading code for ia3

* Added feedforward implementation in utils and seq cls example

* Added feedforward implementation in utils and seq cls example

* Implemented merge, unmerge, enable/disable adapters functionality

* Fixed feedforward during merge

* Debugging Merge

* Removing debug messages

* Cleaned up repo

* Removed non-IA3 changes

* Refactor save and load

* Added support to all models in tests; Added IA3Config for common tests

* Added half-precision support and test for gradient checkpointing; Formatted jupyter notebooks

* Added target modules for new models GPTBigCode and LLama

* Cleaned up code

* Cleaned up code

* Cleaned up example notebook

* Cleaned up  seq2seq notebook

* Corrected function docstrings; refactored find_and_replace

* Corrected function docstrings; refactored find_and_replace

* Added basic docs for IA3

* Added new conceptual guide in source tree for documentation

* Minor fix to documentation

* Minor fixes to docstrings; Added error handling for 4bit quantization; Cleaned unused merge/unmerge methods

* styling changes after merge from main

* Update src/peft/tuners/ia3.py

Remove unused attribute merge_weights

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>

---------

Co-authored-by: Abhishek2304 <abhishekgupta2304@gmail.com>
Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-06-01T09:16:38Z,return load_result when load_adapter (#481)
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-04-25T06:54:18Z,"Implement adaption prompt from Llama-Adapter paper (#268)

* Implement adaption prompt from Llama-Adapter paper

* Support multi-adapters

* Refactor adaption prompt to target attn modules instead of layers

* Refactor adaption prompt to be more generic

* Fix adaption prompt not on right device

* Apply suggestions from code review

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>

* Fix style

* Add support for Llama config use_cache=True

* Fix rebase issues

---------

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>"
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-04-06T22:38:10Z,fixing adalora saving and loading
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-04-06T14:32:31Z,fix
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-04-06T13:35:31Z,Merge branch 'main' into smangrul/multi-lora-support
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-04-05T20:52:12Z,Run make style and make quality
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-04-04T14:35:59Z,😅. Fix 🐛
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-04-04T14:14:58Z,fixing 🐛
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-03-30T06:19:45Z,Finish the test for model load and save
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-03-30T05:59:26Z,Implement the save_pretrained for AdaLoRA
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-03-28T14:49:06Z,fix bugs
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-03-28T13:26:24Z,"multi adapter for training and inference

Might have breaking changes"
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-03-02T01:04:48Z,finish the testing and debugging
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-02-08T06:59:03Z,remove `peft_model_load_and_dispatch` as it is part of `PeftModel.from_pretrained`
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-02-08T03:55:21Z,Update save_and_load.py
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-02-08T03:51:49Z,update `peft_model_load_and_dispatch`
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-01-30T08:01:01Z,fixes
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-01-18T21:53:57Z,fixing doc strings to follow hf doc-builder format
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-01-18T14:13:29Z,addressing comments and bug fixes
github.com/huggingface/peft,src/peft/utils/save_and_load.py,2023-01-15T13:39:56Z,addressing comments and renaming `pet` to `peft`
github.com/AI4Finance-Foundation/FinGPT,fingpt/FinGPT_Benchmark/benchmarks/benchmarks.py,2023-10-28T03:47:59Z,Update benchmarks.py
github.com/AI4Finance-Foundation/FinGPT,fingpt/FinGPT_Benchmark/benchmarks/benchmarks.py,2023-10-28T03:40:39Z,Update benchmarks.py
github.com/AI4Finance-Foundation/FinGPT,fingpt/FinGPT_Benchmark/benchmarks/benchmarks.py,2023-10-28T03:35:35Z,Update benchmarks.py
github.com/AI4Finance-Foundation/FinGPT,fingpt/FinGPT_Benchmark/benchmarks/benchmarks.py,2023-10-16T04:39:45Z,FinGPT 2023-10-16 code refactoring
github.com/NVIDIA/NeMo,nemo/collections/multimodal/parts/utils.py,2024-02-27T17:43:26Z,"Add Gemma Support and Conversion Scripts (#8468)

* Add taurus pytorch to nemo

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>

* Add a taurus jax to nemo conversion script and few other fixes

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>

* Clean up code

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>

* bug fix

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>

* renaming

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>

* Fix arguments

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>

* Add HF Gemma converter

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>

* Turn off `apply_rope_fusion` during inference

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>

* update conversion scripts

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>

* Add exporting stuff

* update conversion scripts

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>

* Add readme

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>

* Save readme

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>

* Update jax

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>

* Remove Gemma README_Gemma.rst

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>

* Update import path

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>

* Update docstring

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>

* Revert ""Add exporting stuff""

This reverts commit 17d00b0f64f074ace8cb2607377e1c1744a314fb.

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Remove neva cyclic imports

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>

* Remove not used vars

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Revert ""Remove neva cyclic imports""

This reverts commit 898d9ed5ee4052281aa443e612f8e3dc7c2d63ca.

* Fix cyclic import

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>

* remove neva folder

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* remove not used vars

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>

* update docstrings in converter

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>

* Address comments

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>

* Add docstring

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>

---------

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>
Co-authored-by: Bobby Chen <bobchen@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/multimodal/parts/utils.py,2024-02-16T00:34:36Z,"Fix dreambooth data sampler issue (#8400) (#8413)

* Turn on drop last



* Some neva fixes



* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: yaoyu-33 <yaoyu.094@gmail.com>
Co-authored-by: yaoyu-33 <54727607+yaoyu-33@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/NVIDIA/NeMo,nemo/collections/multimodal/parts/utils.py,2024-01-20T01:59:23Z,"Final multimodal PR with our recent developments on MM side (#8127)

* Hotfix (#7501) (#7568)

Signed-off-by: Jan Baczek <jbaczek@nvidia.com>
Co-authored-by: jbaczek <45043825+jbaczek@users.noreply.github.com>

* Avoid duplicated checkpoint save (#7555) (#7566)

Signed-off-by: Mikołaj Błaż <mblaz@nvidia.com>
Co-authored-by: mikolajblaz <mikolajblaz@users.noreply.github.com>

* Cache FP8 weight and transpose only at the first micro-batch in each validation and test routine (#7470) (#7483)

* Cache weight and transpose only in the first batch in all training, val, and test runs



* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Add an option to disable manual GC in validation (#7467) (#7476)

Signed-off-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: Sangkug Lym <slym@nvidia.com>

* Remove PUBLICATIONS.md, point to github.io NeMo page instead (#7694) (#7695)

* update publications section to point to blog website page



* add hyphen



* use double backquotes for code formatting



---------

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>
Signed-off-by: Elena Rastorgueva <80532067+erastorgueva-nv@users.noreply.github.com>
Co-authored-by: Elena Rastorgueva <80532067+erastorgueva-nv@users.noreply.github.com>

* Fix multi rank finetune for ASR (#7684) (#7699)

* Fix multi rank finetune for ASR



* Actually add time



* Actually add time



---------

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>

* Update docs: readme, getting started, ASR intro (#7679)

* [TTS] Add dataset to path of logged artifacts (#7462)

* [TTS] Add dataset to path of logged artifacts

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Revert axis name back to Audio Frames

Signed-off-by: Ryan <rlangman@nvidia.com>

---------

Signed-off-by: Ryan <rlangman@nvidia.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* move install info to INSTALLATION.md

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* tidy up links

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix sft dataset truncation (#7464)

* Add fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

---------

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Automatic Lip Reading Recognition (ALR) - ASR/CV (Visual ASR) (#7330)

* striding_conv1d_k5 and dw_striding_conv1d_k5 subsampling

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* transpose conv1d inputs

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* Update subsampling.py

change striding_conv1d_k5 to striding_conv1d

Signed-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>

* cv branch

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* video manifest

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* add collection classes

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add test_step_outputs

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* correct manifest bug when having only audio or only videos

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* correct manifest bug when having only audio or only videos

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* clean references

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* freeze unfreeze transcribe cv models

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* correct manifest get_full_path bug

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* update for PR

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* guard torchvision

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Update nemo/collections/cv/data/video_to_text_dataset.py

Co-authored-by: Igor Gitman <igor.a.gitman@gmail.com>
Signed-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>

* _video_speech_collate_fn in cv/data/video_to_text.py

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* add self.out = None to asr subsampling

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* Update nemo/collections/cv/data/video_to_text_dataset.py

Co-authored-by: Igor Gitman <igor.a.gitman@gmail.com>
Signed-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>

* cv -> multimodal/speech_cv branch

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: mburchi <maxime.burchi@gmail.com>
Signed-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Igor Gitman <igor.a.gitman@gmail.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* HF StarCoder to NeMo conversion script (#7421)

* Script to convert HF StarCoder checkpoint to NeMo

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* StarCoder conversion test

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Fix test

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Catch up with save_to changes

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Don't abbreviate args for clarity

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Configurable precision: BF16 vs FP32

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* fix bug when loading dist ckpt in peft (#7452)

Signed-off-by: Hongbin Liu <hongbinl@nvidia.com>
Co-authored-by: Hongbin Liu <hongbinl@nvidia.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix adding positional embeddings in-place in transformer module (#7440)

Signed-off-by: Tamerlan Tabolov <tktabolov@gmail.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix (#7478)

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* add sleep (#7498) (#7499)

* add sleep



* add sleep onto config instead



* add comment



---------

Signed-off-by: Gerald Shen <geshen@nvidia.com>
Co-authored-by: Gerald Shen <119401249+gshennvm@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix exp manager check for sleep (#7503) (#7504)

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* bugfix: trainer.accelerator=auto from None. (#7492) (#7493)

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* [doc] fix broken link (#7481)

Signed-off-by: Stas Bekman <stas00@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* [TTS] Read audio as int32 to avoid flac read errors (#7477)

* [TTS] Read audio as int32 to avoid flac read errors

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Add comment about read failures

Signed-off-by: Ryan <rlangman@nvidia.com>

---------

Signed-off-by: Ryan <rlangman@nvidia.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Add dataset 'AISHELL-3' from OpenSLR for training mandarin TTS (#7409)

* Add dataset 'AISHELL-3' from OpenSLR for training mandarin TTS
* Train 'AISHELL-3' dataset with multi-speakers

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Update get_data.py

update copyright header

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* Update get_data.py

added a disclaimer

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add new configuration file for AISHELL3 with multispeaker of fastpitch

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* dllogger - log on rank 0 only (#7513)

Signed-off-by: Stas Bekman <stas00@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix TTS FastPitch tutorial (#7494) (#7516)

* Fix

---------

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix get_dist() tensor dimension (#7506) (#7515)

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>
Co-authored-by: Jocelyn <jocelynh@nvidia.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* bugfix: specify trainer.strategy=auto when devices=1 (#7509) (#7512)

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* fix (#7511)

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* [TTS] Fix FastPitch data prep tutorial (#7524)

Signed-off-by: Ryan <rlangman@nvidia.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* add italian tokenization (#7486)

* add italian tokenization

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add more ipa lexicon it

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix error deletion

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* add test

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Replace None strategy with auto in tutorial notebooks (#7521) (#7527)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* unpin setuptools (#7534) (#7535)

Signed-off-by: fayejf <36722593+fayejf@users.noreply.github.com>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* remove auto generated examples (#7510)

* explicitly remove autogenerated examples for data parallel evaluation

Signed-off-by: arendu <adithyare@nvidia.com>

* mark autogenrated and remove it for test

Signed-off-by: arendu <adithyare@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: arendu <adithyare@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Add the `strategy` argument to `MegatronGPTModel.generate()` (#7264)

It is passed as an explicit argument rather than through
`**strategy_args` so as to ensure someone cannot accidentally pass other
arguments that would end up being ignored.

It is a keyword-only argument to ensure that if in the future we want to
update the signature to `**strategy_args`, we can do it without breaking
code.

Signed-off-by: Olivier Delalleau <507137+odelalleau@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix PTL2.0 related ASR bugs in r1.21.0: Val metrics logging, None dataloader issue (#7531) (#7533)

* fix none dataloader issue ptl2



* ptl2.0 logging fixes for rnnt_models



---------

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>
Co-authored-by: Kunal Dhawan <kunaldhawan97@gmail.com>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* gpus -> devices (#7542) (#7545)

Signed-off-by: Nithin Rao Koluguri <nithinraok>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Update FFMPEG version to fix issue with torchaudio (#7551) (#7553)

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* PEFT GPT & T5 Refactor (#7308)

* initial implementation of add_adapters API

* correct type hint

* Add config in add_adapters for save and load (@author bobchen)

* Remove AdapterConfig to avoid import error

* Add AdaterConfig back and move adaptermixin to sft model

* Add NLPSaveRestoreConnector as default in NLPModel.restore_from

* Add restore_from_nemo_with_adapter and test script

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* rename t5 file and classes to be consistent with GPT

* add t5 sft dataset

* add support for single-file format with T5SFTDataset

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Various small changes to make T5 SFT work like GPT SFT

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add adapter evaluation test script

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add MultiAdaterConfig for ia3 and fix builder issue

* Make ptuning for T5SFTModel work using mixin

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add IA3_Adapter for AdapterName

* Add adapter name for ptuning and attention adapter

* Make test script GPT/T5 agnostic

* Add layer selection feature

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Integrate adapter name and config

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update gpt peft tuning script to new API

* add t5 peft tuning script with new API

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix IA3 layer selection issue

* Override state_dict on SFT model instead of mixin

* Add load adapter by adapter config

* move peft config map away from example script

* auto get config from nemo adapter

* Move PEFTConfig to new file

* fix ckpt save/load for t5

* name change: add_adapters -> add_adapter

* variable name change

* update t5 script

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix t5 issues

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add weight tying

* update gpt tuning script

* PEFT-API proposal

* Fix according to comments

* update tuning scripts

* move merge_cfg_with to mixin class since it applies to both gpt and t5 and requires the model class for restore

* Add mcore_gpt support for NLPAdapterMixin

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix typo

* variable name change to distinguish ""peft"" and ""adapter""

* override `load_adapters` to support `add_adapter` name change

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update tuning and eval script for adapter save/load

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add Ptuning on first stage only

* add lora tutorial for review

* Fix layer selection for mcore

* add landing page

* fix resume training

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add mcore condition in sharded_state_dict to make sft work

* Update lora_tutorial.md

First edit of this file for PEFT documentation for NeMO

Signed-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>

* rename Adapter to AttentionAdapter to avoid confusion in doc

* Change load_adapters to load .nemo

* add quick start guide

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add load_adapters with .ckpt

* Remove setup_complete changes in load_adapters

* update landing page

* remove typo

* Updated quick_start.md per Chen Cui

Signed-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>

* Add inference config merger and tutorial

* Add doc string for NLPAdapterModelMixin and deprecated warning on MegatronGPTPEFTModel

* add supported_methods.md and update other documentations

* Update supported_methods.md

minor updates.

Signed-off-by: Adi Renduchintala <adithyare@nvidia.com>

* Update landing_page.md

minor update.

Signed-off-by: Adi Renduchintala <adithyare@nvidia.com>

* Modify doc string for NLPAdapterModelMixin

* Add doc string add_adapters in NLPAdapterModelMixin

* rename canonical adapters

* remove mcore hard dependency

* [PATCH] move microbatch calculator to nemo from apex

* remove apex dependency in gpt and t5 sft models

* remove apex dependency in gpt model

* render doc strings

* fix

* Add missing virtual_tokens on ptuning

* fix docstrings

* update gpt-style model coverage in docs

* update docstring

* Remove pdb

* add lightning_fabric to make docstring rendering work

* Add Ptuning missing key

* try docstring rendering

* Fix ptuning issue

* update gpt t5 peft tuning and eval scripts

* typos

* update eval config

* fix bug relating to apex dependency removal

* typo

* make predict step behave the same as test step

* make lora tutorial work in notebook

* cosmetics

* update yaml scripts

* mcore_gpt attribute optional

* typo

* update eval scripts and fix T5 eval bugs

* add NLPDDPStrategyNotebook and trainer builder logic to use it

* update lora notebook to use new trainer builder

* fix microbatch calculator bug for inference after training

* Convert markdown files to RST and incorporate with doc

* typo

* revise language

* remove extra cell

* remove unnecessary inheritance

* remove old tests

* move layer selection default so logging messages make sense

* remove `save_adapters` as adapter weights are saved automatically during training

* initialize weights from a checkpoint instead of randomly

* multiple fields can form a context (#7147)

* list of context fields and flexible prompt template

Signed-off-by: arendu <adithya.r@gmail.com>

* list of fields for context

Signed-off-by: arendu <adithya.r@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Add multiple truncation fields and middle truncation

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Compatible to old ckpt

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix tokenize detokenize issue

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Remove detokenization, add truncation augmentation

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Resolve comments

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Remove unused import

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* revert eos

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Add tokenizer space_sensitive attribute

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix error

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Fix erorr and use re

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Change assert logic

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Follow adi suggestion

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Remove merge function

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add example and comment

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Remove context_key and add comment

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Remove random truncation

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix template none

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

---------

Signed-off-by: arendu <adithya.r@gmail.com>
Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Signed-off-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>

* revert config changes

* remove accidental breakpoint

* support TP>1 loading

* infer adapter type from checkpoint in during eval

* breakup add adapter

* enable interpolation of train_ds and validation_ds

* update metric calc script to conform to single-file eval format

* remove extraneous print

* update lora notebook for updated merge_inference_cfg

* Update nlp_adapter_mixins.py

variable name change

Signed-off-by: Chen Cui <chcui@nvidia.com>

* turn off grad scaler for PP to match old scripts

* remove PEFTSaveRestoreConnector since functionality all covered by the new mixin class

* remove resume_from_checkpoint check since covered in #7335

* revert changes made in eval config interpolation

* more interpolation

* typo

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* remove dup line

Signed-off-by: Chen Cui <chcui@nvidia.com>

* code style warnings

Signed-off-by: Chen Cui <chcui@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix config mistake

Signed-off-by: Chen Cui <chcui@nvidia.com>

* add copyright header

Signed-off-by: Chen Cui <chcui@nvidia.com>

* fix code check warnings

Signed-off-by: Chen Cui <chcui@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* revert changes to remove apex dependency (mixed apex+nemo microbatch calculator broke some CI tests)

Signed-off-by: Chen Cui <chcui@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add more deprecation notices

Signed-off-by: Chen Cui <chcui@nvidia.com>

* update deprecation notices

Signed-off-by: Chen Cui <chcui@nvidia.com>

* update deprecation notices

Signed-off-by: Chen Cui <chcui@nvidia.com>

* consolidate peft and sft scripts

Signed-off-by: Chen Cui <chcui@nvidia.com>

* update CI tests

Signed-off-by: Chen Cui <chcui@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* notebook branch points to main to prepare for merge

Signed-off-by: Chen Cui <chcui@nvidia.com>

* fix gpt and t5 validation with any metric other than loss

Signed-off-by: Chen Cui <chcui@nvidia.com>

* support pre-extracted checkpoints

Signed-off-by: Chen Cui <chcui@nvidia.com>

---------

Signed-off-by: jasonwan <jasonwan@nvidia.com>
Signed-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>
Signed-off-by: Adi Renduchintala <adithyare@nvidia.com>
Signed-off-by: arendu <adithya.r@gmail.com>
Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Signed-off-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>
Signed-off-by: Chen Cui <chcui@nvidia.com>
Co-authored-by: Chen Cui <chcui@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Marc Romeyn <marcromeyn@gmail.com>
Co-authored-by: jasonwan <jasonwan@nvidia.com>
Co-authored-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>
Co-authored-by: Adi Renduchintala <adithyare@nvidia.com>
Co-authored-by: Yuanzhe Dong <yudong@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* fix a typo (#7496)

Signed-off-by: BestJuly <chntaoli@163.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* [TTS] remove curly braces from ${BRANCH} in jupyer notebook cell. (#7554) (#7560)

* remove curly braces.
* remove installation of pynini.
---------

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* add youtube embed url (#7570)

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Remap speakers to continuous range of speaker_id for dataset AISHELL3 (#7536)

* Remap speakers to continuous range of speaker_id for dataset AISHELL3
* Add new key/value pair to record raw speaker for AISHELL3 dataset

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* fix validation_step_outputs initialization for multi-dataloader (#7546) (#7572)

* added correct validation_step_outputs initialization for mutli-dataloader



* changed kernel for display



* Update logic for validation and test step outputs



* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* revert multidataloader changes in multilang ASR notebook



---------

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>
Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: Kunal Dhawan <kunaldhawan97@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Append output of val step to self.validation_step_outputs (#7530) (#7532)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* [TTS] fixed trainer's accelerator and strategy. (#7569) (#7574)

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Append val/test output to instance variable in EncDecSpeakerLabelModel (#7562) (#7573)

* Append val/test output to the instance variable in EncDecSpeakerLabelModel



* Handle test case in evaluation_step



* Replace type with isinstance



---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix CustomProgressBar for resume (#7427) (#7522)

* Fix CustomProgress Bar for resume and multiple epochs



* Edit num_training_batches



* Use max_steps as total for progress bar for resume



* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* fix typos in nfa and speech enhancement tutorials (#7580) (#7583)

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>
Co-authored-by: Elena Rastorgueva <80532067+erastorgueva-nv@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Add strategy as ddp_find_unused_parameters_true for glue_benchmark.py (#7454) (#7461)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* update strategy (#7577) (#7578)

Signed-off-by: Nithin Rao Koluguri <nithinraok>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix typos (#7581)

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Change hifigan finetune strategy to ddp_find_unused_parameters_true (#7579) (#7584)

* Change strategy to auto



---------

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* [BugFix] Add missing quotes for auto strategy in tutorial notebooks (#7541) (#7548)

* Add missing quotes for auto strategy



* Revert trainer.gpus to trainer.devices in Self_Supervised_Pre_Training.ipynb



---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Signed-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* add build os key (#7596) (#7599)

* add build os key



* add tools



* update to stable version



---------

Signed-off-by: Nithin Rao Koluguri <nithinraok>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* StarCoder SFT test + bump PyT NGC image to 23.09 (#7540)

* Add SFT StarCoder test

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Remove _modify_config call as it is covered in load_from_nemo just below

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Test with pyt:23.09 container

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

---------

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* defaults changed (#7600)

* defaults changed

Signed-off-by: arendu <adithyare@nvidia.com>

* typo

Signed-off-by: arendu <adithyare@nvidia.com>

* update

Signed-off-by: arendu <adithyare@nvidia.com>

---------

Signed-off-by: arendu <adithyare@nvidia.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* add ItalianPhonemesTokenizer (#7587)

* add ItalianPhonemesTokenizer

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix Italian phonemes

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add test

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

---------

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* best ckpt fix (#7564) (#7588)

Signed-off-by: dimapihtar <dpihtar@gmail.com>
Co-authored-by: Dmytro Pykhtar <37850217+dimapihtar@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Add files via upload (#7598)

specifies the branch

Signed-off-by: George <37293288+Jorjeous@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix validation in G2PModel and ThutmoseTaggerModel (#7597) (#7606)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain (#7576) (#7586)

* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain
* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Safeguard nemo_text_processing installation on ARM (#7485)

* safeguard nemo_text_processing installing

Signed-off-by: Jason <jasoli@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update check

Signed-off-by: Jason <jasoli@nvidia.com>

---------

Signed-off-by: Jason <jasoli@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Bound transformers version in requirements (#7620)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* fix llama2 70b lora tuning bug (#7622)

* fix llama2 70b lora tuning bug

Signed-off-by: Chen Cui <chcui@nvidia.com>

* Update peft_config.py

brackets

Signed-off-by: Adi Renduchintala <adithyare@nvidia.com>

---------

Signed-off-by: Chen Cui <chcui@nvidia.com>
Signed-off-by: Adi Renduchintala <adithyare@nvidia.com>
Co-authored-by: Adi Renduchintala <adithyare@nvidia.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix import error no module name model_utils (#7629)

Signed-off-by: Mehadi Hasan Menon <mehadihasan80@gmail.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* add fc large ls models (#7641)

Signed-off-by: Nithin Rao Koluguri <nithinraok>
Co-authored-by: Nithin Rao Koluguri <nithinraok>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* bugfix: trainer.gpus, trainer.strategy, trainer.accelerator (#7621) (#7642)

* [TTS] bugfix for Tacotron2 tutorial due to PTL 2.0
* trainer.gpus -> trainer.devices
* fixed related tutorial bugs
---------
Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* fix ssl models ptl monitor val through logging (#7608) (#7614)

Signed-off-by: Nithin Rao Koluguri <nithinraok>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix metrics for SE tutorial (#7604) (#7612)

Signed-off-by: Ante Jukić <ajukic@nvidia.com>
Co-authored-by: anteju <108555623+anteju@users.noreply.github.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Add ddp_find_unused_parameters=True and change accelerator to auto (#7623) (#7644)

* Add ddp_find_unused_parameters=True and change acclerator to auto



* Add ddp_find_unused_parameters True for normalization_as_tagging_train.py



---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix py3.11 dataclasses issue  (#7616)

* Fix py3.11 dataclasses issue  (#7582)

* Update ASR configs to support Python 3.11

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update TTS configs to support Python 3.11

Signed-off-by: smajumdar <titu1994@gmail.com>

* Guard MeCab and Ipadic

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix remaining ASR dataclasses

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix remaining ASR dataclasses

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix scripts

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Update name to ConfidenceMethodConfig

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain (#7576) (#7586)

* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain
* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Safeguard nemo_text_processing installation on ARM (#7485)

* safeguard nemo_text_processing installing

Signed-off-by: Jason <jasoli@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update check

Signed-off-by: Jason <jasoli@nvidia.com>

---------

Signed-off-by: Jason <jasoli@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Fix changes to confidence measure

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: smajumdar <titu1994@gmail.com>
Signed-off-by: Sangkug Lym <slym@nvidia.com>
Signed-off-by: Jason <jasoli@nvidia.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>
Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: Jason <jasoli@nvidia.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Fix issues with Dockerfile (#7650) (#7652)

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* [ASR] RNN-T greedy decoding max_frames fix for alignment and confidence (#7635)

* decoding and test fix

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* [ASR] Fix type error in jasper (#7636) (#7653)

Signed-off-by: Ryan <rlangman@nvidia.com>
Co-authored-by: Ryan Langman <rlangman@nvidia.com>
Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* [TTS] Add STFT and SI-SDR loss to audio codec recipe (#7468)

* [TTS] Add STFT and SI-SDR loss to audio codec recipe

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Fix STFT resolution

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Fix training metric logging

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Add docstring to mel and stft losses

Signed-off-by: Ryan <rlangman@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Ryan <rlangman@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* add outline of asr quickstart info to asr/intro.rst

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* add CLI, LM and real-time transcription sections

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>

* Create per.py (#7538)

* Move model precision copy (#7336)

* move cfg precision set to megatron base model

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* remove copy from other models

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* modify attribute not arg

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* fix gpt model test for ptl 2.0

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* rename function and add docstring

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* replace precision to dtype conditionals with func call

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* unnecessary function and cfg reset

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* set default value

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* fix precision lookup in a few more places

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* rename mapping function

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* ununsed import

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* save torch datatype to model

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* set weights precision wrt amp o2

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* Revert ""set weights precision wrt amp o2""

This reverts commit 313a4bfe5eb69d771a6d2433898c0685836aef5c.

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* revert half precision at inference attempt

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* move autocast dtype to base model

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* move params dtype to base model, enable fp16 O2 inf

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* unused imports

Signed-off-by: Maanu Grover <maanug@nvidia.com>

---------

Signed-off-by: Maanu Grover <maanug@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix PEFT checkpoint loading (#7388)

* Fix PEFT checkpoint loading

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Jason Wang <jasonwan@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Use distributed optimizer support for multiple dtypes (#7359)

* Update distopt wrapper with multiple dtype support

Remove manual handling of separate FP32 optimizer.

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Use distopt support for contiguous buffers with multiple dtypes

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Fix typo

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Separate distopt buckets for first GPT layer and non-overlapped params

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Add distopt logic for int dtypes

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Update Apex commit

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Remove unused variables

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Update Apex commit in README and Jenkensfile

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Debug Dockerfile and Jenkinsfile

Signed-off-by: Tim Moon <tmoon@nvidia.com>

---------

Signed-off-by: Tim Moon <tmoon@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* minor fix for llama ckpt conversion script (#7387)

* minor fix for llama ckpt conversion script

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* Update Jenkinsfile

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* remove fast_swiglu configuration

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

---------

Signed-off-by: Jason Wang <jasonwan@nvidia.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix wrong calling of librosa.get_duration() in notebook (#7376)

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [PATCH] PEFT import mcore (#7393)

* [PATCH] PEFT import mcore

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Jason Wang <jasonwan@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Create per.py

Script for calculation Punctuation Error Rate and related rates (correct rate, deletions rate, etc.)

Signed-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [TTS] Added a callback for logging initial data (#7384)

Signed-off-by: Ante Jukić <ajukic@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Update Core Commit (#7402)

* Update Core Commit

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* update commit

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

---------

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Use cfg attribute in bert (#7394)

* use cfg attribute instead of arg

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* use torch_dtype in place of cfg.precision

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* move precision copy before super constructor

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* use trainer arg

Signed-off-by: Maanu Grover <maanug@nvidia.com>

---------

Signed-off-by: Maanu Grover <maanug@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add support for bias conversion in Swiglu models (#7386)

* Add support for bias conversion in Swiglu models

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add support for auto extracting tokenizer model

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add support for auto extracting tokenizer model

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix issue with missing tokenizer

Signed-off-by: smajumdar <titu1994@gmail.com>

* Refactor

Signed-off-by: smajumdar <titu1994@gmail.com>

* Refactor

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Update save_to and restore_from for dist checkpointing (#7343)

* add dist ckpt to save to, in progress

Signed-off-by: eharper <eharper@nvidia.com>

* move dist ckpt

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* clean up

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update restore from, need to figure out how to initialize distributed

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* launch distrib if needed when restoring dist ckpt

Signed-off-by: eharper <eharper@nvidia.com>

* when using mcore we can change tp pp on the fly

Signed-off-by: eharper <eharper@nvidia.com>

* add load_from_checkpoint support for dist ckpt

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update llama convert script to save dist .nemo

Signed-off-by: eharper <eharper@nvidia.com>

* fix load dist ckpt

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* setup TE TP groups if needed

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* setup te tp groups if needed

Signed-off-by: eharper <eharper@nvidia.com>

* remove import

Signed-off-by: eharper <eharper@nvidia.com>

---------

Signed-off-by: eharper <eharper@nvidia.com>
Signed-off-by: jasonwan <jasonwan@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: jasonwan <jasonwan@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* fix forward for with mcore=false (#7403)

Signed-off-by: Jimmy Zhang <jiemingz@nvidia.com>
Co-authored-by: Jimmy Zhang <jiemingz@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix logging to remove 's/it' from progress bar in Megatron models and add train_step_timing (#7374)

* Add CustomProgressBar class to exp_manager and trainer callbacks

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix the progress bar to reflect total microbatch cnt

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Modify CustomProgressBar class

1) Modify CustomProgressBar class to update progress bar per global_step instead of per microbatch
2) Add the callback to other megatron training/finetuning files that are not using MegatronTrainerBuilder

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add CustomProgressBar callback to tuning files

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Set Activation Checkpointing Defaults (#7404)

* Set Activation Checkpointing Defaults

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* check for None

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* make loss mask default to false (#7407)

Signed-off-by: eharper <eharper@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add dummy userbuffer config files (#7408)

Signed-off-by: Sangkug Lym <slym@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* add missing ubconf files (#7412)

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* New tutorial on Speech Data Explorer (#7405)

* Added Google Colab based tutorial on Speech Data Explorer

Signed-off-by: George Zelenfroynd <gzelenfroind@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Update ptl training ckpt conversion script to work with dist ckpt (#7416)

* update ptl convert script

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* don't break legacy

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: eharper <eharper@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Allow disabling sanity checking when num_sanity_val_steps=0 (#7413)

* Allow disabling sanity checking when num_sanity_val_steps=0

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Update num_sanity_val_steps to be a multiple of num_microbatches

Signed-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Signed-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add comprehensive error messages (#7261)

Signed-off-by: Anton Peganov <apeganov@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* check NEMO_PATH (#7418)

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* layer selection for ia3 (#7417)

* layer selection for ia3

Signed-off-by: arendu <adithyare@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: arendu <adithyare@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix missing pip package 'einops' (#7397)

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix failure of pyaudio in Google Colab (#7396)

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Update README.md: output_path --> output_manifest_filepath (#7442)

Signed-off-by: Samuele Cornell <cornellsamuele@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add rope dynamic linear scaling (#7437)

* Add dynamic linear scaling

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

---------

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix None dataloader issue in PTL2.0 (#7455)

* Fix None dataloader issue in PTL2.0

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* updating values of self._validation_dl and self._test_dl as well

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* updating values of self._validation_dl and self._test_dl as well

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [ASR] Confidence measure -> method renames (#7434)

* measure -> method

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add steps for document of getting dataset 'SF Bilingual Speech' (#7378)

* Add steps for document of getting dataset 'SF Bilingual Speech'

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Update datasets.rst

added a link from a tutorial demonstrating detailed data prep steps.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* RNN-T confidence and alignment bugfix (#7381)

* new frame_confidence and alignments lists are now always created after the while loop

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

* tests added

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

---------

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix resume from checkpoint in exp_manager (#7424) (#7426)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix checking of cuda/cpu device for inputs of Decoder (#7444)

* Fix checking of cuda/cpu device for inputs of Decoder

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Update tacotron2.py

Signed-off-by: Jason <jasoli@nvidia.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Jason <jasoli@nvidia.com>
Co-authored-by: Jason <jasoli@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix failure of ljspeech's get_data.py (#7430)

* Fix failure of ljspeech's get_data.py

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [TTS] Fix audio codec type checks (#7373)

* [TTS] Fix audio codec type checks

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Fix audio codec tests

Signed-off-by: Ryan <rlangman@nvidia.com>

---------

Signed-off-by: Ryan <rlangman@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* […"
github.com/NVIDIA/NeMo,nemo/collections/multimodal/parts/utils.py,2023-12-13T02:12:55Z,"Add All Multimodal Source Code (#7791)

* Add comprehensive error messages (#7261)

Signed-off-by: Anton Peganov <apeganov@nvidia.com>

* check NEMO_PATH (#7418)

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>

* layer selection for ia3 (#7417)

* layer selection for ia3

Signed-off-by: arendu <adithyare@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: arendu <adithyare@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Fix missing pip package 'einops' (#7397)

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Fix failure of pyaudio in Google Colab (#7396)

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Update README.md: output_path --> output_manifest_filepath (#7442)

Signed-off-by: Samuele Cornell <cornellsamuele@gmail.com>

* Updating FlashAttention API to match FlashAttentionV2

* Multiple fixes for mm

* Fix CI inductor issue and update to torch compile

* Remove suppress error

* Fix when conversion config uses fp16 and it complains about precision plugin

* Fixing FAv2 API usage

* Initial release of content filtering model

* Added synthetic dataloader for precached and online mode

* Mingyuanm/dreambooth opt

* Add llama2 support in neva training

* Fix sampler length

* Fix all precision issues in nemo multimodal

* Add rope dynamic linear scaling (#7437)

* Add dynamic linear scaling

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

---------

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>

* Fix None dataloader issue in PTL2.0 (#7455)

* Fix None dataloader issue in PTL2.0

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* updating values of self._validation_dl and self._test_dl as well

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* updating values of self._validation_dl and self._test_dl as well

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* [ASR] Confidence measure -> method renames (#7434)

* measure -> method

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Add steps for document of getting dataset 'SF Bilingual Speech' (#7378)

* Add steps for document of getting dataset 'SF Bilingual Speech'

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Update datasets.rst

added a link from a tutorial demonstrating detailed data prep steps.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* RNN-T confidence and alignment bugfix (#7381)

* new frame_confidence and alignments lists are now always created after the while loop

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

* tests added

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

---------

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

* Fix resume from checkpoint in exp_manager (#7424) (#7426)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>

* Fix checking of cuda/cpu device for inputs of Decoder (#7444)

* Fix checking of cuda/cpu device for inputs of Decoder

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Update tacotron2.py

Signed-off-by: Jason <jasoli@nvidia.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Jason <jasoli@nvidia.com>
Co-authored-by: Jason <jasoli@nvidia.com>

* Fix failure of ljspeech's get_data.py (#7430)

* Fix failure of ljspeech's get_data.py

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* [TTS] Fix audio codec type checks (#7373)

* [TTS] Fix audio codec type checks

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Fix audio codec tests

Signed-off-by: Ryan <rlangman@nvidia.com>

---------

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Add dataset to path of logged artifacts (#7462)

* [TTS] Add dataset to path of logged artifacts

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Revert axis name back to Audio Frames

Signed-off-by: Ryan <rlangman@nvidia.com>

---------

Signed-off-by: Ryan <rlangman@nvidia.com>

* Fix sft dataset truncation (#7464)

* Add fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

---------

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Automatic Lip Reading Recognition (ALR) - ASR/CV (Visual ASR) (#7330)

* striding_conv1d_k5 and dw_striding_conv1d_k5 subsampling

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* transpose conv1d inputs

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* Update subsampling.py

change striding_conv1d_k5 to striding_conv1d

Signed-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>

* cv branch

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* video manifest

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* add collection classes

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add test_step_outputs

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* correct manifest bug when having only audio or only videos

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* correct manifest bug when having only audio or only videos

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* clean references

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* freeze unfreeze transcribe cv models

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* correct manifest get_full_path bug

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* update for PR

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* guard torchvision

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Update nemo/collections/cv/data/video_to_text_dataset.py

Co-authored-by: Igor Gitman <igor.a.gitman@gmail.com>
Signed-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>

* _video_speech_collate_fn in cv/data/video_to_text.py

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* add self.out = None to asr subsampling

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* Update nemo/collections/cv/data/video_to_text_dataset.py

Co-authored-by: Igor Gitman <igor.a.gitman@gmail.com>
Signed-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>

* cv -> multimodal/speech_cv branch

Signed-off-by: mburchi <maxime.burchi@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: mburchi <maxime.burchi@gmail.com>
Signed-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Igor Gitman <igor.a.gitman@gmail.com>

* HF StarCoder to NeMo conversion script (#7421)

* Script to convert HF StarCoder checkpoint to NeMo

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* StarCoder conversion test

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Fix test

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Catch up with save_to changes

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Don't abbreviate args for clarity

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Configurable precision: BF16 vs FP32

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* fix bug when loading dist ckpt in peft (#7452)

Signed-off-by: Hongbin Liu <hongbinl@nvidia.com>
Co-authored-by: Hongbin Liu <hongbinl@nvidia.com>

* Fix adding positional embeddings in-place in transformer module (#7440)

Signed-off-by: Tamerlan Tabolov <tktabolov@gmail.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>

* Fix (#7478)

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* add sleep (#7498) (#7499)

* add sleep



* add sleep onto config instead



* add comment



---------

Signed-off-by: Gerald Shen <geshen@nvidia.com>
Co-authored-by: Gerald Shen <119401249+gshennvm@users.noreply.github.com>

* Fix exp manager check for sleep (#7503) (#7504)

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>

* bugfix: trainer.accelerator=auto from None. (#7492) (#7493)

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>

* [doc] fix broken link (#7481)

Signed-off-by: Stas Bekman <stas00@users.noreply.github.com>

* [TTS] Read audio as int32 to avoid flac read errors (#7477)

* [TTS] Read audio as int32 to avoid flac read errors

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Add comment about read failures

Signed-off-by: Ryan <rlangman@nvidia.com>

---------

Signed-off-by: Ryan <rlangman@nvidia.com>

* Add dataset 'AISHELL-3' from OpenSLR for training mandarin TTS (#7409)

* Add dataset 'AISHELL-3' from OpenSLR for training mandarin TTS
* Train 'AISHELL-3' dataset with multi-speakers

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Update get_data.py

update copyright header

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* Update get_data.py

added a disclaimer

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add new configuration file for AISHELL3 with multispeaker of fastpitch

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* dllogger - log on rank 0 only (#7513)

Signed-off-by: Stas Bekman <stas00@users.noreply.github.com>

* Fix TTS FastPitch tutorial (#7494) (#7516)

* Fix

---------

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>

* Fix get_dist() tensor dimension (#7506) (#7515)

Signed-off-by: Jocelyn Huang <jocelynh@nvidia.com>
Co-authored-by: Jocelyn <jocelynh@nvidia.com>

* bugfix: specify trainer.strategy=auto when devices=1 (#7509) (#7512)

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>

* fix (#7511)

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* [TTS] Fix FastPitch data prep tutorial (#7524)

Signed-off-by: Ryan <rlangman@nvidia.com>

* add italian tokenization (#7486)

* add italian tokenization

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add more ipa lexicon it

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix error deletion

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* add test

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Replace None strategy with auto in tutorial notebooks (#7521) (#7527)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

* unpin setuptools (#7534) (#7535)

Signed-off-by: fayejf <36722593+fayejf@users.noreply.github.com>
Co-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>

* remove auto generated examples (#7510)

* explicitly remove autogenerated examples for data parallel evaluation

Signed-off-by: arendu <adithyare@nvidia.com>

* mark autogenrated and remove it for test

Signed-off-by: arendu <adithyare@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: arendu <adithyare@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Add the `strategy` argument to `MegatronGPTModel.generate()` (#7264)

It is passed as an explicit argument rather than through
`**strategy_args` so as to ensure someone cannot accidentally pass other
arguments that would end up being ignored.

It is a keyword-only argument to ensure that if in the future we want to
update the signature to `**strategy_args`, we can do it without breaking
code.

Signed-off-by: Olivier Delalleau <507137+odelalleau@users.noreply.github.com>

* Fix PTL2.0 related ASR bugs in r1.21.0: Val metrics logging, None dataloader issue (#7531) (#7533)

* fix none dataloader issue ptl2



* ptl2.0 logging fixes for rnnt_models



---------

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>
Co-authored-by: Kunal Dhawan <kunaldhawan97@gmail.com>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>

* gpus -> devices (#7542) (#7545)

Signed-off-by: Nithin Rao Koluguri <nithinraok>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>

* Update FFMPEG version to fix issue with torchaudio (#7551) (#7553)

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>

* PEFT GPT & T5 Refactor (#7308)

* initial implementation of add_adapters API

* correct type hint

* Add config in add_adapters for save and load (@author bobchen)

* Remove AdapterConfig to avoid import error

* Add AdaterConfig back and move adaptermixin to sft model

* Add NLPSaveRestoreConnector as default in NLPModel.restore_from

* Add restore_from_nemo_with_adapter and test script

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* rename t5 file and classes to be consistent with GPT

* add t5 sft dataset

* add support for single-file format with T5SFTDataset

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Various small changes to make T5 SFT work like GPT SFT

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add adapter evaluation test script

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add MultiAdaterConfig for ia3 and fix builder issue

* Make ptuning for T5SFTModel work using mixin

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add IA3_Adapter for AdapterName

* Add adapter name for ptuning and attention adapter

* Make test script GPT/T5 agnostic

* Add layer selection feature

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Integrate adapter name and config

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update gpt peft tuning script to new API

* add t5 peft tuning script with new API

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix IA3 layer selection issue

* Override state_dict on SFT model instead of mixin

* Add load adapter by adapter config

* move peft config map away from example script

* auto get config from nemo adapter

* Move PEFTConfig to new file

* fix ckpt save/load for t5

* name change: add_adapters -> add_adapter

* variable name change

* update t5 script

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix t5 issues

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add weight tying

* update gpt tuning script

* PEFT-API proposal

* Fix according to comments

* update tuning scripts

* move merge_cfg_with to mixin class since it applies to both gpt and t5 and requires the model class for restore

* Add mcore_gpt support for NLPAdapterMixin

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix typo

* variable name change to distinguish ""peft"" and ""adapter""

* override `load_adapters` to support `add_adapter` name change

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update tuning and eval script for adapter save/load

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add Ptuning on first stage only

* add lora tutorial for review

* Fix layer selection for mcore

* add landing page

* fix resume training

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add mcore condition in sharded_state_dict to make sft work

* Update lora_tutorial.md

First edit of this file for PEFT documentation for NeMO

Signed-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>

* rename Adapter to AttentionAdapter to avoid confusion in doc

* Change load_adapters to load .nemo

* add quick start guide

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add load_adapters with .ckpt

* Remove setup_complete changes in load_adapters

* update landing page

* remove typo

* Updated quick_start.md per Chen Cui

Signed-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>

* Add inference config merger and tutorial

* Add doc string for NLPAdapterModelMixin and deprecated warning on MegatronGPTPEFTModel

* add supported_methods.md and update other documentations

* Update supported_methods.md

minor updates.

Signed-off-by: Adi Renduchintala <adithyare@nvidia.com>

* Update landing_page.md

minor update.

Signed-off-by: Adi Renduchintala <adithyare@nvidia.com>

* Modify doc string for NLPAdapterModelMixin

* Add doc string add_adapters in NLPAdapterModelMixin

* rename canonical adapters

* remove mcore hard dependency

* [PATCH] move microbatch calculator to nemo from apex

* remove apex dependency in gpt and t5 sft models

* remove apex dependency in gpt model

* render doc strings

* fix

* Add missing virtual_tokens on ptuning

* fix docstrings

* update gpt-style model coverage in docs

* update docstring

* Remove pdb

* add lightning_fabric to make docstring rendering work

* Add Ptuning missing key

* try docstring rendering

* Fix ptuning issue

* update gpt t5 peft tuning and eval scripts

* typos

* update eval config

* fix bug relating to apex dependency removal

* typo

* make predict step behave the same as test step

* make lora tutorial work in notebook

* cosmetics

* update yaml scripts

* mcore_gpt attribute optional

* typo

* update eval scripts and fix T5 eval bugs

* add NLPDDPStrategyNotebook and trainer builder logic to use it

* update lora notebook to use new trainer builder

* fix microbatch calculator bug for inference after training

* Convert markdown files to RST and incorporate with doc

* typo

* revise language

* remove extra cell

* remove unnecessary inheritance

* remove old tests

* move layer selection default so logging messages make sense

* remove `save_adapters` as adapter weights are saved automatically during training

* initialize weights from a checkpoint instead of randomly

* multiple fields can form a context (#7147)

* list of context fields and flexible prompt template

Signed-off-by: arendu <adithya.r@gmail.com>

* list of fields for context

Signed-off-by: arendu <adithya.r@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Add multiple truncation fields and middle truncation

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Compatible to old ckpt

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix tokenize detokenize issue

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Remove detokenization, add truncation augmentation

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Resolve comments

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Remove unused import

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* revert eos

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Add tokenizer space_sensitive attribute

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix error

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Fix erorr and use re

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Change assert logic

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Follow adi suggestion

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Remove merge function

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add example and comment

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Remove context_key and add comment

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* Remove random truncation

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix template none

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

---------

Signed-off-by: arendu <adithya.r@gmail.com>
Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Signed-off-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>

* revert config changes

* remove accidental breakpoint

* support TP>1 loading

* infer adapter type from checkpoint in during eval

* breakup add adapter

* enable interpolation of train_ds and validation_ds

* update metric calc script to conform to single-file eval format

* remove extraneous print

* update lora notebook for updated merge_inference_cfg

* Update nlp_adapter_mixins.py

variable name change

Signed-off-by: Chen Cui <chcui@nvidia.com>

* turn off grad scaler for PP to match old scripts

* remove PEFTSaveRestoreConnector since functionality all covered by the new mixin class

* remove resume_from_checkpoint check since covered in #7335

* revert changes made in eval config interpolation

* more interpolation

* typo

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* remove dup line

Signed-off-by: Chen Cui <chcui@nvidia.com>

* code style warnings

Signed-off-by: Chen Cui <chcui@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix config mistake

Signed-off-by: Chen Cui <chcui@nvidia.com>

* add copyright header

Signed-off-by: Chen Cui <chcui@nvidia.com>

* fix code check warnings

Signed-off-by: Chen Cui <chcui@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* revert changes to remove apex dependency (mixed apex+nemo microbatch calculator broke some CI tests)

Signed-off-by: Chen Cui <chcui@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add more deprecation notices

Signed-off-by: Chen Cui <chcui@nvidia.com>

* update deprecation notices

Signed-off-by: Chen Cui <chcui@nvidia.com>

* update deprecation notices

Signed-off-by: Chen Cui <chcui@nvidia.com>

* consolidate peft and sft scripts

Signed-off-by: Chen Cui <chcui@nvidia.com>

* update CI tests

Signed-off-by: Chen Cui <chcui@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* notebook branch points to main to prepare for merge

Signed-off-by: Chen Cui <chcui@nvidia.com>

* fix gpt and t5 validation with any metric other than loss

Signed-off-by: Chen Cui <chcui@nvidia.com>

* support pre-extracted checkpoints

Signed-off-by: Chen Cui <chcui@nvidia.com>

---------

Signed-off-by: jasonwan <jasonwan@nvidia.com>
Signed-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>
Signed-off-by: Adi Renduchintala <adithyare@nvidia.com>
Signed-off-by: arendu <adithya.r@gmail.com>
Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Signed-off-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>
Signed-off-by: Chen Cui <chcui@nvidia.com>
Co-authored-by: Chen Cui <chcui@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Marc Romeyn <marcromeyn@gmail.com>
Co-authored-by: jasonwan <jasonwan@nvidia.com>
Co-authored-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>
Co-authored-by: Adi Renduchintala <adithyare@nvidia.com>
Co-authored-by: Yuanzhe Dong <yudong@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>

* fix a typo (#7496)

Signed-off-by: BestJuly <chntaoli@163.com>

* [TTS] remove curly braces from ${BRANCH} in jupyer notebook cell. (#7554) (#7560)

* remove curly braces.
* remove installation of pynini.
---------

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>

* add youtube embed url (#7570)

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>

* Remap speakers to continuous range of speaker_id for dataset AISHELL3 (#7536)

* Remap speakers to continuous range of speaker_id for dataset AISHELL3
* Add new key/value pair to record raw speaker for AISHELL3 dataset

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* fix validation_step_outputs initialization for multi-dataloader (#7546) (#7572)

* added correct validation_step_outputs initialization for mutli-dataloader



* changed kernel for display



* Update logic for validation and test step outputs



* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* revert multidataloader changes in multilang ASR notebook



---------

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>
Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: Kunal Dhawan <kunaldhawan97@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Append output of val step to self.validation_step_outputs (#7530) (#7532)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

* [TTS] fixed trainer's accelerator and strategy. (#7569) (#7574)

Signed-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>

* Append val/test output to instance variable in EncDecSpeakerLabelModel (#7562) (#7573)

* Append val/test output to the instance variable in EncDecSpeakerLabelModel



* Handle test case in evaluation_step



* Replace type with isinstance



---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

* Fix CustomProgressBar for resume (#7427) (#7522)

* Fix CustomProgress Bar for resume and multiple epochs



* Edit num_training_batches



* Use max_steps as total for progress bar for resume



* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* fix typos in nfa and speech enhancement tutorials (#7580) (#7583)

Signed-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>
Co-authored-by: Elena Rastorgueva <80532067+erastorgueva-nv@users.noreply.github.com>

* Add strategy as ddp_find_unused_parameters_true for glue_benchmark.py (#7454) (#7461)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

* update strategy (#7577) (#7578)

Signed-off-by: Nithin Rao Koluguri <nithinraok>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>

* Fix typos (#7581)

* Change hifigan finetune strategy to ddp_find_unused_parameters_true (#7579) (#7584)

* Change strategy to auto



---------

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>

* [BugFix] Add missing quotes for auto strategy in tutorial notebooks (#7541) (#7548)

* Add missing quotes for auto strategy



* Revert trainer.gpus to trainer.devices in Self_Supervised_Pre_Training.ipynb



---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Signed-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

* add build os key (#7596) (#7599)

* add build os key



* add tools



* update to stable version



---------

Signed-off-by: Nithin Rao Koluguri <nithinraok>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>

* StarCoder SFT test + bump PyT NGC image to 23.09 (#7540)

* Add SFT StarCoder test

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Remove _modify_config call as it is covered in load_from_nemo just below

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* Test with pyt:23.09 container

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

---------

Signed-off-by: Jan Lasek <janek.lasek@gmail.com>

* defaults changed (#7600)

* defaults changed

Signed-off-by: arendu <adithyare@nvidia.com>

* typo

Signed-off-by: arendu <adithyare@nvidia.com>

* update

Signed-off-by: arendu <adithyare@nvidia.com>

---------

Signed-off-by: arendu <adithyare@nvidia.com>

* add ItalianPhonemesTokenizer (#7587)

* add ItalianPhonemesTokenizer

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix Italian phonemes

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* add test

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>

---------

Signed-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* best ckpt fix (#7564) (#7588)

Signed-off-by: dimapihtar <dpihtar@gmail.com>
Co-authored-by: Dmytro Pykhtar <37850217+dimapihtar@users.noreply.github.com>

* Add files via upload (#7598)

specifies the branch

Signed-off-by: George <37293288+Jorjeous@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* Fix validation in G2PModel and ThutmoseTaggerModel (#7597) (#7606)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain (#7576) (#7586)

* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain
* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Safeguard nemo_text_processing installation on ARM (#7485)

* safeguard nemo_text_processing installing

Signed-off-by: Jason <jasoli@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update check

Signed-off-by: Jason <jasoli@nvidia.com>

---------

Signed-off-by: Jason <jasoli@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Bound transformers version in requirements (#7620)

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* fix llama2 70b lora tuning bug (#7622)

* fix llama2 70b lora tuning bug

Signed-off-by: Chen Cui <chcui@nvidia.com>

* Update peft_config.py

brackets

Signed-off-by: Adi Renduchintala <adithyare@nvidia.com>

---------

Signed-off-by: Chen Cui <chcui@nvidia.com>
Signed-off-by: Adi Renduchintala <adithyare@nvidia.com>
Co-authored-by: Adi Renduchintala <adithyare@nvidia.com>

* Fix import error no module name model_utils (#7629)

Signed-off-by: Mehadi Hasan Menon <mehadihasan80@gmail.com>

* add fc large ls models (#7641)

Signed-off-by: Nithin Rao Koluguri <nithinraok>
Co-authored-by: Nithin Rao Koluguri <nithinraok>

* bugfix: trainer.gpus, trainer.strategy, trainer.accelerator (#7621) (#7642)

* [TTS] bugfix for Tacotron2 tutorial due to PTL 2.0
* trainer.gpus -> trainer.devices
* fixed related tutorial bugs
---------
Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* fix ssl models ptl monitor val through logging (#7608) (#7614)

Signed-off-by: Nithin Rao Koluguri <nithinraok>
Co-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

* Fix metrics for SE tutorial (#7604) (#7612)

Signed-off-by: Ante Jukić <ajukic@nvidia.com>
Co-authored-by: anteju <108555623+anteju@users.noreply.github.com>

* Add ddp_find_unused_parameters=True and change accelerator to auto (#7623) (#7644)

* Add ddp_find_unused_parameters=True and change acclerator to auto



* Add ddp_find_unused_parameters True for normalization_as_tagging_train.py



---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

* Fix py3.11 dataclasses issue  (#7616)

* Fix py3.11 dataclasses issue  (#7582)

* Update ASR configs to support Python 3.11

Signed-off-by: smajumdar <titu1994@gmail.com>

* Update TTS configs to support Python 3.11

Signed-off-by: smajumdar <titu1994@gmail.com>

* Guard MeCab and Ipadic

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix remaining ASR dataclasses

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix remaining ASR dataclasses

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix scripts

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Update name to ConfidenceMethodConfig

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain (#7576) (#7586)

* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain
* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Safeguard nemo_text_processing installation on ARM (#7485)

* safeguard nemo_text_processing installing

Signed-off-by: Jason <jasoli@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update check

Signed-off-by: Jason <jasoli@nvidia.com>

---------

Signed-off-by: Jason <jasoli@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Fix changes to confidence measure

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: smajumdar <titu1994@gmail.com>
Signed-off-by: Sangkug Lym <slym@nvidia.com>
Signed-off-by: Jason <jasoli@nvidia.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>
Co-authored-by: Sangkug Lym <slym@nvidia.com>
Co-authored-by: Jason <jasoli@nvidia.com>

* [Stable Diffusion/ControlNet] Enable O2 training for SD and Fix ControlNet CI failure

* Mingyuanm/dreambooth fix

* Fix NeMo CI Infer Issue

* DreamFusion

* Move neva export changes

* Add Imagen Synthetic Dataloader

* Add VITWrapper and export stuff to wrapper

* Update neva with megatron-core support

* Fix issues with Dockerfile (#7650) (#7652)

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>

* [ASR] RNN-T greedy decoding max_frames fix for alignment and confidence (#7635)

* decoding and test fix

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* [ASR] Fix type error in jasper (#7636) (#7653)

Signed-off-by: Ryan <rlangman@nvidia.com>
Co-authored-by: Ryan Langman <rlangman@nvidia.com>

* [TTS] Add STFT and SI-SDR loss to audio codec recipe (#7468)

* [TTS] Add STFT and SI-SDR loss to audio codec recipe

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Fix STFT resolution

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Fix training metric logging

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Add docstring to mel and stft losses

Signed-off-by: Ryan <rlangman@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Ryan <rlangman@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>

* Create per.py (#7538)

* Move model precision copy (#7336)

* move cfg precision set to megatron base model

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* remove copy from other models

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* modify attribute not arg

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* fix gpt model test for ptl 2.0

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* rename function and add docstring

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* replace precision to dtype conditionals with func call

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* unnecessary function and cfg reset

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* set default value

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* fix precision lookup in a few more places

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* rename mapping function

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* ununsed import

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* save torch datatype to model

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* set weights precision wrt amp o2

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* Revert ""set weights precision wrt amp o2""

This reverts commit 313a4bfe5eb69d771a6d2433898c0685836aef5c.

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* revert half precision at inference attempt

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* move autocast dtype to base model

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* move params dtype to base model, enable fp16 O2 inf

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* unused imports

Signed-off-by: Maanu Grover <maanug@nvidia.com>

---------

Signed-off-by: Maanu Grover <maanug@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix PEFT checkpoint loading (#7388)

* Fix PEFT checkpoint loading

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Jason Wang <jasonwan@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Use distributed optimizer support for multiple dtypes (#7359)

* Update distopt wrapper with multiple dtype support

Remove manual handling of separate FP32 optimizer.

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Use distopt support for contiguous buffers with multiple dtypes

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Fix typo

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Separate distopt buckets for first GPT layer and non-overlapped params

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Add distopt logic for int dtypes

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Update Apex commit

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Remove unused variables

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Update Apex commit in README and Jenkensfile

Signed-off-by: Tim Moon <tmoon@nvidia.com>

* Debug Dockerfile and Jenkinsfile

Signed-off-by: Tim Moon <tmoon@nvidia.com>

---------

Signed-off-by: Tim Moon <tmoon@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* minor fix for llama ckpt conversion script (#7387)

* minor fix for llama ckpt conversion script

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* Update Jenkinsfile

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* remove fast_swiglu configuration

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

---------

Signed-off-by: Jason Wang <jasonwan@nvidia.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix wrong calling of librosa.get_duration() in notebook (#7376)

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Co-authored-by: Somshubra Majumdar <titu1994@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [PATCH] PEFT import mcore (#7393)

* [PATCH] PEFT import mcore

Signed-off-by: Jason Wang <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Jason Wang <jasonwan@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Create per.py

Script for calculation Punctuation Error Rate and related rates (correct rate, deletions rate, etc.)

Signed-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [TTS] Added a callback for logging initial data (#7384)

Signed-off-by: Ante Jukić <ajukic@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Update Core Commit (#7402)

* Update Core Commit

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* update commit

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

---------

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Use cfg attribute in bert (#7394)

* use cfg attribute instead of arg

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* use torch_dtype in place of cfg.precision

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* move precision copy before super constructor

Signed-off-by: Maanu Grover <maanug@nvidia.com>

* use trainer arg

Signed-off-by: Maanu Grover <maanug@nvidia.com>

---------

Signed-off-by: Maanu Grover <maanug@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add support for bias conversion in Swiglu models (#7386)

* Add support for bias conversion in Swiglu models

Signed-off-by: smajumdar <titu1994@gmail.com>

* Add support for auto extracting tokenizer model

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Add support for auto extracting tokenizer model

Signed-off-by: smajumdar <titu1994@gmail.com>

* Fix issue with missing tokenizer

Signed-off-by: smajumdar <titu1994@gmail.com>

* Refactor

Signed-off-by: smajumdar <titu1994@gmail.com>

* Refactor

Signed-off-by: smajumdar <titu1994@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: smajumdar <titu1994@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Update save_to and restore_from for dist checkpointing (#7343)

* add dist ckpt to save to, in progress

Signed-off-by: eharper <eharper@nvidia.com>

* move dist ckpt

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* clean up

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update restore from, need to figure out how to initialize distributed

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* launch distrib if needed when restoring dist ckpt

Signed-off-by: eharper <eharper@nvidia.com>

* when using mcore we can change tp pp on the fly

Signed-off-by: eharper <eharper@nvidia.com>

* add load_from_checkpoint support for dist ckpt

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* update llama convert script to save dist .nemo

Signed-off-by: eharper <eharper@nvidia.com>

* fix load dist ckpt

Signed-off-by: jasonwan <jasonwan@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* setup TE TP groups if needed

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* setup te tp groups if needed

Signed-off-by: eharper <eharper@nvidia.com>

* remove import

Signed-off-by: eharper <eharper@nvidia.com>

---------

Signed-off-by: eharper <eharper@nvidia.com>
Signed-off-by: jasonwan <jasonwan@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: jasonwan <jasonwan@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* fix forward for with mcore=false (#7403)

Signed-off-by: Jimmy Zhang <jiemingz@nvidia.com>
Co-authored-by: Jimmy Zhang <jiemingz@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix logging to remove 's/it' from progress bar in Megatron models and add train_step_timing (#7374)

* Add CustomProgressBar class to exp_manager and trainer callbacks

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix the progress bar to reflect total microbatch cnt

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Modify CustomProgressBar class

1) Modify CustomProgressBar class to update progress bar per global_step instead of per microbatch
2) Add the callback to other megatron training/finetuning files that are not using MegatronTrainerBuilder

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Add CustomProgressBar callback to tuning files

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Set Activation Checkpointing Defaults (#7404)

* Set Activation Checkpointing Defaults

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* check for None

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* make loss mask default to false (#7407)

Signed-off-by: eharper <eharper@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add dummy userbuffer config files (#7408)

Signed-off-by: Sangkug Lym <slym@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* add missing ubconf files (#7412)

Signed-off-by: Abhinav Khattar <aklife97@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* New tutorial on Speech Data Explorer (#7405)

* Added Google Colab based tutorial on Speech Data Explorer

Signed-off-by: George Zelenfroynd <gzelenfroind@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Update ptl training ckpt conversion script to work with dist ckpt (#7416)

* update ptl convert script

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* don't break legacy

Signed-off-by: eharper <eharper@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: eharper <eharper@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Allow disabling sanity checking when num_sanity_val_steps=0 (#7413)

* Allow disabling sanity checking when num_sanity_val_steps=0

Signed-off-by: Abhishree <abhishreetm@gmail.com>

* Update num_sanity_val_steps to be a multiple of num_microbatches

Signed-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Signed-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add comprehensive error messages (#7261)

Signed-off-by: Anton Peganov <apeganov@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* check NEMO_PATH (#7418)

Signed-off-by: Nikolay Karpov <karpnv@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* layer selection for ia3 (#7417)

* layer selection for ia3

Signed-off-by: arendu <adithyare@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: arendu <adithyare@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix missing pip package 'einops' (#7397)

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix failure of pyaudio in Google Colab (#7396)

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Update README.md: output_path --> output_manifest_filepath (#7442)

Signed-off-by: Samuele Cornell <cornellsamuele@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add rope dynamic linear scaling (#7437)

* Add dynamic linear scaling

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix bug

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* Fix

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>

---------

Signed-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix None dataloader issue in PTL2.0 (#7455)

* Fix None dataloader issue in PTL2.0

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* updating values of self._validation_dl and self._test_dl as well

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* updating values of self._validation_dl and self._test_dl as well

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: KunalDhawan <kunaldhawan97@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [ASR] Confidence measure -> method renames (#7434)

* measure -> method

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Add steps for document of getting dataset 'SF Bilingual Speech' (#7378)

* Add steps for document of getting dataset 'SF Bilingual Speech'

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Update datasets.rst

added a link from a tutorial demonstrating detailed data prep steps.

Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Co-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* RNN-T confidence and alignment bugfix (#7381)

* new frame_confidence and alignments lists are now always created after the while loop

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

* tests added

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>

---------

Signed-off-by: Aleksandr Laptev <alaptev@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix resume from checkpoint in exp_manager (#7424) (#7426)

Signed-off-by: Abhishree <abhishreetm@gmail.com>
Co-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>
Co-authored-by: Eric Harper <complex451@gmail.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix checking of cuda/cpu device for inputs of Decoder (#7444)

* Fix checking of cuda/cpu device for inputs of Decoder

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* Update tacotron2.py

Signed-off-by: Jason <jasoli@nvidia.com>

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Signed-off-by: Jason <jasoli@nvidia.com>
Co-authored-by: Jason <jasoli@nvidia.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* Fix failure of ljspeech's get_data.py (#7430)

* Fix failure of ljspeech's get_data.py

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Robin Dong <robin.k.dong@gmail.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Signed-off-by: Sasha Meister <sasha.meister.work@gmail.com>

* [TTS] Fix audio codec type checks (#7373)

* [TTS] Fix audio codec type checks

Signed-off-by: Ryan <rlangman@nvidia.com>

* [TTS] Fix audio codec tests

Signed-off-by: Ryan <rlangman@nvidia.com>

---------

Signed-off-by: Ryan <rlangman@nvidia.com>
Signe…"
github.com/nlpxucan/WizardLM,training/src/generate.py,2023-07-08T14:21:04Z,training code
github.com/artidoro/qlora,examples/guanaco_generate.py,2023-07-23T22:05:52Z,generage example
github.com/ray-project/ray,doc/source/templates/04_finetuning_llms_with_deepspeed/merge_lora_weights.py,2023-10-19T03:13:00Z,"[Train][Templates] Add LoRA support to Llama-2 finetuning example (#37794)

Signed-off-by: Artur Niederfahrenhorst <attaismyname@googlemail.com>
Co-authored-by: kourosh hakhamaneshi <31483498+kouroshHakha@users.noreply.github.com>"
github.com/RUCAIBox/LLMSurvey,Experiments/InstructTuning/mmlu/modeling.py,2023-06-29T09:47:51Z,add:prompts
github.com/apple/ml-ferret,ferret/model/builder.py,2023-10-31T03:44:41Z,first code commit
github.com/NVIDIA/TensorRT-LLM,examples/enc_dec/run.py,2024-02-27T09:37:34Z,"Update TensorRT-LLM (#1168)

* Update TensorRT-LLM

---------

Co-authored-by: Bhuvanesh Sridharan <bhuvan.sridharan@gmail.com>
Co-authored-by: Shixiaowei02 <39303645+Shixiaowei02@users.noreply.github.com>"
github.com/NVIDIA/TensorRT-LLM,examples/enc_dec/run.py,2024-02-21T13:30:55Z,"Update TensorRT-LLM (#1122)

* Update TensorRT-LLM

---------

Co-authored-by: Eddie-Wang1120 <wangjinheng1120@163.com>
Co-authored-by: Shixiaowei02 <39303645+Shixiaowei02@users.noreply.github.com>"
github.com/NVIDIA/TensorRT-LLM,examples/enc_dec/run.py,2024-02-18T07:48:08Z,"Update TensorRT-LLM (#1098)

* Update TensorRT-LLM

* update submodule

* Remove unused binaries"
github.com/NVIDIA/TensorRT-LLM,examples/enc_dec/run.py,2024-01-31T13:55:32Z,"Update TensorRT-LLM (#1019)

* Update TensorRT-LLM

---------

Co-authored-by: erenup <ping.nie@pku.edu.cn>
Co-authored-by: Shixiaowei02 <39303645+Shixiaowei02@users.noreply.github.com>"
github.com/NVIDIA/TensorRT-LLM,examples/enc_dec/run.py,2024-01-23T15:22:35Z,"Update TensorRT-LLM (#941)

* Update TensorRT-LLM

---------

Co-authored-by: Shixiaowei02 <39303645+Shixiaowei02@users.noreply.github.com>"
github.com/NVIDIA/TensorRT-LLM,examples/enc_dec/run.py,2024-01-09T13:03:35Z,"Update TensorRT-LLM (#846)

* Update TensorRT-LLM

---------

Co-authored-by: Shixiaowei02 <39303645+Shixiaowei02@users.noreply.github.com>"
github.com/NVIDIA/TensorRT-LLM,examples/enc_dec/run.py,2024-01-02T09:54:32Z,"Update TensorRT-LLM (#787)

* Update TensorRT-LLM

---------

Co-authored-by: Shixiaowei02 <39303645+Shixiaowei02@users.noreply.github.com>"
github.com/NVIDIA/TensorRT-LLM,examples/enc_dec/run.py,2023-12-27T09:41:24Z,"Update TensorRT-LLM main branch (#754)

* Update TensorRT-LLM

---------

Co-authored-by: Shixiaowei02 <39303645+Shixiaowei02@users.noreply.github.com>"
github.com/NVIDIA/TensorRT-LLM,examples/enc_dec/run.py,2023-12-15T14:14:51Z,"Update TensorRT-LLM (#667)

* Update TensorRT-LLM

---------

Co-authored-by: 0xymoro <jerrymeng100@gmail.com>
Co-authored-by: Shixiaowei02 <39303645+Shixiaowei02@users.noreply.github.com>"
github.com/NVIDIA/TensorRT-LLM,examples/enc_dec/run.py,2023-12-08T09:49:24Z,"Update TensorRT-LLM (#613)

* Update TensorRT-LLM

---------

Co-authored-by: Shixiaowei02 <39303645+Shixiaowei02@users.noreply.github.com>
Co-authored-by: zhang-ge-hao <842720660@qq.com>"
github.com/NVIDIA/TensorRT-LLM,examples/enc_dec/run.py,2023-12-01T14:27:51Z,Update TensorRT-LLM (#524)
github.com/NVIDIA/TensorRT-LLM,examples/enc_dec/run.py,2023-11-24T14:12:26Z,"Update TensorRT-LLM (#465)

* Update TensorRT-LLM

---------

Co-authored-by: Shixiaowei02 <39303645+Shixiaowei02@users.noreply.github.com>"
github.com/NVIDIA/TensorRT-LLM,examples/enc_dec/run.py,2023-11-17T16:05:54Z,"Update TensorRT-LLM (#422)

* Update TensorRT-LLM

---------

Co-authored-by: Tltin <TltinDeng01@gmail.com>
Co-authored-by: zhaohb <zhaohbcloud@126.com>
Co-authored-by: Bradley Heilbrun <brad@repl.it>
Co-authored-by: nqbao11 <nqbao11.01@gmail.com>
Co-authored-by: Nikhil Varghese <nikhil@bot-it.ai>"
github.com/NVIDIA/TensorRT-LLM,examples/enc_dec/run.py,2023-10-18T14:38:53Z,"Kaiyu/update main (#5)

* Update

* Update"
github.com/pengxiao-song/LaWGPT,merge.py,2023-05-22T17:11:58Z,[ENH] Add merge.py
github.com/LianjiaTech/BELLE,train/src/trainer.py,2023-09-26T09:10:33Z,"add ppo (#519)

* add ppo

* update doc

* update readme"
github.com/LianjiaTech/BELLE,train/src/trainer.py,2023-08-10T10:27:54Z,add zero inference
github.com/LianjiaTech/BELLE,train/src/trainer.py,2023-07-18T15:44:57Z,add peft & deepspeed + peft resume
github.com/LianjiaTech/BELLE,train/src/trainer.py,2023-06-15T15:58:12Z,"remove peft
adjust source tree
fix peft + deepspeed save model
add client server inference"
github.com/pengxiao-song/LaWGPT,infer.py,2023-06-06T13:00:30Z,[MNT] Fix infer scripts
github.com/pengxiao-song/LaWGPT,infer.py,2023-05-22T13:37:02Z,[ENH] Add infer.py
github.com/wenda-LLM/wenda,llms/llm_llama.py,2023-06-24T11:02:21Z,将所有请求参数全部暴露给模型
github.com/wenda-LLM/wenda,llms/llm_llama.py,2023-06-09T12:59:54Z,Update llm_llama.py
github.com/wenda-LLM/wenda,llms/llm_llama.py,2023-06-09T11:19:33Z,Update llm_llama.py
github.com/wenda-LLM/wenda,llms/llm_llama.py,2023-06-08T06:24:43Z,Update llm_llama.py
github.com/wenda-LLM/wenda,llms/llm_llama.py,2023-05-30T17:58:22Z,LLAMA系列支持torch推理，使用和RWKV相同的strategy逻辑进行控制torch和llamacpp的切换
github.com/wenda-LLM/wenda,llms/llm_llama.py,2023-05-07T10:20:26Z,结构调整
github.com/wenda-LLM/wenda,llms/llm_glm6b.py,2023-11-03T13:02:23Z,Update llm_glm6b.py
github.com/wenda-LLM/wenda,llms/llm_glm6b.py,2023-11-03T11:59:39Z,支持chatglm3工具使用，并增加auto示例
github.com/wenda-LLM/wenda,llms/llm_glm6b.py,2023-11-03T05:34:41Z,修复chatglm3历史对话
github.com/wenda-LLM/wenda,llms/llm_glm6b.py,2023-07-08T15:58:10Z,修改device_map以支持ChatGLM2多卡运行，用法及参数设置不变
github.com/wenda-LLM/wenda,llms/llm_glm6b.py,2023-06-30T06:15:54Z,Update llm_glm6b.py
github.com/wenda-LLM/wenda,llms/llm_glm6b.py,2023-06-29T13:17:39Z,Update llm_glm6b.py
github.com/wenda-LLM/wenda,llms/llm_glm6b.py,2023-06-29T12:37:32Z,"Revert ""Update llm_glm6b.py""

This reverts commit 58f01e6823a9c2bf64a98df2da59123d9fa1dd49."
github.com/wenda-LLM/wenda,llms/llm_glm6b.py,2023-06-27T17:09:56Z,Update llm_glm6b.py
github.com/wenda-LLM/wenda,llms/llm_glm6b.py,2023-06-26T08:10:11Z,torch1.x支持chatglm2
github.com/wenda-LLM/wenda,llms/llm_glm6b.py,2023-06-24T11:02:21Z,将所有请求参数全部暴露给模型
github.com/wenda-LLM/wenda,llms/llm_glm6b.py,2023-06-24T10:52:44Z,优化Lora在线切换逻辑，百川支持优化Lora在线切换
github.com/wenda-LLM/wenda,llms/llm_glm6b.py,2023-06-15T13:15:44Z,Update llm_glm6b.py
github.com/wenda-LLM/wenda,llms/llm_glm6b.py,2023-06-15T13:13:31Z,-
github.com/wenda-LLM/wenda,llms/llm_glm6b.py,2023-06-15T13:05:08Z,初步支持aquila的hf版
github.com/wenda-LLM/wenda,llms/llm_glm6b.py,2023-06-01T03:21:33Z,静态资源路由由fastapi实现
github.com/wenda-LLM/wenda,llms/llm_glm6b.py,2023-05-30T17:58:22Z,LLAMA系列支持torch推理，使用和RWKV相同的strategy逻辑进行控制torch和llamacpp的切换
github.com/wenda-LLM/wenda,llms/llm_glm6b.py,2023-05-27T13:17:26Z,增加GLM-6B Lora在线切换功能
github.com/wenda-LLM/wenda,llms/llm_glm6b.py,2023-05-07T10:20:26Z,结构调整
github.com/pengxiao-song/LaWGPT,webui.py,2023-05-22T09:18:21Z,[MNT] Update webui.py
github.com/pengxiao-song/LaWGPT,webui.py,2023-05-22T07:47:47Z,[MNT] Update some file
github.com/pengxiao-song/LaWGPT,webui.py,2023-05-21T18:39:04Z,[ENH] Restructure the project.
github.com/h2oai/h2ogpt,src/export_hf_checkpoint.py,2023-08-19T19:46:34Z,Merge remote-tracking branch 'origin/main' into perf
github.com/h2oai/h2ogpt,src/export_hf_checkpoint.py,2023-08-17T17:23:04Z,Fixes #678
github.com/h2oai/h2ogpt,src/export_hf_checkpoint.py,2023-08-15T04:24:27Z,WIP for performance benchmarks. Use h2ogpt models.
github.com/h2oai/h2ogpt,src/export_hf_checkpoint.py,2023-07-31T22:49:26Z,Add 70b
github.com/h2oai/h2ogpt,src/export_hf_checkpoint.py,2023-07-31T06:06:48Z,Create 7b/13b merges.
github.com/h2oai/h2ogpt,src/export_hf_checkpoint.py,2023-07-28T19:57:19Z,Create llama2 export.
github.com/h2oai/h2ogpt,src/export_hf_checkpoint.py,2023-07-12T20:03:00Z,"Refactors/cleanups, still fails the same."
github.com/h2oai/h2ogpt,src/export_hf_checkpoint.py,2023-07-12T00:27:30Z,Improve naming of vars.
github.com/h2oai/h2ogpt,src/export_hf_checkpoint.py,2023-07-11T22:50:32Z,Add test code.
github.com/h2oai/h2ogpt,src/export_hf_checkpoint.py,2023-07-11T19:26:05Z,Fix model loader refactor.
github.com/h2oai/h2ogpt,src/export_hf_checkpoint.py,2023-07-11T08:25:19Z,"Simpler use of model loader, and fix instructions to avoid CUDA extension not installed issue"
github.com/h2oai/h2ogpt,src/export_hf_checkpoint.py,2023-07-08T05:10:09Z,Fix test_export_copy
github.com/h2oai/h2ogpt,src/export_hf_checkpoint.py,2023-07-07T22:27:22Z,Move files to src
github.com/RUCAIBox/LLMSurvey,Experiments/InstructTuning/auto_eval/generate.py,2023-06-29T09:47:51Z,add:prompts
github.com/nlpxucan/WizardLM,WizardLM/src/infer_wizardlm13b.py,2023-06-14T08:55:50Z,Add files via upload
github.com/wenda-LLM/wenda,llms/llm_baichuan.py,2023-07-19T15:14:25Z,增加generic_transformers模组，经测试可兼容Llama-2-13B-chat-GPTQ
github.com/wenda-LLM/wenda,llms/llm_baichuan.py,2023-07-19T12:58:15Z,百川模组支持自定义interface
github.com/wenda-LLM/wenda,llms/llm_baichuan.py,2023-07-19T12:14:18Z,Update llm_baichuan.py
github.com/wenda-LLM/wenda,llms/llm_baichuan.py,2023-07-11T14:44:34Z,百川13b自动量化
github.com/wenda-LLM/wenda,llms/llm_baichuan.py,2023-07-07T10:45:35Z,百川增加GPTQ支持
github.com/wenda-LLM/wenda,llms/llm_baichuan.py,2023-06-25T03:59:18Z,-
github.com/wenda-LLM/wenda,llms/llm_baichuan.py,2023-06-24T16:55:38Z,Update llm_baichuan.py
github.com/wenda-LLM/wenda,llms/llm_baichuan.py,2023-06-24T16:48:48Z,Update llm_baichuan.py
github.com/wenda-LLM/wenda,llms/llm_baichuan.py,2023-06-24T16:23:44Z,Update llm_baichuan.py
github.com/wenda-LLM/wenda,llms/llm_baichuan.py,2023-06-24T11:02:21Z,将所有请求参数全部暴露给模型
github.com/wenda-LLM/wenda,llms/llm_baichuan.py,2023-06-24T10:52:44Z,优化Lora在线切换逻辑，百川支持优化Lora在线切换
github.com/wenda-LLM/wenda,llms/llm_baichuan.py,2023-06-24T10:02:43Z,百川支持流式输出和多用户并行（需更新transformers）
github.com/wenda-LLM/wenda,llms/llm_baichuan.py,2023-06-24T03:26:36Z,百川更新：lora支持、对话prompt、raw模式、多轮对话
github.com/wenda-LLM/wenda,llms/llm_baichuan.py,2023-06-15T09:24:42Z,Update llm_baichuan.py
github.com/wenda-LLM/wenda,llms/llm_baichuan.py,2023-06-15T07:25:15Z,实验性支持aquila、baichuan
github.com/huggingface/trl,trl/trainer/kto_trainer.py,2024-03-01T11:19:55Z,"[KTO] prevent nans from appearing in metrics (#1386)

* add warning for imbalanced data

* update documentation

* update script commands to be same as in dpo

* use batch_size KL examples and batch_size target examples to calculate batch_size losses

* fix deepspeed issue

* speed up forward with no_grad for KL

* add some removed metrics

* Update trl/trainer/kto_trainer.py

* Update trl/trainer/kto_trainer.py

* Update trl/trainer/kto_trainer.py

add reference to paper

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

* add more detailed comments

* convert assert to ValueError

* Update kto_trainer.py

* precommit formatting

* remove nans in metrics by gathering across machines

* fix formatting

---------

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>
Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>"
github.com/huggingface/trl,trl/trainer/kto_trainer.py,2024-03-01T11:15:14Z,"[KTO] merge eval dataset only if it exists (#1383)

* merge eval dataset if it exists

* add eval dataset test"
github.com/huggingface/trl,trl/trainer/kto_trainer.py,2024-02-29T08:01:52Z,"fix bugs in KTO implementation (#1380)

* add warning for imbalanced data

* update documentation

* update script commands to be same as in dpo

* use batch_size KL examples and batch_size target examples to calculate batch_size losses

* fix deepspeed issue

* speed up forward with no_grad for KL

* add some removed metrics

* Update trl/trainer/kto_trainer.py

* Update trl/trainer/kto_trainer.py

* Update trl/trainer/kto_trainer.py

add reference to paper

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

* add more detailed comments

* convert assert to ValueError

* Update kto_trainer.py

* precommit formatting

---------

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>
Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>"
github.com/huggingface/trl,trl/trainer/kto_trainer.py,2024-02-19T13:43:17Z,"Kto trainer (#1181)

* initial file

* initial tokenizer

* UnpairedPreferenceBatchSampler

* use batch_sampler

* use interleave_datasets

* add loss

* fix imports

* use SequentialSampler when training

* formatting

* add other helpers

* add prediction_step

* fix the kto pair docs

* tests

* compute_reference_log_probs

* add get_eval_dataloader

* fix typo

* kto with is_encoder_decoder true

* Update docs/source/dpo_trainer.mdx

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* fixed typo

* Update trl/trainer/kto_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update docs/source/kto_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update docs/source/kto_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* renamed KTO dataset keys

* use DPOTrainer's get_batch_logps

* add get_batch_samples

* typo

* Handle last token in prompt

* Create KTOConfig class that subclasses transformers.TrainingArguments

* Update KTO tests to handle KTOConfig

* Update KTO script to use KTOConfig

* formatting

* Update docs/source/dpo_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update docs/source/kto_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update docs/source/kto_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update trl/trainer/training_configs.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update examples/scripts/kto.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update examples/scripts/kto.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* use max_completion_length

* Update examples/scripts/kto.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* add back get_batch_logps

* use max_completion_length

* move config to its own file

* Check tokenize params on Trainer init

* Clone labels for end-dec model to solve RuntimeError

* formatting

* fix enc-dec later

* completion_decoder_input_ids is optional for enc-dec

* fix breaking test

* add a kl key for KL estimation with shuffled completion

* add loss ad weights

* fix bug in chosen_idx

* add back metrics

* fix typos

* fix kto_loss docs

* typo

* set loss to None when there is no target completions in batch

* use nan tensor instead of none

* fix reference_logps test

* fix logits

* a bit more robust options

* log only the correct prompt-completion during eval

* Update trl/trainer/kto_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update examples/scripts/kto.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update examples/scripts/kto.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update docs/source/kto_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update docs/source/dpo_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* add docs for desirable_weight and undesirable_weight args

* dropout is always disabled

* remove DDP hack

* formatting

* move more arguments of trainer to config

* comment out T5 test for now

* Add docstring to KTOTrainer

* moved Config docstrings to the appropriate class

* add autodoc to markdown

* formatting

* updated copyright year

* add model tags

* do not add BOS to start of completion

* Move data_collator to KTOTrainer

* formatting

* data_collator is not in args

* shuffle_completion with specific input_columns

* remove all but the needed columns

* Update docs/source/dpo_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update examples/scripts/kto.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update tests/test_kto_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* moved more args to kto_config

* fjx test

* use all_exhausted strategy and shuffle after

* use KTOConfig in HfArgumentParser

* use ModelConfig

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>
Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>
Co-authored-by: Pablo Vicente Juan <p.vicente.juan@gmail.com>"
github.com/Facico/Chinese-Vicuna,chat.py,2023-05-10T12:30:03Z,update code
github.com/Facico/Chinese-Vicuna,chat.py,2023-05-10T09:08:35Z,update code
github.com/Facico/Chinese-Vicuna,chat.py,2023-04-18T13:48:23Z,add OutOfMemory exception handler.
github.com/Facico/Chinese-Vicuna,chat.py,2023-04-04T11:39:35Z,rearrange code; add html code support
github.com/Facico/Chinese-Vicuna,chat.py,2023-04-01T14:11:04Z,"add better support for chat (streamly beam search,sample,greedy and beam-sample; cancel button, more prompt type and so on )"
github.com/Facico/Chinese-Vicuna,chat.py,2023-03-30T06:35:32Z,link you can share
github.com/Facico/Chinese-Vicuna,chat.py,2023-03-28T16:21:03Z,add support for clear history; fix original reset btn
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2024-02-27T10:19:16Z,feature request add `force_use_ref_model` (#1367)
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2024-02-19T13:43:17Z,"Kto trainer (#1181)

* initial file

* initial tokenizer

* UnpairedPreferenceBatchSampler

* use batch_sampler

* use interleave_datasets

* add loss

* fix imports

* use SequentialSampler when training

* formatting

* add other helpers

* add prediction_step

* fix the kto pair docs

* tests

* compute_reference_log_probs

* add get_eval_dataloader

* fix typo

* kto with is_encoder_decoder true

* Update docs/source/dpo_trainer.mdx

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* fixed typo

* Update trl/trainer/kto_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update docs/source/kto_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update docs/source/kto_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* renamed KTO dataset keys

* use DPOTrainer's get_batch_logps

* add get_batch_samples

* typo

* Handle last token in prompt

* Create KTOConfig class that subclasses transformers.TrainingArguments

* Update KTO tests to handle KTOConfig

* Update KTO script to use KTOConfig

* formatting

* Update docs/source/dpo_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update docs/source/kto_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update docs/source/kto_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update trl/trainer/training_configs.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update examples/scripts/kto.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update examples/scripts/kto.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* use max_completion_length

* Update examples/scripts/kto.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* add back get_batch_logps

* use max_completion_length

* move config to its own file

* Check tokenize params on Trainer init

* Clone labels for end-dec model to solve RuntimeError

* formatting

* fix enc-dec later

* completion_decoder_input_ids is optional for enc-dec

* fix breaking test

* add a kl key for KL estimation with shuffled completion

* add loss ad weights

* fix bug in chosen_idx

* add back metrics

* fix typos

* fix kto_loss docs

* typo

* set loss to None when there is no target completions in batch

* use nan tensor instead of none

* fix reference_logps test

* fix logits

* a bit more robust options

* log only the correct prompt-completion during eval

* Update trl/trainer/kto_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update examples/scripts/kto.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update examples/scripts/kto.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update docs/source/kto_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update docs/source/dpo_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* add docs for desirable_weight and undesirable_weight args

* dropout is always disabled

* remove DDP hack

* formatting

* move more arguments of trainer to config

* comment out T5 test for now

* Add docstring to KTOTrainer

* moved Config docstrings to the appropriate class

* add autodoc to markdown

* formatting

* updated copyright year

* add model tags

* do not add BOS to start of completion

* Move data_collator to KTOTrainer

* formatting

* data_collator is not in args

* shuffle_completion with specific input_columns

* remove all but the needed columns

* Update docs/source/dpo_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update examples/scripts/kto.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update tests/test_kto_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* moved more args to kto_config

* fjx test

* use all_exhausted strategy and shuffle after

* use KTOConfig in HfArgumentParser

* use ModelConfig

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>
Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>
Co-authored-by: Pablo Vicente Juan <p.vicente.juan@gmail.com>"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2024-02-15T13:47:32Z,"[`core` /  `xxxTrainer`] Automatic tagging (#1329)

* automatic tagging

* add comments

* fix tests

* fix"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2024-02-02T10:02:40Z,"Fix AttributeError in dpo_trainer for reference_free case in dpo_loss function (#1313)

* Update dpo_trainer.py

update reference_free parameter for dpo_loss

* Update dpo_trainer for reference_free case

updated the docstring typo and set device parameter to ref_logratios tensor"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2024-02-01T16:58:00Z,Add num_proc arg to the eval_dataset processing (#1307)
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2024-01-31T13:51:58Z,"Types: Fix PEP 484 implicit-optional compliance (#1297)

This was done automatically with hauntsaninja/no_implicit_optional."
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2024-01-31T13:49:41Z,"Fix `DPOTrainer` docstrings (#1298)

Some issues were leading the auto-generation of the API reference to fail and the args were overlapped in the documentation page"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2024-01-30T09:14:22Z,load data only on main process + fix dpo example test (#1291)
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2024-01-30T07:25:29Z,fix DPO trainer + mistral + FA2 (#1290)
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2024-01-30T07:24:48Z,fix padding in dpo trainer (#1284)
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2024-01-30T07:06:03Z,raise value error if one passes a ref_model and a peft_config (#1289)
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2024-01-30T01:55:07Z,"Add multiprocessing in the DPO trainer. (#1286)

* Update dpo_trainer.py

Added support for num_proc to tokenize the training dataset.

* Update dpo_trainer.py

added type in the new num_proc variable

* added test case

* add test case

* fix type

---------

Co-authored-by: imraviagrawal <ravi.agrawal@umass.edu>
Co-authored-by: Ravi Agrawal <raviagrawal@Ravis-MacBook-Pro.local>"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2024-01-24T11:18:04Z,"[DPO] average_log_prob when loss is IPO (#1265)

* average_log_prob when loss is IPO

* updated docs with the fix"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2024-01-18T16:20:52Z,fix: fix loss_type and some args desc (#1247)
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2024-01-15T14:40:44Z,"Update dpo_trainer.py (#1160)

Log metrics on all distributed processes"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2024-01-09T13:10:22Z,"Check tokenize params on DPOTrainer (#1197)

* Check if tokenizer and max len params are None

* Update warning messages for missing parameters"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2024-01-09T08:35:50Z,"[`DPOTrainer`] Fix peft + DPO + bf16 if one uses `generate_during_eval` or pre-computed logits (#1203)

* fix peft + DPO + bf16

* fix

* revert old behaviour

* fix tests

* fix

* fix

* fix

* fix"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2024-01-08T15:12:45Z,"Allow swapping PEFT adapters for target/ref model. (#1193)

* Allow swapping PEFT adapters for target/ref model.

* Update DPOTrainer docs.

* python format

* isort

* Update docs/source/dpo_trainer.mdx

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

* Update docs/source/dpo_trainer.mdx

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

* Update docs/source/dpo_trainer.mdx

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

* Update docs/source/dpo_trainer.mdx

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

* Update docs/source/dpo_trainer.mdx

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>

---------

Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2024-01-08T09:26:40Z,"Allow separate devices for target/ref models. (#1190)

* Allow separate devices for target/ref models.

* Remove original/duplicate.

* Cleanup original, black formatting.

---------

Co-authored-by: Jon Durbin <jonathan@convai.com>"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2024-01-08T08:15:53Z,"Handle last token from generation prompt (#1153)

* Handle last token from generation prompt

* Remove prints

* Reformat dpo_trainer file"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-12-29T09:55:58Z,change device order of metrics (#1154)
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-12-26T15:39:10Z,"[`xxxTrainer`] Add unsloth tag (#1130)

* add unsloth tag

* add it on all trainers

* few changes

* add in docs

* revert

* final commit"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-12-26T10:25:53Z,"add peft_module_casting_to_bf16 in DPOTrainer (#1143)

* add peft_module_casting_to_bf16 in DPOTrainer

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

* Update trl/trainer/dpo_trainer.py

---------

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>
Co-authored-by: Kashif Rasul <kashif.rasul@gmail.com>"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-12-22T13:52:16Z,"[`xxxTrainer`] multi-tags support for tagging (#1133)

* multi-tags support for tagging

* oops"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-12-22T12:32:16Z,rename kto loss (#1127)
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-12-21T16:28:56Z,save eval_dataset for subsequent calls (#1125)
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-12-21T16:04:18Z,"[`xxxTrainer`] Add tags to all trainers in TRL (#1120)

* add tags to sfttrainer

* extend it to other trainers

* add for ddpo"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-12-12T16:16:46Z,"[DPO] use ref model logprobs if it exists in the data (#885)

* use logprobs if it exists in the batch

* add features to tokenized batch if in data

* make get_batch_logps a static method

* add tokenize_batch_element dataset mapper

* Remove tokenize_batch method from DPODataCollator

* Initial sketch to precompute reference_logps

* run ref model via pytorch dataloader

* add a padding helper

* clean up the helper

* use logprob item()

* default behaviour

* clean up collator

* add docstring

* copy data back to cpu if needed

* use get_train_dataloader methods

* fix tests

* rename: more explicit variable name precompute_ref_log_probs

* improve comment

* update comment

* Update trl/trainer/dpo_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* refactor models into setup parameters

* parametrize precompute_ref_log_probs flag

* remove useless test

* Update trl/trainer/dpo_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update tests/test_dpo_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update tests/test_dpo_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update trl/trainer/dpo_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update trl/trainer/dpo_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* update function arg name

* distinguish between pad token_id and mask values

* fix tokenization #932 by @nrailg

* fix test

* undo test refactor

* new line

* undo breaking change

* Update token counter condition to allow Llama tokenizer

* Acount for merged tokens on certain tokenizers such Llama-2 tokenizer

* Update variable name to match list value when truncating response

* map function on multi-gpu and gather

* Add test cases for DPOTrainer tokenization step

* revert since we need the prepeared model

* Use gather_with_metrics on ref_logps precomputation to keep original dataset size

* Add flag to keep track of when ref_logps are precomputed

* make variable names private

* formatting

* if precompute_ref_log_probs is true one can use non-peft to populate log-probs

* Use tokenizer padding token unless padding_value is set

* Move dataset.map(tokenize_batch) outside dataloader to avoid serialization errors

* eval can be none

* move to cpu to avoid gpu oom

* remove unneeded cast to float32

* remove unneeded

* fix merge

* fix merge

* fix merge

* add precompute log-prob status via tqdm

* Truncate answer if too longer once prompt has been truncated

* Add prompt_input_ids to batch to enable generation

* formatting and add lora example

* fix formatting

* Tokenize row now expects sample to have space on chosen/rejected for llama

* Revert ""Tokenize row now expects sample to have space on chosen/rejected for llama""

This reverts commit dd07a10fe8c19b6ac6bbcc7b8144189756710d52.

* raise error when using zero-3 with precompute_ref_log_probs

---------

Co-authored-by: Pablo Vicente Juan <p.vicente.juan@gmail.com>
Co-authored-by: Shoaib Burq <saburq@gmail.com>
Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-12-11T10:41:03Z,"[DPO] add KTO loss (#1075)

* add KTO loss

* fix docs

* Update trl/trainer/dpo_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* formatting

* add link to papers

---------

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-12-07T07:40:53Z,Add missing `loss_type` in `ValueError` message (#1067)
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-12-01T16:03:09Z,Update dpo_trainer.py (#1049)
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-12-01T09:33:31Z,"Revert ""[DPO] Refactor eval logging of dpo trainer (#954)"" (#1047)

This reverts commit 6d9ea38ae18c7e266f797b62de4a68a12a13aba4."
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-11-30T11:09:33Z,"[DPO] Refactor eval logging of dpo trainer (#954)

* first attempts at refactor of dpo trainer

* removed extra stuff in prediction step

* import fixes

* label names

* all working

---------

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-11-30T10:50:30Z,"[DPO] cDPO loss (#1035)

* add cDPO loss

* add comment

* docs

* info about label_smoothing not being used"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-11-24T14:52:40Z,"[DPO] IPO Training loss (#1022)

* initial IPO loss

* fix loss

* fixed comments

* added docs

* fix doc-strings

* add tests

* Update trl/trainer/dpo_trainer.py

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>

* fixes for review

* Added doc about beta in the Trainer's docstring

---------

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-11-06T13:37:04Z,"fix: dpo trainer ds config (#957)

* fix: dpo trainer ds config

ref_model and model shouldn share the same ds config, so we shouldn modify the ds config directly. or else, it will cause sth wrong when init deepspeed engine

* fix: import sort

import sort by isort"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-11-06T08:48:18Z,"Adds model kwargs to SFT and DPO trainers (#951)

* adds model kwargs to SFT and DPO trainers

* adds checks for model_kwarg passing when model is not str

* changed warning to ValueError

* renames model_kwargs to model_init_kwargs

* corrects argument names in"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-11-06T08:45:46Z,"[DPO] Merge initial peft model if trainer has a peft_config (#956)

* failing test
Co-authored-by: Shoaib Burq <saburq@gmail.com>

* merge initial peft model"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-11-02T10:27:49Z,Update dpo_trainer.py (#941)
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-10-31T17:50:17Z,"[`core` / `DDP`] Fix RM trainer + DDP + quantization + propagate `gradient_checkpointing_kwargs` in SFT & DPO (#912)

* make use of forward hooks

* correctly delete attributes

* fix RM DPP issues

* revert unneeded changes

* more fixes

* fix diff

* fix

* propagate to SFT

* Update examples/scripts/reward_modeling.py

* propagate the fix on DPO trainer

* add to example scripts

* trigger CI"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-10-31T09:58:41Z,"hotfix for dpo trainer (#919)

addresses #914"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-10-31T09:55:46Z,fix DPO + GC issues (#927)
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-10-16T14:02:57Z,"[DPO] add SLiC hinge loss to DPOTrainer (#866)

* add SLiC hinge loss

* fix links

* beta when loss is hinge is reciprocal of margin

* fix tests

* fix docs

* doc strings

* fix method name

* raise error if loss_type is not correct

* Update trl/trainer/dpo_trainer.py

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>

* fix formatting

---------

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-10-03T07:56:00Z,"Fix DeepSpeed ZeRO-{1,2} for DPOTrainer (#825)"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-09-29T10:33:47Z,add option for compute_metrics in DPOTrainer (#822)
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-09-26T15:09:15Z,"init custom eval loop for further DPO evals (#766)

* init

* run

* Update custom eval loop to aid DPO debugging (#770)

* sample_during_eval -> generate_during_eval

* Remove unused return_tokens

* Add import utils for W&B, prevent test fails

* Optimize dataloader random batch selection

* Separate prompt and response in logs

Makes it much easier to quickly read the starts of the generations

* Simplify logging

* reset eval steps

* manual merge fixes

* revert merge

* remove self.max_length

* style

* fix max_length

---------

Co-authored-by: Tom Aarsen <37621491+tomaarsen@users.noreply.github.com>"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-09-12T08:56:10Z,"update to `prepare_model_for_kbit_training` (#728)

* update to `prepare_model_for_kbit_training`

from deprecated `prepare_model_for_int8_training`
and add `use_gradient_checkpointing=args.gradient_checkpointing` to
automatically follow the gradient checkpointing choice

is also the workaround for #694

* workaround for gradient checkpointing issue

calling model.gradient_checkpointing_enable() twice causes issues
this workaround calls it in prepare_model_for_kbit_training and then
changes the arg to false to make sure it isn't called again in
huggingface trainer inner loop

also changes stack_llama_2 sft trainer to use correct device map for ddp
training so that you can test this issue"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-09-08T09:50:06Z,[DPO] self.accelerator._prepare_deepspeed return tuples (#745)
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-09-07T16:03:10Z,"Seq2Seq model support for DPO (#586)

* dpo_collator for seq2seq models

* dpo trainer support

* refactoring

* update collator

* computes decoder input ids if possible

* decoder input ids for dpo trainer

* added test for seq2seq

* quality

* fixed typo

* fixed string padding for seq2seq

* fixed minor issues in padding

* fixed typo in dpo.py

* add docstring

* run all precommit

* fixed gradient accumulation steps in test

* reformatting

* fixing dpo tests

* update .mdx"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-09-07T16:02:20Z,fixed metrics typo (#743)
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-09-01T09:27:54Z,"Fix: RuntimeError: 'weight' must be 2-D issue (#687)

* Update dpo_trainer.py

* Fix: self.args.deepspeed > self.is_deepspeed_enabled

* Update trl/trainer/dpo_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-08-29T10:57:10Z,"[DPO] fix DPO ref_model=None (#703)

* fix by @tannonk

* Update trl/trainer/dpo_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* add import

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-08-18T08:02:16Z,"Allow for ref_model=None in DPOTrainer (#640)

* Update dpo_trainer.py

Make ref_model optional.

* add tests for ref_model=None

* better handling for ref_model=None

* Update dpo_trainer.py

Correct docstring

* move instantiation of self.ref_model closer to model

* use .disable_adapters instead of .get_base_model

* handle ref_model=None in get_batch_samples

* fix failing test in dpo_trainer due to disable_dropout_in_model

* Update trl/trainer/dpo_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-08-14T12:40:45Z,"Disable dropout in DPO Training (#639)

* disable dropout in dpo

* quick fix docs

* precommiot

* add disable_dropout_in_model to DPOTrainer

* disable_dropout -> disable_dropout_in_model

* .

* ."
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-07-26T06:06:25Z,"[`DPO`] Resolve logging for DPOTrainer (#570)

* Resolve logging for DPOTrainer

* Ensure the WandB logger correctly prefixes all logs

* Run pre-commit

Whoops, hadn't run `pre-commit install` yet"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-07-24T11:23:33Z,remove unused batch_size arg (#554)
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-07-19T12:12:23Z,"Relax reward trainer constraint (#539)

* relax reward trainer constraint

* Update trl/trainer/reward_trainer.py

Co-authored-by: Tom Aarsen <37621491+tomaarsen@users.noreply.github.com>

* relax also for DPO

---------

Co-authored-by: Tom Aarsen <37621491+tomaarsen@users.noreply.github.com>"
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-07-18T11:21:17Z,all the concated batches are on same device (#528)
github.com/huggingface/trl,trl/trainer/dpo_trainer.py,2023-07-17T12:52:14Z,"DPO Trainer (#416)

* initial DPO Trainer

* typo

* initial dpo from reward trainer

* calc. log_probs from logits

* remove dpo config for now

* fix inits

* add intial DPODataCollatorWithPadding

* use the RewardDataCollatorWithPadding

* initial test

* means of loss

* add assert

* just call the train instead of step

* functional debug example before refactor

* check the params have changed

* initial DPODataCollatorWithPadding

* Data collator with masking

* going through trainer.accelerate to wrap ref_model

* style / imports

* style / imports

* `broadcast_buffers=False` fix to distributed training

* better fix for DDP issues

* arguments and style clean-up

* better doc, some light refactoring

* better imports

* initial dpo doc

* fix test

* fix formatting

* fix

* called models once

* fix tests

* add example

* fix doc string

* intitial example with anthropic hh dataset

* refactored dpo trainer

* revert

* return metrics

* fixed tests

* updated docs

* update test

* fixed typo

* note about the beta

* added dpo authors

* fix docstrings

* add prediction_step

* remove compute_metrics and log metrics manually

* fix typo

* add DPOTrainer doc

* add dpo to toc

* ValueError

* add to index and example

* fix docs

* fix assert

---------

Co-authored-by: TevenLeScao <teven.lescao@gmail.com>
Co-authored-by: Gaetan LOPEZ <gaetanloplat@gmail.com>
Co-authored-by: younesbelkada <younesbelkada@gmail.com>"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2024-02-27T10:19:06Z,add  `eval_packing` (#1369)
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2024-02-19T13:43:17Z,"Kto trainer (#1181)

* initial file

* initial tokenizer

* UnpairedPreferenceBatchSampler

* use batch_sampler

* use interleave_datasets

* add loss

* fix imports

* use SequentialSampler when training

* formatting

* add other helpers

* add prediction_step

* fix the kto pair docs

* tests

* compute_reference_log_probs

* add get_eval_dataloader

* fix typo

* kto with is_encoder_decoder true

* Update docs/source/dpo_trainer.mdx

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* fixed typo

* Update trl/trainer/kto_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update docs/source/kto_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update docs/source/kto_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* renamed KTO dataset keys

* use DPOTrainer's get_batch_logps

* add get_batch_samples

* typo

* Handle last token in prompt

* Create KTOConfig class that subclasses transformers.TrainingArguments

* Update KTO tests to handle KTOConfig

* Update KTO script to use KTOConfig

* formatting

* Update docs/source/dpo_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update docs/source/kto_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update trl/trainer/kto_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update docs/source/kto_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update trl/trainer/training_configs.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update examples/scripts/kto.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update examples/scripts/kto.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* use max_completion_length

* Update examples/scripts/kto.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* add back get_batch_logps

* use max_completion_length

* move config to its own file

* Check tokenize params on Trainer init

* Clone labels for end-dec model to solve RuntimeError

* formatting

* fix enc-dec later

* completion_decoder_input_ids is optional for enc-dec

* fix breaking test

* add a kl key for KL estimation with shuffled completion

* add loss ad weights

* fix bug in chosen_idx

* add back metrics

* fix typos

* fix kto_loss docs

* typo

* set loss to None when there is no target completions in batch

* use nan tensor instead of none

* fix reference_logps test

* fix logits

* a bit more robust options

* log only the correct prompt-completion during eval

* Update trl/trainer/kto_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update examples/scripts/kto.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update examples/scripts/kto.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update docs/source/kto_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update docs/source/dpo_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* add docs for desirable_weight and undesirable_weight args

* dropout is always disabled

* remove DDP hack

* formatting

* move more arguments of trainer to config

* comment out T5 test for now

* Add docstring to KTOTrainer

* moved Config docstrings to the appropriate class

* add autodoc to markdown

* formatting

* updated copyright year

* add model tags

* do not add BOS to start of completion

* Move data_collator to KTOTrainer

* formatting

* data_collator is not in args

* shuffle_completion with specific input_columns

* remove all but the needed columns

* Update docs/source/dpo_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update examples/scripts/kto.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update tests/test_kto_trainer.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* moved more args to kto_config

* fjx test

* use all_exhausted strategy and shuffle after

* use KTOConfig in HfArgumentParser

* use ModelConfig

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>
Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>
Co-authored-by: Pablo Vicente Juan <p.vicente.juan@gmail.com>"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2024-02-15T13:47:32Z,"[`core` /  `xxxTrainer`] Automatic tagging (#1329)

* automatic tagging

* add comments

* fix tests

* fix"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2024-02-15T03:37:41Z,"pre-commit: replace linters + formatters with Ruff; fix some issues (#1300)

* pre-commit: replace linters + formatters with Ruff

* Don't use bare except

* Clean up `noqa`s

* Enable Ruff UP; apply auto-fixes

* Enable Ruff B; apply fixes

* Enable Ruff T with exceptions

* Enable Ruff C (complexity); autofix

* Upgrade Ruff to 0.2.0"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2024-01-31T13:51:58Z,"Types: Fix PEP 484 implicit-optional compliance (#1297)

This was done automatically with hauntsaninja/no_implicit_optional."
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2024-01-31T02:31:53Z,"Fix sft trainer when args is None (#1295)

* fix sft trainer when args is None

* add test

* fix"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2024-01-26T12:50:37Z,"FIx SFTTrainer bugs on TRL main (#1276)

* Update sft_trainer.py

* Update trl/trainer/sft_trainer.py"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2024-01-26T09:38:07Z,"Only load data on main process (#1255)

* fix: only load data on main process

* define is_main_process once

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* avoid re-initializing PartialState on train dataset check

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* avoid re-initializing PartialState on eval dataset check

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* process dataset on main first to take advantage of caching

* fix typo in docs

* use decorator to manage state

* Revert ""fix typo in docs""

This reverts commit 0880a188812a698f7106853245ce1ba96a036831.

* Revert ""Revert ""fix typo in docs""""

This reverts commit ff7ee33fbeedcd0032b728d86a17cfcb10e43f9b.

* Revert ""use decorator to manage state""

This reverts commit 7ac7a45949f621941fedc522f0d2ca7b29367c3a.

* use is_local_main_process instead of is_main_process

* fix: use context manager instead of attribute

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/sft_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2024-01-23T13:46:13Z,"Fix typo in extra_columns variable name (#1269)

Co-authored-by: Otto Laitila <otto.laitila@op.fi>"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2024-01-17T14:16:07Z,Update sft_trainer.py (#1241)
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2024-01-17T13:45:22Z,"[`core` / SFTTrainer] Fix breaking change (#1229)

* fix breaking change

* revert

* fix

* final fix

* fix

* fix tests"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2024-01-12T07:05:32Z,"Add support for ChatML dataset format in (#1208)

* Add support for ChatML dataset format in
SFTTrainer

* fix formatting

* fix tests

* more comment

* fix intent

* fix doc string

* Update dataset_formatting.py

* Update dataset_formatting.py

* add documentation

* Update sft_trainer.mdx

* add leonardos comment and more tests

* added more tests and fixed batching

* style

* comment in"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2024-01-09T09:20:50Z,"Revert ""Address issue #1122 (#1174)"" (#1205)

This reverts commit d57d0f9ca46a63d370b91791352edda0154576f5."
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2024-01-08T05:09:10Z,SFTTrainer: follow args.remove_unused_columns (#1188)
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2024-01-08T04:43:34Z,"Address issue #1122 (#1174)

* Address issue #1122

    Issue [#1122](https://github.com/huggingface/trl/issues/1122)
    takes care of an inconsistency between `_prepare_packed_dataloader`
    and `_prepare_non_packed_dataloader`

* made attention_mask field in ConstantLengthDataset a tensor"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2024-01-04T15:33:53Z,"Update sft_trainer.py (#1162)

Fix spelling mistakes in argument description for trl -> SFT Trainer"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-12-26T15:39:10Z,"[`xxxTrainer`] Add unsloth tag (#1130)

* add unsloth tag

* add it on all trainers

* few changes

* add in docs

* revert

* final commit"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-12-22T13:52:16Z,"[`xxxTrainer`] multi-tags support for tagging (#1133)

* multi-tags support for tagging

* oops"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-12-21T16:04:18Z,"[`xxxTrainer`] Add tags to all trainers in TRL (#1120)

* add tags to sfttrainer

* extend it to other trainers

* add for ddpo"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-12-20T12:35:56Z,fix gradient checkpointing when using PEFT (#1118)
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-12-20T10:28:50Z,"Make prepending of bos token configurable. (#1114)

* make prepending of bos token configurable.

* address comments

* fix bug

Co-Authored-By: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/sft_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-12-19T16:43:25Z,"`peft_module_casting_to_bf16` util method, `append_concat_token` flag, remove callback `PeftSavingCallback` (#1110)

* SFT Trainer enhancements

* remove the callback `PeftSavingCallback`

* bump the version of transformers to `4.31.0`

* remove `PeftSavingCallback` from all places."
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-12-06T19:26:24Z,"enable multiple eval datasets (#1052)

* enable multiple eval datasets

* added test

* try to avoid infinite computation

* make sure eval set is not infinite

* downsizing the test"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-12-06T18:02:09Z,"[`SFTTrainer`] Fix Trainer when args is None (#1064)

* fix sfttrainer when args is None

* oops"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-12-06T16:46:36Z,update doc for the computer_metrics argument of SFTTrainer (#1062)
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-12-04T12:13:18Z,"[SFT Trainer] precompute packed iterable into a dataset (#979)

* precompute packed iterable into a dataset

* add generator function

* fix typo

* fix style

* fix test

* fix style

* add test

* minor refactor

* fix test

* Apply suggestions from code review

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* style

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>
Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>
Co-authored-by: younesbelkada <younesbelkada@gmail.com>"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-11-20T14:57:46Z,"Adds `requires_grad` to input for non-quantized peft models (#1006)

* Update sft_trainer.py

* style

* add tests"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-11-08T10:11:39Z,fix peft config typehint (#967)
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-11-06T08:48:18Z,"Adds model kwargs to SFT and DPO trainers (#951)

* adds model kwargs to SFT and DPO trainers

* adds checks for model_kwarg passing when model is not str

* changed warning to ValueError

* renames model_kwargs to model_init_kwargs

* corrects argument names in"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-11-05T07:31:47Z,"Fix unwrapping peft models (#948)

* First unwrap the model and then process the input embeddings

* Changed base_model to base_model.model to stay consistent with peft model abstractions"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-10-31T17:50:17Z,"[`core` / `DDP`] Fix RM trainer + DDP + quantization + propagate `gradient_checkpointing_kwargs` in SFT & DPO (#912)

* make use of forward hooks

* correctly delete attributes

* fix RM DPP issues

* revert unneeded changes

* more fixes

* fix diff

* fix

* propagate to SFT

* Update examples/scripts/reward_modeling.py

* propagate the fix on DPO trainer

* add to example scripts

* trigger CI"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-10-31T15:04:09Z,"[`SFTTrainer`] Make sure to not conflict between `transformers` and TRL implementation (#933)

* standardize neftune

* up

* fix again"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-10-30T10:17:14Z,"Generalize NEFTune for FSDP, DDP, ... (#924)

* Update sft_trainer.py

* quality"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-10-24T12:18:44Z,"[`NEFTune`] Make use of forward hooks instead (#889)

* make use of forward hooks

* correctly delete attributes

* address suggestions"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-10-18T21:45:38Z,"fix peft_config type (#883)

Co-authored-by: wanglei.w <wanglei.w@bytedance.com>"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-10-17T04:58:05Z,"[`SFTTrainer`] Adds NEFTune into `SFTTrainer` (#871)

* v1 neftune

* docstring

* add doc + fix nit

* add more docs

* Apply suggestions from code review

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

---------

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-09-12T08:56:10Z,"update to `prepare_model_for_kbit_training` (#728)

* update to `prepare_model_for_kbit_training`

from deprecated `prepare_model_for_int8_training`
and add `use_gradient_checkpointing=args.gradient_checkpointing` to
automatically follow the gradient checkpointing choice

is also the workaround for #694

* workaround for gradient checkpointing issue

calling model.gradient_checkpointing_enable() twice causes issues
this workaround calls it in prepare_model_for_kbit_training and then
changes the arg to false to make sure it isn't called again in
huggingface trainer inner loop

also changes stack_llama_2 sft trainer to use correct device map for ddp
training so that you can test this issue"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-09-06T08:24:55Z,check correctly for condition (#668)
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-08-09T15:48:25Z,"Move repo (#628)

* update actions

* update references"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-07-22T08:53:16Z,"[`SFTTrainer`] Add warning for wrong padding_side (#550)

* add warning for wrong padding_side

* add warning

* revert

* oops"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-07-20T13:41:48Z,"ADD: num_proc to SFTTrainer (#547)

* ADD: num_proc to SFTTrainer

* make precommit

* Update trl/trainer/sft_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/sft_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/sft_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update trl/trainer/sft_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* add batch_size

* Update trl/trainer/sft_trainer.py

Co-authored-by: Tom Aarsen <37621491+tomaarsen@users.noreply.github.com>

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>
Co-authored-by: Tom Aarsen <37621491+tomaarsen@users.noreply.github.com>"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-07-12T13:25:17Z,"[`SFTTrainer`] Fix the sequence length check of `SFTTrainer` (#512)

* fix the sequence length check of `SFTTrainer`

* forward contrib credits from initial contribution

* forward contrib credits from initial contribution

* final comments

---------

Co-authored-by: mrm8488 <mrm8488@users.noreply.github.com>
Co-authored-by: BramVanroy <BramVanroy@users.noreply.github.com>"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-06-27T22:44:15Z,"Update sft_trainer.py (#474)

* Update sft_trainer.py

Allows the user to give their own peft model arg. https://github.com/lvwerra/trl/issues/473

* cleaner"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-06-24T06:43:03Z,"Debug the tortuous logic in `_prepare_dataset` function (#464)

* Debug the tortuous logic in `_prepare_dataset` function

There are two issues with the previous `_prepare_dataset` function.

1. Tortuous and burdensome logic: the `is_already_dataset` variable is confusing and not helpful. So, remove it.
2. The comments and the logics do not match. 

For instance, in the previous version, the comments said ""check if torch dataset ... and do nothing"". However, when ""dataset"" is a torch.utils.data.Dataset and `packing = True`? It will still move into the _prepare_non_packed_dataloader(...) function call. 

The corrected version will do nothing if the dataset is already a torch dataloader/dataset/ConstantLengthDataset.

* Lint: sft_trainer.py

* Lint empty line"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-06-22T09:19:45Z,"Multi adapter RL (MARL) - a single model for RM & Value Head (#373)

* fix doc

* adapt from suggestions

* working v1 multiple adapters

* style

* style && quality

* oops

* docs

* add tests and docs

* add RM script

* Apply suggestions from code review

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update docs/source/0_abstraction_rl.mdx

* Apply suggestions from code review

* Update docs/source/0_abstraction_rl.mdx

* add 4bit

* replace with `reward_adapter`

* explain break

* simple comment

* fix llama tokenizer

* fixes

* fixes

* rename

* quality

* rm unneeded file

* add disclaimer

---------

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-06-22T08:12:55Z,"[`ConstantLengthDataset`] Fix packed dataset issue (#452)

* fix packed dataset issue

* Apply suggestions from code review

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>

* address

* more docs

* trigger CI

* fix failing CI

---------

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-06-20T15:51:23Z,"[`SFTTrainer`] Introducing `DataCollatorForCompletionOnlyLM` (#445)

* v1 of alpaca datacollator

* make sure to match the response tokens

* add test

* add it in main init

* add check

* adapt test

---------

Co-authored-by: Costa Huang <costa.huang@outlook.com>"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-06-16T16:51:20Z,"[`SFTTrainer`] Fix non packed dataset (#444)

* fix non packed dataset

* fixing tests and documentation

* Update docs/source/sft_trainer.mdx"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-06-16T11:55:47Z,fix packing issue (#442)
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-06-02T08:49:41Z,fix 4 bit SFT (#396)
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-05-15T14:26:27Z,"[`docs`] fix SFT doc (#367)

* fix doc

* adapt from suggestions"
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-05-03T10:53:32Z,fix sft issues (#336)
github.com/huggingface/trl,trl/trainer/sft_trainer.py,2023-05-03T08:42:01Z,"[`core`] officially support SFT (Supervised Finetuning)  (#323)

* add v1

* revert

* correct filename

* add tests and final tweaks

* fix tests

* adapt from offline suggestions

* Update trl/trainer/sft_trainer.py

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>

* fixes

* remove warning

* multiple fixes

* fixes

* fix

* final fixes

* final fix

* more clarification

* Apply suggestions from code review

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* add test

* add arg

* add callback instructions

* add formatting_prompts_func

* try docs

* add CLD

* fix docstrings

* format

* Update docs/source/sft_trainer.mdx

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>

* remove `prepare_in_int8_kwargs`

* change `return_overflowing_tokens`

* add warnings

* address comments

* revert pretrained kwargs

* quality

* fix sft script

---------

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>
Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>"
github.com/Facico/Chinese-Vicuna,utils.py,2023-06-01T09:05:42Z,multi GPUs inference
github.com/Facico/Chinese-Vicuna,utils.py,2023-05-10T08:32:19Z,mv shell script and update utils.py
github.com/Facico/Chinese-Vicuna,utils.py,2023-04-27T03:42:18Z,Update utils.py
github.com/Facico/Chinese-Vicuna,utils.py,2023-04-04T11:42:21Z,update
github.com/intel-analytics/BigDL,python/llm/src/bigdl/llm/transformers/qlora.py,2024-01-17T07:51:38Z,"Support fast rope for training (#9745)

* init

* init

* fix style

* add test and fix

* address comment

* update

* merge upstream main"
github.com/intel-analytics/BigDL,python/llm/src/bigdl/llm/transformers/qlora.py,2023-12-25T06:49:30Z,"LLM: fix `BF16Linear` related training & inference issue (#9755)

* fix bf16 related issue

* fix

* update based on comment & add arc lora script

* update readme

* update based on comment

* update based on comment

* update

* force to bf16

* fix style

* move check input dtype into function

* update convert

* meet code review

* meet code review

* update merged model to support new training_mode api

* fix typo"
github.com/intel-analytics/BigDL,python/llm/src/bigdl/llm/transformers/qlora.py,2023-12-22T03:05:39Z,"LLM: bigdl-llm lora support & lora example (#9740)

* lora support and single card example

* support multi-card, refactor code

* fix model id and style

* remove torch patch, add two new class for bf16, update example

* fix style

* change to training_mode

* small fix

* add more info in help

* fixstyle, update readme

* fix ut

* fix ut

* Handling compatibility issues with default LoraConfig"
github.com/intel-analytics/BigDL,python/llm/src/bigdl/llm/transformers/qlora.py,2023-12-08T08:13:03Z,"Support peft LoraConfig (#9636)

* support peft loraconfig

* use testcase to test

* fix style

* meet comments"
github.com/intel-analytics/BigDL,python/llm/src/bigdl/llm/transformers/qlora.py,2023-12-07T08:32:02Z,"LLM: fix unlora module in qlora finetune (#9621)

* fix unlora module

* split train and inference"
github.com/intel-analytics/BigDL,python/llm/src/bigdl/llm/transformers/qlora.py,2023-12-06T07:36:21Z,"QALora example (#9551)

* Support qa-lora

* init

* update

* update

* update

* update

* update

* update merge

* update

* fix style & update scripts

* update

* address comments

* fix typo

* fix typo

---------

Co-authored-by: Yang Wang <yang3.wang@intel.com>"
github.com/intel-analytics/BigDL,python/llm/src/bigdl/llm/transformers/qlora.py,2023-11-29T07:16:18Z,LLM: fix loss error on Arc (#9550)
github.com/intel-analytics/BigDL,python/llm/src/bigdl/llm/transformers/qlora.py,2023-11-21T09:08:36Z,"LLM: GPU QLoRA update to bf16 to accelerate gradient checkpointing (#9499)

* update to bf16 to accelerate gradient checkpoint

* add utils and fix ut"
github.com/intel-analytics/BigDL,python/llm/src/bigdl/llm/transformers/qlora.py,2023-11-08T09:46:49Z,"LLM: optimize QLoRA by updating lora convert logic (#9372)

* update convert logic of qlora

* update

* refactor and further improve performance

* fix style

* meet code review"
github.com/intel-analytics/BigDL,python/llm/src/bigdl/llm/transformers/qlora.py,2023-10-17T03:47:19Z,"Support XPU DDP training and autocast for LowBitMatmul (#9167)

* support autocast in low bit matmul

* Support XPU DDP training

* fix  amp"
github.com/intel-analytics/BigDL,python/llm/src/bigdl/llm/transformers/qlora.py,2023-10-15T04:28:59Z,"Fixes for xpu Bf16 training (#9156)

* Support bf16 training

* Use a stable transformer version

* remove env

* fix style"
github.com/intel-analytics/BigDL,python/llm/src/bigdl/llm/transformers/qlora.py,2023-10-05T04:18:52Z,"add export merged model example (#9018)

* add export merged model example

* add sources

* add script

* fix style"
github.com/intel-analytics/BigDL,python/llm/src/bigdl/llm/transformers/qlora.py,2023-09-19T17:15:44Z,"Experiment XPU QLora Finetuning (#8937)

* Support xpu finetuning

* support xpu finetuning

* fix style

* fix style

* fix style

* refine example

* add readme

* refine readme

* refine api

* fix fp16

* fix example

* refactor

* fix style

* fix compute type

* add qlora

* refine training args

* fix example

* fix style

* fast path forinference

* address comments

* refine readme

* revert lint"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2024-03-03T12:35:22Z,"Vllm update DP+TP (#1508)

* use `@ray.remote` with distributed vLLM

* update versions

* bugfix

* unpin vllm

* fix pre-commit

* added version assertion error

* Revert ""added version assertion error""

This reverts commit 8041e9b78e95eea9f4f4d0dc260115ba8698e9cc.

* added version assertion for DP

* expand DP note

* add warning

* nit

* pin vllm

* fix typos"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2024-03-01T16:04:44Z,always include EOS token in stopsequences if possible (#1480)
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2024-02-27T23:31:10Z,"Fix AttributeError in huggingface.py When 'model_type' is Missing (#1489)

* model_type attribute error

Getting attribute error when using a model without a 'model_type'

* fix w/ and w/out the 'model_type' specification

* use getattr(), also fix other config.model_type reference

* Update huggingface.py

---------

Co-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2024-02-27T14:03:56Z,"Refactor `evaluater.evaluate` (#1441)

* change `all_gather` to `gather`

* add TaskOutput utility class

* Add FilterResults class and refactor task handling.

* Rename `key` to `filter_key` for clarity

* Add `print_writeout` function in utils.py

* Add function to calculate limit size.

* Add doc_iterator method to Task class

* Refactor `doc_iterator` and cleanup in Task class

* remove superfluous bits

* change `all_gather` to `gather`

* bugfix

* bugfix

* fix `gather`

* Refactor `gather` loop

* Refactor aggregate metrics calculation

* Refactor and simplify aggregate metrics calculation
Removed unused code

* Simplify metrics calculation and remove unused code.

* simplify the metrics calculation in `utils.py` and `evaluator.py`.

* Fix group metric

* change evaluate to hf_evaluate

* change evaluate to hf_evaluate

* add docs

* add docs

* nits

* make isslice keyword only

* nit

* add todo

* nit

* nit

* nit: swap order samples_metrics tuple

* move instance sorting outside loop

* nit

* nit

* Add __repr__ for ConfigurableTask

* nit

* nit

* Revert ""nit""

This reverts commit dab8d9977a643752a17f840fd8cf7e4b107df28f.

* fix some logging

* nit

* fix `predict_only` bug. thanks to `@LSinev`!

* change `print_tasks` to `prepare_print_tasks`

* nits

* move eval utils

* move eval utils

* nit

* add comment

* added tqdm descriptions

* Update lm_eval/evaluator_utils.py

Co-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>

* fix mgsm bug

* nit

* fix `build_all_requests`

* pre-commit

* add ceil to limit

---------

Co-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2024-02-26T14:21:35Z,"Revert ""setting trust_remote_code (#1467)"" (#1474)

This reverts commit c1145dfdd8f9ddfceec0410d328ac1c590738a5d."
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2024-02-26T14:02:21Z,"Add Gemma support (Add flag to control BOS token usage) (#1465)

* add add_bos_token to HFLM

* add BOS token flag to other local model classes

---------

Co-authored-by: Lintang Sutawika <lintang@eleuther.ai>"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2024-02-26T13:05:05Z,setting trust_remote_code (#1467)
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2024-02-22T00:39:42Z,"Add TemplateLM boilerplate LM class (#1279)

* loglikelihood refactor using template lm

* linter

* fix whitespace in target + prompt for CoT gsm8k (#1275)

* Make `parallelize=True` vs. `accelerate launch` distinction clearer in docs (#1261)

* Make parallelize=True distinction clearer in documentation.

* run linter

* Allow parameter edits for registered tasks when listed in a benchmark (#1273)

* benchmark yamls allow minor edits of already registered tasks

* add documentation

* removed print

* Fix data-parallel evaluation with quantized models (#1270)

* add WIP device_map overrides

* update handling outside of accelerate launcher

* change .to(device) log to debug level

* run linter

* Rework documentation for explaining local dataset (#1284)

* rewor documentation for explaining local dataset

* fix typo

* Update new_task_guide.md

* Re-add citation

It looks like Google Scholar has [already noticed](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C9&authuser=2&q=%22A+framework+for+few-shot+language+model+evaluation%2C+12+2023%22&btnG=) the updated citation block so let's add it back in.

* Update CITATION.bib (#1285)

Bumping CITATION.bib to match re-adding the citation in readme. 

cc @StellaAthena

* Update nq_open.yaml (#1289)

* Update README.md with custom integration doc (#1298)

* Update README.md

* punctuation

---------

Co-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>

* Update nq_open.yaml (#1305)

* Update nq_open.yaml

change regex

* Bump NQ version

---------

Co-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>

* Update task_guide.md (#1306)

* Update pyproject.toml (#1312)

* Fix polemo2_in.yaml config name (#1313)

* Update pyproject.toml (#1314)

* Fix group register (#1315)

* tuple should be considered as well

* set option to keep callable as callable

* Update task_guide.md (#1316)

* Update polemo2_in.yaml (#1318)

* don't pass extra kwargs to mamba any more (#1328)

* Fix Issue regarding stderr (#1327)

* add fix fordeciding if stderr is N/A or not

* process N/A

* Add `local-completions` support using OpenAI interface (#1277)

* Add `local-completions` support using OpenAI interface

* Refactor oa_completion

* Address tokenizer comments and change request chunks to batch size

* Add warning message for tiktoken backend

* fix formatting

* fix whitespace

* Update README.md

---------

Co-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>

* fallback to classname when LM doesnt have config (#1334)

* fix a trailing whitespace that breaks a lint job (#1335)

* skip ""benchmarks"" in changed_tasks (#1336)

* Update migrated HF dataset paths (#1332)

* Update arc_easy.yaml

* Update flan_cot.yaml

* update HF dataset path

* Update freeform.yaml

* Update flan_cot.yaml

---------

Co-authored-by: Lintang Sutawika <lintang@eleuther.ai>

* Don't use `get_task_dict()` in task registration / initialization (#1331)

* don't use get_task_dict() as a helper, it will download the dataset!

* pre-commit

* Update README.md

---------

Co-authored-by: lintangsutawika <lintang@eleuther.ai>

* manage default (greedy) gen_kwargs in vllm (#1341)

* manage default (greedy) gen_kwargs in vllm better

* mirror HF `do_sample`

* just need to set temp=0 for greedy

* modified default gen_kwargs to work better with CLI; changed prompt_logprobs=1 (#1345)

* update links to task_guide.md (#1348)

* `Filter` docs not offset by `doc_id`  (#1349)

* get `doc` from instance

* acceletate bugfix: get ground doc from instance

* convert filter to `process_result`

* get docs from instances in `FilterEnsemble`

* rename

* nit

* better looping

* fix typehint

* Add FAQ on `lm_eval.tasks.initialize_tasks()` to README (#1330)

* Update README.md

* [!Tip]

* Refix issue regarding stderr (#1357)

* Add causalLM OpenVino models (#1290)

* added intel optimum

* added intel optimum in readme

* modified intel optimum

* modified intel optimum

* modified intel optimum

* modified install optimum

* modified path of IR file

* added openvino_device

* added openvino_device2

* changed optimum-causal to openvino-causal

* Update README.md

* Update README.md

* remove `lm_eval.base` import

* update openvino-causal -> openvino ; pass device through super().__init__()

* Update README.md

* Add optimum to tests dependencies

* apply pre-commit

* fix so tests pass

---------

Co-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>
Co-authored-by: haileyschoelkopf <hailey@eleuther.ai>

* Apply some best practices and guideline recommendations to code (#1363)

* raise Exception, not a string

Additional info https://peps.python.org/pep-0352/#exception-hierarchy-changes
https://docs.python.org/3.8/tutorial/errors.html#raising-exceptions

* Apply PEP8 recommendation to prefer isinstance

""Object type comparisons should always use isinstance() instead of comparing types directly""
https://peps.python.org/pep-0008/

* Remove dangerous default mutable values in arguments

https://pylint.readthedocs.io/en/stable/user_guide/messages/warning/dangerous-default-value.html

* Format logging messages with fstring (not with format)

Additional info
https://pylint.readthedocs.io/en/stable/user_guide/messages/warning/logging-format-interpolation.html
There are also discussions about the speed of formatting while logging or some unintended code executions
https://github.com/pylint-dev/pylint/issues/2395
https://stackoverflow.com/a/54368109
but at least one format (fstring one) will be used throughout the project

* Specify utf-8 encoding for `open` explicitly

If not specified, it may be supposed differently in different environments, OSes, and Python versions. See
https://peps.python.org/pep-0597/
https://docs.python.org/3.11/library/locale.html#locale.getencoding
https://docs.python.org/3.10/library/os.html#utf8-mode
https://pylint.readthedocs.io/en/stable/user_guide/messages/warning/unspecified-encoding.html

Helps also if some code from English language tasks is taken as inspiration for tasks in non-English languages.

* Use inline-ignoring comments to pass pre-commit instead of identity process

https://flake8.pycqa.org/en/3.0.1/user/ignoring-errors.html#in-line-ignoring-errors
https://www.flake8rules.com/rules/F841.html

flake8 comments are supported by ruff: https://docs.astral.sh/ruff/linter/#error-suppression

* serialize callable functions in config (#1367)

* delay filter init; remove `*args` (#1369)

* delay filter init; remove `*args`

* bugfix

* optimize

* type hint

* Fix unintuitive `--gen_kwargs` behavior (#1329)

* don't override do_sample if no value for it is passed

* Update gen_kwargs override condition

* Update huggingface.py

* Update huggingface.py

* run linters

* silence an erroneous warning

* Publish to pypi (#1194)

* publish to pypi

* lint

* Update publish.yml

* minor

* Make dependencies compatible with PyPI (#1378)

* make deps not point to github urls

* formatting

* try making PyPI only run on tag pushes

* Add support for RWKV models with World tokenizer (#1374)

* Add support for RWKV models with World tokenizer

The RWKV line of model with the World tokenizer, does not allow the padding token to be configured, and has its value preset as 0

This however fails all the ""if set"" checks, and would cause the tokenizer to crash.

A tokenizer class name check was added, in addition to a model type check, as there exists RWKV models which uses the neox tokenizers

* Update huggingface.py

Genericized so that this supports any RWKVWorld tokenizer, and added a fall-back for if the HF implementation name changes.

* Comply with formatting guidelines

* fix format

---------

Co-authored-by: Stella Biderman <stellabiderman@gmail.com>
Co-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>

* add bypass metric (#1156)

* add bypass metric

* fixed `bypass` metric.

* add task attributes if predict_only

* add `predict_only` checks

* add docs

* added `overide_metric`, `override_config` to `Task`

* nits

* nit

* changed --predict_only to generations; nits

* nits

* nits

* change gen_kwargs warning

* add note about `--predict_only` in README.md

* added `predict_only`

* move table to bottom

* nit

* change null aggregation to bypass (conflict)

* bugfix; default `temp=0.0`

* typo

* loglikelihood refactor using template lm

* lint

* code review

* neuron optimum

* Mention TemplateLM in model_guide.md

* Update lm_eval/api/model.py

* fix linter

* fix format

* fix format

* fix format

---------

Co-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>
Co-authored-by: Lintang Sutawika <lintang@eleuther.ai>
Co-authored-by: Stella Biderman <stellabiderman@gmail.com>
Co-authored-by: Mark Saroufim <marksaroufim@meta.com>
Co-authored-by: Hannibal046 <38466901+Hannibal046@users.noreply.github.com>
Co-authored-by: Danielle Pintz <38207072+daniellepintz@users.noreply.github.com>
Co-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>
Co-authored-by: kwrobel.eth <djstrong@gmail.com>
Co-authored-by: Michael Goin <michael@neuralmagic.com>
Co-authored-by: Brian Vaughan <nairbv@users.noreply.github.com>
Co-authored-by: Baber Abbasi <92168766+baberabb@users.noreply.github.com>
Co-authored-by: thnkinbtfly <70014488+thnkinbtfly@users.noreply.github.com>
Co-authored-by: NoushNabi <33136068+NoushNabi@users.noreply.github.com>
Co-authored-by: haileyschoelkopf <hailey@eleuther.ai>
Co-authored-by: LSinev <LSinev@users.noreply.github.com>
Co-authored-by: Eugene Cheah <PicoCreator@users.noreply.github.com>"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2024-02-20T14:57:01Z,"Group reqs by context (#1425)

* add key lookup for same contexts

* nit

* appease pre-commit

* nit

* use `expand` (in-place view) rather than `repeat`

* try mixed grouping

* add docs.

* nit

* nit

* nits

* fix tests

* Move greedy_tokens calculation out of cache loop

* nit

* nits

* add test

* nits

* fix name conflict

* fix name conflict

* chunk tensor

* move Collator

* nits/docstring

* fixup

* fixup

* group contexts only for decoders

* pre-commit

* fix `generate_until` test

* fix `generate_until` test

* Update lm_eval/models/huggingface.py

Co-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>

* add docs

* nit

* add docs

* add docs

* add 'logits_cache' arg

* bugfix

---------

Co-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2024-02-14T18:21:03Z,Refactor utilities into a separate model utils file. (#1429)
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2024-02-10T20:54:36Z,"Fix watchdog timeout (#1404)

* Fix watchdog timeout

* Pre-commit fix

* Timedelta"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2024-02-10T15:03:50Z,"Fixes https://github.com/EleutherAI/lm-evaluation-harness/issues/1416 (#1418)

* Fixes https://github.com/EleutherAI/lm-evaluation-harness/issues/1416

Sets `do_sample = False` if `temperature == 0.0` and `do_sample = None`

* Update huggingface.py

* Update huggingface.py

making linter happy"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2024-02-07T13:48:47Z,"`batch_size` with `auto` defaults to 1 if `No executable batch size found` (#1405)

Fixes https://github.com/EleutherAI/lm-evaluation-harness/issues/1323"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2024-02-01T09:13:29Z,"Hf: minor egde cases (#1380)

* edge cases where variable might not be assigned.

* type hint"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2024-01-31T22:33:14Z,"add bypass metric (#1156)

* add bypass metric

* fixed `bypass` metric.

* add task attributes if predict_only

* add `predict_only` checks

* add docs

* added `overide_metric`, `override_config` to `Task`

* nits

* nit

* changed --predict_only to generations; nits

* nits

* nits

* change gen_kwargs warning

* add note about `--predict_only` in README.md

* added `predict_only`

* move table to bottom

* nit

* change null aggregation to bypass (conflict)

* bugfix; default `temp=0.0`

* typo"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2024-01-31T20:59:31Z,"Add support for RWKV models with World tokenizer (#1374)

* Add support for RWKV models with World tokenizer

The RWKV line of model with the World tokenizer, does not allow the padding token to be configured, and has its value preset as 0

This however fails all the ""if set"" checks, and would cause the tokenizer to crash.

A tokenizer class name check was added, in addition to a model type check, as there exists RWKV models which uses the neox tokenizers

* Update huggingface.py

Genericized so that this supports any RWKVWorld tokenizer, and added a fall-back for if the HF implementation name changes.

* Comply with formatting guidelines

* fix format

---------

Co-authored-by: Stella Biderman <stellabiderman@gmail.com>
Co-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2024-01-31T14:19:59Z,"Fix unintuitive `--gen_kwargs` behavior (#1329)

* don't override do_sample if no value for it is passed

* Update gen_kwargs override condition

* Update huggingface.py

* Update huggingface.py

* run linters

* silence an erroneous warning"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2024-01-28T20:44:08Z,"Apply some best practices and guideline recommendations to code (#1363)

* raise Exception, not a string

Additional info https://peps.python.org/pep-0352/#exception-hierarchy-changes
https://docs.python.org/3.8/tutorial/errors.html#raising-exceptions

* Apply PEP8 recommendation to prefer isinstance

""Object type comparisons should always use isinstance() instead of comparing types directly""
https://peps.python.org/pep-0008/

* Remove dangerous default mutable values in arguments

https://pylint.readthedocs.io/en/stable/user_guide/messages/warning/dangerous-default-value.html

* Format logging messages with fstring (not with format)

Additional info
https://pylint.readthedocs.io/en/stable/user_guide/messages/warning/logging-format-interpolation.html
There are also discussions about the speed of formatting while logging or some unintended code executions
https://github.com/pylint-dev/pylint/issues/2395
https://stackoverflow.com/a/54368109
but at least one format (fstring one) will be used throughout the project

* Specify utf-8 encoding for `open` explicitly

If not specified, it may be supposed differently in different environments, OSes, and Python versions. See
https://peps.python.org/pep-0597/
https://docs.python.org/3.11/library/locale.html#locale.getencoding
https://docs.python.org/3.10/library/os.html#utf8-mode
https://pylint.readthedocs.io/en/stable/user_guide/messages/warning/unspecified-encoding.html

Helps also if some code from English language tasks is taken as inspiration for tasks in non-English languages.

* Use inline-ignoring comments to pass pre-commit instead of identity process

https://flake8.pycqa.org/en/3.0.1/user/ignoring-errors.html#in-line-ignoring-errors
https://www.flake8rules.com/rules/F841.html

flake8 comments are supported by ruff: https://docs.astral.sh/ruff/linter/#error-suppression"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2024-01-26T20:43:45Z,"Add causalLM OpenVino models (#1290)

* added intel optimum

* added intel optimum in readme

* modified intel optimum

* modified intel optimum

* modified intel optimum

* modified install optimum

* modified path of IR file

* added openvino_device

* added openvino_device2

* changed optimum-causal to openvino-causal

* Update README.md

* Update README.md

* remove `lm_eval.base` import

* update openvino-causal -> openvino ; pass device through super().__init__()

* Update README.md

* Add optimum to tests dependencies

* apply pre-commit

* fix so tests pass

---------

Co-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>
Co-authored-by: haileyschoelkopf <hailey@eleuther.ai>"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2024-01-15T15:02:35Z,"Fix data-parallel evaluation with quantized models (#1270)

* add WIP device_map overrides

* update handling outside of accelerate launcher

* change .to(device) log to debug level

* run linter"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2024-01-11T15:20:02Z,"Fix bug in multi-token Stop Sequences (#1268)

* fix incorrect lookback protections

* bump generate_until task versions"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-12-27T13:08:03Z,fix unbounded local variable (#1218)
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-12-23T17:58:50Z,"Consolidate batching (#1197)

* refactor dataloader

* cleanup + add docs

* change arg

* renamed Collator and added testing

* parametrized test for Collator

* appease pre-commit

* added edge case batch 0 (no batching)

* fix typos

---------

Co-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-12-22T19:37:46Z,Fixes https://github.com/EleutherAI/lm-evaluation-harness/issues/437 (#1180)
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-12-20T14:14:46Z,"Switch Linting to `ruff` (#1166)

* add ruff and isort. remove black and flake8

* remove unnecessary dependencies

* remove dependency from table

* change order

* ran ruff

* check 3.9

* exclude evaluator

* update CI workflow

* use ruff config in pyproject.toml

* test

* add isort rules to ruff

* sort imports

* import `make_table`

* try stages for no-commit-to-branch

* turn on mypy for pre-commit

* test

* test

* test

* change no-commit-to-branch to default

* nits

* fixed dependency"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-12-19T16:23:00Z,"self.device in huggingface.py line 210 treated as torch.device but might be a string (#1172)

* self.device in huggingface.py line 210

In huggingface.py line 210, self.device is str and does not have a ""type"" attribute

* Update huggingface.py

This handles both the case where `self.device` is a `torch.device` and a string

* Update huggingface.py

---------

Co-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-12-19T13:11:13Z,generalize qwen pad token fix (#1146)
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-12-15T12:08:56Z,place device onto `mps` (#1133)
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-12-14T14:02:35Z,"Refactor `hf` modeling code (#1096)

* modularize HFLM code

* pass through extra kwargs to AutoModel.from_pretrained call

* remove explicit model_kwargs

* rename gptq -> autogptq

* fix tokenizer pad token errors

* ensure model always respects device_map and autogptq's selected devices

* add a _get_config helper fn"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-11-29T06:58:31Z,change torch req for mps
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-11-28T14:20:44Z,"Merge pull request #1024 from EleutherAI/fix-mbart

[Refactor] Use correct HF model type for MBart-like models"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-11-27T17:11:05Z,"Merge pull request #1011 from baberabb/big-refactor_vllm

[Refactor] vllm support"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-11-26T14:59:10Z,use Seq2Seq backend where either can be loaded from HF
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-11-21T16:28:52Z,update multi-token stopsequence handling
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-11-20T15:56:30Z,add typehints
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-11-17T06:44:14Z,edits and format
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-11-10T15:09:28Z,added initialize_task and updated where eval_logger is imported from
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-11-02T18:34:26Z,cleanup hf tqdm
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-11-01T15:43:13Z,fix tqdm total  for hf model
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-10-19T14:42:02Z,fix issue with default metrics and aggregation functions
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-10-19T12:07:31Z,"Merge pull request #916 from jasonkrone/big-refactor

Fix 'tqdm' object is not subscriptable"" error in huggingface.py when batch size is auto"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-10-17T14:42:37Z,change all mentions of `greedy_until` to `generate_until`
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-10-13T17:46:10Z,"Fix ""TypeError: 'tqdm' object is not subscriptable"" error that occurs
in hugging face model loglikelihood_tokens and greedy_util functions
when batch-size is set to auto"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-10-11T10:20:42Z,check with pre-commit
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-10-11T03:54:55Z,finished test code
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-10-11T03:36:31Z,add _batch_scheduler in greedy_until
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-09-25T14:43:39Z,merged latest
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-09-21T13:09:09Z,Fix positional arguments in HF model generate
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-09-21T11:03:05Z,merged with latest big-refactor
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-09-13T19:37:30Z,Update device list and dtype detection for MPS
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-09-06T15:22:40Z,Merge branch 'big-refactor' into flan-benchmark
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-09-05T16:05:39Z,format for pre-commit
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-09-05T15:44:23Z,"Merge pull request #833 from EleutherAI/fix-ppl

[Refactor] Fix wikitext task"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-09-05T15:43:22Z,pre-commit
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-09-05T11:56:57Z,Merge branch 'big-refactor' into mypy
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-09-04T18:50:00Z,"Merge pull request #812 from fattorib/bump-accelerate

[Refactor] Bump min accelerate version and update documentation"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-09-04T18:48:05Z,placate precommit
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-09-04T10:41:57Z,modified changes to fix loglikelihood prediction for seq2seq
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-08-26T14:31:31Z,fix FSDP error with .prepare_model()
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-08-25T21:46:34Z,"Add suggestions from autotyping

This adds a bunch of simple annotations suggested by https://github.com/JelleZijlstra/autotyping."
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-08-25T04:27:34Z,reformat
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-08-22T15:22:24Z,added truncation option
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-08-21T16:33:54Z,Merge branch 'big-refactor' of https://github.com/EleutherAI/lm-evaluation-harness into superglue
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-08-11T00:44:27Z,placate pre-commit
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-08-10T21:14:36Z,Use evaluation_mode=True for accelerate to prevent OOM
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-08-10T17:00:24Z,Merge branch 'big-refactor' of https://github.com/EleutherAI/lm-evaluation-harness into superglue
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-08-07T13:58:11Z,making t5 version of superglue prompt
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-08-04T10:04:15Z,pre-commit format
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-08-04T06:42:07Z,"Merge pull request #673 from fattorib/big-refactor-autobatching

[Refactor] Port over Autobatching"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-08-03T13:24:38Z,"Update huggingface.py

`max_length` was misdefined as a tuple."
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-08-02T15:11:43Z,fix to add a case for if a user add `max_length` to generation_kwargs
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-08-01T02:33:42Z,merge conflicts
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-07-27T17:45:51Z,Merge remote-tracking branch 'upstream/big-refactor' into big-refactor-autobatching
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-07-27T17:45:22Z,skip recomputing batch size if it is maximal
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-07-27T09:27:05Z,autobatching support for enc-dec
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-07-26T17:53:39Z,"materialze ""out"" tensor during batch calculation"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-07-24T03:56:02Z,"Early stop bug of greedy_until (primary_until should be a list of str)

I discovered that the accuracy of all models (e.g., llama7b, llama13b, starcoder) in the 'gsm8k-cot' task was 0%. After a thorough investigation, I realized that the generated text for each question was causing an early stop, preventing the 'regex_pattern' from finding any answers. This issue was caused by an incorrect assignment of the 'primary_until' variable in the 'greedy_until' function. Specifically, 'primary_until' should be a list of strings instead of a single string, as the 'stop_sequences' parameter in the 'stop_sequences_criteria' function requires a List[str]. Once I assigned 'primary_until' to '[until[0]]', the accuracy of llama7b in the 'gsm8k-cot' task increased to 1.67%."
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-07-22T18:11:19Z,multi-device: take minimum computed autobatch over all ranks
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-07-22T17:46:16Z,Merge remote-tracking branch 'upstream/big-refactor' into big-refactor-autobatching
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-07-21T14:04:46Z,handle trust_remote_code models better
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-07-17T19:05:20Z,edge case for P-Tuning in causal LM
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-07-16T04:02:50Z,set fp32 if device=mps
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-07-15T16:51:05Z,add mps
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-07-15T16:44:05Z,removed rnd from task.fewshot_context in write_out.py; add mps in huggingface.py
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-07-13T21:37:20Z,add fast tokenizer flag
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-07-12T17:44:35Z,remove TODO comment
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-07-10T19:58:55Z,pass override bs to _loglikelihood_tokens
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-07-10T19:49:24Z,initial autobatching commit
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-07-03T16:24:36Z,don't place quantized models .to(device)
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-07-03T16:05:44Z,"Merge pull request #647 from EleutherAI/handle-multigpu-errors

[Refactor] Handle `cuda:0` device assignment"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-07-03T16:04:24Z,revert typehint
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-07-03T16:03:41Z,revert gpus > num processes case
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-07-03T15:56:31Z,handle device assignment better
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-07-03T15:43:33Z,auto-gptq -> gptq and an import warning
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-07-02T11:27:47Z,"Add PEFT, quantization, remote code, LLaMA fix"
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-06-29T01:42:58Z,Merge branch 'big-refactor' into device-mapping
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-06-29T01:40:37Z,remove stray print
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-06-28T20:30:45Z,cleanup + change to name=parallelize
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-06-28T18:49:34Z,add device_map options for HFLM
github.com/EleutherAI/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-06-28T15:02:47Z,Merge branch 'big-refactor' into add-back-cache
github.com/SCIR-HI/Huatuo-Llama-Med-Chinese,infer.py,2023-08-07T13:46:05Z,"Update Huozi-based model

Major update. Please try our new Huozi-based model, which is much better."
github.com/SCIR-HI/Huatuo-Llama-Med-Chinese,infer.py,2023-04-12T12:43:41Z,add test
github.com/SCIR-HI/Huatuo-Llama-Med-Chinese,infer.py,2023-04-01T09:37:49Z,init code
github.com/LlamaFamily/Llama-Chinese,examples/chat_gradio_no_merge.py,2024-01-30T06:52:49Z,requirements update
github.com/LlamaFamily/Llama-Chinese,examples/chat_gradio_no_merge.py,2024-01-23T11:57:23Z,"调整模型加载代码,以及支持tensorrt_llm的推理"
github.com/LlamaFamily/Llama-Chinese,examples/chat_gradio_no_merge.py,2023-09-02T05:20:43Z,添加lora 不合并的gradio 测试
github.com/mosaicml/composer,composer/models/huggingface.py,2024-02-28T17:59:20Z,"Remove ""generation_length"" in favor of ""generation_kwargs"" (#3014)

* kill generation_length

* fix tests

* fix test

* add deprecation warning

* fix test

* add gen_len back into static_keys

* simplify setting variable in forward and add test

* simply test

* trailing comma

* trailing comma

* linting

---------

Co-authored-by: Daniel King <43149077+dakinggg@users.noreply.github.com>"
github.com/mosaicml/composer,composer/models/huggingface.py,2024-02-12T16:29:26Z,"Refactor `update_metric` (#2965)

* commit one

* rm unused imports

* Update nlp.py

* Update huggingface.py

* Update nlp.py

Make all args same name

* Update huggingface.py

* Update nlp.py

* Update nlp.py

* Update nlp.py

* fix

* wip

* Update composer/metrics/nlp.py

Co-authored-by: Daniel King <43149077+dakinggg@users.noreply.github.com>

* finish

* Update composer/metrics/nlp.py

Co-authored-by: Daniel King <43149077+dakinggg@users.noreply.github.com>

* add tests, dont return logits, fix linting

* rm incorrect asserts

* del incorrect asserts

* rm pyright ignore

* Update composer/metrics/nlp.py

Co-authored-by: Daniel King <43149077+dakinggg@users.noreply.github.com>

* rm comments

---------

Co-authored-by: Jeremy D <115047575+bmosaicml@users.noreply.github.com>
Co-authored-by: Jeremy Dohmann <jeremy@mosaicml.com>
Co-authored-by: Daniel King <43149077+dakinggg@users.noreply.github.com>"
github.com/mosaicml/composer,composer/models/huggingface.py,2024-01-31T15:55:53Z,"Remove torch 1.13 (#2941)

* v1

* kill torch 1.13

* more cleanup

* remove local

* lint

* raise value error

* remove import

* lint

* fix

* fix type

* lint

* fix test

* remove type

* fix types

* fix

* fix

* fix

* remove vision

* fix tests"
github.com/mosaicml/composer,composer/models/huggingface.py,2024-01-30T06:03:43Z,Fix daily tests for peft on 1.13 (#2923)
github.com/mosaicml/composer,composer/models/huggingface.py,2024-01-29T18:59:43Z,Integrate PEFT LoRA with HuggingFaceModel (#2829)
github.com/mosaicml/composer,composer/models/huggingface.py,2024-01-12T21:04:53Z,Upgrade pyright to 1.1.310 (#2841)
github.com/mosaicml/composer,composer/models/huggingface.py,2024-01-08T17:51:15Z,Add encoding=utf-8 (#2824)
github.com/mosaicml/composer,composer/models/huggingface.py,2023-10-23T21:44:20Z,"Upgrade to transformers 4.34.1 (#2635)

* bump transformers version

* add new special casing to tokenizer equivalence check

* try/except for flash v1 issue"
github.com/mosaicml/composer,composer/models/huggingface.py,2023-10-05T21:04:29Z,Change the tokenizer json file to read binary (#2608)
github.com/mosaicml/composer,composer/models/huggingface.py,2023-10-04T19:04:19Z," Add chain of thought eval (#2466)

* implement cot

* fix tests

* debug print statement

* fix max answer lengths for cot

* fix pyright; git push

* Update composer/datasets/in_context_learning_evaluation.py

Co-authored-by: Brian <23239305+b-chu@users.noreply.github.com>

* Update composer/metrics/nlp.py

Co-authored-by: Brian <23239305+b-chu@users.noreply.github.com>

* address comments from daniel

* fix debugging

* address comments

* address comments

* address comments

* Update composer/datasets/in_context_learning_evaluation.py

Co-authored-by: Brian <23239305+b-chu@users.noreply.github.com>

* Update composer/datasets/in_context_learning_evaluation.py

Co-authored-by: Brian <23239305+b-chu@users.noreply.github.com>

* we outtie

* prepend _

---------

Co-authored-by: Brian <23239305+b-chu@users.noreply.github.com>"
github.com/mosaicml/composer,composer/models/huggingface.py,2023-08-28T18:35:34Z,Fix huggingface tokenizer loading for slow tokenizers (#2483)
github.com/mosaicml/composer,composer/models/huggingface.py,2023-08-22T05:50:49Z,kwarg for input_ids (#2459)
github.com/mosaicml/composer,composer/models/huggingface.py,2023-08-04T23:32:44Z,"Add ruff pre-commit  (#2414)

* Add ruff pre-commit hook and apply some sane autofixes

* Reduce number of autofixes

* Fix pyproject

* Update ruff hook

---------

Co-authored-by: Daniel King <43149077+dakinggg@users.noreply.github.com>"
github.com/mosaicml/composer,composer/models/huggingface.py,2023-07-25T21:35:04Z,"Change transformers (#2383)

* fix autoresume with slashed directory

* Revert ""fix autoresume with slashed directory""

This reverts commit 3dfb5f5430da5512bbf418820086b4f291d814f6.

revert

* upgrade transformers to 4.31.0

* add transformers support

* fix precommit

* fix precommit

* fix precommit

* add trust_remote_code

* pre-commit fix"
github.com/mosaicml/composer,composer/models/huggingface.py,2023-07-20T23:09:13Z,"Add code eval dataset and metric (#2301)

* fix autoresume with slashed directory

* Revert ""fix autoresume with slashed directory""

This reverts commit 3dfb5f5430da5512bbf418820086b4f291d814f6.

revert

* add dataset file -- testing next

* add dataloader for code eval

* more testing stuff

* add unit tests

* add full human_eval dataset

* push metric and tests with timeout and stopping criteria

* finish code eval

* fix precommit

* fix test bug

* add autouse fixture to clear cache

* try to fix tests

* try to fix tests

* fix tests

* fix distributed tests

* make tests less bulky

* add lambda support for code eval

* finish lambdas

* fix boto3 import

* Update nlp.py

* Update nlp.py

* fix metric stuff

* fix pyright

* delete go targz

* Delete icl_0.jsonl

* fix yapf

* finish PR

* finish tests

* change timeout

* add clients

* add clients

* add all clients

* add clients finished

* add default sampling params

* change sentencepiece tokenization

* fix precommit

* fix sentencepiece

* Update composer/metrics/nlp.py

Co-authored-by: Mihir Patel <mihir.v.patel7@gmail.com>

* Update composer/datasets/in_context_learning_evaluation.py

Co-authored-by: Mihir Patel <mihir.v.patel7@gmail.com>

* Update in_context_learning_evaluation.py

---------

Co-authored-by: brandon <bcui8377@gmail.com>
Co-authored-by: Mihir Patel <mihir.v.patel7@gmail.com>"
github.com/mosaicml/composer,composer/models/huggingface.py,2023-06-08T17:37:26Z,"fixed adding tokenizer to hf (#2290)

Fixed the function 'write_huggingface_pretrained_from_composer_checkpoint' to properly write the tokenizer from the composer checkpoint by additionally saving the tokenizer from the composer checkpoint to the HuggingFace checkpoint.

An additional test was implemented for this issue.

---------

Co-authored-by: Vincent Chen <vincent@mosaicml.com>"
github.com/mosaicml/composer,composer/models/huggingface.py,2023-06-05T19:53:50Z,Patch for tokenizers that have python files in save_pretrained output (#2279)
github.com/mosaicml/composer,composer/models/huggingface.py,2023-06-05T16:51:41Z,Confirming the output variable has two dimensions before confirming the shape of the second element. (#2275)
github.com/mosaicml/composer,composer/models/huggingface.py,2023-05-02T03:04:31Z,"Update warning->info for number of tokens (#2192)

* update warning

* fix test

* update warning

---------

Co-authored-by: nik-mosaic <101217697+nik-mosaic@users.noreply.github.com>"
github.com/mosaicml/composer,composer/models/huggingface.py,2023-04-27T22:29:51Z,"Add support for torch 2.0 (#2172)

* fix prefetch default
* fix pyright issues with torch2
* xfail onnx test on torch2
* attempt to add torch2 workflows
* summon full params for generate
* fix optimizer monitor
* fix gradient clipping tests
* fix autolog hparams
* backwards compat for optimizer monitor test
* Upgrade FSDP checkpointing to be compatible with torch 2.0 (#2169)
* rename using_torch_2
* add torch2 to daily gpu"
github.com/mosaicml/composer,composer/models/huggingface.py,2023-04-14T21:05:08Z,"Better defaults for `get_num_tokens_in_batch` (#2139)

* add better defaults for num tokens counting
* add tests for max duration in tokens"
github.com/mosaicml/composer,composer/models/huggingface.py,2023-04-06T20:22:50Z,add logic for direct instantiation (#2122)
github.com/mosaicml/composer,composer/models/huggingface.py,2023-03-31T21:14:11Z,add sentencepiece support (#2093)
github.com/mosaicml/composer,composer/models/huggingface.py,2023-03-31T17:00:03Z,"adjust decoding for eval forward (#2107)

* adjust decoding for eval forward to the proper length"
github.com/mosaicml/composer,composer/models/huggingface.py,2023-03-28T21:47:07Z,"gate the extra forward call specifically for fsdp (#2102)

* Gate extra forward call before generate with FSDP check
* adjust FSDP check helper to check top level module and early exit"
github.com/mosaicml/composer,composer/models/huggingface.py,2023-03-21T00:47:40Z,"Add support for ICL QA tasks and generation during evaluation with `HuggingFaceModel` (#2045)

* Add InContextLearningQADataset and InContextLearningQAAccuracy for generation style tasks
* Add support for calling generate using HuggingFaceModel
* Fixes duplicated download of ICL datasets per node"
github.com/mosaicml/composer,composer/models/huggingface.py,2023-03-20T18:14:10Z,fix name_or_path usage in HF save/load usage (#2075)
github.com/mosaicml/composer,composer/models/huggingface.py,2023-03-09T19:01:40Z,"Backward Compat with Torchmetrics (#2046)

* backwards compatbility

* fix test

* debug

* safe load

* add logs

* remove print

* add print

* flip check

* fix eval

* add lint"
github.com/mosaicml/composer,composer/models/huggingface.py,2023-03-07T21:41:56Z,"Remove deprecated code (#2026)

* cleanup

* cleanup

* fix hf model

* fix test

* fix test

* set to evla metrics

* fix bug in metrics

* switch to warnings

* add warnings

* filter warnings

* retry

* lint

* remove dead comment

* remove test"
github.com/mosaicml/composer,composer/models/huggingface.py,2023-03-06T20:40:38Z,Adjust how HuggingFaceModel handles embedding resizing (#2027)
github.com/mosaicml/composer,composer/models/huggingface.py,2023-02-21T18:03:31Z,add map location cpu to huggingface utils (#1980)
github.com/mosaicml/composer,composer/models/huggingface.py,2023-02-17T18:45:22Z,Util for writing HuggingFace save_pretrained from a composer checkpoint (#1974)
github.com/mosaicml/composer,composer/models/huggingface.py,2023-02-15T21:07:09Z,"allow eval metrics to be passed in to HuggingFaceModel directly (#1971)

* allow eval metrics to be passed in to HuggingFaceModel directly
* add deprecation warning and fix typo"
github.com/mosaicml/composer,composer/models/huggingface.py,2023-02-09T21:41:35Z,add support for enc-dec batches without decoder_input_ids (#1950)
github.com/mosaicml/composer,composer/models/huggingface.py,2023-02-09T18:29:30Z,add return dict false test and bug fix (#1948)
github.com/mosaicml/composer,composer/models/huggingface.py,2023-02-09T18:20:25Z,Change how we check for forwards args in models for hf models (#1955)
github.com/mosaicml/composer,composer/models/huggingface.py,2023-02-01T22:36:49Z,"Add few shot and multiple choice to ICL evaluation (#1876)

* new branch

* unittest multi gpu

* add testing for batch padding and idx sampling

* update

* change naming of file

* reimplement lambada

* merge

* add multiple choice eval

* merge multiple choice

* update

* reimplement lambada

* update changes

* finish rebase

* merge mc and lm into same file

* add testing

* add testing for batch padding and idx sampling

* add testing for batch padding and idx sampling

* new branch

* unittest multi gpu

* add testing for batch padding and idx sampling

* update

* change naming of file

* reimplement lambada

* merge

* add multiple choice eval

* update

* reimplement lambada

* update changes

* finish rebase

* merge mc and lm into same file

* add testing

* add testing for batch padding and idx sampling

* add testing for batch padding and idx sampling

* merge updates

* finished merge

* merge

* fix broken merge

* fix broken tests

* yapf

* fix comment

* move label shifting back where it was and dont store labels in the no logits case

* fix duplicated dep in merge;

* fix nits

* fix nits

---------

Co-authored-by: Daniel King <daniel@mosaicml.com>"
github.com/mosaicml/composer,composer/models/huggingface.py,2023-01-27T00:48:37Z,"Add logic for shifting labels before computing metrics (#1913)

Adds a `shift_labels` init argument to `HuggingFaceModel` class. This instructs the model whether to shift labels by one token before computing metrics, which mimics the way HF Causal LM classes handle labels when computing loss. This fixes the current implementation, which never does this shifting and produces incorrect metric results for Causal LMs.

If `shift_labels` is not specified, `HuggingFaceModel` will try to infer the correct behavior based on whether the model is an instance of a registered HF Causal LM class (or a subclass of one)."
github.com/mosaicml/composer,composer/models/huggingface.py,2023-01-11T20:49:50Z,"Fix unprotected import  (#1874)

As title. Along for the ride, change default recommendation to use .[all] instead of .[dev] for lint (latter is only for docs + tests)"
github.com/mosaicml/composer,composer/models/huggingface.py,2023-01-09T22:51:44Z,"Feature/lambada evaluator (#1845)

* new branch

* unittest multi gpu

* reimplement lambada

* update

* pyright

* change naming of file

Co-authored-by: Daniel King <daniel@mosaicml.com>"
github.com/mosaicml/composer,composer/models/huggingface.py,2022-12-03T04:34:55Z,"Huggingface pretrain + finetune notebook (#1775)

Adds an example notebook for pretrain + finetune using huggingface"
github.com/mosaicml/composer,composer/models/huggingface.py,2022-11-30T21:55:15Z,"Autoload HuggingFace model/tokenizer (#1754)

Add autoload of HF model/tokenizer from composer checkpoint to HuggingFaceModel"
github.com/mosaicml/composer,composer/models/huggingface.py,2022-11-29T21:57:47Z,"Add huggingface info to state dict (#1744)

Add huggingface info to the state dict in the new integrations key."
github.com/mosaicml/composer,composer/models/huggingface.py,2022-11-14T21:50:07Z,"Simple nlp tests (#1716)

Adds a simple transformer classifier and a simple ""text"" (really just input ids, not using a tokenizer) classification dataset for testing purposes. Adds a test for HuggingfaceModel using the above dataset to train, eval, and predict. Removes the model_inputs from HuggingfaceModel"
github.com/mosaicml/composer,composer/models/huggingface.py,2022-10-06T16:28:45Z,"fix loss and eval_forward for HF models (#1597)

* fix loss and eval_forward for HF models

* add a comment

Co-authored-by: nik-mosaic <101217697+nik-mosaic@users.noreply.github.com>"
github.com/mosaicml/composer,composer/models/huggingface.py,2022-09-01T14:11:08Z,update model token embeddings according to tokenizer len (#1493)
github.com/mosaicml/composer,composer/models/huggingface.py,2022-08-26T00:59:51Z,"Metrics Refactor Part 2 (#1419)

This PR implements the second half of the metrics refactor (#1411) that updates the training loop and all the Composer models.

The training loop now uses the previously added state.train_metrics and state.eval_metrics from #1411 to perform all training and evaluation on the correct set of metrics.

The main changes with respect to models is that the models now have split up validate() into eval_forward() and update_metrics() methods, which run an evaluation forward pass and update the metrics with the outputs of the evaluation forward pass respectively. This is mainly to get rid of the double forward pass (#467)."
github.com/mosaicml/composer,composer/models/huggingface.py,2022-06-29T21:30:09Z,"Add Debug Log Statements; Fix Pyright (#1218)

This PR adds debug log statements throughout the Composer trainer and engine. When running with the log level set to debug, it will generate a (verbose) log containing a paper trail of the events, callbacks, and algorithms that fire.

Coming along for the ride:

* Pyright version bump and type fixes
* Some refactoring to make the autoresume logic in the trainer more readable"
github.com/mosaicml/composer,composer/models/huggingface.py,2022-06-28T20:10:29Z,"refactor bert and gpt (#1130)

Converting the composer GPT2 and BERT models to use a HuggingFaceModel base class compatible with any pretrained model off the HF Hub. Converted the model creations to factory functions and moved all logic out of the hparams classes."
github.com/mosaicml/composer,composer/models/huggingface.py,2022-06-11T00:22:17Z,"Switch to single quotes (#1152)

Enable the ""double-quote-string-fixer"" pre-commit hook."
github.com/mosaicml/composer,composer/models/huggingface.py,2022-05-24T21:18:36Z,Huggingface part1 (#1047)
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2024-02-02T09:25:58Z,"fix: proper SSE handling for vllm (#877)

fix: proper SSE handling"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-12-18T17:41:19Z,"feat(vllm): support GPTQ with 0.2.6 (#797)

* feat(vllm): GPTQ support passthrough

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: run scripts

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>

* fix(install): set order of xformers before vllm

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* feat: support GPTQ with vLLM

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>
Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-12-14T22:27:32Z,"fix(infra): conform ruff to 150 LL (#781)

Generally correctly format it with ruff format and manual style

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-30T12:58:35Z,"fix(style): setup correct block format

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-30T12:36:34Z,"fix(setter): correct item with the same kwargs with stubs

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-29T18:00:08Z,"fix(llm): remove unecessary parsing

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-29T06:45:14Z,"chore(vllm): add arguments for gpu memory utilization

Probably not going to fix anything, just delaying the problem.

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-26T09:53:36Z,"fix(sdk): remove broken sdk

codespace now around 2.8k lines

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-26T07:49:48Z,"fix(gpus): disable slots for now to enable cached_property

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-26T06:44:59Z,"fix(style): reduce boilerplate and format to custom logics

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-24T06:11:31Z,"refactor: focus (#730)

* perf: remove based images

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: update changelog

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: move dockerifle to run on release only

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: cleanup unused types

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-22T11:49:14Z,"feat(openai): chat templates and complete control of prompt generation (#725)

* feat(openai): chat templates and complete control of prompt generation

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>

* fix: correctly use base chat templates

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>

* fix: remove symlink

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-22T09:21:13Z,"fix(openai): correct stop tokens and finish_reason state (#722)

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-22T06:15:19Z,"fix(base-image): update base image to include cuda for now (#720)

* fix(base-image): update base image to include cuda for now

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* fix: build core and client on release images

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: cleanup style changes

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-21T23:54:51Z,"chore(logger): fix warnings and streamline style (#717)


Sorry but there are too much wasted spacing in `_llm.py`, and I'm unhappy and not productive anytime I look or want to do anything with it

---------

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>
Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-21T09:39:48Z,"refactor: delete unused code (#716)

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-21T07:01:36Z,"feat(generation): add support for eos_token_id (#714)

chore: add support for custom eos_token_id

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-21T06:56:51Z,"chore: cleanup unused prompt templates (#713)

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-21T01:37:15Z,"refactor: update runner helpers and add max_model_len (#712)

* chore(runner): cleanup unecessary checks for runnable backend

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: saving llm reference to runner

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: correct inject item

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: update support for max_seq_len

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* fix: correct max_model_len

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: update and warning backward compatibility

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: remove unused sets

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-20T22:06:25Z,"fix(build): only load model when eager is True

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-20T03:42:25Z,"fix(backend): correct use variable for backend when initialisation (#702)

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-19T15:25:08Z,"feat(engine): CTranslate2 (#698)

* chore: update instruction for dependencies

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* feat(experimental): CTranslate2

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-19T07:52:32Z,"feat(vllm): bump to 0.2.2 (#695)

* feat(vllm): bump to 0.2.2

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: update changelog

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: move up to CUDA 12.1

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* fix: remove auto-gptq installation

since the builder image doesn't have access to GPU

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* fix: update containerization warning

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-19T06:48:33Z,"feat(ctranslate): initial infrastructure support (#694)

* perf: compact and improve speed and agility

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* --wip--

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: cleanup infrastructure

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: update styles notes and autogen mypy configuration

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-19T00:26:20Z,"feat: heuristics logprobs (#692)

* fix(encoder): bring back T5 support on PyTorch

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* feat: support logprobs and prompt_logprobs

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* docs: update changelog

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-18T07:02:16Z,"fix(annotations): check library through find_spec (#691)

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-17T16:23:22Z,"fix(llm): remove unnecessary check (#683)

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-17T15:52:05Z,"fix(torch_dtype): correctly infer based on options (#682)

Users should be able to set the dtype during build, as we it doesn't effect start time

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-16T09:45:49Z,"perf: reduce footprint (#668)

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-16T08:12:52Z,"infra: makes huggingface-hub requirements on fine-tune (#665)

infra: makes huggingface-hub core deps

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-16T08:05:58Z,"feat(llm): respect warnings environment for dtype warning (#664)

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-16T07:58:45Z,"feat(type): provide structured annotations stubs (#663)

* feat(type): provide client stubs

separation of concern for more brevity code base

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* docs: update changelog

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-15T05:18:31Z,"perf: improve build logics and cleanup speed (#657)

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-15T02:55:24Z,"feat: Yi models (#651)

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-14T01:40:50Z,"fix(cpu): more verbose definition for dtype casting (#639)

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-14T01:32:07Z,"fix(generation): compatibility dtype with CPU (#638)

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-13T18:48:04Z,"fix(torch_dtype): load eagerly (#631)

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-13T10:25:50Z,"feat(cli): `--dtype` arguments (#627)

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-13T10:08:33Z,"feat(vllm): support passing specific dtype (#626)

* feat(vllm): support passing specific dtype

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>

* fix: correctly cached the item

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>

* ci: auto fixes from pre-commit.ci

For more information, see https://pre-commit.ci

---------

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-13T02:12:50Z,"fix(ruff): correct consistency between isort and formatter (#624)

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-12T22:39:06Z,"feat(llm): update warning envvar and add embedded mode (#618)

* chore: unify warning envvar and update type inference

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore; update documentation about embedded

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-12T19:55:37Z,"chore(llm): expose quantise and lazy load heavy imports (#617)

* chore(llm): expose quantise and lazy load heavy imports

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: move transformers to TYPE_CHECKING block

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-12T03:36:10Z,"refactor(config): simplify configuration and update start CLI output (#611)

* chore(config): simplify configuration and update start CLI output
handling

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: remove state and message sent after server lifecycle

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: update color stream and refactor reusable logic

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: update documentations and mypy

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-10T07:36:12Z,"fix: loading correct local models (#599)

* fix(model): loading local correctly

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>

* chore: update repr and correct bentomodel processor

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* ci: auto fixes from pre-commit.ci

For more information, see https://pre-commit.ci

* chore: cleanup transformers implementation

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* fix: ruff to ignore I001 on all stubs

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>
Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-09T17:44:05Z,"infra: using ruff formatter (#594)

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-09T16:40:17Z,"refactor(cli): cleanup API (#592)

* chore: remove unused imports

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* refactor(cli): update to only need model_id

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* feat: `openllm start model-id`

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: add changelog

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: update changelog notice

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: update correct config and running tools

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: update backward compat options and treat JSON outputs
corespondingly

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-08T11:57:11Z,"fix(awq): correct awq detection for support (#586)

* fix(awq): correct detection for awq

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>

* chore: update base docker to work

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>

* chore: disable awq on pytorch for now

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>

* ci: auto fixes from pre-commit.ci

For more information, see https://pre-commit.ci

---------

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-08T07:53:08Z,"chore(service): cleanup API (#579)

* chore(service): cleanup API

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: running tools

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* fix: tests import

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-08T07:23:08Z,"refactor(strategies): move logics into openllm-python (#578)

fix(strategies): move to openllm

Strategies shouldn't be a part of openllm-core

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-08T06:24:03Z,"refactor: cleanup typing to expose correct API (#576)

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-08T03:34:11Z,"chore(runner): yield the outputs directly (#573)

update openai client examples to >1

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-07T21:42:20Z,"fix: update build dependencies and format chat prompt (#569)

chore: update correct check and format prompt

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-07T02:34:44Z,"infra: update docs on serving fine-tuning layers (#567)

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-07T01:39:43Z,"perf: unify LLM interface (#518)


Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>
Signed-off-by: paperspace <29749331+aarnphm@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-11-03T17:44:25Z,"fix: Max new tokens (#550)

Bug fix for retrieving user input max_new_tokens"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-10-30T07:20:43Z,"fix(openai): Chat templates (#519)

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>
Co-authored-by: Aaron <29749331+aarnphm@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-10-14T20:04:35Z,fix(breaking): remove embeddings and update client implementation (#500)
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-10-12T21:21:54Z,feat(client): simple implementation and streaming (#256)
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-10-10T16:29:20Z,"fix: do not reply on env var for built bento/docker (#477)

Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-10-07T06:21:31Z,"fix(style): remove weird break on split item

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-10-07T04:50:03Z,"feat: OpenAI-compatible API (#417)

Co-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-10-03T13:53:37Z,"feat: PromptTemplate and system prompt support (#407)

Signed-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>
Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>
Co-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-09-19T07:04:59Z,"feat: support continuous batching on `generate` (#375)

* feat: support continuous batching on `generate`

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: add changelog

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-09-18T06:26:53Z,"fix: set default serialisation methods (#355)

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-09-14T07:09:36Z,"feat: continuous batching with vLLM (#349)

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* feat: continuous batching

Signed-off-by: paperspace <29749331+aarnphm@users.noreply.github.com>

* chore: add changeloe

Signed-off-by: paperspace <29749331+aarnphm@users.noreply.github.com>

* chore: add one shot generation

Signed-off-by: paperspace <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: paperspace <29749331+aarnphm@users.noreply.github.com>
Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-09-12T21:44:01Z,"fix(serialisation): vLLM safetensors support (#324)

* fix(serilisation): vllm support for safetensors

Signed-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>

* chore: running tools

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: generalize one shot generation

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: add changelog [skip ci]

Signed-off-by: paperspace <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>
Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>
Signed-off-by: paperspace <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-09-07T01:48:45Z,"fix(quantize): dyn quant for int8 and int4

only set tokenizer when it is gptq

Signed-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-09-06T18:08:52Z,"fix: synchronize device for inference

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-09-05T01:08:23Z,"chore: only add bentomodel branch during generated service with
OpenLLM

Signed-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-09-04T18:05:50Z,"fix(gptq): use upstream integration (#297)

* wip

Signed-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>

* feat: GPTQ transformers integration

Signed-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>

* fix: only load if variable is available and add changelog

Signed-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>

* chore: remove boilerplate check

Signed-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-09-01T17:00:49Z,"chore: ignore new lines split [skip ci]

Signed-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-09-01T09:34:22Z,"fix(yapf): align weird new lines break [generated] [skip ci] (#284)

fix(yapf): align weird new lines break

Signed-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-09-01T09:15:19Z,"refactor(breaking): unify LLM API (#283)

Signed-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>
Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-08-30T17:52:35Z,"style: google

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-08-30T15:37:41Z,fix: persistent styling between ruff and yapf (#279)
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-08-26T11:36:57Z,"chore(style): add one blank line

to conform with Google style

Signed-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-08-26T11:27:32Z,feat(vllm): streaming (#260)
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-08-25T10:38:59Z,cron(style): run formatter [generated] [skip ci] (#257)
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-08-25T08:36:35Z,"chore: ignore peft and fix adapter loading issue (#255)

Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-08-23T12:46:22Z,"chore(style): synchronized style across packages [skip ci]

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-08-22T17:02:00Z,"chore(style): reduce line length and truncate compression

Signed-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-08-22T14:03:06Z,"chore(style): enable yapf to match with style guidelines

Signed-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-08-22T12:55:46Z,refactor: packages (#249)
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-08-21T12:02:35Z,"fix(generate): Correct set batch output for generate from iterator

Signed-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-08-20T11:32:49Z,feat: token streaming and SSE support (#240)
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-08-17T19:17:00Z,feat(embedding): Adding generic endpoint (#227)
github.com/bentoml/OpenLLM,openllm-python/src/openllm/_llm.py,2023-08-15T06:11:14Z,refactor: monorepo (#203)
github.com/Facico/Chinese-Vicuna,generate.py,2023-06-29T07:59:57Z,rename
github.com/Facico/Chinese-Vicuna,generate.py,2023-06-01T09:05:42Z,multi GPUs inference
github.com/Facico/Chinese-Vicuna,generate.py,2023-05-10T12:30:03Z,update code
github.com/Facico/Chinese-Vicuna,generate.py,2023-05-10T09:08:35Z,update code
github.com/Facico/Chinese-Vicuna,generate.py,2023-04-16T07:55:58Z,release continue-finetune-7epoch-cMedQA2
github.com/Facico/Chinese-Vicuna,generate.py,2023-04-04T11:39:35Z,rearrange code; add html code support
github.com/Facico/Chinese-Vicuna,generate.py,2023-04-01T03:19:51Z,update chat
github.com/Facico/Chinese-Vicuna,generate.py,2023-03-28T07:52:13Z,merge origin
github.com/Facico/Chinese-Vicuna,generate.py,2023-03-28T07:38:15Z,add new parameter: min new tokens
github.com/Facico/Chinese-Vicuna,generate.py,2023-03-28T03:30:02Z,Merge branch 'master' into generation_visualization
github.com/Facico/Chinese-Vicuna,generate.py,2023-03-28T03:26:19Z,clean and reformat code
github.com/Facico/Chinese-Vicuna,generate.py,2023-03-28T03:20:49Z,add support for beam search streamly output
github.com/Facico/Chinese-Vicuna,generate.py,2023-03-28T01:15:23Z,Merge branch 'master' of https://github.com/Facico/Chinese-Vicuna
github.com/Facico/Chinese-Vicuna,generate.py,2023-03-28T01:15:11Z,添加远程lora加载
github.com/Facico/Chinese-Vicuna,generate.py,2023-03-27T14:44:39Z,merge
github.com/Facico/Chinese-Vicuna,generate.py,2023-03-27T12:53:12Z,update
github.com/Facico/Chinese-Vicuna,generate.py,2023-03-27T12:33:58Z,增加简单的交互脚本
github.com/Facico/Chinese-Vicuna,generate.py,2023-03-24T02:15:18Z,新增8000checkpoint
github.com/Facico/Chinese-Vicuna,generate.py,2023-03-23T08:56:14Z,添加repetition_penalty
github.com/Facico/Chinese-Vicuna,generate.py,2023-03-23T06:30:13Z,修改generate，让其能分享网址
github.com/Facico/Chinese-Vicuna,generate.py,2023-03-23T03:17:12Z,init
github.com/pengxiao-song/LaWGPT,utils/merge.py,2023-05-21T18:39:04Z,[ENH] Restructure the project.
github.com/yangjianxin1/Firefly,component/utils.py,2023-09-02T10:26:07Z,使用4bit进行推理 & 无需手动合并权重进行推理
github.com/huggingface/peft,examples/loftq_finetuning/train_gsm8k_llama.py,2024-02-07T11:52:35Z,MNT Move code quality fully to ruff (#1421)
github.com/huggingface/peft,examples/loftq_finetuning/train_gsm8k_llama.py,2023-12-17T14:21:25Z,"LoftQ: edit README.md and example files (#1276)

* fix when num_bits == 2 or 8

* try 13b"
github.com/huggingface/peft,examples/loftq_finetuning/train_gsm8k_llama.py,2023-12-04T14:15:19Z,remove HF tokens (#1207)
github.com/huggingface/peft,examples/loftq_finetuning/train_gsm8k_llama.py,2023-11-29T16:08:17Z,"Add LoftQ initialization method for LoRA (#1150)

---------

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>
Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora.py,2023-06-25T13:47:49Z,stylistic update based on Codacy
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora.py,2023-06-08T02:14:02Z,Merge branch 'main' into 33b
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora.py,2023-05-27T08:57:03Z,improve error message
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora.py,2023-05-26T00:40:23Z,add 33b/65b configs
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora.py,2023-05-25T10:06:36Z,improve error message
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora.py,2023-05-08T10:17:13Z,fix saving to mulitple shards
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora.py,2023-04-28T09:17:06Z,improve format
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora.py,2023-04-28T09:16:10Z,fix bug when loading from hub
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora.py,2023-04-28T00:33:43Z,update merge scripts
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora.py,2023-04-27T17:13:09Z,update merge scripts
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora.py,2023-04-27T16:58:52Z,update merge scripts
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora.py,2023-04-11T16:00:10Z,add assertion on merging
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora.py,2023-04-10T08:39:42Z,deprecate --model_size argument
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora.py,2023-04-07T01:11:32Z,update script
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora.py,2023-04-07T00:58:04Z,preparing for 13b release
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora.py,2023-04-06T06:03:32Z,"add low-ram support, especially for 13b"
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora.py,2023-04-04T12:17:47Z,update print info
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora.py,2023-04-04T12:17:05Z,update print info
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora.py,2023-04-04T09:19:19Z,update merge script to support multiple shards
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora.py,2023-04-03T09:07:55Z,update merge script to support 13B model
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora.py,2023-03-31T01:51:17Z,easier merge
github.com/Facico/Chinese-Vicuna,interaction.py,2023-06-01T09:05:42Z,multi GPUs inference
github.com/Facico/Chinese-Vicuna,interaction.py,2023-04-01T14:17:23Z,update readme for chat.py
github.com/Facico/Chinese-Vicuna,interaction.py,2023-04-01T14:11:04Z,"add better support for chat (streamly beam search,sample,greedy and beam-sample; cancel button, more prompt type and so on )"
github.com/Facico/Chinese-Vicuna,interaction.py,2023-04-01T07:01:06Z,remove the last assistant int exemplar
github.com/Facico/Chinese-Vicuna,interaction.py,2023-03-31T15:46:51Z,fix exemplar
github.com/Facico/Chinese-Vicuna,interaction.py,2023-03-30T06:35:32Z,link you can share
github.com/Facico/Chinese-Vicuna,interaction.py,2023-03-29T12:40:37Z,fix instruction
github.com/Facico/Chinese-Vicuna,interaction.py,2023-03-28T01:15:11Z,添加远程lora加载
github.com/Facico/Chinese-Vicuna,interaction.py,2023-03-27T12:33:58Z,增加简单的交互脚本
github.com/SuperDuperDB/superduperdb,superduperdb/ext/llm/model.py,2024-03-01T17:39:07Z,Optimize the logic of llm infenrence
github.com/SuperDuperDB/superduperdb,superduperdb/ext/llm/model.py,2024-03-01T17:39:07Z,Add Checkpoint component to save checkpoint
github.com/SuperDuperDB/superduperdb,superduperdb/ext/llm/model.py,2024-03-01T17:39:07Z,Supports direct use of datasets for fine-tuning.
github.com/SuperDuperDB/superduperdb,superduperdb/ext/llm/model.py,2024-03-01T17:39:07Z,Fix bugs in LLM under new model code and support new adapter checkpoint saving method
github.com/SuperDuperDB/superduperdb,superduperdb/ext/llm/model.py,2024-03-01T08:20:53Z,Lazy model initialization project wide
github.com/SuperDuperDB/superduperdb,superduperdb/ext/llm/model.py,2024-02-22T11:43:42Z,Improve _Predictor developer contract
github.com/SuperDuperDB/superduperdb,superduperdb/ext/llm/model.py,2024-02-13T14:16:15Z,"Merge Serializer, Datatype and Document"
github.com/SuperDuperDB/superduperdb,superduperdb/ext/llm/model.py,2024-02-06T10:54:43Z,Add integration tests for LLM finetune
github.com/SuperDuperDB/superduperdb,superduperdb/ext/llm/model.py,2024-02-02T07:27:29Z,Added llm finetune code comments
github.com/SuperDuperDB/superduperdb,superduperdb/ext/llm/model.py,2024-02-02T07:27:29Z,Fixed bug when using ray + lora + gradient_checkpointing
github.com/SuperDuperDB/superduperdb,superduperdb/ext/llm/model.py,2024-02-02T07:27:29Z,Support finetuning on remote ray
github.com/SuperDuperDB/superduperdb,superduperdb/ext/llm/model.py,2024-02-02T07:27:29Z,Support finetuning on deepspped and ray
github.com/SuperDuperDB/superduperdb,superduperdb/ext/llm/model.py,2024-01-23T03:27:24Z,Optimize the logic of multi-LORA loading and predicting
github.com/SuperDuperDB/superduperdb,superduperdb/ext/llm/model.py,2024-01-23T03:27:24Z,Delete datasets dependency
github.com/SuperDuperDB/superduperdb,superduperdb/ext/llm/model.py,2024-01-23T03:27:24Z,Optimize llm parameter messages
github.com/SuperDuperDB/superduperdb,superduperdb/ext/llm/model.py,2024-01-23T03:27:24Z,Optimize llm compatible prompt logic
github.com/SuperDuperDB/superduperdb,superduperdb/ext/llm/model.py,2024-01-23T03:27:24Z,"Optimize metrics, validation_sets, etc. in llm training"
github.com/SuperDuperDB/superduperdb,superduperdb/ext/llm/model.py,2024-01-23T03:27:24Z,Revert query_dataset.py
github.com/SuperDuperDB/superduperdb,superduperdb/ext/llm/model.py,2024-01-23T03:27:24Z,"Optimize some use cases, code and comments of llm model"
github.com/SuperDuperDB/superduperdb,superduperdb/ext/llm/model.py,2024-01-23T03:27:24Z,Support llm model based on huggingface
github.com/shibing624/pycorrector,pycorrector/gpt/gpt_model.py,2024-02-18T10:43:15Z,Set data type to float32
github.com/shibing624/pycorrector,pycorrector/gpt/gpt_model.py,2023-11-06T08:42:13Z,update model predict demo.
github.com/shibing624/pycorrector,pycorrector/gpt/gpt_model.py,2023-11-02T08:38:30Z,update chat result.
github.com/shibing624/pycorrector,pycorrector/gpt/gpt_model.py,2023-11-02T06:43:18Z,update infer demo.
github.com/shibing624/pycorrector,pycorrector/gpt/gpt_model.py,2023-11-02T04:12:12Z,update infer demo.
github.com/shibing624/pycorrector,pycorrector/gpt/gpt_model.py,2023-11-02T03:36:44Z,update test case.
github.com/shibing624/pycorrector,pycorrector/gpt/gpt_model.py,2023-11-02T02:41:33Z,update gpt training with eval.
github.com/shibing624/pycorrector,pycorrector/gpt/gpt_model.py,2023-11-01T02:18:01Z,update optim saved.
github.com/shibing624/pycorrector,pycorrector/gpt/gpt_model.py,2023-10-31T04:51:06Z,update chatglm3 output layer.
github.com/shibing624/pycorrector,pycorrector/gpt/gpt_model.py,2023-10-31T02:40:26Z,update args.
github.com/shibing624/pycorrector,pycorrector/gpt/gpt_model.py,2023-10-31T02:26:44Z,update do sample.
github.com/shibing624/pycorrector,pycorrector/gpt/gpt_model.py,2023-10-30T09:38:49Z,add llm model for correction.
github.com/tloen/alpaca-lora,export_hf_checkpoint.py,2023-04-09T21:07:59Z,"Update export_hf_checkpoint.py (#302)

* Update export_hf_checkpoint.py

* Update finetune.py

New tokenizer base model for the current dev branch of transformers

* Update generate.py

* Update export_state_dict_checkpoint.py

* Update export_hf_checkpoint.py"
github.com/tloen/alpaca-lora,export_hf_checkpoint.py,2023-03-28T15:33:47Z,remove asserts
github.com/tloen/alpaca-lora,export_hf_checkpoint.py,2023-03-27T17:31:44Z,"Add HF dataset loading, add linters, pyproject.toml (#175)

* add HF dataset loading, add linters, pyproject.toml

- applied markdownlint
- add black, black[jupyter], isort
- fix noqa codes
- add .github workflow linting
- update README.md

* restore default settings

* resume_from_checkpoint

Co-authored-by: AngainorDev <54739135+AngainorDev@users.noreply.github.com>

* Print warning on checkpoint not found

* add HF dataset loading, add linters, pyproject.toml

- applied markdownlint
- add black, black[jupyter], isort
- fix noqa codes
- add .github workflow linting
- update README.md

* Default to local copy and update it

* Typo

* Remove duplicate code block

---------

Co-authored-by: Eric Wang <eric.james.wang@gmail.com>
Co-authored-by: AngainorDev <54739135+AngainorDev@users.noreply.github.com>"
github.com/tloen/alpaca-lora,export_hf_checkpoint.py,2023-03-23T20:54:39Z,"Remove LLaMA download code, as a precaution"
github.com/tloen/alpaca-lora,export_hf_checkpoint.py,2023-03-18T23:42:58Z,fix HF export script
github.com/tloen/alpaca-lora,export_hf_checkpoint.py,2023-03-18T00:56:10Z,HF export script
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2023-07-05T08:20:51Z,"added enable input embeddings for lora

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2023-06-29T02:03:44Z,"fixed some changes

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2023-06-29T01:44:47Z,"reorganized lora

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2023-06-27T09:02:32Z,"saved word

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2023-06-25T13:39:04Z,"rechanged autoloader

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2023-06-25T10:37:32Z,"added lora inference file

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2023-06-25T06:27:22Z,"fixed fp16 and device error

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2023-06-19T09:03:41Z,"added license and fix issue416

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2023-06-09T07:42:32Z,"removed some parts

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2023-06-08T12:16:08Z,"modified docs

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2023-06-08T10:08:34Z,"modified docs and files

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2023-06-08T02:30:11Z,"updated docs

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2023-06-07T01:46:47Z,"renamed and removed some files

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2023-06-06T12:41:46Z,"added aquila

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2023-06-06T02:32:37Z,"raw add for testing

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2023-03-29T00:39:38Z,"local conflicts resolved

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2023-03-28T13:24:36Z,"add altclip-m18

Signed-off-by: zhaohu xing <920232796@qq.com>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2023-03-16T13:41:46Z, fix bug in setting for mp size >1
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2023-03-07T07:50:42Z,"Revert ""add llama model"""
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2023-03-07T02:15:50Z,"add llama model

Signed-off-by: zhaohu xing <920232796@qq.com>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2023-01-13T02:19:04Z,"enabled fp16 in AltDiffusion

Signed-off-by: Anhforth <yanzaodong2021@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2022-11-26T10:40:46Z, download from server
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2022-11-26T10:20:27Z,merge changes from the simplification of AltDiffusion loading
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2022-11-26T10:15:13Z,simplify the loading of AltDiffusion model
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2022-11-23T10:12:46Z,"fix device bug of AltDiffusion (#134)


Signed-off-by: zhaohu xing <920232796@qq.com>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2022-11-11T08:03:16Z,"Add AltDiffusion and AltCLIP (#90)

* added diffusion model

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* fixed download path

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* added online version of diffusion

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* removed unused code

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* solved pr issue

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* solve unicode issue

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* solve unicode issue

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* fixed pr issue

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* fixed pr issue

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* fix issue

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* updated requireed packages

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updated requirement.txt

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updated import error

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updatde the import

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* seperate the cn_clip module

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updated the packages

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updatde the package versions

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updated torchvision version

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* fixed tokenizer import

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* recover change

* Delete Bert.py

* reformate the code

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* fix json read warning and reformat code

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updated the changes

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* remove local path

* added readme file

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* fix cpm3 infer bugs

* fix cpm3 model bugs

* Update README.md

* updated the diffusion README and commets

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* removed bmtrain install to avoid error

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* add xlmroberta

* added flagstudio

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* enabled online loading

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* imporved README and changed filename

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* changed the filename

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* add readme file for AltCLIP

Signed-off-by: zhaohu xing <920232796@qq.com>

* removed pdb

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* add model_dir

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* updated

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* added more requirements

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* updated pr info

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* modified readme

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* modified readme

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* added the RightBrainAI for our long image generation technology

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* updated the readme

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* fix bugs

Signed-off-by: zhaohu xing <920232796@qq.com>

* tokenizer

Signed-off-by: zhaohu xing <920232796@qq.com>

* Delete vocab.txt

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>
Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>
Signed-off-by: Anhforth <yanzhaodong2021@163.com>
Signed-off-by: zhaohu xing <920232796@qq.com>
Co-authored-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>
Co-authored-by: shunxing1234 <xw747777271@gmail.com>
Co-authored-by: zhaohu xing <920232796@qq.com>
Co-authored-by: Anhforth <yanzhaodong2021@163.com>
Co-authored-by: Zac Liu <liuguang@baai.ac.cn>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2022-10-24T09:25:42Z,"merged upstream

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2022-09-20T03:02:37Z,"Fix issue#85 (#86)

* updated the requirement packages list
* tried to fix the data directory not found error
* fixed issues in running glm_seq2seq
* Update test_glm_seq2seq.py
* fix glm tokenizer bug
* add news section in Readme
* updated docs for tokenizer
* update required packages
* fix_issue_85
* fix bugs in bert forward with different shape of attention mask
* enable offline loading
* fixed error in test_files and add robert ch tokenizer
* fix error in action
* solved Filenotfounderror
Signed-off-by: Anhforth <yanzhaodong2021@163.com>
Signed-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>
Signed-off-by: zhaohu xing <920232796@qq.com>
Signed-off-by: marscrazy <liuguang@baai.ac.cn>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2022-09-06T07:17:20Z,fix_issue_85
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2022-07-21T13:11:52Z,"Develop (#71)



* add vit and examples

* vit and examples

* Update base_model.py

remove unused glob

* Update vit.py

remove data statis

* modify readme.md

Signed-off-by: zhaohu xing <920232796@qq.com>

* modify readme.md

Signed-off-by: zhaohu xing <920232796@qq.com>

* delete annotating code

Signed-off-by: zhaohu xing <920232796@qq.com>

* Vit xzh (#25)

* add vit and examples

* vit and examples

* Update base_model.py

remove unused glob

* Update vit.py

remove data statis

* modify readme.md

Signed-off-by: zhaohu xing <920232796@qq.com>

* modify readme.md

Signed-off-by: zhaohu xing <920232796@qq.com>

* delete annotating code

Signed-off-by: zhaohu xing <920232796@qq.com>

Co-authored-by: Zac Liu <liuguang@baai.ac.cn>

* env trainer

Signed-off-by: zhaohu xing <920232796@qq.com>

* Create README.md

fix typo in flagai introduction.

* vit-checkpoint-activations

Signed-off-by: zhaohu xing <920232796@qq.com>

* vit-checkpoint-activations

Signed-off-by: zhaohu xing <920232796@qq.com>

Co-authored-by: zhaohu xing <32668889+920232796@users.noreply.github.com>
Co-authored-by: Zhaodong Yan <94831503+Anhforth@users.noreply.github.com>
Co-authored-by: Zac Liu <liuguang@baai.ac.cn>
Co-authored-by: Anhforth <yanzhaodong2021@163.com>
Co-authored-by: zhaohu xing <920232796@qq.com>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2022-07-15T13:59:11Z,"Vit xzh (#25)

* add vit and examples

* vit and examples

* Update base_model.py

remove unused glob

* Update vit.py

remove data statis

* modify readme.md

Signed-off-by: zhaohu xing <920232796@qq.com>

* modify readme.md

Signed-off-by: zhaohu xing <920232796@qq.com>

* delete annotating code

Signed-off-by: zhaohu xing <920232796@qq.com>

Co-authored-by: Zac Liu <liuguang@baai.ac.cn>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2022-07-06T09:38:48Z,"fixed tokenizer issues (#63)

* Opt 30b (#16)
* clean codes 
Co-authored-by: Zac Liu <liuguang@baai.ac.cn>
* fix bert tokenizer issue (#18)
* fix bert tokenizer issue
* updated t5, opt and roberta tokenizers
* fixed doc 404 error
Signed-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>
* Opt 66b (#19)
* autoloader for opt
* opt-66b inference
* Update train.py
* Load data from example dir
* add readme of multi GPU inference
Co-authored-by: Zac Liu <liuguang@baai.ac.cn>
* updated release version
Signed-off-by: Anhforth <yanzhaodong2021@163.com>
* fix tokenizer issue
Signed-off-by: Anhforth <yanzhaodong2021@163.com>
Co-authored-by: zhaohu xing <32668889+920232796@users.noreply.github.com>
Co-authored-by: Zhaodong Yan <94831503+Anhforth@users.noreply.github.com>
Co-authored-by: Zac Liu <liuguang@baai.ac.cn>
Co-authored-by: Anhforth <yanzhaodong2021@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2022-07-06T06:19:29Z,"Opt 66b (#19)

* autoloader for opt
* opt-66b inference
* Update train.py
* Load data from example dir
* add readme of multi GPU inference

Co-authored-by: Zac Liu <liuguang@baai.ac.cn>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2022-06-29T03:07:34Z,opt-30b (#15)
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2022-06-24T05:52:51Z,"test merging upstream Develop to master (#5)

fix bugs in distributed training & evaluating

Co-authored-by: Zac Liu <liuguang@baai.ac.cn>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2022-06-23T14:16:24Z,"Add OPT models  (#6)

* added opt tokenizer and updated cpm1 tokenizer
* fixed superglue traininng error
Co-authored-by: Anhforth <yanzhaodong2021@163.com>
* fixed auto_loader
* opt model
* add opt examples, tests
* add opt_6.7b
* opt-13b
* opt-13b-en
Signed-off-by: zhaohu xing <920232796@qq.com>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2022-06-19T18:42:49Z,"Release v1.0.4 (#61)

* add gradient accumulation and fix bugs in distributed evaluation
* fix bugs in accumulate gradients in DDP / Pytorch / DeepSpeed
* release v1.0.4
* set epoch for distributed sampler
* add warnings for DDP with FP16
* change save_epoch to save_interval
* code formate with yapf
* fix typos in utils.py
* del train_test.py
* fix bugs while test seq2seq task with  rather than
* the glm superglue need some works
Signed-off-by: marscrazy <marscrazy_90@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/model/base_model.py,2022-06-01T15:23:40Z,init
github.com/microsoft/LMOps,minillm/finetune.py,2023-12-27T16:36:34Z,use_bf16_for_qwen
github.com/microsoft/LMOps,minillm/finetune.py,2023-11-18T15:45:46Z,Update dev-num to 500
github.com/microsoft/LMOps,minillm/finetune.py,2023-11-18T15:30:20Z,llama2 fp16 sft mp
github.com/microsoft/LMOps,minillm/finetune.py,2023-11-18T09:23:45Z,upgrade transformers
github.com/microsoft/LMOps,minillm/finetune.py,2023-10-21T15:47:28Z,lora
github.com/microsoft/LMOps,minillm/finetune.py,2023-08-16T15:50:30Z,fix mp=1 bug
github.com/microsoft/LMOps,minillm/finetune.py,2023-08-16T15:44:33Z,faster load
github.com/microsoft/LMOps,minillm/finetune.py,2023-06-14T14:05:05Z,fix eval
github.com/microsoft/LMOps,minillm/finetune.py,2023-06-14T11:44:35Z,add minillm
github.com/yangjianxin1/Firefly,script/merge_lora.py,2024-02-03T13:33:36Z,update train args
github.com/yangjianxin1/Firefly,script/merge_lora.py,2023-08-18T07:32:58Z,在cpu中合并权重，缓解显存不足导致的合并问题
github.com/yangjianxin1/Firefly,script/merge_lora.py,2023-08-15T08:29:46Z,更新权重合并脚本
github.com/yangjianxin1/Firefly,script/merge_lora.py,2023-06-26T12:02:49Z,合并lora与base model的权重
github.com/microsoft/LMOps,minillm/train_minillm.py,2023-12-27T16:36:34Z,use_bf16_for_qwen
github.com/microsoft/LMOps,minillm/train_minillm.py,2023-11-28T14:01:10Z,HP configuration
github.com/microsoft/LMOps,minillm/train_minillm.py,2023-11-18T15:30:20Z,llama2 fp16 sft mp
github.com/microsoft/LMOps,minillm/train_minillm.py,2023-10-21T15:47:28Z,lora
github.com/microsoft/LMOps,minillm/train_minillm.py,2023-08-16T15:50:30Z,fix mp=1 bug
github.com/microsoft/LMOps,minillm/train_minillm.py,2023-08-16T15:44:33Z,faster load
github.com/microsoft/LMOps,minillm/train_minillm.py,2023-06-14T11:44:35Z,add minillm
github.com/SCIR-HI/Huatuo-Llama-Med-Chinese,generate.py,2023-08-07T13:46:05Z,"Update Huozi-based model

Major update. Please try our new Huozi-based model, which is much better."
github.com/SCIR-HI/Huatuo-Llama-Med-Chinese,generate.py,2023-04-01T09:37:49Z,init code
github.com/facebookresearch/llama-recipes,src/llama_recipes/inference/model_utils.py,2024-02-06T03:10:30Z,adding sdpa for flash attn
github.com/facebookresearch/llama-recipes,src/llama_recipes/inference/model_utils.py,2024-02-06T02:48:31Z,update the model load with native flash attn
github.com/facebookresearch/llama-recipes,src/llama_recipes/inference/model_utils.py,2023-08-30T22:59:15Z,Move modules into separate src folder
github.com/intel-analytics/BigDL,python/llm/example/GPU/LLM-Finetuning/common/utils/util.py,2024-01-25T11:02:38Z,LLM: reorganize GPU finetuning examples (#9952)
github.com/Facico/Chinese-Vicuna,generate_4bit.py,2023-06-01T09:06:31Z,finetune & inference & requirements for 4bit
github.com/huggingface/peft,examples/fp4_finetuning/finetune_fp4_opt_bnb_peft.py,2023-11-22T11:23:50Z,FIX Dataset loaded twice in 4-bit finetuning script (#1164)
github.com/huggingface/peft,examples/fp4_finetuning/finetune_fp4_opt_bnb_peft.py,2023-06-02T08:07:46Z,"Remove device_map when training 4,8-bit model. (#534)

* Remove device_map when training 4,8-bit model.

* Fix style"
github.com/huggingface/peft,examples/fp4_finetuning/finetune_fp4_opt_bnb_peft.py,2023-05-20T15:47:15Z,"4-bit QLoRA via bitsandbytes (4-bit base model + LoRA) (#476)

* 4bit lora

* 4bit test

* fixing 4bits bugs

* fp4 pass variables

* fix inference datatype and generation config

* updating prep for int8 function to work for 4-bit

* Added FP4 LoRA and FP4 fine-tuning example.

* LinearFP4 -> Linear4bit

* fixes

* Fixed 4-bit example.

* Style changes.

* final changes

---------

Co-authored-by: Artidoro Pagnoni <pagnoni.artidoro@gmail.com>
Co-authored-by: younesbelkada <younesbelkada@gmail.com>"
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-12-27T03:18:14Z,"Add 64k-context models (#478)

For full changes, see the latest release.

---------

Co-authored-by: iMountTai <2506700016@qq.com>
Co-authored-by: Xin Yao <35353688+iMountTai@users.noreply.github.com>"
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-10-26T07:56:48Z,"Add flash attention support for inference (#367)

* Add flash attention support for inference

* update: output information when using flash-attention/xformers-attention

* Remove trailing whitespace

---------

Co-authored-by: Ziqing Yang <yangziqing@163.com>"
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-10-20T06:17:08Z,"Add Speculative sampling support (#328)

* update readme

* add speculative sample

* update inference scripts about speculative sampling

* update README.md

* update readme

* update readme

* update readme

* update readme

* update HF links

* Update speculative_sample.py

* Update gradio_demo.py

* Update README.md

* Update README_EN.md

* Update speculative_sample.py

* Update speculative_sample.py

* Update speculative_sample.py

* Update speculative_sample.py

* Update gradio_demo.py

* fix bugs in speculative sampling

---------

Co-authored-by: GoGoJoestar <qazxc59419@163.com>
Co-authored-by: GoGoJoestar <58219543+GoGoJoestar@users.noreply.github.com>"
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-09-21T03:28:33Z,"Fix quantized inference (#302)

Fixed possible mismatches caused by high version dependencies."
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-08-17T01:23:31Z,fix inference with lora
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-08-08T01:06:08Z,Update gradio_demo.py
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-08-07T09:51:14Z,Update gradio_demo.py
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-08-07T08:43:09Z,add cfg sampling
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-08-04T00:51:49Z,"Merge pull request #69 from ymcui/update-hyperparameters

Update default decoding hyperparameters"
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-08-03T09:36:00Z,update default hyperparameters
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-08-03T06:48:31Z,update system prompt label
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-08-03T04:21:26Z,style update
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-08-03T04:17:56Z,add system prompt input
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-08-02T11:08:49Z,"Merge pull request #47 from iMountTai/main

add support for 4bit inference"
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-08-02T09:30:57Z,Update gradio_demo.py
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-08-02T09:22:47Z,modify message information
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-08-02T08:49:13Z,Reserve load_in_8bit argument
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-08-02T07:47:07Z,fix bug in gradio_demo.py when launch vLLM server
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-08-02T07:03:11Z,add support for 4bit inference
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-08-01T09:22:41Z,fix spelling error: tokenzier_vocab_size -> tokenizer_vocab_size
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-08-01T09:05:14Z,change subprocess.call to subprocess.check_call
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-08-01T08:55:53Z,change subprocess call without shell=True
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-08-01T08:41:22Z,fix bugs
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-08-01T08:01:27Z,merge
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-08-01T07:34:01Z,add vLLM support
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-08-01T07:30:28Z,add arguments for setting system prompts
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/inference/gradio_demo.py,2023-07-31T06:12:17Z,add scripts
github.com/intel-analytics/BigDL,python/llm/example/CPU/PyTorch-Models/Model/llava/generate.py,2023-11-02T06:48:29Z,"add llava gpu example (#9324)

* add llava gpu example

* use 7b model

* fix typo

* add in README"
github.com/intel-analytics/BigDL,python/llm/example/CPU/PyTorch-Models/Model/llava/generate.py,2023-10-27T10:59:20Z,"add cpu example of LLaVA (#9269)

* add LLaVA cpu example

* Small text updates

* update link

---------

Co-authored-by: Yuwen Hu <yuwen.hu@intel.com>"
github.com/intel-analytics/BigDL,python/llm/example/GPU/PyTorch-Models/Model/llava/generate.py,2024-01-29T03:25:11Z,"LLM: GPU Example Updates for Windows (#9992)

* modify aquila

* modify aquila2

* add baichuan

* modify baichuan2

* modify blue-lm

* modify chatglm3

* modify chinese-llama2

* modiy codellama

* modify distil-whisper

* modify dolly-v1

* modify dolly-v2

* modify falcon

* modify flan-t5

* modify gpt-j

* modify internlm

* modify llama2

* modify mistral

* modify mixtral

* modify mpt

* modify phi-1_5

* modify qwen

* modify qwen-vl

* modify replit

* modify solar

* modify starcoder

* modify vicuna

* modify voiceassistant

* modify whisper

* modify yi

* modify aquila2

* modify baichuan

* modify baichuan2

* modify blue-lm

* modify chatglm2

* modify chatglm3

* modify codellama

* modify distil-whisper

* modify dolly-v1

* modify dolly-v2

* modify flan-t5

* modify llama2

* modify llava

* modify mistral

* modify mixtral

* modify phi-1_5

* modify qwen-vl

* modify replit

* modify solar

* modify starcoder

* modify yi

* correct the comments

* remove cpu_embedding in code for whisper and distil-whisper

* remove comment

* remove cpu_embedding for voice assistant

* revert modify voice assistant

* modify for voice assistant

* add comment for voice assistant

* fix comments

* fix comments"
github.com/intel-analytics/BigDL,python/llm/example/GPU/PyTorch-Models/Model/llava/generate.py,2024-01-04T05:33:29Z,"[LLM] IPEX auto importer set on by default (#9832)

* Set BIGDL_IMPORT_IPEX default to True

* Remove import intel_extension_for_pytorch as ipex from GPU example"
github.com/intel-analytics/BigDL,python/llm/example/GPU/PyTorch-Models/Model/llava/generate.py,2023-12-22T08:38:24Z,"Revert ""[LLM] IPEX auto importer turn on by default for XPU (#9730)"" (#9759)

This reverts commit aa377035affbfa57100310a1d80e62b01bd58141."
github.com/intel-analytics/BigDL,python/llm/example/GPU/PyTorch-Models/Model/llava/generate.py,2023-12-22T08:20:32Z,"[LLM] IPEX auto importer turn on by default for XPU (#9730)

* Set BIGDL_IMPORT_IPEX default to true, i.e., auto import IPEX for XPU.
* Remove import intel_extension_for_pytorch as ipex from GPU example.
* Add support for bigdl-core-xe-21."
github.com/intel-analytics/BigDL,python/llm/example/GPU/PyTorch-Models/Model/llava/generate.py,2023-11-02T06:48:29Z,"add llava gpu example (#9324)

* add llava gpu example

* use 7b model

* fix typo

* add in README"
github.com/LlamaFamily/Llama-Chinese,train/merge_peft_model/merge_peft_adapter.py,2024-01-23T11:57:23Z,"调整模型加载代码,以及支持tensorrt_llm的推理"
github.com/LlamaFamily/Llama-Chinese,train/merge_peft_model/merge_peft_adapter.py,2023-08-16T06:54:49Z,add merge model
github.com/OpenBMB/ToolBench,toolbench/model/model_adapter.py,2023-07-27T08:09:51Z,update new version
github.com/OpenBMB/ToolBench,toolbench/model/model_adapter.py,2023-06-05T09:32:51Z,add lora inference
github.com/OpenBMB/ToolBench,toolbench/model/model_adapter.py,2023-05-28T03:47:24Z,initial commit
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/openai_server_demo/openai_api_server.py,2023-07-15T06:10:54Z,update emperical formula
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/openai_server_demo/openai_api_server.py,2023-07-13T13:11:14Z,refactoring code
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/openai_server_demo/openai_api_server.py,2023-07-13T04:19:21Z,fix potential inv_freq issue; add alpha argument
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/openai_server_demo/openai_api_server.py,2023-07-13T01:56:50Z,update max_position_embeddings
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/openai_server_demo/openai_api_server.py,2023-07-05T09:26:51Z,"Merge pull request #705 from ymcui/context_extend

Extend context size (8K+) without fine-tuning"
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/openai_server_demo/openai_api_server.py,2023-07-05T08:39:09Z,replace position interploation with NTK method
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/openai_server_demo/openai_api_server.py,2023-07-03T01:37:10Z,add Position Interpolation for inference scripts
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/openai_server_demo/openai_api_server.py,2023-06-30T06:15:38Z,fix: add do_sample argument for /v1/completions and /v1/chat/completions
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/openai_server_demo/openai_api_server.py,2023-06-28T10:47:52Z,fix system message attribute error
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/openai_server_demo/openai_api_server.py,2023-06-25T14:36:35Z,Update openai_api_server.py
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/openai_server_demo/openai_api_server.py,2023-06-25T14:36:00Z,Update openai_api_server.py
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/openai_server_demo/openai_api_server.py,2023-06-25T14:19:38Z,Stylistic fixes based on Codacy suggestions
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/openai_server_demo/openai_api_server.py,2023-06-25T14:18:16Z,Stylistic fixes based on Codacy suggestions
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/openai_server_demo/openai_api_server.py,2023-06-25T13:48:00Z,"Update openai_api_server.py

remove unused import"
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/openai_server_demo/openai_api_server.py,2023-06-25T12:56:20Z,stylistic update based on Codacy
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/openai_server_demo/openai_api_server.py,2023-06-19T13:28:45Z,chore: delete print
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/openai_server_demo/openai_api_server.py,2023-06-19T13:27:33Z,fix: fix template of chat completion in api demo
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/openai_server_demo/openai_api_server.py,2023-06-19T06:36:43Z,fix cpu inference
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/openai_server_demo/openai_api_server.py,2023-06-16T05:52:36Z,fix inference on cpu
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/openai_server_demo/openai_api_server.py,2023-06-15T14:14:04Z,fix: fix the bug in the text embedding calculation and add a pad_token if the tokenizer do not already have one
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/openai_server_demo/openai_api_server.py,2023-06-07T14:57:55Z,feat: add openai api demo
github.com/yangjianxin1/Firefly,script/http/start_service.py,2023-09-02T09:34:59Z,支持将模型封装成http接口
github.com/huggingface/peft,examples/lora_dreambooth/convert_peft_sd_lora_to_kohya_ss.py,2023-06-27T08:26:54Z,"[Bugfix] Inserted adapter_name to get_peft_model_state_dict function (#626)

* Update train_dreambooth.py

Accelerator init updated from logging_dir to project_dir. Newer versions of accelerate uses project_dir. logging_dir is deprecated

* Bugfix: Adapter name variable inserted, when changing LORA_ADAPTER_NAME it causes error

* Adapter name added as kwarg

* Black code formatted

* Style & Quality check"
github.com/huggingface/peft,examples/lora_dreambooth/convert_peft_sd_lora_to_kohya_ss.py,2023-06-21T14:04:39Z,"Added Civitai LoRAs conversion to PEFT, PEFT LoRAs conversion to webui (#596)

* Fixed kohya_ss to peft lora conversion, added script for backward conversion

* Fixed getting alpha from PEFT

---------

Co-authored-by: Alexander Kovalchuk <a.kovalchuk@prequelapp.com>"
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-03-03T07:21:44Z,"Fix Gemma activation function (#214)

* Update save.py

* Update save.py

* Update save.py

* save

* trainer

* spaces

* original

* Gemma

* Update pyproject.toml

* Update mapper.py

* Update fast_lora.py

* FastGemmaModel

* model_type

* Update llama.py

* Update llama.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update llama.py

* Update fast_lora.py

* Update llama.py

* Update llama.py

* Update cross_entropy_loss.py

* Update llama.py

* Update llama.py

* gemma

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update fast_lora.py

* Update fast_lora.py

* Fast CE Loss

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* CE

* Update llama.py

* Update llama.py

* Update cross_entropy_loss.py

* Update geglu.py

* Update cross_entropy_loss.py

* revert

* Update llama.py

* Update llama.py

* norm

* Update gemma.py

* Update gemma.py

* position_ids

* Update gemma.py

* Update gemma.py

* pos

* Update llama.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update cross_entropy_loss.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* revert

* revert

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update cross_entropy_loss.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* rope

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* llama

* Update llama.py

* gemma

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update save.py

* RoPE

* Update llama.py

* Update llama.py

* Update llama.py

* Update gemma.py

* correct_dtype

* Update gemma.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Chat Templates

* Update README.md

* Update README.md

* Update llama.py

* DoRA

* Update _utils.py

* Update chat_templates.py

* Update llama.py

* Hotfix - fix DoRA, Gemma prompt template (#202) (#203)

* Update save.py

* saving

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update __init__.py

* Update save.py

* Update save.py

* Update save.py

* save

* trainer

* spaces

* original

* Gemma

* Update pyproject.toml

* Update mapper.py

* Update fast_lora.py

* FastGemmaModel

* model_type

* Update llama.py

* Update llama.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update llama.py

* Update fast_lora.py

* Update llama.py

* Update llama.py

* Update cross_entropy_loss.py

* Update llama.py

* Update llama.py

* gemma

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update fast_lora.py

* Update fast_lora.py

* Fast CE Loss

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* CE

* Update llama.py

* Update llama.py

* Update cross_entropy_loss.py

* Update geglu.py

* Update cross_entropy_loss.py

* revert

* Update llama.py

* Update llama.py

* norm

* Update gemma.py

* Update gemma.py

* position_ids

* Update gemma.py

* Update gemma.py

* pos

* Update llama.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update cross_entropy_loss.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* revert

* revert

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update cross_entropy_loss.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* rope

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* llama

* Update llama.py

* gemma

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update save.py

* RoPE

* Update llama.py

* Update llama.py

* Update llama.py

* Update gemma.py

* correct_dtype

* Update gemma.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Chat Templates

* Update README.md

* Update README.md

* Update llama.py

* DoRA

* Update _utils.py

* Update chat_templates.py

* Update pyproject.toml

* Small fixes

* Update pyproject.toml

* Approx gelu

* Update geglu.py

* Approx gelu

* Update llama.py

* Update __init__.py

* Update __init__.py

* Update _utils.py

* Update geglu.py"
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-02-28T13:18:38Z,"Nightly (#204)

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update __init__.py

* Update save.py

* Update save.py

* Update save.py

* save

* trainer

* spaces

* original

* Gemma

* Update pyproject.toml

* Update mapper.py

* Update fast_lora.py

* FastGemmaModel

* model_type

* Update llama.py

* Update llama.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update llama.py

* Update fast_lora.py

* Update llama.py

* Update llama.py

* Update cross_entropy_loss.py

* Update llama.py

* Update llama.py

* gemma

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update fast_lora.py

* Update fast_lora.py

* Fast CE Loss

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* CE

* Update llama.py

* Update llama.py

* Update cross_entropy_loss.py

* Update geglu.py

* Update cross_entropy_loss.py

* revert

* Update llama.py

* Update llama.py

* norm

* Update gemma.py

* Update gemma.py

* position_ids

* Update gemma.py

* Update gemma.py

* pos

* Update llama.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update cross_entropy_loss.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* revert

* revert

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update cross_entropy_loss.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* rope

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* llama

* Update llama.py

* gemma

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update save.py

* RoPE

* Update llama.py

* Update llama.py

* Update llama.py

* Update gemma.py

* correct_dtype

* Update gemma.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Chat Templates

* Update README.md

* Update README.md

* Update llama.py

* DoRA

* Update _utils.py

* Update chat_templates.py

* Update llama.py

* Hotfix - fix DoRA, Gemma prompt template (#202) (#203)

* Update save.py

* saving

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update __init__.py

* Update save.py

* Update save.py

* Update save.py

* save

* trainer

* spaces

* original

* Gemma

* Update pyproject.toml

* Update mapper.py

* Update fast_lora.py

* FastGemmaModel

* model_type

* Update llama.py

* Update llama.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update llama.py

* Update fast_lora.py

* Update llama.py

* Update llama.py

* Update cross_entropy_loss.py

* Update llama.py

* Update llama.py

* gemma

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update fast_lora.py

* Update fast_lora.py

* Fast CE Loss

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* CE

* Update llama.py

* Update llama.py

* Update cross_entropy_loss.py

* Update geglu.py

* Update cross_entropy_loss.py

* revert

* Update llama.py

* Update llama.py

* norm

* Update gemma.py

* Update gemma.py

* position_ids

* Update gemma.py

* Update gemma.py

* pos

* Update llama.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update cross_entropy_loss.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* revert

* revert

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update cross_entropy_loss.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* rope

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* llama

* Update llama.py

* gemma

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update save.py

* RoPE

* Update llama.py

* Update llama.py

* Update llama.py

* Update gemma.py

* correct_dtype

* Update gemma.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Chat Templates

* Update README.md

* Update README.md

* Update llama.py

* DoRA

* Update _utils.py

* Update chat_templates.py"
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-02-26T14:42:10Z,"2.4x faster Gemma (#197)

* Update save.py

* Update save.py

* linking

* llama.cpp bugs

* Update save.py

* Update save.py

* saving

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update __init__.py

* Update save.py

* Update save.py

* Update save.py

* save

* trainer

* spaces

* original

* Gemma

* Update pyproject.toml

* Update mapper.py

* Update fast_lora.py

* FastGemmaModel

* model_type

* Update llama.py

* Update llama.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update llama.py

* Update fast_lora.py

* Update llama.py

* Update llama.py

* Update cross_entropy_loss.py

* Update llama.py

* Update llama.py

* gemma

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update fast_lora.py

* Update fast_lora.py

* Fast CE Loss

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* CE

* Update llama.py

* Update llama.py

* Update cross_entropy_loss.py

* Update geglu.py

* Update cross_entropy_loss.py

* revert

* Update llama.py

* Update llama.py

* norm

* Update gemma.py

* Update gemma.py

* position_ids

* Update gemma.py

* Update gemma.py

* pos

* Update llama.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update cross_entropy_loss.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* revert

* revert

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update cross_entropy_loss.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* rope

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* llama

* Update llama.py

* gemma

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update save.py

* RoPE

* Update llama.py

* Update llama.py

* Update llama.py

* Update gemma.py

* correct_dtype

* Update gemma.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Chat Templates

* Update README.md

* Update README.md"
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-02-20T16:58:59Z,"Feb 2024 Release (#187)

* Fast inference repatch

* Update llama.py

* Update utils.py

* Update utils.py

* Update utils.py

* Update mistral.py

* Update __init__.py

* Fix inference

* Update mistral.py

* fast lm_head

* Remove fast path

* Update rope_embedding.py

* Update loader.py

* LlamaAttention_fast_forward_inference

* if past_key_value is not None and q_len == 1:

* revert inference

* Update loader.py

* past_key_value

* Update llama.py

* Update llama.py

* Fix SDPA

* Update llama.py

* padding

* Inference

* Update llama.py

* Revert

* Update mistral.py

* faster inference

* inference

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* inference

* Update llama.py

* Update utils.py

* faster inference

* Update llama.py

* revert

* lm_head

* Update llama.py

* inference

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* faster inference

* Update llama.py

* fast inference

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* torch compile

* past_key_values

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update utils.py

* Update utils.py

* Update utils.py

* Update utils.py

* Update llama.py

* fast inference + saving config.json

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update mistral.py

* fast inference again

* more temp matrices

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* fast inference

* Update mistral.py

* Update llama.py

* SDPA

* attention_mask

* New version

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update utils.py

* Update utils.py

* Update save.py

* Update save.py

* Torch 2.2.0

* Update save.py

* mistral swa

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Fix SWA inference

* Fix llm_int8_skip_modules

* SWA inference

* Update save.py

* Update save.py

* Update pyproject.toml

* __version__

* __version__

* Update save.py

* Update save.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Chat Templates

* Update chat_templates.py

* Update chat_templates.py

* Update chat_templates.py

* Update chat_templates.py

* patch tokenizer

* Update chat_templates.py

* Saving, LlamaRotaryEmbedding issues

* Update llama.py

* Update mistral.py

* Update mapper.py

* Fix RoPE precision issues

* Bugs

* saving bugs

* Update llama.py

* readme

* spaces

* spaces

* globals

* slash

* slashes

* spaces

* apache

* Update save.py

* Update save.py

* Update loader.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* trainer

* Update save.py

* Update pyproject.toml

* install

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* PeftModel token + saving

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* linking

* llama.cpp bugs

* Update save.py

* Update save.py

* saving

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update __init__.py

* Update save.py

* Update save.py

* Update save.py

* save

* trainer

* spaces

* original"
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-02-14T13:07:42Z,"Prelim Feb release (#173)

* Works?

* Update pyproject.toml

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Swiglu

* Update swiglu.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update swiglu.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* attention_mask

* Update llama.py

* Update llama.py

* labels

* Update mistral.py

* Update llama.py

* attention mask

* Update save.py

* Update save.py

* Update mistral.py

* attention mask

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update dpo.py

* Patch saving

* Update save.py

* Update save.py

* patch_saving_functions

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* print

* Mistral patch

* Update mistral.py

* Update save.py

* saving

* Update llama.py

* Update llama.py

* Fast inference repatch

* Update llama.py

* Update utils.py

* Update utils.py

* Update utils.py

* Update mistral.py

* Update __init__.py

* Fix inference

* Update mistral.py

* fast lm_head

* Remove fast path

* Update rope_embedding.py

* Update loader.py

* LlamaAttention_fast_forward_inference

* if past_key_value is not None and q_len == 1:

* revert inference

* Update loader.py

* past_key_value

* Update llama.py

* Update llama.py

* Fix SDPA

* Update llama.py

* padding

* Inference

* Update llama.py

* Revert

* Update mistral.py

* faster inference

* inference

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* inference

* Update llama.py

* Update utils.py

* faster inference

* Update llama.py

* revert

* lm_head

* Update llama.py

* inference

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* faster inference

* Update llama.py

* fast inference

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* torch compile

* past_key_values

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update utils.py

* Update utils.py

* Update utils.py

* Update utils.py

* Update llama.py

* fast inference + saving config.json

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update mistral.py

* fast inference again

* more temp matrices

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* fast inference

* Update mistral.py

* Update llama.py

* SDPA

* attention_mask

* New version

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update utils.py

* Update utils.py

* Update save.py

* Update save.py

* Torch 2.2.0

* Update save.py

* mistral swa

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Fix SWA inference

* Fix llm_int8_skip_modules

* SWA inference

* Update save.py

* Update save.py

* Update pyproject.toml

* __version__

* __version__

* Update save.py

* Update save.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Chat Templates

* Update chat_templates.py

* Update chat_templates.py

* Update chat_templates.py

* Update chat_templates.py

* patch tokenizer

* Update chat_templates.py

* Saving, LlamaRotaryEmbedding issues

* Update llama.py

* Update mistral.py"
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-02-07T16:40:28Z,"Nightly (#161)

* Update fast_lora.py

* Update utils.py

* Update llama.py

* Update fast_lora.py

* Update swiglu.py

* Update save.py

* Update save.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Revert ""Update llama.py""

This reverts commit a208ec46e012cf470ecefe6268a66358215df7b6.

* Update llama.py

* Works?

* Update pyproject.toml

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Swiglu

* Update swiglu.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update swiglu.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* attention_mask

* Update llama.py

* Update llama.py

* labels

* Update mistral.py

* Update llama.py

* attention mask

* Update save.py

* Update save.py

* Update mistral.py

* attention mask

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update dpo.py

* Patch saving

* Update save.py

* Update save.py

* patch_saving_functions

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* print

* Mistral patch

* Update mistral.py

* Update save.py

* saving

* Update llama.py

* Update llama.py

* Fast inference repatch

* Update llama.py

* Update utils.py

* Update utils.py

* Update utils.py

* Update mistral.py

* Update __init__.py

* Fix inference

* Update mistral.py

* fast lm_head

* Remove fast path

* Update rope_embedding.py

* Update loader.py

* LlamaAttention_fast_forward_inference

* if past_key_value is not None and q_len == 1:

* revert inference

* Update loader.py

* past_key_value

* Update llama.py

* Update llama.py

* Fix SDPA

* Update llama.py

* padding

* Inference

* Update llama.py

* Revert

* Update mistral.py

* faster inference

* inference

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* inference

* Update llama.py

* Update utils.py

* faster inference

* Update llama.py

* revert

* lm_head

* Update llama.py

* inference

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* faster inference

* Update llama.py

* fast inference

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* torch compile

* past_key_values

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update utils.py

* Update utils.py

* Update utils.py

* Update utils.py

* Update llama.py

* fast inference + saving config.json

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update mistral.py

* fast inference again

* more temp matrices

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* fast inference

* Update mistral.py

* Update llama.py

* SDPA

* attention_mask

* New version

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update utils.py

* Update utils.py

* Update save.py

* Update save.py

* Torch 2.2.0

* Update save.py

* mistral swa

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Fix SWA inference

* Fix llm_int8_skip_modules

* SWA inference

* Update save.py

* Update save.py

* Update pyproject.toml

* __version__

* __version__

* Update save.py

* Update save.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py"
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-02-06T17:40:50Z,"Torch 2.2 (#157)

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update save.py

* Update fast_lora.py

* Update utils.py

* Update llama.py

* Update fast_lora.py

* Update swiglu.py

* Update save.py

* Update save.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Revert ""Update llama.py""

This reverts commit a208ec46e012cf470ecefe6268a66358215df7b6.

* Update llama.py

* Works?

* Update pyproject.toml

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Swiglu

* Update swiglu.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update swiglu.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* attention_mask

* Update llama.py

* Update llama.py

* labels

* Update mistral.py

* Update llama.py

* attention mask

* Update save.py

* Update save.py

* Update mistral.py

* attention mask

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update dpo.py

* Patch saving

* Update save.py

* Update save.py

* patch_saving_functions

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* print

* Mistral patch

* Update mistral.py

* Update save.py

* saving

* Update llama.py

* Update llama.py

* Fast inference repatch

* Update llama.py

* Update utils.py

* Update utils.py

* Update utils.py

* Update mistral.py

* Update __init__.py

* Fix inference

* Update mistral.py

* fast lm_head

* Remove fast path

* Update rope_embedding.py

* Update loader.py

* LlamaAttention_fast_forward_inference

* if past_key_value is not None and q_len == 1:

* revert inference

* Update loader.py

* past_key_value

* Update llama.py

* Update llama.py

* Fix SDPA

* Update llama.py

* padding

* Inference

* Update llama.py

* Revert

* Update mistral.py

* faster inference

* inference

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* inference

* Update llama.py

* Update utils.py

* faster inference

* Update llama.py

* revert

* lm_head

* Update llama.py

* inference

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* faster inference

* Update llama.py

* fast inference

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* torch compile

* past_key_values

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update utils.py

* Update utils.py

* Update utils.py

* Update utils.py

* Update llama.py

* fast inference + saving config.json

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update mistral.py

* fast inference again

* more temp matrices

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* fast inference

* Update mistral.py

* Update llama.py

* SDPA

* attention_mask

* New version

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update utils.py

* Update utils.py

* Update save.py

* Update save.py

* Torch 2.2.0

* Update save.py

* mistral swa

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Fix SWA inference

* Fix llm_int8_skip_modules

* SWA inference

* Update save.py

* Update save.py

* Update pyproject.toml

* __version__

* __version__

* Update save.py

* Update save.py

* Update mistral.py"
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-02-04T06:35:56Z,"2x faster inference (#151)

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update llama.py

* Update fast_lora.py

* Update llama.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update swiglu.py

* Update fast_lora.py

* Update swiglu.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update save.py

* Update fast_lora.py

* Update utils.py

* Update llama.py

* Update fast_lora.py

* Update swiglu.py

* Update save.py

* Update save.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Revert ""Update llama.py""

This reverts commit a208ec46e012cf470ecefe6268a66358215df7b6.

* Update llama.py

* Works?

* Update pyproject.toml

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Swiglu

* Update swiglu.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update swiglu.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* attention_mask

* Update llama.py

* Update llama.py

* labels

* Update mistral.py

* Update llama.py

* attention mask

* Update save.py

* Update save.py

* Update mistral.py

* attention mask

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update dpo.py

* Patch saving

* Update save.py

* Update save.py

* patch_saving_functions

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* print

* Mistral patch

* Update mistral.py

* Update save.py

* saving

* Update llama.py

* Update llama.py

* Fast inference repatch

* Update llama.py

* Update utils.py

* Update utils.py

* Update utils.py

* Update mistral.py

* Update __init__.py

* Fix inference

* Update mistral.py

* fast lm_head

* Remove fast path

* Update rope_embedding.py

* Update loader.py

* LlamaAttention_fast_forward_inference

* if past_key_value is not None and q_len == 1:

* revert inference

* Update loader.py

* past_key_value

* Update llama.py

* Update llama.py

* Fix SDPA

* Update llama.py

* padding

* Inference

* Update llama.py

* Revert

* Update mistral.py

* faster inference

* inference

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* inference

* Update llama.py

* Update utils.py

* faster inference

* Update llama.py

* revert

* lm_head

* Update llama.py

* inference

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* faster inference

* Update llama.py

* fast inference

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* torch compile

* past_key_values

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update utils.py

* Update utils.py

* Update utils.py

* Update utils.py

* Update llama.py

* fast inference + saving config.json

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update mistral.py

* fast inference again

* more temp matrices

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* fast inference

* Update mistral.py

* Update llama.py

* SDPA

* attention_mask

* New version

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update utils.py

* Update utils.py"
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-01-30T17:03:37Z,"Hotfix - fix inference (#146)

* faster saving & inference

* Update llama.py

* Update save.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* fast inference

* Update llama.py

* Update save.py

* Update llama.py

* Mistral correct RoPE scaling

* Max sequence lengths

* Apache 2

* fast_linear_forward

* Update utils.py

* Update utils.py

* No print

* Update utils.py

* Update utils.py

* inference

* Update llama.py

* Fast inference RoPE

* Update llama.py

* Update llama.py

* RoPE

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* LoRA

* Fast LoRA saving

* Update llama.py

* hidden_states

* q_len == 1

* q_len issue

* Update mistral.py

* Update mistral.py

* incorrect inference

* Update to transformers 4.37

* Graceful FA2 error + torch 2.1.1

* Update mapper.py

* Update pyproject.toml

* Fix saving and bnb-4bit

* Update fast_lora.py

* Update fast_lora.py

* remove patching

* Update llama.py

* Update llama.py

* Update swiglu.py

* Repatch

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update llama.py

* Update fast_lora.py

* Update llama.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update swiglu.py

* Update fast_lora.py

* Update swiglu.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update save.py

* Update fast_lora.py

* Update utils.py

* Update llama.py

* Update fast_lora.py

* Update swiglu.py

* Update save.py

* Update save.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Revert ""Update llama.py""

This reverts commit a208ec46e012cf470ecefe6268a66358215df7b6.

* Update llama.py

* Works?

* Update pyproject.toml

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Swiglu

* Update swiglu.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update swiglu.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* attention_mask

* Update llama.py

* Update llama.py

* labels

* Update mistral.py

* Update llama.py

* attention mask

* Update save.py

* Update save.py

* Update mistral.py

* attention mask

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update dpo.py

* Patch saving

* Update save.py

* Update save.py

* patch_saving_functions

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* print

* Mistral patch

* Update mistral.py

* Update save.py

* saving

* Update llama.py

* Update llama.py

* Fast inference repatch

* Update llama.py

* Update utils.py

* Update utils.py

* Update utils.py

* Update mistral.py

* Update __init__.py

* Fix inference

* Update mistral.py

* fast lm_head

* Remove fast path

* Update rope_embedding.py

* Update loader.py

* LlamaAttention_fast_forward_inference

* if past_key_value is not None and q_len == 1:

* revert inference

* Update loader.py

* past_key_value"
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-01-29T06:49:54Z,"Fix inference attention mask (#142)

* faster saving & inference

* Update llama.py

* Update save.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* fast inference

* Update llama.py

* Update save.py

* Update llama.py

* Mistral correct RoPE scaling

* Max sequence lengths

* Apache 2

* fast_linear_forward

* Update utils.py

* Update utils.py

* No print

* Update utils.py

* Update utils.py

* inference

* Update llama.py

* Fast inference RoPE

* Update llama.py

* Update llama.py

* RoPE

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* LoRA

* Fast LoRA saving

* Update llama.py

* hidden_states

* q_len == 1

* q_len issue

* Update mistral.py

* Update mistral.py

* incorrect inference

* Update to transformers 4.37

* Graceful FA2 error + torch 2.1.1

* Update mapper.py

* Update pyproject.toml

* Fix saving and bnb-4bit

* Update fast_lora.py

* Update fast_lora.py

* remove patching

* Update llama.py

* Update llama.py

* Update swiglu.py

* Repatch

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update llama.py

* Update fast_lora.py

* Update llama.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update swiglu.py

* Update fast_lora.py

* Update swiglu.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update save.py

* Update fast_lora.py

* Update utils.py

* Update llama.py

* Update fast_lora.py

* Update swiglu.py

* Update save.py

* Update save.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Revert ""Update llama.py""

This reverts commit a208ec46e012cf470ecefe6268a66358215df7b6.

* Update llama.py

* Works?

* Update pyproject.toml

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Swiglu

* Update swiglu.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update swiglu.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* attention_mask

* Update llama.py

* Update llama.py

* labels

* Update mistral.py

* Update llama.py

* attention mask

* Update save.py

* Update save.py

* Update mistral.py

* attention mask

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update dpo.py

* Patch saving

* Update save.py

* Update save.py

* patch_saving_functions

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* print

* Mistral patch

* Update mistral.py

* Update save.py

* saving

* Update llama.py

* Update llama.py"
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-01-28T15:52:39Z,"Fix saving issues (#139)

* faster saving & inference

* Update llama.py

* Update save.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* fast inference

* Update llama.py

* Update save.py

* Update llama.py

* Mistral correct RoPE scaling

* Max sequence lengths

* Apache 2

* fast_linear_forward

* Update utils.py

* Update utils.py

* No print

* Update utils.py

* Update utils.py

* inference

* Update llama.py

* Fast inference RoPE

* Update llama.py

* Update llama.py

* RoPE

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* LoRA

* Fast LoRA saving

* Update llama.py

* hidden_states

* q_len == 1

* q_len issue

* Update mistral.py

* Update mistral.py

* incorrect inference

* Update to transformers 4.37

* Graceful FA2 error + torch 2.1.1

* Update mapper.py

* Update pyproject.toml

* Fix saving and bnb-4bit

* Update fast_lora.py

* Update fast_lora.py

* remove patching

* Update llama.py

* Update llama.py

* Update swiglu.py

* Repatch

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update llama.py

* Update fast_lora.py

* Update llama.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update swiglu.py

* Update fast_lora.py

* Update swiglu.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update save.py

* Update fast_lora.py

* Update utils.py

* Update llama.py

* Update fast_lora.py

* Update swiglu.py

* Update save.py

* Update save.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Revert ""Update llama.py""

This reverts commit a208ec46e012cf470ecefe6268a66358215df7b6.

* Update llama.py

* Works?

* Update pyproject.toml

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Swiglu

* Update swiglu.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update swiglu.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* attention_mask

* Update llama.py

* Update llama.py

* labels

* Update mistral.py

* Update llama.py

* attention mask

* Update save.py

* Update save.py

* Update mistral.py

* attention mask

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update dpo.py

* Patch saving

* Update save.py

* Update save.py

* patch_saving_functions

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* print"
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-01-26T17:50:22Z,"Inference bug fix (#134)

* faster saving & inference

* Update llama.py

* Update save.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* fast inference

* Update llama.py

* Update save.py

* Update llama.py

* Mistral correct RoPE scaling

* Max sequence lengths

* Apache 2

* fast_linear_forward

* Update utils.py

* Update utils.py

* No print

* Update utils.py

* Update utils.py

* inference

* Update llama.py

* Fast inference RoPE

* Update llama.py

* Update llama.py

* RoPE

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* LoRA

* Fast LoRA saving

* Update llama.py

* hidden_states

* q_len == 1

* q_len issue

* Update mistral.py

* Update mistral.py

* incorrect inference

* Update to transformers 4.37

* Graceful FA2 error + torch 2.1.1

* Update mapper.py

* Update pyproject.toml

* Fix saving and bnb-4bit

* Update fast_lora.py

* Update fast_lora.py

* remove patching

* Update llama.py

* Update llama.py

* Update swiglu.py

* Repatch

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update llama.py

* Update fast_lora.py

* Update llama.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update swiglu.py

* Update fast_lora.py

* Update swiglu.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update save.py

* Update fast_lora.py

* Update utils.py

* Update llama.py

* Update fast_lora.py

* Update swiglu.py

* Update save.py

* Update save.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Revert ""Update llama.py""

This reverts commit a208ec46e012cf470ecefe6268a66358215df7b6.

* Update llama.py"
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-01-26T17:47:54Z,"More bug fixes (#133)

* faster saving & inference

* Update llama.py

* Update save.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* fast inference

* Update llama.py

* Update save.py

* Update llama.py

* Mistral correct RoPE scaling

* Max sequence lengths

* Apache 2

* fast_linear_forward

* Update utils.py

* Update utils.py

* No print

* Update utils.py

* Update utils.py

* inference

* Update llama.py

* Fast inference RoPE

* Update llama.py

* Update llama.py

* RoPE

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* LoRA

* Fast LoRA saving

* Update llama.py

* hidden_states

* q_len == 1

* q_len issue

* Update mistral.py

* Update mistral.py

* incorrect inference

* Update to transformers 4.37

* Graceful FA2 error + torch 2.1.1

* Update mapper.py

* Update pyproject.toml

* Fix saving and bnb-4bit

* Update fast_lora.py

* Update fast_lora.py

* remove patching

* Update llama.py

* Update llama.py

* Update swiglu.py

* Repatch

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update llama.py

* Update fast_lora.py

* Update llama.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update swiglu.py

* Update fast_lora.py

* Update swiglu.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update save.py

* Update fast_lora.py

* Update utils.py

* Update llama.py

* Update fast_lora.py

* Update swiglu.py

* Update save.py

* Update save.py

* Update llama.py

* Update llama.py

* Update llama.py"
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-01-25T17:19:17Z,"Fix bugs (#129)

* faster saving & inference

* Update llama.py

* Update save.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* fast inference

* Update llama.py

* Update save.py

* Update llama.py

* Mistral correct RoPE scaling

* Max sequence lengths

* Apache 2

* fast_linear_forward

* Update utils.py

* Update utils.py

* No print

* Update utils.py

* Update utils.py

* inference

* Update llama.py

* Fast inference RoPE

* Update llama.py

* Update llama.py

* RoPE

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* LoRA

* Fast LoRA saving

* Update llama.py

* hidden_states

* q_len == 1

* q_len issue

* Update mistral.py

* Update mistral.py

* incorrect inference

* Update to transformers 4.37

* Graceful FA2 error + torch 2.1.1

* Update mapper.py

* Update pyproject.toml

* Fix saving and bnb-4bit

* Update fast_lora.py

* Update fast_lora.py

* remove patching

* Update llama.py

* Update llama.py

* Update swiglu.py

* Repatch

* Update fast_lora.py"
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-01-22T16:55:24Z,"2-4x faster native HF inference (#119)

* faster saving & inference

* Update llama.py

* Update save.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* fast inference

* Update llama.py

* Update save.py

* Update llama.py

* Mistral correct RoPE scaling

* Max sequence lengths

* Apache 2

* fast_linear_forward

* Update utils.py

* Update utils.py

* No print

* Update utils.py

* Update utils.py

* inference

* Update llama.py

* Fast inference RoPE

* Update llama.py

* Update llama.py

* RoPE

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* LoRA

* Fast LoRA saving"
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-01-21T11:20:22Z,"Hotfix (#118)

* faster saving & inference

* Update llama.py

* Update save.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py"
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-01-20T12:23:00Z,"Hotfix for Jan 2024 Release (#110)

* Fix tokenizer, dropout, bias for LoRA

* Update loader.py

* Fix LoRA downcasting

* Update _utils.py

* Saving to GGUF

* fix

* colab_quantize_to_gguf

* move save modules

* save module

* Update __init__.py

* Update save.py

* Temp downgrade due to TRL issue

* Fix up bugs

* Faster saving + other changes

* Update llama.py

* Saving modules

* spelling

* Update llama.py

* Update save.py

* Update save.py

* Update loader.py

* Update llama.py

* patch saving

* Update save.py

* Update save.py

* Update save.py

* patch saving

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* original_model

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* saving to RAM leakage?

* Update save.py

* new_save_directory

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update pyproject.toml

* Update pyproject.toml

* Update pyproject.toml

* Quick fixes

* Update llama.py

* Update llama.py

* Update dpo.py

* Update dpo.py

* Update llama.py

* Update save.py

* getattr

* RSLoRA and LoftQ direct support

* Update llama.py

* Update llama.py

* Update llama.py

* Fix DPO + GGUF

* Fix quantization_method

* Fix quantization_config

* patch model

* Update llama.py

* Update llama.py

* Update llama.py

* Update save.py

* Update save.py

* tokenizer_save_settings

* Update save.py

* quantization and loftq

* Update save.py

* Update llama.py

* Update save.py"
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-01-19T17:25:06Z,"Quick fixes (#106)

* Fix tokenizer, dropout, bias for LoRA

* Update loader.py

* Fix LoRA downcasting

* Update _utils.py

* Saving to GGUF

* fix

* colab_quantize_to_gguf

* move save modules

* save module

* Update __init__.py

* Update save.py

* Temp downgrade due to TRL issue

* Fix up bugs

* Faster saving + other changes

* Update llama.py

* Saving modules

* spelling

* Update llama.py

* Update save.py

* Update save.py

* Update loader.py

* Update llama.py

* patch saving

* Update save.py

* Update save.py

* Update save.py

* patch saving

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* original_model

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* saving to RAM leakage?

* Update save.py

* new_save_directory

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update pyproject.toml

* Update pyproject.toml

* Update pyproject.toml

* Quick fixes

* Update llama.py

* Update llama.py

* Update dpo.py

* Update dpo.py

* Update llama.py

* Update save.py

* getattr

* RSLoRA and LoftQ direct support

* Update llama.py

* Update llama.py

* Update llama.py

* Fix DPO + GGUF"
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-01-19T11:57:22Z,"getattr issues (#103)

* Fix tokenizer, dropout, bias for LoRA

* Update loader.py

* Fix LoRA downcasting

* Update _utils.py

* Saving to GGUF

* fix

* colab_quantize_to_gguf

* move save modules

* save module

* Update __init__.py

* Update save.py

* Temp downgrade due to TRL issue

* Fix up bugs

* Faster saving + other changes

* Update llama.py

* Saving modules

* spelling

* Update llama.py

* Update save.py

* Update save.py

* Update loader.py

* Update llama.py

* patch saving

* Update save.py

* Update save.py

* Update save.py

* patch saving

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* original_model

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* saving to RAM leakage?

* Update save.py

* new_save_directory

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update pyproject.toml

* Update pyproject.toml

* Update pyproject.toml

* Quick fixes

* Update llama.py

* Update llama.py

* Update dpo.py

* Update dpo.py

* Update llama.py

* Update save.py

* getattr"
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-01-19T11:52:30Z,"Quick fixes (#101)

* Fix tokenizer, dropout, bias for LoRA

* Update loader.py

* Fix LoRA downcasting

* Update _utils.py

* Saving to GGUF

* fix

* colab_quantize_to_gguf

* move save modules

* save module

* Update __init__.py

* Update save.py

* Temp downgrade due to TRL issue

* Fix up bugs

* Faster saving + other changes

* Update llama.py

* Saving modules

* spelling

* Update llama.py

* Update save.py

* Update save.py

* Update loader.py

* Update llama.py

* patch saving

* Update save.py

* Update save.py

* Update save.py

* patch saving

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* original_model

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* saving to RAM leakage?

* Update save.py

* new_save_directory

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update pyproject.toml

* Update pyproject.toml

* Update pyproject.toml

* Quick fixes

* Update llama.py

* Update llama.py

* Update dpo.py

* Update dpo.py

* Update llama.py

* Update save.py"
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-01-18T17:51:19Z,"2024 Release (#96)

* Fix tokenizer, dropout, bias for LoRA

* Update loader.py

* Fix LoRA downcasting

* Update _utils.py

* Saving to GGUF

* fix

* colab_quantize_to_gguf

* move save modules

* save module

* Update __init__.py

* Update save.py

* Temp downgrade due to TRL issue

* Fix up bugs

* Faster saving + other changes

* Update llama.py

* Saving modules

* spelling

* Update llama.py

* Update save.py

* Update save.py

* Update loader.py

* Update llama.py

* patch saving

* Update save.py

* Update save.py

* Update save.py

* patch saving

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* original_model

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* saving to RAM leakage?

* Update save.py

* new_save_directory

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update pyproject.toml

* Update pyproject.toml

* Update pyproject.toml"
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-01-10T17:08:03Z,"Fix some bugs (#83)

* Fix tokenizer, dropout, bias for LoRA

* Update loader.py

* Fix LoRA downcasting

* Update _utils.py

* Saving to GGUF

* fix

* colab_quantize_to_gguf

* move save modules

* save module

* Update __init__.py

* Update save.py

* Temp downgrade due to TRL issue

* Fix up bugs"
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-01-09T14:02:44Z,fix_tokenizer
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-01-09T12:40:43Z,check_tokenizer
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-01-06T08:13:39Z,"Fix tokenizer, bias, dropout supported for LoRA (#69)

* Fix tokenizer, dropout, bias for LoRA

* Update loader.py"
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-01-04T17:08:53Z,"Fix tokenizer + docs (#62)

* Patch tokenizer

* Update _utils.py

* Update _utils.py

* Update _utils.py

* Cleanup

* Add comments to functions

* Update rope_embedding.py

* Update rope_embedding.py

* Update llama.py

* New logos!

* Update README.md"
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-01-01T16:41:05Z,Update llama.py
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-01-01T16:37:04Z,Update llama.py
github.com/unslothai/unsloth,unsloth/models/llama.py,2024-01-01T16:20:11Z,Add tokenizer checking + TinyLlama
github.com/unslothai/unsloth,unsloth/models/llama.py,2023-12-29T17:35:35Z,"Prelim Qwen, Deepseek support (#58)

* Pytorch 2.1.1 install path, 4bit loading

* Update README.md

* Update README.md

* Update README.md

* Update README.md

* Update README.md

* Update README.md

* Update README.md

* Update loader.py

* Update loader.py

* Update loader.py

* Update loader.py

* Spelling errors

* Update __init__.py

* DPO loss fix

* Update fast_lora.py

* Update fast_lora.py

* Out of bounds tokenization

* Fix Mistral SWA

* Prelim support Qwen, Deepseek etc"
github.com/unslothai/unsloth,unsloth/models/llama.py,2023-12-28T17:55:01Z,"DPO, SWA fixes (#57)

* Pytorch 2.1.1 install path, 4bit loading

* Update README.md

* Update README.md

* Update README.md

* Update README.md

* Update README.md

* Update README.md

* Update README.md

* Update loader.py

* Update loader.py

* Update loader.py

* Update loader.py

* Spelling errors

* Update __init__.py

* DPO loss fix

* Update fast_lora.py

* Update fast_lora.py

* Out of bounds tokenization

* Fix Mistral SWA"
github.com/unslothai/unsloth,unsloth/models/llama.py,2023-12-27T17:19:12Z,"Nightly (#56)

* Pytorch 2.1.1 install path, 4bit loading

* Update README.md

* Update README.md

* Update README.md

* Update README.md

* Update README.md

* Update README.md

* Update README.md

* Update loader.py

* Update loader.py

* Update loader.py

* Update loader.py

* Spelling errors

* Update __init__.py"
github.com/unslothai/unsloth,unsloth/models/llama.py,2023-12-26T16:51:31Z,Fix inference
github.com/unslothai/unsloth,unsloth/models/llama.py,2023-12-26T16:01:45Z,Fix FastLanguageModel
github.com/unslothai/unsloth,unsloth/models/llama.py,2023-12-25T17:32:04Z,"Fix RoPE Scaling issues (#52)

* Fix RoPE Scaling

* Update llama.py

* Update llama.py"
github.com/unslothai/unsloth,unsloth/models/llama.py,2023-12-25T12:28:05Z,Fix Pytorch 2.1.1
github.com/unslothai/unsloth,unsloth/models/llama.py,2023-12-22T17:22:48Z,"Small fixes (#48)

* Fix generation for GQA

* Update _utils.py

* flash attn

* Update _utils.py

* Update llama.py

* Update mistral.py

* platform

* Update _utils.py

* Update llama.py

* Logo changed

* Update README.md

* Update README.md"
github.com/unslothai/unsloth,unsloth/models/llama.py,2023-12-12T02:07:38Z,Update llama.py
github.com/unslothai/unsloth,unsloth/models/llama.py,2023-12-11T15:54:54Z,Update llama.py
github.com/unslothai/unsloth,unsloth/models/llama.py,2023-12-11T15:40:37Z,tokenizer pad fix
github.com/unslothai/unsloth,unsloth/models/llama.py,2023-12-11T13:57:06Z,Update llama.py
github.com/unslothai/unsloth,unsloth/models/llama.py,2023-12-11T12:58:36Z,"Mistral, GQA support"
github.com/unslothai/unsloth,unsloth/models/llama.py,2023-12-06T13:13:29Z,Fix generation
github.com/unslothai/unsloth,unsloth/models/llama.py,2023-12-05T15:59:22Z,"Pre-release 2023 December version (Mistral, Prelim DPO, WSL, bug fixes) (#16)

* Immediate bug fixes

* Update README.md

* Update README.md

* Update llama.py

* Update llama.py

* Rope Scaling and max_seq_len will change

* Update llama.py

* new images

* Update README.md

* Images

* Update README.md

* Update pyproject.toml

* GQA

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py"
github.com/unslothai/unsloth,unsloth/models/llama.py,2023-11-29T16:51:54Z,First upload of Unsloth code
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora_low_mem.py,2023-07-02T16:16:02Z,fix vocab_size after low-mem-merge
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora_low_mem.py,2023-06-25T13:47:49Z,stylistic update based on Codacy
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora_low_mem.py,2023-06-25T12:56:20Z,stylistic update based on Codacy
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora_low_mem.py,2023-06-16T03:04:20Z,Update merge_llama_with_chinese_lora_low_mem.py
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora_low_mem.py,2023-06-16T03:03:10Z,update help info
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora_low_mem.py,2023-06-16T02:58:08Z,improve naming
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora_low_mem.py,2023-06-16T02:10:02Z,"Update merge_llama_with_chinese_lora_low_mem.py

报错信息容易引起混淆，可删除"
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora_low_mem.py,2023-06-15T16:11:39Z,remove comments
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora_low_mem.py,2023-06-15T16:06:10Z,add assertions
github.com/ymcui/Chinese-LLaMA-Alpaca,scripts/merge_llama_with_chinese_lora_low_mem.py,2023-06-15T09:13:17Z,add low-mem-merge script
github.com/LianjiaTech/BELLE,train/src/entry_point/interface.py,2023-09-26T11:39:39Z,"update parameter name and bug fix (#523)

* update docker

* update parameter name and bug fix

* delete redundancy file"
github.com/LianjiaTech/BELLE,train/src/entry_point/interface.py,2023-08-11T08:35:23Z,update readme
github.com/LianjiaTech/BELLE,train/src/entry_point/interface.py,2023-08-10T10:27:54Z,add zero inference
github.com/LianjiaTech/BELLE,train/src/entry_point/interface.py,2023-06-30T07:38:17Z,add parallelize generation
github.com/LianjiaTech/BELLE,train/src/entry_point/interface.py,2023-06-25T14:50:22Z,fix bug in train & add retry in MultiClient
github.com/LianjiaTech/BELLE,train/src/entry_point/interface.py,2023-06-15T15:58:12Z,"remove peft
adjust source tree
fix peft + deepspeed save model
add client server inference"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-10-23T03:12:54Z,"added low_cpu_mem_usage

Signed-off-by: root <yanzhaodong2021@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-10-16T13:36:31Z,"Add all_devices config in AutoLoader.

Signed-off-by: ldwang <ftgreat@gmail.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-10-10T08:43:19Z,"updated loader

Signed-off-by: 严照东 <yanzhaodong2-21@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-10-10T08:30:44Z,"enabled aquila2 quantize

Signed-off-by: 严照东 <yanzhaodong2-21@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-10-09T03:35:57Z,"enabled auto torchdtype detecting

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-10-07T08:11:38Z,"fixed bug

Signed-off-by: 严照东 <yanzhaodong2-21@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-10-07T07:13:14Z,"reformat autoloader

Signed-off-by: 严照东 <yanzhaodong2-21@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-10-07T05:37:38Z,"added aquila2 finetuning

Signed-off-by: 严照东 <yanzhaodong2-21@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-09-28T09:51:07Z,"enabledaquila2 finetuning

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-09-26T07:39:15Z,"remove unused files

Signed-off-by: 严照东 <yanzhaodong2-21@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-09-26T07:21:02Z,"updated model usage method

Signed-off-by: 严照东 <yanzhaodong2-21@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-09-25T09:58:43Z,"flagai fit for aquila2

Signed-off-by: 严照东 <yanzhaodong2-21@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-07-19T08:23:58Z,"docs updated

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-06-29T02:32:49Z,"updated lora generation

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-06-29T01:44:47Z,"reorganized lora

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-06-25T10:37:32Z,"added lora inference file

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-06-25T10:14:37Z,"updated lora inference

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-06-25T06:29:06Z,"delete a line

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-06-25T06:27:22Z,"fixed fp16 and device error

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-06-14T07:34:08Z,"released 1,7,3

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-06-08T02:30:11Z,"updated docs

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-06-07T01:46:47Z,"renamed and removed some files

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-06-06T12:41:46Z,"added aquila

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-06-06T02:32:37Z,"raw add for testing

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-04-12T08:38:47Z,"name changed

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-04-12T08:12:51Z,"added AltDiffusion-m18

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-04-11T07:34:10Z,"work saved

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-03-29T03:07:21Z,Merge branch 'add_adm18' of github.com:Anhforth/FlagAI into add_adm18
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-03-29T00:39:38Z,"local conflicts resolved

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-03-28T13:24:36Z,"add altclip-m18

Signed-off-by: zhaohu xing <920232796@qq.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-03-28T10:04:53Z,"added altdiffusion_m18

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-03-20T08:40:24Z,"updated

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-03-07T07:50:42Z,"Revert ""add llama model"""
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-03-07T02:15:50Z,"add llama model

Signed-off-by: zhaohu xing <920232796@qq.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-02-28T09:19:16Z,Merge branch 'master' into add_bminf
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-02-27T07:58:46Z,add galactica model
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-02-22T09:23:58Z,"changed special token fule

Signed-off-by: Anhforth <yanzaodong2021@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-02-20T02:48:32Z,"updated

Signed-off-by: Anhforth <yanzaodong2021@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-02-15T09:38:02Z,"Add docs

Signed-off-by: Anhforth <yanzaodong2021@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-01-13T02:19:04Z,"enabled fp16 in AltDiffusion

Signed-off-by: Anhforth <yanzaodong2021@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2023-01-03T09:35:20Z,"updated

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-12-08T09:41:49Z,"fix downfile name err

Signed-off-by: zhaohu xing <920232796@qq.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-12-08T09:40:36Z,"fix downfile name err

Signed-off-by: zhaohu xing <920232796@qq.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-12-08T02:28:41Z,"load model no network

Signed-off-by: zhaohu xing <920232796@qq.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-12-02T04:11:47Z,"add alm module (#160)

* Update setup.py

* add alm model

* add alm

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* add almseq2seq model

* add almseq2seq

* fix

* add new alm

Signed-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>

* add seq2seq alm example

* Update train.py

* add rouge

* add rouge

* add rouge metric

* add bleu

* fix

* delete def

* add generate

* updated tokenizer encode_plus

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* move pdb

* fix bug

* delete pdb

* fix oom

* readjust pad token

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* adjust command token ids

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* fix bugs

* fix error

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* modified according to comments

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updated model dir

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updated readme

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* Added readme for ALM

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* Update README.md

* Update README_zh.md

* Fix generation alm (#156)

* enable normal generated random results for alm
* enabled nomal generation of alm beamsearch
* remove sys import

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updated

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* recovered

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* canceled random

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* modified reademe

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* add alm module

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updated alm

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* remove temp path

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updated version number

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updated

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* modified docs

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* changed filename

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* Update predictor.py

* removed unnecessary import

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>
Signed-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>
Co-authored-by: shunxing1234 <xw747777271@gmail.com>
Co-authored-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>
Co-authored-by: shunxing1234 <33774367+shunxing1234@users.noreply.github.com>
Co-authored-by: ZhaodongYan1 <yzdrogeryan@gmail.com>
Co-authored-by: BAAI-OpenPlatform <107522723+BAAI-OpenPlatform@users.noreply.github.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-11-28T08:46:14Z,"Add alm (#150)

* add alm model
Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>
Signed-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>
Co-authored-by: shunxing1234 <33774367+shunxing1234@users.noreply.github.com>
Co-authored-by: ZhaodongYan1 <yzdrogeryan@gmail.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-11-26T10:40:46Z, download from server
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-11-26T10:20:27Z,merge changes from the simplification of AltDiffusion loading
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-11-26T10:15:13Z,simplify the loading of AltDiffusion model
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-11-24T01:47:11Z,"add eva clip && unit test

Signed-off-by: Quan Sun <quansun84@gmail.com>
Signed-off-by: quansun <sunquan@baai.ac.cn>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-11-22T10:33:33Z,"Revert ""add_eva_clip"""
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-11-22T09:11:11Z,"add_eva_clip

Signed-off-by: quansun <sunquan@baai.ac.cn>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-11-19T09:26:19Z,"add M9 model to autoloader (#116)



Signed-off-by: zhaohu xing <920232796@qq.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-11-12T12:47:11Z,"update the documents and removed redundant files (#97)

* added diffusion model

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* fixed download path

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* added online version of diffusion

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* removed unused code

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* solved pr issue

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* solve unicode issue

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* solve unicode issue

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* fixed pr issue

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* fixed pr issue

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* fix issue

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* updated requireed packages

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updated requirement.txt

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updated import error

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updatde the import

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* seperate the cn_clip module

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updated the packages

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updatde the package versions

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updated torchvision version

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* fixed tokenizer import

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* recover change

* Delete Bert.py

* reformate the code

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* fix json read warning and reformat code

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updated the changes

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* remove local path

* added readme file

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* fix cpm3 infer bugs

* fix cpm3 model bugs

* Update README.md

* updated the diffusion README and commets

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* removed bmtrain install to avoid error

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* add xlmroberta

* added flagstudio

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* enabled online loading

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* imporved README and changed filename

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* changed the filename

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* add readme file for AltCLIP

Signed-off-by: zhaohu xing <920232796@qq.com>

* removed pdb

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* add model_dir

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* updated

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* added more requirements

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* updated pr info

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* modified readme

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* modified readme

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* added the RightBrainAI for our long image generation technology

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* updated the readme

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* fix bugs

Signed-off-by: zhaohu xing <920232796@qq.com>

* tokenizer

Signed-off-by: zhaohu xing <920232796@qq.com>

* Delete vocab.txt

* fix tokenizer bugs

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* added huggingface open-source address to docs

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* delete some files (#95)

Signed-off-by: zhaohu xing <920232796@qq.com>

Signed-off-by: zhaohu xing <920232796@qq.com>

* modified docs

Signed-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>

* modified laion link and turn parameters descriptions in a table

Signed-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>

* modify file name (#96)



Signed-off-by: zhaohu xing <920232796@qq.com>

* updated the docs

Signed-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>

* modified the docs

Signed-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>

* updated

Signed-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>

* updated the docs

Signed-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>

* solve license linke issye

Signed-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>

* modify classnames

Signed-off-by: zhaohu xing <920232796@qq.com>

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>
Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>
Signed-off-by: Anhforth <yanzhaodong2021@163.com>
Signed-off-by: zhaohu xing <920232796@qq.com>
Signed-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>
Co-authored-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>
Co-authored-by: shunxing1234 <xw747777271@gmail.com>
Co-authored-by: zhaohu xing <920232796@qq.com>
Co-authored-by: Anhforth <yanzhaodong2021@163.com>
Co-authored-by: Zac Liu <liuguang@baai.ac.cn>
Co-authored-by: zhaohu xing <32668889+920232796@users.noreply.github.com>
Co-authored-by: ZhaodongYan1 <yzdrogeryan@gmail.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-11-11T08:03:16Z,"Add AltDiffusion and AltCLIP (#90)

* added diffusion model

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* fixed download path

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* added online version of diffusion

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* removed unused code

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* solved pr issue

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* solve unicode issue

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* solve unicode issue

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* fixed pr issue

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* fixed pr issue

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* fix issue

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>

* updated requireed packages

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updated requirement.txt

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updated import error

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updatde the import

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* seperate the cn_clip module

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updated the packages

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updatde the package versions

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updated torchvision version

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* fixed tokenizer import

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* recover change

* Delete Bert.py

* reformate the code

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* fix json read warning and reformat code

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* updated the changes

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* remove local path

* added readme file

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* fix cpm3 infer bugs

* fix cpm3 model bugs

* Update README.md

* updated the diffusion README and commets

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* removed bmtrain install to avoid error

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* add xlmroberta

* added flagstudio

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* enabled online loading

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* imporved README and changed filename

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* changed the filename

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* add readme file for AltCLIP

Signed-off-by: zhaohu xing <920232796@qq.com>

* removed pdb

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* add model_dir

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* updated

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* added more requirements

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* updated pr info

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* modified readme

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* modified readme

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* added the RightBrainAI for our long image generation technology

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* updated the readme

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* fix bugs

Signed-off-by: zhaohu xing <920232796@qq.com>

* tokenizer

Signed-off-by: zhaohu xing <920232796@qq.com>

* Delete vocab.txt

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>
Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>
Signed-off-by: Anhforth <yanzhaodong2021@163.com>
Signed-off-by: zhaohu xing <920232796@qq.com>
Co-authored-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>
Co-authored-by: shunxing1234 <xw747777271@gmail.com>
Co-authored-by: zhaohu xing <920232796@qq.com>
Co-authored-by: Anhforth <yanzhaodong2021@163.com>
Co-authored-by: Zac Liu <liuguang@baai.ac.cn>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-11-02T03:29:23Z,fix tokenizer
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-11-02T02:01:50Z,add cpm3
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-10-24T09:25:42Z,"merged upstream

Signed-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-10-24T06:00:15Z,add cpm3 model
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-09-20T03:02:37Z,"Fix issue#85 (#86)

* updated the requirement packages list
* tried to fix the data directory not found error
* fixed issues in running glm_seq2seq
* Update test_glm_seq2seq.py
* fix glm tokenizer bug
* add news section in Readme
* updated docs for tokenizer
* update required packages
* fix_issue_85
* fix bugs in bert forward with different shape of attention mask
* enable offline loading
* fixed error in test_files and add robert ch tokenizer
* fix error in action
* solved Filenotfounderror
Signed-off-by: Anhforth <yanzhaodong2021@163.com>
Signed-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>
Signed-off-by: zhaohu xing <920232796@qq.com>
Signed-off-by: marscrazy <liuguang@baai.ac.cn>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-09-06T07:17:20Z,fix_issue_85
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-08-29T09:00:37Z,"Fixed errors by upper-case of model name, and changed the description (#82)


* fix a glm tokenizer bug
Signed-off-by: zhaohu xing <920232796@qq.com>
* Update tokenizer.py
Signed-off-by: Anhforth <yanzhaodong2021@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-08-29T07:54:27Z,"Added CLIP module and redesigned tokenizer apis (#81)

* merged clip tokenizer

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* Update inference_clip.py

* Update auto_loader.py

* Update glm_10b_en_tokenizer.py

* swinv1v2

Signed-off-by: zhaohu xing <920232796@qq.com>

* updated the version

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* updated the requirement packages list

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* fixed some issues

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* fixed some issues

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* tried to fix the data directory not found error

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* fixed issues in running glm_seq2seq

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>

* Update test_glm_seq2seq.py

* Update setup.py

Signed-off-by: Anhforth <yanzhaodong2021@163.com>
Signed-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>
Signed-off-by: zhaohu xing <920232796@qq.com>
Signed-off-by: shunxing1234 <xw747777271@gmail.com>
Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>
Co-authored-by: Anhforth <yanzhaodong2021@163.com>
Co-authored-by: zhaohu xing <32668889+920232796@users.noreply.github.com>
Co-authored-by: Zhaodong Yan <94831503+Anhforth@users.noreply.github.com>
Co-authored-by: ZhaodongYan1 <yzdrogeryan@gmail.com>
Co-authored-by: Zac Liu <liuguang@baai.ac.cn>
Co-authored-by: zhaohu xing <920232796@qq.com>
Co-authored-by: jongjyh <jongjyh0221@gmail.com>
Co-authored-by: wchh-2000 <2371156095@qq.com>
Co-authored-by: xuanricheng <xuanricheng@hotmail.com>
Co-authored-by: shunxing1234 <xw747777271@gmail.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-08-25T07:56:50Z,"swinv1v2

Signed-off-by: zhaohu xing <920232796@qq.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-08-25T01:37:51Z,Update auto_loader.py
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-08-23T01:29:21Z,"merged clip tokenizer

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-08-22T07:57:39Z,"merged the clip tokenizer

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-08-19T02:30:04Z,"updated according to comments

Signed-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-08-18T07:35:47Z,Merge branch 'master' into transform_tokenizer
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-08-16T08:35:10Z,update
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-08-15T07:23:53Z,add autoloader and example training data
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-08-07T01:23:09Z,"add cpm model

Signed-off-by: marscrazy <liuguang@baai.ac.cn>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-07-29T01:32:32Z,inference and train
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-07-26T02:29:35Z,"merged the master

Signed-off-by: Anhforth <yanzhaodong2021@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-07-21T13:11:52Z,"Develop (#71)



* add vit and examples

* vit and examples

* Update base_model.py

remove unused glob

* Update vit.py

remove data statis

* modify readme.md

Signed-off-by: zhaohu xing <920232796@qq.com>

* modify readme.md

Signed-off-by: zhaohu xing <920232796@qq.com>

* delete annotating code

Signed-off-by: zhaohu xing <920232796@qq.com>

* Vit xzh (#25)

* add vit and examples

* vit and examples

* Update base_model.py

remove unused glob

* Update vit.py

remove data statis

* modify readme.md

Signed-off-by: zhaohu xing <920232796@qq.com>

* modify readme.md

Signed-off-by: zhaohu xing <920232796@qq.com>

* delete annotating code

Signed-off-by: zhaohu xing <920232796@qq.com>

Co-authored-by: Zac Liu <liuguang@baai.ac.cn>

* env trainer

Signed-off-by: zhaohu xing <920232796@qq.com>

* Create README.md

fix typo in flagai introduction.

* vit-checkpoint-activations

Signed-off-by: zhaohu xing <920232796@qq.com>

* vit-checkpoint-activations

Signed-off-by: zhaohu xing <920232796@qq.com>

Co-authored-by: zhaohu xing <32668889+920232796@users.noreply.github.com>
Co-authored-by: Zhaodong Yan <94831503+Anhforth@users.noreply.github.com>
Co-authored-by: Zac Liu <liuguang@baai.ac.cn>
Co-authored-by: Anhforth <yanzhaodong2021@163.com>
Co-authored-by: zhaohu xing <920232796@qq.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-07-19T03:22:17Z,Merge branch 'develop' into vit_xzh
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-07-15T09:57:37Z,add vit and examples
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-07-15T06:08:41Z,"modified encoder_plus

Signed-off-by: Anhforth <yanzhaodong2021@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-07-15T01:39:53Z,"updated

Signed-off-by: Anhforth <yanzhaodong2021@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-07-11T06:42:25Z,"Develop (#66)



* fix bug multi_gpu_training

* Update trainer.py

remove comment

* changed the version

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* Update trainer.py

* fix_validation_bug (#24)

* updated the version

Signed-off-by: Anhforth <yanzhaodong2021@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-07-11T05:37:05Z,fix_validation_bug (#24)
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-07-07T10:11:30Z,"fixed conflicts

Signed-off-by: Anhforth <yanzhaodong2021@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-07-07T10:05:20Z,"test of tokenizer transform

Signed-off-by: Anhforth <yanzhaodong2021@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-07-06T09:38:48Z,"fixed tokenizer issues (#63)

* Opt 30b (#16)
* clean codes 
Co-authored-by: Zac Liu <liuguang@baai.ac.cn>
* fix bert tokenizer issue (#18)
* fix bert tokenizer issue
* updated t5, opt and roberta tokenizers
* fixed doc 404 error
Signed-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>
* Opt 66b (#19)
* autoloader for opt
* opt-66b inference
* Update train.py
* Load data from example dir
* add readme of multi GPU inference
Co-authored-by: Zac Liu <liuguang@baai.ac.cn>
* updated release version
Signed-off-by: Anhforth <yanzhaodong2021@163.com>
* fix tokenizer issue
Signed-off-by: Anhforth <yanzhaodong2021@163.com>
Co-authored-by: zhaohu xing <32668889+920232796@users.noreply.github.com>
Co-authored-by: Zhaodong Yan <94831503+Anhforth@users.noreply.github.com>
Co-authored-by: Zac Liu <liuguang@baai.ac.cn>
Co-authored-by: Anhforth <yanzhaodong2021@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-07-06T06:19:29Z,"Opt 66b (#19)

* autoloader for opt
* opt-66b inference
* Update train.py
* Load data from example dir
* add readme of multi GPU inference

Co-authored-by: Zac Liu <liuguang@baai.ac.cn>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-07-01T05:40:17Z,"transformed tokenizer: progressing

Signed-off-by: Anhforth <yanzhaodong2021@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-06-28T09:38:57Z,"fixed tokenizer in glm blankfilling (#13)

* fixed tokenizer in glm blankfilling

Signed-off-by: Anhforth <yanzhaodong2021@163.com>

* Update glm_generate_samples_en.py

Co-authored-by: Anhforth <yanzhaodong2021@163.com>
Co-authored-by: Zac Liu <liuguang@baai.ac.cn>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-06-24T05:52:51Z,"test merging upstream Develop to master (#5)

fix bugs in distributed training & evaluating

Co-authored-by: Zac Liu <liuguang@baai.ac.cn>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-06-24T05:03:50Z,"Add opt (#9)

fix bug in autoloader

Signed-off-by: zhaohu xing <920232796@qq.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-06-24T02:43:30Z,"change the save dirname for autoloader，keep the dirname == cased modelname (#8)


* fix autoloader
Signed-off-by: zhaohu xing <920232796@qq.com>
Co-authored-by: Zac Liu <liuguang@baai.ac.cn>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-06-23T14:16:24Z,"Add OPT models  (#6)

* added opt tokenizer and updated cpm1 tokenizer
* fixed superglue traininng error
Co-authored-by: Anhforth <yanzhaodong2021@163.com>
* fixed auto_loader
* opt model
* add opt examples, tests
* add opt_6.7b
* opt-13b
* opt-13b-en
Signed-off-by: zhaohu xing <920232796@qq.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-06-19T18:42:49Z,"Release v1.0.4 (#61)

* add gradient accumulation and fix bugs in distributed evaluation
* fix bugs in accumulate gradients in DDP / Pytorch / DeepSpeed
* release v1.0.4
* set epoch for distributed sampler
* add warnings for DDP with FP16
* change save_epoch to save_interval
* code formate with yapf
* fix typos in utils.py
* del train_test.py
* fix bugs while test seq2seq task with  rather than
* the glm superglue need some works
Signed-off-by: marscrazy <marscrazy_90@163.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-06-10T04:00:46Z,"fix_cpm

Signed-off-by: zhaohu xing <920232796@qq.com>"
github.com/FlagAI-Open/FlagAI,flagai/auto_model/auto_loader.py,2022-06-01T15:23:40Z,init
github.com/microsoft/LMOps,minillm/transformers/src/transformers/trainer.py,2023-11-16T12:47:19Z,upgrade transformers
github.com/microsoft/LMOps,minillm/transformers/src/transformers/trainer.py,2023-08-16T03:11:32Z,upgrade transformers
github.com/microsoft/LMOps,minillm/transformers/src/transformers/trainer.py,2023-06-14T11:44:35Z,add minillm
github.com/LianjiaTech/BELLE,train/src/merge_llama_with_lora.py,2023-05-09T10:40:42Z,Update train v2
github.com/pengxiao-song/LaWGPT,utils/evaluate.py,2023-05-21T18:39:04Z,[ENH] Restructure the project.
github.com/liguodongiot/llm-action,train/alpaca-lora/generate.py,2023-07-23T11:43:58Z,fix
github.com/huggingface/alignment-handbook,scripts/run_dpo.py,2024-03-01T16:29:42Z,"🪁 (#129)

* Add Gemma 7B recipe

* Use Gemma template

* Make it work for dolly lol

* Enable cahce

* Clean up

* DPO to the max

* DPO, DPO, DPO

* Add openhermes

* Add custom configs

* Add kwargs

* Fix config

* Bump deps

* Move old recipes

* Add doc

* Add norte

* Renable cache

* Nuke

* Clean

* Apply suggestions from code review

Co-authored-by: Alvaro Bartolome <alvaro@argilla.io>

* Fix isort

* Update README.md

* Update config_full.yaml

---------

Co-authored-by: Alvaro Bartolome <alvaro@argilla.io>
Co-authored-by: Philipp Schmid <32632186+philschmid@users.noreply.github.com>"
github.com/huggingface/alignment-handbook,scripts/run_dpo.py,2024-02-28T19:05:44Z,"Add `auto_insert_empty_system_msg` config flag (#123)

* Make system messages optional

Also use the `maybe_insert_system_message` in dpo setting

* add `auto_insert_empty_system_msg` flag

* add `auto_insert_empty_system_msg`

* add auto_insert_empty_system_msg

* Update src/alignment/configs.py

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* make style

---------

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>"
github.com/huggingface/alignment-handbook,scripts/run_dpo.py,2024-02-05T15:50:17Z,"Apply quantization during DPO QLoRA (#115)

* Add QLoRA fix

* Update script"
github.com/huggingface/alignment-handbook,scripts/run_dpo.py,2024-01-10T06:42:24Z,"Update Zephyr configs to account for UltraFeedback & TRL fixes (#88)

* Add files

* Add checkpointing

* Add checkpointing to SFT

* Add loss type

* Fix setup|

* Clean SFT

* Add lora config

* Rename config

* Remove max eval samples

* Add kwargs tp push to hub

* Add DPO configs

* Fix dpo configs

* Extend chat template test to multi-turn

* Add warmup

* Refactor

* Fix LoRA -> QLoRA

* Fix configs

* Specify chat template

* Add sample logging

* Fix push to hub hanging

* Add reentrant

* Fix quality

* Add transformer logging

* Tweak grad acc

* Add null type

* Add doc"
github.com/huggingface/alignment-handbook,scripts/run_dpo.py,2024-01-04T22:06:47Z,Clean deprecated max_samples arguments (#89)
github.com/huggingface/alignment-handbook,scripts/run_dpo.py,2023-12-04T08:10:41Z,"Add check that parameters are not intended to be offloaded (#51)

* Add check that parameters are not intended to be offloaded

* Only push model to device if quantization config is set."
github.com/huggingface/alignment-handbook,scripts/run_dpo.py,2023-11-10T13:38:45Z,Refactor imports
github.com/huggingface/alignment-handbook,scripts/run_dpo.py,2023-11-10T13:15:44Z,adds auto adapter merge to dpo script
github.com/huggingface/alignment-handbook,scripts/run_dpo.py,2023-11-09T13:39:03Z,Add more doc
github.com/huggingface/alignment-handbook,scripts/run_dpo.py,2023-11-08T22:58:34Z,Make DPO work!
github.com/huggingface/alignment-handbook,scripts/run_dpo.py,2023-11-08T13:21:57Z,Add skeleton
github.com/liguodongiot/llm-action,train/qlora/inference_merge.py,2023-07-23T11:43:58Z,fix
github.com/liguodongiot/llm-action,train/qlora/inference_qlora.py,2023-07-23T11:43:58Z,fix
github.com/dvlab-research/LongLoRA,demo.py,2023-11-05T16:58:41Z,fix starting token repetition
github.com/dvlab-research/LongLoRA,demo.py,2023-10-30T15:24:42Z,Update demo.py
github.com/dvlab-research/LongLoRA,demo.py,2023-10-26T10:08:36Z,[demo] add ip and port input
github.com/dvlab-research/LongLoRA,demo.py,2023-10-09T14:00:53Z,"Update demo.py

Added 'skip_special_tokens=True' to remove special tokens from the output."
github.com/dvlab-research/LongLoRA,demo.py,2023-10-09T11:30:35Z,"Update demo.py

Added skip_prompt argument to TextStreamer"
github.com/dvlab-research/LongLoRA,demo.py,2023-10-09T10:59:19Z,Update demo.py
github.com/dvlab-research/LongLoRA,demo.py,2023-10-09T10:20:31Z,"Update demo.py

- Generated words are displayed immediately by using TextStreamer.

- Removed examples as the example .txt files are not there."
github.com/dvlab-research/LongLoRA,demo.py,2023-10-09T08:17:03Z,Update demo.py
github.com/dvlab-research/LongLoRA,demo.py,2023-10-03T03:43:44Z,Update demo.py
github.com/dvlab-research/LongLoRA,demo.py,2023-10-03T03:12:47Z,Update demo.py
github.com/dvlab-research/LongLoRA,demo.py,2023-09-23T05:35:05Z,Add files via upload
github.com/dvlab-research/LongLoRA,demo.py,2023-09-21T18:13:56Z,Add files via upload
github.com/AutoGPTQ/AutoGPTQ,auto_gptq/utils/peft_utils.py,2024-02-13T16:06:07Z,"Use ruff for linting (#537)

* lint

* add back init

* remove pyproject.toml that automatically triggers build isolation

* install torch

* maybe use 3.9?

* debug

* upgrade setuptools

* maybe ubuntu 22.04?

* wheel

* working now?

* typog

* indent

* fix indent

* do not use powershell

* free space

* fix cuda path

* prints

* where is conda?

* should finally work

* fix

* final fixN

* arch ist

* typog

* add quality extra

* last fix"
github.com/AutoGPTQ/AutoGPTQ,auto_gptq/utils/peft_utils.py,2023-11-08T11:41:25Z,"Fix windows (no triton) and cpu-only support (#411)

* fix windows (no triton) and cpu-only support

From: fxmarty <9808326+fxmarty@users.noreply.github.com>
From: Yang Wang <yang3.wang@intel.com>

* Add co-author

Co-authored-by: Yang Wang <yang3.wang@intel.com>

* fix tests when cuda ext are not installed

---------

Co-authored-by: Yang Wang <yang3.wang@intel.com>"
github.com/AutoGPTQ/AutoGPTQ,auto_gptq/utils/peft_utils.py,2023-11-07T16:16:52Z,fix windows support (#407)
github.com/AutoGPTQ/AutoGPTQ,auto_gptq/utils/peft_utils.py,2023-10-27T07:12:16Z,"PEFT initialization fix (#361)

* Initial code for GPTQLoraLinear initialization

* Working AdaLora

* remove unused methods and pin peft

---------

Co-authored-by: Felix Marty <9808326+fxmarty@users.noreply.github.com>"
github.com/AutoGPTQ/AutoGPTQ,auto_gptq/utils/peft_utils.py,2023-09-24T13:11:19Z,Use `adapter_name` for `get_gptq_peft_model` with `train_mode=True`
github.com/AutoGPTQ/AutoGPTQ,auto_gptq/utils/peft_utils.py,2023-05-30T15:03:51Z,remove  raise
github.com/AutoGPTQ/AutoGPTQ,auto_gptq/utils/peft_utils.py,2023-05-28T14:35:34Z,raise NotImplementedError when model with fused attention injected try to use ADAPTION_PROMPT peft type
github.com/AutoGPTQ/AutoGPTQ,auto_gptq/utils/peft_utils.py,2023-05-28T14:11:02Z,reset value of AdaptionPromptConfig.adapter_layers to number of model's hidden layers when exceeds
github.com/AutoGPTQ/AutoGPTQ,auto_gptq/utils/peft_utils.py,2023-05-28T13:30:45Z,support AdaLora
github.com/AutoGPTQ/AutoGPTQ,auto_gptq/utils/peft_utils.py,2023-05-28T09:36:18Z,make GPTQLoraModel to inherit from LoraModel to simplify code
github.com/AutoGPTQ/AutoGPTQ,auto_gptq/utils/peft_utils.py,2023-05-28T09:04:38Z,add 'auto_find_all_linears' argument to get_gptq_peft_model function
github.com/AutoGPTQ/AutoGPTQ,auto_gptq/utils/peft_utils.py,2023-05-28T08:57:31Z,add warning to guide users interact with lora properly
github.com/AutoGPTQ/AutoGPTQ,auto_gptq/utils/peft_utils.py,2023-05-26T23:49:17Z,"add find_all_linear_names help function, make customized lora module more general"
github.com/AutoGPTQ/AutoGPTQ,auto_gptq/utils/peft_utils.py,2023-05-26T06:06:53Z,set xavier_uniform_ as lora_A's init function
github.com/AutoGPTQ/AutoGPTQ,auto_gptq/utils/peft_utils.py,2023-05-25T23:18:16Z,refactor file structure of qlinears
github.com/AutoGPTQ/AutoGPTQ,auto_gptq/utils/peft_utils.py,2023-05-25T11:44:53Z,lora compatibility
github.com/AutoGPTQ/AutoGPTQ,auto_gptq/utils/peft_utils.py,2023-05-25T07:11:11Z,first upload peft_utils.py
github.com/tloen/alpaca-lora,export_state_dict_checkpoint.py,2023-04-09T21:07:59Z,"Update export_hf_checkpoint.py (#302)

* Update export_hf_checkpoint.py

* Update finetune.py

New tokenizer base model for the current dev branch of transformers

* Update generate.py

* Update export_state_dict_checkpoint.py

* Update export_hf_checkpoint.py"
github.com/tloen/alpaca-lora,export_state_dict_checkpoint.py,2023-03-28T15:33:47Z,remove asserts
github.com/tloen/alpaca-lora,export_state_dict_checkpoint.py,2023-03-27T17:31:44Z,"Add HF dataset loading, add linters, pyproject.toml (#175)

* add HF dataset loading, add linters, pyproject.toml

- applied markdownlint
- add black, black[jupyter], isort
- fix noqa codes
- add .github workflow linting
- update README.md

* restore default settings

* resume_from_checkpoint

Co-authored-by: AngainorDev <54739135+AngainorDev@users.noreply.github.com>

* Print warning on checkpoint not found

* add HF dataset loading, add linters, pyproject.toml

- applied markdownlint
- add black, black[jupyter], isort
- fix noqa codes
- add .github workflow linting
- update README.md

* Default to local copy and update it

* Typo

* Remove duplicate code block

---------

Co-authored-by: Eric Wang <eric.james.wang@gmail.com>
Co-authored-by: AngainorDev <54739135+AngainorDev@users.noreply.github.com>"
github.com/tloen/alpaca-lora,export_state_dict_checkpoint.py,2023-03-23T20:54:39Z,"Remove LLaMA download code, as a precaution"
github.com/tloen/alpaca-lora,export_state_dict_checkpoint.py,2023-03-16T19:11:47Z,Catch outdated installs
github.com/tloen/alpaca-lora,export_state_dict_checkpoint.py,2023-03-16T19:11:29Z,Update alpaca-lora to use transformers main branch
github.com/tloen/alpaca-lora,export_state_dict_checkpoint.py,2023-03-16T07:50:24Z,Fix LoRa weight merging
github.com/tloen/alpaca-lora,export_state_dict_checkpoint.py,2023-03-16T00:17:32Z,Add script for converting weights from HF
github.com/Facico/Chinese-Vicuna,tools/merge_lora.py,2023-04-02T06:36:48Z,path typo
github.com/Facico/Chinese-Vicuna,tools/merge_lora.py,2023-03-27T14:51:27Z,merge changes for cpp inference
github.com/Facico/Chinese-Vicuna,tools/merge_lora.py,2023-03-27T14:41:25Z,"Revert ""Merge branch 'chatglm' of ../Chinese-Vicuna""

This reverts commit ca4f9ae8f1f2534ece5694a73457eda44537dc4b, reversing
changes made to 1a788ea8952b6c3df6550f9e69c986794e45a0ca."
github.com/Facico/Chinese-Vicuna,tools/merge_lora.py,2023-03-27T14:39:43Z,update cpp inference files
github.com/dvlab-research/LongLoRA,eval.py,2023-10-31T15:03:45Z,Update eval.py
github.com/dvlab-research/LongLoRA,eval.py,2023-10-04T02:57:32Z,Update eval.py
github.com/dvlab-research/LongLoRA,eval.py,2023-09-21T18:13:56Z,Add files via upload
github.com/liguodongiot/llm-action,train/chatglm-lora/inference.py,2023-07-23T11:43:58Z,fix
github.com/deep-diver/LLM-As-Chatbot,models/byom.py,2023-06-11T13:09:39Z,fix: remove force download option
github.com/deep-diver/LLM-As-Chatbot,models/byom.py,2023-06-11T12:54:36Z,fix: byom.py to take modes
github.com/deep-diver/LLM-As-Chatbot,models/byom.py,2023-06-08T01:59:11Z,exp w/ camel and sam
github.com/deep-diver/LLM-As-Chatbot,models/byom.py,2023-06-04T14:50:44Z,disable force download & example view
github.com/deep-diver/LLM-As-Chatbot,models/byom.py,2023-05-31T07:55:49Z,add byom feature
github.com/unslothai/unsloth,unsloth/models/loader.py,2024-02-26T14:42:10Z,"2.4x faster Gemma (#197)

* Update save.py

* Update save.py

* linking

* llama.cpp bugs

* Update save.py

* Update save.py

* saving

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update __init__.py

* Update save.py

* Update save.py

* Update save.py

* save

* trainer

* spaces

* original

* Gemma

* Update pyproject.toml

* Update mapper.py

* Update fast_lora.py

* FastGemmaModel

* model_type

* Update llama.py

* Update llama.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update llama.py

* Update fast_lora.py

* Update llama.py

* Update llama.py

* Update cross_entropy_loss.py

* Update llama.py

* Update llama.py

* gemma

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update fast_lora.py

* Update fast_lora.py

* Fast CE Loss

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* CE

* Update llama.py

* Update llama.py

* Update cross_entropy_loss.py

* Update geglu.py

* Update cross_entropy_loss.py

* revert

* Update llama.py

* Update llama.py

* norm

* Update gemma.py

* Update gemma.py

* position_ids

* Update gemma.py

* Update gemma.py

* pos

* Update llama.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update cross_entropy_loss.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* revert

* revert

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update cross_entropy_loss.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* rope

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* llama

* Update llama.py

* gemma

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update gemma.py

* Update save.py

* RoPE

* Update llama.py

* Update llama.py

* Update llama.py

* Update gemma.py

* correct_dtype

* Update gemma.py

* Update cross_entropy_loss.py

* Update cross_entropy_loss.py

* Chat Templates

* Update README.md

* Update README.md"
github.com/unslothai/unsloth,unsloth/models/loader.py,2024-02-20T16:58:59Z,"Feb 2024 Release (#187)

* Fast inference repatch

* Update llama.py

* Update utils.py

* Update utils.py

* Update utils.py

* Update mistral.py

* Update __init__.py

* Fix inference

* Update mistral.py

* fast lm_head

* Remove fast path

* Update rope_embedding.py

* Update loader.py

* LlamaAttention_fast_forward_inference

* if past_key_value is not None and q_len == 1:

* revert inference

* Update loader.py

* past_key_value

* Update llama.py

* Update llama.py

* Fix SDPA

* Update llama.py

* padding

* Inference

* Update llama.py

* Revert

* Update mistral.py

* faster inference

* inference

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* inference

* Update llama.py

* Update utils.py

* faster inference

* Update llama.py

* revert

* lm_head

* Update llama.py

* inference

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* faster inference

* Update llama.py

* fast inference

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* torch compile

* past_key_values

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update utils.py

* Update utils.py

* Update utils.py

* Update utils.py

* Update llama.py

* fast inference + saving config.json

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update mistral.py

* fast inference again

* more temp matrices

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* fast inference

* Update mistral.py

* Update llama.py

* SDPA

* attention_mask

* New version

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update utils.py

* Update utils.py

* Update save.py

* Update save.py

* Torch 2.2.0

* Update save.py

* mistral swa

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Fix SWA inference

* Fix llm_int8_skip_modules

* SWA inference

* Update save.py

* Update save.py

* Update pyproject.toml

* __version__

* __version__

* Update save.py

* Update save.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Chat Templates

* Update chat_templates.py

* Update chat_templates.py

* Update chat_templates.py

* Update chat_templates.py

* patch tokenizer

* Update chat_templates.py

* Saving, LlamaRotaryEmbedding issues

* Update llama.py

* Update mistral.py

* Update mapper.py

* Fix RoPE precision issues

* Bugs

* saving bugs

* Update llama.py

* readme

* spaces

* spaces

* globals

* slash

* slashes

* spaces

* apache

* Update save.py

* Update save.py

* Update loader.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* trainer

* Update save.py

* Update pyproject.toml

* install

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* PeftModel token + saving

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* linking

* llama.cpp bugs

* Update save.py

* Update save.py

* saving

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update __init__.py

* Update save.py

* Update save.py

* Update save.py

* save

* trainer

* spaces

* original"
github.com/unslothai/unsloth,unsloth/models/loader.py,2024-02-13T07:28:42Z,add HF tagging in unsloth (#170)
github.com/unslothai/unsloth,unsloth/models/loader.py,2024-02-06T17:40:50Z,"Torch 2.2 (#157)

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update save.py

* Update fast_lora.py

* Update utils.py

* Update llama.py

* Update fast_lora.py

* Update swiglu.py

* Update save.py

* Update save.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Revert ""Update llama.py""

This reverts commit a208ec46e012cf470ecefe6268a66358215df7b6.

* Update llama.py

* Works?

* Update pyproject.toml

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Swiglu

* Update swiglu.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update swiglu.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* Update fast_lora.py

* attention_mask

* Update llama.py

* Update llama.py

* labels

* Update mistral.py

* Update llama.py

* attention mask

* Update save.py

* Update save.py

* Update mistral.py

* attention mask

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update dpo.py

* Patch saving

* Update save.py

* Update save.py

* patch_saving_functions

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* print

* Mistral patch

* Update mistral.py

* Update save.py

* saving

* Update llama.py

* Update llama.py

* Fast inference repatch

* Update llama.py

* Update utils.py

* Update utils.py

* Update utils.py

* Update mistral.py

* Update __init__.py

* Fix inference

* Update mistral.py

* fast lm_head

* Remove fast path

* Update rope_embedding.py

* Update loader.py

* LlamaAttention_fast_forward_inference

* if past_key_value is not None and q_len == 1:

* revert inference

* Update loader.py

* past_key_value

* Update llama.py

* Update llama.py

* Fix SDPA

* Update llama.py

* padding

* Inference

* Update llama.py

* Revert

* Update mistral.py

* faster inference

* inference

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* inference

* Update llama.py

* Update utils.py

* faster inference

* Update llama.py

* revert

* lm_head

* Update llama.py

* inference

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* faster inference

* Update llama.py

* fast inference

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* torch compile

* past_key_values

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update utils.py

* Update utils.py

* Update utils.py

* Update utils.py

* Update llama.py

* fast inference + saving config.json

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update mistral.py

* fast inference again

* more temp matrices

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* fast inference

* Update mistral.py

* Update llama.py

* SDPA

* attention_mask

* New version

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update utils.py

* Update utils.py

* Update save.py

* Update save.py

* Torch 2.2.0

* Update save.py

* mistral swa

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Fix SWA inference

* Fix llm_int8_skip_modules

* SWA inference

* Update save.py

* Update save.py

* Update pyproject.toml

* __version__

* __version__

* Update save.py

* Update save.py

* Update mistral.py"
github.com/unslothai/unsloth,unsloth/models/loader.py,2024-01-25T17:19:17Z,"Fix bugs (#129)

* faster saving & inference

* Update llama.py

* Update save.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update mistral.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* fast inference

* Update llama.py

* Update save.py

* Update llama.py

* Mistral correct RoPE scaling

* Max sequence lengths

* Apache 2

* fast_linear_forward

* Update utils.py

* Update utils.py

* No print

* Update utils.py

* Update utils.py

* inference

* Update llama.py

* Fast inference RoPE

* Update llama.py

* Update llama.py

* RoPE

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* Update llama.py

* LoRA

* Fast LoRA saving

* Update llama.py

* hidden_states

* q_len == 1

* q_len issue

* Update mistral.py

* Update mistral.py

* incorrect inference

* Update to transformers 4.37

* Graceful FA2 error + torch 2.1.1

* Update mapper.py

* Update pyproject.toml

* Fix saving and bnb-4bit

* Update fast_lora.py

* Update fast_lora.py

* remove patching

* Update llama.py

* Update llama.py

* Update swiglu.py

* Repatch

* Update fast_lora.py"
github.com/unslothai/unsloth,unsloth/models/loader.py,2024-01-20T12:23:00Z,"Hotfix for Jan 2024 Release (#110)

* Fix tokenizer, dropout, bias for LoRA

* Update loader.py

* Fix LoRA downcasting

* Update _utils.py

* Saving to GGUF

* fix

* colab_quantize_to_gguf

* move save modules

* save module

* Update __init__.py

* Update save.py

* Temp downgrade due to TRL issue

* Fix up bugs

* Faster saving + other changes

* Update llama.py

* Saving modules

* spelling

* Update llama.py

* Update save.py

* Update save.py

* Update loader.py

* Update llama.py

* patch saving

* Update save.py

* Update save.py

* Update save.py

* patch saving

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* original_model

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* saving to RAM leakage?

* Update save.py

* new_save_directory

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update pyproject.toml

* Update pyproject.toml

* Update pyproject.toml

* Quick fixes

* Update llama.py

* Update llama.py

* Update dpo.py

* Update dpo.py

* Update llama.py

* Update save.py

* getattr

* RSLoRA and LoftQ direct support

* Update llama.py

* Update llama.py

* Update llama.py

* Fix DPO + GGUF

* Fix quantization_method

* Fix quantization_config

* patch model

* Update llama.py

* Update llama.py

* Update llama.py

* Update save.py

* Update save.py

* tokenizer_save_settings

* Update save.py

* quantization and loftq

* Update save.py

* Update llama.py

* Update save.py"
github.com/unslothai/unsloth,unsloth/models/loader.py,2024-01-19T11:52:30Z,"Quick fixes (#101)

* Fix tokenizer, dropout, bias for LoRA

* Update loader.py

* Fix LoRA downcasting

* Update _utils.py

* Saving to GGUF

* fix

* colab_quantize_to_gguf

* move save modules

* save module

* Update __init__.py

* Update save.py

* Temp downgrade due to TRL issue

* Fix up bugs

* Faster saving + other changes

* Update llama.py

* Saving modules

* spelling

* Update llama.py

* Update save.py

* Update save.py

* Update loader.py

* Update llama.py

* patch saving

* Update save.py

* Update save.py

* Update save.py

* patch saving

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* original_model

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* saving to RAM leakage?

* Update save.py

* new_save_directory

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update pyproject.toml

* Update pyproject.toml

* Update pyproject.toml

* Quick fixes

* Update llama.py

* Update llama.py

* Update dpo.py

* Update dpo.py

* Update llama.py

* Update save.py"
github.com/unslothai/unsloth,unsloth/models/loader.py,2024-01-18T17:51:19Z,"2024 Release (#96)

* Fix tokenizer, dropout, bias for LoRA

* Update loader.py

* Fix LoRA downcasting

* Update _utils.py

* Saving to GGUF

* fix

* colab_quantize_to_gguf

* move save modules

* save module

* Update __init__.py

* Update save.py

* Temp downgrade due to TRL issue

* Fix up bugs

* Faster saving + other changes

* Update llama.py

* Saving modules

* spelling

* Update llama.py

* Update save.py

* Update save.py

* Update loader.py

* Update llama.py

* patch saving

* Update save.py

* Update save.py

* Update save.py

* patch saving

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* original_model

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* saving to RAM leakage?

* Update save.py

* new_save_directory

* Update save.py

* Update save.py

* Update save.py

* Update save.py

* Update pyproject.toml

* Update pyproject.toml

* Update pyproject.toml"
github.com/unslothai/unsloth,unsloth/models/loader.py,2024-01-09T14:02:44Z,fix_tokenizer
github.com/unslothai/unsloth,unsloth/models/loader.py,2024-01-09T12:40:43Z,check_tokenizer
github.com/unslothai/unsloth,unsloth/models/loader.py,2024-01-06T08:13:39Z,"Fix tokenizer, bias, dropout supported for LoRA (#69)

* Fix tokenizer, dropout, bias for LoRA

* Update loader.py"
github.com/unslothai/unsloth,unsloth/models/loader.py,2023-12-31T07:36:43Z,PatchDPOTrainer
github.com/unslothai/unsloth,unsloth/models/loader.py,2023-12-31T06:57:45Z,DPO
github.com/unslothai/unsloth,unsloth/models/loader.py,2023-12-27T17:19:12Z,"Nightly (#56)

* Pytorch 2.1.1 install path, 4bit loading

* Update README.md

* Update README.md

* Update README.md

* Update README.md

* Update README.md

* Update README.md

* Update README.md

* Update loader.py

* Update loader.py

* Update loader.py

* Update loader.py

* Spelling errors

* Update __init__.py"
github.com/unslothai/unsloth,unsloth/models/loader.py,2023-12-26T16:01:45Z,Fix FastLanguageModel
github.com/unslothai/unsloth,unsloth/models/loader.py,2023-12-22T17:22:48Z,"Small fixes (#48)

* Fix generation for GQA

* Update _utils.py

* flash attn

* Update _utils.py

* Update llama.py

* Update mistral.py

* platform

* Update _utils.py

* Update llama.py

* Logo changed

* Update README.md

* Update README.md"
github.com/unslothai/unsloth,unsloth/models/loader.py,2023-12-17T17:23:16Z,"Torch version, docs, readme, general loader"
github.com/huggingface/peft,tests/test_poly.py,2024-02-14T15:43:47Z,"TST Use plain asserts in tests (#1448)

Use pytest style asserts instead of unittest methods.

Use `pytest.raises` and `pytest.warns` where suitable."
github.com/huggingface/peft,tests/test_poly.py,2024-01-12T16:19:12Z,"FEAT Add Poly Adapter (#1129)

Implement the Poly (Polytropon) adapter.

Papers:

- https://arxiv.org/abs/2202.13914
- https://arxiv.org/abs/2211.03831

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/OpenBMB/BMTools,bmtools/models/lora_model.py,2023-06-05T08:47:12Z,add lora model
github.com/OpenBMB/BMTools,bmtools/models/lora_model.py,2023-06-05T08:27:01Z,add lora model
github.com/LlamaFamily/Llama-Chinese,train/merge_peft_model/merge_muilt_peft_adapter.py,2023-08-16T06:54:49Z,add merge model
github.com/microsoft/LMOps,minillm/transformers/src/transformers/modeling_utils.py,2023-11-16T12:47:19Z,upgrade transformers
github.com/microsoft/LMOps,minillm/transformers/src/transformers/modeling_utils.py,2023-08-16T03:11:32Z,upgrade transformers
github.com/microsoft/LMOps,minillm/transformers/src/transformers/modeling_utils.py,2023-06-14T11:44:35Z,add minillm
github.com/LianjiaTech/BELLE,train/src/entry_point/zero_inference.py,2023-09-26T11:39:39Z,"update parameter name and bug fix (#523)

* update docker

* update parameter name and bug fix

* delete redundancy file"
github.com/LianjiaTech/BELLE,train/src/entry_point/zero_inference.py,2023-08-10T10:27:54Z,add zero inference
github.com/huggingface/peft,tests/test_config.py,2024-02-14T15:43:47Z,"TST Use plain asserts in tests (#1448)

Use pytest style asserts instead of unittest methods.

Use `pytest.raises` and `pytest.warns` where suitable."
github.com/huggingface/peft,tests/test_config.py,2024-02-07T11:52:35Z,MNT Move code quality fully to ruff (#1421)
github.com/huggingface/peft,tests/test_config.py,2024-01-29T06:25:01Z,add peft type constructor (#1398)
github.com/huggingface/peft,tests/test_config.py,2024-01-12T16:19:12Z,"FEAT Add Poly Adapter (#1129)

Implement the Poly (Polytropon) adapter.

Papers:

- https://arxiv.org/abs/2202.13914
- https://arxiv.org/abs/2211.03831

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_config.py,2023-12-06T14:11:00Z,"Release: 0.7.0 (#1214)

In preparation for the 0.7.0 release. Also remove obsolete TODO
comments."
github.com/huggingface/peft,tests/test_config.py,2023-11-30T15:58:42Z,"[Feature] Support OFT (#1160)

* Support OFT

* add test

* Update README

* fix code quality

* fix test

* Skip 1 test

* fix eps rule and add more test

* feat: added examples to new OFT method

* fix: removed wrong arguments from model example

* fix: changed name of inference file

* fix: changed prompt variable

* fix docs

* fix: dreambooth inference revision based on feedback

* fix: review from BenjaminBossan

* apply safe merge

* del partially

* refactor oft

* refactor oft

* del unused line

* del unused line

* fix skip in windows

* skip test

* Add comments about bias added place

* rename orig_weights to new_weights

* use inverse instead of linalg.inv

* delete alpha and scaling

---------

Co-authored-by: Lukas Kuhn <lukaskuhn.lku@gmail.com>
Co-authored-by: Lukas Kuhn <lukas.kuhn@deutschebahn.com>"
github.com/huggingface/peft,tests/test_config.py,2023-11-07T10:44:27Z,"Improve documentation for IA³ (#984)

- Improve ia3 documentation
- Raise value error for incorrect feedforward_module list
- Added tests

---------

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>
Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_config.py,2023-11-06T13:04:19Z,"FIX: fix adaptation prompt CI and compatibility with latest transformers (4.35.0) (#1084)

* fix adaptation prompt CI

* undo some other changes"
github.com/huggingface/peft,tests/test_config.py,2023-11-03T14:52:51Z,"FIX: Skip adaption prompt tests with new transformers versions (#1077)

Adaption prompt is failing with transformers v4.35.0. This PR skips the
adaption prompt tests so that CI is green again. The PR also adds an
error when users try to use adaption prompt with that version,
instructing them to use an older transformers version instead.

This should be removed as soon as the issue is fixed in
PEFT/transformers."
github.com/huggingface/peft,tests/test_config.py,2023-11-01T10:39:40Z,"TST test coverage for layer matching (#1031)

Add tests for module name matching using regex and other custom arguments."
github.com/huggingface/peft,tests/test_config.py,2023-10-24T10:06:01Z,"Fix target_modules type in config.from_pretrained (#1046)

Fixes #1045, supersedes #1041

Description

When loading a config from a file, we currently set the loaded
attributes on the config directly. However, this sidesteps the
__post_init__ call, which is required to convert the target_modules to a
set. This PR fixes this by avoiding to set attributes on the config
class directly, instead of going through __init__.

Other changes

While working on this, I did a slight refactor of the config tests.

1. All config classes are included now (some where missing before).
2. Use parameterized instead of looping through the classes.
3. Added a unit test for the aforementioned bug."
github.com/huggingface/peft,tests/test_config.py,2023-07-19T14:08:29Z,"Fixes warning when initializing prompt encoder (#716)

Right now, when the user initializes a prompt encoder with MLP, they get
a warning that a certain argument is ignored, and there is no possible
value for the argument that would stop the warning. Usually, warnings
are for issues that something is (probably) going wrong, but here,
everything is going as expected. Therefore, by default, I would not give
this warning, thus avoiding users getting confused.

However, I would still give the warning if the user set the argument for
encoder_num_layers explicitly to a different value. In that case, they
expect the change to make a difference, but since the argument is
ignored, their expectation is not met, which warrants a warning."
github.com/huggingface/peft,tests/test_config.py,2023-07-13T07:45:50Z,"Add functionality to support IA3 (#578)

* Added initial ia3 code

* Implemented ia3 correctly for feedforward layers; Fixed regex matching

* Fixed module mapping for mt5

* Merged changes from huggingface:main

* Merged changes

* Fixed lora merge conflicts

* Different bloom config

* Added save option for ia3

* Added loading code for ia3

* Added feedforward implementation in utils and seq cls example

* Added feedforward implementation in utils and seq cls example

* Implemented merge, unmerge, enable/disable adapters functionality

* Fixed feedforward during merge

* Debugging Merge

* Removing debug messages

* Cleaned up repo

* Removed non-IA3 changes

* Refactor save and load

* Added support to all models in tests; Added IA3Config for common tests

* Added half-precision support and test for gradient checkpointing; Formatted jupyter notebooks

* Added target modules for new models GPTBigCode and LLama

* Cleaned up code

* Cleaned up code

* Cleaned up example notebook

* Cleaned up  seq2seq notebook

* Corrected function docstrings; refactored find_and_replace

* Corrected function docstrings; refactored find_and_replace

* Added basic docs for IA3

* Added new conceptual guide in source tree for documentation

* Minor fix to documentation

* Minor fixes to docstrings; Added error handling for 4bit quantization; Cleaned unused merge/unmerge methods

* styling changes after merge from main

* Update src/peft/tuners/ia3.py

Remove unused attribute merge_weights

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>

---------

Co-authored-by: Abhishek2304 <abhishekgupta2304@gmail.com>
Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_config.py,2023-07-06T07:06:41Z,"Fix bug resulting in config copies not working (#653)

Resolves #424

The bug was caused by __dict__ being overwritten to return a copy of the
dataclass. This can lead to unpredictable behavior, as shown in the
issue. This fix removes the __dict__ property and preservers the
original behavior where needed.

All three added tests would fail without the fix."
github.com/huggingface/peft,tests/test_config.py,2023-06-15T10:23:05Z,"[`core`] Correctly passing the kwargs all over the place (#575)

* v1 of the fix

* forward contrib credits from discussions

* add tests

---------

Co-authored-by: winglian <winglian@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_config.py,2023-06-01T09:09:54Z,"Enable PeftConfig & PeftModel to load from revision (#433)

* Enable PeftConfig to load from revision

* Add revision to PeftModel

* Fix weights download with revision"
github.com/huggingface/peft,tests/test_config.py,2023-04-25T06:54:18Z,"Implement adaption prompt from Llama-Adapter paper (#268)

* Implement adaption prompt from Llama-Adapter paper

* Support multi-adapters

* Refactor adaption prompt to target attn modules instead of layers

* Refactor adaption prompt to be more generic

* Fix adaption prompt not on right device

* Apply suggestions from code review

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>

* Fix style

* Add support for Llama config use_cache=True

* Fix rebase issues

---------

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_config.py,2023-01-30T08:01:01Z,fixes
github.com/huggingface/peft,tests/test_config.py,2023-01-29T10:49:31Z,"adapt from code review

- remove `README`
- inherit from `dataclass`
- add new test"
github.com/huggingface/peft,tests/test_config.py,2023-01-26T11:17:24Z,"working v1

- push to hub method works
- add tests
- add config super class
- add Lora support for `from_pretrained`"
github.com/huggingface/peft,tests/test_config.py,2023-01-26T10:12:51Z,add config tests
github.com/shibing624/MedicalGPT,chatpdf.py,2024-03-01T09:30:23Z,update return pt.
github.com/shibing624/MedicalGPT,chatpdf.py,2024-03-01T09:10:27Z,fix model has no tokenizer.apply_chat_template.
github.com/shibing624/MedicalGPT,chatpdf.py,2024-01-15T03:20:37Z,update rag demo.
github.com/shibing624/MedicalGPT,chatpdf.py,2024-01-15T03:09:08Z,update rag torch type.
github.com/shibing624/MedicalGPT,chatpdf.py,2024-01-14T13:05:40Z,update rag.
github.com/shibing624/MedicalGPT,chatpdf.py,2024-01-14T03:45:08Z,update rag.
github.com/dvlab-research/LongLoRA,inference.py,2023-11-19T07:33:44Z,Update inference.py
github.com/dvlab-research/LongLoRA,inference.py,2023-11-19T06:28:46Z,Update inference.py
github.com/dvlab-research/LongLoRA,inference.py,2023-11-05T16:58:41Z,fix starting token repetition
github.com/dvlab-research/LongLoRA,inference.py,2023-10-30T15:24:59Z,Update inference.py
github.com/dvlab-research/LongLoRA,inference.py,2023-10-30T15:22:42Z,Update inference.py
github.com/dvlab-research/LongLoRA,inference.py,2023-10-09T10:22:22Z,"Update inference.py

- Generated words are printed immediately by using TextStreamer."
github.com/dvlab-research/LongLoRA,inference.py,2023-10-07T12:48:38Z,Add files via upload
github.com/dvlab-research/LongLoRA,inference.py,2023-10-03T03:44:07Z,Update inference.py
github.com/dvlab-research/LongLoRA,inference.py,2023-10-03T03:12:16Z,Update inference.py
github.com/dvlab-research/LongLoRA,inference.py,2023-09-24T15:27:08Z,Update inference.py
github.com/dvlab-research/LongLoRA,inference.py,2023-09-24T15:25:25Z,Update inference.py
github.com/dvlab-research/LongLoRA,inference.py,2023-09-21T18:13:56Z,Add files via upload
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2024-02-21T18:36:10Z,Token accuracy metrics (#983)
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2024-02-09T17:02:30Z,"Fix eval.py with lora (#965)

* just remove it?

* or not

* fix

* fix up

* clean up

* fix example yaml

* precommit

* add test"
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2024-02-02T21:52:55Z,Switch to the Composer integration of LoRA (works with FSDP) (#886)
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-12-20T00:57:14Z,"Updating the Flash Attention version to fix cross entropy loss (#812)

* ..

* ..

* ..

* ..

* .."
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-12-15T23:42:35Z,"Clean up the logs, bump datasets and transformers (#804)"
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-11-02T22:38:50Z,Fix HF local module copy contention with a meta init on local rank 0 (#710)
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-10-24T18:25:35Z,"Allow flash attention 2 and upgrade to transformers 4.34.1 (#672)

* more special casing in tokenizer equivalence check
* fix addedtoken -> str
* add lazy load option
* add gc collect
* updates for the patch release
* add documentation for flash attention options"
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-10-03T21:56:02Z,Fix overriding of rope_scaling config (#644)
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-10-03T15:45:13Z,"Add flag to disable train metrics (#642)

* free mem

* lint

* lint"
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-09-26T23:28:03Z,"Add code eval (#587)

Final commit of code eval support.

---------

Co-authored-by: Michael Carbin <michael.carbin@databricks.com>
Co-authored-by: Daniel King <43149077+dakinggg@users.noreply.github.com>"
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-09-26T18:07:36Z,"Add node rank to signal paths (#629)

* add node rank

* lint"
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-09-13T00:08:56Z,"Refactor logging (#234)

Replaces most print statements with proper logging. Deprecates the `verbose` argument in favor of using the `python_log_level` argument that is also used by composer."
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-09-12T21:09:34Z,"Fix ComposerHFCausalLM instantiation with PeftModel (#593)

* Fix bug in hf_causal_lm, causing errors with evaluating peft models

* Move attention patch

* Fix typing"
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-09-07T20:28:42Z,"Update to transformers 4.32 (#561)

* monkeypatch config for mpt
* add error if trying to use mpt without trust remote code"
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-08-25T04:11:11Z,"Enable eval script for HuggingFace 8bit models (#516)

* Enable eval script for HF 8bit models

* nit

* Address comments

* Add eval yaml and checks

* nit

* pre-commit

---------

Co-authored-by: Daniel King <43149077+dakinggg@users.noreply.github.com>"
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-08-24T22:46:06Z,"Revert ""Add Programming to Foundry (#441)"" (#557)

This reverts commit 52a3500ff54923919d7d8e88d05e3cafa3cf594d."
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-08-24T21:10:29Z,"Add Programming to Foundry (#441)

* add programming to the gauntlet

* fix pre-commit

* add to builders

* remove programming from gauntlet yaml before v0.1

* add language to human eval jsonl

* test multilingual

* add data

* fix typo

* revert yamls

* add beams to yaml

* final fixes

* final fixes

* upgrade composer

* change beam #s

* upgrade tensorbaord

* remove c dataset:

---------

Co-authored-by: bcui19 <bcui8377@gmail.com>"
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-08-15T16:53:46Z,Monkeypatch flash attention in for llama (#520)
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-08-02T18:20:04Z,Adding pyright to pre-commit (#477)
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-06-29T22:49:03Z,"make peft installs a extra_dep (#397)

* make peft installs a extra_dep

* updt import error

* allow peft import to fail

* Update llmfoundry/models/hf/hf_causal_lm.py

Co-authored-by: Sam Havens <samhavens@gmail.com>

* updt

* lint

---------

Co-authored-by: Sam Havens <samhavens@gmail.com>"
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-06-27T19:42:54Z,"Feature/peft compatible models (#346)

* ignore venv in dir

* adding a lean ComposerHFCausalLMFromPython that converts a loaded hf model into a composer one

* typechecking the composer convertor, import peft and transformers.

* support for input_embeds in forward calls for peft compatibility

* fixing inputs_embeds typos

* precommit fixes docs

* refactored hf causal

* removed python convertor from inits

* wip train.py

* added lora deps

* removed 8 bit defaults

* Update llmfoundry/models/mpt/modeling_mpt.py

Co-authored-by: Sam Havens <samhavens@gmail.com>

* precommit edits models

* Update llmfoundry/models/mpt/modeling_mpt.py

Co-authored-by: Mihir Patel <mihir.v.patel7@gmail.com>

* delete deprecated hf class from init

* removed 8-bit and device map support for now

* formatting the peft builder for precommit

* fixed comments on model ifs

* added a util for printing trainable params

* deps pinned down and sent to gpu

* scipy dep for bitsandbytes

* sent lora deps to regular install_requires

* pinned down scipy

---------

Co-authored-by: danbider <dan@mosaicml.com>
Co-authored-by: Mihir Patel <mihir.v.patel7@gmail.com>
Co-authored-by: Cody Blakeney <cjb92@txstate.edu>
Co-authored-by: Sam Havens <samhavens@gmail.com>
Co-authored-by: Cody Blakeney <cody@mosaicml.com>"
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-06-13T22:57:27Z,"Huggingface Mixed Initialization (#303)

Adding code s.t. Huggingface mixed initialization should work.
---------

Co-authored-by: Karan Jariwala <karankjariwala@gmail.com>
Co-authored-by: Daniel King <daniel@mosaicml.com>
Co-authored-by: Daniel King <43149077+dakinggg@users.noreply.github.com>"
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-06-06T17:20:40Z,add shift_labels arg to HF wrappers (#288)
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-05-26T00:59:51Z,"Removing deprecated vocabulary size parameter from composer CE metrics (#222)

* Removing deprecated vocab param from composer metric

* Cleaning up formatting

* Removing all instances of passing in vocab size"
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-05-24T23:46:55Z,"torch2 updt with hf fixes (#193)

* fix and test

* Revert ""Revert ""Torch2 (#177) (#178)"" (#181)""

This reverts commit 89f56d2df5f4c0ea10e3209ef0d9bd58dbf05565.

* updt import try except

* updt hf model

* updt imports

* lint

* add mpt hf model init / gen test

* updt for temp testing

* lint

* rerun tests

* Update .github/workflows/release.yaml

Co-authored-by: Daniel King <43149077+dakinggg@users.noreply.github.com>

* Update tests/test_hf_mpt_gen.py

Co-authored-by: Daniel King <43149077+dakinggg@users.noreply.github.com>

* add cpu test

* updt tests / cpu img

* updt cpu test install

* rerun tests

* fix hf import structure

* fix test

* pull_request -> pull_request_target

* make onnx test smaller

---------

Co-authored-by: Daniel King <daniel@mosaicml.com>
Co-authored-by: Daniel King <43149077+dakinggg@users.noreply.github.com>"
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-05-11T02:31:50Z,"hf dict cfg overrides (#90)

* hf dict cfg overrides

* custom dict overrides with checks; set all init defaults

* pr cmt"
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-05-10T17:49:47Z,"Updates to prefixlm and t5 (#85)

* Overlooked updates to prefixlm and t5

* Tokenizer type check"
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-05-04T20:04:38Z,Add LLaMa and MPT HFCausalLM support (#32)
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-05-04T17:50:35Z,"Add ICL datasets, plus functionality for subcategories in datasets (#30)"
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-05-02T20:24:16Z,Fix `ComposerHFCausalLM` initialization (#6)
github.com/mosaicml/llm-foundry,llmfoundry/models/hf/hf_causal_lm.py,2023-04-29T02:39:48Z,Refactor codebase for LLM only code
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/training/peft/peft_model.py,2023-09-05T01:57:07Z,fix style
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/training/peft/peft_model.py,2023-09-05T01:47:50Z,fix style
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/training/peft/peft_model.py,2023-09-05T01:31:38Z,fix style
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/training/peft/peft_model.py,2023-09-04T08:00:57Z,fix style
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/training/peft/peft_model.py,2023-09-04T06:02:25Z,Add support for kbits training
github.com/huggingface/peft,tests/test_hub_features.py,2024-02-14T15:43:47Z,"TST Use plain asserts in tests (#1448)

Use pytest style asserts instead of unittest methods.

Use `pytest.raises` and `pytest.warns` where suitable."
github.com/huggingface/peft,tests/test_hub_features.py,2024-02-07T11:52:35Z,MNT Move code quality fully to ruff (#1421)
github.com/huggingface/peft,tests/test_hub_features.py,2023-07-19T05:47:15Z,"Fix subfolder issue (#721)

* fix subfolder issue

* added tests"
github.com/ludwig-ai/ludwig,tests/ludwig/encoders/test_llm_encoders.py,2024-01-19T21:03:20Z,Enable IA3 adapters in `LLMEncoder` (#3902)
github.com/ludwig-ai/ludwig,tests/ludwig/encoders/test_llm_encoders.py,2024-01-19T18:36:25Z,"Cast `LLMEncoder` output to `torch.float32`, freeze final layer at init. (#3900)"
github.com/ludwig-ai/ludwig,tests/ludwig/encoders/test_llm_encoders.py,2024-01-19T18:06:04Z,Enable AdaLoRA tests for LLM adapter (#3896)
github.com/ludwig-ai/ludwig,tests/ludwig/encoders/test_llm_encoders.py,2024-01-11T17:26:09Z,Add custom `prepare_for_trianing` logic to ECD model for LLM encoder adapter initialization (#3874)
github.com/ludwig-ai/ludwig,tests/ludwig/encoders/test_llm_encoders.py,2023-12-20T22:07:29Z," fix: Handle missing and unexpected keys during LLMEncoder state dict load, part 2 (#3843)"
github.com/ludwig-ai/ludwig,tests/ludwig/encoders/test_llm_encoders.py,2023-12-20T00:40:41Z,fix: Handle missing and unexpected keys during LLMEncoder state dict load (#3841)
github.com/ludwig-ai/ludwig,tests/ludwig/encoders/test_llm_encoders.py,2023-12-15T00:04:03Z,Add LLM Text Encoder (#3828)
github.com/huggingface/peft,tests/test_custom_models.py,2024-02-27T11:02:11Z,"FEAT Implement DoRA (#1474)

Add DoRA (Weight-Decomposed Low-Rank Adaptation).

https://arxiv.org/abs/2402.09353

To use this with LoRA, add use_dora=True to the LoraConfig.

Currently only supports nn.Linear layers, not other types or
quantized linear layers like bnb."
github.com/huggingface/peft,tests/test_custom_models.py,2024-02-26T09:37:36Z,"FIX Safe merging with LoHa and LoKr (#1505)

There was a small bug when merging the LoHa and LoKr tuners with
safe_merge=True due to a missing clone call. This is now fixed.

Furthermore, the test coverage for merging with LoHa and LoKr has been
extended, as there were a few tests where these methods were excluded
unnecessarily."
github.com/huggingface/peft,tests/test_custom_models.py,2024-02-20T14:12:34Z,"FIX Correctly unload double wrapped modules (#1490)

Resolves #1485, but note that some additional solutions are mentioned in
thet issue.

This checks that when unloading a PEFT model, if the
ModulesToSaveWrapper contains a tuner module, it is correctly unloaded.
The unloaded model should not have PEFT layers at the end."
github.com/huggingface/peft,tests/test_custom_models.py,2024-02-14T15:43:47Z,"TST Use plain asserts in tests (#1448)

Use pytest style asserts instead of unittest methods.

Use `pytest.raises` and `pytest.warns` where suitable."
github.com/huggingface/peft,tests/test_custom_models.py,2024-02-07T13:59:48Z,"TST Improve test coverage by skipping fewer tests (#1445)

Many of the common tests are skipped because of lines such as:

if config_cls not in (LoraConfig, IA3Config):
    return

These lines were often added before we had more PEFT methods like OFT,
LoHa, etc. However, these new methods should also pass the common tests.
Therefore, I relaxed many of these conditions so that they would not
skip the new methods.

Note:

There were a handful of test cases that failed. I added TODO comments
for those, as it was unclear to me why they failed. As investigating
this could take some time, I chose not to fix those cases in this PR."
github.com/huggingface/peft,tests/test_custom_models.py,2024-02-07T11:52:35Z,MNT Move code quality fully to ruff (#1421)
github.com/huggingface/peft,tests/test_custom_models.py,2024-01-31T06:52:04Z,"FIX: Make merging of adapter weights idempotent (#1355)

* FIX Make merging of adapter weights idempotent

Right now, merging of adapters weights such as LoRA and IA³ is not
idempotent. This means that if a user calls merge multiple times, the
resulting weights will be different each time because the delta weights
are added again and again.

This fix checks that only those adapters are merged that are not yet
merged. Also, gives a more precise warning:

- Say when there is nothing to merge.
- If there are some adapters to merge, only mention those

This bug is more subtle than it may seem at first, since we sometimes
merge implicitly without the user necessarily being aware of it (e.g.
when calling merge_and_unload). Therefore, this bug can occur quite
easily, even if the user does not explicitly call merge twice in a row.

* Make style"
github.com/huggingface/peft,tests/test_custom_models.py,2023-12-12T16:53:36Z,"Release: 0.7.1 (#1257)

Also fix some more seeds to prevent flakiness"
github.com/huggingface/peft,tests/test_custom_models.py,2023-11-30T15:58:42Z,"[Feature] Support OFT (#1160)

* Support OFT

* add test

* Update README

* fix code quality

* fix test

* Skip 1 test

* fix eps rule and add more test

* feat: added examples to new OFT method

* fix: removed wrong arguments from model example

* fix: changed name of inference file

* fix: changed prompt variable

* fix docs

* fix: dreambooth inference revision based on feedback

* fix: review from BenjaminBossan

* apply safe merge

* del partially

* refactor oft

* refactor oft

* del unused line

* del unused line

* fix skip in windows

* skip test

* Add comments about bias added place

* rename orig_weights to new_weights

* use inverse instead of linalg.inv

* delete alpha and scaling

---------

Co-authored-by: Lukas Kuhn <lukaskuhn.lku@gmail.com>
Co-authored-by: Lukas Kuhn <lukas.kuhn@deutschebahn.com>"
github.com/huggingface/peft,tests/test_custom_models.py,2023-11-29T13:58:41Z,"Training PEFT models with new tokens being added to the embedding layers and tokenizer (#1147)

* add support for saving base layers weights along with adapter weights

* Update save_and_load.py

* Add an example showing the usage of the added feature

* refactor the functionality

* fix

* refactoring code

1. Add `is_embedding_layer_resized` parameter to `save_pretrained`
2. Fix the deduplication in README when adding PEFT details.
3. `save_pretrained` should only save the model when `is_main_process=True` which is one of the parameters of `save_pretrained`.

* update example

* fix the model card

* fix model card

* 😅

* fix model card

* automate setting `is_embedding_layer_resized`

* nits

* Update peft_lora_clm_with_additional_tokens.ipynb

* add test

* fix tests

* maybe fixes the issue?

* address comments

Co-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_custom_models.py,2023-11-16T11:45:12Z,"Refactor base layer pattern (#1106)

Description

Refactor all tuners (where it applies, i.e. not prompt tuning) to use
the ""base layer pattern"". This means that the adapter layer will always
hold a reference to the original layer that it modifies. This pattern is
already partly used (e.g. LoRA bnb, gptq layers), now it is consistently
used everywhere when applicable.

This PR is a companion PR to #1069, where I first added these changes.
They are now extracted to a separate PR to make code review easier and
to advance more quickly.

Implementation

The main change is that the adapter layer wraps the original layer and
calls forward on that layer, instead of doing stuff like this:

F.linear(input, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)

which completely circumvents the call to the target layer's forward
method. With the base layer pattern, we now call the target layer's
forward method. Therefore, if the target layer is another adapter
layer (which will be crucial for mixed adapters), we call its forward
method correctly. Also, this should allow passing extra arguments, like
lora_scale to forward.

This change has the nice side benefit that we no longer need to use
_init_empty_weights -- in fact, we don't initialize any of the target
layer's weights anymore, since we have a reference to it. There is thus
no risk of having slow but superfluous initialization of layers.

Moreover, I could greatly simplify merge_and_unload by just using the
base_layer instead of having to create a completely new layer. For
OPT-350m, this results in a 15x speedup.

Note that same as for the bnb layers, this should be backwards
incompatible, since the adapter weights and their state_dicts are not
affected by this change. I used #1115 for regression testing.

Somewhat unrelated changes

During debugging, I got very annoyed with the fact that the reprs of
adapter layers and normal PyTorch layers are hard to distinguish, e.g.
the type is just ""Linear"". Now, for adapter layers, it is prefixed by
the adapter type, e.g. ""lora.Linear"". This should have no further
implications except for the repr (e.g. state_dict remains unaffected).

For LoHa and LoKr, I had to change the init of weights when using
init_weights=False. This is because of what is discussed in Numerical
instabilities with LoHa #1058.

IA³ now has the unload method too.

LoHa and LoKr now support safe_merge=True when merging layers.

Migration guide

For 99% of users, the code should continue working as ususal, because
the API stays the same. Only low level details have been changed.

Code that relies on isinstance checks on specific PEFT classes may
break. E.g. the LoRA Linear layer no longer inherits from nn.Linear. It
is, however, still a BaseTunerLayer. The same logic applies for other
layer types like Conv2d and for other tuners like IA³.

To retrieve the base layer of an adapter layer, you should now call
module.get_base_layer() if you deal with a BaseTunerLayer. Don't rely on
something like module.weight being present (though it might be)."
github.com/huggingface/peft,tests/test_custom_models.py,2023-11-16T11:05:22Z,"FEAT: Merging only specified `adapter_names` when calling `merge` (#1132)

* working v1

* add tests

* remove

* add it also for lokr and loha, left a todo

* Update tests/testing_common.py

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>

* better test

* up

* fix tests

* credits contrib and suggestions from disscussions

* credits contrib and suggestions from disscussions

* address last comments

---------

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>
Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_custom_models.py,2023-11-15T10:21:23Z,"FEAT: Make safe serialization the default one (#1088)

* make safe serialization the default one

* adapt tests

* fix final tests'

* adapt from suggestion"
github.com/huggingface/peft,tests/test_custom_models.py,2023-11-14T12:14:49Z,"TST Improve requires grad testing: (#1131)

Previously, the corresponding tests were testing only whether specific
parameters had requires_grad True or False. Now, all parameters are
being checked. This is more rigorous.

Also, tests for Embedding, Conv1D, Conv2d were added, thus superseding
PR #1115.

Finally, tests for LoHa and LoKr were added.

Note

I considered moving the tests to a separate module, as they were getting
quite big and this would help with readability. For now, I left them in
the same module because it leads to a better diff view and is thus
easier to review. LMK if I should move the tests to a separate file."
github.com/huggingface/peft,tests/test_custom_models.py,2023-11-10T12:33:56Z,"Refactor adapter deletion (#1105)

Description

The job of deleting an adapter is now transferred to the adapter layer,
instead of the adapter model. This makes it easier for users or other
libraries who don't use the adapter model to delete adapters.

Implementation

The code should now be more generic, relying less on hard-coded
attributes.

As a precaution, I also changed the type of adapter_layer_names from
list to tuple, as it should not be mutated.

When deleting the active adapter, the logic for choosing the new active
adapter has been changed slightly to ensure consistency across layers.
In practice, this should rarely make a difference. An error is now
raised if the last remaining adapter is deleted.

Test coverage has been increased:

- Deleting adapters is now also tested for custom models.
- It is also tested for LoHa, LoKr, not only LoRA.
- I added a test for deleting the non-active adapter.

Not implemented

I did not add adapter deletion to IA³, since it is included in #980. LMK
if it should be added here instead."
github.com/huggingface/peft,tests/test_custom_models.py,2023-10-30T14:36:41Z,"Add implementation of LyCORIS LoKr for SD&SDXL models (#978)

KronA-like adapter"
github.com/huggingface/peft,tests/test_custom_models.py,2023-10-26T13:51:49Z,FIX Conv1D merge error for IA3 (#1014)
github.com/huggingface/peft,tests/test_custom_models.py,2023-10-24T13:26:42Z,"FIX: wrong construction of LoHa weights (#1021)

Also: Update convert_sd_adapter_to_peft.py to account for a bug in
Lycoris-LoRA. See https://github.com/KohakuBlueleaf/LyCORIS/pull/115"
github.com/huggingface/peft,tests/test_custom_models.py,2023-10-13T10:23:16Z,"FEAT: Add fp16 + cpu merge support (#1017)

* add fp16 + cpu merge support

* fix tests

* add fp16 tests for custom models

* fix tests

* adapt from comments

* more clarifications"
github.com/huggingface/peft,tests/test_custom_models.py,2023-10-09T10:20:19Z,"ENH Support Conv2d layers for IA³ (#972)

Adds support for Conv2D layers to the IA³ tuner. Tests are added to
check that they work.

Notes:

Unfortunately, when unmerging the Conv2d IA³ layers, there is quite a
bit of rounding error. I had to increase the tolerances for this
specific test case to make the tests pass. I'm not 100% sure why this
is, but I could imagine that for Conv2d, small errors accumulate because
of the convolution operation.

I also added tests for IA³ Linear layers for the custom models, which
also pass. However, there is an error when using Conv1D. The reason is
that merging fails because there is a shape mismatch when
fan_in_fan_out=True (which is set automatically for Conv1D). This is
left for a future PR."
github.com/huggingface/peft,tests/test_custom_models.py,2023-10-04T07:44:10Z,"Add base model metadata to model card (#975)

Resolves #938

This PR adds the base model metadata, if present, to the model card.

On top of this, the code for creating the model card has been refactored
to use the huggingface_hub classes instead of doing ad hoc parsing and
writing.
---------

Co-authored-by: Lucain <lucainp@gmail.com>"
github.com/huggingface/peft,tests/test_custom_models.py,2023-10-03T11:23:33Z,"FIX: issues with (un)merging multiple LoRA and IA³ adapters (#976)

* Fix issues with merging multiple adapters

This should resolve the failing slow test
test_4bit_merge_and_disable_lora.

While investigating, I also noticed that merging multiple adapters was
not correct for IA³. I added a test that should catch this bug and
provided a fix for it too. However, the test does not check IA³ at the
moment because the test parameters do not contain IA³. For this, #972
needs to be merged too, which adds IA³ to the test parameters.

* Small adjustments to tests

Previously, tests had some exploding gradients, making them unstable."
github.com/huggingface/peft,tests/test_custom_models.py,2023-10-02T08:44:51Z,"FEAT Add LyCORIS LoHa for SD&SDXL models (#956)

https://arxiv.org/abs/2108.06098"
github.com/huggingface/peft,tests/test_custom_models.py,2023-09-29T04:14:30Z,"[tests] add multiple active adapters tests (#961)

* add tests for multiple active adapters

* add multiple active adapter tests

* fix tests

* fix the device error

* fix typo

* fix the variables

* fix the `adalora` config

* add util function for proper naming of tests

* fix bugs

1. fix `add_weighted_adapter` when working with adapters targeting different layers
2. fix `ia3` model and layer to handle adapters targeting different layers
3. fix the multiple active adapter tests

* fix `ia3` issue

* remove debug statements

* fix test

* fix bug

* address comments

* address comments

Co-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* fix tests

* remove unused code

* Update test_custom_models.py

* increasing tolerance for a test

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_custom_models.py,2023-09-26T07:31:05Z,"FIX: setting requires_grad on adapter layers (#905)

* [WIP] Fix setting requires_grad on adapter layers

This is an alternative to #900, resolves #899.

Description

Currently, we don't handle setting requires_grad on adapter layers
really well. The main issue is that it can be set to True on adapter
parameters that are not being used, e.g. the original_module in
ModulesToSaveWrapper or inactive adapters in LoRA.

Normally, this is not a big issue, except maybe if we want to correctly
count the number of trainable parameters. However, when training with
DistributedDataParallel, this results in errors, as PyTorch thinks that
all parameters with requires_grad=True should participate in the loss
computation, but those mentioned parameters don't. For that reason,
training with DDP currently fails when using modules_to_save or multiple
adapters.

Implementation

This turned out to be more complicated than I initially thought. The
logic for setting requires_grad is all over the place, it was hard to
encapsulate the logic and I only succeeded partially. As is, this PR is
more complex than the one it tries to supersede, #900, but it is also
""more correct"".

Tests were added to check whether requires_grad is set correctly. There
are (so far) no tests for whether DDP indeed works, they could be added
with multi-GPU. I did, however, test an early stage of this PR with DDP
and setting requires_grad correctly will indeed fix the DDP error.

DONE/TODO

- [x] ModulesToSaveWrapper
- [x] LoRA
- [ ] IA³
- [ ] AdaLora

Since some tuners are not implemented yet, tests are expected to fail.
Check the new tests at the bottom of test_custom.py, those should pass.

* Refactor: move more requires_grad machinery to ABC

* [skip ci] [WIP] Add requires_grad logic to IA³

* Add AdaLora

* Fix some minor issues

* Make style"
github.com/huggingface/peft,tests/test_custom_models.py,2023-09-22T20:03:44Z,"support multiple ranks and alphas for LoRA (#873)

* support multiple ranks and alphas

* Update lora.py

* Update lora.py

* commit suggestions

Co-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* address comments

Co-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* Fixed multirank + multialpha for sequential LoRAs, added correct support of LoRA-C3Lier conversion (#937)

* Fixed multirank multialpha for sequential loras, added tests, fixed docs

* Refactored kohya_ss conversion script for proper support of LoRA-C3Lier

* Fixed styling

* Removed old comment from docstring

* shift `scale_layer`/`unscale_layer` to `LoraLayer` class to support all the child classes

* support multiple active adapters

* add `active_adapters` property

Co-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* fix bug related to active adapter of `ModulesToSaveWrapper`

* revert the change wrt active_adapter assignment

Co-Authored-By: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* addressing comments

* address comments

* address comment

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>
Co-authored-by: Alexander Kovalchuk <kovalexal@gmail.com>
Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_custom_models.py,2023-09-21T07:46:28Z,"Fix some tests that would fail with torch.compile (#949)

Some tests would currently fail with torch.compile, not because there is
anything wrong with how PEFT works with compiled models, but simply
because of the way the tests are written. This is because when models
are compiled, the keys of the state dict change. Tests have now been
adapted to unwrap the compiled model first before getting the state
dict.

Note that the mentioned issue does not affect saving and loading,
because save_pretrained is already called on the original module, so
there is no issue with mismatched keys.

Also fixed the docstring of get_peft_model_state_dict."
github.com/huggingface/peft,tests/test_custom_models.py,2023-09-12T09:05:29Z,"ENH speed up init emb conv2d (#915)

Partly resolves #872

Description

After getting faster initialization of the LoRA Linear layer,
initialization of Conv2D and Embedding is now sped up.

Implementation

The approach of how to achieve the speed up has slightly changed
compared to last time. To refresh memory, in #887, we avoided the
unnecessary initialization of the full weight matrix by completely
skipping nn.Linear.__init__.

Although it is possible to do the same for Embedding and Conv2d, we run
into some trouble here. The issue is that the __init__ methods of these
classes have quite a lot more arguments and some custom logic (i.e. not
only self.foo = foo but more on top). If we wanted to skip __init__
entirely, we would have to basically copy all of that into our code.
Although that is possible, it is brittle (e.g. the logic could be
different for different PyTorch versions or change over time).

For that reason, I opted to implement this differently, using a
suggestion we had discussed earlier. The approach is to call __init__ of
the parent class but enforce empty weights (this is what
torch.nn.utils.skip_init does, although we cannot use that function
directly). This way, we can avoid having to copy the __init__ code while
still avoiding expensive initialization of the weights.

I did not change the code for Linear to also use this approach because
the logic inside of Linear.__init__ is quite simple (at least for now),
so we are good here with the existing approach.

However, I was curious how changing the approach for Linear would affect
the initialization speed. Therefore, I ran the script from #872 again, 3
times each.

Current approach:

test 1 with model bert-base took 0.021 sec.
test 1 with model bert-base took 0.020 sec.
test 1 with model bert-base took 0.020 sec.
test 2 with model bloomz-1b7 took 0.030 sec.
test 2 with model bloomz-1b7 took 0.030 sec.
test 2 with model bloomz-1b7 took 0.030 sec.

New approach if applied to Linear:

test 1 with model bert-base took 0.038 sec.
test 1 with model bert-base took 0.039 sec.
test 1 with model bert-base took 0.038 sec.
test 2 with model bloomz-1b7 took 0.072 sec.
test 2 with model bloomz-1b7 took 0.048 sec.
test 2 with model bloomz-1b7 took 0.048 sec.

This shows that the new approach is indeed a bit slower than the
existing one, though still a lot faster than what we had before. IMHO, I
think we're safe to leave the code inside of Linear as is and benefit
from the slightly better performance at the cost of slightly more
fragile code."
github.com/huggingface/peft,tests/test_custom_models.py,2023-09-06T15:31:55Z,"ENH Remove redundant initialization layer calls (#887)

This should lead to a big speedup when initializing LoRA layers.

---------

Co-authored-by: poedator <ruslansv@gmail.com>"
github.com/huggingface/peft,tests/test_custom_models.py,2023-08-30T12:40:56Z,"MNT Run tests that were skipped previously (#884)

Some tests were skipped because of an issue with how LoRA weights were
initialized for embeddings. This issue has been fixed for some time now,
so the tests no longer need to be skipped."
github.com/huggingface/peft,tests/test_custom_models.py,2023-08-16T08:57:38Z,"TST: add test about loading custom models (#827)

Prompted by #808, I added a test that shows that loading a trained
custom model works as expected. I only added this to custom models
because it involves a few steps of training and I didn't want to slow
down tests too much. LMK if this should be added to all tests.

In addition, I renamed some custom model tests which had strange
names (probably caused by an overeager query-replace)."
github.com/huggingface/peft,tests/test_custom_models.py,2023-08-08T12:35:19Z,"Add adapter error handling (#800)

When a user tries to add a 2nd adapter, Lora and AdaLora make some checks to
ensure the new adapter is compatible with existing adapters. Currently, that
check is performed halfway through the method. This means that if the check
fails, the new adapter is partially applied, leaving the model in a bad state.
The main purpose of this PR is to ensure that the model state is correct after
such a failure is encountered.

Tests were added to catch this potential bug.

While working on this, I also did some related, but not strictly necessary
changes to the add_adapter methods:

- Previously, the peft_config from the PeftModel was passed to the base
  model. This meant that sometimes, the base model would hold a reference
  to PeftModel.peft_config, but not always, as some base models would
  create new dicts. This is problematic, because some code would rely on
  the objects being the same. Now, they are never the same, leading to
  more consistency.
- I think that the check if multiple adapters have biases (which is not
  supported) was accidentally removed by #749. It is added back in.
- Add some type annotations
- Extend docstrings to contain adapter_name"
github.com/huggingface/peft,tests/test_custom_models.py,2023-07-24T11:23:23Z,"FIX: Disabling adapter works with modules_to_save (#736)

Resolves #493

For LoRA and IA³, there was a bug that even even using the
disable_adapter context, if the module was listed in modules_to_save,
the updated weights would be used instead of the original weights. This
meant that disable_adapter would not return the same results as the base
model without adaptation. This PR fixes the issue and provides a test.

Note: I tried to adjust AdaLoRA too, since it seemed that the same
reasoning should apply there. However, I think that AdaLoRA does not
really support disabling adapters at all. E.g. there is no
disable_adapter_layers method. Therefore, AdaLoRA was not changed."
github.com/huggingface/peft,tests/test_custom_models.py,2023-07-24T08:34:39Z,"ENH: Warn when disabling adapters and bias != 'none' (#741)

For LoRA, given that bias='all' or bias='none', when doing inference
with a model in the disable_adapter context, the output will not be
identical to the output of the base model. This may be surprising to
users. Therefore, a warning is given. Furthermore, the docstring has
been extended to reflect this fact."
github.com/huggingface/peft,tests/test_custom_models.py,2023-07-17T08:02:30Z,"FEAT: Make LoRA work with custom models (#676)

Enable custom models to work with LoRA

This PR enables custom models to work with LoRA in peft by performing a few
changes required for non-transformers models. New tests for linear,
transformers conv1d, and conv2d layers were added.

Not yet contained in this PR:

- support for AdaLoRA and IA³
- documentation
- examples

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/training/peft/tuners/lora.py,2023-09-05T01:57:07Z,fix style
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/training/peft/tuners/lora.py,2023-09-05T01:47:50Z,fix style
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/training/peft/tuners/lora.py,2023-09-05T01:31:38Z,fix style
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/training/peft/tuners/lora.py,2023-09-04T07:15:55Z,delete unused params
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/training/peft/tuners/lora.py,2023-09-04T06:02:25Z,Add support for kbits training
github.com/liguodongiot/llm-action,train/qlora/export_hf_checkpoint.py,2023-07-23T11:43:58Z,fix
github.com/FlagAI-Open/FlagAI,flagai/model/tools/peft/peft_model.py,2023-07-05T08:20:51Z,"added enable input embeddings for lora

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/microsoft/LMOps,minillm/transformers/src/transformers/integrations/peft.py,2023-11-16T12:47:19Z,upgrade transformers
github.com/deep-diver/LLM-As-Chatbot,models/bloom.py,2023-07-25T10:11:50Z,add more models + gptq mode
github.com/deep-diver/LLM-As-Chatbot,models/bloom.py,2023-07-01T17:56:43Z,add local files only option
github.com/deep-diver/LLM-As-Chatbot,models/bloom.py,2023-06-10T13:08:36Z,add use_safetensor=False
github.com/deep-diver/LLM-As-Chatbot,models/bloom.py,2023-06-08T15:32:11Z,tested gpu and cpu options
github.com/deep-diver/LLM-As-Chatbot,models/bloom.py,2023-06-08T01:59:11Z,exp w/ camel and sam
github.com/deep-diver/LLM-As-Chatbot,models/bloom.py,2023-06-05T00:25:50Z,update examples
github.com/deep-diver/LLM-As-Chatbot,models/bloom.py,2023-06-04T14:50:44Z,disable force download & example view
github.com/deep-diver/LLM-As-Chatbot,models/bloom.py,2023-05-18T00:47:42Z,update
github.com/huggingface/peft,tests/test_decoder_models.py,2024-02-16T11:16:49Z,TST Make tests more work with MPS (#1463)
github.com/huggingface/peft,tests/test_decoder_models.py,2024-02-14T15:43:47Z,"TST Use plain asserts in tests (#1448)

Use pytest style asserts instead of unittest methods.

Use `pytest.raises` and `pytest.warns` where suitable."
github.com/huggingface/peft,tests/test_decoder_models.py,2024-02-07T11:52:35Z,MNT Move code quality fully to ruff (#1421)
github.com/huggingface/peft,tests/test_decoder_models.py,2024-01-30T11:32:39Z,"Add positional args to PeftModelForCausalLM.generate (#1393)

* add positional args

* update tests"
github.com/huggingface/peft,tests/test_decoder_models.py,2023-11-16T11:45:12Z,"Refactor base layer pattern (#1106)

Description

Refactor all tuners (where it applies, i.e. not prompt tuning) to use
the ""base layer pattern"". This means that the adapter layer will always
hold a reference to the original layer that it modifies. This pattern is
already partly used (e.g. LoRA bnb, gptq layers), now it is consistently
used everywhere when applicable.

This PR is a companion PR to #1069, where I first added these changes.
They are now extracted to a separate PR to make code review easier and
to advance more quickly.

Implementation

The main change is that the adapter layer wraps the original layer and
calls forward on that layer, instead of doing stuff like this:

F.linear(input, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)

which completely circumvents the call to the target layer's forward
method. With the base layer pattern, we now call the target layer's
forward method. Therefore, if the target layer is another adapter
layer (which will be crucial for mixed adapters), we call its forward
method correctly. Also, this should allow passing extra arguments, like
lora_scale to forward.

This change has the nice side benefit that we no longer need to use
_init_empty_weights -- in fact, we don't initialize any of the target
layer's weights anymore, since we have a reference to it. There is thus
no risk of having slow but superfluous initialization of layers.

Moreover, I could greatly simplify merge_and_unload by just using the
base_layer instead of having to create a completely new layer. For
OPT-350m, this results in a 15x speedup.

Note that same as for the bnb layers, this should be backwards
incompatible, since the adapter weights and their state_dicts are not
affected by this change. I used #1115 for regression testing.

Somewhat unrelated changes

During debugging, I got very annoyed with the fact that the reprs of
adapter layers and normal PyTorch layers are hard to distinguish, e.g.
the type is just ""Linear"". Now, for adapter layers, it is prefixed by
the adapter type, e.g. ""lora.Linear"". This should have no further
implications except for the repr (e.g. state_dict remains unaffected).

For LoHa and LoKr, I had to change the init of weights when using
init_weights=False. This is because of what is discussed in Numerical
instabilities with LoHa #1058.

IA³ now has the unload method too.

LoHa and LoKr now support safe_merge=True when merging layers.

Migration guide

For 99% of users, the code should continue working as ususal, because
the API stays the same. Only low level details have been changed.

Code that relies on isinstance checks on specific PEFT classes may
break. E.g. the LoRA Linear layer no longer inherits from nn.Linear. It
is, however, still a BaseTunerLayer. The same logic applies for other
layer types like Conv2d and for other tuners like IA³.

To retrieve the base layer of an adapter layer, you should now call
module.get_base_layer() if you deal with a BaseTunerLayer. Don't rely on
something like module.weight being present (though it might be)."
github.com/huggingface/peft,tests/test_decoder_models.py,2023-11-16T11:05:22Z,"FEAT: Merging only specified `adapter_names` when calling `merge` (#1132)

* working v1

* add tests

* remove

* add it also for lokr and loha, left a todo

* Update tests/testing_common.py

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>

* better test

* up

* fix tests

* credits contrib and suggestions from disscussions

* credits contrib and suggestions from disscussions

* address last comments

---------

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>
Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_decoder_models.py,2023-11-15T10:21:23Z,"FEAT: Make safe serialization the default one (#1088)

* make safe serialization the default one

* adapt tests

* fix final tests'

* adapt from suggestion"
github.com/huggingface/peft,tests/test_decoder_models.py,2023-11-14T11:28:55Z,"Prompt tuning: fix AutoTokenizer.from_pretrained (#1053)

Fixes #1032

Description

Currently, when using prompt tuning with TEXT, we call
AutoTokenizer.from_pretrained with only the model id. However, it may be
necessary to pass additional arguments, e.g. trust_remote_code=True.
This fix allows to pass more arguments by setting the argument
tokenizer_kwargs in the PromptTuningConfig.

I also added a check that when tokenizer_kwargs is set, the TEXT option
is actually being used.

Moreover, I noticed that we have no tests for prompt tuning with TEXT,
so I added those tests for decoder models.

Additional changes

There was a bug in PromptEmbedding where the device of the
init_token_ids was not set, which resulted in errors when using CUDA.

Finally, I removed an unused constant CONFIG_CLASSES from a test."
github.com/huggingface/peft,tests/test_decoder_models.py,2023-11-10T12:33:56Z,"Refactor adapter deletion (#1105)

Description

The job of deleting an adapter is now transferred to the adapter layer,
instead of the adapter model. This makes it easier for users or other
libraries who don't use the adapter model to delete adapters.

Implementation

The code should now be more generic, relying less on hard-coded
attributes.

As a precaution, I also changed the type of adapter_layer_names from
list to tuple, as it should not be mutated.

When deleting the active adapter, the logic for choosing the new active
adapter has been changed slightly to ensure consistency across layers.
In practice, this should rarely make a difference. An error is now
raised if the last remaining adapter is deleted.

Test coverage has been increased:

- Deleting adapters is now also tested for custom models.
- It is also tested for LoHa, LoKr, not only LoRA.
- I added a test for deleting the non-active adapter.

Not implemented

I did not add adapter deletion to IA³, since it is included in #980. LMK
if it should be added here instead."
github.com/huggingface/peft,tests/test_decoder_models.py,2023-10-13T10:23:16Z,"FEAT: Add fp16 + cpu merge support (#1017)

* add fp16 + cpu merge support

* fix tests

* add fp16 tests for custom models

* fix tests

* adapt from comments

* more clarifications"
github.com/huggingface/peft,tests/test_decoder_models.py,2023-10-09T16:28:00Z,"FEAT: Add `safe_merge` option in `merge` (#1001)

* add `safe_merge` option in `merge`

* oops

* Apply suggestions from code review

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* address final comments

* Update src/peft/tuners/lora/layer.py

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* Update src/peft/tuners/lora/layer.py

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* add it for ia3

* add it for adalora

* up

* revert for loha

* style

* fix CI

* adapt from suggestions

* add tests

* up

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_decoder_models.py,2023-09-07T09:36:42Z,"support prefix tuning for starcoder models (#913)

* support prefix tuning for starcoder models

* remoce the test filter for prefix tuning tests for StarCoder models"
github.com/huggingface/peft,tests/test_decoder_models.py,2023-08-08T12:35:19Z,"Add adapter error handling (#800)

When a user tries to add a 2nd adapter, Lora and AdaLora make some checks to
ensure the new adapter is compatible with existing adapters. Currently, that
check is performed halfway through the method. This means that if the check
fails, the new adapter is partially applied, leaving the model in a bad state.
The main purpose of this PR is to ensure that the model state is correct after
such a failure is encountered.

Tests were added to catch this potential bug.

While working on this, I also did some related, but not strictly necessary
changes to the add_adapter methods:

- Previously, the peft_config from the PeftModel was passed to the base
  model. This meant that sometimes, the base model would hold a reference
  to PeftModel.peft_config, but not always, as some base models would
  create new dicts. This is problematic, because some code would rely on
  the objects being the same. Now, they are never the same, leading to
  more consistency.
- I think that the check if multiple adapters have biases (which is not
  supported) was accidentally removed by #749. It is added back in.
- Add some type annotations
- Extend docstrings to contain adapter_name"
github.com/huggingface/peft,tests/test_decoder_models.py,2023-08-02T14:59:11Z,"Allow passing inputs_embeds instead of input_ids (#757)

Resolves #727

Right now, there is an issue with a few PeftModelForXxx classes when
users pass only inputs_embeds but not input_ids. First of all, the batch
size was being derived on input_ids, now it is derived from
inputs_embeds instead if input_ids is None. Furthermore, a few forward
calls to the base model were not passing the inputs_embeds along, which
resulted in errors down the line. These issues have been fixed now."
github.com/huggingface/peft,tests/test_decoder_models.py,2023-07-28T11:06:53Z,"Add tests for AdaLoRA, fix a few bugs (#734)

So far, there have been no tests for AdaLoRA. This PR adds tests similar
to the existing ones. While working on those tests, a few bugs were
encountered and fixed.

The changes made to AdaLoRA:

- Linked to paper abstract, not pdf.
- Don't assume that target modules have a .bias attribute (same as for
  LoRA).
- Fixed an issue where it was assumed that if an output object from
  forward has a .loss attribute, it is a scalar, when it can be None.
- Fixed an issue that when init_lora_weights=False, the weights were
  still initialized to be an identity transform.
- When replacing modules, if a target module is a ModuleList or
  ModuleDict, they are now skipped instead of raising an error that the
  module type is not supported. My reasoning was that it is never intended
  to change those modules, so if their names are matched, it must be a
  false positive. The issue arose because for some target modules, the
  names are just k"" etc., and since we match with endswith, this can
  easily lead to modules like ""block"" to match."
github.com/huggingface/peft,tests/test_decoder_models.py,2023-07-17T08:02:30Z,"FEAT: Make LoRA work with custom models (#676)

Enable custom models to work with LoRA

This PR enables custom models to work with LoRA in peft by performing a few
changes required for non-transformers models. New tests for linear,
transformers conv1d, and conv2d layers were added.

Not yet contained in this PR:

- support for AdaLoRA and IA³
- documentation
- examples

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_decoder_models.py,2023-07-14T14:28:03Z,"[`Feature`] Save only selected adapters for LoRA (#705)

* v1 working for LoRA

* more checks

* fix prompt learning issues

* fix failing test

* Apply suggestions from code review

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* fixed indentation

* move the check above

* added tests for adaption prompt, enc-dec and feature extraction

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_decoder_models.py,2023-07-14T14:14:51Z,"[Core] Enhancements and refactoring of LoRA method (#695)

* refactor lora and add utils

1. Refactor LoRA code
2. Add method to delete LoRA adapters
3. Add method to unload the PEFT LoRA model.
4. Add `svd` weighted adapter support.
5. minor fixes

* fixes

* fixes

* Update lora.py

* fixes

* Update lora.py

* docstrings for the added public APIs

* docs

* Update src/peft/tuners/lora.py

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* resolve comments, refactoring and adding tests

* fix the remaining failing tests

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_decoder_models.py,2023-07-14T12:33:33Z,"[WIP] FIX for disabling adapter, adding tests (#683)

This PR deals with some issues with disabling adapter:

- typo in active.adapter
- prompt encoder could be on wrong device
- when using prompt learning + generate, disabling did not work

For the last point, there is a somewhat ugly fix in place for now,
pending a more comprehensive refactor (a comment was added to that
effect).

Comprehensive tests were added to check that everything works now.

The following tests still not working:

- adaption prompt
- seq2seq with prompt tuning/prompt encoding
- stable diffusion is a little bit flaky but test is hopefully robust enough

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_decoder_models.py,2023-07-13T09:12:40Z,FIX: base_model_torch_dtype when using model.half() after init (#688)
github.com/huggingface/peft,tests/test_decoder_models.py,2023-06-27T12:57:57Z,"feat(model): Allow from_pretrained to accept PeftConfig class (#612)

* feat(model): Allow from_pretrained to accept PeftConfig class

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* tests: add test cases for config construction

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: address comments and run tools

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* fix: style

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_decoder_models.py,2023-06-21T12:13:40Z,add `adapter_name` in `get_peft_model` (#610)
github.com/huggingface/peft,tests/test_decoder_models.py,2023-06-16T09:06:48Z,add more CI tests (#586)
github.com/huggingface/peft,tests/test_decoder_models.py,2023-06-15T10:23:05Z,"[`core`] Correctly passing the kwargs all over the place (#575)

* v1 of the fix

* forward contrib credits from discussions

* add tests

---------

Co-authored-by: winglian <winglian@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_decoder_models.py,2023-06-09T10:33:13Z,"[`core`] Add safetensors integration (#553)

* add v1

* clean up

* more improvements

* add device

* final adjustements

* use `EntryNotFoundError`

* better checks

* add tests and final fixes

* make style && make quality

* remove `push_to_hub` because of the release"
github.com/huggingface/peft,tests/test_decoder_models.py,2023-06-01T07:35:24Z,"[`LoRA`] Allow applying LoRA at different stages (#429)

* working v1

- working v1
- added tests
- needs some documentation

* more fixes

- stronger tests
- documentation
- remove unneeded common layers pattern

* add more docstring

* Apply suggestions from code review

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* quality & style

* style

---------

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_decoder_models.py,2023-05-31T10:14:27Z,"[`core`] Add gradient checkpointing check (#404)

* add automatic input enable gradients when calling `get_peft_model`

* style

* better check

* add 4bit check"
github.com/huggingface/peft,tests/test_decoder_models.py,2023-04-14T12:34:01Z,"[`tests`] add CI training tests (#311)

* add training tests

* styling"
github.com/huggingface/peft,tests/test_decoder_models.py,2023-04-07T10:48:22Z,add and fix tests
github.com/huggingface/peft,tests/test_decoder_models.py,2023-04-06T13:36:33Z,making adalora compatible with multiple adapters
github.com/huggingface/peft,tests/test_decoder_models.py,2023-04-05T08:17:26Z,"[`tests`] Adds more tests + fix failing tests (#238)

* adds more tests

- refactor tests
- add enc-dec tests
- skips generate tests for non-lora adapters

* rm unneeded file

* fix tests

* fix

* more checks

* fix issue"
github.com/OptimalScale/LMFlow,scripts/export_llama_state_dict_checkpoint.py,2023-04-16T02:02:06Z,diffusion
github.com/OptimalScale/LMFlow,scripts/export_llama_state_dict_checkpoint.py,2023-04-06T04:23:17Z,add readme
github.com/OptimalScale/LMFlow,scripts/export_llama_state_dict_checkpoint.py,2023-04-05T14:59:36Z,update
github.com/deep-diver/LLM-As-Chatbot,models/baize.py,2023-07-25T10:11:50Z,add more models + gptq mode
github.com/deep-diver/LLM-As-Chatbot,models/baize.py,2023-07-01T17:56:43Z,add local files only option
github.com/deep-diver/LLM-As-Chatbot,models/baize.py,2023-06-10T13:08:36Z,add use_safetensor=False
github.com/deep-diver/LLM-As-Chatbot,models/baize.py,2023-06-09T04:33:05Z,update vram requirements records
github.com/deep-diver/LLM-As-Chatbot,models/baize.py,2023-06-08T15:32:11Z,tested gpu and cpu options
github.com/deep-diver/LLM-As-Chatbot,models/baize.py,2023-06-08T01:59:11Z,exp w/ camel and sam
github.com/deep-diver/LLM-As-Chatbot,models/baize.py,2023-06-05T00:25:50Z,update examples
github.com/deep-diver/LLM-As-Chatbot,models/baize.py,2023-06-04T14:50:44Z,disable force download & example view
github.com/deep-diver/LLM-As-Chatbot,models/baize.py,2023-05-24T05:43:07Z,add baize
github.com/huggingface/peft,tests/test_adaption_prompt.py,2024-02-16T11:16:49Z,TST Make tests more work with MPS (#1463)
github.com/huggingface/peft,tests/test_adaption_prompt.py,2024-02-14T15:43:47Z,"TST Use plain asserts in tests (#1448)

Use pytest style asserts instead of unittest methods.

Use `pytest.raises` and `pytest.warns` where suitable."
github.com/huggingface/peft,tests/test_adaption_prompt.py,2024-02-07T11:52:35Z,MNT Move code quality fully to ruff (#1421)
github.com/huggingface/peft,tests/test_adaption_prompt.py,2024-02-06T00:54:06Z,Fix typos (#1435)
github.com/huggingface/peft,tests/test_adaption_prompt.py,2024-01-30T11:32:39Z,"Add positional args to PeftModelForCausalLM.generate (#1393)

* add positional args

* update tests"
github.com/huggingface/peft,tests/test_adaption_prompt.py,2023-12-12T14:16:00Z,"FIX Issues with transformers 4.36 (#1252)

Adjust for different type of past_key_values when using caching.

Also: Fix some seeds for flaky tests.

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>
Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>"
github.com/huggingface/peft,tests/test_adaption_prompt.py,2023-11-15T10:21:23Z,"FEAT: Make safe serialization the default one (#1088)

* make safe serialization the default one

* adapt tests

* fix final tests'

* adapt from suggestion"
github.com/huggingface/peft,tests/test_adaption_prompt.py,2023-11-06T13:04:19Z,"FIX: fix adaptation prompt CI and compatibility with latest transformers (4.35.0) (#1084)

* fix adaptation prompt CI

* undo some other changes"
github.com/huggingface/peft,tests/test_adaption_prompt.py,2023-11-03T14:52:51Z,"FIX: Skip adaption prompt tests with new transformers versions (#1077)

Adaption prompt is failing with transformers v4.35.0. This PR skips the
adaption prompt tests so that CI is green again. The PR also adds an
error when users try to use adaption prompt with that version,
instructing them to use an older transformers version instead.

This should be removed as soon as the issue is fixed in
PEFT/transformers."
github.com/huggingface/peft,tests/test_adaption_prompt.py,2023-07-14T14:28:03Z,"[`Feature`] Save only selected adapters for LoRA (#705)

* v1 working for LoRA

* more checks

* fix prompt learning issues

* fix failing test

* Apply suggestions from code review

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* fixed indentation

* move the check above

* added tests for adaption prompt, enc-dec and feature extraction

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_adaption_prompt.py,2023-07-14T12:33:33Z,"[WIP] FIX for disabling adapter, adding tests (#683)

This PR deals with some issues with disabling adapter:

- typo in active.adapter
- prompt encoder could be on wrong device
- when using prompt learning + generate, disabling did not work

For the last point, there is a somewhat ugly fix in place for now,
pending a more comprehensive refactor (a comment was added to that
effect).

Comprehensive tests were added to check that everything works now.

The following tests still not working:

- adaption prompt
- seq2seq with prompt tuning/prompt encoding
- stable diffusion is a little bit flaky but test is hopefully robust enough

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_adaption_prompt.py,2023-06-01T09:14:11Z,"[`Llama-Adapter`] fix half precision inference + add tests (#456)

* fix + add tests

* forward contrib credits from discussions

---------

Co-authored-by: HamidShojanazeri <HamidShojanazeri@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_adaption_prompt.py,2023-04-25T06:54:18Z,"Implement adaption prompt from Llama-Adapter paper (#268)

* Implement adaption prompt from Llama-Adapter paper

* Support multi-adapters

* Refactor adaption prompt to target attn modules instead of layers

* Refactor adaption prompt to be more generic

* Fix adaption prompt not on right device

* Apply suggestions from code review

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>

* Fix style

* Add support for Llama config use_cache=True

* Fix rebase issues

---------

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>"
github.com/huggingface/peft,tests/regression/test_regression.py,2024-02-14T15:43:47Z,"TST Use plain asserts in tests (#1448)

Use pytest style asserts instead of unittest methods.

Use `pytest.raises` and `pytest.warns` where suitable."
github.com/huggingface/peft,tests/regression/test_regression.py,2024-02-07T11:52:35Z,MNT Move code quality fully to ruff (#1421)
github.com/huggingface/peft,tests/regression/test_regression.py,2023-12-08T10:50:48Z,"TST: Add tolerance for regression tests (#1241)

Tests currently call torch.allclose without any tolerance, which is
probably the cause of the CI failure. Now, tolerance is set to 1e-4."
github.com/huggingface/peft,tests/regression/test_regression.py,2023-12-06T14:07:05Z,"TST: Add regression tests 2 (#1115)

Description

In general, for regression tests, we need two steps:

1. Creating the regression artifacts, in this case the adapter
   checkpoint and the expected output of the model.
2. Running the regression tests, i.e. loading the adapter and checking
   that the output of the model is the same as the expected output.

My approach is to re-use as much code as possible between those two
steps. Therefore, the same test script can be used for both, with only
an environment variable to distinguish between the two. Step 1 is
invoked by calling:

`REGRESSION_CREATION_MODE=True pytest tests/regression/test_regression.py`

and to run the second step, we call:

`pytest tests/regression/test_regression.py`

Creating regression artifacts

The first step will create an adapter checkpoint and an output for the
given PEFT version and test setting in a new directory. E.g. it will
create a directory `tests/regression/lora_opt-125m_bnb_4bit/0.5.0/` that
contains adapter_model.bin and output.pt.

Before this step runs, there is a check that the git repo is clean (no
dirty worktree) and that the commit is tagged (i.e. corresponds to a
release version of PEFT). Otherwise, we may accidentally create
regression artifacts that do not correspond to any PEFT release.

The easiest way to get such a clean state (say, for PEFT v0.5.0) is by
checking out a tagged commit, e.g:

`git checkout v0.5.0`

before running the first step.

The first step will also skip the creation of regression artifacts if
they already exist.

It is possible to circumvent all the aforementioned checks by setting
the environment variable `REGRESSION_FORCE_MODE` to True like so:

`REGRESSION_FORCE_MODE=True REGRESSION_CREATION_MODE=True pytest tests/regression/test_regression.py`

You should only do this if you know exactly what you're doing.

Running regression tests

The second step is much simpler. It will load the adapters and the
output created in the first step, and compare the output to the output
from a new PEFT model using the loaded adapter. The outputs should be
the same.

If more than one version is discovered for a given test setting, all of
them are tested.

Notes

Regression artifacts are stored on HF Hub."
github.com/facebookresearch/llama-recipes,examples/hf_text_generation_inference/merge_lora_weights.py,2023-09-01T20:17:39Z,Move inference scripts into example folder
github.com/huggingface/peft,tests/test_encoder_decoder_models.py,2024-02-07T11:52:35Z,MNT Move code quality fully to ruff (#1421)
github.com/huggingface/peft,tests/test_encoder_decoder_models.py,2024-01-30T11:32:39Z,"Add positional args to PeftModelForCausalLM.generate (#1393)

* add positional args

* update tests"
github.com/huggingface/peft,tests/test_encoder_decoder_models.py,2023-11-15T10:21:23Z,"FEAT: Make safe serialization the default one (#1088)

* make safe serialization the default one

* adapt tests

* fix final tests'

* adapt from suggestion"
github.com/huggingface/peft,tests/test_encoder_decoder_models.py,2023-11-10T12:33:56Z,"Refactor adapter deletion (#1105)

Description

The job of deleting an adapter is now transferred to the adapter layer,
instead of the adapter model. This makes it easier for users or other
libraries who don't use the adapter model to delete adapters.

Implementation

The code should now be more generic, relying less on hard-coded
attributes.

As a precaution, I also changed the type of adapter_layer_names from
list to tuple, as it should not be mutated.

When deleting the active adapter, the logic for choosing the new active
adapter has been changed slightly to ensure consistency across layers.
In practice, this should rarely make a difference. An error is now
raised if the last remaining adapter is deleted.

Test coverage has been increased:

- Deleting adapters is now also tested for custom models.
- It is also tested for LoHa, LoKr, not only LoRA.
- I added a test for deleting the non-active adapter.

Not implemented

I did not add adapter deletion to IA³, since it is included in #980. LMK
if it should be added here instead."
github.com/huggingface/peft,tests/test_encoder_decoder_models.py,2023-11-09T13:50:35Z,"[`core`] Fix safetensors serialization for shared tensors (#1101)

* fix st serialization

* add test

* add CI test

* add comment"
github.com/huggingface/peft,tests/test_encoder_decoder_models.py,2023-08-08T12:35:19Z,"Add adapter error handling (#800)

When a user tries to add a 2nd adapter, Lora and AdaLora make some checks to
ensure the new adapter is compatible with existing adapters. Currently, that
check is performed halfway through the method. This means that if the check
fails, the new adapter is partially applied, leaving the model in a bad state.
The main purpose of this PR is to ensure that the model state is correct after
such a failure is encountered.

Tests were added to catch this potential bug.

While working on this, I also did some related, but not strictly necessary
changes to the add_adapter methods:

- Previously, the peft_config from the PeftModel was passed to the base
  model. This meant that sometimes, the base model would hold a reference
  to PeftModel.peft_config, but not always, as some base models would
  create new dicts. This is problematic, because some code would rely on
  the objects being the same. Now, they are never the same, leading to
  more consistency.
- I think that the check if multiple adapters have biases (which is not
  supported) was accidentally removed by #749. It is added back in.
- Add some type annotations
- Extend docstrings to contain adapter_name"
github.com/huggingface/peft,tests/test_encoder_decoder_models.py,2023-07-28T11:06:53Z,"Add tests for AdaLoRA, fix a few bugs (#734)

So far, there have been no tests for AdaLoRA. This PR adds tests similar
to the existing ones. While working on those tests, a few bugs were
encountered and fixed.

The changes made to AdaLoRA:

- Linked to paper abstract, not pdf.
- Don't assume that target modules have a .bias attribute (same as for
  LoRA).
- Fixed an issue where it was assumed that if an output object from
  forward has a .loss attribute, it is a scalar, when it can be None.
- Fixed an issue that when init_lora_weights=False, the weights were
  still initialized to be an identity transform.
- When replacing modules, if a target module is a ModuleList or
  ModuleDict, they are now skipped instead of raising an error that the
  module type is not supported. My reasoning was that it is never intended
  to change those modules, so if their names are matched, it must be a
  false positive. The issue arose because for some target modules, the
  names are just k"" etc., and since we match with endswith, this can
  easily lead to modules like ""block"" to match."
github.com/huggingface/peft,tests/test_encoder_decoder_models.py,2023-07-17T08:02:30Z,"FEAT: Make LoRA work with custom models (#676)

Enable custom models to work with LoRA

This PR enables custom models to work with LoRA in peft by performing a few
changes required for non-transformers models. New tests for linear,
transformers conv1d, and conv2d layers were added.

Not yet contained in this PR:

- support for AdaLoRA and IA³
- documentation
- examples

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_encoder_decoder_models.py,2023-07-14T14:28:03Z,"[`Feature`] Save only selected adapters for LoRA (#705)

* v1 working for LoRA

* more checks

* fix prompt learning issues

* fix failing test

* Apply suggestions from code review

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* fixed indentation

* move the check above

* added tests for adaption prompt, enc-dec and feature extraction

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_encoder_decoder_models.py,2023-07-14T14:14:51Z,"[Core] Enhancements and refactoring of LoRA method (#695)

* refactor lora and add utils

1. Refactor LoRA code
2. Add method to delete LoRA adapters
3. Add method to unload the PEFT LoRA model.
4. Add `svd` weighted adapter support.
5. minor fixes

* fixes

* fixes

* Update lora.py

* fixes

* Update lora.py

* docstrings for the added public APIs

* docs

* Update src/peft/tuners/lora.py

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* resolve comments, refactoring and adding tests

* fix the remaining failing tests

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_encoder_decoder_models.py,2023-07-14T12:33:33Z,"[WIP] FIX for disabling adapter, adding tests (#683)

This PR deals with some issues with disabling adapter:

- typo in active.adapter
- prompt encoder could be on wrong device
- when using prompt learning + generate, disabling did not work

For the last point, there is a somewhat ugly fix in place for now,
pending a more comprehensive refactor (a comment was added to that
effect).

Comprehensive tests were added to check that everything works now.

The following tests still not working:

- adaption prompt
- seq2seq with prompt tuning/prompt encoding
- stable diffusion is a little bit flaky but test is hopefully robust enough

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_encoder_decoder_models.py,2023-07-13T09:12:40Z,FIX: base_model_torch_dtype when using model.half() after init (#688)
github.com/huggingface/peft,tests/test_encoder_decoder_models.py,2023-07-07T12:19:10Z,"Remove skipping certain tests (#668)

The generate tests so far were skipped for non-lora, non-prefix tuning
cases. However, those cases are now passing, so it is no longer
necessary to skip the tests."
github.com/huggingface/peft,tests/test_encoder_decoder_models.py,2023-06-27T12:57:57Z,"feat(model): Allow from_pretrained to accept PeftConfig class (#612)

* feat(model): Allow from_pretrained to accept PeftConfig class

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* tests: add test cases for config construction

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: address comments and run tools

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* fix: style

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_encoder_decoder_models.py,2023-06-21T12:13:40Z,add `adapter_name` in `get_peft_model` (#610)
github.com/huggingface/peft,tests/test_encoder_decoder_models.py,2023-06-15T10:23:05Z,"[`core`] Correctly passing the kwargs all over the place (#575)

* v1 of the fix

* forward contrib credits from discussions

* add tests

---------

Co-authored-by: winglian <winglian@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_encoder_decoder_models.py,2023-06-09T10:33:13Z,"[`core`] Add safetensors integration (#553)

* add v1

* clean up

* more improvements

* add device

* final adjustements

* use `EntryNotFoundError`

* better checks

* add tests and final fixes

* make style && make quality

* remove `push_to_hub` because of the release"
github.com/huggingface/peft,tests/test_encoder_decoder_models.py,2023-06-01T07:35:24Z,"[`LoRA`] Allow applying LoRA at different stages (#429)

* working v1

- working v1
- added tests
- needs some documentation

* more fixes

- stronger tests
- documentation
- remove unneeded common layers pattern

* add more docstring

* Apply suggestions from code review

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* quality & style

* style

---------

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_encoder_decoder_models.py,2023-05-31T10:14:27Z,"[`core`] Add gradient checkpointing check (#404)

* add automatic input enable gradients when calling `get_peft_model`

* style

* better check

* add 4bit check"
github.com/huggingface/peft,tests/test_encoder_decoder_models.py,2023-04-14T12:34:01Z,"[`tests`] add CI training tests (#311)

* add training tests

* styling"
github.com/huggingface/peft,tests/test_encoder_decoder_models.py,2023-04-07T10:48:22Z,add and fix tests
github.com/huggingface/peft,tests/test_encoder_decoder_models.py,2023-04-06T13:36:33Z,making adalora compatible with multiple adapters
github.com/huggingface/peft,tests/test_encoder_decoder_models.py,2023-04-05T17:17:01Z,fix test
github.com/huggingface/peft,tests/test_encoder_decoder_models.py,2023-04-05T08:17:26Z,"[`tests`] Adds more tests + fix failing tests (#238)

* adds more tests

- refactor tests
- add enc-dec tests
- skips generate tests for non-lora adapters

* rm unneeded file

* fix tests

* fix

* more checks

* fix issue"
github.com/huggingface/peft,tests/test_multitask_prompt_tuning.py,2024-02-19T12:53:39Z,"FIX: Multitask prompt tuning with other tuning init (#1144)

Resolves #1082.

Also, adding tests for prompt_tuning_init != RANDOM.

---------

Co-authored-by: Mayank Mishra <32954280+mayank31398@users.noreply.github.com>"
github.com/huggingface/peft,tests/test_multitask_prompt_tuning.py,2024-02-14T15:43:47Z,"TST Use plain asserts in tests (#1448)

Use pytest style asserts instead of unittest methods.

Use `pytest.raises` and `pytest.warns` where suitable."
github.com/huggingface/peft,tests/test_multitask_prompt_tuning.py,2024-02-07T11:52:35Z,MNT Move code quality fully to ruff (#1421)
github.com/huggingface/peft,tests/test_multitask_prompt_tuning.py,2024-01-30T11:32:39Z,"Add positional args to PeftModelForCausalLM.generate (#1393)

* add positional args

* update tests"
github.com/huggingface/peft,tests/test_multitask_prompt_tuning.py,2023-12-12T14:16:00Z,"FIX Issues with transformers 4.36 (#1252)

Adjust for different type of past_key_values when using caching.

Also: Fix some seeds for flaky tests.

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>
Co-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>"
github.com/huggingface/peft,tests/test_multitask_prompt_tuning.py,2023-11-15T10:21:23Z,"FEAT: Make safe serialization the default one (#1088)

* make safe serialization the default one

* adapt tests

* fix final tests'

* adapt from suggestion"
github.com/huggingface/peft,tests/test_multitask_prompt_tuning.py,2023-08-25T06:12:11Z,"🎉 Add Multitask Prompt Tuning (#400)

* mpt

* fix save

* fix save

* add jupyter notebook

* add jupyter notebook

* add jupyter notebook

* drop shuffling

* drop classify_dataset

* drop classify_dataset

* fix keys

* fix keys

* add comments

* use EXACT_SOURCE_TASK in the example

* formatting

* Fix dict index in embedding retrieval

* run style and quality

* run style and quality

* run style and quality

* style

* final fix

* style

* comment out failing tests

* fix generation tests

* fix style and save test

* all testcases

* fix import

* add license header

* reformat

* fix encoder-decoder models

* fix tests running multiple times

* fix paper name for IA3 and add MPT paper

* Trigger CI

* address the recommended changes

* reformat

* address suggestions

* address suggestions

* revert reformatting

* revert reformatting

---------

Co-authored-by: Alex-Brooks <Alex.Brooks@ibm.com>"
github.com/shibing624/pycorrector,pycorrector/gpt/merge_peft_adapter.py,2023-11-05T08:35:41Z,update conv seq2seq.
github.com/shibing624/pycorrector,pycorrector/gpt/merge_peft_adapter.py,2023-11-03T04:40:56Z,update merge.
github.com/shibing624/pycorrector,pycorrector/gpt/merge_peft_adapter.py,2023-10-30T09:38:49Z,add llm model for correction.
github.com/deep-diver/LLM-As-Chatbot,models/replit.py,2023-07-25T10:11:50Z,add more models + gptq mode
github.com/deep-diver/LLM-As-Chatbot,models/replit.py,2023-07-01T17:56:43Z,add local files only option
github.com/deep-diver/LLM-As-Chatbot,models/replit.py,2023-06-08T01:59:11Z,exp w/ camel and sam
github.com/deep-diver/LLM-As-Chatbot,models/replit.py,2023-06-05T00:25:50Z,update examples
github.com/deep-diver/LLM-As-Chatbot,models/replit.py,2023-06-04T14:50:44Z,disable force download & example view
github.com/deep-diver/LLM-As-Chatbot,models/replit.py,2023-06-03T14:04:14Z,add 40b wizard falcon
github.com/arcee-ai/mergekit,mergekit/common.py,2024-01-30T00:03:09Z,"Add mergekit-moe script (#141)

Also incidentally adds support for merging Mixtral models."
github.com/arcee-ai/mergekit,mergekit/common.py,2024-01-23T23:16:10Z,"Computation Graph Overhaul (Again) (#127)

Refactor the internal computational graph to make writing new tasks easier.

This time for real."
github.com/arcee-ai/mergekit,mergekit/common.py,2024-01-22T20:57:46Z,Revert #67
github.com/arcee-ai/mergekit,mergekit/common.py,2024-01-22T19:55:04Z,Fix str/bytes issue
github.com/arcee-ai/mergekit,mergekit/common.py,2024-01-22T18:46:46Z,"Computational Graph Overhaul (#67)

Refactor the internal computational graph to make writing new tasks easier.

Huge, invasive change for minimal immediate benefit, but makes my life
easier down the road for implementing more complicated merge methods."
github.com/arcee-ai/mergekit,mergekit/common.py,2024-01-03T05:58:30Z,"Phi 2 (#66)

Add support for Phi 2 models."
github.com/arcee-ai/mergekit,mergekit/common.py,2024-01-02T20:01:24Z,Bump copyright date to 2024
github.com/arcee-ai/mergekit,mergekit/common.py,2023-12-30T05:36:56Z,Propagate trust_remote_code to .config()
github.com/arcee-ai/mergekit,mergekit/common.py,2023-12-03T20:35:50Z,Cleanup and add pre-commit hook
github.com/arcee-ai/mergekit,mergekit/common.py,2023-11-29T07:02:50Z,"Lazy Pytorch Unpickler (#13)

Adds a flag `--lazy-unpickle` to `mergekit-yaml` that lazily loads
individual tensors instead of whole shards.

With this flag enabled I'm able to do a linear merge of two 13b models
in 16GB of RAM.

It's a little hacky and has to touch pytorch internals - I can't
guarantee it'll work right for 100% of models. Do let me know if it
breaks anything."
github.com/arcee-ai/mergekit,mergekit/common.py,2023-11-11T22:34:18Z,Add option for trust_remote_code when merging LoRA
github.com/arcee-ai/mergekit,mergekit/common.py,2023-10-06T03:06:09Z,Create mergekit package
github.com/wenda-LLM/wenda,llms/llm_generic_transformers.py,2023-12-10T14:27:27Z,支持awq
github.com/wenda-LLM/wenda,llms/llm_generic_transformers.py,2023-07-19T15:14:25Z,增加generic_transformers模组，经测试可兼容Llama-2-13B-chat-GPTQ
github.com/netease-youdao/QAnything,third_party/FastChat/fastchat/model/apply_lora.py,2024-01-25T04:29:46Z,ADD: add FastChat assets from commit e86e70d0b48c3d6e2223de96b20e377315dbf73e
github.com/open-compass/opencompass,opencompass/models/modelscope.py,2023-11-22T07:32:21Z,"[Feature] support download from modelscope (#534)

* [Feature] download from modelscope

* [Feature] download from modelscope

* minor fix

---------

Co-authored-by: yingfhu <yingfhu@gmail.com>"
github.com/deep-diver/LLM-As-Chatbot,models/falcon.py,2023-07-26T11:56:18Z,add gptq vram req. templates
github.com/deep-diver/LLM-As-Chatbot,models/falcon.py,2023-07-25T10:11:50Z,add more models + gptq mode
github.com/deep-diver/LLM-As-Chatbot,models/falcon.py,2023-07-01T17:56:43Z,add local files only option
github.com/deep-diver/LLM-As-Chatbot,models/falcon.py,2023-06-11T13:31:52Z,remove half in falcon
github.com/deep-diver/LLM-As-Chatbot,models/falcon.py,2023-06-10T13:08:36Z,add use_safetensor=False
github.com/deep-diver/LLM-As-Chatbot,models/falcon.py,2023-06-08T15:32:11Z,tested gpu and cpu options
github.com/deep-diver/LLM-As-Chatbot,models/falcon.py,2023-06-08T01:59:11Z,exp w/ camel and sam
github.com/deep-diver/LLM-As-Chatbot,models/falcon.py,2023-06-05T00:25:50Z,update examples
github.com/deep-diver/LLM-As-Chatbot,models/falcon.py,2023-05-30T02:12:34Z,add falcon families
github.com/deep-diver/LLM-As-Chatbot,models/alpaca.py,2023-07-25T10:11:50Z,add more models + gptq mode
github.com/deep-diver/LLM-As-Chatbot,models/alpaca.py,2023-07-01T17:56:43Z,add local files only option
github.com/deep-diver/LLM-As-Chatbot,models/alpaca.py,2023-06-10T13:08:36Z,add use_safetensor=False
github.com/deep-diver/LLM-As-Chatbot,models/alpaca.py,2023-06-09T04:33:05Z,update vram requirements records
github.com/deep-diver/LLM-As-Chatbot,models/alpaca.py,2023-06-08T15:32:11Z,tested gpu and cpu options
github.com/deep-diver/LLM-As-Chatbot,models/alpaca.py,2023-06-08T01:59:11Z,exp w/ camel and sam
github.com/deep-diver/LLM-As-Chatbot,models/alpaca.py,2023-06-04T14:50:44Z,disable force download & example view
github.com/deep-diver/LLM-As-Chatbot,models/alpaca.py,2023-06-04T13:37:13Z,add model desc
github.com/deep-diver/LLM-As-Chatbot,models/alpaca.py,2023-06-03T13:18:00Z,add nous-hermes model
github.com/deep-diver/LLM-As-Chatbot,models/alpaca.py,2023-05-04T22:11:52Z,add optimum for bettertrasformer integration
github.com/deep-diver/LLM-As-Chatbot,models/alpaca.py,2023-04-20T16:15:07Z,rebranding
github.com/open-compass/opencompass,opencompass/models/huggingface.py,2024-02-05T15:29:10Z,[Sync] Merge branch 'dev' into zfz/update-keyset-demo (#876)
github.com/open-compass/opencompass,opencompass/models/huggingface.py,2024-01-17T05:48:12Z,"[Sync] Add InternLM2 Keyset Evaluation Demo (#807)

Co-authored-by: zhangyifan1 <zhangyifan1@pjlab.org.cn>"
github.com/open-compass/opencompass,opencompass/models/huggingface.py,2024-01-08T14:07:24Z,[Sync] Sync with internal codes 2023.01.08 (#777)
github.com/open-compass/opencompass,opencompass/models/huggingface.py,2024-01-08T08:40:02Z,"[Feature] *_batch_generate* function, add the MultiTokenEOSCriteria (#772)

* jiangjin1999: in the _batch_generate function, add the MultiTokenEOSCriteria feature to speed up inference.

* jiangjin1999: in the _batch_generate function, add the MultiTokenEOSCriteria feature to speed up inference.

---------

Co-authored-by: jiangjin08 <jiangjin08@MBP-2F32S5MD6P-0029.local>
Co-authored-by: jiangjin08 <jiangjin08@a.sh.vip.dianping.com>"
github.com/open-compass/opencompass,opencompass/models/huggingface.py,2023-12-29T10:46:34Z,"[Feat] update code config (#749)

* [Feat] update code dataset

* [Feat] update code dataset

* [Feat] update code dataset"
github.com/open-compass/opencompass,opencompass/models/huggingface.py,2023-12-11T09:42:53Z,[Sync] minor test (#683)
github.com/open-compass/opencompass,opencompass/models/huggingface.py,2023-12-01T07:08:38Z,"[Feat] update gsm8k and math agent config (#652)

* [Feat] update gsm8k and math agent config

* minor fix"
github.com/open-compass/opencompass,opencompass/models/huggingface.py,2023-11-23T06:05:59Z,"[Sync] Fix cmnli, fix vicuna meta template, fix longbench postprocess and other minor fixes (#625)"
github.com/open-compass/opencompass,opencompass/models/huggingface.py,2023-11-16T13:22:06Z,"[Feat] support humaneval and mbpp pass@k (#598)

* [Feat] support pass@ k

* [Feat] support pass@k

* [Feat] support pass@k

* [Feat] support pass@k

* [Feat] support pass@k

* [Feat] support pass@k docs

* update naming

---------

Co-authored-by: Leymore <zfz-960727@163.com>"
github.com/open-compass/opencompass,opencompass/models/huggingface.py,2023-11-13T07:15:34Z,[Sync] update model configs (#574)
github.com/open-compass/opencompass,opencompass/models/huggingface.py,2023-10-27T08:54:19Z,"[Fix] Enforce `do_sample=False` in HF model (#506)

* update hf model wrapper

* patch llama

---------

Co-authored-by: bot <bot@bot.com>"
github.com/open-compass/opencompass,opencompass/models/huggingface.py,2023-10-27T03:45:41Z,"[Feat] Add _set_model_kwargs_torch_dtype for HF model (#507)

* add _set_model_kwargs_torch_dtype for hf models

* add logger"
github.com/open-compass/opencompass,opencompass/models/huggingface.py,2023-09-27T08:32:40Z,[Sync] Update LongEval (#443)
github.com/open-compass/opencompass,opencompass/models/huggingface.py,2023-08-31T08:53:39Z,"[Fix] Fix when missing both pad and eos token (#287)

* fix when missing both pad and eos token

* update pad_token_id impl"
github.com/open-compass/opencompass,opencompass/models/huggingface.py,2023-08-25T09:36:30Z,"[Feature] Simplify entry script (#204)

* [Feature] Simply entry script

* update"
github.com/open-compass/opencompass,opencompass/models/huggingface.py,2023-08-24T06:07:33Z,"[Fix] Fix bugs for PeftModel generate (#252)

* fix bugs

* fix typo"
github.com/open-compass/opencompass,opencompass/models/huggingface.py,2023-07-28T09:29:37Z,"[Feature] Add SC (#126)

* add self-consistency

* add CoT method Self-Consistency

* fix typo error and update openicl_eval

* add tydiQA-GoldP task

* fix sc

* rename gsm8k_sc

* fix sc

* add self-consistency doc

* refine sc

---------

Authored-by: liushz <qq1791167085@163.com>"
github.com/open-compass/opencompass,opencompass/models/huggingface.py,2023-07-18T08:21:43Z,"[Feature] Support load PEFT adapter for HuggingFace model (#74)

* support peft for HuggingFace model

* add docstring"
github.com/open-compass/opencompass,opencompass/models/huggingface.py,2023-07-17T07:59:10Z,"[Enhancement] Test linting in CI and fix existing linting errors (#69)

* [Enhancement] Test linting in CI

* fix linting"
github.com/open-compass/opencompass,opencompass/models/huggingface.py,2023-07-04T13:34:55Z,initial commit
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2024-02-15T14:31:17Z,LLMs
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2024-01-31T13:25:21Z,bug fixes
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2023-12-07T20:54:50Z,Adding pretraining support for autoencoder 1/n
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2023-11-16T09:35:03Z,Improving Unified RR
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2023-11-12T13:42:04Z,Fix: Add reranking model to optimizer
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2023-11-11T23:14:27Z,LM updates
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2023-08-23T10:39:01Z,Implemented ANCE + clustered
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2023-02-11T23:37:19Z,Unified RR with separate poolers for RR
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2023-02-11T20:56:42Z,Unified RR works but not too well
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2023-02-01T13:04:12Z,Working clustering and large representations
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2022-09-18T17:04:27Z,Pass betas to adamw
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2022-06-10T10:42:52Z,Updated AdamW for all classes
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2022-05-29T15:39:15Z,Bug fixes
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2022-05-05T22:41:00Z,"Added Nystromformer support for language model
Tested 512 and 2048 LM
Added Nystromformer support for NER"
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2022-05-02T17:27:23Z,"Added LayoutLMv2 to ner
Added RemBert to classification
Added Rembert to multiclass classification
Added Rembert to language modeling
Added Rembert to ner
Updated docs
Updated examples"
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2022-01-20T12:11:10Z,Fixed bug where wrong predictions weren't returned in Classification
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2022-01-20T11:42:32Z,Fixed merge conflict
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2022-01-06T22:10:05Z,Removed extraneous quotes from special tokens xlmroberta tokenizer
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2022-01-05T20:02:35Z,Default special tokens corrected
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-11-29T19:56:16Z,bigbird training dataset samples were truncated
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-11-10T20:22:17Z,Merge branch 'master' into dpr
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-11-10T20:18:45Z,Resolved conflicts
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-11-04T09:18:07Z,"Bugfix for SentencePiece trainger
Command line execution requires quotes
API errantly adds the quote to the token"
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-10-02T12:57:47Z,tensorboard_fix
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-10-02T11:08:52Z,Merge branch 'whr778-language_modeling_SentencePiece_bugfix'
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-10-02T11:08:44Z,Resolved conflict when merging sentencepiece fix
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-10-02T11:04:31Z,replaced tensorboardx imports with default torch version
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-09-24T12:31:40Z,Merge branch 'Zhylkaaa-cls_loss_refactor'
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-09-24T12:30:19Z,Formatting
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-09-11T18:38:00Z,added run id to model attributes
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-09-07T11:49:23Z,"SentencePiece is not returning a '<unk>' token, even though it is set correctly in the tokenizer.
Explicitly set tokenizer.unk_token.
Stripping and not setting tokenizer.unk_token caused misaligned predictions from original input tokens.  Modified the block to use tokenizer.unk_token if word_tokens is empty."
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-08-30T10:54:29Z,"BigBird and XLMRoberta required tokens were not accounted for so that the vocab_size of a pretrained model was off by 2 for XLMRoberta and 3 for BigBird. Subsequently, the embedding size of a pretrained model was different than the size saved in config.json"
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-08-25T18:16:33Z,DPR working
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-08-08T17:07:42Z,Started dpr
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-07-24T10:41:55Z,bug with sentencepiece model staging
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-07-24T10:32:00Z,sentencepiece saves the vocabulary and tokenization model on the fly... no need to save
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-07-24T10:29:44Z,Comments
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-07-24T10:26:20Z,updated call to sentencepiece vocab trainer based on xlmroberta or bigbird
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-07-24T10:22:48Z,Updated special tokens
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-07-24T10:21:02Z,Updated special tokens
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-07-24T10:02:18Z,Sentencepiece for xlmroberta... pretrain support
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-07-23T17:05:20Z,Bugfix: Wrong initialization class for xlmroberta
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-07-23T16:59:39Z,Bugfix: Wrong initialization class for xlmroberta
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-07-23T11:35:30Z,Whatever IDE code reformatter was used on this file on 5/18 injected a lot of parenthesis around if blocks and assignments changing a tensor into a single tuple of one tensor... I backed out almost all of the formatting retaining the changes up to 0.6.10
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-07-22T20:21:17Z,Added XLMRoberta to Language Modeling
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-07-19T16:57:22Z,Fixed a bug with loading the tokenizer model
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-07-19T16:41:51Z,"Updated paths for moving the sentencepiece models after building
There is no way to provide an output path documented"
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-07-14T13:06:54Z,Changed sentencepiece to google... transformers/tokenizers does not produce a bigbird compatible model
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-07-14T12:38:11Z,Changed sentencepiece to google... transformers/tokenizers does not produce a bigbird compatible model
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-07-13T14:41:46Z,Added bigbird and sentencepiece to language modeling
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-07-13T14:39:56Z,Added bigbird and sentencepiece to language modeling
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-07-09T03:22:03Z,fix tensorboard error
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-06-20T21:11:46Z,wandb repo label update
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-05-18T19:39:41Z,Formatted with black defaults
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-05-18T19:38:42Z,Added repo to wandb config
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-05-17T14:06:58Z,Huggingface Datasets fixes
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-03-27T10:21:23Z,formatting
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-03-27T09:56:37Z,Merge branch 'fix/1002-fix-load-on-cpu' of https://github.com/iomedhealth/simpletransformers into iomedhealth-fix/1002-fix-load-on-cpu
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-03-19T10:03:33Z,Updated changelog
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-03-07T18:58:40Z,T5 done too. Probably.
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-02-24T10:57:05Z,fix bug when loading pretrained model on machine without GPU
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-02-19T09:58:17Z,HF datasets for LM
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-02-18T16:49:15Z,Fixed polynomial_decay_schedule_power not being used correctly
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2021-01-11T03:14:59Z,Fix evaluate_during_training train mode
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-12-05T14:45:14Z,Added support for adafactor and various schedulers for all models
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-12-01T11:26:02Z,Compatibility fix for transformer v4.0
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-11-23T12:58:09Z,Fixed wandb sweep issues
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-11-09T13:07:55Z,Added layout lm. Electra compatibility
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-10-29T07:35:04Z,Updated arguments names for HF methods
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-10-12T13:19:31Z,Fixed issues with using mixed precision training with LanguageModelingModel
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-10-12T08:52:13Z,Merge branch 'fix-sweep-wandb-compatibility' of https://github.com/jonatanklosko/simpletransformers into jonatanklosko-fix-sweep-wandb-compatibility
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-10-11T13:23:02Z,The train_model method now returns training detail
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-10-10T12:31:08Z,Fix compatibility with different versions of wandb when sweeping.
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-10-07T15:04:24Z,Moved model.train()
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-09-26T07:39:39Z,Only average eval_loss for multigpu
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-09-25T18:45:32Z,Implemented layoutlm model
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-09-02T18:35:51Z,Added dynamic quantization support for more models
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-08-04T21:49:42Z,Removed nullcontext for python 3.6 compatibility
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-08-04T20:44:35Z,Fixed bug where amp import was attempted when fp16 is False
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-07-29T12:48:04Z,Use native PyTorch implementation of AMP instead of Apex
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-07-07T22:05:57Z,Bug fix in ELECTRA
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-07-05T21:53:08Z,Merge branch 'param_groups'
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-07-05T21:50:59Z,Merge branch 'master' into docs
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-07-05T21:50:51Z,Added get_named_parameters() method
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-07-05T20:10:20Z,Electra pre-training no longer replaces with random tokens
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-07-05T19:23:28Z,Added options to set custom param_groups
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-07-05T10:03:11Z,Resolved merge conflict
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-07-05T09:48:53Z,Merge branch 'master' into logging
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-07-04T20:49:22Z,fixed tokenizer saving issue in language modeling
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-07-04T19:11:13Z,Merge branch 'tokenizer-model-args' of https://github.com/taranais/simpletransformers into taranais-tokenizer-model-args
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-07-02T07:23:27Z,tokenizer model args
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-06-26T14:32:35Z,Don't create cache_dir when no_cache is True
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-06-24T17:27:37Z,Moved running loss inside tqdm description
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-06-22T14:29:41Z,Added sweep support in multimodal
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-06-22T14:11:25Z,"Added sweep support to convai, language_generation, seq2seq"
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-06-18T20:54:31Z,Added sweeps support for T5 and LM training
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-06-14T20:31:55Z,Added sweep support for multilabel classification
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-06-09T12:08:46Z,Formatting
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-06-09T11:59:23Z,Merge branch 'cahya-wirawan-dev'
github.com/ThilinaRajapakse/simpletransformers,simpletransformers/language_modeling/language_modeling_model.py,2020-06-09T11:58:40Z,Bug fix for when local_rank is not explicitly specified
github.com/dvlab-research/LongLoRA,inference-qlora.py,2023-11-19T06:30:19Z,Update inference-qlora.py
github.com/dvlab-research/LongLoRA,inference-qlora.py,2023-11-07T15:51:37Z,uploaded inference script using qlora
github.com/PKU-YuanGroup/Video-LLaVA,videollava/model/builder.py,2024-01-16T03:38:37Z,reformat code
github.com/liucongg/ChatGLM-Finetuning,merge_lora.py,2023-08-06T08:33:35Z,update code
github.com/liucongg/ChatGLM-Finetuning,merge_lora.py,2023-08-06T08:32:47Z,update code
github.com/allenai/OLMo,inference/compression/dependencies/AutoGPTQ/auto_gptq/utils/peft_utils.py,2023-09-14T00:10:10Z,style and lib sort changes
github.com/allenai/OLMo,inference/compression/dependencies/AutoGPTQ/auto_gptq/utils/peft_utils.py,2023-09-13T20:19:23Z,compression files
github.com/dvlab-research/LongLoRA,eval_distributed.py,2023-11-19T00:39:45Z,Added multiple GPUs evaluation.
github.com/OpenPipe/OpenPipe,examples/classify-recipes/utils.py,2023-08-24T23:49:44Z,more work
github.com/OpenPipe/OpenPipe,examples/classify-recipes/utils.py,2023-08-24T18:43:42Z,generate-data and some eval
github.com/huggingface/trl,examples/research_projects/stack_llama/scripts/merge_peft_adapter.py,2023-10-06T09:04:58Z,"Clarify docstrings, help messages, assert messages in merge_peft_adapter.py (#838)

An assertion was also corrected to the intended test condition"
github.com/huggingface/trl,examples/research_projects/stack_llama/scripts/merge_peft_adapter.py,2023-07-14T10:00:56Z,"[`examples`] Big refactor of examples and documentation (#509)

* added sfttrainer and rmtrainer example scripts.

* added few lines in the documentation.

* moved notebooks.

* delete `examples/summarization`

* remove from docs as well

* refactor sentiment tuning

* more refactoring.

* updated docs for multi-adapter RL.

* add research projects folder

* more refactor

* refactor docs.

* refactor structure

* add correct scripts all over the place

* final touches

* final touches

* updated documentation from feedback."
github.com/netease-youdao/QAnything,third_party/FastChat/fastchat/model/model_adapter.py,2024-01-25T04:29:46Z,ADD: add FastChat assets from commit e86e70d0b48c3d6e2223de96b20e377315dbf73e
github.com/bigscience-workshop/petals,tests/test_full_model.py,2023-08-30T02:59:33Z,Fix `.generate(input_ids=...)` (#485)
github.com/bigscience-workshop/petals,tests/test_full_model.py,2023-08-20T15:18:36Z,"Make client compatible with transformers' GenerationMixin (#464)

This PR drops custom generation codes and introduces compatibility with `transformers.GenerationMixin` instead. This includes support for more sampling options (`top_p`, `top_k`, `repetition_penalty` requested in #460) and beam search - all that is now identical to running model with transformers locally.

Most features (excluding beam search and other rarely used stuff) are also compatible with resuming existing sessions.

### Breaking changes

If `.generate()` or forward passes are being run inside an `.inference_session()` context, they now use the opened session by default. So, these snippets are now equivalent:

```python
# Using default session
with model.inference_session(max_length=100):
    output_ids = model.generate(input_ids, max_new_tokens=3)

# Explicitly specifying a session
with model.inference_session(max_length=100) as sess:
    output_ids = model.generate(input_ids, max_new_tokens=3, session=sess)
```

Earlier, the 1st snippet was creating a new session, which is not what most people expected (= such code was most likely to introduce a bug, which is now fixed)."
github.com/bigscience-workshop/petals,tests/test_full_model.py,2023-08-08T15:10:27Z,"Test Llama, rebalancing, throughput eval, and all CLI scripts (#452)

This PR extends CI to:

1. Test Llama code using [TinyLlama-v0](https://huggingface.co/Maykeye/TinyLLama-v0).
2. Test rebalancing (sets up a situation where the 1st server needs to change its original position).
3. Check if benchmark scripts run (in case someone breaks its code). Note that the benchmark results are meaningless here (since they're measured on a tiny swarm of CPU servers, with low `--n_steps`).
4. Test `petals.cli.run_dht`.
5. Increase swap space and watch free RAM (a common issue is that actions are cancelled without explanation if there's not enough RAM - so it's a useful reminder + debug tool).
6. Fix flapping tests for bloom-560m by increasing tolerance.

Other minor changes: fix `--help` messages to show defaults, fix docs, tune rebalancing constants."
github.com/bigscience-workshop/petals,tests/test_full_model.py,2023-07-22T19:10:46Z,"Split long sequences into chunks (#403)

This PR is designed to avoid OOMs when processing long sequences that happen due to the huge attention logits matrices.

Co-authored-by: Alexander Borzunov <borzunov.alexander@gmail.com>"
github.com/bigscience-workshop/petals,tests/test_full_model.py,2023-07-12T12:22:28Z,"Support peft LoRA adapters (#335)

Implement an option to deploy PEFT adapters to a server. Clients can set active_adapter=... to use these adapters.

---------

Co-authored-by: Aleksandr Borzunov <borzunov.alexander@gmail.com>
Co-authored-by: justheuristic <justheuristic@gmail.com>"
github.com/bigscience-workshop/petals,tests/test_full_model.py,2023-06-23T11:46:10Z,"Add LLaMA support (#323)

This PR:

1. **Abolishes the model conversion procedure.** Now, models are downloaded directly from original repositories like https://huggingface.co/bigscience/bloom. Servers download only shards with blocks to be hosted, and clients download only shards with input/output embeddings and layernorms.

    - BLOOM is loaded from `bigscience/bloom`, but we use the DHT prefix `bigscience/bloom-petals` for backward compatibility. Same with smaller BLOOMs and BLOOMZ.
    - LLaMA can be loaded from any repo like `username/llama-65b-hf`, but we use the DHT prefix `llama-65b-hf` (without the username) to accomodate blocks from different repos (there're a few of them with minor differences, such as `Llama` vs. `LLaMA` in the class name).

2. **Refactors the client to generalize it for multiple models.** Now, we have `petals.models` packages that contain model-specific code (e.g. `petals.models.bloom`, `petals.models.llama`). General code (e.g. CPU-efficient LM head, p-tuning) is kept in `petals.client`.

3. **Introduces** `WrappedLlamaBlock`, `DistributedLlamaConfig`, `DistributedLlamaForCausalLM`, `DistributedLlamaForSequenceClassification`, and `DistributedLlamaModel` compatible with Petals functionality (p-tuning, adapters, etc.).

4. **Introduces** `AutoDistributedConfig` that automatically chooses the correct config class (`DistributedLlamaConfig` or `DistributedBloomConfig`). The refactored configs contain all model-specific info for both clients and servers.

Upgrade instructions:

- Remove disk caches for blocks in old (converted) format to save disk space. That is, remove `~/.cache/petals/model--bigscience--bloom-petals` and  `~/.cache/petals/model--bigscience--bloomz-petals` directories (if present)."
github.com/bigscience-workshop/petals,tests/test_full_model.py,2023-03-12T21:49:04Z,"Speed up loading blocks using init with meta weights (#285)

* Init WrappedBloomBlock with meta weights

---------

Co-authored-by: Alexander Borzunov <borzunov.alexander@gmail.com>"
github.com/bigscience-workshop/petals,tests/test_full_model.py,2023-02-19T01:46:17Z,Use get_logger(__name__) instead of get_logger(__file__) (#265)
github.com/bigscience-workshop/petals,tests/test_full_model.py,2022-12-15T05:12:18Z,"Fix logging: do not duplicate lines, enable colors in Colab (#156)"
github.com/bigscience-workshop/petals,tests/test_full_model.py,2022-12-13T17:09:15Z,"Add missing methods for SamplingAlgorithm, fix docstrings (#107)

* Add missing methods for SamplingAlgorithm, fix docstrings

* Add SamplingAlgorithm to _choose_sample_algorithm

* Add test_sampling

* Add a warning if sampling options were passed, but do_sample=False

* Skip the sampling test for now

Co-authored-by: Alexander Borzunov <borzunov.alexander@gmail.com>"
github.com/bigscience-workshop/petals,tests/test_full_model.py,2022-12-13T08:03:49Z,"Bump transformers to 4.25.1 (#151)

- latest accelerate, transformers, huggingface_hub
- rearrange attention caches to support https://github.com/huggingface/transformers/pull/18344
- remove unused code
- fix edge case where session crashes when receiving seq length 0
- assert transformer version when importing WrappedBloomBlock

Co-authored-by: Alexander Borzunov <borzunov.alexander@gmail.com>
Co-authored-by: Max Ryabinin <mryabinin0@gmail.com>"
github.com/bigscience-workshop/petals,tests/test_full_model.py,2022-11-30T06:41:13Z,"Make Petals a pip-installable package (attempt 2) (#102)

1. Petals can be now installed using `pip install git+https://github.com/bigscience-workshop/petals`
    - In case if you already cloned the repo, you can do `pip install .` or `pip install .[dev]`
2. Moved `src` => `src/petals`
    - Replaced `from src.smth import smth` with `from petals.smth import smth`
3. Moved `cli` => `src/petals/cli`
    - Replaced `python -m cli.run_smth` with `python -m petals.cli.run_smth` (all utilities are now available right after pip installation)
4. Moved the `requirements*.txt` contents to `setup.cfg` (`requirements.txt` for packages is not supported well by modern packaging utils)
5. Increased the package version from `0.2` to `1.0alpha1`"
github.com/bigscience-workshop/petals,tests/test_full_model.py,2022-11-28T09:02:07Z,"Add Beam Search decoding algorithm (#87)

Add beam_search"
github.com/bigscience-workshop/petals,tests/test_full_model.py,2022-08-29T18:04:37Z,"Let users specify sequence length instead of assuming 2048 (#52)

- Maximum length is now provided in `.inference_session(max_length=100)`
   - previously, we would always assume max length = 2048
- added a generic way to forward **kwargs to inference session
  - for compatibility with #47 
  - Note to @borzunov : it does *not* pass them arbitrarily, but instead checks for kwarg names at the bottom level
- run_server can be started with a custom max_length for inference
- renamed --cache_size_bytes to --attention_cache_bytes (to avoid collision with --cache_dir)
- --attn_cache_bytes can now support humane file sizes (e.g. 300MB instead of 314572800)
- made some server-side errors more human-readable to user (e.g. when max length is exceeded)

Co-authored-by: Aleksandr Borzunov <borzunov.alexander@gmail.com>
Co-authored-by: Alexander Borzunov <hxrussia@gmail.com>"
github.com/bigscience-workshop/petals,tests/test_full_model.py,2022-08-17T15:50:52Z,"Reduce vocabulary size in test model, fix bug in routing when overlapped (#45)

This PR reduces this vocabulary size to save memory during conversion, keeping only the first 50k tokens
As a result, 

* tests that load client-side embeddings need significantly less RAM
* we can now run CI tests with 4 servers instead of 2 - needed to test routing - see bugs uncovered
* some of the servers now use load balancing
* CI convert_model now takes 4-5 minutes (was 6-7)"
github.com/bigscience-workshop/petals,tests/test_full_model.py,2022-07-27T06:19:45Z,"Pack of Inference Changes (#37)

* Return multibatch mode

* Add tests

* fixes"
github.com/bigscience-workshop/petals,tests/test_full_model.py,2022-07-22T19:38:40Z,"Miscellaneous fixes to automatic tests (#35)

1. __Reduce memory usage in in test_full_model__ 
     - previously, loading the full model would consistently fail IF github is enforcing memory limit [example](https://github.com/bigscience-workshop/distributed-bloom/runs/7473920049?check_suite_focus=true)
     - the new version uses accelerate to save 2GB of peak memory, that was previously used when loading both reference model AND its state dict at the same time - only to load that state dict :)
2. __Safer delays when creating servers__
    - run-tests will now wait for a few seconds after creating the first server - and before creating the second one, so as to make 
sure that the first server creates a DHT instance that subsequent servers can connect to.
    - also increased the wait time after creating servers by 30 seconds to make sure we load the model in time even when bumping into slow remotes on HF side
3. __Fix environment variables in CI to avoid build conflicts__
    - the previous code was using a wrong environment variable that was always ""main"". The current one will correctly resolve branch name, both in main and on pull request.
    - For reference, below you can find sample environments when running CI in both cases: on pull request and on push to main.

<details>
<summary> Environment variables when building this branch (on pull request) </summary>

SELENIUM_JAR_PATH=/usr/share/java/selenium-server.jar GOROOT_1_17_X64=/opt/hostedtoolcache/go/1.17.12/x64 CONDA=/usr/share/miniconda GITHUB_WORKSPACE=/home/runner/work/distributed-bloom/distributed-bloom JAVA_HOME_11_X64=/usr/lib/jvm/temurin-11-jdk-amd64 GITHUB_PATH=/home/runner/work/_temp/_runner_file_commands/add_path_0aba811a-a04b-40a2-ba42-79efb2723e9e GITHUB_ACTION=__run_2 JAVA_HOME=/usr/lib/jvm/temurin-11-jdk-amd64 GITHUB_RUN_NUMBER=98 RUNNER_NAME=GitHub Actions 3 GRADLE_HOME=/usr/share/gradle-7.5 XDG_CONFIG_HOME=/home/runner/.config DOTNET_SKIP_FIRST_TIME_EXPERIENCE=1 ANT_HOME=/usr/share/ant JAVA_HOME_8_X64=/usr/lib/jvm/temurin-8-jdk-amd64 HOMEBREW_PREFIX=/home/linuxbrew/.linuxbrew pythonLocation=/opt/hostedtoolcache/Python/3.9.13/x64 GITHUB_REF_TYPE=branch HOMEBREW_CLEANUP_PERIODIC_FULL_DAYS=3650 BOOTSTRAP_HASKELL_NONINTERACTIVE=1 *** PIPX_BIN_DIR=/opt/pipx_bin DEPLOYMENT_BASEPATH=/opt/runner GITHUB_ACTIONS=true ANDROID_NDK_LATEST_HOME=/usr/local/lib/android/sdk/ndk/24.0.8215888 GITHUB_SHA=3b457e8a14e5ecb0d65d6e4c0e9161f7756a8861 POWERSHELL_DISTRIBUTION_CHANNEL=GitHub-Actions-ubuntu20 DOTNET_MULTILEVEL_LOOKUP=0 GITHUB_REF=refs/pull/35/merge RUNNER_OS=Linux GITHUB_REF_PROTECTED=false HOME=/home/runner GITHUB_API_URL=https://api.github.com/ LANG=C.UTF-8 BLOOM_TESTING_WRITE_TOKEN=*** RUNNER_TRACKING_ID=github_cc9b46e4-56a1-40c5-ba08-5a91e21f0f95 STATS_KEEPALIVE=false RUNNER_ARCH=X64 RUNNER_TEMP=/home/runner/work/_temp EDGEWEBDRIVER=/usr/local/share/edge_driver GITHUB_ENV=/home/runner/work/_temp/_runner_file_commands/set_env_0aba811a-a04b-40a2-ba42-79efb2723e9e GITHUB_EVENT_PATH=/home/runner/work/_temp/_github_workflow/event.json INVOCATION_ID=8f0072e74f2847c0851e7ff9b5e4af7c GITHUB_EVENT_NAME=pull_request GITHUB_RUN_ID=2720198689 JAVA_HOME_17_X64=/usr/lib/jvm/temurin-17-jdk-amd64 ANDROID_NDK_HOME=/usr/local/lib/android/sdk/ndk-bundle GITHUB_STEP_SUMMARY=/home/runner/work/_temp/_runner_file_commands/step_summary_0aba811a-a04b-40a2-ba42-79efb2723e9e HOMEBREW_NO_AUTO_UPDATE=1 GITHUB_ACTOR=justheuristic NVM_DIR=/home/runner/.nvm SGX_AESM_ADDR=1 GITHUB_RUN_ATTEMPT=1 ANDROID_HOME=/usr/local/lib/android/sdk GITHUB_GRAPHQL_URL=https://api.github.com/graphql ACCEPT_EULA=Y RUNNER_USER=runner USER=runner GITHUB_SERVER_URL=https://github.com/ HOMEBREW_CELLAR=/home/linuxbrew/.linuxbrew/Cellar PIPX_HOME=/opt/pipx GECKOWEBDRIVER=/usr/local/share/gecko_driver CHROMEWEBDRIVER=/usr/local/share/chrome_driver SHLVL=0 ANDROID_SDK_ROOT=/usr/local/lib/android/sdk VCPKG_INSTALLATION_ROOT=/usr/local/share/vcpkg HOMEBREW_REPOSITORY=/home/linuxbrew/.linuxbrew/Homebrew RUNNER_TOOL_CACHE=/opt/hostedtoolcache ImageVersion=20220717.1 DOTNET_NOLOGO=1 GITHUB_REF_NAME=35/merge STATS_PFS=true GRAALVM_11_ROOT=/usr/local/graalvm/graalvm-ce-java11-22.1.0 GITHUB_JOB=convert-model LD_LIBRARY_PATH=/opt/hostedtoolcache/Python/3.9.13/x64/lib XDG_RUNTIME_DIR=/run/user/1001 AZURE_EXTENSION_DIR=/opt/az/azcliextensions PERFLOG_LOCATION_SETTING=RUNNER_PERFLOG GITHUB_REPOSITORY=bigscience-workshop/distributed-bloom ANDROID_NDK_ROOT=/usr/local/lib/android/sdk/ndk-bundle CHROME_BIN=/usr/bin/google-chrome GOROOT_1_18_X64=/opt/hostedtoolcache/go/1.18.4/x64 GITHUB_RETENTION_DAYS=90 JOURNAL_STREAM=8:23653 RUNNER_WORKSPACE=/home/runner/work/distributed-bloom LEIN_HOME=/usr/local/lib/lein LEIN_JAR=/usr/local/lib/lein/self-installs/leiningen-2.9.8-standalone.jar GITHUB_ACTION_REPOSITORY= PATH=/opt/hostedtoolcache/Python/3.9.13/x64/bin:/opt/hostedtoolcache/Python/3.9.13/x64:/home/linuxbrew/.linuxbrew/bin:/home/linuxbrew/.linuxbrew/sbin:/home/runner/.local/bin:/opt/pipx_bin:/home/runner/.cargo/bin:/home/runner/.config/composer/vendor/bin:/usr/local/.ghcup/bin:/home/runner/.dotnet/tools:/snap/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin RUNNER_PERFLOG=/home/runner/perflog GITHUB_BASE_REF=main CI=true SWIFT_PATH=/usr/share/swift/usr/bin ImageOS=ubuntu20 GITHUB_REPOSITORY_OWNER=bigscience-workshop GITHUB_HEAD_REF=fix-branch-name GITHUB_ACTION_REF= GITHUB_WORKFLOW=Tests DEBIAN_FRONTEND=noninteractive AGENT_TOOLSDIRECTORY=/opt/hostedtoolcache GOROOT_1_16_X64=/opt/hostedtoolcache/go/1.16.15/x64 _=/usr/bin/env
</details>
<details>
<summary> Environment variables when building in main (on push) </summary>

SELENIUM_JAR_PATH=/usr/share/java/selenium-server.jar GOROOT_1_17_X64=/opt/hostedtoolcache/go/1.17.11/x64 CONDA=/usr/share/miniconda GITHUB_WORKSPACE=/home/runner/work/distributed-bloom/distributed-bloom JAVA_HOME_11_X64=/usr/lib/jvm/temurin-11-jdk-amd64 GITHUB_PATH=/home/runner/work/_temp/_runner_file_commands/add_path_cd6c1ed2-0d0f-496d-b7a6-ffa476dcc144 GITHUB_ACTION=__run_2 JAVA_HOME=/usr/lib/jvm/temurin-11-jdk-amd64 GITHUB_RUN_NUMBER=53 RUNNER_NAME=GitHub Actions 3 GRADLE_HOME=/usr/share/gradle-7.4.2 XDG_CONFIG_HOME=/home/runner/.config DOTNET_SKIP_FIRST_TIME_EXPERIENCE=1 ANT_HOME=/usr/share/ant JAVA_HOME_8_X64=/usr/lib/jvm/temurin-8-jdk-amd64 HOMEBREW_PREFIX=/home/linuxbrew/.linuxbrew pythonLocation=/opt/hostedtoolcache/Python/3.9.13/x64 GITHUB_REF_TYPE=branch HOMEBREW_CLEANUP_PERIODIC_FULL_DAYS=3650 BOOTSTRAP_HASKELL_NONINTERACTIVE=1 *** PIPX_BIN_DIR=/opt/pipx_bin DEPLOYMENT_BASEPATH=/opt/runner GITHUB_ACTIONS=true ANDROID_NDK_LATEST_HOME=/usr/local/lib/android/sdk/ndk/24.0.8215888 GITHUB_SHA=49242d81006454d687ff3293c49f6bf234793627 POWERSHELL_DISTRIBUTION_CHANNEL=GitHub-Actions-ubuntu20 DOTNET_MULTILEVEL_LOOKUP=0 GITHUB_REF=refs/heads/main RUNNER_OS=Linux GITHUB_REF_PROTECTED=true HOME=/home/runner GITHUB_API_URL=https://api.github.com/ LANG=C.UTF-8 BLOOM_TESTING_WRITE_TOKEN=*** RUNNER_TRACKING_ID=github_7668f06a-99e1-4ed1-81e9-46d75fab3f33 STATS_KEEPALIVE=false RUNNER_ARCH=X64 RUNNER_TEMP=/home/runner/work/_temp EDGEWEBDRIVER=/usr/local/share/edge_driver GITHUB_ENV=/home/runner/work/_temp/_runner_file_commands/set_env_cd6c1ed2-0d0f-496d-b7a6-ffa476dcc144 GITHUB_EVENT_PATH=/home/runner/work/_temp/_github_workflow/event.json INVOCATION_ID=3dadac48981b4a679a33224db89be1ed GITHUB_EVENT_NAME=push GITHUB_RUN_ID=2680158280 JAVA_HOME_17_X64=/usr/lib/jvm/temurin-17-jdk-amd64 ANDROID_NDK_HOME=/usr/local/lib/android/sdk/ndk-bundle GITHUB_STEP_SUMMARY=/home/runner/work/_temp/_runner_file_commands/step_summary_cd6c1ed2-0d0f-496d-b7a6-ffa476dcc144 HOMEBREW_NO_AUTO_UPDATE=1 GITHUB_ACTOR=justheuristic NVM_DIR=/home/runner/.nvm SGX_AESM_ADDR=1 GITHUB_RUN_ATTEMPT=1 ANDROID_HOME=/usr/local/lib/android/sdk GITHUB_GRAPHQL_URL=https://api.github.com/graphql ACCEPT_EULA=Y RUNNER_USER=runner USER=runner GITHUB_SERVER_URL=https://github.com/ HOMEBREW_CELLAR=/home/linuxbrew/.linuxbrew/Cellar PIPX_HOME=/opt/pipx GECKOWEBDRIVER=/usr/local/share/gecko_driver CHROMEWEBDRIVER=/usr/local/share/chrome_driver SHLVL=0 ANDROID_SDK_ROOT=/usr/local/lib/android/sdk VCPKG_INSTALLATION_ROOT=/usr/local/share/vcpkg HOMEBREW_REPOSITORY=/home/linuxbrew/.linuxbrew/Homebrew RUNNER_TOOL_CACHE=/opt/hostedtoolcache ImageVersion=20220710.1 DOTNET_NOLOGO=1 GITHUB_REF_NAME=main STATS_PFS=true GRAALVM_11_ROOT=/usr/local/graalvm/graalvm-ce-java11-22.1.0 GITHUB_JOB=convert-model LD_LIBRARY_PATH=/opt/hostedtoolcache/Python/3.9.13/x64/lib XDG_RUNTIME_DIR=/run/user/1001 AZURE_EXTENSION_DIR=/opt/az/azcliextensions PERFLOG_LOCATION_SETTING=RUNNER_PERFLOG GITHUB_REPOSITORY=bigscience-workshop/distributed-bloom CHROME_BIN=/usr/bin/google-chrome ANDROID_NDK_ROOT=/usr/local/lib/android/sdk/ndk-bundle GOROOT_1_18_X64=/opt/hostedtoolcache/go/1.18.3/x64 GITHUB_RETENTION_DAYS=90 JOURNAL_STREAM=8:22000 RUNNER_WORKSPACE=/home/runner/work/distributed-bloom LEIN_HOME=/usr/local/lib/lein LEIN_JAR=/usr/local/lib/lein/self-installs/leiningen-2.9.8-standalone.jar GITHUB_ACTION_REPOSITORY= PATH=/opt/hostedtoolcache/Python/3.9.13/x64/bin:/opt/hostedtoolcache/Python/3.9.13/x64:/home/linuxbrew/.linuxbrew/bin:/home/linuxbrew/.linuxbrew/sbin:/home/runner/.local/bin:/opt/pipx_bin:/home/runner/.cargo/bin:/home/runner/.config/composer/vendor/bin:/usr/local/.ghcup/bin:/home/runner/.dotnet/tools:/snap/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin RUNNER_PERFLOG=/home/runner/perflog GITHUB_BASE_REF= CI=true SWIFT_PATH=/usr/share/swift/usr/bin ImageOS=ubuntu20 GITHUB_REPOSITORY_OWNER=bigscience-workshop GITHUB_HEAD_REF= GITHUB_ACTION_REF= GITHUB_WORKFLOW=Tests DEBIAN_FRONTEND=noninteractive AGENT_TOOLSDIRECTORY=/opt/hostedtoolcache GOROOT_1_16_X64=/opt/hostedtoolcache/go/1.16.15/x64 _=/usr/bin/env
</details>



Co-authored-by: Dmitry Baranchuk <dmitrybaranchuk@gmail.com>"
github.com/bigscience-workshop/petals,tests/test_full_model.py,2022-07-19T01:28:04Z,"Implement RemoteSequential slicing and extra repr, add tests (#30)

- finish renaming RemoteSequenceInfo -> RemoteSequenceManager (why: if it was an *Info, user would expect it to be similar - to a dataclass; whereas in actuality, the class is doing heavy network interactions on its own)
- implement RemoteSequenceManager.make_sequence (from https://pastebin.com/uXgy2U8B )
- make RemoteSequentialInferenceSession use RemoteSequenceManager.make_sequence
- make tests pass again
- make it possible to create inference session without RemoteTransformerBlock
- make a standalone test for RemoteSequential
- rollback convert-model

Co-authored-by: Tim Dettmers <tim.dettmers@gmail.com>"
github.com/bigscience-workshop/petals,tests/test_full_model.py,2022-07-15T22:59:23Z,"Add automated tests (#23)

This PR will run basic tests automatically on each subsequent PR

- convert a small model on every PR
- run existing tests on every PR
- enforce black / isort
- require checks on merge
- make sure tests are not flappy

Co-authored-by: Alexander Borzunov <hxrussia@gmail.com>
Co-authored-by: Dmitry Baranchuk <dmitrybaranchuk@gmail.com>"
github.com/bigscience-workshop/petals,tests/test_full_model.py,2022-07-08T16:11:55Z,rm prefix from tests
github.com/bigscience-workshop/petals,tests/test_full_model.py,2022-07-06T22:06:03Z,black-isort
github.com/bigscience-workshop/petals,tests/test_full_model.py,2022-07-06T22:04:47Z,black-isort
github.com/bigscience-workshop/petals,tests/test_full_model.py,2022-07-04T22:54:47Z,design interface & refactoring
github.com/bigscience-workshop/petals,tests/test_full_model.py,2022-07-01T00:48:36Z,compare logits to logits
github.com/bigscience-workshop/petals,tests/test_full_model.py,2022-07-01T00:38:38Z,test full model exact match
github.com/shibing624/MedicalGPT,inference.py,2024-01-30T04:26:15Z,update infer.
github.com/shibing624/MedicalGPT,inference.py,2024-01-29T07:20:20Z,update eos token.
github.com/shibing624/MedicalGPT,inference.py,2024-01-28T13:23:01Z,update model infer.
github.com/shibing624/MedicalGPT,inference.py,2024-01-18T10:20:23Z,add single tune.
github.com/shibing624/MedicalGPT,inference.py,2024-01-07T15:03:40Z,update group
github.com/shibing624/MedicalGPT,inference.py,2023-11-16T07:48:21Z,update infer.
github.com/shibing624/MedicalGPT,inference.py,2023-11-15T06:48:12Z,update infer use greedy decoding.
github.com/shibing624/MedicalGPT,inference.py,2023-10-23T09:06:58Z,update left padding.
github.com/shibing624/MedicalGPT,inference.py,2023-10-23T08:51:09Z,update decode with batch infer.
github.com/shibing624/MedicalGPT,inference.py,2023-10-23T07:40:32Z,update readme.
github.com/shibing624/MedicalGPT,inference.py,2023-10-23T07:02:02Z,update batch infer.
github.com/shibing624/MedicalGPT,inference.py,2023-10-23T06:57:54Z,update batch infer.
github.com/shibing624/MedicalGPT,inference.py,2023-10-23T04:30:54Z,update infer for do sample.
github.com/shibing624/MedicalGPT,inference.py,2023-10-23T03:51:37Z,update longlora finetune.
github.com/shibing624/MedicalGPT,inference.py,2023-10-17T11:16:06Z,update template name.
github.com/shibing624/MedicalGPT,inference.py,2023-09-10T14:53:08Z,update inf.
github.com/shibing624/MedicalGPT,inference.py,2023-09-07T06:00:19Z,update multi gpu infer.
github.com/shibing624/MedicalGPT,inference.py,2023-08-29T02:43:46Z,update save to jsonl.
github.com/shibing624/MedicalGPT,inference.py,2023-08-28T07:40:12Z,update stop str.
github.com/shibing624/MedicalGPT,inference.py,2023-08-28T07:21:32Z,inference support qwen.
github.com/shibing624/MedicalGPT,inference.py,2023-08-06T07:47:42Z,update inference file.
github.com/shibing624/MedicalGPT,inference.py,2023-08-06T03:25:18Z,update inference stream.
github.com/shibing624/MedicalGPT,inference.py,2023-08-06T02:39:29Z,update gen
github.com/shibing624/MedicalGPT,inference.py,2023-08-06T02:16:17Z,update inference with gradio
github.com/shibing624/MedicalGPT,inference.py,2023-08-06T02:03:31Z,update inference with cli.
github.com/shibing624/MedicalGPT,inference.py,2023-08-04T08:38:50Z,update model max length.
github.com/shibing624/MedicalGPT,inference.py,2023-08-04T04:25:12Z,support special template tokenizer.
github.com/shibing624/MedicalGPT,inference.py,2023-08-03T12:50:14Z,del join.
github.com/shibing624/MedicalGPT,inference.py,2023-08-03T12:37:50Z,update infer with stream.
github.com/shibing624/MedicalGPT,inference.py,2023-08-03T11:18:42Z,update infer.
github.com/shibing624/MedicalGPT,inference.py,2023-08-03T09:14:51Z,update infer.
github.com/shibing624/MedicalGPT,inference.py,2023-08-03T08:52:10Z,update thread.
github.com/shibing624/MedicalGPT,inference.py,2023-08-03T02:02:59Z,update default template name.
github.com/shibing624/MedicalGPT,inference.py,2023-07-28T08:49:23Z,update infer stream.
github.com/shibing624/MedicalGPT,inference.py,2023-07-28T07:07:07Z,update infer
github.com/shibing624/MedicalGPT,inference.py,2023-07-27T13:54:29Z,update infer
github.com/shibing624/MedicalGPT,inference.py,2023-07-27T04:53:20Z,update conv for openchat sharegpt gpt4
github.com/shibing624/MedicalGPT,inference.py,2023-07-26T04:00:03Z,update eos token is none.
github.com/shibing624/MedicalGPT,inference.py,2023-07-23T15:39:35Z,update infer
github.com/shibing624/MedicalGPT,inference.py,2023-07-23T14:43:29Z,update infer
github.com/shibing624/MedicalGPT,inference.py,2023-07-23T14:26:26Z,update repetition_penalty
github.com/shibing624/MedicalGPT,inference.py,2023-07-23T14:22:36Z,update gradio infer.
github.com/shibing624/MedicalGPT,inference.py,2023-07-23T03:08:03Z,update inference.
github.com/shibing624/MedicalGPT,inference.py,2023-07-21T11:42:51Z,adjust alpaca data.
github.com/shibing624/MedicalGPT,inference.py,2023-07-21T11:09:37Z,update multi-round demo.
github.com/shibing624/MedicalGPT,inference.py,2023-07-21T09:50:37Z,support multi round infer.
github.com/shibing624/MedicalGPT,inference.py,2023-07-14T02:55:07Z,update inference.
github.com/shibing624/MedicalGPT,inference.py,2023-07-13T03:14:59Z,update merged model.
github.com/shibing624/MedicalGPT,inference.py,2023-07-12T14:05:33Z,update infer.
github.com/shibing624/MedicalGPT,inference.py,2023-07-12T13:44:51Z,update infer.
github.com/shibing624/MedicalGPT,inference.py,2023-06-16T02:57:11Z,update auto model.
github.com/shibing624/MedicalGPT,inference.py,2023-06-15T12:19:04Z,update gradio demo.
github.com/shibing624/MedicalGPT,inference.py,2023-06-15T10:17:15Z,support baichuan-7b.
github.com/shibing624/MedicalGPT,inference.py,2023-06-14T09:34:37Z,remove folder.
github.com/SCIR-HI/Huatuo-Llama-Med-Chinese,infer_literature.py,2023-08-07T13:46:05Z,"Update Huozi-based model

Major update. Please try our new Huozi-based model, which is much better."
github.com/SCIR-HI/Huatuo-Llama-Med-Chinese,infer_literature.py,2023-04-24T02:00:47Z,"add literature data

We tried to use the GPT3.5 API to integrate the conclusion in the medical literature as external information into multiple rounds of dialogue, and based on this, we fine-tuned the instructions of LLaMA."
github.com/bentoml/OpenLLM,openllm-python/src/openllm_cli/playground/llama2_qlora.py,2023-11-19T15:25:08Z,"feat(engine): CTranslate2 (#698)

* chore: update instruction for dependencies

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* feat(experimental): CTranslate2

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/bentoml/OpenLLM,openllm-python/src/openllm_cli/playground/llama2_qlora.py,2023-11-15T04:20:50Z,"chore(cli): move playground to CLI components (#655)

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/dvlab-research/LongLoRA,passkey_retrivial.py,2023-11-15T04:59:52Z,Update passkey_retrivial.py
github.com/dvlab-research/LongLoRA,passkey_retrivial.py,2023-09-26T13:11:12Z,Add files via upload
github.com/shibing624/pycorrector,examples/gpt/use_origin_transformers_demo.py,2023-11-03T14:25:45Z,update t5 and conv seq2seq model.
github.com/shibing624/pycorrector,examples/gpt/use_origin_transformers_demo.py,2023-11-02T10:01:53Z,update chat result.
github.com/shibing624/pycorrector,examples/gpt/use_origin_transformers_demo.py,2023-11-02T08:38:30Z,update chat result.
github.com/shibing624/pycorrector,examples/gpt/use_origin_transformers_demo.py,2023-11-02T08:13:29Z,update origin model for predict.
github.com/shibing624/pycorrector,examples/gpt/use_origin_transformers_demo.py,2023-11-02T08:05:24Z,update origin model for predict.
github.com/shibing624/pycorrector,examples/gpt/use_origin_transformers_demo.py,2023-11-02T08:01:19Z,update origin model.
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2024-02-29T06:36:29Z,"Add User input + max tokens requested exceeds model context window error response (#1325)

* added User input + max tokens requested exceeds model context window error response.

Signed-off-by: Ye, Xinyu <xinyu.ye@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2024-02-29T04:57:26Z,"[NeuralChat] Support microsoft/biogpt model as per the request (#1327)

* Support microsoft/biogpt model

* add dependency

---------

Signed-off-by: lvliang-intel <liang1.lv@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2024-02-23T13:33:51Z,"[API Changed] Dispatch the backend based on model_type (#1298)

Co-authored-by: changwangss <chang1.wang@intel.com>
Co-authored-by: VincyZhang <wenxin.zhang@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2024-02-23T11:50:17Z,"[NeuralChat] Fix Qwen chat model output undesirable on HPU issue caused by input padding (#1303)

* Fix Qwen chat model output undesirable on HPU issue caused by input padding.

* change pad_token instead of not padding, fix issue of qwen model ignore attention_mask when batch size is 1.

Signed-off-by: Ye, Xinyu <xinyu.ye@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2024-02-23T10:54:06Z,"update requirements_xpu.txt in neural chat to add neural_speed and ip… (#1294)

Co-authored-by: lvliang-intel <liang1.lv@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2024-02-23T06:17:18Z,[Neuralchat] Notebook retrieval update (#1182)
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2024-02-08T09:16:07Z,"[NeuralChat] Support GGUF model in NeuralChat (#1200)

* Support GGUF model in NeuralChat

Signed-off-by: lvliang-intel <liang1.lv@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2024-02-08T09:14:07Z,"[NeuralChat] Fix response issue of model.predict (#1221)

* fix response issue of model.predict

Signed-off-by: LetongHan <letong.han@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2024-02-08T02:42:32Z,"resolve woq quantization error when running neuralchat (#1268)

* fixes quantization error caused by pad_token_id not being set"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2024-02-04T22:45:09Z,"[NeuralChat] Support DeciLM-7B and DeciLM-7B-instruct (#1228)

* Support DeciLM-7B and DeciLM-7B-instruct

Signed-off-by: lvliang-intel <liang1.lv@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2024-02-04T00:52:38Z,"[NeuralChat] Support deepseek-coder models in NeuralChat (#1251)

* Support deepseek-coder model

Signed-off-by: lvliang-intel <liang1.lv@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2024-02-02T14:47:45Z,"[NeuralChat] Add Docker use case for CodeGen (#1236)

* Add Docker use case for CodeGen

Signed-off-by: LetongHan <letong.han@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2024-02-02T14:46:56Z,"[NeuralChat] Support SQL generation (#1234)

* Support SQL generation

Signed-off-by: lvliang-intel <liang1.lv@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2024-02-01T05:12:24Z,"[NeuralChat] Support GPTQ, AWQ model in NeuralChat (#1206)"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2024-01-31T09:18:17Z,Add precommit config (#1216)
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2024-01-29T09:25:47Z,add pre-commit-ci codespell check (#1188)
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2024-01-24T09:36:10Z,add validated sw (#1168)
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2024-01-19T14:19:24Z,[NeuralChat] Support vLLM serving (#1120)
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2024-01-19T14:12:24Z,[NeuralChat] Fix GPT-J model name issue (#1157)
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2024-01-18T13:32:51Z,"[Neuralchat] Improve error code UT coverage (#1132)

Co-authored-by: lvliang-intel <liang1.lv@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2024-01-17T03:46:43Z,"[LLM Runtime] neural_speed_replace_graph (#1129)

Co-authored-by: Meng, Hengyu <hengyu.meng@intel.com>
Co-authored-by: Wenxin Zhang <wenxin.zhang@intel.com>
Co-authored-by: lvliang-intel <liang1.lv@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2024-01-16T01:24:50Z,[NeuralChat] Support compatible stats format (#1112)
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2024-01-13T00:06:19Z,"Support Phi-2 model (#1137)

Signed-off-by: lvliang-intel <liang1.lv@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2024-01-02T12:00:54Z,"[NeuralChat] Align inference parameters num_beams with HF transformers (#1092)

Align inference parameters num_beams with HF transformers 

Signed-off-by: lvliang-intel <liang1.lv@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2024-01-01T23:35:58Z,"[NeuralChat] Support LLM runtime ggml int4 (#1098)

* Support llm runtime ggml int4

Signed-off-by: lvliang-intel <liang1.lv@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-12-27T23:15:09Z,"Fix magicoder model tokenizer issue and remove codegen streaming redundant end format (#1086)

* Fix magicoder tokenizer issue and streaming redundant end format

Signed-off-by: lvliang-intel <liang1.lv@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-12-25T23:08:57Z,"[NeuralChat] Support Mixtral-8x7B-v0.1 model (#972)

* Support Mixstral-8x7b model

Signed-off-by: lvliang-intel <liang1.lv@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-12-25T23:05:46Z,"[NeuralChat] Fix magicoder model tokenizer issue (#1075)

* fix magicoder tokenizer issue

Signed-off-by: lvliang-intel <liang1.lv@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-12-25T12:33:24Z,"[NeuralChat] Support magicoder model (#1067)

* Support magicoder model and refine load model

Signed-off-by: lvliang-intel <liang1.lv@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-12-20T11:54:31Z,"[NeuralChat] Support Salesforce codegen model (#973)

* Support Salesforce/codegen model

Signed-off-by: lvliang-intel <liang1.lv@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-12-20T11:53:20Z,"Fix LLM runtime int4 issues (#959)

Signed-off-by: lvliang-intel <liang1.lv@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-12-20T05:53:50Z,"Fix llama model inferene hang issue (#970)

Signed-off-by: lvliang-intel <liang1.lv@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-12-20T05:34:31Z,"add deepspeed support for ise-uiuc/Magicoder-S-CL-7B which only has safetensors chkpt (#968)

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-12-14T02:56:50Z,[NeuralChat] Fix PC codegen streaming issue and set 'Intel/neural-chat-7b-v3-1' as default model (#920)
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-12-14T02:52:30Z,[NeuralChat] Add Qwen model unit test case (#919)
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-12-12T12:03:52Z,"[NeuralChat] support return error code (#650)

* support error coed for NeuralChat

Signed-off-by: lvliang-intel <liang1.lv@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-12-09T05:19:50Z,add peft model support in deepspeed sharded mode (#884)
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-12-09T02:33:16Z,[Infra] use python logging (#752)
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-12-08T13:55:09Z,"support assisted generation for neuralchat (#896)

Signed-off-by: LetongHan <letong.han@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-11-30T06:23:43Z,[LLM] Support chatglm/falcon series and unified int8 loading (#814)
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-11-28T05:18:57Z,"[NeuralChat]  Add optimization pipeline for LLaMa model familly (#783)

* Add optimization pipeline for LLaMa model familly.

Signed-off-by: Mikolaj Zyczynski <mikolaj.zyczynski@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-11-24T09:06:52Z,add neuralchat support ipex.optimize_transformers sq int8 model loading (#764)
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-11-24T00:55:42Z,"[NeuralChat] Fix tokenizer issue for optimized model (#761)

* Fix tokenizer issue for optimized models

Signed-off-by: lvliang-intel <liang1.lv@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-11-21T14:03:16Z,"[NeuralChat] Remove unnecessary model load during optimizing model (#722)

* Remove unnecessary model load during optimizing model

Signed-off-by: lvliang-intel <liang1.lv@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-11-20T06:23:32Z,Support Mistral model in NeuralChat (#710)
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-11-20T02:08:06Z,"Support CodeLlama model in NeuralChat (#711)

* Support neural-chat-7b-v3 and neural-chat-7b-v3-1

Signed-off-by: lvliang-intel <liang1.lv@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-11-17T12:14:08Z,"Support neural-chat-7b-v3 and neural-chat-7b-v3-1 (#698)

* Support neural-chat-7b-v3 and neural-chat-7b-v3-1

Signed-off-by: lvliang-intel <liang1.lv@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-11-17T01:29:01Z,"enable mistral in habana, fix issue in generate.py (#683)

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-11-16T00:47:19Z,"[Infra] update docker related readme (#690)

* update docker related readme

* Revert ""Fix ChatGLM2 llm runtime int4 issue (#679)""

This reverts commit 0e9a29d48c3ae9b5c5e382e57b8b43b92c648de0."
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-11-15T11:42:51Z,"Fix ChatGLM2 llm runtime int4 issue (#679)

Signed-off-by: Lv, Liang1 <liang1.lv@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-11-09T02:33:12Z,[NeuralChat] Support optimization for server mode and add ut cases (#543)
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-11-08T05:19:01Z,Fix llm runtime int4 multi-turn inference issue (#642)
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-10-27T02:14:44Z,Add itrex llm runtime graph int4 notebook (#399)
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-10-26T01:46:48Z,Habana 1.12 (#535)
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-10-23T03:53:10Z,"Remove OneDNN env setint for BF16 inference (#509)

Signed-off-by: lvliang-intel <liang1.lv@intel.com>
Co-authored-by: VincyZhang <wenxin.zhang@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-10-23T03:53:10Z,"Fix ChatGLM2 model loading issue (#510)

* Fix ChatGLM2 model loading issue

Signed-off-by: lvliang-intel <liang1.lv@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-10-19T01:40:23Z,"Fix NeuralChat starcoder mha fusion issue (#494)

Signed-off-by: Wang, Chang <chang1.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-10-17T22:44:32Z,"NeuralChat support IPEX int8 model (#486)

* to support ipex int8 model

Signed-off-by: changwangss <chang1.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-10-13T03:31:47Z,"Enable Qwen-7B-Chat (#432)

Co-authored-by: lvliang-intel <liang1.lv@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-09-25T00:53:19Z,Enable xpu for neuralchat predict (#379)
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/models/model_utils.py,2023-09-18T06:02:15Z,"Refactored inference API, and extracted prompt template as individual module (#315)

* Refactored inference API, and extracted prompt template as individual module.

Signed-off-by: Ye, Xinyu <xinyu.ye@intel.com>

* restore workflow code.

Signed-off-by: Ye, Xinyu <xinyu.ye@intel.com>

* fixed pylint error.

Signed-off-by: Ye, Xinyu <xinyu.ye@intel.com>

* moved mpt_trace.py to llm.

Signed-off-by: Ye, Xinyu <xinyu.ye@intel.com>

* create prompts folder.

Signed-off-by: Ye, Xinyu <xinyu.ye@intel.com>

* path changes.

Signed-off-by: Ye, Xinyu <xinyu.ye@intel.com>

---------

Signed-off-by: Ye, Xinyu <xinyu.ye@intel.com>"
github.com/xusenlinzy/api-for-open-llm,api/adapter/model.py,2024-01-19T04:12:30Z,Improve code and add support internlm2
github.com/xusenlinzy/api-for-open-llm,api/adapter/model.py,2023-12-14T07:28:16Z,Replace |= with update method
github.com/xusenlinzy/api-for-open-llm,api/adapter/model.py,2023-12-07T04:14:14Z,Fix bug for p-tuning
github.com/xusenlinzy/api-for-open-llm,api/adapter/model.py,2023-12-04T05:49:14Z,Add docstrings for functions
github.com/xusenlinzy/api-for-open-llm,api/adapter/model.py,2023-11-30T11:43:08Z,Fix typo error
github.com/intel/intel-extension-for-transformers,workflows/chatbot/demo/basic_frontend/fastchat/model/apply_lora.py,2024-01-31T09:18:17Z,Add precommit config (#1216)
github.com/intel/intel-extension-for-transformers,workflows/chatbot/demo/basic_frontend/fastchat/model/apply_lora.py,2023-06-06T03:30:42Z,Add chatbot basic mode frontend (#984)
github.com/huggingface/trl,tests/test_no_peft.py,2024-02-15T03:37:41Z,"pre-commit: replace linters + formatters with Ruff; fix some issues (#1300)

* pre-commit: replace linters + formatters with Ruff

* Don't use bare except

* Clean up `noqa`s

* Enable Ruff UP; apply auto-fixes

* Enable Ruff B; apply fixes

* Enable Ruff T with exceptions

* Enable Ruff C (complexity); autofix

* Upgrade Ruff to 0.2.0"
github.com/huggingface/trl,tests/test_no_peft.py,2024-02-01T22:49:03Z,"Codemod Unittest assertions to bare asserts (#1301)

* Remove stray commas from test data

* Codemod Unittest assertions to bare asserts

* Make `assertAlmostEqual` tests more idiomatic

* DRY some test strings"
github.com/huggingface/trl,tests/test_no_peft.py,2023-03-07T14:08:21Z,"`peft` integration (#163)

* adds a hacky peft example

* fixes bug due to missing ""prepare_model_for_training""

* Formatting

* adds peft to requirements

* Update trl/trainer/ppo_trainer.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* gpt neo runs

* changes requested on the PR

* style

* updates to prepare_model_for_int8_training PEFT PR https://github.com/huggingface/peft/pull/105

* updates to prepare_model_for_int8_training PEFT PR https://github.com/huggingface/peft/pull/105

* adds missing 8-bit attribute to modeling base

* adds lr to example script

* adds missing train to trainer

* disables caching temporarily while I debug something

* debugging issues with unstable training

* Fix peft + int8 (#170)

* add fix

* another fix

* Auto stash before merge of ""peft-example"" and ""origin/peft-example""

* adds peft model types to modeling base

* reduces memory usage using adapters and no ref model.

* adds support for EleutherAI/gpt-neox-20b

* example for peft finetune of cm model

* removes hacky research code

* fixing the rebase and some typos

* style

* style2

* adds gradient checkpointing to base model

* cleans up comments

* moves config and other pretrained_model properties to __init__

* make style

* added tests

* change dependency

* Update .github/workflows/tests.yml

* fix test

* fix style and failing tests

* make quality

* revert change

* rm unneeded change

* revert changes

* rm changes

* rm changes

* rm uneeded change

* Update trl/models/modeling_base.py

* revert uneeded changes

* make style

* adapt suggestions

* fix tests

* attempt to fix

* fix

* fix

* add no peft test

* revert

* remove unneded check

* more tests

* fix logic

* add `save_pretrained` support

* fix quality

* clean up

* clean up

* stronger test

* refactor comments

* make style

* attempt to add non-peft tests

* remove test runner

* format

* fix test

* move `train` on top

* fix peft import

* make quality

* fixes typo

* adds peft example to docs

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>
Co-authored-by: younesbelakda <younesbelkada@gmail.com>"
github.com/HarderThenHarder/transformers_tasks,LLM/chatglm_finetune/train.py,2023-08-02T12:16:49Z,add LLMs Trainer
github.com/LianjiaTech/BELLE,train/src/entry_point/zero_inference_backend_without_trainer.py,2023-09-26T11:39:39Z,"update parameter name and bug fix (#523)

* update docker

* update parameter name and bug fix

* delete redundancy file"
github.com/LianjiaTech/BELLE,train/src/entry_point/zero_inference_backend_without_trainer.py,2023-08-10T10:27:54Z,add zero inference
github.com/PixArt-alpha/PixArt-alpha,app/app_lcm.py,2024-01-20T06:05:46Z,"Refractoring (#61)

Approved"
github.com/PixArt-alpha/PixArt-alpha,app/app_lcm.py,2024-01-11T05:11:15Z,Feat/pixart ctrlnet public (#70)
github.com/lxe/simple-llm-finetuner,trainer.py,2023-04-19T01:27:29Z,Add abort button
github.com/lxe/simple-llm-finetuner,trainer.py,2023-04-11T04:14:47Z,resolved adapater not found bug (#45)
github.com/lxe/simple-llm-finetuner,trainer.py,2023-04-06T23:43:16Z,Full rework: Version 2 release (#37)
github.com/OpenBMB/ToolBench,toolbench/inference/LLM/tool_llama_lora_model.py,2023-09-26T16:38:44Z,add inference args
github.com/OpenBMB/ToolBench,toolbench/inference/LLM/tool_llama_lora_model.py,2023-07-27T08:09:51Z,update new version
github.com/OpenPipe/OpenPipe,trainer/src/trainer/export_weights.py,2024-02-15T07:58:17Z,Merge on CPU
github.com/OpenPipe/OpenPipe,trainer/src/trainer/export_weights.py,2024-02-09T02:09:40Z,"Mixtral support

Allow users with beta access to fine-tune Mixtral.

Also, allow all users to fine-tune Llama 2 and Mistral."
github.com/OpenPipe/OpenPipe,trainer/src/trainer/export_weights.py,2024-01-23T22:38:40Z,"Support exports of merged models

People keep asking for this so I've productized it."
github.com/deep-diver/LLM-As-Chatbot,models/guanaco.py,2023-07-25T10:11:50Z,add more models + gptq mode
github.com/deep-diver/LLM-As-Chatbot,models/guanaco.py,2023-07-01T17:56:43Z,add local files only option
github.com/deep-diver/LLM-As-Chatbot,models/guanaco.py,2023-06-10T13:08:36Z,add use_safetensor=False
github.com/deep-diver/LLM-As-Chatbot,models/guanaco.py,2023-06-09T04:33:05Z,update vram requirements records
github.com/deep-diver/LLM-As-Chatbot,models/guanaco.py,2023-06-08T15:32:11Z,tested gpu and cpu options
github.com/deep-diver/LLM-As-Chatbot,models/guanaco.py,2023-06-08T01:59:11Z,exp w/ camel and sam
github.com/deep-diver/LLM-As-Chatbot,models/guanaco.py,2023-06-05T00:25:50Z,update examples
github.com/deep-diver/LLM-As-Chatbot,models/guanaco.py,2023-06-04T14:50:44Z,disable force download & example view
github.com/deep-diver/LLM-As-Chatbot,models/guanaco.py,2023-05-27T00:22:42Z,add guanaco
github.com/zjunlp/DeepKE,example/llm/InstructKGC/src/model/adapter.py,2024-02-27T08:23:07Z,update InstructKGC add IEPile
github.com/project-baize/baize-chatbot,merge_lora.py,2023-05-25T19:22:35Z,Add v2 collect
github.com/project-baize/baize-chatbot,merge_lora.py,2023-04-21T19:34:33Z,Create merge_lora.py
github.com/liguodongiot/llm-action,train/alpaca-lora/export_hf_checkpoint.py,2023-07-23T11:43:58Z,fix
github.com/shibing624/MedicalGPT,gradio_demo.py,2024-01-09T07:36:48Z,add base model template.
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-11-15T06:48:12Z,update infer use greedy decoding.
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-10-30T03:59:14Z,update gradio
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-10-26T06:49:18Z,update gradio history
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-10-26T06:19:01Z,update gradio
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-10-26T06:13:06Z,update gradio version.
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-10-26T03:49:51Z,update gradio sample.
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-10-17T11:16:06Z,update template name.
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-09-12T05:31:07Z,update gradio history.
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-09-12T05:18:59Z,update gradio .
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-09-12T04:11:11Z,update history.
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-09-12T04:07:54Z,update history.
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-09-12T03:46:45Z,update special tokens.
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-08-28T07:40:12Z,update stop str.
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-08-28T07:21:32Z,inference support qwen.
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-08-06T03:25:18Z,update inference stream.
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-08-06T03:04:38Z,update gradio
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-08-06T02:59:31Z,update gradio
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-08-06T02:50:53Z,update gradio
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-08-06T02:39:29Z,update gen
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-08-06T02:16:17Z,update inference with gradio
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-08-04T08:38:50Z,update model max length.
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-08-04T04:25:12Z,support special template tokenizer.
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-08-03T02:02:59Z,update default template name.
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-07-27T13:54:29Z,update infer
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-07-27T04:53:50Z,update conv for openchat sharegpt gpt4
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-07-26T04:00:03Z,update eos token is none.
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-07-23T15:39:35Z,update infer
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-07-23T14:43:29Z,update infer
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-07-23T14:22:36Z,update gradio infer.
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-07-21T11:42:51Z,adjust alpaca data.
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-07-21T11:09:37Z,update multi-round demo.
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-07-16T02:13:41Z,优化prompt
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-06-16T02:57:11Z,update auto model.
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-06-15T12:19:04Z,update gradio demo.
github.com/shibing624/MedicalGPT,gradio_demo.py,2023-06-14T09:34:37Z,remove folder.
github.com/deep-diver/LLM-As-Chatbot,models/mistral.py,2023-11-19T14:28:03Z,add mistral and zephyr
github.com/InternLM/xtuner,xtuner/tools/chat.py,2024-02-06T08:16:50Z,"[Fix] Fix no space in chat output using InternLM2. (#357) (#404)

* [Fix] Fix no space in chat output using InternLM2. (#357)

* Update chat.py

* Update utils.py

* Update utils.py

* fix pre-commit

---------

Co-authored-by: Zhihao Lin <36994684+LZHgrla@users.noreply.github.com>
Co-authored-by: LZHgrla <linzhihao@pjlab.org.cn>"
github.com/InternLM/xtuner,xtuner/tools/chat.py,2024-01-24T05:33:46Z,"[Improve] Add `--repetition-penalty` for `xtuner chat` (#351)

fix"
github.com/InternLM/xtuner,xtuner/tools/chat.py,2024-01-17T02:47:15Z,"[Fix] Add `trust_remote_code=True` for AutoModel  (#328)

update"
github.com/InternLM/xtuner,xtuner/tools/chat.py,2024-01-11T15:29:39Z,"[Improve] Redesign the `prompt_template` (#294)

* update

* update cfgs

* update

* fix bugs

* upload docs

* rename

* update

* Revert ""update cfgs""

This reverts commit 93966aa7578fc3963d7c9c3e25eb1f9489e6e6fa.

* update cfgs

* update

* rename

* rename

* fix bc

* fix stop_word

* fix

* fix

* Update prompt_template.md"
github.com/InternLM/xtuner,xtuner/tools/chat.py,2023-12-26T09:39:01Z,"[Feature] Support LLaVA (#196)

* v1

* add load_image

* update cfg image url

* del fig

* update

* temp

* update convert

* update chat_mm

* add exclude_frozen_parameters for deepspeed

* update chat

* update xtuner help msg

* fix bugs

* revert bf16 deepspeed

* fix bugs

* add visual_select_layer for chat

* improve pth_to_hf

* rename projecter_pth to pretrained_pth

* temp

* update requirements

* add cfgs

* update

* fix pre-commit

* optim chat

* optim chat

* Delete xtuner/model/unused.py

* move dispatch to a deeper folder

* add projector

* update

* del model/projector

* fix bugs

* add docs

* update

* update

* update

* update

* enhance resume for map_fn

* update import

* add llava_internlm_chat_7b_clip_vit_large_p14

* update dispatch

* update dispatch

* add link

* update max_length

* update max_length

* update hyp

* align

* move yi flash attn

* fix pre-commit

* update deepspeed requirements

* add mmbench script

* install openpyxl

* add entry_point for mmbench

* save args

* update mmbench

* update max_length

* add llama2 qlora

* update mmbench

* fix mmbench bugs

* use osp instead of os.path

* refactor pth_to_hf

* update chat and mmbench to support --llava

* align to chat

* update entry_point

* add vicuna template

* add vicuna_7b_v15

* fix pre-commit

* add vicuna_7b_v1.5 qlora

* skip_special_tokens for decode text

* remove do_sample

* add warmup

* fix pre-commit

* Update dataset_prepare.md

* Update dataset_prepare.md

* Add KEEP_STSTEM for template

* remove

* fix vicuna template

* clean cfgs

* add cfgs

* fix pre-commit

* add --language for mmbench

* fix bugs

* fix pretrain bug

* support visual_encoder lora

* fix bugs

* add paramwise_cfg

* remove print_peft_model_trainable_parameters

* fix bugs

* add paramwise_cfg for DeepSpeedOptimWrapper

* fix engine deepspeed paramwise_cfg bug

* fix encode_fn bug

* fix

* fix pad_image_to_square bugs

* Add space for system to avoid mismatch of 'USER' token

* revert to adding bos_token at each conv

* revert for paramwise_cfg

* better cfgs?

* fix import bug

* fix import bug

* pretrain align

* update prepare_inputs_labels_for_multimodal

* 1792

* support length_grouped_samplers

* 1792

* remove KEEP_SYSTEM

* remove system in cfg

* update 336 cfg

* add torch_dtype for mmbench and chat

* group 50

* quant for pretrain

* update cfgs

* refactor cfgs

* add length for concat dataset

* update requirements

* fix typo

* add template for internlm pretrain

* no zh

* remove 20b cfgs

* fix pre-commit

* revert invalid input

* rename

* Update README.md

* Update README_zh-CN.md

* fix pre-commit

* remove llava_zh from docs

* qlora 512

* rename llava map_fn

* update cfgs

* update model urls

* add docs link

* add llava docs

* update docs

* update urls

* add citation

* fix README

* move

* update

* vicuna pretrain with prompt

* rename

* add results

* fix pre-commit

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* Update README.md

* Update README_zh-CN.md

* Update README_zh.md

* Update README_zh.md

* Update README.md

* Update README_zh.md

* Update README.md

* Update README.md

* fix typo

* fix

* Update README.md

* Update README_zh-CN.md

* rename

* auto cn_string

* fix pre-commit

* rename

* remove language

* add VLMEvalKit

* rename VLLM to VLM

* add the download links of MMBench

* update

* update readme

* update

* update

* update merge

* fix cfg bug

* Update README.md

* Update README_zh.md

* update

* fix

* update requirements

* Update runtime.txt

* Update runtime.txt

* Update runtime.txt

* Update README.md

* Update README.md

* Update README_zh.md

* fix pre-commit

* fix

* update mmbench prompt

* fix bugs

* fix bugs

* update docs

* update

* update

* Update README.md"
github.com/InternLM/xtuner,xtuner/tools/chat.py,2023-12-18T02:57:37Z,"[Bug] Fix bugs when chat with --lagent (#269)

fix_chat_for_model_kwargs"
github.com/InternLM/xtuner,xtuner/tools/chat.py,2023-11-30T06:39:02Z,"[Bug] Support auto detect torch_dtype in chat.py (#250)

* fix torch dtype

* fix torch dtype

* Update xtuner/tools/chat.py

Co-authored-by: Zhihao Lin <36994684+LZHgrla@users.noreply.github.com>

---------

Co-authored-by: Zhihao Lin <36994684+LZHgrla@users.noreply.github.com>"
github.com/InternLM/xtuner,xtuner/tools/chat.py,2023-11-14T12:48:02Z,"[Feature] Support ChatGLM3-6B (#222)

* add chatglm cfgs

* add chatglm3 template

* fix bos_token_id bug

* add encode_special_tokens

* update readme"
github.com/InternLM/xtuner,xtuner/tools/chat.py,2023-11-08T09:31:02Z,"[Feature] Add mistral pretrain (#204)

* [Feature] Add mistral pretrain

* [feat] rename pretrain_map_fn

* [feat] add custom hook

* [feat] change mistral config name

* Update chat.py

* Update xtuner/utils/templates.py

Co-authored-by: Zhihao Lin <36994684+LZHgrla@users.noreply.github.com>

* Update xtuner/configs/mistral/mistral_7b_qlora_skypile_pretrain_e1.py

Co-authored-by: Zhihao Lin <36994684+LZHgrla@users.noreply.github.com>

* Update xtuner/configs/mistral/mistral_7b_qlora_skypile_pretrain_e1.py

Co-authored-by: Zhihao Lin <36994684+LZHgrla@users.noreply.github.com>

* Update xtuner/configs/mistral/mistral_7b_qlora_skypile_pretrain_e1.py

Co-authored-by: Zhihao Lin <36994684+LZHgrla@users.noreply.github.com>

* fix pre-commit

---------

Co-authored-by: Zhihao Lin <36994684+LZHgrla@users.noreply.github.com>
Co-authored-by: LZHgrla <linzhihao@pjlab.org.cn>"
github.com/InternLM/xtuner,xtuner/tools/chat.py,2023-10-23T07:21:48Z,"[Feature] Support the fine-tuning of MSAgent dataset (#156)

* fix bugs for invalid data

* add modelscope in requirements

* add process_ms_dataset

* add msagent dataset

* fix

* add msagent map_fn

* add cfg

* fix pre-commit

* fix eval generate input

* fix f-string bug

* modify question

* add more cfgs

* rename

* support lagent chat

* msagent uses system text

* update cfgs

* update chat

* update readme"
github.com/InternLM/xtuner,xtuner/tools/chat.py,2023-10-12T10:55:45Z,"[Refactor] Refactor the preprocess of dataset (#163)

* support system for template

* fix alpaca map_fn

* refactor map_fns

* refactor internlm-7b cfgs

* fix moss_sft

* fix templates

* fix cfgs

* add system for evaluate_chat_hook

* use template system

* fix bugs

* add task

* fix bugs

* rename

* fix

* update templates

* update cfgs

* Update dataset_format.md

* Update dataset_format.md

* Update dataset_format.md

* Update dataset_format.md

* Update dataset_format.md

* Update dataset_format.md

* Update dataset_format.md

* Update dataset_format.md

* Update single_turn_conversation.md

* update

* fix pre-commit

* update

* add toc

* chat supports system

* Update README.md

* Update README_zh-CN.md

* fix typo

* remove chat docs

* Update README_zh-CN.md

* Update README.md

* Update README.md

* Update README_zh-CN.md

* fix pre-commit

* fix language

* update help msg

* add eos_token for qwen

* fix

* Update single_turn_conversation.md

* Update single_turn_conversation.md

* fix apis

* support system-output text

* fix

* fix

* fix typo"
github.com/InternLM/xtuner,xtuner/tools/chat.py,2023-09-27T06:49:12Z,"[Feature] Support to remove history for chat script (#144)

* [Feature] Support to remove history for chat script

* fix typo"
github.com/InternLM/xtuner,xtuner/tools/chat.py,2023-09-27T06:48:07Z,"[Fix] Add `--offload-folder` for merge and chat (#140)

add `--offload-folder` for merge and chat"
github.com/InternLM/xtuner,xtuner/tools/chat.py,2023-09-07T09:56:07Z,"[Fix] Use `token_id` instead of `token` for `encode_fn` & Set eval mode before generate (#107)

* set eval mode before generate

* use token_id instead of token"
github.com/InternLM/xtuner,xtuner/tools/chat.py,2023-09-05T14:05:41Z,"[Fix] fix generation config (#98)

* fix generation config

* add code llama chat template

* add code llama chat template"
github.com/InternLM/xtuner,xtuner/tools/chat.py,2023-09-05T08:08:28Z,"[Improve] Redesign convert tools (#96)

* refactor tools

* modify entry_point

* modify docs

* update docs

* fix

* fix

* Update README.md

* Update README.md

* Update README.md

* Update README_zh-CN.md

* fix pre-commit

* rename converter

* update pth2hf

* rename pth2hf to pth_to_hf

* add fp32 for pth_to_hf

* Update README.md

* Update README_zh-CN.md

* Update README_zh-CN.md

* Update README.md

* Update README_zh-CN.md

* Update README_zh-CN.md

* Update README.md

* Update README.md

* Update README_zh-CN.md

* fix pre-commit"
github.com/InternLM/xtuner,xtuner/tools/chat.py,2023-08-29T10:48:52Z,"[Feature] Support ChatGLM (#62)

* no space for ChatGLM

* cast to inference during generate

* support chatglm tokenizer

* support chatglm qlora

* fix pre-commit

* update doc

* update

* Update README.md

* Update README_zh-CN.md

* add chatglm template

* fix chat bugs

* add round for template_map_fn

* add round for EvaluateChatHook"
github.com/InternLM/xtuner,xtuner/tools/chat.py,2023-08-28T12:01:27Z,"[Refactor] Decouple map fn (#54)

* decouple map_fn

* decouple map_fn

* decouple map_fn

* modify config

* advance the --prompt-template args

* fix config

* Update sql_map_fn.py

* Update sql_map_fn.py

* Update alpaca_map_fn.py

* rename hooks

* refactor template_map_fn

* add openai_map_fn

* fix config

* fix config

* fix print_log

* fix medical map_fn

* improve

* rename dataset_map_fn to dataset_map_fns

* add alpaca_enzh & oasst1 concat dataset config

---------

Co-authored-by: LZHgrla <36994684+LZHgrla@users.noreply.github.com>
Co-authored-by: LZHgrla <linzhihao@pjlab.org.cn>"
github.com/InternLM/xtuner,xtuner/tools/chat.py,2023-08-22T05:57:46Z,"[Feature] Design HuggingFace framework configs & Add more datasets (#41)

* refactor registry

* close bf16 for deepspeed_zero2

* support huggingface framework for train.py

* fix bugs

* add llama2_70b open_platypus

* update configs

* add shuffle_before_pack for process_hf_dataset

* enhance SampleGenerateHook

* enhance error report

* fix bugs

* fix pre-commit

* add code, colors, laywer, openplatypus datasets

* fix typo

* set default launcher to pytorch

* cli uses torchrun

* remove unused code

* add sql

* set default port to 0 for cli"
github.com/InternLM/xtuner,xtuner/tools/chat.py,2023-08-22T05:57:32Z,"[Improve] Use lagent to implement google_search (#42)

* fix bugs for input in chat

* use lagent for google_search

* rename SERPER_KEY to SERPER_API_KEY"
github.com/InternLM/xtuner,xtuner/tools/chat.py,2023-08-18T05:47:17Z,"[Improve] Add generate_test_freq for configs & Add note for config argument (#40)

* add generate_test_freq for configs

* add note for configs"
github.com/InternLM/xtuner,xtuner/tools/chat.py,2023-08-17T09:56:47Z,"[Improve] Support deepspeed adapter to use merge_adapter (#37)

merge support deepspeed adatper"
github.com/InternLM/xtuner,xtuner/tools/chat.py,2023-08-17T06:10:43Z,"[Improve] Support deepspeed adapter to use chat and adapter_pth2hf  (#36)

tools support deepspeed adapter"
github.com/InternLM/xtuner,xtuner/tools/chat.py,2023-08-16T10:47:12Z,"[Improve] Move configs to xtuner; Compatible with QwenTokenizer (#29)

* add oasst1 new template

* move configs to xtuner

* rename to xtuner

* improve

* improve

* add internlm configs

* modify oasst1 map_fn

* fix bugs

* remove data_preprocessor from sft

* fix alpaca bugs

* add param_scheduler url

* rename to pack_to_max_length

* fix bugs in internlm_map_fn

* rename prompt_template plugins to moss_sft

* rename input_with_labels to input_ids_with_output

* add e* for config name

* fix pre-commit

* limit max_new_tokens in SampleGenerateHook

* remove unused noqa

* new configs

* compatible with QwenTokenizer

* fix cli bugs

* add per_device comment for batch_size

* add copyright

* fix typo

---------

Co-authored-by: LZHgrla <linzhihao@pjlab.org.cn>"
github.com/InternLM/xtuner,xtuner/tools/chat.py,2023-08-16T02:34:41Z,"[Feature] Add CLI for xtuner (#30)

* add cli

* fix typo

* update readme

* update docs

* update docs

* fix typo

* fix typo"
github.com/huggingface/autotrain-advanced,src/autotrain/trainers/clm/utils.py,2024-02-29T12:02:37Z,update requirements
github.com/huggingface/autotrain-advanced,src/autotrain/trainers/clm/utils.py,2024-02-22T22:55:25Z,clm fixes
github.com/huggingface/autotrain-advanced,src/autotrain/trainers/clm/utils.py,2024-02-22T15:42:14Z,support for more chat templates
github.com/huggingface/autotrain-advanced,src/autotrain/trainers/clm/utils.py,2024-02-22T14:55:14Z,all-linear + token ui fix
github.com/huggingface/autotrain-advanced,src/autotrain/trainers/clm/utils.py,2024-02-08T15:15:00Z,Token classification (#493)
github.com/huggingface/autotrain-advanced,src/autotrain/trainers/clm/utils.py,2024-02-07T07:42:09Z,peft model merge
github.com/huggingface/autotrain-advanced,src/autotrain/trainers/clm/utils.py,2023-12-22T13:37:21Z,fix apply_chat_template
github.com/huggingface/autotrain-advanced,src/autotrain/trainers/clm/utils.py,2023-12-22T10:54:29Z,apply chat template (#433)
github.com/huggingface/autotrain-advanced,src/autotrain/trainers/clm/utils.py,2023-12-21T12:35:31Z,clm improvements
github.com/huggingface/autotrain-advanced,src/autotrain/trainers/clm/utils.py,2023-11-30T14:15:21Z,fix dpo model_ref / ngc (#368)
github.com/huggingface/autotrain-advanced,src/autotrain/trainers/clm/utils.py,2023-11-24T11:31:45Z,update llm model card
github.com/huggingface/autotrain-advanced,src/autotrain/trainers/clm/utils.py,2023-11-21T15:17:23Z,Update to the app (#354)
github.com/huggingface/autotrain-advanced,src/autotrain/trainers/clm/utils.py,2023-11-15T15:22:25Z,update requirements
github.com/huggingface/autotrain-advanced,src/autotrain/trainers/clm/utils.py,2023-11-03T13:51:03Z,fix db + target modules
github.com/huggingface/autotrain-advanced,src/autotrain/trainers/clm/utils.py,2023-10-12T12:56:16Z,Reward modelling (#297)
github.com/huggingface/autotrain-advanced,src/autotrain/trainers/clm/utils.py,2023-08-21T12:39:14Z,prepare for llm ui
github.com/huggingface/autotrain-advanced,src/autotrain/trainers/clm/utils.py,2023-08-16T09:58:29Z,improvements to clm training
github.com/PKU-YuanGroup/MoE-LLaVA,moellava/model/builder.py,2024-02-15T09:03:23Z,Update builder.py
github.com/PKU-YuanGroup/MoE-LLaVA,moellava/model/builder.py,2024-02-14T03:37:09Z,Update builder.py
github.com/PKU-YuanGroup/MoE-LLaVA,moellava/model/builder.py,2024-01-23T13:41:46Z,demo
github.com/PKU-YuanGroup/MoE-LLaVA,moellava/model/builder.py,2024-01-21T15:03:14Z,1
github.com/PKU-YuanGroup/MoE-LLaVA,moellava/model/builder.py,2023-12-28T10:28:13Z,'1'
github.com/InternLM/xtuner,xtuner/utils/fileio.py,2024-01-24T04:51:57Z,"[Feature]Support CEPH (#266)

* support petrelfs

* fix deepspeed save/load/resume

* add ENV to toggle petrelfs

* support hf save_pretrained

* patch deepspeed engine"
github.com/InternLM/xtuner,xtuner/tools/mmbench.py,2024-01-24T11:37:07Z,"[Feature] Support MMBench DDP Evaluate (#300)

* support ddp mmbench evaluate

* Update xtuner/tools/mmbench.py

Co-authored-by: Zhihao Lin <36994684+LZHgrla@users.noreply.github.com>

* Update xtuner/tools/mmbench.py

Co-authored-by: Zhihao Lin <36994684+LZHgrla@users.noreply.github.com>

* update minimum version of mmengine

* Update runtime.txt

---------

Co-authored-by: Zhihao Lin <36994684+LZHgrla@users.noreply.github.com>"
github.com/InternLM/xtuner,xtuner/tools/mmbench.py,2024-01-17T02:47:15Z,"[Fix] Add `trust_remote_code=True` for AutoModel  (#328)

update"
github.com/InternLM/xtuner,xtuner/tools/mmbench.py,2024-01-12T07:28:37Z,"[Fix] Fix errors about `stop_words`  (#313)

* fix bugs

* Update mmbench.py"
github.com/InternLM/xtuner,xtuner/tools/mmbench.py,2024-01-11T15:29:39Z,"[Improve] Redesign the `prompt_template` (#294)

* update

* update cfgs

* update

* fix bugs

* upload docs

* rename

* update

* Revert ""update cfgs""

This reverts commit 93966aa7578fc3963d7c9c3e25eb1f9489e6e6fa.

* update cfgs

* update

* rename

* rename

* fix bc

* fix stop_word

* fix

* fix

* Update prompt_template.md"
github.com/InternLM/xtuner,xtuner/tools/mmbench.py,2023-12-26T09:39:01Z,"[Feature] Support LLaVA (#196)

* v1

* add load_image

* update cfg image url

* del fig

* update

* temp

* update convert

* update chat_mm

* add exclude_frozen_parameters for deepspeed

* update chat

* update xtuner help msg

* fix bugs

* revert bf16 deepspeed

* fix bugs

* add visual_select_layer for chat

* improve pth_to_hf

* rename projecter_pth to pretrained_pth

* temp

* update requirements

* add cfgs

* update

* fix pre-commit

* optim chat

* optim chat

* Delete xtuner/model/unused.py

* move dispatch to a deeper folder

* add projector

* update

* del model/projector

* fix bugs

* add docs

* update

* update

* update

* update

* enhance resume for map_fn

* update import

* add llava_internlm_chat_7b_clip_vit_large_p14

* update dispatch

* update dispatch

* add link

* update max_length

* update max_length

* update hyp

* align

* move yi flash attn

* fix pre-commit

* update deepspeed requirements

* add mmbench script

* install openpyxl

* add entry_point for mmbench

* save args

* update mmbench

* update max_length

* add llama2 qlora

* update mmbench

* fix mmbench bugs

* use osp instead of os.path

* refactor pth_to_hf

* update chat and mmbench to support --llava

* align to chat

* update entry_point

* add vicuna template

* add vicuna_7b_v15

* fix pre-commit

* add vicuna_7b_v1.5 qlora

* skip_special_tokens for decode text

* remove do_sample

* add warmup

* fix pre-commit

* Update dataset_prepare.md

* Update dataset_prepare.md

* Add KEEP_STSTEM for template

* remove

* fix vicuna template

* clean cfgs

* add cfgs

* fix pre-commit

* add --language for mmbench

* fix bugs

* fix pretrain bug

* support visual_encoder lora

* fix bugs

* add paramwise_cfg

* remove print_peft_model_trainable_parameters

* fix bugs

* add paramwise_cfg for DeepSpeedOptimWrapper

* fix engine deepspeed paramwise_cfg bug

* fix encode_fn bug

* fix

* fix pad_image_to_square bugs

* Add space for system to avoid mismatch of 'USER' token

* revert to adding bos_token at each conv

* revert for paramwise_cfg

* better cfgs?

* fix import bug

* fix import bug

* pretrain align

* update prepare_inputs_labels_for_multimodal

* 1792

* support length_grouped_samplers

* 1792

* remove KEEP_SYSTEM

* remove system in cfg

* update 336 cfg

* add torch_dtype for mmbench and chat

* group 50

* quant for pretrain

* update cfgs

* refactor cfgs

* add length for concat dataset

* update requirements

* fix typo

* add template for internlm pretrain

* no zh

* remove 20b cfgs

* fix pre-commit

* revert invalid input

* rename

* Update README.md

* Update README_zh-CN.md

* fix pre-commit

* remove llava_zh from docs

* qlora 512

* rename llava map_fn

* update cfgs

* update model urls

* add docs link

* add llava docs

* update docs

* update urls

* add citation

* fix README

* move

* update

* vicuna pretrain with prompt

* rename

* add results

* fix pre-commit

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* Update README.md

* Update README_zh-CN.md

* Update README_zh.md

* Update README_zh.md

* Update README.md

* Update README_zh.md

* Update README.md

* Update README.md

* fix typo

* fix

* Update README.md

* Update README_zh-CN.md

* rename

* auto cn_string

* fix pre-commit

* rename

* remove language

* add VLMEvalKit

* rename VLLM to VLM

* add the download links of MMBench

* update

* update readme

* update

* update

* update merge

* fix cfg bug

* Update README.md

* Update README_zh.md

* update

* fix

* update requirements

* Update runtime.txt

* Update runtime.txt

* Update runtime.txt

* Update README.md

* Update README.md

* Update README_zh.md

* fix pre-commit

* fix

* update mmbench prompt

* fix bugs

* fix bugs

* update docs

* update

* update

* Update README.md"
github.com/gururise/AlpacaDataCleaned,eval/eval.py,2023-04-05T04:37:02Z,fix more issues in piqa scoring
github.com/gururise/AlpacaDataCleaned,eval/eval.py,2023-04-05T00:16:26Z,update regex to extract correct answer
github.com/gururise/AlpacaDataCleaned,eval/eval.py,2023-04-04T23:20:06Z,fix typo
github.com/gururise/AlpacaDataCleaned,eval/eval.py,2023-04-04T23:16:53Z,update piqa prompt/scoring.
github.com/gururise/AlpacaDataCleaned,eval/eval.py,2023-04-04T21:09:03Z,fix piqa scoring
github.com/gururise/AlpacaDataCleaned,eval/eval.py,2023-04-03T16:12:42Z,redo scoring in squad
github.com/gururise/AlpacaDataCleaned,eval/eval.py,2023-04-03T03:16:39Z,benchmark fixes
github.com/gururise/AlpacaDataCleaned,eval/eval.py,2023-04-03T02:07:01Z,add piqa dataset benchmark
github.com/gururise/AlpacaDataCleaned,eval/eval.py,2023-04-02T19:35:00Z,updates to benchmark
github.com/gururise/AlpacaDataCleaned,eval/eval.py,2023-04-02T17:38:17Z,fix bug in ppl output
github.com/gururise/AlpacaDataCleaned,eval/eval.py,2023-04-02T04:32:47Z,update program arguments
github.com/gururise/AlpacaDataCleaned,eval/eval.py,2023-04-02T04:28:53Z,add some clarity in the options
github.com/gururise/AlpacaDataCleaned,eval/eval.py,2023-04-02T04:06:36Z,program to evaluate models on Squad and Wikitext
github.com/TigerResearch/TigerBot,opencompass/opencompass/models/huggingface.py,2023-09-11T01:04:57Z,fix huggingface.py bug
github.com/TigerResearch/TigerBot,opencompass/opencompass/models/huggingface.py,2023-09-01T09:34:30Z,fix bugs
github.com/TigerResearch/TigerBot,opencompass/opencompass/models/huggingface.py,2023-08-31T03:37:27Z,fix bugs
github.com/TigerResearch/TigerBot,opencompass/opencompass/models/huggingface.py,2023-08-24T13:49:19Z,add opencompass
github.com/MetaGLM/FinGLM,code/finglm_all/chat_run/nl2sql.py,2023-09-26T15:27:39Z,Add files via upload
github.com/togethercomputer/OpenChatKit,training/lora/example/redpajama-incite-chat-3b_inference.py,2023-06-06T21:31:11Z,Moved redpajama scripts to example
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2024-03-01T12:27:40Z,"Improve WOQ algo autoround (#1330)

Signed-off-by: changwangss <chang1.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2024-02-27T06:02:00Z,"WOQ support autoround algo on cpu device (#1312)

* woq support autoround algo on cpu device

Signed-off-by: changwangss <chang1.wang@intel.com>

---------

Signed-off-by: changwangss <chang1.wang@intel.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2024-02-23T13:33:51Z,"[API Changed] Dispatch the backend based on model_type (#1298)

Co-authored-by: changwangss <chang1.wang@intel.com>
Co-authored-by: VincyZhang <wenxin.zhang@intel.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2024-02-21T12:18:03Z,"add GPTQ static_groups (#1291)

Signed-off-by: changwangss <chang1.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2024-02-06T11:55:27Z,Fix text-generation example accuracy test (#1262)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2024-02-01T03:33:07Z,[LLM] Support woq model save and load (#1211)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2024-01-25T08:50:12Z,"Update run_generation.py (#1169)

Signed-off-by: Wang, Chang <chang1.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2024-01-19T16:29:58Z,Support weight-only kernel with IPEX for intel GPU (#1153)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2024-01-19T11:43:33Z,"remove GPTQ llama dependence (#1163)

Signed-off-by: Wang, Chang1 <chang1.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2024-01-19T02:45:51Z,Add WOQ GPTQ frontend and example (#1107)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-12-28T06:13:48Z,"[LLM example] add calib_shuffle args for text-generation example (#1087)

Signed-off-by: Wang, Chang1 <chang1.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-12-21T10:38:22Z,[LLM] Fix llm models extension issue (#955)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-12-21T02:37:06Z,"parser recipes to dict (#979)

Signed-off-by: Wang, Chang1 <chang1.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-12-20T12:34:55Z,"[LLM] Support recipes with calibration changes and add example parser args. (#978)

* support recipes for calibration func change

Signed-off-by: Wang, Chang1 <chang1.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-12-14T03:19:47Z,[LLM] Support fp8 config and update examples (#915)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-12-08T05:35:25Z,"[LLM] Support restoring sq optimized model for text-generation  (#860)

* add ipex restoration evaluate

Signed-off-by: Tang, Kaihui <kaihui.tang@intel.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-12-04T13:24:09Z,"fix chatglm accuracy regression and support chatglm2 prompt (#859)

Signed-off-by: Wang, Chang1 <chang1.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-12-04T12:30:54Z,"[LLM] make text-generation example woq default weight dtype to ""nf4"" (#853)

* make woq int4 default weight dtype to nf4

Signed-off-by: Wang, Chang1 <chang1.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-12-04T03:39:23Z,"[LLM] add an option to enable fallback_add (#849)

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-12-01T12:36:57Z,[LLM] add revision to text-generaion example (#843)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-12-01T06:31:36Z,[LLM] add chatglm and codellama extension test (#837)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-11-30T06:23:43Z,[LLM] Support chatglm/falcon series and unified int8 loading (#814)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-11-28T03:40:59Z,"improve text-generation example (#792)

Signed-off-by: Wang, Chang1 <chang1.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-11-24T09:06:37Z,Fix nightly CI issue (#765)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-11-24T00:57:46Z,"move the class path (#758)

Signed-off-by: Wang, Chang1 <chang1.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-11-23T06:05:01Z,SmoothQuantConfig support recipes (#750)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-11-23T02:09:14Z,[LLM] Support gpt_neox ipex.optimize_transformers and update quantization workflow. (#741)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-11-22T07:40:55Z,Move TSModelCausalLMForOPTLLM to lm-eval (#744)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-11-21T05:24:06Z,Smoothquant support ipex.optimize_transformers feature (#695)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-11-15T07:18:09Z,"add save for mixedprecision model (#687)

Signed-off-by: changwangss <chang1.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-11-13T10:04:12Z,"[LLM] text-generation example support peft model quantization (#668)

* text-generation example support peft model

Signed-off-by: changwangss <chang1.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-11-13T05:31:56Z,[LLM] text-generation example support chatglm2&3 (#638)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-11-01T05:25:55Z,"support op_name_dict (#604)

Signed-off-by: changwangss <chang1.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-10-31T09:40:19Z,Fix example wrong name and past_kv shape (#561)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-10-24T11:51:23Z,"support sq auto tune (#537)

Signed-off-by: changwangss <chang1.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-10-23T14:50:09Z,[Optimization] Text-generation support qwen (#513)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-09-26T04:10:10Z,change mainpage (#340)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-09-21T12:04:57Z,"Support load_in_4bit and load_in_8bit on CPU device (#354)

* support load_in_4bit and load_in_8bit, remove gptq parser

Signed-off-by: changwangss <chang1.wang@intel.com>

* fix readme

Signed-off-by: changwangss <chang1.wang@intel.com>

* add ut

Signed-off-by: changwangss <chang1.wang@intel.com>

* add awq/teq

Signed-off-by: changwangss <chang1.wang@intel.com>

* Update modeling_auto.py

* Update pytorch_optimize.json

---------

Signed-off-by: changwangss <chang1.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-09-20T07:21:02Z,Fix text-generation logger info (#349)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-09-18T23:05:36Z,"Enable text-generation with new API (#318)

* enable text-generation with NeuralChat API

Signed-off-by: changwangss <chang1.wang@intel.com>

* fix wrong typing and hide import

Signed-off-by: changwangss <chang1.wang@intel.com>

* improve import check

* rebase main

Signed-off-by: changwangss <chang1.wang@intel.com>

* remove the outdated code

Signed-off-by: changwangss <chang1.wang@intel.com>

* update order

* improve sqconfig and add ut

Signed-off-by: changwangss <chang1.wang@intel.com>

* refine woq

Signed-off-by: changwangss <chang1.wang@intel.com>

* fix mp name

Signed-off-by: changwangss <chang1.wang@intel.com>

* fix pylint

Signed-off-by: changwangss <chang1.wang@intel.com>

* fix import

Signed-off-by: changwangss <chang1.wang@intel.com>

* Fixed shape error for weight-only quantization op

Signed-off-by: Cheng, Penghui <penghui.cheng@intel.com>

* Fixed UT error for weight-only quantization

Signed-off-by: Cheng, Penghui <penghui.cheng@intel.com>

* improve the example

Signed-off-by: changwangss <chang1.wang@intel.com>

* Update README.md

* fix long line

Signed-off-by: changwangss <chang1.wang@intel.com>

* fix import

Signed-off-by: changwangss <chang1.wang@intel.com>

* Update README.md

* Update test_quantization.py

---------

Signed-off-by: changwangss <chang1.wang@intel.com>
Signed-off-by: Cheng, Penghui <penghui.cheng@intel.com>
Co-authored-by: Cheng, Penghui <penghui.cheng@intel.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-09-12T23:34:04Z,Update run_generation.py (#301)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-09-12T05:13:58Z,"Improve text-generation example ipex installation (#292)

* improve ipex installation

Signed-off-by: Wang, Chang1 <chang1.wang@intel.com>

* Update README.md

* Update README.md

---------

Signed-off-by: Wang, Chang1 <chang1.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-09-08T08:22:19Z,"Refine mpt text-generation example (#257)

* refine mpt text-generation example

Signed-off-by: Wang, Chang1 <chang1.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-08-29T10:33:08Z,refact folder stracture (#170)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-08-22T02:48:16Z,"enhance text-generation example (#155)

Signed-off-by: changwangss <chang1.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-07-07T09:43:05Z,Improve MPT model loading folder (#1142)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-07-05T06:00:56Z,"fix text-generation perf (#1119)

Signed-off-by: changwangss <chang1.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-07-03T08:35:57Z,Add MPT example for text-generation (#1091)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-06-21T10:51:47Z,Fix poor generation issue (#1065)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-06-19T10:07:17Z,"fix evaluation init import issue (#1053)

Signed-off-by: changwangss <chang1.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-06-14T03:33:28Z,"remove exit(0) from examples (#1032)

* remove exit(0) from examples

* fix exit(0)

Signed-off-by: Wang, Chang1 <chang1.wang@intel.com>

---------

Signed-off-by: Wang, Chang1 <chang1.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-06-07T08:59:06Z,support lm-eval accuracy metric for generation task torchscript model with past-kv (#973)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-06-02T02:37:26Z,Enable opt models for text-generation (#968)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-05-25T08:48:30Z,support bloom in text generation task (#941)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-05-16T08:30:32Z,fix gpt-neox text-generation (#940)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-05-16T07:15:02Z,update text-generation example to support transformers 4.28.1 (#917)
github.com/intel/intel-extension-for-transformers,examples/huggingface/pytorch/text-generation/quantization/run_generation.py,2023-05-08T10:08:05Z,Refine language modeling (#900)
github.com/Fanghua-Yu/SUPIR,llava/model/builder.py,2024-01-25T14:42:59Z,20240125
github.com/huggingface/trl,tests/test_reward_trainer.py,2024-02-15T13:47:32Z,"[`core` /  `xxxTrainer`] Automatic tagging (#1329)

* automatic tagging

* add comments

* fix tests

* fix"
github.com/huggingface/trl,tests/test_reward_trainer.py,2024-02-15T03:37:41Z,"pre-commit: replace linters + formatters with Ruff; fix some issues (#1300)

* pre-commit: replace linters + formatters with Ruff

* Don't use bare except

* Clean up `noqa`s

* Enable Ruff UP; apply auto-fixes

* Enable Ruff B; apply fixes

* Enable Ruff T with exceptions

* Enable Ruff C (complexity); autofix

* Upgrade Ruff to 0.2.0"
github.com/huggingface/trl,tests/test_reward_trainer.py,2024-02-01T22:49:03Z,"Codemod Unittest assertions to bare asserts (#1301)

* Remove stray commas from test data

* Codemod Unittest assertions to bare asserts

* Make `assertAlmostEqual` tests more idiomatic

* DRY some test strings"
github.com/huggingface/trl,tests/test_reward_trainer.py,2023-09-20T08:18:38Z,"Add margin to RM training (#719)

* Start adding margin to RM training

* Fix typo and cleanup

* Fix incompatibilities when not using margin

* Format using 'make precommit'

* Add documentation and test for reward trainer

* Run 'make precommit'

* Update docs/source/reward_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Fix missed merge conflict in reward trainer docs

---------

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>"
github.com/huggingface/trl,tests/test_reward_trainer.py,2023-09-05T07:05:42Z,"Refactor RewardTrainer hyperparameters into dedicated dataclass (#726)

* Refactor RewardTrainer hyperparameters into dedicated dataclass

* Revert

* Add doc string

* Fix warning

* Handle backwards compat

* Fix tests

* Add docs

* Refactor to RewardConfig

* Fix case conditions

* Fix"
github.com/huggingface/trl,tests/test_reward_trainer.py,2023-06-26T12:30:06Z,fix CI RM (#468)
github.com/huggingface/trl,tests/test_reward_trainer.py,2023-06-09T13:52:41Z,Update test_reward_trainer.py (#421)
github.com/huggingface/trl,tests/test_reward_trainer.py,2023-06-07T10:22:22Z,Update test_reward_trainer.py (#410)
github.com/huggingface/trl,tests/test_reward_trainer.py,2023-06-06T14:49:30Z,"Resolve broken evaluation/prediction for RewardTrainer (#404)

* Implement evaluation/prediction for RewardTrainer

* Stick with unittest assertions

* Perform prediction forward calls without gradient

* Remove Literal to preserve Python 3.7 support

I recognize that I can also import from typing_extensions with a try-except,
but that is a bit overkill for this I feel.

* Remove eval_steps=1 to prevent flaky test on CI

The flaky test is caused by a division by zero when dividing by the runtime.
This is done on the transformers side, so it's not a TRL issue.
In practice, this won't happen - it only happens because both the model
and dataset are tiny."
github.com/huggingface/trl,tests/test_reward_trainer.py,2023-04-26T09:51:56Z,"[`core`] Officially Support Reward Modeling (#303)

* v1

- add working version
- add all possible tests
- add docs

* add some contents

* clean up

* fixes

* patch test for now

* fix test

* clean up

* fix

* this time fix

* Update docs/source/trainer.mdx

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>

* fixe

* update

* final changes

* oops

* Update docs/source/reward_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update docs/source/reward_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update docs/source/reward_trainer.mdx

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* switch to chosen / rejected

* fixes

* add example

* add accuracy metric

* pass PEFT config

* refactor compute metrics

---------

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>
Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>"
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2024-02-18T08:06:49Z,修改check_checkpoint_config_path逻辑
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2024-02-16T14:45:01Z,增加PipelineBase基类
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2024-02-15T15:06:58Z,添加参数备注，开始允许下载
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2024-02-03T16:17:29Z,debug生成速度
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2024-02-02T10:45:18Z,减少耗时
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2024-01-27T14:48:52Z,modify load_variable
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2024-01-26T10:59:30Z,修改load_variable
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2024-01-24T10:17:44Z,fix hierarchical
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2024-01-22T16:09:46Z,debug save pretrained
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2024-01-22T10:57:11Z,add copy_tree
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2024-01-21T15:47:50Z,debug t5 use states
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2024-01-16T15:42:35Z,v0.4.6
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2024-01-16T10:46:38Z,modify from_pretrain_single
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2024-01-16T10:26:21Z,test save_pretrained
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2024-01-15T15:33:19Z,test save_pretrained
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2024-01-14T16:18:54Z,test basic
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2024-01-12T09:46:30Z,debug from_pretrained
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2024-01-12T06:46:09Z,debug save_pretrained
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2023-12-20T04:18:20Z,import_utils
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2023-12-19T15:23:10Z,v0.4.2
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2023-12-18T05:27:01Z,add some print
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2023-12-17T15:38:00Z,support safetensors
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2023-12-17T12:17:36Z,debug chat
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2023-12-13T15:45:06Z,fix chat module
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2023-12-13T10:48:55Z,add chat module
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2023-11-06T15:10:06Z,v0.3.7
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2023-11-02T10:18:49Z,rm gamma beta
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2023-11-02T08:20:10Z,modify load_ckpt
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2023-11-02T06:17:33Z,modify load_ckpt
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2023-09-03T16:18:58Z,modify comment
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2023-08-26T16:58:24Z,modify rlhf
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2023-08-12T06:30:07Z,v0.3.3
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2023-08-08T16:06:02Z,fix load_ckpt
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2023-08-08T03:09:58Z,rely accelerate
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2023-08-07T15:53:52Z,fix shards load
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2023-08-06T15:47:40Z,add qwen
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2023-07-16T11:35:07Z,v0.3.0
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2023-07-11T09:52:22Z,modify layernorm
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2023-07-10T12:43:29Z,fix skip init bugs
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2023-07-10T08:27:46Z,add low_cpu_mem_usage
github.com/Tongjilibo/bert4torch,bert4torch/models/base.py,2023-07-06T09:28:04Z,modify models and layers to dir
github.com/SCIR-HI/Huatuo-Llama-Med-Chinese,export_hf_checkpoint.py,2023-04-01T09:37:49Z,init code
github.com/InternLM/xtuner,xtuner/tools/eval_refcoco.py,2024-02-26T06:55:32Z,"add refcoco to llava (#425)

* add base dataset

* update dataset generation

* update refcoco

* add convert refcooc

* add eval_refcoco

* add config

* update dataset

* fix bug

* fix bug

* update data prepare

* fix error

* refactor eval_refcoco

* fix bug

* fix error

* update readme

* add entry_point

* update config

* update config

* update entry point

* update

* update doc

* update

---------

Co-authored-by: jacky <jacky@xx.com>"
github.com/xusenlinzy/api-for-open-llm,api/utils/apply_lora.py,2023-08-24T02:11:15Z,Make embedding api compatible for openai
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-05-25T16:38:44Z,fixed prefix tuning inference bug
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-05-16T16:02:09Z,upload commonsense evaluate scripts
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-21T09:49:20Z,delete locating root path code
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-21T07:51:57Z,Merge branch 'main' into main
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-21T07:20:57Z,added display progress for evaluation and fixed NoneType error
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-21T07:19:28Z,Merge branch 'main' of github.com:HZQ950419/LLM-Adapters into main
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-21T07:14:05Z,added display process for evaluation and fixed NoneType error
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-14T11:13:05Z,Add files via upload
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-13T19:24:41Z,Update evaluate.py
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-10T12:06:22Z,enable checking progress
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-08T17:55:36Z,enable testing
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-07T13:43:02Z,"Update evaluate.py

create experiment folder when folder not created"
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-07T12:04:58Z,fixed inference error without loading int8 model
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-07T11:58:38Z,fixed inference error without loading int8 model
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-07T11:14:17Z,fixed inference error without loading int8 model
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-07T02:21:16Z,fixed unexpected character in prompt
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-04T11:21:26Z,modified peft/setup.py for installation
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-04T08:31:25Z,update evaluate code
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-04T02:44:45Z,fixed a forward bug with autocast for bottleneck adapters
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-03T15:51:49Z,fix evaluate error and upload README
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-02T19:18:11Z,update test code
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-02T18:44:29Z,update test code
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-02T18:34:07Z,update test code
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-02T18:16:38Z,update test code
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-02T17:59:25Z,update test code
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-02T17:57:29Z,update test code
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-02T17:53:42Z,update test code
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-02T17:51:52Z,update test code
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-02T17:47:18Z,update test code
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-02T16:27:57Z,update dataset
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-02T14:01:25Z,update evaluate.py
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-02T13:43:14Z,update evaluate.py
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-02T08:27:11Z,update test code
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-02T08:23:45Z,update test data and code
github.com/AGI-Edgerunners/LLM-Adapters,evaluate.py,2023-04-02T08:00:12Z,update test code running math data
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/llm/evaluation/lm_eval/models/huggingface.py,2024-01-31T09:18:17Z,Add precommit config (#1216)
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/llm/evaluation/lm_eval/models/huggingface.py,2024-01-15T06:26:19Z,"[Runtime] calculate accuracy of runtime (#1123)

* acc runtime

Signed-off-by: zhenwei-intel <zhenwei.liu@intel.com>

* llama2

Signed-off-by: zhenwei-intel <zhenwei.liu@intel.com>

* update

Signed-off-by: Dong, Bo <bo1.dong@intel.com>

* use evaluate

Signed-off-by: zhenwei-intel <zhenwei.liu@intel.com>

* fix llama

Signed-off-by: zhenwei-intel <zhenwei.liu@intel.com>

* fix rope scale

Signed-off-by: zhenwei-intel <zhenwei.liu@intel.com>

* add args

Signed-off-by: zhenwei-intel <zhenwei.liu@intel.com>

* logits_all

Signed-off-by: zhenwei-intel <zhenwei.liu@intel.com>

* add copyright

Signed-off-by: zhenwei-intel <zhenwei.liu@intel.com>

* fix rope scale

Signed-off-by: zhenwei-intel <zhenwei.liu@intel.com>

* update model format

Signed-off-by: zhenwei-intel <zhenwei.liu@intel.com>

* update convert

Signed-off-by: zhenwei-intel <zhenwei.liu@intel.com>

* add args

Signed-off-by: zhenwei-intel <zhenwei.liu@intel.com>

* update format

Signed-off-by: zhenwei-intel <zhenwei.liu@intel.com>

* update script

Signed-off-by: zhenwei-intel <zhenwei.liu@intel.com>

* enable GLIBCXX_3.4.26

Signed-off-by: Wenxin Zhang <wenxin.zhang@intel.com>

* fix conda env name

Signed-off-by: Wenxin Zhang <wenxin.zhang@intel.com>

* fix pylint

Signed-off-by: zhenwei-intel <zhenwei.liu@intel.com>

* add gptq

Signed-off-by: zhenwei-intel <zhenwei.liu@intel.com>

* move to examples

Signed-off-by: zhenwei-intel <zhenwei.liu@intel.com>

---------

Signed-off-by: zhenwei-intel <zhenwei.liu@intel.com>
Signed-off-by: Dong, Bo <bo1.dong@intel.com>
Signed-off-by: Wenxin Zhang <wenxin.zhang@intel.com>
Co-authored-by: Dong, Bo <bo1.dong@intel.com>
Co-authored-by: Wenxin Zhang <wenxin.zhang@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/llm/evaluation/lm_eval/models/huggingface.py,2024-01-12T09:00:19Z,"fix Qwen-72b eval precision issue (#1134)

* fix Qwen-72b eval precision issue

Signed-off-by: Zhang, Weiwei1 <weiwei1.zhang@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/llm/evaluation/lm_eval/models/huggingface.py,2023-12-21T10:38:22Z,[LLM] Fix llm models extension issue (#955)
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/llm/evaluation/lm_eval/models/huggingface.py,2023-12-14T05:54:58Z,Support evaluation for onnx model exported with optimum >= 1.14.0 (#910)
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/llm/evaluation/lm_eval/models/huggingface.py,2023-11-13T05:31:56Z,[LLM] text-generation example support chatglm2&3 (#638)
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/llm/evaluation/lm_eval/models/huggingface.py,2023-11-09T13:22:26Z,"Fix ONNXRT session bug with upgraded optimum 1.14.0 (#644)

Fix ONNXRT session bug with upgraded optimum 1.14.0 (#644)

Signed-off-by: yuwenzho <yuwen.zhou@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/llm/evaluation/lm_eval/models/huggingface.py,2023-11-06T01:22:23Z,"[LLM] Update lm-eval commit id (#617)

* update lm-eval commit

Signed-off-by: Wang, Chang <chang1.wang@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/llm/evaluation/lm_eval/models/huggingface.py,2023-09-22T07:50:44Z,"Support lm-eval for merged decoder ONNX models (#371)

* update for merged onnx model

Signed-off-by: yuwenzho <yuwen.zhou@intel.com>"
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/llm/evaluation/lm_eval/models/huggingface.py,2023-08-29T10:33:08Z,refact folder stracture (#170)
github.com/li-plus/chatglm.cpp,chatglm_cpp/convert.py,2023-11-22T01:51:25Z,Support all features for ChatGLM3 (system prompt / function call / code interpreter) (#197)
github.com/li-plus/chatglm.cpp,chatglm_cpp/convert.py,2023-10-30T07:39:08Z,Fix conversion error for ChatGLM3 32k (#163)
github.com/li-plus/chatglm.cpp,chatglm_cpp/convert.py,2023-10-29T13:42:06Z,Support ChatGLM3 (#158)
github.com/li-plus/chatglm.cpp,chatglm_cpp/convert.py,2023-10-22T02:43:25Z,Support InternLM 7B & 20B (#149)
github.com/li-plus/chatglm.cpp,chatglm_cpp/convert.py,2023-09-28T11:58:50Z,"Support Baichuan-7B architecture (#134)

* Support Baichuan-7B architecture

* Bump version

* Unify model config"
github.com/li-plus/chatglm.cpp,chatglm_cpp/convert.py,2023-08-31T11:40:14Z,Support Baichuan-13B model on CPU & CUDA (#115)
github.com/li-plus/chatglm.cpp,chatglm_cpp/convert.py,2023-08-11T12:09:39Z,"Support loading from HF models and convert to GGML on the fly (#90)

* Auto assign cuda buffer

* Support convert-on-load for py binding"
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/training/peft/tuners/prompt_tuning.py,2023-09-04T06:02:25Z,Add support for kbits training
github.com/X-PLUG/mPLUG-Owl,mPLUG-Owl2/mplug_owl2/model/builder.py,2024-02-02T17:48:32Z,Merge branch 'main' into kwargs-for-model-loading
github.com/X-PLUG/mPLUG-Owl,mPLUG-Owl2/mplug_owl2/model/builder.py,2024-01-31T17:04:59Z,owl 2.1
github.com/X-PLUG/mPLUG-Owl,mPLUG-Owl2/mplug_owl2/model/builder.py,2023-11-27T18:33:44Z,"🩹 make load_pretrained_model accept kwargs

- this allows it to e.g. take a cache_dir argument"
github.com/X-PLUG/mPLUG-Owl,mPLUG-Owl2/mplug_owl2/model/builder.py,2023-11-08T10:50:07Z,Launched mPLUG-Owl2
github.com/AGI-Edgerunners/LLM-Adapters,generate.py,2023-04-13T19:23:52Z,Update generate.py
github.com/AGI-Edgerunners/LLM-Adapters,generate.py,2023-04-07T02:21:16Z,fixed unexpected character in prompt
github.com/AGI-Edgerunners/LLM-Adapters,generate.py,2023-04-04T11:21:26Z,modified peft/setup.py for installation
github.com/AGI-Edgerunners/LLM-Adapters,generate.py,2023-04-02T06:50:08Z,solve the generate bug using bottleneck adapters
github.com/AGI-Edgerunners/LLM-Adapters,generate.py,2023-04-01T11:59:26Z,support parallel adapter and gpt-j model
github.com/AGI-Edgerunners/LLM-Adapters,generate.py,2023-03-29T13:37:38Z,initial upload
github.com/liguodongiot/llm-action,train/peft/multimodal/blip2_lora_inference.py,2023-11-30T14:04:31Z,fix-2023-11-30_22:04:31
github.com/GoogleCloudPlatform/vertex-ai-samples,community-content/vertex_model_garden/model_oss/peft/handler.py,2024-02-20T15:17:32Z,Unify the model name as MODEL_ID in related containers and notebooks. (#2720)
github.com/GoogleCloudPlatform/vertex-ai-samples,community-content/vertex_model_garden/model_oss/peft/handler.py,2024-01-31T16:12:34Z,Add Peft additional parameters (#2657)
github.com/GoogleCloudPlatform/vertex-ai-samples,community-content/vertex_model_garden/model_oss/peft/handler.py,2023-12-04T15:11:07Z,copy vertex_vision_model_garden as vertex_model_garden (#2565)
github.com/THUDM/AgentTuning,eval_heldout/rewoo/alpaca/lora.py,2023-10-18T19:04:28Z,First commit
github.com/bigcode-project/starcoder,finetune/merge_peft_adapters.py,2023-05-31T17:20:13Z,Removed unused line
github.com/bigcode-project/starcoder,finetune/merge_peft_adapters.py,2023-04-26T14:44:42Z,merging script
github.com/tatsu-lab/alpaca_eval,src/alpaca_eval/decoders/huggingface_local.py,2024-01-20T23:19:13Z,"[TEST]: float preference (#214)

* [TEST]: float preference

* debug

* passing all the tests"
github.com/tatsu-lab/alpaca_eval,src/alpaca_eval/decoders/huggingface_local.py,2024-01-16T19:06:28Z,[DOC] explain params
github.com/tatsu-lab/alpaca_eval,src/alpaca_eval/decoders/huggingface_local.py,2024-01-16T09:34:36Z,"[ENH] add internlm2-chat-20b-ppo (#207)

* add model config

* modify `huggingface_local_completion` to remove EOS

* add results & update leaderboard

* delete extra leaderboard csv

* add docstring of `remove_ending`

* remove reference_outputs.json"
github.com/tatsu-lab/alpaca_eval,src/alpaca_eval/decoders/huggingface_local.py,2023-07-20T02:09:54Z,"[ENH] add replicate + llama 70B (#90)

* [ENH] add replicate decoder

* adding replicate llama 70B

* del .env

* add 70B llama results

* add 70B llama results noprompt"
github.com/tatsu-lab/alpaca_eval,src/alpaca_eval/decoders/huggingface_local.py,2023-07-05T16:58:10Z,"[CLEAN] setting up precommit (#61)

* setting up precommit

* applying isort"
github.com/tatsu-lab/alpaca_eval,src/alpaca_eval/decoders/huggingface_local.py,2023-07-05T11:42:46Z,black (#55)
github.com/tatsu-lab/alpaca_eval,src/alpaca_eval/decoders/huggingface_local.py,2023-07-05T11:31:18Z,"[TEST] test  (#50)

* [TEST] test_string_to_dict

* [TEST] test analyze

* [TEST] test parisers

* [TEST] test decoders integration

* [TEST] test decoders unit

* [TEST] test utils

* [TEST] rm openai test

* [TEST] test_pairwise_eval

* [TEST] test_main

* minor

* minor

* [BUG] docstring r""

* minor

* [BUG] allow no OPENAI_API_KEYS

* update evaluator lb

* setup for tests

* [BUG] doctest

* alpaca_eval_fn -> verified

* black

* ensure all test pass

* ensure all test pass"
github.com/tatsu-lab/alpaca_eval,src/alpaca_eval/decoders/huggingface_local.py,2023-07-05T00:11:06Z,fix falcon decoding
github.com/tatsu-lab/alpaca_eval,src/alpaca_eval/decoders/huggingface_local.py,2023-06-13T21:11:20Z,add AttributeError exception
github.com/tatsu-lab/alpaca_eval,src/alpaca_eval/decoders/huggingface_local.py,2023-06-13T14:09:39Z,remove comment about progress
github.com/tatsu-lab/alpaca_eval,src/alpaca_eval/decoders/huggingface_local.py,2023-06-13T14:03:07Z,add progress bar
github.com/tatsu-lab/alpaca_eval,src/alpaca_eval/decoders/huggingface_local.py,2023-06-08T12:24:23Z,"gitignore

setting up

clean utils

pairwise lb

types

initial setup

initial requirements

README

pairwise annotator done

openai done

main

metrics

setting up empty

license

all prompts

examples

add anthropic

add claude prompts

minor OAI

anthropic installation

get_decoder

get_decoder

max_instances

adding guanaco

oasst

stablelm

hugging face

remove langchain

minor

finish all decoders

huggingface_local_completions

huggingface_api_completions

PACKAGES_ALL

add opt test

update packages

debugging huggingface_local_completions

api_completions

[ENH] add timer

[ENH] fast hugging face local

[CONF] better default models

[CONF] adding all basic conf

tested all basic configs

add constatns

add constatns

add constatns

docstrings

gigignore

[ENH] cohere

[CLEAN] use hf datasets

cleaning

cleaning

WIP analyze

fn_completions

mino

[ENH] return price and time per example

[ENH] return price and time per example

add price and time for turkers

WIP agreement_of_annotations

[ENH] agreement_of_annotations

[ENH] add vicuna parsing

finish vicuna adding

[SCRIPT] add precompute script

[SCRIPT] add precompute script

add falcon

add vicuna with inputs

black

[ENH] list bias

[ENH] vicuna -> lmsys

[ENH] vicuna -> lmsys

black

alpaca_farm_ppo_human_7b

setup

max_instances

bug vicuna

[ENH] analyze_evaluators

clean prompts

minor

leaderboards

make_evaluator_leaderboard

rm make_evaluator_leaderboard

change gpt3 to text-davinci-003

[ENH] max_instances to precompute

solve merging

evaluator leaderboard

minor

add plotting

add plotting

rename all and finish leaderboard

rm json

add local models to lb

add local models to lb

add local models to lb

add local models to lb

README

update the readme

update the readme

initial adding of constants

ignore

claude lb

formatting

add make_model_leaderboard

update lb

add constants

minor

is_return_instead_of_print

save main outputs

MODELS_TO_BENCHMARK

update claude leaderboard

leaderbaords

rename

minor

minor

minor

[NOTEBOOK] compare annotators

rm i.dea

update readme

caching

prices

prices

gpt

leadeboards

instruction-following prompt

minor

minor

rm caches

leaderboard claude drop

aviary

aviary

README

aviary

readme

API constants

API constants

making new evaluator

formatting readme

minor

Making a new evaluator

minor

installation

developing notebooks

rm unecessary

ranking

better error

readme

minor

is_single_annotator

leaderboard

ANTHROPIC_MAX_CONCURRENCY

[enh] is_save_to_leaderboard

[enh] is_save_to_leaderboard

imports

ranking_parser

ranking_parser

minor rename

check imports

caching leaderboard

caching leaderboard

rename completion kwargs

rohan benchmarking

rm example

moving to evaluators_configs

single prompt

remove all unecessary prompts

model_configs

rm all input field

update readme

update readme

adding strip

documentation

[CONF] add improved configs

prompts

leaderboards

gitignore

anthropic n_retries

names of models to keep

hugging face inference_helper

save to results

constants

update readme

allow globing

leaderboards

cleaning leaderboards

cleaning leaderboards

package_data

delete example

add manifest

add outputs example

AlpacaEval

finish developing evalset

leaderboards

leaderboards

aviary

bug alpaca farm prompt

leaderboards

leaderboards

bias 1

compare annotators

notebook anntoators

constants

precompute

allow additional columns

leaderboard

update lb

add table of content

add TOC

adding more dropdowns

update leaderboard

update leaderboards

boilerplate for website

move boilerplate

Create CNAME

Delete CNAME

AlpacaFarm -> AlpacaEval

adding doc

update html

adding helper

adding all helper to README

update all leaderboards

update all leaderboards

smaller example of outputs

add leaderboard modes

udpate readmes

evaluators leaderboard

print_leaderboard

udpate precompute

constants

leaderboard_mode_to_print to analyze eval

update html

add radio buttons

udpate differences with alpacafarm

update all notebooks

error out

003 leaderboard

notebooks analyzing all

analyzing_annotators

finish plotting of analyzs

add figures

add figures

dding first plot

finish readme

finish readme

fix typos in readme.

fix citation issues.

fix readme.

fix setup.

minor.

add outputs.json example

fix small issues with first headline cmd.

title aesthetics.

title.

add filters button

add all model configs

add results export file

minor diffs

prettify website

udpate leaderboards

finish website

scoping intro

scoping intro

scoping intro

bug fix

add gpt4 full leaderboard

udpate gpt4 leaderboard website

add interpretation of leaderboards

finish explanation of main eval metrics

finish explanation of all eval metrics

finish explanation of all eval metrics

finish explanation of all eval metrics

finish up to evaluator

test

test

run on claude instead of gpt4

add related work

shorter section

add limitation section

add to related work

add to related work

finish readme

update website:

format dividers

update readme

make image bigger

make image bigger

add contribution guidelines

typo

update readmes

running notebook

add wizard lm

change subtitle webiste

add link

add github

update leaderboards

last

update

finished through tatsu PR

finished through tatsu PR

pass through tatsu PR

pass through tatsu PR

add github"
github.com/microsoft/Olive,olive/passes/pytorch/lora.py,2024-02-29T10:04:27Z,"enable lint f-string check (#973)

## Describe your changes
* Enable linter check for f-string logging

## Checklist before requesting a review
- [ ] Add unit tests for this change.
- [ ] Make sure all tests can pass.
- [ ] Update documents if necessary.
- [ ] Lint and apply fixes to your code by running `lintrunner -a`
- [ ] Is this a user-facing change? If yes, give a description of this
change to be included in the release notes.

## (Optional) Issue link"
github.com/microsoft/Olive,olive/passes/pytorch/lora.py,2024-01-26T07:39:32Z,"fix pylint failure caused by wrong ignore-paths pattern (#902)

## Describe your changes

## Checklist before requesting a review
- [ ] Add unit tests for this change.
- [ ] Make sure all tests can pass.
- [ ] Update documents if necessary.
- [x] Lint and apply fixes to your code by running `lintrunner -a`
- [ ] Is this a user-facing change? If yes, give a description of this
change to be included in the release notes.

## (Optional) Issue link"
github.com/microsoft/Olive,olive/passes/pytorch/lora.py,2024-01-09T23:10:18Z,"Add `LoftQ` pass (#861)

## Describe your changes
This PR adds a new pass `LoftQ` which is based on the paper
https://arxiv.org/abs/2310.08659.
It adds on the qlora fine tuning approach by initializing the
quantization and lora adapters together. This ""alleviates the
discrepancy between the quantized and full-precision model and
significantly improves generalization in downstream tasks"".

Since most of the fine-tuning steps are the same as `QLoRA`, the common
code has been abstracted into a new base class called `QLoRABase`.
`LoftQ` is a separate pass and not an option for `QLoRA` so that we
don't conflate it too much and make the configuration confusing for the
user. They can just switch the pass type instead.

 
## Checklist before requesting a review
- [x] Add unit tests for this change.
- [x] Make sure all tests can pass.
- [x] Update documents if necessary.
- [x] Lint and apply fixes to your code by running `lintrunner -a`
- [x] Is this a user-facing change? If yes, give a description of this
change to be included in the release notes.

Introduced a new pass `LoftQ` which performs model fine-tuning using the
LoftQ initialization proposed in https://arxiv.org/abs/2310.08659.

## (Optional) Issue link"
github.com/microsoft/Olive,olive/passes/pytorch/lora.py,2023-12-12T02:47:52Z,"Fix onnx conversion of bnb quantized model, Remove static methods from `HfConfigMixin`  (#807)

## Describe your changes
This PR fixes some issues with the conversion of 4bit quantized models
to onnx:
- The adapters are unloaded first so that the correct quantized moduels
can be found
- A new pytorch handler is created. Previously, we were ignoring the
adapters

Changes the static method in `HfConfigMixin` to regular methods since
these were made static to support the above use case. But they are no
longer required to be static. The code is now more consistent.

Test requirements updated to for transformers since lora unit tests fail
otherwise.
 
## Checklist before requesting a review
- [x] Add unit tests for this change.
- [x] Make sure all tests can pass.
- [ ] Update documents if necessary.
- [x] Lint and apply fixes to your code by running `lintrunner -a`
- [ ] Is this a user-facing change? If yes, give a description of this
change to be included in the release notes.

## (Optional) Issue link"
github.com/microsoft/Olive,olive/passes/pytorch/lora.py,2023-12-08T07:51:33Z,"refactoring OliveModel and change its name to OliveModelHandler. (#786)

## Describe your changes
* Rename all OliveModel by adding suffix ""Handler"". For example
PyTorchModel -> PyTorchModelHandler.
* Split the logic doesn't belong to model handler into separated mixins.
HfConfigMixin, IoConfigMixin, DummyInputsMixin, etc.
* Merge the CompositeOnnxModel and CompositePyTorchModel into one
CompositeModelHandler.
* Keep using the old ""OliveModel"" types

Basically, all the logic is keeping same except some logic simplify
changes:
* Merge the Composite ONNX and PyTorch model. 
* Simplify the composite model enumerate logic by using a single method.
* Same for original PyTorch model to get the hugging face model
components. In the new implementation, only one get_hf_components is
used to enumerate both name and model.
* As a side effect the merging composite onnx model and pytorch model,
the original CompositePyTorch model conversion logic is in
conversion.py, but with the new changes, the child model get and run for
each child model will be done by the run of olive_pass.py
* As a side effect of providing only single entry point to enumerate the
model components, the insert_beam_search pass is updated by using the
new get model component logic, so does the olive_pass.run.

From the time being, the following logic is not tested
* Distributed models
* Composite PyTorch model

## Checklist before requesting a review
- [x] Add unit tests for this change.
- [ ] Make sure all tests can pass.
- [x] Update documents if necessary.
- [x] Lint and apply fixes to your code by running `lintrunner -a`
- [ ] Is this a user-facing change? If yes, give a description of this
change to be included in the release notes.

## (Optional) Issue link"
github.com/microsoft/Olive,olive/passes/pytorch/lora.py,2023-12-07T02:48:35Z,"Add `pydantic_v1` module to make Olive `pydantic` version agnostic (#788)

## Describe your changes
Pydantic v2 had significant breaking changes which blocks migration to
it. So, we pinned the version to v1.

Pydantic v2 has a `v1` namespace inside it. 

This PR introduces `olive.common.pydantic_v1` module that handles the
import of `v1` api so that Olive can be version agnostic wrt pydantic.
This way, we won't have version conflict with client applications such
as that in https://github.com/microsoft/Olive/issues/709.

Note:
The doc builder still needs pydantic v1 and autodoc_pydantic v1 since we
are using the v1 api.

## Checklist before requesting a review
- [ ] Add unit tests for this change.
- [ ] Make sure all tests can pass.
- [ ] Update documents if necessary.
- [ ] Lint and apply fixes to your code by running `lintrunner -a`
- [ ] Is this a user-facing change? If yes, give a description of this
change to be included in the release notes.

## (Optional) Issue link"
github.com/microsoft/Olive,olive/passes/pytorch/lora.py,2023-12-06T10:43:39Z,"Rename HFModelLoadingArgs to HFFromPretrainedArgs (#785)

## Describe your changes

Rename `HFModelLoadingArgs` to `HFFromPretrainedArgs`.

###
User interface change:
In model config, the `model_loading_args` should be
`from_pretrained_args` instead:
```
""input_model"":{
    ""type"": ""PyTorchModel"",
    ""config"": {
        ""hf_config"": {
            ""model_name"": ""microsoft/phi-1_5"",
            ""task"": ""text-generation"",
            ""model_loading_args"" -> ""from_pretrained_args"": {
                ""trust_remote_code"": true
            }
        }
    }
}
```


## Checklist before requesting a review
- [ ] Add unit tests for this change.
- [ ] Make sure all tests can pass.
- [ ] Update documents if necessary.
- [ ] Lint and apply fixes to your code by running `lintrunner -a`
- [x] Is this a user-facing change? If yes, give a description of this
change to be included in the release notes.

## (Optional) Issue link"
github.com/microsoft/Olive,olive/passes/pytorch/lora.py,2023-12-01T08:48:06Z,"LoRA/QLoRA: Add `bfloat16` support for ort-training (#769)

## Describe your changes
ONNX Runtime training now supports `bfloat16` for lora and qlora.

This PR
- Enables ort-training + bfloat16 by setting
`ORTMODULE_ONNX_OPSET_VERSION` environment variable. Default value is 16
which is required for models with operators such as Where.
- Adds unit test for the environment variable.
- Improves dependency checks in the lora passes.
- Better organizes the extras for lora/qlora. Workflow setup allows
assigning multiple extras to a single pass.

## Checklist before requesting a review
- [x] Add unit tests for this change.
- [x] Make sure all tests can pass.
- [x] Update documents if necessary.
- [x] Lint and apply fixes to your code by running `lintrunner -a`
- [x] Is this a user-facing change? If yes, give a description of this
change to be included in the release notes.
LoRA/QLoRA support `bfloat16` with ort-training.
## (Optional) Issue link"
github.com/microsoft/Olive,olive/passes/pytorch/lora.py,2023-11-17T01:06:03Z,"`trust_remote_code` for hf data components, Default text-gen corpus dataset_type (#728)

## Describe your changes
This PR 
- exposes `trust_remote_code` arguments to huggingface data components
that require user permission to run remote code. For input models with
hf config model_loading_args, the value for `trust_remote_code` is auto
inserted into the data configs without these specified.
- makes the default value for `dataset_type` in
`test_generation_pre_process` component to `corpus`. Previously, the
data config set it to `None` if not given and this results in using
`pair` strategy.
- Casts attention mask to input_id's data type to ensure it has the
expected data type.
- Removes `""dataset_type"":""corpus""` from the examples since it is the
default and the most common type.
- Corrects the dataset config in falcon example. Dataset is also changed
to `timdettmers/openassistant-guanaco` since the previous dataset is
massive (>1TB) and we only need it for latency measurement.
 
## Checklist before requesting a review
- [x] Add unit tests for this change.
- [x] Make sure all tests can pass.
- [ ] Update documents if necessary.
- [x] Lint and apply fixes to your code by running `lintrunner -a`
- [ ] Is this a user-facing change? If yes, give a description of this
change to be included in the release notes.

## (Optional) Issue link"
github.com/microsoft/Olive,olive/passes/pytorch/lora.py,2023-11-14T02:34:45Z,"LoRA/QLoRA: Mixed precision for `torch_dtype=float16`, Support onnxruntime-training (#722)

## Describe your changes
Changes in this PR:
- Full `float16` training is unstable. Now the lora passes use
mixed-precision training (`fp16=True` in the trainer) when `torch_dtype`
is `float16`.
- `compute_dtype` for the quantized modules is a separate config
parameter to allow for flexibility. If not provided, use the same dtype
as `torch_dtype`
- Support for `onnxruntime-training`. 

Also adds an example for qlora with onnxruntime-training. 

## Checklist before requesting a review
- [ ] Add unit tests for this change.
- [ ] Make sure all tests can pass.
- [ ] Update documents if necessary.
- [ ] Lint and apply fixes to your code by running `lintrunner -a`
- [ ] Is this a user-facing change? If yes, give a description of this
change to be included in the release notes.

## (Optional) Issue link"
github.com/microsoft/Olive,olive/passes/pytorch/lora.py,2023-11-08T05:35:56Z,"Accept device and dtype in `OnnxConversion`, Add `OnnxBnb4Quantization`, llama2 e2e qlora example (#703)

## Describe your changes
**OnnxConversion**
- The code has been refactored a bit so that the methods
`_convert_model_on_device` and `_convert_distributed_model_on_device`
have similar signature and behavior
- Doc strings for the methods
- The above also makes the logic for composite models simpler
- New config parameters added to make the pass more flexible:
- `use_device`: users can specify what device to run conversion on. For
example, there is enough gpu memory to speed up conversion or model is
float16.
- `torch_dtype`: torch data type to cast the model to before conversion.
For example, model is originally float16 but want to convert on cpu and
run another pass later to convert to float16.  
- It is more aware about loading models using `hf_config`.
  - Check and update `torch_dtype`
  - Check if model is quantized using bitsandbytes. 
  
**OnnxBnb4Quantization**
- New quantization pass to quantize a model using nf4/fp4. Uses the
`MatMulBnb4` quantizer and contrib op
- It can handle both pass config and model attributes such as
`quantized_modules` and `quantization_config`
- Using this pass, we usually don't want to quantize all `MaMul` nodes,
only the ones that were originally quantized in the source model to
retain accuracy

**llama2**
- Add E2E qlora + ort optimization workflow
- Update readme


![qlora-e2e](https://github.com/microsoft/Olive/assets/94929125/05600ba9-a862-4a98-8258-62a52bbfb713)

## Checklist before requesting a review
- [ ] Add unit tests for this change.
- [ ] Make sure all tests can pass.
- [x] Update documents if necessary.
- [x] Format your code by running `pre-commit run --all-files`
- [x] Is this a user-facing change? If yes, give a description of this
change to be included in the release notes.
`OnnxConversion` pass has new parameters `use_device` and `torch_dtype`
for more flexible conversion.
New `OnnxBnb4Quantization` to quantize an ONNX model using `FP4/NF4`
data types.
## (Optional) Issue link"
github.com/microsoft/Olive,olive/passes/pytorch/lora.py,2023-11-06T18:33:07Z,"[LoRA/QLoRA] Check for gradient checkpointing support before enabling it (#695)

## Describe your changes
These passes enabled gradient checkpointing without checking if the
model supports it.

Now, it not supported, they log a warning and force
`gradient_checkpointing` to `False`.
Also set `gradient_checkpointing` in `phi` example to `False` since it
doesn't actually do gradient checkpointing and the flag might be set to
False in a future update.
 
## Checklist before requesting a review
- [ ] Add unit tests for this change.
- [ ] Make sure all tests can pass.
- [ ] Update documents if necessary.
- [ ] Format your code by running `pre-commit run --all-files`
- [ ] Is this a user-facing change? If yes, give a description of this
change to be included in the release notes.

## (Optional) Issue link"
github.com/microsoft/Olive,olive/passes/pytorch/lora.py,2023-10-31T03:00:30Z,"Add LoRA pass (#638)

## Describe your changes

Add LoRA pass

## Checklist before requesting a review
- [x] Add unit tests for this change.
- [x] Make sure all tests can pass.
- [x] Update documents if necessary.
- [x] Format your code by running `pre-commit run --all-files`
- [ ] Is this a user-facing change? If yes, give a description of this
change to be included in the release notes.

## (Optional) Issue link

---------

Co-authored-by: Jambay Kinley <jambaykinley@microsoft.com>"
github.com/Clouditera/secgpt,train.py,2023-12-07T02:43:41Z,训练脚本小优化
github.com/Clouditera/secgpt,train.py,2023-11-20T10:16:26Z,代码优化
github.com/Clouditera/secgpt,train.py,2023-11-20T07:33:28Z,update readme
github.com/Clouditera/secgpt,train.py,2023-11-20T06:00:38Z,first blood
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/openai_server_demo/openai_api_server.py,2023-12-27T03:18:14Z,"Add 64k-context models (#478)

For full changes, see the latest release.

---------

Co-authored-by: iMountTai <2506700016@qq.com>
Co-authored-by: Xin Yao <35353688+iMountTai@users.noreply.github.com>"
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/openai_server_demo/openai_api_server.py,2023-09-21T03:28:33Z,"Fix quantized inference (#302)

Fixed possible mismatches caused by high version dependencies."
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/openai_server_demo/openai_api_server.py,2023-08-09T03:29:18Z,"Merge pull request #43 from lealaxy/stream-openai-api

Streaming OpenAI API support"
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/openai_server_demo/openai_api_server.py,2023-08-07T10:24:54Z,fix api first toekn missed
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/openai_server_demo/openai_api_server.py,2023-08-04T13:25:46Z,fix openai api  repetition of the last word
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/openai_server_demo/openai_api_server.py,2023-08-04T03:31:17Z,fix api extra token
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/openai_server_demo/openai_api_server.py,2023-08-03T17:09:16Z,fix stream api bug
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/openai_server_demo/openai_api_server.py,2023-08-03T16:58:34Z,fix stream api split
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/openai_server_demo/openai_api_server.py,2023-08-03T09:36:00Z,update default hyperparameters
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/openai_server_demo/openai_api_server.py,2023-08-03T06:29:31Z,Add openai stream api docs.
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/openai_server_demo/openai_api_server.py,2023-08-03T01:05:52Z,fix spelling error
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/openai_server_demo/openai_api_server.py,2023-08-02T12:36:06Z,Merge branch 'main' into stream-openai-api
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/openai_server_demo/openai_api_server.py,2023-08-02T09:31:57Z,Update openai_api_server.py
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/openai_server_demo/openai_api_server.py,2023-08-02T09:22:47Z,modify message information
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/openai_server_demo/openai_api_server.py,2023-08-02T08:49:13Z,Reserve load_in_8bit argument
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/openai_server_demo/openai_api_server.py,2023-08-02T03:04:09Z,"Merge pull request #35 from GoGoJoestar/main

add vLLM surpport for gradio demo, inference script and openai api demo"
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/openai_server_demo/openai_api_server.py,2023-08-01T16:06:17Z,add stream openai api support
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/openai_server_demo/openai_api_server.py,2023-08-01T12:21:05Z,fix system prompt
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/openai_server_demo/openai_api_server.py,2023-08-01T09:22:41Z,fix spelling error: tokenzier_vocab_size -> tokenizer_vocab_size
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/openai_server_demo/openai_api_server.py,2023-07-31T06:12:17Z,add scripts
github.com/lyhue1991/torchkeras,torchkeras/hugmodel.py,2024-03-01T06:08:50Z,update 3.9.7
github.com/lyhue1991/torchkeras,torchkeras/hugmodel.py,2023-12-04T07:24:11Z,master: Add comments according to Google Code Rules
github.com/lyhue1991/torchkeras,torchkeras/hugmodel.py,2023-07-15T08:09:08Z,update 3.9.1
github.com/lyhue1991/torchkeras,torchkeras/hugmodel.py,2023-06-25T16:06:26Z,update 3.8.9
github.com/lyhue1991/torchkeras,torchkeras/hugmodel.py,2023-06-11T15:23:16Z,update 3.8.8
github.com/lyhue1991/torchkeras,torchkeras/hugmodel.py,2023-05-29T14:32:11Z,update 3.8.6
github.com/lyhue1991/torchkeras,torchkeras/hugmodel.py,2023-05-27T06:00:49Z,update 3.8.6
github.com/FlagAI-Open/FlagAI,flagai/model/tools/peft/tuners/prompt_tuning.py,2023-07-05T08:20:51Z,"added enable input embeddings for lora

Signed-off-by: ftgreat <ftgreat@163.com>"
github.com/mosaicml/composer,tests/models/test_hf_model.py,2024-02-28T17:59:20Z,"Remove ""generation_length"" in favor of ""generation_kwargs"" (#3014)

* kill generation_length

* fix tests

* fix test

* add deprecation warning

* fix test

* add gen_len back into static_keys

* simplify setting variable in forward and add test

* simply test

* trailing comma

* trailing comma

* linting

---------

Co-authored-by: Daniel King <43149077+dakinggg@users.noreply.github.com>"
github.com/mosaicml/composer,tests/models/test_hf_model.py,2024-01-31T15:55:53Z,"Remove torch 1.13 (#2941)

* v1

* kill torch 1.13

* more cleanup

* remove local

* lint

* raise value error

* remove import

* lint

* fix

* fix type

* lint

* fix test

* remove type

* fix types

* fix

* fix

* fix

* remove vision

* fix tests"
github.com/mosaicml/composer,tests/models/test_hf_model.py,2024-01-30T06:03:43Z,Fix daily tests for peft on 1.13 (#2923)
github.com/mosaicml/composer,tests/models/test_hf_model.py,2024-01-30T00:20:14Z,Fix daily tests for peft + fsdp (#2920)
github.com/mosaicml/composer,tests/models/test_hf_model.py,2024-01-29T18:59:43Z,Integrate PEFT LoRA with HuggingFaceModel (#2829)
github.com/mosaicml/composer,tests/models/test_hf_model.py,2024-01-23T01:43:51Z,Bump transformers to 4.37 (#2894)
github.com/mosaicml/composer,tests/models/test_hf_model.py,2024-01-12T21:04:53Z,Upgrade pyright to 1.1.310 (#2841)
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-10-23T21:44:20Z,"Upgrade to transformers 4.34.1 (#2635)

* bump transformers version

* add new special casing to tokenizer equivalence check

* try/except for flash v1 issue"
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-10-05T21:04:29Z,Change the tokenizer json file to read binary (#2608)
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-09-26T00:48:04Z,"Add log_model to MLFlowLogger (#2541)

* tie module run name to client run name"
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-08-30T23:19:46Z,"Removing min_params (#2494)

* Removing min_params

* formatting?

* removing overlap with another commit"
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-08-28T18:35:34Z,Fix huggingface tokenizer loading for slow tokenizers (#2483)
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-07-25T21:35:04Z,"Change transformers (#2383)

* fix autoresume with slashed directory

* Revert ""fix autoresume with slashed directory""

This reverts commit 3dfb5f5430da5512bbf418820086b4f291d814f6.

revert

* upgrade transformers to 4.31.0

* add transformers support

* fix precommit

* fix precommit

* fix precommit

* add trust_remote_code

* pre-commit fix"
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-06-08T17:37:26Z,"fixed adding tokenizer to hf (#2290)

Fixed the function 'write_huggingface_pretrained_from_composer_checkpoint' to properly write the tokenizer from the composer checkpoint by additionally saving the tokenizer from the composer checkpoint to the HuggingFace checkpoint.

An additional test was implemented for this issue.

---------

Co-authored-by: Vincent Chen <vincent@mosaicml.com>"
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-06-05T19:53:50Z,Patch for tokenizers that have python files in save_pretrained output (#2279)
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-06-05T16:51:41Z,Confirming the output variable has two dimensions before confirming the shape of the second element. (#2275)
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-05-22T18:05:00Z,"CE loss vs CE metric equivalence (#2241)

* add equivalence test
* add the xfail test"
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-05-06T00:54:11Z,Add support for saving HF info in state dict when using DDP (#2206)
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-05-02T03:04:31Z,"Update warning->info for number of tokens (#2192)

* update warning

* fix test

* update warning

---------

Co-authored-by: nik-mosaic <101217697+nik-mosaic@users.noreply.github.com>"
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-04-20T21:43:04Z,"Upgrade to transformers 4.28 (#2152)

* upgrade transformers version
* fix tokenizer equivalence test"
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-04-06T20:22:50Z,add logic for direct instantiation (#2122)
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-04-03T18:07:46Z,"update transformers to latest version (#2109)

* update transformers to latest version
* xfail gpt + ddp"
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-03-31T21:14:11Z,add sentencepiece support (#2093)
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-03-24T00:37:58Z,skip fsdp tests for <1.13 (#2090)
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-03-21T00:47:40Z,"Add support for ICL QA tasks and generation during evaluation with `HuggingFaceModel` (#2045)

* Add InContextLearningQADataset and InContextLearningQAAccuracy for generation style tasks
* Add support for calling generate using HuggingFaceModel
* Fixes duplicated download of ICL datasets per node"
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-03-20T18:14:10Z,fix name_or_path usage in HF save/load usage (#2075)
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-03-07T21:41:56Z,"Remove deprecated code (#2026)

* cleanup

* cleanup

* fix hf model

* fix test

* fix test

* set to evla metrics

* fix bug in metrics

* switch to warnings

* add warnings

* filter warnings

* retry

* lint

* remove dead comment

* remove test"
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-03-06T20:40:38Z,Adjust how HuggingFaceModel handles embedding resizing (#2027)
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-03-01T22:53:40Z,"Upgrade torchmetrics (#2017)

Change default Accuracy to MulticlassAccuracy

---------

Co-authored-by: nik-mosaic <None>"
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-02-25T00:18:26Z,Test hf fsdp (#1972)
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-02-17T18:45:22Z,Util for writing HuggingFace save_pretrained from a composer checkpoint (#1974)
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-02-15T21:07:09Z,"allow eval metrics to be passed in to HuggingFaceModel directly (#1971)

* allow eval metrics to be passed in to HuggingFaceModel directly
* add deprecation warning and fix typo"
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-02-09T21:41:35Z,add support for enc-dec batches without decoder_input_ids (#1950)
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-02-09T18:29:30Z,add return dict false test and bug fix (#1948)
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-02-06T18:59:27Z,"Deprecate HFCrossEntropy and Perplexity (#1857)

* add languageperplexity and deprecate hfperplexity"
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-01-30T22:31:26Z,"Remove synthetic testing infrastructure for HF/NLP (#1895)

This PR removes the ""synthetic"" testing infrastructure setup for HF models, instead using a set of simple fixtures and pytest configuration to avoid downloading things from the HF hub multiple times. These fixtures should be easier to use (they are just the expected HF objects), and do away with a lot of extra code written for setting up the synthetic infrastructure."
github.com/mosaicml/composer,tests/models/test_hf_model.py,2023-01-27T00:48:37Z,"Add logic for shifting labels before computing metrics (#1913)

Adds a `shift_labels` init argument to `HuggingFaceModel` class. This instructs the model whether to shift labels by one token before computing metrics, which mimics the way HF Causal LM classes handle labels when computing loss. This fixes the current implementation, which never does this shifting and produces incorrect metric results for Causal LMs.

If `shift_labels` is not specified, `HuggingFaceModel` will try to infer the correct behavior based on whether the model is an instance of a registered HF Causal LM class (or a subclass of one)."
github.com/mosaicml/composer,tests/models/test_hf_model.py,2022-12-19T22:29:58Z,"HF factory function tests (#1832)

Add tests for the HF -> ComposerModel factory functions"
github.com/mosaicml/composer,tests/models/test_hf_model.py,2022-12-06T21:09:05Z,"Speeding up tests (#1779)

Reducing time spent in unit tests"
github.com/mosaicml/composer,tests/models/test_hf_model.py,2022-12-02T00:57:30Z,"Fix hf tokenizer test for new hf version (#1772)

Fix hf tokenizer test for new hf version"
github.com/mosaicml/composer,tests/models/test_hf_model.py,2022-11-30T21:55:15Z,"Autoload HuggingFace model/tokenizer (#1754)

Add autoload of HF model/tokenizer from composer checkpoint to HuggingFaceModel"
github.com/mosaicml/composer,tests/models/test_hf_model.py,2022-11-29T21:57:47Z,"Add huggingface info to state dict (#1744)

Add huggingface info to the state dict in the new integrations key."
github.com/mosaicml/composer,tests/models/test_hf_model.py,2022-11-14T21:50:07Z,"Simple nlp tests (#1716)

Adds a simple transformer classifier and a simple ""text"" (really just input ids, not using a tokenizer) classification dataset for testing purposes. Adds a test for HuggingfaceModel using the above dataset to train, eval, and predict. Removes the model_inputs from HuggingfaceModel"
github.com/mosaicml/composer,tests/models/test_hf_model.py,2022-07-27T16:45:58Z,"Remove `pytest-timeout` (#1317)

This PR removes pytest-timeout, all timeout markers from tests, and the --duration flag from running pytest.

The original intent of adding pytest-timeout was to a) incentivise tests to be fast, b) prevent stalled tests from hogging up system resources, and c) provide a heuristic that could be used to split in parallelize tests in the future. However, it has accomplished none of these goals.

a) Slow-running tests have just been giving longer timeouts to get it to pass ci/cd. Instead, slow-running tests should either be moved to run only on the daily build (i.e. @pytest.mark.daily), or be made faster.

b) If a test is stalling due to a distributed error, pytest-timeout doesn't help at all, since Composer will hang on a distributed barrier. Because of inconsistent CI/CD hardware that caused tests to sporadically exceed the timeout, we probably have used more CI/CD minutes re-running failed builds due to timeout failures compared to the amount of CI/CD time the timeout has saved. Instead, we should rely on the pytestTimeout variable in the Jenkinsfile -- this controls how long the k8s pod will run before dying, and it is not affected by torch.dist calls that could freeze the python process. It is set to 30 minutes right now, which is relatively short.

c) For when we want to split and parallelize tests, the junitxml reports already include the amount of time each test case took. So, we can split tests based on their historical execution time, rather than requiring an upper-bound be specified in the codebase.

Closes https://mosaicml.atlassian.net/browse/CO-769"
github.com/mosaicml/composer,tests/models/test_hf_model.py,2022-06-11T00:22:17Z,"Switch to single quotes (#1152)

Enable the ""double-quote-string-fixer"" pre-commit hook."
github.com/mosaicml/composer,tests/models/test_hf_model.py,2022-05-26T12:46:43Z,"AutoYAHP Part 2: Cleanup the Callbacks and Loggers for AutoYAHP (#1071)

Similar to #1056, this PR cleans up the callbacks and loggers for AutoYAHP. It does not depend on AutoYAHP itself (a future PR will remove the underlying hparam classes).

- Refactored callback and logger tests to not depend on hparams
- Reformatted the docstrings so they would be correctly parsed by auto-yahp
- Added `callback_settings.py`, similar to `algorithm_settings.py`, to return a list of pytest param objects for parameterization across callback tests. These param objects include appropriate markers (e.g. conditional skipping for wandb and mlperf; requiring that the memory monitor runs on GPU, ...)
- Moved the `TestTrainerAssets` into `tests/callbacks/test_callbacks.py`, since it tests the individual callbacks and loggers, not the trainer, and thus should live in `tests/callbacks`.
- Cleaned up the `MemoryMonitor` warnings when cuda is not available. Now, it warns when the model is not on cuda, to catch the edge case where one does CPU training when GPUs are available."
github.com/mosaicml/composer,tests/models/test_hf_model.py,2022-05-25T05:03:15Z,"AutoYAHP Part 1: Cleanup the Algorithms for AutoYAHP (#1056)

This PR refactors the algorithms and tests as will be required by AutoYAHP. It does not depend on AutoYAHP itself (a future PR will remove the underlying hparam classes).

- Refactored algorithm tests to not depend on hparams
- Reformatted the factorize and selective backprop docstrings so they would be correctly parsed by auto-yahp
- Refactor algorithm_settings.py to not depend on hparams and to return a list of pytest.param objects for a pytest.mark.parametrize. This change makes it more re-usable since it now includes information about markers required for each algorithm.
- Moved the TestTrainerAlgorithms into tests/algorithms/test_algorithms_train.py, since it tests the individual algorithms, not the trainer, and thus should live in tests/algorithms.
- Add helper methods for scanning a module to discover subclass implementations, check that the registry contains an entry, and test that a class is constructible from yaml"
github.com/mosaicml/composer,tests/models/test_hf_model.py,2022-05-24T21:18:36Z,Huggingface part1 (#1047)
github.com/liguodongiot/llm-action,train/alpaca-lora/export_state_dict_checkpoint.py,2023-07-23T11:43:58Z,fix
github.com/InternLM/lmdeploy,lmdeploy/pytorch/adapter/adapter.py,2024-02-23T03:12:05Z,"Support mistral and sliding window attention (#1075)

* support yi

* update docs

* add kernel

* fix seq

* add win block manager

* finish mistral

* fix drop block

* update docs

* fix ut

* fix for transformers 4.37.1

* update docs

* mistral template

* readme-zh-cn

* remove print

* remove div up

---------

Co-authored-by: grimoire <yaoqian@pjlab.org.cn>"
github.com/InternLM/lmdeploy,lmdeploy/pytorch/adapter/adapter.py,2024-02-19T03:32:30Z,fix pytorch engine with peft==0.8.2 (#1122)
github.com/InternLM/lmdeploy,lmdeploy/pytorch/adapter/adapter.py,2024-01-26T08:51:13Z,"Fix baichuan2 lora (#1042)

* update alibi attention

* fix baichuan tp

* remove func usage

* fix lora

* hide local scaling

* fix"
github.com/InternLM/lmdeploy,lmdeploy/pytorch/adapter/adapter.py,2024-01-16T10:22:55Z,support mlp s-lora (#957)
github.com/InternLM/lmdeploy,lmdeploy/pytorch/adapter/adapter.py,2024-01-09T07:33:41Z,"S-LoRA support (#894)

* WIP

* cache engine wip

* finish cache engine

* fix cache and scheduler

* add paged attention

* step and stop

* add infer

* add request process

* fix end

* request without schedulersession

* add logits processor

* better context

* update patch

* [Improve] Use 4d input in pytorch poc (#371)

* 4D input, model.eval and llama config

* use auto dtype

* tp wip

* almost

* update logger

* run_check=false

* little optimize

current best

redist w/o dtensor

host mem in que

less rewrite

less code

update model weight

* share attention forward

* fix end

* Support Baichuan (#382)

* add baichuan WIP

* support baichuan

* support baichuan-13b

* fix

* add chat template

* lint

* comments

* fix

* Move `q_seq_info` into `context` (#398)

* move q seq info into context

* remove debugs

* remove debugs

* alibi wip

* add alibi

* reduce logic block (#435)

* add docstring

* add baichuan lint (#445)

* add fill cache back

* support internlm

* fix path of weight index

* Support chatglm2 in pytorch_poc (#360)

* draft support for chatglm2

* debug llama

* gitignore

* update input_id

* better patching

* patch chatglm2 model

* fix after merge

* remove inits

* q_seq_info & remove some debug & orig_self

* remove old unqeuzze inputid

* update patch and model config

* remove debugs and clean codes

* clean codes

* add credit

* add update id / fix dependency

* rename modules (#504)

Co-authored-by: grimoire <yaoqian@pjlab.org.cn>

* optimize fill kv cache (#523)

* optimize fill kv cache

* update internlm

* faster embedding

* fix bias tp

* fix baichuan2

* fix fill kv cache

* fix lint

---------

* Make trust_remote_code as cli argument (#434)

* trust_remote_code_argument

* format

* update tokenizer

* optimize rotary

* wtf

* Support Falcon models (#406)

* move q seq info into context

* falcon aligned

* trust_remote_code_argument

* fix for falcon

* comment out debugs

* comment out debugs

* use position id in context

* remove codes in falcon model

* Revert ""comment out debugs""

This reverts commit ee26a25c36f11c7afdad315b8101ebb5c2c637fd.

* 7b correct

* 1b aligned

* remove debugs

* patch to ignore position ids

* remove debug in alibi, avoid empty inputs

* fix

* rename dir to replace to ""models""

* use position_id and new fill kernel

* remove useless get_prompt func

* fix batch>2

* Refactor scheduler (#551)

* optimize block manager

* scheduler wip

* finish scheduler

* update engine

* profile pytorch poc (#455)

* profile pytorch poc

* update doc and import if need

* arg

* support profile_throughput.py

* reuse pytorch session

* end session

* Support Tensor parallel on Falcon models (#582)

* tp falcon 1b and 7b works

* remove debugs

* update copyright

* add some comments

* remove a debug

* support new hub models

* support 40b

* support 40b model config

* try

* recover

* fix remain len

* Apply rotary kernel (#572)

* apply rotary kernel

* format

* update rmsnorm

* update rms norm

* better unittest

* add docstring

---------

Co-authored-by: grimoire <yaoqian@pjlab.org.cn>

* fix(pytorch_poc): memory cal (#606)

* fix(pytorch_poc): memory cal

* Optimize attention (#597)

* add unittest

* add split k

* add docstring

* fast split k

* optimize load

* manually setup device and stream

* lint

---------

Co-authored-by: grimoire <yaoqian@pjlab.org.cn>

* feat(pytorch_poc): implement ReRoPE (#625)

* fix(pytorch_poc): memory cal

* style(pytorch_poc): lint

* style(.pre-commit-config.yaml): update

* style(pytorch_poc): remove useless

* feat(pytorch_poc): llama2 support rerope

* feat(pytorch_poc): fix long input generate

* feat(lmdeploy): add kernel

* feat(lmdeploy): update

* feat(lmdeploy): add rerope implementation

* fix(lmdeploy/pytorch_poc): apply rotary_emb

* fix(lmdeploy): update

* style(pytorch_poc): format

* style(lmdeploy): fix lint

* style(lmdeploy): typo

* style(pytorch_poc): format

* style(pytorch_poc): format

* fix(pytorch_poc): rms_norm add mask

* style(pytorch_poc/kernels): format rerope

* style(pytorch_poc): format rerope attn function description

* style(lmdeploy/pytorch_poc): format

* style(pytorch_poc): add code ref

* style(pytorch_poc): format rerope attn

* Refactor engine (#623)

* add agent

* optimize postprocess

* optimize decoding fill cache

* add docstring

* logit to cuda

* blocksize 128

* optimize pre/post process

* fix postprocess

* cpu pre/post process

* manually setup stream and device

* remove context

* update model agent

* update max session len

* remove tqdm

* update pre/post process

* inplace kernel

* avoid kv_len computation

* flash decoding with one cache

* remove comment

* add warning when no enough resources

* step if has unfinish

* add request manager

* better fill kv cache

* fix fill kv cache

* optimize prefill attention

* refractor

* refactoring...

* add custom output

* use cache

---------

Co-authored-by: grimoire <yaoqian@pjlab.org.cn>

* [Feature] w8a8 based on pytorch poc (#595)

* refactor smoothquant and support load w8a8 model by from_pretrained

* add w8a8 docs

* add w8a8 en docs

* add convert_to_qmodules function

---------

Co-authored-by: grimoire <streetyao@live.com>

* feat(lmdeploy): add rerope quantization (#718)

* feat(lmdeploy): add rerope quantization

* feat(lmdeploy): fix review

* [Refactor & Doc] Improve w8a8 and add docstring (#768)

* WIP

* improve w8a8 and add doc string

* add docstring

* add docstring

* fix lint

* rename pytorch poc (#764)

* rename pytorch poc

* fix lint

* add docstring

* add docstring

* refactor patch

* add recompute eviction support

* recovery modeling

* add docstring

* Unified paging (#860)

* change 'model_format' to 'qwen' when 'model_name' starts with 'qwen' (#575)

* avoid split chinese characters during decoding (#566)

* add solar chat template (#576)

* robust incremental decode for leading space (#581)

* robust incremental decode for leading space

* speed up lookup as prefix_space_tokens is shorter than no_prefix_space_tokens

* add UT and fix qwen stuff

* update solar chat template (#587)

* Revert ""[Docs] Simplify `build.md` (#370)"" (#586)

This reverts commit 4b5c2bda074eb4ac2e70c3c793fb5ef48f87d9c8.

* Fix crash and remove `sys_instruct` from `chat.py` and `client.py`(#591)

* fix crash

* update profile_generation.py

* format

* use self.bos_id

* remove sys_instruct

* bump version to v0.0.12 (#604)

* Add ""build from docker"" section (#602)

* add build from docker section

* update

* install python package

* update

* update

* update

* Add more user-friendly CLI  (#541)

* add

* import fire in main

* wrap to speed up fire cli

* update

* update docs

* update docs

* fix

* resolve commennts

* resolve confict and add test for cli

* support inference a batch of prompts (#467)

* support inference a batch of prompts

* docstring and assert

* bump version to v0.0.13 (#620)

* Improve api_server and webui usage (#544)

* make IPv6 compatible, safe run for coroutine interrupting

* instance_id -> session_id and fix api_client.py

* update doc

* remove useless faq

* safe ip mapping

* update app.py

* WIP completion

* completion

* update doc

* disable interactive mode for /v1/chat/completions

* docstring

* docstring

* refactor gradio

* update gradio

* udpate

* update doc

* rename

* session_id default -1

* missed two files

* add a APIClient

* add chat func for APIClient

* refine

* add concurrent function

* sequence_start, sequence_end --> interactive_mode

* update doc

* comments

* doc

* better text completion

* remove /v1/embeddings

* comments

* deprecate generate and use /v1/interactive/completions

* /v1/interactive/completion -> /v1/chat/interactive

* embeddings

* rename

* remove wrong arg description

* docstring

* fix

* update cli

* update doc

* strict session_len limit condition

* pass model args to api_server

* fix: gradio gr.Button.update deprecated after 4.0.0 (#637)

* add cli to list the supported model names (#639)

* update

* resolve comment

* Refactor model conversion (#296)

* split deploy.py

* fix get_cuda_tensor

* deploy qwen_awq

* fix lint

* add docstring

* fix

* support baichuan/baichuan-awq

* parameterizing size_per_head

* remove try/except

* limit input model_format

* add quant_path param

* remove old deploy.py

* fix path

* fix transformer layer range when load bins

* fix qwen init

* split & save log

* relative import

* update get_config

* WeightFileMgr -> Reader

* rename

* update

* fix init_layer_id

* rename llama.py -> meta_llama.py, hf.py -> llama.py

* reduce code

* update arg description

* fix meta llama

* manually cleanup meta model params

* [Enchance] internlm message to prompt (#499)

* update turbomind session_len with model.session_len (#634)

* [Fix] Qwen's quantization results are abnormal & Baichuan cannot be quantized (#605)

* fix awq

* adapt new qwen code

* adapt qwen 14b and baichuan2 7b

* add docstring

* add runtime error for qwen

* FIX: fix stop_session func bug (#578)

* FIX: fix stop_session func bug

* keep sequence_end = False

---------

Co-authored-by: honglei.yan <honglei.yan@nio.com>
Co-authored-by: AllentDan <AllentDan@yeah.net>

* Manage session id using random int for gradio local mode (#553)

* Use session id from gradio state

* use a new session id after reset

* rename session id like a state

* update comments

* reformat files

* init session id on block loaded

* use auto increased session id

* remove session id textbox

* apply to api_server and tritonserver

* update docstring

* add lock for safety

---------

Co-authored-by: AllentDan <AllentDan@yeah.net>

* fix benchmark serving computation mistake (#630)

* fix benchmark serving computation mistake

* fix timestamps computations

* remove speed up

* no mp

* mp seems faster?

* remove

* update

* remove

* fix

* update

* update print log

* typo

* print fist token latency only stream==True

* remove renew_session

* update AsyncEngine

* fix tokenizer_info when convert the model (#661)

* Add check env sub command (#654)

* add check env

* update issue template'

* remove some reqs from check env

* resolve comment

* fix Tokenizer load error when the path of the being-converted  model is not writable (#669)

* Add UltraCM and WizardLM chat templates (#599)

* add ultracm eval chat template

* add WizardLM chat template

* use ultrachat template instead of ultracm usecase

* bump version to v0.0.14 (#663)

* Add extra_requires to reduce dependencies (#580)

* update reqs

* update docs

* resolve comments

* upgrade pydantic

* fix rebase

* update doc

* update

* update

* update readme

* update

* add flash-attn

* TurboMind 2 (#590)

* refresh decoder attention kernel

* block-level kv cache

* `BlockManager` & `SequenceManager`

* update

* update

* update

* update

* rename

* GQA support

* fix context length

* GQA dispatch

* kv8

* tune

* async stream cb

* nvtx

* config parsing

* debug

* optimize output cost

* split-k decoding

* minor

* truncate `session_len` by available blocks

* minor

* license

* fix

* dispatch `cp.async`

* fix linking

* fix

* fix deadlock

* guard input length

* correct start offset

* fix prefill chunking

* fix `cache_block_seq_len` param passing

* fix `block_size` fmtstr

* fix output tokens

* fix batch resizing

* fix masking of finished sequences

* add debug util

* free unused block early

* add ntk scaling and logn scaling

* cmake flags

* fix typo

* w4a16 for sm75

* fix msvc build

* fix msvc build

* fix block verification

* fix msvc build

* use `std::shuffle`

* fix lint

* fix lint

* fix lint

* clear incoming buffer

* clear finished requests

* fix batch initialization

* fix typo

* fix typo

* fix comparison

* [Docs] Update Supported Matrix (#679)

* update supported matrix

* change the default shard size when saving quantized weights

* baichuan2 kv8

* update kv8 docs (#681)

* Fix init of batch state (#682)

* fix init of finished buf

* fix `finished_count`

* fix turbomind stream canceling (#686)

* fix

* instance for each forward

* [Fix] Fix load_checkpoint_in_model bug (#690)

* fix load_checkpoint_in_model bug

* fix comments

* fix comments

* fix bugs

* [Doc] Update restful api doc (#662)

* update restful_api.md

* add a hint

* repeat 3 time

* Fix Tokenizer encode (#645)

* same encode with HF

* sequence_start -> add_bos

* complement

* Fix wrong eos_id and bos_id obtained through grpc api (#644)

* Fix wrong eos_id and bos_id obtained through grpc api

* fix according to review comments

* update

* Optimize for throughput (#701)

* tmp

* update

* update

* optimize for throughput

* update

* fix eos

* clean up

* fix serving

* fix indexed copy

* minor

* minor

---------

Co-authored-by: lvhan028 <lvhan_028@163.com>

* Check-in user guide about turbomind config (#680)

* update

* update config guide

* update guide

* upate user guide according to review comments

* Replace mmengine with mmengine-lite (#715)

* Support loading hf model directly (#685)

* turbomind support export model params

* fix overflow

* support turbomind.from_pretrained

* fix tp

* support AutoModel

* support load kv qparams

* update auto_awq

* udpate docstring

* export lmdeploy version

* update doc

* remove download_hf_repo

* LmdeployForCausalLM -> LmdeployForCausalLM

* refactor turbomind.py

* update comment

* add bfloat16 convert back

* support gradio run_locl load hf

* support resuful api server load hf

* add docs

* support loading previous quantized model

* adapt pr 690

* udpate docs

* not export turbomind config when quantize a model

* check model_name when can not get it from config.json

* update readme

* remove model_name in auto_awq

* update

* update

* udpate

* fix build

* absolute import

* Fix cache/output length calculation (#738)

* bump version to v0.1.0a0 (#709)

* [Fix] Skip empty batch (#747)

* [Fix] build docker image failed since `packaging` is missing (#753)

* [Fix] Rollback the data type of input_ids to TYPE_UINT32 in preprocessor's proto (#758)

* Set the default value of `max_context_token_num` 1 (#761)

* rename pytorch poc

* fix lint

* add docstring

* add docstring

* refactor patch

* add recompute eviction support

* fix typo (#769)

* add triton server test and workflow yml (#760)

* add triton server test and workflow yml

* update

* revert changes in dockerfile

* update prompts

* recovery modeling

* fix turbomind build on sm<80 (#754)

* fix

* fix lint

* improvement(build): enable ninja and gold linker (#767)

* feat(build): enable ninja and lld

* fix(.github): add ninja installation

* fix(CI): remove dimsize=256

* fix(CI): add option for generate.sh

* fix(docs): update

* Report first-token-latency and token-latency percentiles (#736)

* update profile scripts

* add top_p, top_k and temperature as input arguments

* fix input_ids

* update profile_throughput

* update profile_restful_api

* update profile_serving

* update

* update

* add progress bar

* remove TODO comments

* update

* remove useless profile_* argument

* remove log level

* change concurrency default value to 64

* update restful_api.md

* update according to review comments

* fix docstring

* convert model with hf repo_id (#774)

* bump version to 0.1.0a1 (#776)

* Update benchmark user guide (#763)

* user guide of benchmark generation

* update benchmark generation guide

* update profiling throughput guide

* update profiling api_server guide

* rename file names

* update profile tis user guide

* update

* fix according to review comments

* update

* update according to review comments

* updaste

* add an example

* update

* add docstring

* add unified paging attention support

* refactor block manager

* do not alloc zero

* Fix early exit condition in attention kernel (#788)

* add chat template for Yi (#779)

* Fix missed arguments when benchmark static inference performance (#787)

* minor fix in the profile scripts and docs

* miss arguments

* typo

* fix lint

* update

* Unify prefill & decode passes (#775)

* Unify prefill and decode passes

* dynamic split-fuse

* refactor

* correct input count calculation

* remove unused

* lint

* lint

* fix msvc build

* fix msvc build

* fix msvc build

* fix msvc build

* fix msvc build

* fix msvc build

* fix msvc build

* fix msvc build

* fix msvc build

* add cuda12.1 build check ci (#782)

* update cuda12.1 build check ci

* use matrix

* auto upload cuda12.1 python pkg to release when create new tag (#784)

* add cuda12-whl-release ci

* enable environment

* test py310-311 windows wheel

* fix py310, py311 setup.py error on windows

* fix lint

* fix extra colon in InternLMChat7B (#796)

* fix local kv head num (#806)

* Report the inference benchmark of models with different size (#794)

* update test scripts for models with different sizes

* update

* only test after tunning gemm

* chmod +x

* fix typo

* benchmark on a100

* fix typo

* fix typo

* per-token latency percentile in profile_throughput

* fix

* fix

* rename

* make the script accept parameters

* minor fix

* indent

* reformat table

* change to 3000

* minor fix

* bump version to v0.1.0a2 (#807)

* fix out of bounds access (#809)

* update scheduler

* optimize request

* Simplify block manager (#812)

* simplify block manager

* fix lint

* set smem size for repetition penalty kernel (#818)

* add mbgemm&mbgemv

* fix recompute, fix mbgmm

---------

Co-authored-by: Lyu Han <lvhan_028@163.com>
Co-authored-by: AllentDan <41138331+AllentDan@users.noreply.github.com>
Co-authored-by: pppppM <67539920+pppppM@users.noreply.github.com>
Co-authored-by: Chen Xin <irexyc@gmail.com>
Co-authored-by: RunningLeon <mnsheng@yeah.net>
Co-authored-by: Yam(长琴) <haoshaochun@gmail.com>
Co-authored-by: liukuikun <24622904+Harold-lkk@users.noreply.github.com>
Co-authored-by: yunzhongyan0 <549713537@qq.com>
Co-authored-by: honglei.yan <honglei.yan@nio.com>
Co-authored-by: AllentDan <AllentDan@yeah.net>
Co-authored-by: aisensiy <aisensiy@163.com>
Co-authored-by: Li Zhang <lzhang329@gmail.com>
Co-authored-by: whcao <41630003+HIT-cwh@users.noreply.github.com>
Co-authored-by: Zaida Zhou <58739961+zhouzaida@users.noreply.github.com>
Co-authored-by: tpoisonooo <khj.application@aliyun.com>
Co-authored-by: Qian Zhao <112053249+C1rN09@users.noreply.github.com>

* [Fix] Adapt to the pyTorch poc branch (#863)

* Adapt to the pyTorch poc branch

* Adapt to the pyTorch poc branch

* fix comments

* update model

* wip

* wrong implementation

* s-lora single gpu

* refactor tp patch

* add tp support

* add tp gather

* recover profile generation

* daemon process

* inplace gather

* hf style

* add assert when input nothing

* find available port

---------

Co-authored-by: grimoire <yaoqian@pjlab.org.cn>
Co-authored-by: WRH <12756472+wangruohui@users.noreply.github.com>
Co-authored-by: AllentDan <AllentDan@yeah.net>
Co-authored-by: AllentDan <41138331+AllentDan@users.noreply.github.com>
Co-authored-by: tpoisonooo <khj.application@aliyun.com>
Co-authored-by: whcao <41630003+HIT-cwh@users.noreply.github.com>
Co-authored-by: Lyu Han <lvhan_028@163.com>
Co-authored-by: pppppM <67539920+pppppM@users.noreply.github.com>
Co-authored-by: Chen Xin <irexyc@gmail.com>
Co-authored-by: RunningLeon <mnsheng@yeah.net>
Co-authored-by: Yam(长琴) <haoshaochun@gmail.com>
Co-authored-by: liukuikun <24622904+Harold-lkk@users.noreply.github.com>
Co-authored-by: yunzhongyan0 <549713537@qq.com>
Co-authored-by: honglei.yan <honglei.yan@nio.com>
Co-authored-by: aisensiy <aisensiy@163.com>
Co-authored-by: Li Zhang <lzhang329@gmail.com>
Co-authored-by: Zaida Zhou <58739961+zhouzaida@users.noreply.github.com>
Co-authored-by: Qian Zhao <112053249+C1rN09@users.noreply.github.com>"
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2024-01-18T16:05:54Z,support left padding and prefix post-processing for models like chatglm
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2024-01-03T18:22:30Z,merge
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-12-24T14:04:00Z,Add --save_references_path
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-11-16T14:29:41Z,better naming convention
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-11-15T18:46:22Z,add defaults to parallel_generations
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-11-14T20:24:24Z,pass intermediate_generations as kwarg
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-11-09T18:44:20Z,add intermediate generations to continue generating from
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-11-09T18:01:58Z,save intermediate code generations
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-11-09T16:06:27Z,save gen and ref per task
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-11-09T14:54:03Z,save intermediate res
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-10-24T15:49:06Z,rename lm_eval => bigcode_eval
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-09-12T12:41:35Z,"Merge pull request #133 from bigcode-project/wizardcoder_codellama_based

Add  WizardCoder models (that are CodeLLama based) evaluation"
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-09-11T08:01:58Z,allow auto for max_memory_per_gpu
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-09-01T08:39:53Z,fix bos token
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-08-24T09:38:39Z,remove parentheses
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-08-24T09:37:35Z,print precision in correct setting
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-08-16T14:05:24Z,Fix str concat
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-08-08T07:42:53Z,Add import & use acc.processes
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-08-08T07:39:25Z,"Use warnings

Co-authored-by: Loubna Ben Allal <44069155+loubnabnl@users.noreply.github.com>"
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-08-07T17:01:38Z,Add warnings
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-08-07T16:09:10Z,Merge main
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-08-07T16:02:27Z,"Apply suggestions from code review

Co-authored-by: Loubna Ben Allal <44069155+loubnabnl@users.noreply.github.com>"
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-07-29T17:48:27Z,Move to try except
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-07-28T20:07:46Z,"Update main.py

Co-authored-by: Loubna Ben Allal <44069155+loubnabnl@users.noreply.github.com>"
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-07-27T12:43:42Z,Allow no pad token
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-07-26T19:54:24Z,Rename mutate_method to prompt
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-07-26T18:30:31Z,Merge main
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-07-10T18:39:45Z,merghe
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-07-10T18:36:13Z,attempt at loading peft model
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-06-23T12:35:55Z,Add limit start
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-06-18T13:05:55Z,Simplify
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-06-18T10:51:50Z,Add
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-06-14T15:28:29Z,add support for 4bit
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-06-14T12:42:48Z,add load_in_8bit arg
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-06-13T08:57:09Z,reformatting
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-06-12T09:29:41Z,further modification
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-06-12T08:54:40Z,Fix encdec; Fix carper none
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-06-09T07:17:45Z,Add max mem
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-06-06T12:15:47Z,Add modeltype
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-06-05T19:48:43Z,Merge
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-05-31T01:35:35Z,"enforce tokenizer padding_side=""right"""
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-05-07T07:03:59Z,Fix kwarg
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-05-07T06:55:51Z,Merge main
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-05-03T07:16:15Z,"add trust_remote_code to tokenizer loading

some tokenizers require running local code hence the need for trust_remote_code argument"
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-04-22T16:34:23Z,reformat code
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-04-22T16:33:03Z,fix save generation path arg
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-04-21T17:14:53Z,Merge branch 'main' into rename-path-arguments
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-04-21T17:11:55Z,add temp and nsamples to metrics file
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-04-21T15:01:50Z,use torch dtype to set precision instead of converting model
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-04-21T11:30:25Z,"Update main.py

Co-authored-by: Loubna Ben Allal <44069155+loubnabnl@users.noreply.github.com>"
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-04-21T11:25:20Z,"Reword

Co-authored-by: Loubna Ben Allal <44069155+loubnabnl@users.noreply.github.com>"
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-04-21T09:08:44Z,update help message in  precision arg
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-04-20T20:37:00Z,remove model from accelerate prepare and add precision
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-04-06T07:29:08Z,Fix dtype
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-04-06T06:53:57Z,Add import
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-04-06T06:49:09Z,A
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-04-06T06:48:05Z,Enable multiple GPUs
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-04-05T21:00:36Z,Enforce right padding
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-04-03T22:05:01Z,Fix race condition
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-04-03T16:22:11Z,Revert the commit.
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-04-03T13:25:16Z,add custom save generations path
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-04-03T13:14:12Z,rename output path for metrics
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-04-03T13:00:02Z,rename the generations_path arg for loading generations
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-04-02T15:05:07Z,Merge branch 'parity' of https://github.com/bigcode-project/bigcode-evaluation-harness into parity
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-04-02T12:28:04Z,"Refactor main.py

Assign the default file location to args.generations_path instead of generations_path."
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-04-02T11:39:39Z,"Update main.py

Fix a little bug in generations_path. Handle the case where the user would like to generate only when the generationed file is already here."
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-04-02T11:32:53Z,"Handle the existing file case when add --generation-only.
Fix a little issue on generation path."
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-03-31T14:36:10Z,Add ref checking
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-03-25T15:16:52Z,Add args to config
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-03-24T22:07:01Z,Fix path
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-03-24T21:50:40Z,Gen path compat
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-03-24T21:14:43Z,Allow gen path
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-03-24T19:18:31Z,Edit compat for HE
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-01-30T11:05:27Z,change format of boolean arguments
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-01-29T16:26:05Z,add trust_remote_code and use_auth_token args
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2023-01-05T11:34:24Z,"Merge pull request #20 from infinitylogesh/few_shot_codexglue_t2t

[WIP] Add CodeXGLUE-text-to-text benchmark for documentation translation"
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2022-12-20T17:08:27Z,fix typos
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2022-12-17T19:51:03Z,Removed redundant condition after refactoring
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2022-12-17T19:40:37Z,Merge branch 'main' into few_shot_codexglue_t2t
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2022-12-13T21:46:50Z,Add revision kwarg
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2022-12-13T11:06:19Z,put truncation=left in tokenizer call instead of encoding
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2022-12-05T01:47:02Z,add citations and reformat code
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2022-12-04T23:24:44Z,change default values of bs and nsamples
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2022-11-28T21:17:36Z,rename num_tasks argument
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2022-11-28T20:54:42Z,remove evaluation_only arg
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2022-11-28T20:50:44Z,import all_tasks firctly and swap args in docstring
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2022-11-27T06:08:09Z,"Added task to evaluation

Added CodexGlue Text - to - text task to evaluation code. Summary of changes
- Added task in `main.py`
- Added task-specifc arguments in `arguments.py`
- Added prompt formatting function in `lm_eval/prompts.py`
- Added generation settings and post-processing in `lm_eval/utils.py`"
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2022-11-02T08:48:06Z,remove unecessary args
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2022-10-28T09:09:42Z,change default model name and max_generation_length
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2022-10-24T13:27:59Z,add postprocess option
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2022-10-20T21:01:44Z,add model name to results of evaluation_only mode
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2022-10-20T16:21:34Z,skip saving results on generation only mode
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2022-10-20T13:03:59Z,update evaluation and generation separation
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2022-10-19T21:45:25Z,separate evaluation and generation
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2022-09-26T08:34:10Z,handle an edge case of number_tasks wrt num_devices
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2022-09-22T10:06:23Z, move max_length_generation arg
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2022-09-18T23:17:48Z,update code-to-text prompts
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2022-09-16T15:51:21Z,add concode benchmark
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2022-09-16T13:06:47Z,add spider benchmark
github.com/bigcode-project/bigcode-evaluation-harness,main.py,2022-09-15T15:23:47Z,add conala 2-shot eval
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/merge_llama2_with_chinese_lora_low_mem.py,2023-12-27T03:18:14Z,"Add 64k-context models (#478)

For full changes, see the latest release.

---------

Co-authored-by: iMountTai <2506700016@qq.com>
Co-authored-by: Xin Yao <35353688+iMountTai@users.noreply.github.com>"
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/merge_llama2_with_chinese_lora_low_mem.py,2023-12-11T03:49:37Z,"Update merge_lora script (#457)

Fix merging bug for 1.3B models."
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/merge_llama2_with_chinese_lora_low_mem.py,2023-08-24T11:16:09Z,update config after  merge llama-2-16k
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/merge_llama2_with_chinese_lora_low_mem.py,2023-08-04T09:31:06Z,Update merge_llama2_with_chinese_lora_low_mem.py
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/merge_llama2_with_chinese_lora_low_mem.py,2023-08-04T09:22:04Z,improve the error message
github.com/ymcui/Chinese-LLaMA-Alpaca-2,scripts/merge_llama2_with_chinese_lora_low_mem.py,2023-07-31T06:12:17Z,add scripts
github.com/jondurbin/airoboros,airoboros/lmoe/api.py,2023-12-04T14:30:17Z,"Fix misc randomization errors, typos, etc. (#33)

* Fix misc randomization errors, typos

* Fix typos

* Remove redundant spaces

* Fix OpenAI API defaults"
github.com/jondurbin/airoboros,airoboros/lmoe/api.py,2023-08-29T12:03:40Z,LMoE updates.
github.com/jondurbin/airoboros,airoboros/lmoe/api.py,2023-08-29T11:39:26Z,LMoE quant/dequant updates.
github.com/jondurbin/airoboros,airoboros/lmoe/api.py,2023-08-28T15:34:43Z,Quantize -> dequantize -> merge adapters.
github.com/jondurbin/airoboros,airoboros/lmoe/api.py,2023-08-27T11:50:12Z,embedding router by default
github.com/jondurbin/airoboros,airoboros/lmoe/api.py,2023-08-26T17:56:17Z,Remove early stopping.
github.com/jondurbin/airoboros,airoboros/lmoe/api.py,2023-08-26T17:10:07Z,Split change.
github.com/jondurbin/airoboros,airoboros/lmoe/api.py,2023-08-26T17:07:59Z,Agent-based routing.
github.com/jondurbin/airoboros,airoboros/lmoe/api.py,2023-08-26T12:15:46Z,"More performance updates, configurable embedding model."
github.com/jondurbin/airoboros,airoboros/lmoe/api.py,2023-08-24T17:25:43Z,"Flash attn server, more stopping_criteria changes, etc."
github.com/jondurbin/airoboros,airoboros/lmoe/api.py,2023-08-23T18:08:57Z,CORS
github.com/jondurbin/airoboros,airoboros/lmoe/api.py,2023-08-23T17:09:38Z,Fix split.
github.com/jondurbin/airoboros,airoboros/lmoe/api.py,2023-08-23T17:03:10Z,Fix = vs ==
github.com/jondurbin/airoboros,airoboros/lmoe/api.py,2023-08-23T16:44:47Z,Cleanup from stopping criteria.
github.com/jondurbin/airoboros,airoboros/lmoe/api.py,2023-08-23T16:38:36Z,More stop testing...
github.com/jondurbin/airoboros,airoboros/lmoe/api.py,2023-08-23T16:33:43Z,offset stop
github.com/jondurbin/airoboros,airoboros/lmoe/api.py,2023-08-23T16:32:43Z,Fixes.
github.com/jondurbin/airoboros,airoboros/lmoe/api.py,2023-08-23T16:01:44Z,More lmoe fixes.
github.com/jondurbin/airoboros,airoboros/lmoe/api.py,2023-08-23T15:58:47Z,Fix accessor.
github.com/jondurbin/airoboros,airoboros/lmoe/api.py,2023-08-23T13:56:52Z,More lmoe updates.
github.com/jondurbin/airoboros,airoboros/lmoe/api.py,2023-08-23T12:58:48Z,"remove flash-attn, for now"
github.com/jondurbin/airoboros,airoboros/lmoe/api.py,2023-08-23T12:26:18Z,"LMoE - HF server, since vllm isn't working"
github.com/jondurbin/airoboros,airoboros/lmoe/api.py,2023-08-22T12:25:31Z,LMoE
github.com/arielnlee/Platypus,merge.py,2023-08-14T20:33:37Z,update pipeline
github.com/arielnlee/Platypus,merge.py,2023-08-05T20:16:02Z,update scripts
github.com/lyuchenyang/Macaw-LLM,run_clm_llms.py,2023-07-01T03:46:41Z,Update run_clm_llms.py
github.com/lyuchenyang/Macaw-LLM,run_clm_llms.py,2023-07-01T03:44:06Z,Update run_clm_llms.py
github.com/lyuchenyang/Macaw-LLM,run_clm_llms.py,2023-05-31T01:57:01Z,Update run_clm_llms.py
github.com/lyuchenyang/Macaw-LLM,run_clm_llms.py,2023-05-31T01:37:22Z,Update run_clm_llms.py
github.com/lyuchenyang/Macaw-LLM,run_clm_llms.py,2023-05-31T01:25:09Z,Update run_clm_llms.py
github.com/lyuchenyang/Macaw-LLM,run_clm_llms.py,2023-05-31T01:21:42Z,Update run_clm_llms.py
github.com/lyuchenyang/Macaw-LLM,run_clm_llms.py,2023-05-30T11:43:18Z,Update run_clm_llms.py
github.com/lyuchenyang/Macaw-LLM,run_clm_llms.py,2023-05-30T07:45:43Z,Update run_clm_llms.py
github.com/lyuchenyang/Macaw-LLM,run_clm_llms.py,2023-05-26T05:39:45Z,Create run_clm_llms.py
github.com/modelscope/swift,swift/tuners/base.py,2024-02-29T04:14:08Z,Support LLaMAPRO and LoRA+ (#472)
github.com/modelscope/swift,swift/tuners/base.py,2024-02-22T13:26:16Z,support peft format (#438)
github.com/modelscope/swift,swift/tuners/base.py,2024-02-19T05:34:37Z,fix zero3 & swift lora (#416)
github.com/modelscope/swift,swift/tuners/base.py,2024-01-23T13:08:27Z,Update orion 14b (#341)
github.com/modelscope/swift,swift/tuners/base.py,2024-01-11T04:33:27Z,Fix the saving behaviour of modules without state dict (#309)
github.com/modelscope/swift,swift/tuners/base.py,2024-01-10T12:57:55Z,Fix bugs (#305)
github.com/modelscope/swift,swift/tuners/base.py,2024-01-09T10:35:20Z,Support studio (#300)
github.com/modelscope/swift,swift/tuners/base.py,2024-01-06T10:03:42Z,fix scedit bug (#290)
github.com/modelscope/swift,swift/tuners/base.py,2024-01-06T08:15:17Z,Fix offload (#288)
github.com/modelscope/swift,swift/tuners/base.py,2024-01-06T06:23:36Z,support ModuleToSave original module offloading (#282)
github.com/modelscope/swift,swift/tuners/base.py,2024-01-05T07:35:50Z,Support offload (#281)
github.com/modelscope/swift,swift/tuners/base.py,2023-12-30T07:53:08Z,update perf (#264)
github.com/modelscope/swift,swift/tuners/base.py,2023-12-27T13:31:27Z,"Feat/scedit (#253)

Co-authored-by: zeyinzi.jzyz <zeyinzi.jzyz@alibaba-inc.com>"
github.com/modelscope/swift,swift/tuners/base.py,2023-12-16T09:13:46Z,Compatible with peft>=0.7.0 (#220)
github.com/modelscope/swift,swift/tuners/base.py,2023-12-06T07:44:13Z,Fix compatible error (#201)
github.com/modelscope/swift,swift/tuners/base.py,2023-12-06T05:41:23Z,Fix model saving in new format (#198)
github.com/modelscope/swift,swift/tuners/base.py,2023-11-29T10:08:45Z,Refine LoRA to peft (#176)
github.com/modelscope/swift,swift/tuners/base.py,2023-11-13T02:21:09Z,Add neftune (#145)
github.com/modelscope/swift,swift/tuners/base.py,2023-09-25T13:54:32Z,Fix dtype error (#92)
github.com/modelscope/swift,swift/tuners/base.py,2023-09-21T01:43:37Z,Replace with loralib and add unload lora interface  (#83)
github.com/modelscope/swift,swift/tuners/base.py,2023-09-20T09:26:22Z,Add sphinx doc builder (#77)
github.com/modelscope/swift,swift/tuners/base.py,2023-09-15T06:43:59Z,"Support restuning/side/lora-embedding/qlora, etc (#48)

## New algorithms:

* Add bnb 4bit & 8bit and autogptq lora support
* Add lora support for torch.nn.Embedding
* Add sidetuner
* Add restuner-bypass
* Fix some bugs

## New features:

* llm_sft support cross-validation with model.generate
* llm_sft support perf recording
* All tuners support activate and deactivate
* Add more unit tests
* Fix some bugs"
github.com/modelscope/swift,swift/tuners/base.py,2023-08-02T02:16:26Z,First commit
github.com/microsoft/Olive,olive/model/handler/pytorch.py,2024-02-29T10:04:27Z,"enable lint f-string check (#973)

## Describe your changes
* Enable linter check for f-string logging

## Checklist before requesting a review
- [ ] Add unit tests for this change.
- [ ] Make sure all tests can pass.
- [ ] Update documents if necessary.
- [ ] Lint and apply fixes to your code by running `lintrunner -a`
- [ ] Is this a user-facing change? If yes, give a description of this
change to be included in the release notes.

## (Optional) Issue link"
github.com/microsoft/Olive,olive/model/handler/pytorch.py,2024-02-26T09:28:31Z,"ignore ruff PTH123 rule (#958)

## Describe your changes

## Checklist before requesting a review
- [ ] Add unit tests for this change.
- [ ] Make sure all tests can pass.
- [ ] Update documents if necessary.
- [ ] Lint and apply fixes to your code by running `lintrunner -a`
- [ ] Is this a user-facing change? If yes, give a description of this
change to be included in the release notes.

## (Optional) Issue link"
github.com/microsoft/Olive,olive/model/handler/pytorch.py,2024-01-26T07:39:32Z,"fix pylint failure caused by wrong ignore-paths pattern (#902)

## Describe your changes

## Checklist before requesting a review
- [ ] Add unit tests for this change.
- [ ] Make sure all tests can pass.
- [ ] Update documents if necessary.
- [x] Lint and apply fixes to your code by running `lintrunner -a`
- [ ] Is this a user-facing change? If yes, give a description of this
change to be included in the release notes.

## (Optional) Issue link"
github.com/microsoft/Olive,olive/model/handler/pytorch.py,2023-12-20T05:53:16Z,"Implement ONNX decoder merge (#828)

## Describe your changes

Implement ONNX decoder merge

* Introducing logic to merge onnnx decoders
* OnnxConversion now returns a merged ONNX model conditionally.

## Checklist before requesting a review
- [ ] Add unit tests for this change.
- [x] Make sure all tests can pass.
- [ ] Update documents if necessary.
- [x] Lint and apply fixes to your code by running `lintrunner -a`
- [ ] Is this a user-facing change? If yes, give a description of this
change to be included in the release notes.

## (Optional) Issue link"
github.com/microsoft/Olive,olive/model/handler/pytorch.py,2023-12-12T02:47:52Z,"Fix onnx conversion of bnb quantized model, Remove static methods from `HfConfigMixin`  (#807)

## Describe your changes
This PR fixes some issues with the conversion of 4bit quantized models
to onnx:
- The adapters are unloaded first so that the correct quantized moduels
can be found
- A new pytorch handler is created. Previously, we were ignoring the
adapters

Changes the static method in `HfConfigMixin` to regular methods since
these were made static to support the above use case. But they are no
longer required to be static. The code is now more consistent.

Test requirements updated to for transformers since lora unit tests fail
otherwise.
 
## Checklist before requesting a review
- [x] Add unit tests for this change.
- [x] Make sure all tests can pass.
- [ ] Update documents if necessary.
- [x] Lint and apply fixes to your code by running `lintrunner -a`
- [ ] Is this a user-facing change? If yes, give a description of this
change to be included in the release notes.

## (Optional) Issue link"
github.com/microsoft/Olive,olive/model/handler/pytorch.py,2023-12-08T10:50:03Z,Add `jsonify_config_keys` to `JsonMixin` to abstract model handler json serialization (#796)
github.com/microsoft/Olive,olive/model/handler/pytorch.py,2023-12-08T07:51:33Z,"refactoring OliveModel and change its name to OliveModelHandler. (#786)

## Describe your changes
* Rename all OliveModel by adding suffix ""Handler"". For example
PyTorchModel -> PyTorchModelHandler.
* Split the logic doesn't belong to model handler into separated mixins.
HfConfigMixin, IoConfigMixin, DummyInputsMixin, etc.
* Merge the CompositeOnnxModel and CompositePyTorchModel into one
CompositeModelHandler.
* Keep using the old ""OliveModel"" types

Basically, all the logic is keeping same except some logic simplify
changes:
* Merge the Composite ONNX and PyTorch model. 
* Simplify the composite model enumerate logic by using a single method.
* Same for original PyTorch model to get the hugging face model
components. In the new implementation, only one get_hf_components is
used to enumerate both name and model.
* As a side effect the merging composite onnx model and pytorch model,
the original CompositePyTorch model conversion logic is in
conversion.py, but with the new changes, the child model get and run for
each child model will be done by the run of olive_pass.py
* As a side effect of providing only single entry point to enumerate the
model components, the insert_beam_search pass is updated by using the
new get model component logic, so does the olive_pass.run.

From the time being, the following logic is not tested
* Distributed models
* Composite PyTorch model

## Checklist before requesting a review
- [x] Add unit tests for this change.
- [ ] Make sure all tests can pass.
- [x] Update documents if necessary.
- [x] Lint and apply fixes to your code by running `lintrunner -a`
- [ ] Is this a user-facing change? If yes, give a description of this
change to be included in the release notes.

## (Optional) Issue link"
github.com/beyondguo/LLM-Tuning,predict.py,2023-10-06T05:28:56Z,"baichuan2, llama2"
github.com/beyondguo/LLM-Tuning,predict.py,2023-09-08T06:12:44Z,internlm no_prompt_loss
github.com/beyondguo/LLM-Tuning,predict.py,2023-08-14T07:46:54Z,predict
github.com/HarderThenHarder/transformers_tasks,LLM/chatglm_finetune/train_multi_gpu.py,2023-08-02T12:16:49Z,add LLMs Trainer
github.com/deep-diver/LLM-As-Chatbot,models/freewilly.py,2023-07-25T10:11:50Z,add more models + gptq mode
github.com/deep-diver/LLM-As-Chatbot,models/freewilly.py,2023-07-23T14:03:48Z,add freewilly2
github.com/Ber666/llm-reasoners,reasoners/lm/hf_model.py,2023-09-07T11:03:32Z,clean code and example
github.com/Ber666/llm-reasoners,reasoners/lm/hf_model.py,2023-09-07T09:42:52Z,make awq optional ; update exllama submodule info
github.com/Ber666/llm-reasoners,reasoners/lm/hf_model.py,2023-08-31T20:29:51Z,Merge branch 'dev-hf' of https://github.com/Ber666/llm-reasoners into dev-hf
github.com/Ber666/llm-reasoners,reasoners/lm/hf_model.py,2023-08-31T20:29:45Z,revert awq error
github.com/Ber666/llm-reasoners,reasoners/lm/hf_model.py,2023-08-31T20:27:55Z,Merge branch 'main' into dev-hf
github.com/Ber666/llm-reasoners,reasoners/lm/hf_model.py,2023-08-31T03:26:21Z,awq
github.com/Ber666/llm-reasoners,reasoners/lm/hf_model.py,2023-08-31T03:24:08Z,awq revise
github.com/Ber666/llm-reasoners,reasoners/lm/hf_model.py,2023-08-30T11:22:52Z,update exllama and README
github.com/Ber666/llm-reasoners,reasoners/lm/hf_model.py,2023-08-23T19:33:24Z,fix the dependecy and setup and req.txt
github.com/Ber666/llm-reasoners,reasoners/lm/hf_model.py,2023-08-18T23:27:48Z,add docs
github.com/Ber666/llm-reasoners,reasoners/lm/hf_model.py,2023-08-18T23:20:57Z,"works well for hf(bnb,awq),exllama on 30b and 70b"
github.com/Ber666/llm-reasoners,reasoners/lm/hf_model.py,2023-08-16T19:51:42Z,blocksworld run
github.com/Ber666/llm-reasoners,reasoners/lm/hf_model.py,2023-07-14T11:25:44Z,update package name
github.com/InternLM/xtuner,xtuner/tools/model_converters/merge.py,2024-01-17T02:47:15Z,"[Fix] Add `trust_remote_code=True` for AutoModel  (#328)

update"
github.com/InternLM/xtuner,xtuner/tools/model_converters/merge.py,2023-12-26T09:39:01Z,"[Feature] Support LLaVA (#196)

* v1

* add load_image

* update cfg image url

* del fig

* update

* temp

* update convert

* update chat_mm

* add exclude_frozen_parameters for deepspeed

* update chat

* update xtuner help msg

* fix bugs

* revert bf16 deepspeed

* fix bugs

* add visual_select_layer for chat

* improve pth_to_hf

* rename projecter_pth to pretrained_pth

* temp

* update requirements

* add cfgs

* update

* fix pre-commit

* optim chat

* optim chat

* Delete xtuner/model/unused.py

* move dispatch to a deeper folder

* add projector

* update

* del model/projector

* fix bugs

* add docs

* update

* update

* update

* update

* enhance resume for map_fn

* update import

* add llava_internlm_chat_7b_clip_vit_large_p14

* update dispatch

* update dispatch

* add link

* update max_length

* update max_length

* update hyp

* align

* move yi flash attn

* fix pre-commit

* update deepspeed requirements

* add mmbench script

* install openpyxl

* add entry_point for mmbench

* save args

* update mmbench

* update max_length

* add llama2 qlora

* update mmbench

* fix mmbench bugs

* use osp instead of os.path

* refactor pth_to_hf

* update chat and mmbench to support --llava

* align to chat

* update entry_point

* add vicuna template

* add vicuna_7b_v15

* fix pre-commit

* add vicuna_7b_v1.5 qlora

* skip_special_tokens for decode text

* remove do_sample

* add warmup

* fix pre-commit

* Update dataset_prepare.md

* Update dataset_prepare.md

* Add KEEP_STSTEM for template

* remove

* fix vicuna template

* clean cfgs

* add cfgs

* fix pre-commit

* add --language for mmbench

* fix bugs

* fix pretrain bug

* support visual_encoder lora

* fix bugs

* add paramwise_cfg

* remove print_peft_model_trainable_parameters

* fix bugs

* add paramwise_cfg for DeepSpeedOptimWrapper

* fix engine deepspeed paramwise_cfg bug

* fix encode_fn bug

* fix

* fix pad_image_to_square bugs

* Add space for system to avoid mismatch of 'USER' token

* revert to adding bos_token at each conv

* revert for paramwise_cfg

* better cfgs?

* fix import bug

* fix import bug

* pretrain align

* update prepare_inputs_labels_for_multimodal

* 1792

* support length_grouped_samplers

* 1792

* remove KEEP_SYSTEM

* remove system in cfg

* update 336 cfg

* add torch_dtype for mmbench and chat

* group 50

* quant for pretrain

* update cfgs

* refactor cfgs

* add length for concat dataset

* update requirements

* fix typo

* add template for internlm pretrain

* no zh

* remove 20b cfgs

* fix pre-commit

* revert invalid input

* rename

* Update README.md

* Update README_zh-CN.md

* fix pre-commit

* remove llava_zh from docs

* qlora 512

* rename llava map_fn

* update cfgs

* update model urls

* add docs link

* add llava docs

* update docs

* update urls

* add citation

* fix README

* move

* update

* vicuna pretrain with prompt

* rename

* add results

* fix pre-commit

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* update

* Update README.md

* Update README_zh-CN.md

* Update README_zh.md

* Update README_zh.md

* Update README.md

* Update README_zh.md

* Update README.md

* Update README.md

* fix typo

* fix

* Update README.md

* Update README_zh-CN.md

* rename

* auto cn_string

* fix pre-commit

* rename

* remove language

* add VLMEvalKit

* rename VLLM to VLM

* add the download links of MMBench

* update

* update readme

* update

* update

* update merge

* fix cfg bug

* Update README.md

* Update README_zh.md

* update

* fix

* update requirements

* Update runtime.txt

* Update runtime.txt

* Update runtime.txt

* Update README.md

* Update README.md

* Update README_zh.md

* fix pre-commit

* fix

* update mmbench prompt

* fix bugs

* fix bugs

* update docs

* update

* update

* Update README.md"
github.com/InternLM/xtuner,xtuner/tools/model_converters/merge.py,2023-11-14T12:48:02Z,"[Feature] Support ChatGLM3-6B (#222)

* add chatglm cfgs

* add chatglm3 template

* fix bos_token_id bug

* add encode_special_tokens

* update readme"
github.com/InternLM/xtuner,xtuner/tools/model_converters/merge.py,2023-09-27T06:48:07Z,"[Fix] Add `--offload-folder` for merge and chat (#140)

add `--offload-folder` for merge and chat"
github.com/InternLM/xtuner,xtuner/tools/model_converters/merge.py,2023-09-22T05:04:53Z,"[Fix] Fix CPU OOM during the merge step (#133)

fix"
github.com/InternLM/xtuner,xtuner/tools/model_converters/merge.py,2023-09-05T08:08:28Z,"[Improve] Redesign convert tools (#96)

* refactor tools

* modify entry_point

* modify docs

* update docs

* fix

* fix

* Update README.md

* Update README.md

* Update README.md

* Update README_zh-CN.md

* fix pre-commit

* rename converter

* update pth2hf

* rename pth2hf to pth_to_hf

* add fp32 for pth_to_hf

* Update README.md

* Update README_zh-CN.md

* Update README_zh-CN.md

* Update README.md

* Update README_zh-CN.md

* Update README_zh-CN.md

* Update README.md

* Update README.md

* Update README_zh-CN.md

* fix pre-commit"
github.com/haonan-li/CMMLU,src/glm.py,2023-06-16T12:48:28Z,init
github.com/predibase/lorax,server/lorax_server/server.py,2024-02-28T01:00:29Z,Generate to `max_total_tokens` during warmup (#286)
github.com/predibase/lorax,server/lorax_server/server.py,2024-02-17T06:15:54Z,Fix concatenate for flash batch (#254)
github.com/predibase/lorax,server/lorax_server/server.py,2024-02-12T23:38:08Z,"Added Outlines logits processor for JSON schema validation (#224)

Co-authored-by: Jeffrey Tang <jeff@predibase.com>"
github.com/predibase/lorax,server/lorax_server/server.py,2024-02-01T05:39:34Z,"Merge multiple LoRA adapters per request (linear, TIES, DARE) (#212)"
github.com/predibase/lorax,server/lorax_server/server.py,2024-01-17T20:45:09Z,Added pbase adapter_source and expose api_token in client (#181)
github.com/predibase/lorax,server/lorax_server/server.py,2024-01-10T18:35:46Z,OpenAI v1 Chat Completions API (#171)
github.com/predibase/lorax,server/lorax_server/server.py,2024-01-04T16:40:08Z,CUDA graph compilation (#154)
github.com/predibase/lorax,server/lorax_server/server.py,2023-12-14T23:55:51Z,Add predibase as a source for adapters (#125)
github.com/predibase/lorax,server/lorax_server/server.py,2023-12-05T20:48:43Z,fix gptq fp16 inference (#104)
github.com/predibase/lorax,server/lorax_server/server.py,2023-12-01T17:26:29Z,doing some documentation stuff (#91)
github.com/predibase/lorax,server/lorax_server/server.py,2023-11-16T01:20:58Z,Rename tgi and text-generation to lorax in rust (#19)
github.com/predibase/lorax,server/lorax_server/server.py,2023-11-15T22:41:32Z,Rename to lorax (#16)
github.com/AetherCortex/Llama-X,src/generate.py,2023-03-30T11:58:42Z,Add files via upload
github.com/deep-diver/LLM-As-Chatbot,models/llama_rlhf.py,2023-07-25T10:11:50Z,add more models + gptq mode
github.com/deep-diver/LLM-As-Chatbot,models/llama_rlhf.py,2023-07-01T17:56:43Z,add local files only option
github.com/deep-diver/LLM-As-Chatbot,models/llama_rlhf.py,2023-06-08T01:59:11Z,exp w/ camel and sam
github.com/deep-diver/LLM-As-Chatbot,models/llama_rlhf.py,2023-06-04T14:50:44Z,disable force download & example view
github.com/deep-diver/LLM-As-Chatbot,models/llama_rlhf.py,2023-04-07T05:57:29Z,remaining works flushed
github.com/shibing624/MedicalGPT,merge_peft_adapter.py,2023-11-16T06:16:27Z,update merge for reward.
github.com/shibing624/MedicalGPT,merge_peft_adapter.py,2023-11-13T02:32:40Z,update merge peft adapter.
github.com/shibing624/MedicalGPT,merge_peft_adapter.py,2023-11-10T15:38:29Z,update merge peft adapter.
github.com/shibing624/MedicalGPT,merge_peft_adapter.py,2023-11-10T15:37:47Z,update merge peft adapter.
github.com/shibing624/MedicalGPT,merge_peft_adapter.py,2023-11-10T15:36:30Z,update merge peft adapter.
github.com/shibing624/MedicalGPT,merge_peft_adapter.py,2023-11-10T15:30:14Z,update merge peft adapter.
github.com/shibing624/MedicalGPT,merge_peft_adapter.py,2023-11-05T06:27:51Z,update merge save model.
github.com/shibing624/MedicalGPT,merge_peft_adapter.py,2023-11-05T06:24:43Z,update merge tokenizer.
github.com/shibing624/MedicalGPT,merge_peft_adapter.py,2023-11-03T04:40:10Z,update merge adapter.
github.com/shibing624/MedicalGPT,merge_peft_adapter.py,2023-08-01T10:22:31Z,update reward num labels.
github.com/shibing624/MedicalGPT,merge_peft_adapter.py,2023-07-21T11:09:37Z,update multi-round demo.
github.com/shibing624/MedicalGPT,merge_peft_adapter.py,2023-07-14T02:08:07Z,update merged model
github.com/shibing624/MedicalGPT,merge_peft_adapter.py,2023-07-12T13:03:36Z,update merge script
github.com/shibing624/MedicalGPT,merge_peft_adapter.py,2023-06-16T02:57:11Z,update auto model.
github.com/shibing624/MedicalGPT,merge_peft_adapter.py,2023-06-15T10:17:15Z,support baichuan-7b.
github.com/shibing624/MedicalGPT,merge_peft_adapter.py,2023-06-14T09:34:37Z,remove folder.
github.com/michael-wzhu/Chinese-LlaMA2,peft/peft_model.py,2023-07-19T06:57:59Z,2023/07/19 add further pretraining code
github.com/haonan-li/CMMLU,src/qwen.py,2023-08-13T11:55:12Z,add Qwen-7B script and results
github.com/shibing624/MedicalGPT,fastapi_server_demo.py,2024-01-10T02:49:34Z,update fastapi demo.
github.com/shibing624/MedicalGPT,fastapi_server_demo.py,2023-11-23T13:31:46Z,update fastapi.
github.com/shibing624/MedicalGPT,fastapi_server_demo.py,2023-11-20T08:36:52Z,update fastapi.
github.com/shibing624/MedicalGPT,fastapi_server_demo.py,2023-11-15T07:05:20Z,update infer use greedy decoding.
github.com/shibing624/MedicalGPT,fastapi_server_demo.py,2023-10-23T06:57:54Z,update batch infer.
github.com/shibing624/MedicalGPT,fastapi_server_demo.py,2023-10-19T06:37:37Z,add fastapi server api demo.
github.com/shibing624/MedicalGPT,fastapi_server_demo.py,2023-10-19T06:36:33Z,add fastapi server api demo.
github.com/AGI-Edgerunners/LLM-Adapters,commonsense_evaluate.py,2023-05-16T16:26:36Z,update commonsense_evaluate.py
github.com/27182812/ChatGLM-LLaMA-chinese-insturct,generate_llama1.py,2023-03-28T03:36:48Z,add llama
github.com/Clouditera/secgpt,train_dpo.py,2023-12-07T04:08:07Z,dpo ref模型加入
github.com/Clouditera/secgpt,train_dpo.py,2023-12-07T02:43:41Z,训练脚本小优化
github.com/Clouditera/secgpt,train_dpo.py,2023-12-07T02:33:25Z,开源DPO训练脚本
github.com/Clouditera/secgpt,train_dpo.py,2023-12-06T07:18:45Z,开源DPO训练脚本
github.com/AGI-Edgerunners/LLM-Adapters,export_hf_checkpoint.py,2023-03-29T13:37:38Z,initial upload
github.com/jquesnelle/yarn,eval/model_loader.py,2023-11-02T15:33:05Z,mistral and v2 of paper
github.com/jquesnelle/yarn,eval/model_loader.py,2023-08-31T17:53:20Z,publish YaRN
github.com/haonan-li/CMMLU,src/chatglm.py,2023-07-28T16:24:52Z,update ABCD token mapping
github.com/haonan-li/CMMLU,src/chatglm.py,2023-06-28T11:13:12Z,Update ChatGLM2 results
github.com/haonan-li/CMMLU,src/chatglm.py,2023-06-16T12:48:28Z,init
github.com/zjunlp/KnowLM,tools/weight_diff.py,2023-08-06T03:17:08Z,Update weight_diff.py
github.com/zjunlp/KnowLM,tools/weight_diff.py,2023-07-04T09:39:49Z,Update weight_diff.py
github.com/zjunlp/KnowLM,tools/weight_diff.py,2023-06-19T08:28:04Z,"Adding merge function on weight_diff.py

Providing a merge lora function. This will output a hf format model which can be used for further lora stf or full-finetuning"
github.com/zjunlp/KnowLM,tools/weight_diff.py,2023-06-17T11:38:03Z,Update weight_diff.py
github.com/zjunlp/KnowLM,tools/weight_diff.py,2023-06-17T11:30:47Z,Update weight_diff.py
github.com/zjunlp/KnowLM,tools/weight_diff.py,2023-05-24T15:16:00Z,Add files via upload
github.com/dvlab-research/LISA,model/llava/model/builder.py,2023-08-23T13:49:06Z,Refactor the code to support hf model module format & support grefcoco dataset
github.com/SCIR-HI/Huatuo-Llama-Med-Chinese,export_state_dict_checkpoint.py,2023-04-01T09:37:49Z,init code
github.com/yeyupiaoling/Whisper-Finetune,merge_lora.py,2023-11-16T11:36:55Z,优化hf推理
github.com/yeyupiaoling/Whisper-Finetune,merge_lora.py,2023-09-28T14:51:22Z,更文档和修复评估
github.com/yeyupiaoling/Whisper-Finetune,merge_lora.py,2023-07-02T05:14:38Z,更新文档
github.com/yeyupiaoling/Whisper-Finetune,merge_lora.py,2023-06-01T15:06:29Z,修改默认参数
github.com/yeyupiaoling/Whisper-Finetune,merge_lora.py,2023-05-29T14:41:39Z,训练时不适合预测评估
github.com/yeyupiaoling/Whisper-Finetune,merge_lora.py,2023-05-20T05:36:50Z,增加指定多语言
github.com/yeyupiaoling/Whisper-Finetune,merge_lora.py,2023-05-12T09:58:24Z,修复bool类型参数无法输入，增加训练数据时长控制
github.com/yeyupiaoling/Whisper-Finetune,merge_lora.py,2023-05-11T14:12:32Z,修复多卡训练和没有保存tokenizer.json问题
github.com/yeyupiaoling/Whisper-Finetune,merge_lora.py,2023-05-11T13:09:22Z,修复8bit训练，优化评估方式和其他代码
github.com/yeyupiaoling/Whisper-Finetune,merge_lora.py,2023-04-26T10:13:52Z,增加指定语言
github.com/yeyupiaoling/Whisper-Finetune,merge_lora.py,2023-04-22T07:08:40Z,first commit
github.com/billxbf/ReWOO,alpaca/lora.py,2023-05-19T20:52:16Z,initial commit
github.com/InternLM/lmdeploy,lmdeploy/pytorch/engine/model_agent.py,2024-03-03T14:42:41Z,"Async torch engine (#1206)

* add multinomial sampling kernel

* add multinomial ut

* update sampling

* fix block offsets

* solve conflict

* not copy bw

* async engine

* add threadsafe

* fix ut

* recovery doc

* fix run_until_complete

* fix chat

* fix chat

* fix async-engine"
github.com/InternLM/lmdeploy,lmdeploy/pytorch/engine/model_agent.py,2024-03-01T10:50:15Z,"optmize chatglm3 in pytorch engine(#1215)

* optmize chatglm3

* update docs"
github.com/InternLM/lmdeploy,lmdeploy/pytorch/engine/model_agent.py,2024-02-28T11:15:44Z,Fix all devices occupation when applying tp to torch engine by updating device map (#1172)
github.com/InternLM/lmdeploy,lmdeploy/pytorch/engine/model_agent.py,2024-02-23T03:12:05Z,"Support mistral and sliding window attention (#1075)

* support yi

* update docs

* add kernel

* fix seq

* add win block manager

* finish mistral

* fix drop block

* update docs

* fix ut

* fix for transformers 4.37.1

* update docs

* mistral template

* readme-zh-cn

* remove print

* remove div up

---------

Co-authored-by: grimoire <yaoqian@pjlab.org.cn>"
github.com/InternLM/lmdeploy,lmdeploy/pytorch/engine/model_agent.py,2024-02-20T14:01:00Z,"Support torch cache_max_entry_count (#1166)

* max entry only

* update cli

---------
Co-authored-by: RunningLeon <mnsheng@yeah.net>"
github.com/InternLM/lmdeploy,lmdeploy/pytorch/engine/model_agent.py,2024-01-29T02:26:28Z,"Fix modelconfig in pytorch engine, support YI. (#1052)

* support yi

* update docs

---------"
github.com/InternLM/lmdeploy,lmdeploy/pytorch/engine/model_agent.py,2024-01-25T06:50:03Z,"add alignment tools (#1004)

* add align tools

* fix doc"
github.com/InternLM/lmdeploy,lmdeploy/pytorch/engine/model_agent.py,2024-01-25T05:10:31Z,"fix tp mem usage of PytorchEngine (#987)

* fix tp mem usage

* fix

* check arch and auto_map"
github.com/InternLM/lmdeploy,lmdeploy/pytorch/engine/model_agent.py,2024-01-22T04:07:43Z,"fix TorchEngine stuck when benchmarking with `tp>1` (#942)

* fix benchmark tp

* slient warning

* fix profile batch

* support torch2.0

* fix dtensor

* fix get type dtype"
github.com/InternLM/lmdeploy,lmdeploy/pytorch/engine/model_agent.py,2024-01-16T16:51:45Z,"Support internlm2 (#963)

* support internlm2

* update

* update chat template

* add internlm2-chat-7b chat template

* pass model_cfg to model reader

* use config.ini as default if not pass engine config

* Fix auto-awq and smooth quant

* support internlm2-4bit

* rollback cmakelist

* Add long context docs

* support internlm2 pytorch

* rename args

* add doc links

* fix doc

* update doc

* update long context guide (#970)

* update long context inference guide

---------

Co-authored-by: irexyc <irexyc@gmail.com>
Co-authored-by: 曹巍瀚 <caoweihan@pjlab.org.cn>
Co-authored-by: grimoire <streetyao@live.com>"
github.com/InternLM/lmdeploy,lmdeploy/pytorch/engine/model_agent.py,2024-01-16T10:22:55Z,support mlp s-lora (#957)
github.com/InternLM/lmdeploy,lmdeploy/pytorch/engine/model_agent.py,2024-01-09T07:33:41Z,"S-LoRA support (#894)

* WIP

* cache engine wip

* finish cache engine

* fix cache and scheduler

* add paged attention

* step and stop

* add infer

* add request process

* fix end

* request without schedulersession

* add logits processor

* better context

* update patch

* [Improve] Use 4d input in pytorch poc (#371)

* 4D input, model.eval and llama config

* use auto dtype

* tp wip

* almost

* update logger

* run_check=false

* little optimize

current best

redist w/o dtensor

host mem in que

less rewrite

less code

update model weight

* share attention forward

* fix end

* Support Baichuan (#382)

* add baichuan WIP

* support baichuan

* support baichuan-13b

* fix

* add chat template

* lint

* comments

* fix

* Move `q_seq_info` into `context` (#398)

* move q seq info into context

* remove debugs

* remove debugs

* alibi wip

* add alibi

* reduce logic block (#435)

* add docstring

* add baichuan lint (#445)

* add fill cache back

* support internlm

* fix path of weight index

* Support chatglm2 in pytorch_poc (#360)

* draft support for chatglm2

* debug llama

* gitignore

* update input_id

* better patching

* patch chatglm2 model

* fix after merge

* remove inits

* q_seq_info & remove some debug & orig_self

* remove old unqeuzze inputid

* update patch and model config

* remove debugs and clean codes

* clean codes

* add credit

* add update id / fix dependency

* rename modules (#504)

Co-authored-by: grimoire <yaoqian@pjlab.org.cn>

* optimize fill kv cache (#523)

* optimize fill kv cache

* update internlm

* faster embedding

* fix bias tp

* fix baichuan2

* fix fill kv cache

* fix lint

---------

* Make trust_remote_code as cli argument (#434)

* trust_remote_code_argument

* format

* update tokenizer

* optimize rotary

* wtf

* Support Falcon models (#406)

* move q seq info into context

* falcon aligned

* trust_remote_code_argument

* fix for falcon

* comment out debugs

* comment out debugs

* use position id in context

* remove codes in falcon model

* Revert ""comment out debugs""

This reverts commit ee26a25c36f11c7afdad315b8101ebb5c2c637fd.

* 7b correct

* 1b aligned

* remove debugs

* patch to ignore position ids

* remove debug in alibi, avoid empty inputs

* fix

* rename dir to replace to ""models""

* use position_id and new fill kernel

* remove useless get_prompt func

* fix batch>2

* Refactor scheduler (#551)

* optimize block manager

* scheduler wip

* finish scheduler

* update engine

* profile pytorch poc (#455)

* profile pytorch poc

* update doc and import if need

* arg

* support profile_throughput.py

* reuse pytorch session

* end session

* Support Tensor parallel on Falcon models (#582)

* tp falcon 1b and 7b works

* remove debugs

* update copyright

* add some comments

* remove a debug

* support new hub models

* support 40b

* support 40b model config

* try

* recover

* fix remain len

* Apply rotary kernel (#572)

* apply rotary kernel

* format

* update rmsnorm

* update rms norm

* better unittest

* add docstring

---------

Co-authored-by: grimoire <yaoqian@pjlab.org.cn>

* fix(pytorch_poc): memory cal (#606)

* fix(pytorch_poc): memory cal

* Optimize attention (#597)

* add unittest

* add split k

* add docstring

* fast split k

* optimize load

* manually setup device and stream

* lint

---------

Co-authored-by: grimoire <yaoqian@pjlab.org.cn>

* feat(pytorch_poc): implement ReRoPE (#625)

* fix(pytorch_poc): memory cal

* style(pytorch_poc): lint

* style(.pre-commit-config.yaml): update

* style(pytorch_poc): remove useless

* feat(pytorch_poc): llama2 support rerope

* feat(pytorch_poc): fix long input generate

* feat(lmdeploy): add kernel

* feat(lmdeploy): update

* feat(lmdeploy): add rerope implementation

* fix(lmdeploy/pytorch_poc): apply rotary_emb

* fix(lmdeploy): update

* style(pytorch_poc): format

* style(lmdeploy): fix lint

* style(lmdeploy): typo

* style(pytorch_poc): format

* style(pytorch_poc): format

* fix(pytorch_poc): rms_norm add mask

* style(pytorch_poc/kernels): format rerope

* style(pytorch_poc): format rerope attn function description

* style(lmdeploy/pytorch_poc): format

* style(pytorch_poc): add code ref

* style(pytorch_poc): format rerope attn

* Refactor engine (#623)

* add agent

* optimize postprocess

* optimize decoding fill cache

* add docstring

* logit to cuda

* blocksize 128

* optimize pre/post process

* fix postprocess

* cpu pre/post process

* manually setup stream and device

* remove context

* update model agent

* update max session len

* remove tqdm

* update pre/post process

* inplace kernel

* avoid kv_len computation

* flash decoding with one cache

* remove comment

* add warning when no enough resources

* step if has unfinish

* add request manager

* better fill kv cache

* fix fill kv cache

* optimize prefill attention

* refractor

* refactoring...

* add custom output

* use cache

---------

Co-authored-by: grimoire <yaoqian@pjlab.org.cn>

* [Feature] w8a8 based on pytorch poc (#595)

* refactor smoothquant and support load w8a8 model by from_pretrained

* add w8a8 docs

* add w8a8 en docs

* add convert_to_qmodules function

---------

Co-authored-by: grimoire <streetyao@live.com>

* feat(lmdeploy): add rerope quantization (#718)

* feat(lmdeploy): add rerope quantization

* feat(lmdeploy): fix review

* [Refactor & Doc] Improve w8a8 and add docstring (#768)

* WIP

* improve w8a8 and add doc string

* add docstring

* add docstring

* fix lint

* rename pytorch poc (#764)

* rename pytorch poc

* fix lint

* add docstring

* add docstring

* refactor patch

* add recompute eviction support

* recovery modeling

* add docstring

* Unified paging (#860)

* change 'model_format' to 'qwen' when 'model_name' starts with 'qwen' (#575)

* avoid split chinese characters during decoding (#566)

* add solar chat template (#576)

* robust incremental decode for leading space (#581)

* robust incremental decode for leading space

* speed up lookup as prefix_space_tokens is shorter than no_prefix_space_tokens

* add UT and fix qwen stuff

* update solar chat template (#587)

* Revert ""[Docs] Simplify `build.md` (#370)"" (#586)

This reverts commit 4b5c2bda074eb4ac2e70c3c793fb5ef48f87d9c8.

* Fix crash and remove `sys_instruct` from `chat.py` and `client.py`(#591)

* fix crash

* update profile_generation.py

* format

* use self.bos_id

* remove sys_instruct

* bump version to v0.0.12 (#604)

* Add ""build from docker"" section (#602)

* add build from docker section

* update

* install python package

* update

* update

* update

* Add more user-friendly CLI  (#541)

* add

* import fire in main

* wrap to speed up fire cli

* update

* update docs

* update docs

* fix

* resolve commennts

* resolve confict and add test for cli

* support inference a batch of prompts (#467)

* support inference a batch of prompts

* docstring and assert

* bump version to v0.0.13 (#620)

* Improve api_server and webui usage (#544)

* make IPv6 compatible, safe run for coroutine interrupting

* instance_id -> session_id and fix api_client.py

* update doc

* remove useless faq

* safe ip mapping

* update app.py

* WIP completion

* completion

* update doc

* disable interactive mode for /v1/chat/completions

* docstring

* docstring

* refactor gradio

* update gradio

* udpate

* update doc

* rename

* session_id default -1

* missed two files

* add a APIClient

* add chat func for APIClient

* refine

* add concurrent function

* sequence_start, sequence_end --> interactive_mode

* update doc

* comments

* doc

* better text completion

* remove /v1/embeddings

* comments

* deprecate generate and use /v1/interactive/completions

* /v1/interactive/completion -> /v1/chat/interactive

* embeddings

* rename

* remove wrong arg description

* docstring

* fix

* update cli

* update doc

* strict session_len limit condition

* pass model args to api_server

* fix: gradio gr.Button.update deprecated after 4.0.0 (#637)

* add cli to list the supported model names (#639)

* update

* resolve comment

* Refactor model conversion (#296)

* split deploy.py

* fix get_cuda_tensor

* deploy qwen_awq

* fix lint

* add docstring

* fix

* support baichuan/baichuan-awq

* parameterizing size_per_head

* remove try/except

* limit input model_format

* add quant_path param

* remove old deploy.py

* fix path

* fix transformer layer range when load bins

* fix qwen init

* split & save log

* relative import

* update get_config

* WeightFileMgr -> Reader

* rename

* update

* fix init_layer_id

* rename llama.py -> meta_llama.py, hf.py -> llama.py

* reduce code

* update arg description

* fix meta llama

* manually cleanup meta model params

* [Enchance] internlm message to prompt (#499)

* update turbomind session_len with model.session_len (#634)

* [Fix] Qwen's quantization results are abnormal & Baichuan cannot be quantized (#605)

* fix awq

* adapt new qwen code

* adapt qwen 14b and baichuan2 7b

* add docstring

* add runtime error for qwen

* FIX: fix stop_session func bug (#578)

* FIX: fix stop_session func bug

* keep sequence_end = False

---------

Co-authored-by: honglei.yan <honglei.yan@nio.com>
Co-authored-by: AllentDan <AllentDan@yeah.net>

* Manage session id using random int for gradio local mode (#553)

* Use session id from gradio state

* use a new session id after reset

* rename session id like a state

* update comments

* reformat files

* init session id on block loaded

* use auto increased session id

* remove session id textbox

* apply to api_server and tritonserver

* update docstring

* add lock for safety

---------

Co-authored-by: AllentDan <AllentDan@yeah.net>

* fix benchmark serving computation mistake (#630)

* fix benchmark serving computation mistake

* fix timestamps computations

* remove speed up

* no mp

* mp seems faster?

* remove

* update

* remove

* fix

* update

* update print log

* typo

* print fist token latency only stream==True

* remove renew_session

* update AsyncEngine

* fix tokenizer_info when convert the model (#661)

* Add check env sub command (#654)

* add check env

* update issue template'

* remove some reqs from check env

* resolve comment

* fix Tokenizer load error when the path of the being-converted  model is not writable (#669)

* Add UltraCM and WizardLM chat templates (#599)

* add ultracm eval chat template

* add WizardLM chat template

* use ultrachat template instead of ultracm usecase

* bump version to v0.0.14 (#663)

* Add extra_requires to reduce dependencies (#580)

* update reqs

* update docs

* resolve comments

* upgrade pydantic

* fix rebase

* update doc

* update

* update

* update readme

* update

* add flash-attn

* TurboMind 2 (#590)

* refresh decoder attention kernel

* block-level kv cache

* `BlockManager` & `SequenceManager`

* update

* update

* update

* update

* rename

* GQA support

* fix context length

* GQA dispatch

* kv8

* tune

* async stream cb

* nvtx

* config parsing

* debug

* optimize output cost

* split-k decoding

* minor

* truncate `session_len` by available blocks

* minor

* license

* fix

* dispatch `cp.async`

* fix linking

* fix

* fix deadlock

* guard input length

* correct start offset

* fix prefill chunking

* fix `cache_block_seq_len` param passing

* fix `block_size` fmtstr

* fix output tokens

* fix batch resizing

* fix masking of finished sequences

* add debug util

* free unused block early

* add ntk scaling and logn scaling

* cmake flags

* fix typo

* w4a16 for sm75

* fix msvc build

* fix msvc build

* fix block verification

* fix msvc build

* use `std::shuffle`

* fix lint

* fix lint

* fix lint

* clear incoming buffer

* clear finished requests

* fix batch initialization

* fix typo

* fix typo

* fix comparison

* [Docs] Update Supported Matrix (#679)

* update supported matrix

* change the default shard size when saving quantized weights

* baichuan2 kv8

* update kv8 docs (#681)

* Fix init of batch state (#682)

* fix init of finished buf

* fix `finished_count`

* fix turbomind stream canceling (#686)

* fix

* instance for each forward

* [Fix] Fix load_checkpoint_in_model bug (#690)

* fix load_checkpoint_in_model bug

* fix comments

* fix comments

* fix bugs

* [Doc] Update restful api doc (#662)

* update restful_api.md

* add a hint

* repeat 3 time

* Fix Tokenizer encode (#645)

* same encode with HF

* sequence_start -> add_bos

* complement

* Fix wrong eos_id and bos_id obtained through grpc api (#644)

* Fix wrong eos_id and bos_id obtained through grpc api

* fix according to review comments

* update

* Optimize for throughput (#701)

* tmp

* update

* update

* optimize for throughput

* update

* fix eos

* clean up

* fix serving

* fix indexed copy

* minor

* minor

---------

Co-authored-by: lvhan028 <lvhan_028@163.com>

* Check-in user guide about turbomind config (#680)

* update

* update config guide

* update guide

* upate user guide according to review comments

* Replace mmengine with mmengine-lite (#715)

* Support loading hf model directly (#685)

* turbomind support export model params

* fix overflow

* support turbomind.from_pretrained

* fix tp

* support AutoModel

* support load kv qparams

* update auto_awq

* udpate docstring

* export lmdeploy version

* update doc

* remove download_hf_repo

* LmdeployForCausalLM -> LmdeployForCausalLM

* refactor turbomind.py

* update comment

* add bfloat16 convert back

* support gradio run_locl load hf

* support resuful api server load hf

* add docs

* support loading previous quantized model

* adapt pr 690

* udpate docs

* not export turbomind config when quantize a model

* check model_name when can not get it from config.json

* update readme

* remove model_name in auto_awq

* update

* update

* udpate

* fix build

* absolute import

* Fix cache/output length calculation (#738)

* bump version to v0.1.0a0 (#709)

* [Fix] Skip empty batch (#747)

* [Fix] build docker image failed since `packaging` is missing (#753)

* [Fix] Rollback the data type of input_ids to TYPE_UINT32 in preprocessor's proto (#758)

* Set the default value of `max_context_token_num` 1 (#761)

* rename pytorch poc

* fix lint

* add docstring

* add docstring

* refactor patch

* add recompute eviction support

* fix typo (#769)

* add triton server test and workflow yml (#760)

* add triton server test and workflow yml

* update

* revert changes in dockerfile

* update prompts

* recovery modeling

* fix turbomind build on sm<80 (#754)

* fix

* fix lint

* improvement(build): enable ninja and gold linker (#767)

* feat(build): enable ninja and lld

* fix(.github): add ninja installation

* fix(CI): remove dimsize=256

* fix(CI): add option for generate.sh

* fix(docs): update

* Report first-token-latency and token-latency percentiles (#736)

* update profile scripts

* add top_p, top_k and temperature as input arguments

* fix input_ids

* update profile_throughput

* update profile_restful_api

* update profile_serving

* update

* update

* add progress bar

* remove TODO comments

* update

* remove useless profile_* argument

* remove log level

* change concurrency default value to 64

* update restful_api.md

* update according to review comments

* fix docstring

* convert model with hf repo_id (#774)

* bump version to 0.1.0a1 (#776)

* Update benchmark user guide (#763)

* user guide of benchmark generation

* update benchmark generation guide

* update profiling throughput guide

* update profiling api_server guide

* rename file names

* update profile tis user guide

* update

* fix according to review comments

* update

* update according to review comments

* updaste

* add an example

* update

* add docstring

* add unified paging attention support

* refactor block manager

* do not alloc zero

* Fix early exit condition in attention kernel (#788)

* add chat template for Yi (#779)

* Fix missed arguments when benchmark static inference performance (#787)

* minor fix in the profile scripts and docs

* miss arguments

* typo

* fix lint

* update

* Unify prefill & decode passes (#775)

* Unify prefill and decode passes

* dynamic split-fuse

* refactor

* correct input count calculation

* remove unused

* lint

* lint

* fix msvc build

* fix msvc build

* fix msvc build

* fix msvc build

* fix msvc build

* fix msvc build

* fix msvc build

* fix msvc build

* fix msvc build

* add cuda12.1 build check ci (#782)

* update cuda12.1 build check ci

* use matrix

* auto upload cuda12.1 python pkg to release when create new tag (#784)

* add cuda12-whl-release ci

* enable environment

* test py310-311 windows wheel

* fix py310, py311 setup.py error on windows

* fix lint

* fix extra colon in InternLMChat7B (#796)

* fix local kv head num (#806)

* Report the inference benchmark of models with different size (#794)

* update test scripts for models with different sizes

* update

* only test after tunning gemm

* chmod +x

* fix typo

* benchmark on a100

* fix typo

* fix typo

* per-token latency percentile in profile_throughput

* fix

* fix

* rename

* make the script accept parameters

* minor fix

* indent

* reformat table

* change to 3000

* minor fix

* bump version to v0.1.0a2 (#807)

* fix out of bounds access (#809)

* update scheduler

* optimize request

* Simplify block manager (#812)

* simplify block manager

* fix lint

* set smem size for repetition penalty kernel (#818)

* add mbgemm&mbgemv

* fix recompute, fix mbgmm

---------

Co-authored-by: Lyu Han <lvhan_028@163.com>
Co-authored-by: AllentDan <41138331+AllentDan@users.noreply.github.com>
Co-authored-by: pppppM <67539920+pppppM@users.noreply.github.com>
Co-authored-by: Chen Xin <irexyc@gmail.com>
Co-authored-by: RunningLeon <mnsheng@yeah.net>
Co-authored-by: Yam(长琴) <haoshaochun@gmail.com>
Co-authored-by: liukuikun <24622904+Harold-lkk@users.noreply.github.com>
Co-authored-by: yunzhongyan0 <549713537@qq.com>
Co-authored-by: honglei.yan <honglei.yan@nio.com>
Co-authored-by: AllentDan <AllentDan@yeah.net>
Co-authored-by: aisensiy <aisensiy@163.com>
Co-authored-by: Li Zhang <lzhang329@gmail.com>
Co-authored-by: whcao <41630003+HIT-cwh@users.noreply.github.com>
Co-authored-by: Zaida Zhou <58739961+zhouzaida@users.noreply.github.com>
Co-authored-by: tpoisonooo <khj.application@aliyun.com>
Co-authored-by: Qian Zhao <112053249+C1rN09@users.noreply.github.com>

* [Fix] Adapt to the pyTorch poc branch (#863)

* Adapt to the pyTorch poc branch

* Adapt to the pyTorch poc branch

* fix comments

* update model

* wip

* wrong implementation

* s-lora single gpu

* refactor tp patch

* add tp support

* add tp gather

* recover profile generation

* daemon process

* inplace gather

* hf style

* add assert when input nothing

* find available port

---------

Co-authored-by: grimoire <yaoqian@pjlab.org.cn>
Co-authored-by: WRH <12756472+wangruohui@users.noreply.github.com>
Co-authored-by: AllentDan <AllentDan@yeah.net>
Co-authored-by: AllentDan <41138331+AllentDan@users.noreply.github.com>
Co-authored-by: tpoisonooo <khj.application@aliyun.com>
Co-authored-by: whcao <41630003+HIT-cwh@users.noreply.github.com>
Co-authored-by: Lyu Han <lvhan_028@163.com>
Co-authored-by: pppppM <67539920+pppppM@users.noreply.github.com>
Co-authored-by: Chen Xin <irexyc@gmail.com>
Co-authored-by: RunningLeon <mnsheng@yeah.net>
Co-authored-by: Yam(长琴) <haoshaochun@gmail.com>
Co-authored-by: liukuikun <24622904+Harold-lkk@users.noreply.github.com>
Co-authored-by: yunzhongyan0 <549713537@qq.com>
Co-authored-by: honglei.yan <honglei.yan@nio.com>
Co-authored-by: aisensiy <aisensiy@163.com>
Co-authored-by: Li Zhang <lzhang329@gmail.com>
Co-authored-by: Zaida Zhou <58739961+zhouzaida@users.noreply.github.com>
Co-authored-by: Qian Zhao <112053249+C1rN09@users.noreply.github.com>"
github.com/InternLM/lmdeploy,lmdeploy/pytorch/engine/model_agent.py,2023-12-28T03:48:58Z,"Refactor torch inference engine (#871)

* WIP

* cache engine wip

* finish cache engine

* fix cache and scheduler

* add paged attention

* step and stop

* add infer

* add request process

* fix end

* request without schedulersession

* add logits processor

* better context

* update patch

* [Improve] Use 4d input in pytorch poc (#371)

* 4D input, model.eval and llama config

* use auto dtype

* tp wip

* almost

* update logger

* run_check=false

* little optimize

current best

redist w/o dtensor

host mem in que

less rewrite

less code

update model weight

* share attention forward

* fix end

* Support Baichuan (#382)

* add baichuan WIP

* support baichuan

* support baichuan-13b

* fix

* add chat template

* lint

* comments

* fix

* Move `q_seq_info` into `context` (#398)

* move q seq info into context

* remove debugs

* remove debugs

* alibi wip

* add alibi

* reduce logic block (#435)

* add docstring

* add baichuan lint (#445)

* add fill cache back

* support internlm

* fix path of weight index

* Support chatglm2 in pytorch_poc (#360)

* draft support for chatglm2

* debug llama

* gitignore

* update input_id

* better patching

* patch chatglm2 model

* fix after merge

* remove inits

* q_seq_info & remove some debug & orig_self

* remove old unqeuzze inputid

* update patch and model config

* remove debugs and clean codes

* clean codes

* add credit

* add update id / fix dependency

* rename modules (#504)

Co-authored-by: grimoire <yaoqian@pjlab.org.cn>

* optimize fill kv cache (#523)

* optimize fill kv cache

* update internlm

* faster embedding

* fix bias tp

* fix baichuan2

* fix fill kv cache

* fix lint

---------

* Make trust_remote_code as cli argument (#434)

* trust_remote_code_argument

* format

* update tokenizer

* optimize rotary

* wtf

* Support Falcon models (#406)

* move q seq info into context

* falcon aligned

* trust_remote_code_argument

* fix for falcon

* comment out debugs

* comment out debugs

* use position id in context

* remove codes in falcon model

* Revert ""comment out debugs""

This reverts commit ee26a25c36f11c7afdad315b8101ebb5c2c637fd.

* 7b correct

* 1b aligned

* remove debugs

* patch to ignore position ids

* remove debug in alibi, avoid empty inputs

* fix

* rename dir to replace to ""models""

* use position_id and new fill kernel

* remove useless get_prompt func

* fix batch>2

* Refactor scheduler (#551)

* optimize block manager

* scheduler wip

* finish scheduler

* update engine

* profile pytorch poc (#455)

* profile pytorch poc

* update doc and import if need

* arg

* support profile_throughput.py

* reuse pytorch session

* end session

* Support Tensor parallel on Falcon models (#582)

* tp falcon 1b and 7b works

* remove debugs

* update copyright

* add some comments

* remove a debug

* support new hub models

* support 40b

* support 40b model config

* try

* recover

* fix remain len

* Apply rotary kernel (#572)

* apply rotary kernel

* format

* update rmsnorm

* update rms norm

* better unittest

* add docstring

---------

Co-authored-by: grimoire <yaoqian@pjlab.org.cn>

* fix(pytorch_poc): memory cal (#606)

* fix(pytorch_poc): memory cal

* Optimize attention (#597)

* add unittest

* add split k

* add docstring

* fast split k

* optimize load

* manually setup device and stream

* lint

---------

Co-authored-by: grimoire <yaoqian@pjlab.org.cn>

* feat(pytorch_poc): implement ReRoPE (#625)

* fix(pytorch_poc): memory cal

* style(pytorch_poc): lint

* style(.pre-commit-config.yaml): update

* style(pytorch_poc): remove useless

* feat(pytorch_poc): llama2 support rerope

* feat(pytorch_poc): fix long input generate

* feat(lmdeploy): add kernel

* feat(lmdeploy): update

* feat(lmdeploy): add rerope implementation

* fix(lmdeploy/pytorch_poc): apply rotary_emb

* fix(lmdeploy): update

* style(pytorch_poc): format

* style(lmdeploy): fix lint

* style(lmdeploy): typo

* style(pytorch_poc): format

* style(pytorch_poc): format

* fix(pytorch_poc): rms_norm add mask

* style(pytorch_poc/kernels): format rerope

* style(pytorch_poc): format rerope attn function description

* style(lmdeploy/pytorch_poc): format

* style(pytorch_poc): add code ref

* style(pytorch_poc): format rerope attn

* Refactor engine (#623)

* add agent

* optimize postprocess

* optimize decoding fill cache

* add docstring

* logit to cuda

* blocksize 128

* optimize pre/post process

* fix postprocess

* cpu pre/post process

* manually setup stream and device

* remove context

* update model agent

* update max session len

* remove tqdm

* update pre/post process

* inplace kernel

* avoid kv_len computation

* flash decoding with one cache

* remove comment

* add warning when no enough resources

* step if has unfinish

* add request manager

* better fill kv cache

* fix fill kv cache

* optimize prefill attention

* refractor

* refactoring...

* add custom output

* use cache

---------

Co-authored-by: grimoire <yaoqian@pjlab.org.cn>

* [Feature] w8a8 based on pytorch poc (#595)

* refactor smoothquant and support load w8a8 model by from_pretrained

* add w8a8 docs

* add w8a8 en docs

* add convert_to_qmodules function

---------

Co-authored-by: grimoire <streetyao@live.com>

* feat(lmdeploy): add rerope quantization (#718)

* feat(lmdeploy): add rerope quantization

* feat(lmdeploy): fix review

* [Refactor & Doc] Improve w8a8 and add docstring (#768)

* WIP

* improve w8a8 and add doc string

* add docstring

* add docstring

* fix lint

* rename pytorch poc (#764)

* rename pytorch poc

* fix lint

* add docstring

* add docstring

* refactor patch

* add recompute eviction support

* recovery modeling

* add docstring

* Unified paging (#860)

* change 'model_format' to 'qwen' when 'model_name' starts with 'qwen' (#575)

* avoid split chinese characters during decoding (#566)

* add solar chat template (#576)

* robust incremental decode for leading space (#581)

* robust incremental decode for leading space

* speed up lookup as prefix_space_tokens is shorter than no_prefix_space_tokens

* add UT and fix qwen stuff

* update solar chat template (#587)

* Revert ""[Docs] Simplify `build.md` (#370)"" (#586)

This reverts commit 4b5c2bda074eb4ac2e70c3c793fb5ef48f87d9c8.

* Fix crash and remove `sys_instruct` from `chat.py` and `client.py`(#591)

* fix crash

* update profile_generation.py

* format

* use self.bos_id

* remove sys_instruct

* bump version to v0.0.12 (#604)

* Add ""build from docker"" section (#602)

* add build from docker section

* update

* install python package

* update

* update

* update

* Add more user-friendly CLI  (#541)

* add

* import fire in main

* wrap to speed up fire cli

* update

* update docs

* update docs

* fix

* resolve commennts

* resolve confict and add test for cli

* support inference a batch of prompts (#467)

* support inference a batch of prompts

* docstring and assert

* bump version to v0.0.13 (#620)

* Improve api_server and webui usage (#544)

* make IPv6 compatible, safe run for coroutine interrupting

* instance_id -> session_id and fix api_client.py

* update doc

* remove useless faq

* safe ip mapping

* update app.py

* WIP completion

* completion

* update doc

* disable interactive mode for /v1/chat/completions

* docstring

* docstring

* refactor gradio

* update gradio

* udpate

* update doc

* rename

* session_id default -1

* missed two files

* add a APIClient

* add chat func for APIClient

* refine

* add concurrent function

* sequence_start, sequence_end --> interactive_mode

* update doc

* comments

* doc

* better text completion

* remove /v1/embeddings

* comments

* deprecate generate and use /v1/interactive/completions

* /v1/interactive/completion -> /v1/chat/interactive

* embeddings

* rename

* remove wrong arg description

* docstring

* fix

* update cli

* update doc

* strict session_len limit condition

* pass model args to api_server

* fix: gradio gr.Button.update deprecated after 4.0.0 (#637)

* add cli to list the supported model names (#639)

* update

* resolve comment

* Refactor model conversion (#296)

* split deploy.py

* fix get_cuda_tensor

* deploy qwen_awq

* fix lint

* add docstring

* fix

* support baichuan/baichuan-awq

* parameterizing size_per_head

* remove try/except

* limit input model_format

* add quant_path param

* remove old deploy.py

* fix path

* fix transformer layer range when load bins

* fix qwen init

* split & save log

* relative import

* update get_config

* WeightFileMgr -> Reader

* rename

* update

* fix init_layer_id

* rename llama.py -> meta_llama.py, hf.py -> llama.py

* reduce code

* update arg description

* fix meta llama

* manually cleanup meta model params

* [Enchance] internlm message to prompt (#499)

* update turbomind session_len with model.session_len (#634)

* [Fix] Qwen's quantization results are abnormal & Baichuan cannot be quantized (#605)

* fix awq

* adapt new qwen code

* adapt qwen 14b and baichuan2 7b

* add docstring

* add runtime error for qwen

* FIX: fix stop_session func bug (#578)

* FIX: fix stop_session func bug

* keep sequence_end = False

---------

Co-authored-by: honglei.yan <honglei.yan@nio.com>
Co-authored-by: AllentDan <AllentDan@yeah.net>

* Manage session id using random int for gradio local mode (#553)

* Use session id from gradio state

* use a new session id after reset

* rename session id like a state

* update comments

* reformat files

* init session id on block loaded

* use auto increased session id

* remove session id textbox

* apply to api_server and tritonserver

* update docstring

* add lock for safety

---------

Co-authored-by: AllentDan <AllentDan@yeah.net>

* fix benchmark serving computation mistake (#630)

* fix benchmark serving computation mistake

* fix timestamps computations

* remove speed up

* no mp

* mp seems faster?

* remove

* update

* remove

* fix

* update

* update print log

* typo

* print fist token latency only stream==True

* remove renew_session

* update AsyncEngine

* fix tokenizer_info when convert the model (#661)

* Add check env sub command (#654)

* add check env

* update issue template'

* remove some reqs from check env

* resolve comment

* fix Tokenizer load error when the path of the being-converted  model is not writable (#669)

* Add UltraCM and WizardLM chat templates (#599)

* add ultracm eval chat template

* add WizardLM chat template

* use ultrachat template instead of ultracm usecase

* bump version to v0.0.14 (#663)

* Add extra_requires to reduce dependencies (#580)

* update reqs

* update docs

* resolve comments

* upgrade pydantic

* fix rebase

* update doc

* update

* update

* update readme

* update

* add flash-attn

* TurboMind 2 (#590)

* refresh decoder attention kernel

* block-level kv cache

* `BlockManager` & `SequenceManager`

* update

* update

* update

* update

* rename

* GQA support

* fix context length

* GQA dispatch

* kv8

* tune

* async stream cb

* nvtx

* config parsing

* debug

* optimize output cost

* split-k decoding

* minor

* truncate `session_len` by available blocks

* minor

* license

* fix

* dispatch `cp.async`

* fix linking

* fix

* fix deadlock

* guard input length

* correct start offset

* fix prefill chunking

* fix `cache_block_seq_len` param passing

* fix `block_size` fmtstr

* fix output tokens

* fix batch resizing

* fix masking of finished sequences

* add debug util

* free unused block early

* add ntk scaling and logn scaling

* cmake flags

* fix typo

* w4a16 for sm75

* fix msvc build

* fix msvc build

* fix block verification

* fix msvc build

* use `std::shuffle`

* fix lint

* fix lint

* fix lint

* clear incoming buffer

* clear finished requests

* fix batch initialization

* fix typo

* fix typo

* fix comparison

* [Docs] Update Supported Matrix (#679)

* update supported matrix

* change the default shard size when saving quantized weights

* baichuan2 kv8

* update kv8 docs (#681)

* Fix init of batch state (#682)

* fix init of finished buf

* fix `finished_count`

* fix turbomind stream canceling (#686)

* fix

* instance for each forward

* [Fix] Fix load_checkpoint_in_model bug (#690)

* fix load_checkpoint_in_model bug

* fix comments

* fix comments

* fix bugs

* [Doc] Update restful api doc (#662)

* update restful_api.md

* add a hint

* repeat 3 time

* Fix Tokenizer encode (#645)

* same encode with HF

* sequence_start -> add_bos

* complement

* Fix wrong eos_id and bos_id obtained through grpc api (#644)

* Fix wrong eos_id and bos_id obtained through grpc api

* fix according to review comments

* update

* Optimize for throughput (#701)

* tmp

* update

* update

* optimize for throughput

* update

* fix eos

* clean up

* fix serving

* fix indexed copy

* minor

* minor

---------

Co-authored-by: lvhan028 <lvhan_028@163.com>

* Check-in user guide about turbomind config (#680)

* update

* update config guide

* update guide

* upate user guide according to review comments

* Replace mmengine with mmengine-lite (#715)

* Support loading hf model directly (#685)

* turbomind support export model params

* fix overflow

* support turbomind.from_pretrained

* fix tp

* support AutoModel

* support load kv qparams

* update auto_awq

* udpate docstring

* export lmdeploy version

* update doc

* remove download_hf_repo

* LmdeployForCausalLM -> LmdeployForCausalLM

* refactor turbomind.py

* update comment

* add bfloat16 convert back

* support gradio run_locl load hf

* support resuful api server load hf

* add docs

* support loading previous quantized model

* adapt pr 690

* udpate docs

* not export turbomind config when quantize a model

* check model_name when can not get it from config.json

* update readme

* remove model_name in auto_awq

* update

* update

* udpate

* fix build

* absolute import

* Fix cache/output length calculation (#738)

* bump version to v0.1.0a0 (#709)

* [Fix] Skip empty batch (#747)

* [Fix] build docker image failed since `packaging` is missing (#753)

* [Fix] Rollback the data type of input_ids to TYPE_UINT32 in preprocessor's proto (#758)

* Set the default value of `max_context_token_num` 1 (#761)

* rename pytorch poc

* fix lint

* add docstring

* add docstring

* refactor patch

* add recompute eviction support

* fix typo (#769)

* add triton server test and workflow yml (#760)

* add triton server test and workflow yml

* update

* revert changes in dockerfile

* update prompts

* recovery modeling

* fix turbomind build on sm<80 (#754)

* fix

* fix lint

* improvement(build): enable ninja and gold linker (#767)

* feat(build): enable ninja and lld

* fix(.github): add ninja installation

* fix(CI): remove dimsize=256

* fix(CI): add option for generate.sh

* fix(docs): update

* Report first-token-latency and token-latency percentiles (#736)

* update profile scripts

* add top_p, top_k and temperature as input arguments

* fix input_ids

* update profile_throughput

* update profile_restful_api

* update profile_serving

* update

* update

* add progress bar

* remove TODO comments

* update

* remove useless profile_* argument

* remove log level

* change concurrency default value to 64

* update restful_api.md

* update according to review comments

* fix docstring

* convert model with hf repo_id (#774)

* bump version to 0.1.0a1 (#776)

* Update benchmark user guide (#763)

* user guide of benchmark generation

* update benchmark generation guide

* update profiling throughput guide

* update profiling api_server guide

* rename file names

* update profile tis user guide

* update

* fix according to review comments

* update

* update according to review comments

* updaste

* add an example

* update

* add docstring

* add unified paging attention support

* refactor block manager

* do not alloc zero

* Fix early exit condition in attention kernel (#788)

* add chat template for Yi (#779)

* Fix missed arguments when benchmark static inference performance (#787)

* minor fix in the profile scripts and docs

* miss arguments

* typo

* fix lint

* update

* Unify prefill & decode passes (#775)

* Unify prefill and decode passes

* dynamic split-fuse

* refactor

* correct input count calculation

* remove unused

* lint

* lint

* fix msvc build

* fix msvc build

* fix msvc build

* fix msvc build

* fix msvc build

* fix msvc build

* fix msvc build

* fix msvc build

* fix msvc build

* add cuda12.1 build check ci (#782)

* update cuda12.1 build check ci

* use matrix

* auto upload cuda12.1 python pkg to release when create new tag (#784)

* add cuda12-whl-release ci

* enable environment

* test py310-311 windows wheel

* fix py310, py311 setup.py error on windows

* fix lint

* fix extra colon in InternLMChat7B (#796)

* fix local kv head num (#806)

* Report the inference benchmark of models with different size (#794)

* update test scripts for models with different sizes

* update

* only test after tunning gemm

* chmod +x

* fix typo

* benchmark on a100

* fix typo

* fix typo

* per-token latency percentile in profile_throughput

* fix

* fix

* rename

* make the script accept parameters

* minor fix

* indent

* reformat table

* change to 3000

* minor fix

* bump version to v0.1.0a2 (#807)

* fix out of bounds access (#809)

* update scheduler

* optimize request

* Simplify block manager (#812)

* simplify block manager

* fix lint

* set smem size for repetition penalty kernel (#818)

* add mbgemm&mbgemv

* fix recompute, fix mbgmm

---------

Co-authored-by: Lyu Han <lvhan_028@163.com>
Co-authored-by: AllentDan <41138331+AllentDan@users.noreply.github.com>
Co-authored-by: pppppM <67539920+pppppM@users.noreply.github.com>
Co-authored-by: Chen Xin <irexyc@gmail.com>
Co-authored-by: RunningLeon <mnsheng@yeah.net>
Co-authored-by: Yam(长琴) <haoshaochun@gmail.com>
Co-authored-by: liukuikun <24622904+Harold-lkk@users.noreply.github.com>
Co-authored-by: yunzhongyan0 <549713537@qq.com>
Co-authored-by: honglei.yan <honglei.yan@nio.com>
Co-authored-by: AllentDan <AllentDan@yeah.net>
Co-authored-by: aisensiy <aisensiy@163.com>
Co-authored-by: Li Zhang <lzhang329@gmail.com>
Co-authored-by: whcao <41630003+HIT-cwh@users.noreply.github.com>
Co-authored-by: Zaida Zhou <58739961+zhouzaida@users.noreply.github.com>
Co-authored-by: tpoisonooo <khj.application@aliyun.com>
Co-authored-by: Qian Zhao <112053249+C1rN09@users.noreply.github.com>

* [Fix] Adapt to the pyTorch poc branch (#863)

* Adapt to the pyTorch poc branch

* Adapt to the pyTorch poc branch

* fix comments

* update model

* update benchmark

* [Fix] Fix conflicts in `lite` (#878)

* cherry-pick Fix meta tensor error commits

* fix smooth quant

---------

Co-authored-by: pppppM <67539920+pppppM@users.noreply.github.com>

* [Feature] Support w8a8 tp (#888)

* fix smooth quant save_pretrained

* support w8a8 tp

* change weight and bias in QLinear back to buffer

* remove debug codes and add comments

* fix message step update

* update docs

---------
Co-authored-by: grimoire <streetyao@live.com>
Co-authored-by: WRH <12756472+wangruohui@users.noreply.github.com>
Co-authored-by: AllentDan <AllentDan@yeah.net>
Co-authored-by: AllentDan <41138331+AllentDan@users.noreply.github.com>
Co-authored-by: tpoisonooo <khj.application@aliyun.com>
Co-authored-by: whcao <41630003+HIT-cwh@users.noreply.github.com>
Co-authored-by: pppppM <67539920+pppppM@users.noreply.github.com>
Co-authored-by: Chen Xin <irexyc@gmail.com>
Co-authored-by: RunningLeon <mnsheng@yeah.net>
Co-authored-by: Yam(长琴) <haoshaochun@gmail.com>
Co-authored-by: liukuikun <24622904+Harold-lkk@users.noreply.github.com>
Co-authored-by: yunzhongyan0 <549713537@qq.com>
Co-authored-by: honglei.yan <honglei.yan@nio.com>
Co-authored-by: aisensiy <aisensiy@163.com>
Co-authored-by: Li Zhang <lzhang329@gmail.com>
Co-authored-by: Zaida Zhou <58739961+zhouzaida@users.noreply.github.com>
Co-authored-by: Qian Zhao <112053249+C1rN09@users.noreply.github.com>"
github.com/AILab-CVC/GPT4Tools,gpt4tools/llm.py,2023-12-18T17:47:26Z,update vicuna-v1.5
github.com/project-baize/baize-chatbot,demo/app_modules/utils.py,2023-05-24T21:05:33Z,Update utils.py
github.com/project-baize/baize-chatbot,demo/app_modules/utils.py,2023-04-19T03:39:10Z,"Update utils.py

Iterator is missing"
github.com/project-baize/baize-chatbot,demo/app_modules/utils.py,2023-04-05T15:53:42Z,Rename function
github.com/project-baize/baize-chatbot,demo/app_modules/utils.py,2023-04-05T05:53:56Z,Fix style
github.com/project-baize/baize-chatbot,demo/app_modules/utils.py,2023-04-05T04:05:18Z,Update utils.py
github.com/project-baize/baize-chatbot,demo/app_modules/utils.py,2023-04-04T16:23:06Z,Reformat code
github.com/project-baize/baize-chatbot,demo/app_modules/utils.py,2023-04-04T16:03:54Z,Update utils.py
github.com/project-baize/baize-chatbot,demo/app_modules/utils.py,2023-04-03T07:29:22Z,update code and readme
github.com/project-baize/baize-chatbot,demo/app_modules/utils.py,2023-04-03T06:14:07Z,update code
github.com/project-baize/baize-chatbot,demo/app_modules/utils.py,2023-04-01T14:18:21Z,Add files via upload
github.com/liguodongiot/llm-action,train/chinese-llama-alpaca/merge_llama_with_chinese_lora.py,2023-07-23T11:43:58Z,fix
github.com/nlpai-lab/KULLM,merge_lora.py,2023-08-13T10:40:06Z,fix: truncate embedding size
github.com/nlpai-lab/KULLM,merge_lora.py,2023-06-16T08:07:15Z,add: merge lora weight to model
github.com/InternLM/InternLM-XComposer,projects/ShareGPT4V/share4v/train/train.py,2023-12-13T17:41:19Z,full update
github.com/shibing624/MedicalGPT,inference_multigpu_demo.py,2024-01-30T04:26:15Z,update infer.
github.com/shibing624/MedicalGPT,inference_multigpu_demo.py,2023-11-15T06:48:12Z,update infer use greedy decoding.
github.com/shibing624/MedicalGPT,inference_multigpu_demo.py,2023-11-13T12:39:54Z,update multi gpu.
github.com/shibing624/MedicalGPT,inference_multigpu_demo.py,2023-11-10T02:56:59Z,update fp32 for lm_head.
github.com/shibing624/MedicalGPT,inference_multigpu_demo.py,2023-10-25T05:05:42Z,update peft logger.
github.com/shibing624/MedicalGPT,inference_multigpu_demo.py,2023-10-23T09:06:58Z,update left padding.
github.com/shibing624/MedicalGPT,inference_multigpu_demo.py,2023-10-23T04:30:54Z,update infer for do sample.
github.com/shibing624/MedicalGPT,inference_multigpu_demo.py,2023-10-17T11:16:06Z,update template name.
github.com/shibing624/MedicalGPT,inference_multigpu_demo.py,2023-09-08T11:10:34Z,update file exists.
github.com/shibing624/MedicalGPT,inference_multigpu_demo.py,2023-09-07T07:32:04Z,drop truncation.
github.com/shibing624/MedicalGPT,inference_multigpu_demo.py,2023-09-07T07:08:51Z,update DDP
github.com/shibing624/MedicalGPT,inference_multigpu_demo.py,2023-09-07T06:13:13Z,update show case.
github.com/shibing624/MedicalGPT,inference_multigpu_demo.py,2023-09-07T06:00:19Z,update multi gpu infer.
github.com/arielnlee/Platypus,inference.py,2023-08-14T20:33:37Z,update pipeline
github.com/arielnlee/Platypus,inference.py,2023-06-26T04:26:47Z,initial files
github.com/EvilPsyCHo/Play-with-LLMs,webui.py,2023-07-05T12:39:28Z,webui support baichuan
github.com/EvilPsyCHo/Play-with-LLMs,webui.py,2023-06-28T04:02:48Z,add model inference webui
github.com/nlpai-lab/KULLM,merge_model.py,2023-08-13T10:36:33Z,fix: resize embedding considering vocab size of tokenizer
github.com/eosphoros-ai/DB-GPT-Hub,dbgpt_hub/llm_base/adapter.py,2023-10-29T03:27:55Z,black style un regular code
github.com/eosphoros-ai/DB-GPT-Hub,dbgpt_hub/llm_base/adapter.py,2023-09-22T17:03:10Z,try solve predict import
github.com/eosphoros-ai/DB-GPT-Hub,dbgpt_hub/llm_base/adapter.py,2023-09-19T09:51:03Z,"change class about llm_base dir,add model_trainer,reduce config_parser"
github.com/eosphoros-ai/DB-GPT-Hub,dbgpt_hub/llm_base/adapter.py,2023-09-18T14:23:11Z,"add llm_base adapter,shared modules needed when loading models for different fine-tuning training methods and inference stages, mainly for the shared processing of Peft related to methods like LoRA, QLoRA, not using adapter, etc."
github.com/star-whale/starwhale,example/LLM/guanaco/sw.py,2023-06-09T02:22:16Z,example(LLM): add guanaco evaluation example (#2315)
github.com/FudanDISC/DISC-LawLLM,eval/src/models.py,2024-01-24T09:22:38Z,Update models.py
github.com/FudanDISC/DISC-LawLLM,eval/src/models.py,2023-10-27T05:10:17Z,prepare release of DISC-Law-Eval benchmark scripts
github.com/AGI-Edgerunners/LLM-Adapters,peft/src/peft/peft_model.py,2023-05-25T16:38:44Z,fixed prefix tuning inference bug
github.com/AGI-Edgerunners/LLM-Adapters,peft/src/peft/peft_model.py,2023-04-07T11:23:59Z,fixed inference error without loading int8 model
github.com/AGI-Edgerunners/LLM-Adapters,peft/src/peft/peft_model.py,2023-03-31T12:49:40Z,support bottleneck adapters
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-09-04T15:07:04Z,Clean load_model function and add quantization_inference config
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-08-29T10:50:23Z,CodeModels + TrainingArguments mutable
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-08-09T14:16:22Z,"Revert ""Attempt to fix deespeed issues with learning rate being and make style""

This reverts commit 62f9c6a14b373410d1abb1eaad01c64c82201073."
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-08-09T11:06:02Z,Attempt to fix deespeed issues with learning rate being and make style
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-07-27T17:45:44Z,Ensure that lr is a float
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-07-07T20:28:07Z,Ensure use_cache is set to False
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-06-27T10:49:57Z,Full model fp16
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-06-27T09:58:50Z,Force dtype to match the fp16/bf16 arguments for model quantification
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-05-17T15:58:46Z,Fix print when labels include -100 value
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-05-17T13:02:46Z,Fix debug print
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-05-17T12:54:07Z,Print first batch of data
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-05-03T17:06:38Z,Fix bug in loss function
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-05-02T14:21:36Z,Custom loss
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-24T14:12:17Z,Oversample to ensure same length in datasets
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-24T13:45:50Z,Rotate datasets during training
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-16T16:50:57Z, line-length to 119 and ruff conf to match HF
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-16T16:32:01Z,Add ruff and code lint improvements
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-14T17:15:38Z,Disable rich until fixed
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-14T16:17:57Z,Revert rich console
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-14T16:15:25Z,Attempt to fix rich and fix predictions
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-14T15:49:45Z,Add refresh=True to rich
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-13T23:30:34Z,more bug fixing
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-13T23:07:31Z,fix bug
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-13T22:59:46Z,add rich progress bar to CollieTrainer
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-13T12:26:05Z,Name change for `trainer.py` and `run.py`
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-12T15:27:22Z,Use trainer.save_model instead of save_pretrained
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-12T15:17:23Z,Modify the Seq2SeqTrainer to only save LoRA weights
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-12T10:15:09Z,Evaluate loss zero-shot models in dev
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-11T13:41:58Z,Rename target_modules as lora_target_modules and update find_hparams
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-11T13:25:46Z,Fix bug in dataset and add target modules as a hparam
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-11T10:03:01Z,Fix model and predictions saving
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-10T12:51:18Z,Fix minor bug
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-10T10:04:32Z,Refactor
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-10T10:03:54Z,"Merge remote-tracking branch 'refs/remotes/origin/Train-experiments' into Train-experiments
Merge local changes with git changes"
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-10T10:03:25Z,fix trainer
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-08T14:55:10Z,Fix model save
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-05T20:37:52Z,Fix bug and slurm file
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-05T14:37:19Z,Remove duplicated model saving
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-05T14:25:03Z,Add verbosity
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-05T14:22:05Z,Set padding size to the left. Fix model loading at inference
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-04T19:47:13Z,Save config and eval config
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-04T18:00:06Z,Set logging level INFO
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-04T17:48:44Z,Fix path
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-04T17:40:43Z,Fix bug
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-04T17:25:21Z,Small change in prompt and fixes
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-04T15:51:51Z,Prepare config for debug
github.com/hitz-zentroa/GoLLIE,src/trainer.py,2023-04-04T13:50:52Z,Implement dataset and trainer
github.com/dvlab-research/LLaMA-VID,llamavid/model/builder.py,2023-11-28T17:03:38Z,first commit
github.com/Clouditera/secgpt,webdemo/webdemo.py,2023-11-20T09:52:20Z,update README.md
github.com/Clouditera/secgpt,webdemo/webdemo.py,2023-11-20T07:33:28Z,update readme
github.com/Clouditera/secgpt,webdemo/webdemo.py,2023-11-20T06:00:38Z,first blood
github.com/jerry1993-tech/Cornucopia-LLaMA-Fin-Chinese,infer.py,2023-05-21T14:15:45Z,first version
github.com/jerry1993-tech/Cornucopia-LLaMA-Fin-Chinese,infer.py,2023-05-06T03:26:02Z,init code
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-12-01T07:03:58Z,make adaption to latest peft version
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-07-18T07:23:03Z,fix checkpoint saving issue. work with peft 0.3.0 and transformers 4.30.2
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-05-30T01:34:54Z,Update finetune.py
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-05-10T08:54:57Z,update
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-05-10T08:54:34Z,"Update finetune.py

Add eval strategy"
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-05-10T08:52:11Z,"Add Weights & Biases integrations

Wandb is a module that helps to examine the training process, it would be helpful to add it. The usage of wandb is optional so it will not affect the whole process."
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-04-22T07:25:05Z,add assert
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-04-17T16:35:05Z,fix checks
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-04-17T16:35:05Z,fix circular import and add monkeypatch submodule in setup
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-04-17T16:35:01Z,"make things installable, refactor things"
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-04-17T06:16:05Z,fix continue training for this version
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-04-17T05:42:50Z,fix bug
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-04-17T04:16:21Z,fix bug when loading old lora model
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-04-13T02:25:05Z,"Merge pull request #77 from winglian/upstream-peft

use monkey patch instead of forked peft"
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-04-12T10:54:23Z,addtional fix
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-04-12T04:59:44Z,add xformers support
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-04-11T11:15:56Z,fix bug on v1 finetune
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-04-09T15:40:58Z,use monkey patch instead of forked peft
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-04-09T04:26:22Z,add g_idx support on cuda backend
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-04-07T07:34:06Z,add triton backend support for v2 model
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-04-07T02:40:24Z,merge pull request in new branch
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-04-06T22:43:36Z,Update finetune.py
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-04-06T11:49:12Z,Use flash attention monkeypatch
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-04-05T23:29:36Z,"GPTQv2 support

GPTQv2 support.
1. Adds dependency on `triton`
2. Refactors autograd_4bit to include both GPTQv1 and GPTQv2
3. Introduces new environment variable GPTQ_VERSION to select autograd_4bit version
4. Fixes triton kernels
5. Matrix multiplications are in fp16"
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-04-03T15:55:58Z,update multi gpu support in finetune.py
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-03-31T11:44:36Z,fix device_map bug when using lora_apply_dir
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-03-31T02:40:40Z,"fix gpt4all training to more closely match the released logic, other small fixes and optimizations"
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-03-29T15:21:47Z,"better multi-gpu support, support gpt4all training data"
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-03-29T06:35:39Z,add resume checkpoint to continue a training
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-03-29T03:20:16Z,add padding support as an option
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-03-28T13:45:33Z,update finetune data format
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-03-28T13:12:51Z,fix bug
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-03-28T12:33:55Z,add v2 model support
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-03-27T20:08:20Z,"backwards support for pre-py3.10, add datasets requirement used in train"
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-03-26T03:52:38Z,Tested and should be ready!
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-03-25T04:56:06Z,distributed data parallelism with torchrun
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-03-25T04:03:43Z,model parallelism
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-03-25T02:29:02Z,Merge branch 'main' into finetune-refactor
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-03-24T12:46:03Z,"Reflect last changes in main

Reflect commits:
https://github.com/johnsmith0031/alpaca_lora_4bit/commit/4906961bf1fd22a5e44b27f926bf3b12775ef384
https://github.com/johnsmith0031/alpaca_lora_4bit/commit/60b227d0ba9ff78378afc8703028448be5bee2f3"
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-03-24T11:15:07Z,"Refactor finetune.py

1. Add command line arguments support
2. Add Stanford Alpaca-like dataset support. Used code from - https://github.com/tloen/alpaca-lora
3. Fix LoRA pre-train application"
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-03-23T08:43:18Z,fix minor bug
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-03-23T08:25:29Z,Add gradient checkpointing
github.com/johnsmith0031/alpaca_lora_4bit,finetune.py,2023-03-22T04:09:04Z,add more scripts and adjust code for transformer branch
github.com/opendilab/LMDrive,LAVIS/lavis/models/drive_models/drive.py,2023-12-13T08:53:56Z,first commit
github.com/ise-uiuc/magicoder,src/magicoder/llm_wrapper.py,2023-12-02T12:41:51Z,feat: dscoder 1.3B and 33B
github.com/ise-uiuc/magicoder,src/magicoder/llm_wrapper.py,2023-11-29T05:31:57Z,feat: flash attention and language ablation
github.com/ise-uiuc/magicoder,src/magicoder/llm_wrapper.py,2023-11-26T15:17:05Z,fix: ds1000 postprocessing
github.com/ise-uiuc/magicoder,src/magicoder/llm_wrapper.py,2023-11-26T09:45:43Z,feat: deepseek-coder based model and ds-1000
github.com/ise-uiuc/magicoder,src/magicoder/llm_wrapper.py,2023-11-10T08:23:52Z,refactor: rename from magicode to magicoder
github.com/dvlab-research/LongLoRA,merge_lora_weights_and_save_hf_model.py,2023-09-21T18:13:56Z,Add files via upload
github.com/InternLM/InternLM-XComposer,projects/ShareGPT4V/share4v/model/builder.py,2024-01-03T04:39:51Z,fix load model bug
github.com/InternLM/InternLM-XComposer,projects/ShareGPT4V/share4v/model/builder.py,2023-12-13T17:41:19Z,full update
github.com/xusenlinzy/api-for-open-llm,libs/langchain_llm/langchain_llm/utils.py,2024-01-14T11:01:04Z,Update langchain_llm package
github.com/xusenlinzy/api-for-open-llm,libs/langchain_llm/langchain_llm/utils.py,2024-01-13T01:20:56Z,Add apply lora
github.com/lich99/ChatGLM-finetune-LoRA,old/train.py,2023-04-13T11:39:43Z,update
github.com/AILab-CVC/GPT4Tools,eval/inference.py,2023-12-18T17:47:26Z,update vicuna-v1.5
github.com/AILab-CVC/SEED,MultiModalLLM/src/model/peft_model.py,2024-02-24T04:23:21Z,Upload training code
github.com/AGI-Edgerunners/LLM-Adapters,peft/src/peft/tuners/lora.py,2023-04-07T11:43:28Z,fixed inference error without loading int8 model
github.com/AGI-Edgerunners/LLM-Adapters,peft/src/peft/tuners/lora.py,2023-04-07T11:14:17Z,fixed inference error without loading int8 model
github.com/AGI-Edgerunners/LLM-Adapters,peft/src/peft/tuners/lora.py,2023-03-31T12:49:40Z,support bottleneck adapters
github.com/star-whale/starwhale,example/LLM/llama/finetune.py,2023-11-22T02:28:00Z,enhance(SDK): tune finetune decorator for Dataset type arguments (#3007)
github.com/star-whale/starwhale,example/LLM/llama/finetune.py,2023-11-13T08:01:22Z,feat(sdk): finetune decorator supports auto build and validation datasets (#2962)
github.com/star-whale/starwhale,example/LLM/llama/finetune.py,2023-10-25T04:27:41Z,chore(client): enable flake8 bugbear for python code (#2897)
github.com/star-whale/starwhale,example/LLM/llama/finetune.py,2023-08-04T02:04:03Z,example(LLM): fix llama finetune build model modules typo (#2587)
github.com/star-whale/starwhale,example/LLM/llama/finetune.py,2023-08-02T11:05:32Z,example(LLM): disable save checkpoint for llama finetune (#2585)
github.com/star-whale/starwhale,example/LLM/llama/finetune.py,2023-06-14T09:46:16Z,example(LLM): add llama model evaluation and finetune example (#2341)
github.com/Facico/Chinese-Vicuna,tools/application/chatglm_lora_finetune.py,2023-03-30T09:42:50Z,fix cpp ctx-len
github.com/InternLM/InternLM-XComposer,projects/DualFocus/dualfocus/model/builder.py,2024-02-22T11:28:54Z,add DualFocus
github.com/0nutation/SpeechGPT,speechgpt/src/infer/cli_infer.py,2023-09-15T11:25:27Z,Update cli_infer.py
github.com/0nutation/SpeechGPT,speechgpt/src/infer/cli_infer.py,2023-09-15T11:18:33Z,open-source
github.com/michael-wzhu/PromptCBLUE,peft/peft_model.py,2023-07-18T09:37:52Z,2023-07-18: add LlaMA + lora code; add vllm model serving
github.com/HarderThenHarder/transformers_tasks,LLM/chatglm_finetune/peft-chatglm/src/peft/peft_model.py,2023-08-02T12:16:49Z,add LLMs Trainer
github.com/star-whale/starwhale,example/LLM/llama/evaluation.py,2023-06-14T09:46:16Z,example(LLM): add llama model evaluation and finetune example (#2341)
github.com/visual-openllm/visual-openllm,visual_openllm/llm.py,2024-02-22T19:37:47Z,增加对Chatglm3的支持，新增vqa和pix2pix功能
github.com/visual-openllm/visual-openllm,visual_openllm/llm.py,2024-02-19T19:42:53Z,增加对Chatglm3的支持，新增vqa和pix2pix功能
github.com/visual-openllm/visual-openllm,visual_openllm/llm.py,2023-03-29T10:25:11Z,Insist on putting model to cuda
github.com/visual-openllm/visual-openllm,visual_openllm/llm.py,2023-03-26T14:57:55Z,Add PromptParser to parse model inputs and outputs
github.com/visual-openllm/visual-openllm,visual_openllm/llm.py,2023-03-26T13:42:19Z,Set maximum token length to 2048
github.com/visual-openllm/visual-openllm,visual_openllm/llm.py,2023-03-26T12:11:52Z,Remove instruction prompt
github.com/visual-openllm/visual-openllm,visual_openllm/llm.py,2023-03-26T09:43:26Z,Format code
github.com/visual-openllm/visual-openllm,visual_openllm/llm.py,2023-03-26T08:22:57Z,"add chatglm

Signed-off-by: mymusise <mymusise1@gmail.com>"
github.com/bigcode-project/octopack,finetuning/starcoder/merge-peft-adapters.py,2023-08-15T16:19:37Z,add santacoder fine-tuning
github.com/intelligent-machine-learning/dlrover,atorch/atorch/trainer/atorch_trainer.py,2024-01-17T09:34:01Z,"Merge atorch code (202401a) (#957)

* update atorch 202401a

* ignore E266

* add te in dockerfile

* fix ut

* fix ut"
github.com/intelligent-machine-learning/dlrover,atorch/atorch/trainer/atorch_trainer.py,2023-12-21T02:48:35Z,"sync atorch (#904)

* sync atorch

* update req

* format

* remove file

* fix format

* update

* fix format

* add datasets

* fix

* uplodat

* fix tests

* format"
github.com/CarperAI/trlx,tests/test_peft.py,2023-06-23T21:55:36Z,"peft to opendelta migration (#434) + memory optimization (#320) (#486)

* Migrate to peft from opendelta  for parameter efficient tuning methods (#434) +  Collapse reference+learner hydra heads when using LoRa (#320)

* fix from_config

* Review corrections

* ILQL generate when temperature is 0.

* revert: guard against experimental 8-bit loading support

* format: run `black`

---------

Co-authored-by: jon-tow <jonathantow1@gmail.com>
Co-authored-by: maxreciprocate <56548574+maxreciprocate@users.noreply.github.com>"
github.com/star-whale/starwhale,example/LLM/vicuna/evaluation.py,2023-06-14T09:43:14Z,example(LLM): add vicuna 7B/13B examples (#2349)
github.com/Meituan-AutoML/MobileVLM,scripts/mergelora.py,2024-01-11T12:27:42Z,support finetune with lora
github.com/FlagOpen/FlagEmbedding,Long_LLM/activation_beacon/src/activation_beacon_llama/__init__.py,2024-01-28T13:56:39Z,update activation beacon
github.com/huggingface/notebooks,sagemaker/24_train_bloom_peft_lora/scripts/run_clm.py,2023-06-02T05:37:19Z,upgrade to peft 0.3.0 and reduced context to train on (#383)
github.com/huggingface/notebooks,sagemaker/24_train_bloom_peft_lora/scripts/run_clm.py,2023-04-13T13:59:41Z,"Add peft example (#348)

* added peft test

* test

* working example

* finish example"
github.com/IlyaGusev/rulm,self_instruct/src/util/load.py,2023-10-09T11:28:27Z,Mistral fixes
github.com/IlyaGusev/rulm,self_instruct/src/util/load.py,2023-09-17T11:28:13Z,is_lora in load
github.com/IlyaGusev/rulm,self_instruct/src/util/load.py,2023-09-14T08:47:06Z,Better chat templates
github.com/IlyaGusev/rulm,self_instruct/src/util/load.py,2023-09-02T10:24:21Z,Rm seq2seq mode
github.com/IlyaGusev/rulm,self_instruct/src/util/load.py,2023-07-21T12:03:24Z,eval rsg fixes
github.com/IlyaGusev/rulm,self_instruct/src/util/load.py,2023-07-03T00:11:17Z,fixes
github.com/IlyaGusev/rulm,self_instruct/src/util/load.py,2023-06-07T09:33:20Z,New scripts
github.com/huggingface/notebooks,sagemaker/28_train_llms_with_qlora/scripts/run_clm.py,2023-07-18T11:29:07Z,add remote code again.
github.com/huggingface/notebooks,sagemaker/28_train_llms_with_qlora/scripts/run_clm.py,2023-07-18T08:48:25Z,"Fix QLoRA SageMaker Script with saving directly to safetensors.  (#414)

* Update run_clm.py

* Update requirements.txt

* Update requirements.txt

* Update run_clm.py

* Update requirements.txt"
github.com/huggingface/notebooks,sagemaker/28_train_llms_with_qlora/scripts/run_clm.py,2023-07-13T14:05:44Z,added working example (#410)
github.com/yangjianxin1/Firefly-LLaMA2-Chinese,component/utils.py,2023-09-18T16:55:49Z,单轮对话 & 多轮对话
github.com/BobaZooba/xllm,src/xllm/utils/nn.py,2023-11-15T12:03:41Z,Release: 0.1.1
github.com/BobaZooba/xllm,src/xllm/utils/nn.py,2023-11-14T12:57:06Z,more docs
github.com/BobaZooba/xllm,src/xllm/utils/nn.py,2023-11-13T09:40:43Z,Init
github.com/haonan-li/CMMLU,src/hf_causal_model.py,2023-07-28T16:24:52Z,update ABCD token mapping
github.com/zjunlp/KnowLM,examples/generate_lora.py,2023-12-07T01:26:39Z,Update generate_lora.py
github.com/zjunlp/KnowLM,examples/generate_lora.py,2023-11-08T04:52:35Z,fix bugs in #96 and support multi-gpu inference
github.com/zjunlp/KnowLM,examples/generate_lora.py,2023-10-24T13:26:28Z,fix error about file path
github.com/zjunlp/KnowLM,examples/generate_lora.py,2023-10-24T13:13:52Z,fix bug #82 when run in cpu
github.com/zjunlp/KnowLM,examples/generate_lora.py,2023-08-13T07:44:19Z,remove lora
github.com/zjunlp/KnowLM,examples/generate_lora.py,2023-05-25T06:19:18Z,Update generate_lora.py
github.com/zjunlp/KnowLM,examples/generate_lora.py,2023-05-25T04:22:49Z,Update generate_lora.py
github.com/zjunlp/KnowLM,examples/generate_lora.py,2023-05-24T15:16:00Z,Add files via upload
github.com/penghao-wu/vstar,LLaVA/llava/model/builder.py,2024-01-07T09:38:33Z,int8 support for vqa llm
github.com/penghao-wu/vstar,LLaVA/llava/model/builder.py,2023-12-18T04:32:06Z,init code
github.com/AGI-Edgerunners/LLM-Adapters,export_state_dict_checkpoint.py,2023-03-29T13:37:38Z,initial upload
github.com/allenai/open-instruct,open_instruct/merge_lora.py,2024-02-06T01:56:24Z,fix for newer bitsandbytes
github.com/allenai/open-instruct,open_instruct/merge_lora.py,2023-11-17T10:39:48Z,"Fix several issues in the evals. (#79)

* Merging lora on CPU to avoid GPU memory OOM.

* Update requirements.

* Change the alpaca_eval annotator to `alpaca_eval_gpt4`

* Make TyDiQA random example selection deterministic.

* Cache openai request for truthfulqa.

* Add templates for other models.

* More evaluation experiments.

* Avoid loading the entire leaderboard."
github.com/allenai/open-instruct,open_instruct/merge_lora.py,2023-10-01T20:21:20Z,Refine the lora merging code. (#67)
github.com/allenai/open-instruct,open_instruct/merge_lora.py,2023-09-15T22:52:16Z,tokenizer update
github.com/allenai/open-instruct,open_instruct/merge_lora.py,2023-09-07T22:30:29Z,Added newer qlora code
github.com/allenai/open-instruct,open_instruct/merge_lora.py,2023-08-30T06:12:38Z,first pass on qlora code.
github.com/allenai/open-instruct,open_instruct/merge_lora.py,2023-06-09T16:25:00Z,initial commit
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/examples/finetuning/ppo_pipeline/merge_peft_adapter.py,2023-11-30T12:32:45Z,Update ppo readme (#771)
github.com/Facico/Chinese-Vicuna,tools/application/chatglm_lora_test.py,2023-03-30T09:42:50Z,fix cpp ctx-len
github.com/shalfun/DrivingDiffusion,diffusers_custom/models/modeling_utils.py,2023-12-15T03:20:13Z,update lib
github.com/The-FinAI/PIXIU,src/interface.py,2023-08-21T07:34:37Z,fix bug of self hosted evaluation
github.com/The-FinAI/PIXIU,src/interface.py,2023-08-08T01:19:08Z,add bertscore
github.com/The-FinAI/PIXIU,src/interface.py,2023-06-11T17:06:07Z,add evaluate and preprocess
github.com/SunzeY/AlphaCLIP,demo/with_llm/llava/model/builder.py,2023-12-27T10:10:55Z,add llava deom
github.com/LiuHC0428/LAW-GPT,src/peft/src/peft/peft_model.py,2023-04-22T04:46:57Z,Add files via upload
github.com/CSHaitao/LexiLaw,src/infer_lora.py,2023-05-19T07:55:28Z,update
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/examples/finetuning/finetune_neuralchat_v3/apply_lora.py,2023-11-22T03:15:37Z,"update code for training neuralchat-7b-v3. (#713)

* update code for train neuralchat-7b-v3.

* update README.
---------

Co-authored-by: VincyZhang <wenxin.zhang@intel.com>"
github.com/LiuHC0428/LAW-GPT,src/peft/src/peft/tuners/lora.py,2023-04-22T04:46:57Z,Add files via upload
github.com/OpenGVLab/InternVL,internvl_chat_llava/llava/model/builder.py,2024-02-23T07:40:24Z,Fix OOM error during training when using transformers==4.36.2
github.com/OpenGVLab/InternVL,internvl_chat_llava/llava/model/builder.py,2024-01-15T13:14:52Z,Rename llava to internvl_chat_llava
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-02-26T03:17:55Z,inverse option added
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-02-12T01:35:15Z,adding Torch Truncated SVD methods
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-02-10T07:24:33Z,adding Torch Truncated SVD methods
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-02-10T07:22:57Z,adding Torch Truncated SVD methods
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-02-10T07:21:33Z,adding Torch Truncated SVD methods
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-02-10T07:17:41Z,adding Torch Truncated SVD methods
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-02-10T07:13:13Z,adding Torch Truncated SVD methods
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-02-10T07:11:15Z,adding Torch Truncated SVD methods
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-02-10T07:07:22Z,adding Torch Truncated SVD methods
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-02-10T06:59:23Z,adding Torch Truncated SVD methods
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-02-10T06:54:08Z,adding Torch Truncated SVD methods
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-02-10T06:41:12Z,adding Torch Truncated SVD methods
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-02-10T06:39:58Z,adding Torch Truncated SVD methods
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-02-10T06:38:07Z,adding Torch Truncated SVD methods
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-02-10T05:47:37Z,adding Torch Truncated SVD methods
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-02-10T04:58:55Z,adding Torch Truncated SVD methods
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-02-10T00:08:17Z,adding Torch Truncated SVD methods
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-02-05T07:45:00Z,LASER release 0.7.4.6
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-02-05T07:44:09Z,LASER release 0.7.4.6
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-01-28T03:11:47Z,prepping for 0.7.4.3 release
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-01-25T23:40:03Z,pushed 0.7.4.2 bug fix
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-01-19T19:12:50Z,updating delta iterator
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-01-19T07:55:46Z,updating delta iterator
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-01-19T07:07:09Z,updating delta iterator
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-01-18T21:55:18Z,updating delta iterator
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-01-18T21:17:03Z,updating delta iterator
github.com/CalculatedContent/WeightWatcher,tests/test.py,2024-01-13T01:55:14Z,unit tests added for PEFT release coming
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-08-22T22:52:46Z,0.7.3.1 bug in SVD Smoothing for Keras layers
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-07-09T19:50:21Z,first pass for 0.7.3
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-07-06T23:17:37Z,first push for 0.7.3
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-06-16T16:31:16Z,added layer_map file for safetensors
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-06-15T06:01:58Z,debugging 0.7.2
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-06-14T15:43:35Z,minor bug fix in tests
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-06-14T00:03:38Z,"added WWDeltaLayerIteator, stil testing"
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-06-13T05:18:54Z,working on 0.7.2 release
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-06-12T22:48:25Z,changelog for 0.7.1.8
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-06-11T15:46:23Z,added supported for list of safetensors; still debugging
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-06-05T04:06:59Z,0.7.1.7
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-05-24T21:46:08Z,issue 255
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-04-11T05:19:50Z,@43 debugging lazy loading
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-04-10T23:19:26Z,@43 debugging lazy loading
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-04-10T18:51:53Z,Debugging #243 on Google Colab
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-04-03T05:21:24Z,deugging #238
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-23T20:58:49Z,issues #230 & #231
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-23T01:26:06Z,issue #229
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-22T22:26:41Z,issue #229
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-22T21:56:31Z,issue #229
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-22T21:34:07Z,issue #229
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-18T16:44:47Z,removed best fit for issue 224
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-18T06:35:27Z,changed DEFAULT_MIB_EVALS = 10
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-18T04:56:53Z,changed DEFAULT_MIB_EVALS = 10
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-18T04:22:55Z,issue 223
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-18T04:08:06Z,issue 223
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-18T02:47:39Z,"cleaning up, fixing error introduceds"
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-18T00:44:26Z,trying to reduce memory on PyTorch Layers
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-15T20:57:36Z,added Mac M1/M2 swtich to numpy
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-11T04:32:21Z,addressing issue 217
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-10T16:42:25Z,fixed Test_PyStateDictFileLayers.test_temp
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-09T23:54:59Z,"Tests for PyStateDictFile added, mostly working"
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-09T23:25:29Z,"Tests for PyStateDictFile added, mostly working"
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-09T21:49:47Z,"Tests for PyStateDictFile added, mostly working"
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-09T19:05:50Z,working on Test_PyStateDictFileLayers; more bug fixes
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-09T18:58:03Z,working on Test_PyStateDictFileLayers; more bug fixes
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-09T16:23:17Z,working on Test_PyStateDictFileLayers; bug fix
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-09T07:17:59Z,working on Test_PyStateDictFileLayers
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-09T00:08:50Z,added unit tests for PyStateDictFileLayers
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-08T17:20:13Z,reverted conv2d_fft
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-04T02:28:18Z,"fixed 2 unit tests:test_get_framework_layer, test_density_fit_on_randomized_W"
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-03T19:22:29Z,"fixed 2 unit tests:test_get_framework_layer, test_density_fit_on_randomized_W"
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-03T07:01:15Z,Fixed fc_layers member in SVD test
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-03T07:01:15Z,Typo fix. (Test still fails)
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-03T07:01:15Z,typo fix
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-03T07:01:15Z,Relaxed Albert tests and set loglevel to WARNING where it had been INFO
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-03T07:01:15Z,"Renamed test classes to conform to convention, make tests distinguishable"
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-03T07:01:15Z,self.assertEquals is deprecated and generates a warning. Use assertEqual
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-03T07:01:15Z,"Added 'detX_val_unrescaled' column, and a test for detX columns"
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-03T05:48:36Z,"added test_pl_directly, 213"
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-03T01:38:20Z,closing various issues
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-02T23:31:10Z,fixed issue #159
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-02T17:57:55Z,"ww2x flag depcreated, changed to pool"
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-02T06:18:48Z,debugging fix_fingers
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-02T00:24:56Z,debugging fix_fingers
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-01T22:09:18Z,debugging fix_fingers
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-01T18:23:27Z,bug in main code
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-01T18:00:16Z,fixed xmax not being passed from fit_clipped_powerlaw
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-03-01T02:08:12Z,apply_powerlaw() got renamed by accident to _powerlaw
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-02-28T08:10:54Z,"user can now specify xmax=-1, useful for single layers"
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-02-28T08:06:38Z,debugging conv2d fft
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-02-28T07:11:06Z,"user can now specify xmax=-1, useful for single layers"
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-02-28T05:32:47Z,debugging fix fingers for 0.7 release
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-02-27T21:52:22Z,debugging 0.6.6
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-02-25T00:26:09Z,0.6.5 alpha tests not moved correctly
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-02-24T05:31:59Z,added test_keras_model_with_no_bias to TestKeras for issue #201
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-02-24T04:01:21Z,going to merge into 0.6.5 as preview for 0.6
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-02-23T23:10:16Z,cleaning up unit tests
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-02-23T20:31:23Z,moved alpha calcualtions to seperate VGG test
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-02-23T19:58:43Z,moved alpha calcualtions to seperate VGG test
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-02-23T18:42:21Z,changed VGG tests to use older version tests
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-02-23T07:22:09Z,changed VGG tests to use older version tests
github.com/CalculatedContent/WeightWatcher,tests/test.py,2023-02-23T07:16:16Z,changed VGG tests to use older version tests
github.com/CSHaitao/LexiLaw,inference_lora.py,2023-05-20T11:37:08Z,update
github.com/CSHaitao/LexiLaw,inference_lora.py,2023-05-20T07:57:18Z,update
github.com/danielgross/LlamaAcademy,export_hf.py,2023-04-19T16:28:29Z,inital
github.com/ztxz16/fastllm,test/cmmlu/qwen.py,2023-08-16T02:38:30Z,"修复QWenBase模型generate时停不下来的问题；
增加QWen-7B模型的cmmlu测试结果"
github.com/agiresearch/OpenAGI,benchmark_tasks/generate_model_seq_llama.py,2024-02-20T05:53:17Z, fix: decoder-only models' constraint generation score calculation
github.com/agiresearch/OpenAGI,benchmark_tasks/generate_model_seq_llama.py,2023-05-26T18:46:24Z,add llama/vicuna finetune
github.com/HarderThenHarder/transformers_tasks,LLM/chatglm_finetune/peft-chatglm/src/peft/tuners/prompt_tuning.py,2023-08-02T12:16:49Z,add LLMs Trainer
github.com/AGI-Edgerunners/LLM-Adapters,peft/src/peft/tuners/bottleneck.py,2023-04-10T07:57:10Z,support ChatGLM
github.com/AGI-Edgerunners/LLM-Adapters,peft/src/peft/tuners/bottleneck.py,2023-04-07T11:14:17Z,fixed inference error without loading int8 model
github.com/AGI-Edgerunners/LLM-Adapters,peft/src/peft/tuners/bottleneck.py,2023-04-05T11:37:06Z,for easier using of AdapterP
github.com/AGI-Edgerunners/LLM-Adapters,peft/src/peft/tuners/bottleneck.py,2023-04-05T11:18:41Z,support gpt-neo for bottlenck and parallel adapters
github.com/AGI-Edgerunners/LLM-Adapters,peft/src/peft/tuners/bottleneck.py,2023-04-04T02:44:45Z,fixed a forward bug with autocast for bottleneck adapters
github.com/AGI-Edgerunners/LLM-Adapters,peft/src/peft/tuners/bottleneck.py,2023-04-01T19:27:16Z,support opt and solve autocast bug
github.com/AGI-Edgerunners/LLM-Adapters,peft/src/peft/tuners/bottleneck.py,2023-04-01T12:47:14Z,support bottleneck adapters for BLOOM
github.com/AGI-Edgerunners/LLM-Adapters,peft/src/peft/tuners/bottleneck.py,2023-04-01T11:59:26Z,support parallel adapter and gpt-j model
github.com/AGI-Edgerunners/LLM-Adapters,peft/src/peft/tuners/bottleneck.py,2023-03-31T12:49:40Z,support bottleneck adapters
github.com/beyondguo/LLM-Tuning,RLHF/rl_training.py,2023-07-20T06:57:45Z,baichuan-rlhf!
github.com/beyondguo/LLM-Tuning,RLHF/rl_training.py,2023-07-13T07:51:03Z,rlhf with baichuan
github.com/beyondguo/LLM-Tuning,RLHF/rl_training.py,2023-07-12T09:58:16Z,fix a sample-related bug
github.com/beyondguo/LLM-Tuning,RLHF/rl_training.py,2023-07-10T14:36:27Z,fix
github.com/beyondguo/LLM-Tuning,RLHF/rl_training.py,2023-07-09T02:36:13Z,make it runable
github.com/beyondguo/LLM-Tuning,RLHF/rl_training.py,2023-07-08T15:22:16Z,custom for baichuan
github.com/beyondguo/LLM-Tuning,RLHF/rl_training.py,2023-07-06T11:46:07Z,code reading with some comments
github.com/intel/intel-extension-for-transformers,intel_extension_for_transformers/neural_chat/examples/finetuning/multi_modal/eval/mmmu_eval/run_llava.py,2024-02-06T11:56:39Z,[NeuralChat] Enable llava mmmu evaluation on Gaudi2. (#1259)
github.com/shalfun/DrivingDiffusion,diffusers_custom/pipelines/pipeline_utils.py,2023-12-15T03:20:13Z,update lib
github.com/ztxz16/fastllm,test/cmmlu/baichuan.py,2023-08-13T16:48:01Z,update test for baichuan
github.com/SeungyounShin/Llama2-Code-Interpreter,eval/inference.py,2023-08-21T18:58:29Z,Feat (Finetunning&Eval) add human-eval
github.com/SeungyounShin/Llama2-Code-Interpreter,eval/inference.py,2023-08-21T18:39:43Z,Feat (Eval) add human-eval
github.com/SeungyounShin/Llama2-Code-Interpreter,eval/inference.py,2023-08-12T14:28:59Z,Feat (Llama2 Finetuning) Add inference code for test
github.com/SeungyounShin/Llama2-Code-Interpreter,eval/inference.py,2023-08-10T18:27:07Z,Feat (Llama2 Finetuning) Add Fully working Finetunning and Inference code
github.com/codefuse-ai/codefuse-devops-eval,src/models/base_model.py,2023-12-13T06:24:20Z,add funccall evalution features
github.com/SkunkworksAI/BakLLaVA,llava/model/builder.py,2023-10-10T23:07:33Z,Mistral
github.com/SkunkworksAI/BakLLaVA,llava/model/builder.py,2023-10-10T20:48:22Z,a
github.com/codefuse-ai/codefuse-devops-eval,src/models/qwen_model.py,2023-12-13T06:24:20Z,add funccall evalution features
github.com/AGI-Edgerunners/LLM-Adapters,peft/src/peft/tuners/prompt_tuning.py,2023-03-31T12:49:40Z,support bottleneck adapters
github.com/CSHaitao/LexiLaw,inference_ptuning.py,2023-05-20T11:37:08Z,update
github.com/CSHaitao/LexiLaw,inference_ptuning.py,2023-05-20T07:57:18Z,update
github.com/AutoGPTQ/AutoGPTQ,tests/test_peft_conversion.py,2024-02-13T16:06:07Z,"Use ruff for linting (#537)

* lint

* add back init

* remove pyproject.toml that automatically triggers build isolation

* install torch

* maybe use 3.9?

* debug

* upgrade setuptools

* maybe ubuntu 22.04?

* wheel

* working now?

* typog

* indent

* fix indent

* do not use powershell

* free space

* fix cuda path

* prints

* where is conda?

* should finally work

* fix

* final fixN

* arch ist

* typog

* add quality extra

* last fix"
github.com/AutoGPTQ/AutoGPTQ,tests/test_peft_conversion.py,2023-10-27T07:12:16Z,"PEFT initialization fix (#361)

* Initial code for GPTQLoraLinear initialization

* Working AdaLora

* remove unused methods and pin peft

---------

Co-authored-by: Felix Marty <9808326+fxmarty@users.noreply.github.com>"
github.com/abacusai/Long-Context,python/eval/longeval/utils.py,2023-07-27T21:16:33Z,Add eval and benchmark files for Lines and WikiQA
github.com/star-whale/starwhale,example/llm-leaderboard/src/llm/base.py,2024-02-05T07:30:03Z,example(LLM): add mistral 7b+vllm example for llm-leaderboard (#3133)
github.com/star-whale/starwhale,example/llm-leaderboard/src/llm/base.py,2023-08-30T02:22:10Z,example(LLM): tune llm leaderboard for explanaion and readme (#2670)
github.com/star-whale/starwhale,example/llm-leaderboard/src/llm/base.py,2023-08-25T06:08:05Z,example(LLM): support llama2 chinese models (#2648)
github.com/star-whale/starwhale,example/llm-leaderboard/src/llm/base.py,2023-08-23T02:52:01Z,example(LLM): add llm leaderboard examples (#2610)
github.com/shibing624/textgen,textgen/gpt/merge_peft_adapter.py,2023-11-05T06:51:05Z,update merge.
github.com/shibing624/textgen,textgen/gpt/merge_peft_adapter.py,2023-11-03T04:40:29Z,update merge.
github.com/shibing624/textgen,textgen/gpt/merge_peft_adapter.py,2023-09-05T14:32:26Z,update ddp env.
github.com/shibing624/textgen,textgen/gpt/merge_peft_adapter.py,2023-09-04T12:34:17Z,update gpt merge adapter.
github.com/shibing624/textgen,textgen/gpt/merge_peft_adapter.py,2023-06-19T08:05:33Z,fixed https://github.com/shibing624/textgen/issues/46
github.com/shibing624/textgen,textgen/gpt/merge_peft_adapter.py,2023-06-16T11:54:21Z,update gpt model.
github.com/MeetKai/functionary,functionary/train/merge_lora_weight.py,2023-11-27T04:31:25Z,fix issues based on comments from Jeffrey
github.com/MeetKai/functionary,functionary/train/merge_lora_weight.py,2023-10-31T06:30:19Z,clean code
github.com/MeetKai/functionary,functionary/train/merge_lora_weight.py,2023-10-20T07:42:28Z,update inference code
github.com/MeetKai/functionary,functionary/train/merge_lora_weight.py,2023-10-16T06:53:06Z,update train_lora
github.com/haonan-li/CMMLU,src/chinese_llama_alpaca.py,2023-07-28T16:24:52Z,update ABCD token mapping
github.com/haonan-li/CMMLU,src/chinese_llama_alpaca.py,2023-06-16T12:48:28Z,init
github.com/horseee/DeepCache,DeepCache/svd/pipeline_utils.py,2023-12-20T21:05:12Z,[ADD] code for SVD
github.com/Duxiaoman-DI/XuanYuan,FinanceIQ/src/hf_causal_model.py,2023-09-22T13:24:58Z,add FinanceIQ code
github.com/pjlab-sys4nlp/llama-moe,smoe/utils/merge_llama_with_lora.py,2023-08-01T02:51:06Z,update format
github.com/pjlab-sys4nlp/llama-moe,smoe/utils/merge_llama_with_lora.py,2023-07-27T06:53:19Z,update fpt scripts
github.com/intel/neural-compressor,examples/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py,2024-02-28T14:11:40Z,"align the parameters between 2x and 3x example (#1636)

Signed-off-by: chensuyue <suyue.chen@intel.com>"
github.com/intel/neural-compressor,examples/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py,2024-01-28T12:55:46Z,"Remove gptq_debug options in examples (#1569)

Signed-off-by: YIYANGCAI <yiyang.cai@intel.com>
Signed-off-by: chensuyue <suyue.chen@intel.com>"
github.com/intel/neural-compressor,examples/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py,2024-01-23T12:45:27Z,"Support static_groups options in GPTQ API (#1478)

Signed-off-by: YIYANGCAI <yiyang.cai@intel.com>"
github.com/intel/neural-compressor,examples/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py,2024-01-17T01:55:31Z,"Modify gptq example logic (#1545)

Signed-off-by: YIYANGCAI <yiyang.cai@intel.com>"
github.com/intel/neural-compressor,examples/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py,2024-01-12T05:12:38Z,fix trust remote for llm examples (#1537)
github.com/intel/neural-compressor,examples/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py,2023-12-28T02:31:16Z,"add code-generaion evaluation for woq gptq (#1475)

Signed-off-by: changwangss <chang1.wang@intel.com>
Signed-off-by: YIYANGCAI <yiyang.cai@intel.com>
Signed-off-by: chensuyue <suyue.chen@intel.com>"
github.com/intel/neural-compressor,examples/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py,2023-12-15T07:05:37Z,"format fix for llm example scripts (#1474)

Signed-off-by: chensuyue <suyue.chen@intel.com>"
github.com/intel/neural-compressor,examples/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py,2023-12-06T13:00:27Z,"add llm evaluate for language modeling (#1350)

Signed-off-by: Xin He <xin3.he@intel.com>
Signed-off-by: YIYANGCAI <yiyang.cai@intel.com>
Signed-off-by: chensuyue <suyue.chen@intel.com>"
github.com/datadreamer-dev/DataDreamer,src/llms/hf_transformers.py,2024-02-02T23:53:40Z,Update docs [release]
github.com/datadreamer-dev/DataDreamer,src/llms/hf_transformers.py,2024-02-02T04:00:55Z,Update docs [release]
github.com/datadreamer-dev/DataDreamer,src/llms/hf_transformers.py,2024-02-01T03:26:02Z,Release commit [release]
github.com/michael-wzhu/PromptCBLUE,src/ft_chatglm_lora/main.py,2023-07-12T09:55:23Z,2023-07-12: change generation config for batched generation
github.com/michael-wzhu/PromptCBLUE,src/ft_chatglm_lora/main.py,2023-07-12T06:41:53Z,2023-07-12: change generation config for batched generation
github.com/michael-wzhu/PromptCBLUE,src/ft_chatglm_lora/main.py,2023-07-03T02:56:20Z,2023-07-03: update src/data/CBLUE任务改造说明与举例.md
github.com/michael-wzhu/PromptCBLUE,src/ft_chatglm_lora/main.py,2023-06-26T10:20:19Z,2023-06-26: update train & eval for p-tuning and lora
github.com/michael-wzhu/PromptCBLUE,src/ft_chatglm_lora/main.py,2023-06-26T10:19:53Z,2023-06-26: update train & eval for p-tuning
github.com/michael-wzhu/PromptCBLUE,src/ft_chatglm_lora/main.py,2023-06-26T10:06:03Z,2023-06-26: update train & eval for p-tuning
github.com/michael-wzhu/PromptCBLUE,src/ft_chatglm_lora/main.py,2023-05-27T03:27:07Z,2023/05/27: update main.py; add main_offline.py
github.com/michael-wzhu/PromptCBLUE,src/ft_chatglm_lora/main.py,2023-05-27T03:23:46Z,2023/05/27: update main.py; add main_offline.py
github.com/michael-wzhu/PromptCBLUE,src/ft_chatglm_lora/main.py,2023-05-27T03:22:03Z,2023/05/21: update wechat qr code
github.com/michael-wzhu/PromptCBLUE,src/ft_chatglm_lora/main.py,2023-05-19T07:57:08Z,2023-05-19: modify data proc for eval; change dingding group qrcode;
github.com/michael-wzhu/PromptCBLUE,src/ft_chatglm_lora/main.py,2023-05-10T05:37:48Z,2023-05-10: add code for chatglm+lora ft
github.com/michael-wzhu/PromptCBLUE,src/ft_chatglm_lora/main.py,2023-05-10T01:45:41Z,2023-05-10: add qr code
github.com/zjunlp/KnowLM,examples/generate_lora_web.py,2023-12-06T09:10:52Z,"fix issue #99,"
github.com/zjunlp/KnowLM,examples/generate_lora_web.py,2023-11-08T04:52:35Z,fix bugs in #96 and support multi-gpu inference
github.com/zjunlp/KnowLM,examples/generate_lora_web.py,2023-11-03T08:45:53Z,fix bug #96
github.com/zjunlp/KnowLM,examples/generate_lora_web.py,2023-10-13T12:32:27Z,fix bugs #81 when run in cpu
github.com/zjunlp/KnowLM,examples/generate_lora_web.py,2023-08-13T07:44:53Z,Update generate_lora_web.py
github.com/zjunlp/KnowLM,examples/generate_lora_web.py,2023-06-12T14:32:16Z,Update generate_lora_web.py
github.com/zjunlp/KnowLM,examples/generate_lora_web.py,2023-05-30T08:02:19Z,Update generate_lora_web.py
github.com/zjunlp/KnowLM,examples/generate_lora_web.py,2023-05-29T14:38:51Z,Update generate_lora_web.py
github.com/zjunlp/KnowLM,examples/generate_lora_web.py,2023-05-25T04:19:23Z,Create generate_lora_web.py
github.com/LiuHC0428/LAW-GPT,src/peft/src/peft/tuners/bottleneck.py,2023-04-22T04:46:57Z,Add files via upload
github.com/LC1332/Chat-Haruhi-Suzumiya,ChatHaruhi2.0/ChatHaruhi/ChatGLM2GPT.py,2023-11-01T04:54:05Z,fix initializing bug
github.com/LC1332/Chat-Haruhi-Suzumiya,ChatHaruhi2.0/ChatHaruhi/ChatGLM2GPT.py,2023-10-24T06:23:28Z,"remove all jsonl that used in Haruhi 1.0, update into 2.0 , the jsonl will still kept at Legacy-Haruhi-1.0 project"
github.com/IlyaGusev/rulm,self_instruct/src/tools/merge_lora.py,2023-10-30T10:28:14Z,Update merge_lora.py
github.com/IlyaGusev/rulm,self_instruct/src/tools/merge_lora.py,2023-09-09T14:23:42Z,Flake8 refactoring
github.com/IlyaGusev/rulm,self_instruct/src/tools/merge_lora.py,2023-09-09T13:55:42Z,Removing unused scripts
github.com/zjunlp/KnowLM,tools/export_hf_checkpoint.py,2023-07-01T08:40:46Z,doc: add vLLM serving doc
github.com/michael-wzhu/Chinese-LlaMA2,peft/tuners/prompt_tuning.py,2023-07-19T06:57:59Z,2023/07/19 add further pretraining code
github.com/yangjianxin1/Firefly-LLaMA2-Chinese,script/merge_lora.py,2023-09-18T17:00:07Z,合并模型权重
github.com/datadreamer-dev/DataDreamer,src/trainers/train_hf_ppo.py,2024-02-01T03:26:02Z,Release commit [release]
github.com/basetenlabs/alpaca-7b-truss,model/model.py,2023-05-15T15:18:11Z,Update model.py to use 2 beams instead of 4
github.com/basetenlabs/alpaca-7b-truss,model/model.py,2023-04-27T04:36:46Z,Use docal data
github.com/basetenlabs/alpaca-7b-truss,model/model.py,2023-03-22T04:50:21Z,init
github.com/beyondguo/LLM-Tuning,web_demo/src/toolkit.py,2023-07-25T05:02:02Z,web_demo
github.com/airaria/Visual-Chinese-LLaMA-Alpaca,scripts/inference/inference.py,2023-07-14T02:24:35Z,support pure text instruction inference
github.com/airaria/Visual-Chinese-LLaMA-Alpaca,scripts/inference/inference.py,2023-07-07T03:34:16Z,update README.md and inference.py
github.com/airaria/Visual-Chinese-LLaMA-Alpaca,scripts/inference/inference.py,2023-07-07T02:27:55Z,init
github.com/michael-wzhu/PromptCBLUE,peft/tuners/prompt_tuning.py,2023-07-18T09:37:52Z,2023-07-18: add LlaMA + lora code; add vllm model serving
github.com/CSHaitao/LexiLaw,inference_finetune.py,2023-05-20T11:37:08Z,update
github.com/CSHaitao/LexiLaw,inference_finetune.py,2023-05-20T07:57:18Z,update
github.com/johnsmith0031/alpaca_lora_4bit,src/alpaca_lora_4bit/autograd_4bit.py,2023-08-02T10:08:48Z,fix mat size issue
github.com/johnsmith0031/alpaca_lora_4bit,src/alpaca_lora_4bit/autograd_4bit.py,2023-06-26T08:36:00Z,Use zero initializer for bias weights
github.com/johnsmith0031/alpaca_lora_4bit,src/alpaca_lora_4bit/autograd_4bit.py,2023-06-05T09:26:14Z,fix bug
github.com/johnsmith0031/alpaca_lora_4bit,src/alpaca_lora_4bit/autograd_4bit.py,2023-06-05T09:21:43Z,add auto apply monkeypatch when loading lora
github.com/johnsmith0031/alpaca_lora_4bit,src/alpaca_lora_4bit/autograd_4bit.py,2023-06-02T07:47:08Z,switch to older version of faster kernel as default
github.com/johnsmith0031/alpaca_lora_4bit,src/alpaca_lora_4bit/autograd_4bit.py,2023-06-02T02:19:22Z,set disable_bias=False as default
github.com/johnsmith0031/alpaca_lora_4bit,src/alpaca_lora_4bit/autograd_4bit.py,2023-05-30T02:29:52Z,add 2bit support
github.com/johnsmith0031/alpaca_lora_4bit,src/alpaca_lora_4bit/autograd_4bit.py,2023-04-22T15:01:39Z,optimized matmul for v2 model
github.com/johnsmith0031/alpaca_lora_4bit,src/alpaca_lora_4bit/autograd_4bit.py,2023-04-22T12:53:52Z,fix bug
github.com/johnsmith0031/alpaca_lora_4bit,src/alpaca_lora_4bit/autograd_4bit.py,2023-04-22T07:23:43Z,fix bug and remove bnb
github.com/johnsmith0031/alpaca_lora_4bit,src/alpaca_lora_4bit/autograd_4bit.py,2023-04-19T10:49:58Z,fix bug
github.com/johnsmith0031/alpaca_lora_4bit,src/alpaca_lora_4bit/autograd_4bit.py,2023-04-17T16:47:53Z,fix gptq install
github.com/johnsmith0031/alpaca_lora_4bit,src/alpaca_lora_4bit/autograd_4bit.py,2023-04-17T16:35:05Z,fix imports on cuda/triton since they aren't in __init__
github.com/johnsmith0031/alpaca_lora_4bit,src/alpaca_lora_4bit/autograd_4bit.py,2023-04-17T16:35:05Z,fix circular import and add monkeypatch submodule in setup
github.com/johnsmith0031/alpaca_lora_4bit,src/alpaca_lora_4bit/autograd_4bit.py,2023-04-17T16:35:05Z,fix conditional
github.com/johnsmith0031/alpaca_lora_4bit,src/alpaca_lora_4bit/autograd_4bit.py,2023-04-17T16:35:01Z,"make things installable, refactor things"
github.com/IBM/Dromedary,utils/convert_hf_weights_to_llama_ckpt.py,2023-10-05T11:47:11Z,upgrade to Dromedary-2
github.com/IBM/Dromedary,utils/convert_hf_weights_to_llama_ckpt.py,2023-05-04T09:42:43Z,update README
github.com/IBM/Dromedary,utils/convert_hf_weights_to_llama_ckpt.py,2023-05-04T05:30:20Z,clean step4 code
github.com/locuslab/wanda,dense_ft/trainer.py,2023-11-03T05:37:59Z,add dense ft
github.com/jianzhnie/Efficient-Tuning-LLMs,chatllms/utils/apply_lora.py,2023-07-30T02:46:47Z,update model server
github.com/jianzhnie/Efficient-Tuning-LLMs,chatllms/utils/apply_lora.py,2023-07-28T10:57:53Z,update server
github.com/jianzhnie/Efficient-Tuning-LLMs,chatllms/utils/apply_lora.py,2023-07-28T09:29:48Z,update apply_lora
github.com/jianzhnie/Efficient-Tuning-LLMs,chatllms/utils/apply_lora.py,2023-07-12T08:09:42Z,update data utils
github.com/jianzhnie/Efficient-Tuning-LLMs,chatllms/utils/apply_lora.py,2023-07-11T07:22:56Z,update apply_lora
github.com/jianzhnie/Efficient-Tuning-LLMs,chatllms/utils/apply_lora.py,2023-06-20T03:46:21Z,update apply_lora
github.com/jianzhnie/Efficient-Tuning-LLMs,chatllms/utils/apply_lora.py,2023-06-17T14:58:00Z,Formatting the code
github.com/jianzhnie/Efficient-Tuning-LLMs,chatllms/utils/apply_lora.py,2023-06-17T10:23:15Z,"git commit -m ""Reorganized the code"""
github.com/LiuHC0428/LAW-GPT,src/peft/src/peft/tuners/prompt_tuning.py,2023-04-22T04:46:57Z,Add files via upload
github.com/johnsmith0031/alpaca_lora_4bit,src/alpaca_lora_4bit/server/server.py,2023-04-26T06:38:57Z,fix _SentinelTokenStoppingCriteria
github.com/johnsmith0031/alpaca_lora_4bit,src/alpaca_lora_4bit/server/server.py,2023-04-26T05:13:54Z,fix bug
github.com/johnsmith0031/alpaca_lora_4bit,src/alpaca_lora_4bit/server/server.py,2023-04-26T04:57:14Z,add server
github.com/OpenGVLab/Multi-Modality-Arena,tiny_lvlm_evaluation/models/bliva/models/bliva_vicuna7b_lora.py,2023-10-16T07:03:03Z,Add Ability-level Benchmark
github.com/CMKRG/QiZhenGPT,gradio_cama-demo.py,2023-06-02T09:57:18Z,release instruction tuning on CaMA
github.com/datadreamer-dev/DataDreamer,src/trainers/_train_hf_base.py,2024-02-13T06:50:43Z,v1 HF tags
github.com/datadreamer-dev/DataDreamer,src/trainers/_train_hf_base.py,2024-02-02T04:00:55Z,Update docs [release]
github.com/datadreamer-dev/DataDreamer,src/trainers/_train_hf_base.py,2024-02-01T03:26:02Z,Release commit [release]
github.com/SkunkworksAI/hydra-moe,hydra_moe/utils.py,2023-09-22T07:20:28Z,Merge branch 'v1_alpha' into alpha
github.com/SkunkworksAI/hydra-moe,hydra_moe/utils.py,2023-09-21T22:16:39Z,(artem) refactor part 3
github.com/SkunkworksAI/hydra-moe,hydra_moe/utils.py,2023-09-21T22:02:11Z,refactor pt 2
github.com/shawroad/NLP_pytorch_project,LLM/peft_example/sequence_cls/bert/run_infer_bert_lora.py,2023-04-21T03:21:49Z,add accelerate
github.com/BYU-PRISM/GEKKO,docs/llm/phi2-gekko-eval.py,2024-02-04T15:16:21Z,Add files via upload
github.com/LC1332/Chat-Haruhi-Suzumiya,ChatHaruhi2.0/ChatHaruhi/Qwen118k2GPT.py,2023-12-21T07:22:47Z,move stable Haruhi 2.0 version into main repo
github.com/LC1332/Chat-Haruhi-Suzumiya,ChatHaruhi2.0/ChatHaruhi/BaiChuan2GPT.py,2023-10-24T06:23:28Z,"remove all jsonl that used in Haruhi 1.0, update into 2.0 , the jsonl will still kept at Legacy-Haruhi-1.0 project"
github.com/intel/neural-compressor,examples/3.x_api/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py,2024-02-28T14:11:40Z,"align the parameters between 2x and 3x example (#1636)

Signed-off-by: chensuyue <suyue.chen@intel.com>"
github.com/intel/neural-compressor,examples/3.x_api/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py,2024-02-21T05:03:23Z,"Fix GPTQ/RTN 3.x example & fix asym quantize  (#1611)

Signed-off-by: Kaihui-intel <kaihui.tang@intel.com>
Signed-off-by: Tang, Kaihui <kaihui.tang@intel.com>
Co-authored-by: chen, suyue <suyue.chen@intel.com>"
github.com/intel/neural-compressor,examples/3.x_api/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py,2024-02-05T15:53:35Z,"remove 3.x_api example (#1607)

Signed-off-by: Kaihui-intel <kaihui.tang@intel.com>"
github.com/intel/neural-compressor,examples/3.x_api/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py,2024-02-05T02:30:58Z,"update GPTQConfig and UTs (#1587)

Signed-off-by: xin3he <xin3.he@intel.com>"
github.com/intel/neural-compressor,examples/3.x_api/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py,2024-01-19T02:54:50Z,"Rename RTNWeightOnlyConfig to RTNConfig (#1551)

* Rename RTNWeightOnlyConfig to RTNConfig

Signed-off-by: xin3he <xin3.he@intel.com>"
github.com/intel/neural-compressor,examples/3.x_api/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py,2024-01-02T02:02:04Z,"Add double quant example (#1498)

Signed-off-by: Kaihui-intel <kaihui.tang@intel.com>
Signed-off-by: Tang, Kaihui <kaihui.tang@intel.com>
Signed-off-by: chensuyue <suyue.chen@intel.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: chensuyue <suyue.chen@intel.com>"
github.com/lyuchenyang/Macaw-LLM,run_clm_llms_inference.py,2023-07-01T03:46:24Z,Update run_clm_llms_inference.py
github.com/lyuchenyang/Macaw-LLM,run_clm_llms_inference.py,2023-07-01T03:45:32Z,Update run_clm_llms_inference.py
github.com/lyuchenyang/Macaw-LLM,run_clm_llms_inference.py,2023-07-01T03:41:28Z,Update run_clm_llms_inference.py
github.com/lyuchenyang/Macaw-LLM,run_clm_llms_inference.py,2023-06-14T10:31:48Z,Update run_clm_llms_inference.py
github.com/lyuchenyang/Macaw-LLM,run_clm_llms_inference.py,2023-05-31T01:58:08Z,Update run_clm_llms_inference.py
github.com/lyuchenyang/Macaw-LLM,run_clm_llms_inference.py,2023-05-31T01:21:20Z,Update run_clm_llms_inference.py
github.com/lyuchenyang/Macaw-LLM,run_clm_llms_inference.py,2023-05-30T11:43:53Z,Update run_clm_llms_inference.py
github.com/lyuchenyang/Macaw-LLM,run_clm_llms_inference.py,2023-05-30T07:45:07Z,Update run_clm_llms_inference.py
github.com/lyuchenyang/Macaw-LLM,run_clm_llms_inference.py,2023-05-26T05:39:09Z,Create run_clm_llms_inference.py
github.com/penghao-wu/vstar,VisualSearch/model/llava/model/builder.py,2023-12-18T04:32:06Z,init code
github.com/kakaobrain/honeybee,pipeline/interface.py,2023-12-20T10:08:51Z,Add inference code
github.com/michael-wzhu/Chinese-LlaMA2,src/sft/web_service_with_lora.py,2023-07-24T14:53:56Z,2023/07/24 update readme
github.com/michael-wzhu/Chinese-LlaMA2,src/sft/web_service_with_lora.py,2023-07-22T07:04:40Z,2023/07/22 add sft and serving code
github.com/sail-sg/lorahub,train_model.py,2023-10-17T01:10:10Z,Add files via upload
github.com/shawroad/NLP_pytorch_project,LLM/peft_example/sequence_cls/bert/run_infer_bert_p_tuning.py,2023-04-21T03:21:49Z,add accelerate
github.com/shibing624/textgen,examples/gpt/inference_multigpu_demo.py,2023-09-08T11:14:52Z,update remove file.
github.com/shibing624/textgen,examples/gpt/inference_multigpu_demo.py,2023-09-07T07:31:29Z,del truncation.
github.com/shibing624/textgen,examples/gpt/inference_multigpu_demo.py,2023-09-07T07:05:55Z,to DDP
github.com/shibing624/textgen,examples/gpt/inference_multigpu_demo.py,2023-09-07T06:12:41Z,update show case.
github.com/shibing624/textgen,examples/gpt/inference_multigpu_demo.py,2023-09-07T05:46:42Z,update save to jsonl.
github.com/shibing624/textgen,examples/gpt/inference_multigpu_demo.py,2023-09-07T05:10:44Z,update inference multi gpu token.
github.com/shibing624/textgen,examples/gpt/inference_multigpu_demo.py,2023-09-07T05:04:38Z,update inference multi gpu.
github.com/shibing624/textgen,examples/gpt/inference_multigpu_demo.py,2023-09-07T05:04:13Z,update inference multi gpu.
github.com/shibing624/textgen,examples/gpt/inference_multigpu_demo.py,2023-09-07T05:02:45Z,update inference multi gpu.
github.com/shibing624/textgen,examples/gpt/inference_multigpu_demo.py,2023-09-07T04:50:34Z,update inference multi gpu.
github.com/shibing624/textgen,examples/gpt/inference_multigpu_demo.py,2023-09-07T03:44:55Z,update inference multi.
github.com/shibing624/textgen,examples/gpt/inference_multigpu_demo.py,2023-09-07T03:16:25Z,update inference
github.com/shibing624/textgen,examples/gpt/inference_multigpu_demo.py,2023-09-07T03:07:32Z,update inference with multi gpu.
github.com/codefuse-ai/codefuse-devops-eval,src/models/internlm_model.py,2023-12-13T06:24:20Z,add funccall evalution features
github.com/CambioML/pykoi-rlhf-finetuned-transformers,pykoi/chat/llm/peft_huggingface.py,2023-12-23T01:54:59Z,add TODO and run black and isort
github.com/CambioML/pykoi-rlhf-finetuned-transformers,pykoi/chat/llm/peft_huggingface.py,2023-11-07T09:15:13Z,"update peft sft training, demo default model, and inference result"
github.com/CambioML/pykoi-rlhf-finetuned-transformers,pykoi/chat/llm/peft_huggingface.py,2023-09-13T05:50:52Z,"refactor pykoi into base, huggingface, rag, and rlhf"
github.com/CambioML/pykoi-rlhf-finetuned-transformers,pykoi/chat/llm/peft_huggingface.py,2023-08-07T06:25:05Z,"refactor llm and db into chat folder, refactor their tests into chat folder, and update docs rst"
github.com/star-whale/starwhale,example/llm-finetune/models/baichuan2/evaluation.py,2024-01-16T02:58:20Z,example(LLM): use `@starwhale.argument` feature to refactor llm finetune examples (#3125)
github.com/star-whale/starwhale,example/llm-finetune/models/baichuan2/evaluation.py,2023-11-21T05:41:04Z, example: add llm-finetune example: baichuan2 (#2973)
github.com/AGI-Edgerunners/LLM-Adapters,peft/tests/test_peft_model.py,2023-03-31T12:49:40Z,support bottleneck adapters
github.com/codefuse-ai/codefuse-devops-eval,src/models/baichuan_model.py,2023-12-13T06:24:20Z,add funccall evalution features
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2024-02-27T14:03:21Z,"SQ refactor (#1633)

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2024-02-27T06:22:06Z,"Fixed smooth quant's UTs (#1631)

Signed-off-by: yiliu30 <yi4.liu@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2024-01-22T07:47:27Z,"fix ipex stats bug (#1555)

Signed-off-by: xin3he <xin3.he@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2024-01-18T03:03:59Z,"Stop the tuning process early when enabling smooth quant (#1542)

Signed-off-by: yiliu30 <yi4.liu@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-12-22T14:03:35Z,"Narrow the tuning space of sq auto-tune (#1489)

Signed-off-by: yiliu30 <yi4.liu@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-12-13T01:17:50Z,"Lyt/blockwise (#1441)

* [Algo] blockwise tuning

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>

* [Algo] code update

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>

* [Algo] sq argument update

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>

* [Algo] log update

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* [Algo] code update

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* [Algo] fix bugs

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* [Algo] log update

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* [Algo] enable blockwise on Llama models

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>

* [Algo] enable blockwise on Llama models

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* [Algo] code update

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* [Algo] format code

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>

* [Algo] fix bug

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>

* [Algo] add ut

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* [Algo] fix format issue

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>

* [Algo] log update

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* [Algo] move do_blockwise arg

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* [Algo] fix bug

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* [Algo] fix bug

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* [Algo] fix bug

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* [Algo] fix bug

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>

* [Algo] fix bug

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>

---------

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-12-08T07:13:48Z,"fix peft issue in ut (#1450)

Signed-off-by: Xin He <xin3.he@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-11-22T05:40:44Z,"support for restoring ipex model from json (#1405)

Signed-off-by: Kaihui-intel <kaihui.tang@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-11-16T12:42:23Z,"[SmoothQuant] make alpha search space a config argument (#1392)

Enhance SmoothQuant with tunable alpha search space

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-11-16T06:43:47Z,"[SmoothQuant] make weight_clipping a default_on option (#1386)

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-11-06T05:35:17Z,"support peft model quantization with SmoothQuant (#1282)

Peft model will use below arch: Linears in Linear. This pull request supports this arch with smoothquant.
```
(v): Linear(                                                                                                                          
  in_features=32, out_features=32, bias=False                                                                                         
  (lora_dropout): ModuleDict(                                                                                                         
    (default): Dropout(p=0.1, inplace=False)                                                                                          
  )                                                                                                                                   
  (lora_A): ModuleDict(                                                                                                               
    (default): Linear(in_features=32, out_features=8, bias=False)                                                                     
  )                                                                                                                                   
  (lora_B): ModuleDict(                                                                                                               
    (default): Linear(in_features=8, out_features=32, bias=False)                                                                     
  )                                                                                                                                   
  (lora_embedding_A): ParameterDict()                                                                                                 
  (lora_embedding_B): ParameterDict()  
```
BTW,
when IPEX version<=1.13, HistogramObserver doesn't support asym scheme, the zero_point is 0 for asym uint8, while the MinMaxObserver works well.
Also,
IPEX SmoothQuant Observer can only use save/load_qconf_summary once. The save_qconf_summary API will freeze the scale used in model and calibration won't work anymore. The load_qconf_summary will overwrite the scales used in model but only work in the first call. Here we implement normal observer to workaround this issue.
---------

Signed-off-by: changwangss <chang1.wang@intel.com>
Signed-off-by: Xin He <xin3.he@intel.com>
Signed-off-by: y <xin3.he@intel.com>
Signed-off-by: chensuyue <suyue.chen@intel.com>
Co-authored-by: changwangss <chang1.wang@intel.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: chen, suyue <suyue.chen@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-09-27T11:23:55Z,fix bug in smoothquant for auto alpha (#1287)
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-09-26T13:59:44Z,"[regression fix] sq enhance calibration part  (#1276)

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>
Co-authored-by: xinhe <xin3.he@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-09-12T01:31:18Z,"fix bug in nf4/fp4 (#1241)

Signed-off-by: Xin He <xin3.he@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-09-08T07:54:53Z,"add pre-commit-ci code-spell check (#1206)

Signed-off-by: Sun, Xuehao <xuehao.sun@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-09-07T07:49:34Z,"fix device issue of sq (#1218)

Signed-off-by: Xin He <xin3.he@intel.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-09-06T03:00:49Z,"fix bug when trace failed (#1214)

Signed-off-by: Xin He <xin3.he@intel.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-08-31T03:15:59Z,"sq move_input_to_device tuple issue (#1186)

* fix conflicts

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* fix for ut

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

* code enhance for coverage decrease

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>

* [pre-commit.ci] auto fixes from pre-commit.com hooks

for more information, see https://pre-commit.ci

---------

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>
Co-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-08-28T08:59:33Z,"Add automatic Pre-CI code-scan bug fix tool (#1134)

Signed-off-by: Sun, Xuehao <xuehao.sun@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-08-24T05:42:38Z,"Improve sq device mapping and enhance ut (#1168)

Signed-off-by: changwangss <chang1.wang@intel.com>
Co-authored-by: chensuyue <suyue.chen@intel.com>
Co-authored-by: xinhe <xin3.he@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-08-21T06:03:52Z,"add accuracy test for smoothquant (#1158)

* add accuracy test for smoothquant to avoid accuracy issue in pre-ci

Signed-off-by: Xin He <xin3.he@intel.com>

---------

Signed-off-by: Xin He <xin3.he@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-08-17T05:09:11Z,"fix bug in TorchSmoothQuant (#1149)

* [bug fix] when folding=False and QKV is not fully converted to SQLinear.

Signed-off-by: Xin He <xin3.he@intel.com>

---------

Signed-off-by: Xin He <xin3.he@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-08-15T00:39:59Z,"a new method to perform alpha auto-tuning in smooth_quant (#1065)

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-08-07T02:05:19Z,"[SQ bug] unifiy weight_amax for modules same input (#1139)

unifiy weight_amax for modules same input, or qkv will get different scale and got wrong accuracy."
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-07-28T08:40:24Z,"Enhance SmoothQuant tuning structure. (#1109)

* enhance sq tuning

Signed-off-by: Xin He <xin3.he@intel.com>

* Support the tuning of smooth quant' alpha in strategy layer (#1112)

Signed-off-by: yiliu30 <yi4.liu@intel.com>

* added more UTs

Signed-off-by: yiliu30 <yi4.liu@intel.com>

* fixed ut

Signed-off-by: yiliu30 <yi4.liu@intel.com>

* fixed ut

Signed-off-by: yiliu30 <yi4.liu@intel.com>

* enable sq tuning for both quant_level is auto or 1

Signed-off-by: yiliu30 <yi4.liu@intel.com>

* fix accuracy issue

Signed-off-by: Xin He <xin3.he@intel.com>

* fix UT

Signed-off-by: Xin He <xin3.he@intel.com>

* fix alpha=auto

Signed-off-by: Xin He <xin3.he@intel.com>

* support sq tuning for both auto and O1

Signed-off-by: yiliu30 <yi4.liu@intel.com>

* fixed the typo

Signed-off-by: yiliu30 <yi4.liu@intel.com>

* rename func name in ut

Signed-off-by: Xin He <xin3.he@intel.com>

* remove duplicate Linear if Linear is wrapped by Linear

Signed-off-by: Xin He <xin3.he@intel.com>

* refactor tensorflow interface

* adjust the pre-optimization and sq order for ort

* updated ort ut

Signed-off-by: yiliu30 <yi4.liu@intel.com>

* fix pylint and docstyle

Signed-off-by: Xin He <xin3.he@intel.com>

* add sketch for ort tune sq alpha

Signed-off-by: yiliu30 <yi4.liu@intel.com>

* correct the calib_iter

Signed-off-by: yiliu30 <yi4.liu@intel.com>

* fix tensorflow UT and int8 acc issue

* fix ut

Signed-off-by: Xin He <xin3.he@intel.com>

---------

Signed-off-by: Xin He <xin3.he@intel.com>
Signed-off-by: yiliu30 <yi4.liu@intel.com>
Co-authored-by: Yi30 <106061964+yiliu30@users.noreply.github.com>
Co-authored-by: yiliu30 <yi4.liu@intel.com>
Co-authored-by: spycsh <sihan.chen@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-07-18T07:56:48Z,"fix sq skip_connection bugs (#1011)

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-06-15T00:52:26Z,"output check added after sq transform (#954)

Signed-off-by: chensuyue <suyue.chen@intel.com>
Co-authored-by: wenhuach21 <wenhua.cheng@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-06-13T05:04:16Z,"Fix onnxrt smooth quant (#951)

Signed-off-by: Mengni Wang <mengni.wang@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-05-30T02:15:13Z,"Support sq auto tune for ort (#847)

Signed-off-by: Mengni Wang <mengni.wang@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-05-30T02:06:31Z,"Add weight attribute to SQLinearWrapper class (#900)

Signed-off-by: changwangss <chang1.wang@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-05-26T04:10:03Z,"resolve memory issue of SmoothQuant (#902)

Signed-off-by: Xin He <xin3.he@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-05-05T03:30:40Z,"update qconfig to meet ipex2.1 requirement and fix ipex bugs (#823)

Signed-off-by: Xin He <xin3.he@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-04-19T05:23:49Z,"enable smoothquant with calibration func (#812)

Signed-off-by: changwa1 <chang1.wang@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-04-18T06:22:00Z,"Enhance smoothquant and ipex op_type capability. (#808)

Signed-off-by: Xin He <xin3.he@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-04-13T13:08:10Z,"update SmoothQuant algorithm with folding choice (#799)

Signed-off-by: Xin He <xin3.he@intel.com>
Co-authored-by: wenhuach21 <wenhua.cheng@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-04-11T06:15:54Z,"fix smooth quant depthwise conv issue (#774)

Signed-off-by: wenhuach21 <wenhua.cheng@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-03-29T01:45:31Z,"smoothquant alpha auto-tuning (#747)

Signed-off-by: Lu, Yintong <yintong.lu@intel.com>
Signed-off-by: wenhuach21 <wenhua.cheng@intel.com>
Co-authored-by: Wang, Mengni <mengni.wang@intel.com>
Co-authored-by: yiliu30 <yi4.liu@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-03-24T15:11:20Z,"quantization/sq support more patterns (#729)

Signed-off-by: wenhuach21 <wenhua.cheng@intel.com>"
github.com/intel/neural-compressor,test/algorithm/test_smooth_quant.py,2023-03-01T09:39:06Z,"Quantization/smooth quant torch backend update (#594)

Signed-off-by: wenhuach21 <wenhua.cheng@intel.com>"
github.com/mallorbc/Finetune_LLMs,finetuning_repo/lora_merge.py,2023-10-24T05:48:39Z,updated merging program
github.com/mallorbc/Finetune_LLMs,finetuning_repo/lora_merge.py,2023-09-09T19:12:02Z,"fixed flash attention for 70B, fixed lora merge for larger models, change default model for analyzing datasets, removed unneeeded code"
github.com/mallorbc/Finetune_LLMs,finetuning_repo/lora_merge.py,2023-09-08T01:41:11Z,added int8 support
github.com/mallorbc/Finetune_LLMs,finetuning_repo/lora_merge.py,2023-09-07T21:38:34Z,updated lora merge and updated logger
github.com/mallorbc/Finetune_LLMs,finetuning_repo/lora_merge.py,2023-07-22T21:18:15Z,added lora merge file
github.com/CambioML/pykoi-rlhf-finetuned-transformers,pykoi/rlhf/supervised_finetuning.py,2023-12-23T01:54:59Z,add TODO and run black and isort
github.com/CambioML/pykoi-rlhf-finetuned-transformers,pykoi/rlhf/supervised_finetuning.py,2023-12-22T22:57:29Z,"update openai, datasets, transformers, peft, and trl packages"
github.com/CambioML/pykoi-rlhf-finetuned-transformers,pykoi/rlhf/supervised_finetuning.py,2023-09-01T07:53:12Z,update telemetry for RLHF
github.com/CambioML/pykoi-rlhf-finetuned-transformers,pykoi/rlhf/supervised_finetuning.py,2023-08-07T06:25:05Z,"refactor llm and db into chat folder, refactor their tests into chat folder, and update docs rst"
github.com/CambioML/pykoi-rlhf-finetuned-transformers,pykoi/rlhf/supervised_finetuning.py,2023-08-04T04:52:28Z,reformat pykoi with black
github.com/CambioML/pykoi-rlhf-finetuned-transformers,pykoi/rlhf/supervised_finetuning.py,2023-07-31T00:12:53Z,supervised finetuning refactor
github.com/CambioML/pykoi-rlhf-finetuned-transformers,pykoi/rlhf/supervised_finetuning.py,2023-07-25T08:54:57Z,rlhf refactor
github.com/nlpai-lab/KULLM,export_hf_checkpoint.py,2023-05-30T11:26:03Z,"add: train and dataset file

Signed-off-by: metterian <dzzy6505@gmail.com>"
github.com/datadreamer-dev/DataDreamer,src/trainers/train_hf_finetune.py,2024-02-01T03:26:02Z,Release commit [release]
github.com/SkunkworksAI/hydra-moe,hydra_moe/router.py,2023-09-22T04:21:17Z,fix roter:
github.com/SkunkworksAI/hydra-moe,hydra_moe/router.py,2023-09-21T22:16:39Z,(artem) refactor part 3
github.com/SkunkworksAI/hydra-moe,hydra_moe/router.py,2023-09-21T22:02:11Z,refactor pt 2
github.com/IlyaGusev/rulm,self_instruct/src/tools/convert_to_native.py,2023-09-16T17:02:24Z,7b v2
github.com/IlyaGusev/rulm,self_instruct/src/tools/convert_to_native.py,2023-09-09T14:23:42Z,Flake8 refactoring
github.com/IlyaGusev/rulm,self_instruct/src/tools/convert_to_native.py,2023-09-09T13:55:42Z,Removing unused scripts
github.com/kbressem/medAlpaca,medalpaca/inferer.py,2023-06-13T06:50:17Z,Allow loading any LlamaForCausalLM model
github.com/kbressem/medAlpaca,medalpaca/inferer.py,2023-04-07T00:21:25Z,"Redesign of the inference class. It now prompts the models the same way, they were prompted during training.
This came with some minor changes in the datahandler and train script"
github.com/kbressem/medAlpaca,medalpaca/inferer.py,2023-04-03T13:19:37Z,"restructure main train function

This function aimes to combine the alpaca-lora and stanford alpaca script, by allowing to toggle between int8/LoRA and fp16 training"
github.com/kbressem/medAlpaca,medalpaca/inferer.py,2023-04-02T23:19:00Z,move scripts to libs folder
github.com/BobaZooba/xllm,src/xllm/utils/post_training.py,2023-11-16T14:53:10Z,Release: 0.1.6
github.com/BobaZooba/xllm,src/xllm/utils/post_training.py,2023-11-16T12:01:00Z,int8 fusing
github.com/BobaZooba/xllm,src/xllm/utils/post_training.py,2023-11-14T12:57:06Z,more docs
github.com/BobaZooba/xllm,src/xllm/utils/post_training.py,2023-11-13T11:12:41Z,update
github.com/BobaZooba/xllm,src/xllm/utils/post_training.py,2023-11-13T09:40:43Z,Init
github.com/IBM/Dromedary,utils/convert_hf_weights_to_llama_ckpt_expanded.py,2023-10-05T11:47:11Z,upgrade to Dromedary-2
github.com/IBM/Dromedary,utils/convert_hf_weights_to_llama_ckpt_expanded.py,2023-05-04T11:55:28Z,update README
github.com/IBM/Dromedary,utils/convert_hf_weights_to_llama_ckpt_expanded.py,2023-05-04T11:26:16Z,update sharding config
github.com/IBM/Dromedary,utils/convert_hf_weights_to_llama_ckpt_expanded.py,2023-05-04T09:59:43Z,add step1 instructions
github.com/IBM/Dromedary,utils/convert_hf_weights_to_llama_ckpt_expanded.py,2023-05-04T09:42:43Z,update README
github.com/IBM/Dromedary,utils/convert_hf_weights_to_llama_ckpt_expanded.py,2023-05-04T08:08:43Z,add inference code
github.com/shawroad/NLP_pytorch_project,LLM/peft_example/sequence_cls/bert/run_infer_bert_prompt_tuning.py,2023-04-21T03:21:49Z,add accelerate
github.com/shawroad/NLP_pytorch_project,LLM/peft_example/sequence_cls/bert/run_infer_bert_prefix_tuning.py,2023-04-21T03:21:49Z,add accelerate
github.com/zjunlp/EasyEdit,easyeditor/models/melo/peft_egg/src/peft/peft_model.py,2024-02-14T09:26:46Z,Add files via upload
github.com/CMKRG/QiZhenGPT,gradio_chatglm_demo.py,2023-05-30T06:43:15Z,release fine tuning chatglm-6b
github.com/yangjianxin1/Firefly-LLaMA2-Chinese,script/http/start_service.py,2023-09-18T16:57:31Z,http服务部署
github.com/johnsmith0031/alpaca_lora_4bit,text-generation-webui/custom_monkey_patch.py,2023-08-22T12:00:13Z,fix custom_monkey_patch for pip version
github.com/johnsmith0031/alpaca_lora_4bit,text-generation-webui/custom_monkey_patch.py,2023-04-18T12:24:23Z,first pass at fixing the dockerfile
github.com/johnsmith0031/alpaca_lora_4bit,text-generation-webui/custom_monkey_patch.py,2023-04-17T16:35:05Z,fix circular import and add monkeypatch submodule in setup
github.com/johnsmith0031/alpaca_lora_4bit,text-generation-webui/custom_monkey_patch.py,2023-04-13T02:35:10Z,update reference
github.com/johnsmith0031/alpaca_lora_4bit,text-generation-webui/custom_monkey_patch.py,2023-04-10T10:41:16Z,"Bugfix in custom_monkey_patch for v1 models

Previously generation would fail with:

    File ""/alpaca_lora_4bit/text-generation-webui/matmul_utils_4bit.py"", line 79, in _matmul4bit_v1_recons
      quant_cuda.vecquant4recons_v1(qweight, buffer, scales, zeros)
  RuntimeError: expected scalar type Half but found Float

See #71"
github.com/johnsmith0031/alpaca_lora_4bit,text-generation-webui/custom_monkey_patch.py,2023-04-10T01:33:41Z,add v1 model as default in custom monkey patch
github.com/johnsmith0031/alpaca_lora_4bit,text-generation-webui/custom_monkey_patch.py,2023-04-06T04:56:27Z,add patch for encode function to remove eos token at the beginning of left side
github.com/johnsmith0031/alpaca_lora_4bit,text-generation-webui/custom_monkey_patch.py,2023-03-28T12:33:55Z,add v2 model support
github.com/johnsmith0031/alpaca_lora_4bit,text-generation-webui/custom_monkey_patch.py,2023-03-26T03:52:38Z,Tested and should be ready!
github.com/johnsmith0031/alpaca_lora_4bit,text-generation-webui/custom_monkey_patch.py,2023-03-22T07:58:51Z,add monkey patch for webui
github.com/locuslab/wanda,lora_ft/evaluate_ppl.py,2023-10-23T07:56:08Z,add lora fine_tuning
github.com/datadreamer-dev/DataDreamer,src/trainers/train_hf_classifier.py,2024-02-01T03:26:02Z,Release commit [release]
github.com/uclaml/SPIN,spin/alignment/trainer.py,2024-02-11T17:24:52Z,"Update trainer.py

a -> an"
github.com/uclaml/SPIN,spin/alignment/trainer.py,2024-02-09T11:03:30Z,code cleaning
github.com/uclaml/SPIN,spin/alignment/trainer.py,2024-02-09T10:50:15Z,solve compatibility issues
github.com/uclaml/SPIN,spin/alignment/trainer.py,2024-02-09T09:49:54Z,updating shell scripts and codes
github.com/uclaml/SPIN,spin/alignment/trainer.py,2024-02-09T09:39:26Z,"Updated config, finetune.sh, convert_data.py and restructure"
github.com/uclaml/SPIN,spin/alignment/trainer.py,2024-02-09T09:10:00Z,restructure codes
github.com/uclaml/SPIN,spin/alignment/trainer.py,2024-02-09T08:10:23Z,change back to original trainer
github.com/uclaml/SPIN,spin/alignment/trainer.py,2024-02-09T00:14:33Z,restructured spin
github.com/sabetAI/BLoRA,blora_utils.py,2023-09-01T17:26:15Z,Removed unused code.
github.com/sabetAI/BLoRA,blora_utils.py,2023-09-01T16:52:30Z,Added requirements.txt and cleaned extraneous files.
github.com/sabetAI/BLoRA,blora_utils.py,2023-08-29T02:40:49Z,Moved lora batch logic into utils.
github.com/sabetAI/BLoRA,blora_utils.py,2023-08-27T16:34:26Z,Replaced peft class methods in inference call stack with blora methods.
github.com/sabetAI/BLoRA,blora_utils.py,2023-08-25T21:29:16Z,Added blora classes.
github.com/sabetAI/BLoRA,blora_utils.py,2023-08-25T04:41:58Z,Added placeholder README and delete irrelevant files.
github.com/sabetAI/BLoRA,blora_utils.py,2023-08-23T23:52:13Z,"Fixed streaming, tested."
github.com/sabetAI/BLoRA,blora_utils.py,2023-08-23T15:03:54Z,Wrote method overrides for streaming peft.
github.com/airaria/Visual-Chinese-LLaMA-Alpaca,scripts/merge_llama_with_visualcla_lora.py,2023-07-07T02:27:55Z,init
github.com/SeungyounShin/Llama2-Code-Interpreter,code_interpreter/llama_hf.py,2023-08-21T18:58:29Z,Feat (Finetunning&Eval) add human-eval
github.com/zjunlp/EasyEdit,easyeditor/models/melo/peft_egg/src/peft/tuners/prompt_tuning.py,2024-02-14T09:26:46Z,Add files via upload
github.com/datadreamer-dev/DataDreamer,src/trainers/_vendored/dpo_trainer.py,2024-02-01T03:26:02Z,Release commit [release]
github.com/SkunkworksAI/hydra-moe,hydra_moe/inference.py,2023-09-21T22:16:39Z,(artem) refactor part 3
github.com/SkunkworksAI/hydra-moe,hydra_moe/inference.py,2023-09-21T22:02:11Z,refactor pt 2
github.com/SkunkworksAI/hydra-moe,hydra_moe/peft_model.py,2023-09-21T22:16:39Z,(artem) refactor part 3
github.com/SkunkworksAI/hydra-moe,hydra_moe/peft_model.py,2023-09-21T22:02:11Z,refactor pt 2
github.com/datadreamer-dev/DataDreamer,src/trainers/train_setfit_classifier.py,2024-02-01T03:26:02Z,Release commit [release]
github.com/datadreamer-dev/DataDreamer,src/trainers/_vendored/_setfit_helper.py,2024-02-01T03:26:02Z,Release commit [release]
github.com/longyuewangdcu/Chinese-Llama-2,test/inference_lora.py,2023-07-20T15:10:13Z,	new file:   test/inference_lora.py
github.com/FudanDISC/DISC-FinLLM,eval/evaluator/finllm.py,2023-10-24T11:01:18Z,Add files via upload
github.com/datadreamer-dev/DataDreamer,src/trainers/train_sentence_transformer.py,2024-02-27T00:09:58Z,Bump version [release]
github.com/datadreamer-dev/DataDreamer,src/trainers/train_sentence_transformer.py,2024-02-01T03:26:02Z,Release commit [release]
github.com/zjunlp/EasyEdit,easyeditor/models/melo/peft_egg/tests/test_adaption_prompt.py,2024-02-14T09:26:46Z,Add files via upload
github.com/BobaZooba/xllm,tests/helpers/patches.py,2023-11-14T16:41:46Z,Update docs
github.com/BobaZooba/xllm,tests/helpers/patches.py,2023-11-13T09:40:43Z,Init
github.com/X-D-Lab/Sunsimiao,scripts/inference_4_bits.py,2023-06-21T12:15:52Z,add inference
github.com/OpenLMLab/LOMO,lomo/src/merge_llama_with_lora.py,2023-10-17T03:15:38Z,add lomo folder and move
github.com/nlpai-lab/KULLM,export_state_dict_checkpoint.py,2023-05-30T11:26:03Z,"add: train and dataset file

Signed-off-by: metterian <dzzy6505@gmail.com>"
github.com/open-compass/VLMEvalKit,vlmeval/vlm/llava_xtuner.py,2024-01-17T05:40:59Z,"[Model] Add `LLaVA-InternLM2` (#53)

* support

* add trust_remote_code"
github.com/open-compass/VLMEvalKit,vlmeval/vlm/llava_xtuner.py,2024-01-02T12:29:54Z,update
github.com/open-compass/VLMEvalKit,vlmeval/vlm/llava_xtuner.py,2024-01-02T07:45:47Z,"[Dataset] HallusionBench (#38)

* support HallusionBench

* add GPT4V_SHORT

* update dataset config

* update smp.py

* bug fix

* BUG FIX

* update MD5

* update rating

* fix bug"
github.com/open-compass/VLMEvalKit,vlmeval/vlm/llava_xtuner.py,2023-12-27T13:04:39Z,"[Feature] Support `LLaVA_XTuner` models (#17)

* support

* fix bugs

* fix bugs

* fix prompt bugs

* Update README.md

* Update README.md

* Update llava.py

* update llava_xtuner

* update .gitignore

* update hyp

* use 1024 token

* modify the gen_config

* update"
github.com/datadreamer-dev/DataDreamer,src/task_models/hf_classification_task_model.py,2024-02-02T23:20:22Z,Update docs
github.com/datadreamer-dev/DataDreamer,src/task_models/hf_classification_task_model.py,2024-02-02T04:00:55Z,Update docs [release]
github.com/datadreamer-dev/DataDreamer,src/task_models/hf_classification_task_model.py,2024-02-01T03:26:02Z,Release commit [release]
github.com/stanford-crfm/levanter,examples/alpaca-lora/hf_lora_inference.py,2023-10-24T06:07:35Z,"Improvements to Alpaca tutorial wip (#349)

* aspirational alpaca tutorial

* rename ShardedDataSource to ShardedDataset

* remove targets

* remove targets

* tweak

* wip

* rename ShardedDataSource to ShardedDataset

* somehow missed this

* create a fancier data api so we can make alpaca look nicer

* fix alpaca_lora.py

* remove old batchpreprocessor from alpaca

* load prompts if provided

* oops

* rename

* refactor the attentionmask stuff w/ hopes of supporting sequence packing

* move tutorials

* add bias and precision to FA

* expose top level attn for all models to use

* tweaks to new attention wrapper

* make flash attention not slice dims the bias doesn't have

* switch mpt to new attention wrapper

* move gpt2 over

* move llama over

* move backpack over

* del todo

* refactor the attentionmask stuff w/ hopes of supporting sequence packing

* add bias and precision to FA

* expose top level attn for all models to use

* tweaks to new attention wrapper

* make flash attention not slice dims the bias doesn't have

* switch mpt to new attention wrapper

* move gpt2 over

* move llama over

* move backpack over

* remove causalmask from alpaca

* fix non-flash test

* dumb prints

* make models work with different length input than max input length

* allow for ""max_tune_length"" in alpaca to speed up training

* fix path imports for new alpaca_lora paths

* missed an import

* save tokenizer in peft checkpoints

* save modified tokenizer in alpaca checkpoints

* don't add special tokens for lora, just use unk

* update docs

* fix up tutorial

* wtf

* nmajlkfnmajlkf hf

* hates

* simple script for lora inference"
github.com/CMKRG/QiZhenGPT,gradio_chinese-llama_demo.py,2023-06-02T09:57:18Z,release instruction tuning on CaMA
github.com/sail-sg/lorahub,lorahub/algorithm.py,2023-11-01T03:17:53Z,use copy.deepcopy to prevent modifying the peft state dict
github.com/sail-sg/lorahub,lorahub/algorithm.py,2023-08-29T15:41:07Z,update reproducing script for bbh results of lorahub
github.com/sail-sg/lorahub,lorahub/algorithm.py,2023-08-15T09:36:42Z,update model to device; update reproduce script for bbh dataset
github.com/sail-sg/lorahub,lorahub/algorithm.py,2023-07-27T07:55:07Z,fix minor typo
github.com/sail-sg/lorahub,lorahub/algorithm.py,2023-07-27T07:51:33Z,update code
github.com/stanford-crfm/levanter,tests/test_lora.py,2024-02-29T07:23:13Z,DoReMi (#416)
github.com/stanford-crfm/levanter,tests/test_lora.py,2024-02-28T22:12:27Z,"make TrainerState a bit beefier, more standalone method for load_or_init (#496)

* bring over load_checkpoint_or_initialize

* extra test

* is this better? I don't know

* wip

* wip

* minor cleanup

* wip

* wip promoting TrainerState

* almost done promoting TrainerState

* make TrainerState know about mp and optimizers, other tweaks"
github.com/stanford-crfm/levanter,tests/test_lora.py,2024-02-14T20:20:16Z,"Causal flash (#469)

* take advantage of block causal structure in flash attention?

* default to FA=on, adjust default block size for tpu

* fix tests

* missed a spot

* ok actually fix tests maybe

* dumb"
github.com/stanford-crfm/levanter,tests/test_lora.py,2024-02-14T04:24:24Z,"Tweaks/Fixes to Lora_LM, add a basic config for lora_lm (#466)

* require a new enough optax

(didn't i do that?)

* add a lora_llama2.yaml

* forgot to set up the data stuff

* support multiple evaluation sets in lora_lm

* try not failing if the ray node is already up

* don't raise on hf datasets with no validation set

* fix trainable_param_count invocations

* reduce batch size for lora_llama2

* fix serialization of lora, which i had broken"
github.com/stanford-crfm/levanter,tests/test_lora.py,2023-09-27T17:51:06Z,"Script/Tutorial for Alpaca LoRA (#326)

* wip

* factor out and test merged model saving in lora

* finish the alpaca-lora doc

* please pre-commit

* Update Alpaca-LoRA.md"
github.com/stanford-crfm/levanter,tests/test_lora.py,2023-09-09T05:23:49Z,move inference_mode to tree_utils
github.com/stanford-crfm/levanter,tests/test_lora.py,2023-09-05T17:57:22Z,Support dropout in LoRA (#297)
github.com/stanford-crfm/levanter,tests/test_lora.py,2023-08-31T17:52:19Z,"Support `inference` as a non-static field in Dropout and other fields. (#296)

* support using `inference` field in models instead of passing an inference param around everywhere"
github.com/stanford-crfm/levanter,tests/test_lora.py,2023-08-10T06:22:39Z,Preliminary LoRA support (#269)
github.com/modelscope/swift,tests/tuners/test_peft.py,2024-02-22T13:26:16Z,support peft format (#438)
github.com/modelscope/swift,tests/tuners/test_peft.py,2023-12-21T11:47:14Z,Support more peft tuners (#245)
github.com/modelscope/swift,tests/tuners/test_peft.py,2023-12-16T09:13:46Z,Compatible with peft>=0.7.0 (#220)
github.com/modelscope/swift,tests/tuners/test_peft.py,2023-10-11T09:04:36Z,fix ci (#109)
github.com/modelscope/swift,tests/tuners/test_peft.py,2023-08-02T02:16:26Z,First commit
github.com/FlagAI-Open/Aquila2,examples/rag_pipe/models/aquila/aquila_model_local.py,2024-01-30T07:38:56Z,update rag
github.com/airaria/Visual-Chinese-LLaMA-Alpaca,scripts/inference/text_generation_webui/visualcla/visualcla.py,2023-07-07T02:27:55Z,init
github.com/modelscope/swift,tests/hub/test_check_model.py,2023-08-02T02:16:26Z,First commit
github.com/michael-wzhu/Chinese-LlaMA2,src/serving/vllm_serving/merge_llama_with_lora.py,2023-07-28T06:45:40Z,2023/07/26 update model; uodate readme
github.com/CMKRG/QiZhenGPT,scripts/merge_llama_with_chinese_lora.py,2023-05-23T06:40:06Z,"🎉 init: 初始化

开源体验版"
github.com/beyondguo/LLM-Tuning,web_demo/src/test.py,2023-07-25T05:02:02Z,web_demo
github.com/SeungyounShin/Llama2-Code-Interpreter,code_interpreter/LlamaCodeInterpreter.py,2023-08-28T17:37:32Z,Rebase :)
github.com/SeungyounShin/Llama2-Code-Interpreter,code_interpreter/LlamaCodeInterpreter.py,2023-08-25T10:35:37Z,Feat (CodeLlama) Update CodeLlama support and Some Eval
github.com/SeungyounShin/Llama2-Code-Interpreter,code_interpreter/LlamaCodeInterpreter.py,2023-08-21T18:58:29Z,Feat (Finetunning&Eval) add human-eval
github.com/SeungyounShin/Llama2-Code-Interpreter,code_interpreter/LlamaCodeInterpreter.py,2023-08-21T18:39:43Z,Feat (Eval) add human-eval
github.com/SeungyounShin/Llama2-Code-Interpreter,code_interpreter/LlamaCodeInterpreter.py,2023-08-18T08:32:33Z,Feat (GSM8K) eval
github.com/SeungyounShin/Llama2-Code-Interpreter,code_interpreter/LlamaCodeInterpreter.py,2023-08-13T18:50:28Z,Feat (Llama2 Finetunning) Modify Finetuning Model Wrapper Class
github.com/SeungyounShin/Llama2-Code-Interpreter,code_interpreter/LlamaCodeInterpreter.py,2023-08-12T14:28:59Z,Feat (Llama2 Finetuning) Add inference code for test
github.com/SeungyounShin/Llama2-Code-Interpreter,code_interpreter/LlamaCodeInterpreter.py,2023-08-09T08:06:42Z,Initial commit
github.com/michael-wzhu/PromptCBLUE,src/ft_llama_lora/merge_llama_with_chinese_lora.py,2023-07-18T09:37:52Z,2023-07-18: add LlaMA + lora code; add vllm model serving
github.com/LiuHC0428/LAW-GPT,src/peft/tests/test_peft_model.py,2023-04-22T04:46:57Z,Add files via upload
github.com/michael-wzhu/PromptCBLUE,src/ft_llama_lora/vllm_serving/merge_llama_with_lora.py,2023-07-18T09:37:52Z,2023-07-18: add LlaMA + lora code; add vllm model serving
github.com/PotatoSpudowski/fastLLaMa,scripts/export-from-huggingface.py,2023-04-24T13:22:48Z,"Sync llama (#63)

* sync and refactor part one

* sync and refactor part two

* sync and refactor part three

* fix percentage callback

* Fix for CMake 11

* fix shadow of variables

* updated the python api and examples

* Updated export to support alpaca 13B and 30B

* add support for mmap, mlock and progress

* update python interface

* refactor code

* fix lora adapters

* remove debug prints

* refactors and fixes

* fix adapter version

* stop users from attach/detach lora to a mmaped model

* use existing function to load tensor

* fix lora adapter for f16 cached matrices

* refactor code

* refactor code

* remove unused struct member

* move scaling to python script

* remove 'constexpr' because gcc 11 complains

* refactor code

* remove redundant variable

* remove unused namespace

* cap progress value to max value

* fix recycling of tokens

* refactor token recycler

* fix token recycler

* fix lora adapter

* add lora adapter support for mmap

* force related tensors to be consecutive

* updated the comment

* set lora adapter path in early exit

* fix colon issue

* clear token buffer after stop token found

* restore state after model is stopped.

* remove unused enum

* remove model kind

* Cleaned up code and updated docs

---------

Co-authored-by: Amit Singh <amitsingh19975@gmail.com>"
github.com/PotatoSpudowski/fastLLaMa,scripts/export-from-huggingface.py,2023-04-15T21:11:00Z,"Big Update! 🦣 (#44)

* rearrange files

* add model load

* add macros

* add model eval

* add ring buffer, bridge

* rearrange files

* add model load

* add macros

* add model eval

* add ring buffer, bridge

* update ggml and add generate with ingest

* refactor code

* fix example.py

* fix conflict issue

* fix wrong mapping of tensor using string

* remove old utils files

* disable quantize for now

* add utils.hpp

* unload the model

* add config structure inside the ModelId

* sync with the `llama.cpp`

* add constant and move `n_batch` to model

* remove constant and move `n_batch` to model

* add batch overhead

* add args

* add logger to builder and rename function

* update cmake

* rename `Builder` to `builder`

* add python support

* add fastllama wrapper for python

* show interface folders that we detected

* updated setup.py

* fix to cmake with the correct python version

* update quantize.cpp

* refactor quantize

* remove old 'bridge.cpp'

* make compiler more strict

* remove compiler specific flag

* updated scripts

* fix model parsing

* refactor to make c++17 complient

* add pad token

* fix header

* fix shell invoke

* update 'setup.py'

* compile c++ in strict mode

* add mode for old quantized models

* add alpaca model test source file

* persist system prompt after token recycle

* fix alpaca example

* set thread count dynamically

* add option to use enum instead of string for model id

* add option to use enum in python instead of string for model id

* change python module name and add c interface

* fix for unused variables

* change cmake vars

* allow for language selection using gui and cmd line args

* disable warnings for now

* move away pybind11 to make it version indepenedent

* uncomment setup.py

* comment the find python version

* add signal to quit the app

* add cross compile mode for android

* enable build

* fix cmake file

* improve c example

* use c malloc and free

* build lib first then examples

* remove copy constructor

* default initialize context struct

* add global compiler flags

* remove pybind11

* add embeddings and perplexity

* update `ggml.c` and `ggml.h`

* Add a array view `span`

* add a array view struct for c interface

* add new fields and bug fixes

* refactor code

* fix alpaca example

* change `set stop words` interface

* fix ctype issue

* fix call to set stop words

* fix call to set stop words

* warn the user about getting embeddings before not setting the flag.

* add type to struct

* add function comments

* fix example and add a new one

* add c perplexity example

* Fixes for gcc

* fix python type issue on windows

* fix msvc errors

* fix windows compile errors

* Added examples and logger fix

* Updated readme

---------

Co-authored-by: Amit Singh <amitsingh19975@gmail.com>"
github.com/jianzhnie/open-chatgpt,chatgpt/models/apply_lora.py,2023-05-22T10:32:13Z,update alpaca_inference
github.com/jianzhnie/open-chatgpt,chatgpt/models/apply_lora.py,2023-05-22T08:24:47Z,Create apply_lora.py
github.com/jianzhnie/open-chatgpt,examples/chatllama/merge_peft_adapter.py,2023-04-27T07:41:00Z,format code
github.com/jianzhnie/open-chatgpt,examples/chatllama/merge_peft_adapter.py,2023-04-27T02:34:50Z,add train chatllama
github.com/LLaVA-VL/LLaVA-Plus-Codebase,llava/model/builder.py,2023-10-31T20:09:27Z,Update macOS support.
github.com/LLaVA-VL/LLaVA-Plus-Codebase,llava/model/builder.py,2023-10-06T23:49:36Z,"For inference in model_worker, allow the device to be specified via a command line parameter.

Right now it has only been tested with Apple Sillicon devices via the mps device."
github.com/LLaVA-VL/LLaVA-Plus-Codebase,llava/model/builder.py,2023-09-01T16:45:00Z,Update instruction for LoRA
github.com/LLaVA-VL/LLaVA-Plus-Codebase,llava/model/builder.py,2023-07-29T23:55:59Z,Update docs
github.com/LLaVA-VL/LLaVA-Plus-Codebase,llava/model/builder.py,2023-07-19T09:08:02Z,Release v1.0.0
github.com/Zeqiang-Lai/Mini-DALLE3,minidalle3/llm/llama.py,2023-12-28T13:52:43Z,add doc for custom llm
github.com/Zeqiang-Lai/Mini-DALLE3,minidalle3/llm/llama.py,2023-10-04T02:47:24Z,refactor
github.com/Joyce94/LLM-RLHF-Tuning,script/rm/run_rm_with_peft.py,2023-08-31T02:54:51Z,fix process data
github.com/Joyce94/LLM-RLHF-Tuning,script/rm/run_rm_with_peft.py,2023-08-23T08:07:52Z,add dpo and ppo_co code
github.com/Joyce94/LLM-RLHF-Tuning,script/rm/run_rm_with_peft.py,2023-08-13T14:21:30Z,update code
github.com/Joyce94/LLM-RLHF-Tuning,script/rm/run_rm_with_peft.py,2023-08-06T10:21:26Z,add code
github.com/Joyce94/LLM-RLHF-Tuning,script/sft/run_sft_with_peft.py,2023-08-31T02:54:51Z,fix process data
github.com/Joyce94/LLM-RLHF-Tuning,script/sft/run_sft_with_peft.py,2023-08-26T07:00:37Z,update ppo
github.com/Joyce94/LLM-RLHF-Tuning,script/sft/run_sft_with_peft.py,2023-08-23T08:07:52Z,add dpo and ppo_co code
github.com/Joyce94/LLM-RLHF-Tuning,script/sft/run_sft_with_peft.py,2023-08-17T09:45:33Z,update code
github.com/Joyce94/LLM-RLHF-Tuning,script/sft/run_sft_with_peft.py,2023-08-13T14:21:30Z,update code
github.com/Joyce94/LLM-RLHF-Tuning,script/sft/run_sft_with_peft.py,2023-08-06T10:21:26Z,add code
github.com/megvii-research/Sparsebit,large_language_models/alpaca-qlora/qlora.py,2023-04-15T09:34:36Z,add alpaca-qlora (#133)
github.com/megvii-research/Sparsebit,large_language_models/alpaca-qlora/generate.py,2023-04-30T04:13:58Z,update the usage of generate.py with a cli (#141)
github.com/megvii-research/Sparsebit,large_language_models/alpaca-qlora/generate.py,2023-04-30T03:01:47Z,add assert in generate.py to force set CHECKPOINT_PATH (#140)
github.com/megvii-research/Sparsebit,large_language_models/alpaca-qlora/generate.py,2023-04-15T13:39:00Z,Update generate.py
github.com/megvii-research/Sparsebit,large_language_models/alpaca-qlora/generate.py,2023-04-15T09:34:36Z,add alpaca-qlora (#133)
github.com/zjunlp/EasyInstruct,experiments/lora/generate.py,2024-01-11T15:12:15Z,update lora experiments
github.com/zjunlp/EasyInstruct,experiments/lora/generate.py,2024-01-07T12:10:23Z,update experiments
github.com/huggingface/api-inference-community,docker_images/peft/app/pipelines/text_generation.py,2024-03-01T21:41:32Z,`model.config` handle breaking change in hub (#416)
github.com/huggingface/api-inference-community,docker_images/peft/app/pipelines/text_generation.py,2023-07-12T09:46:29Z,"Minor fix for PEFT (#300)

* minor fix for widget

* listed dict

* Fixing pydantic (temporarily).

---------

Co-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>"
github.com/huggingface/api-inference-community,docker_images/peft/app/pipelines/text_generation.py,2023-06-20T09:57:35Z,"PEFT integration for inference API (#294)

* initial commit

* fixes

* Update docker_images/peft/app/pipelines/text_generation.py

Co-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>

* fix in pipeline

* added test cases

* style fixes

* added peft tests to github actions workflows

* added test case

* make style

* added timeout

* removed ds_store and refactored pipeline init

* Delete .DS_Store

* Update docker_images/peft/app/pipelines/text_generation.py

Co-authored-by: Omar Sanseviero <osanseviero@gmail.com>

* updated api-inference-community version

---------

Co-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>
Co-authored-by: Omar Sanseviero <osanseviero@gmail.com>"
github.com/defog-ai/sql-eval,eval/hf_runner.py,2024-02-23T05:25:10Z,"New prompts (#88)

* new prompts

* add prev_invalid_sql and prev_error_msg to prompt

* - add prev_invalid_sql and prev_error_msg to prompt
- shift generate_prompt to utils

* add prev_invalid_sql and prev_error_msg to prompt

* - add prev_invalid_sql and prev_error_msg to prompt
- link to prompt templates in prompt folder

* add more types

* change prompts, add dependencies

* remove print statements

* add instructions for mistral and gemini

* linted"
github.com/defog-ai/sql-eval,eval/hf_runner.py,2024-02-16T02:59:27Z,"Added support for Mistral models (#86)

* changed end result printed to correct + error_db_exec, instead of the less useful exact_match and correct

* added mistral runner, though with a hard-coded prompt for now

* linting"
github.com/defog-ai/sql-eval,eval/hf_runner.py,2024-02-13T13:12:37Z,modify pre-processing of generated_query
github.com/defog-ai/sql-eval,eval/hf_runner.py,2024-02-06T15:46:45Z,linted
github.com/defog-ai/sql-eval,eval/hf_runner.py,2024-02-06T15:46:45Z,add table_metadata_string
github.com/defog-ai/sql-eval,eval/hf_runner.py,2024-02-06T15:46:45Z,add glossary
github.com/defog-ai/sql-eval,eval/hf_runner.py,2024-02-05T08:21:42Z,"Added ability to report results to a URL, and sample server code for saving it to a database (#79)

* added utility for reporting results to a webservice

* added webserver for receiving results

* added DDL statements for database tables for storing results

* Add upload_results function to runners

* added upload_url param to args and updated README

* converted uuid4 to hex

* fixed typo in table name

* fixed column name typo

* minor fixes

* converted two columns to string for better compatibility

* formatting

* Shift cloud function for receiving reports into separate folders
Split into separate folders fo Bigquery vs Postgres
Added instructions for deploying cloud functions
Update pandas_gbq calls

* removed bq_table arg

* Remove bq_table flag from command line options

---------

Co-authored-by: jp <wongjingping@gmail.com>"
github.com/defog-ai/sql-eval,eval/hf_runner.py,2024-01-17T05:28:58Z,"Caps `max_new_tokens` in the beam search check (#63)

* Update hf_runner.py

* formatting"
github.com/defog-ai/sql-eval,eval/hf_runner.py,2024-01-15T13:56:03Z,new features (#64)
github.com/defog-ai/sql-eval,eval/hf_runner.py,2024-01-10T12:24:33Z,force num_beams to 1 if beam search is not supported
github.com/defog-ai/sql-eval,eval/hf_runner.py,2024-01-08T07:22:36Z,Add support to MPS backend
github.com/defog-ai/sql-eval,eval/hf_runner.py,2023-11-29T10:13:38Z,lint
github.com/defog-ai/sql-eval,eval/hf_runner.py,2023-11-29T09:27:32Z,remove flag for public data and automatically infer whether db is public based on the name's membership in defog_data.metadata.dbs
github.com/defog-ai/sql-eval,eval/hf_runner.py,2023-11-07T10:07:07Z,"Multiple Prompts
- allows multiple prompts and output files in a single run. this saves the model loading time especially when testing multiple prompts for hf and vllm runners
- we ensure that the number of prompt and output files match early in main.py since it applies to all runners"
github.com/defog-ai/sql-eval,eval/hf_runner.py,2023-10-18T08:06:34Z,"Use dynamic number of beams depending on the prompt's token length. We scale it down approximately quadratically due to the quadratic nature of attention, and allow users to specify max_beams
We now no longer need the statements to explicitly deal with torch memory before the generate statement.
Update prompt to be the same as sql-coder.
Add tests.
Updated requirements.txt to support peft."
github.com/defog-ai/sql-eval,eval/hf_runner.py,2023-10-17T06:38:40Z,"pass args to run_hf_eval (standardize)
-d as a boolean flag to opt in for private data
fix openai runner to able to use private data
fix hf runner to use adapter"
github.com/defog-ai/sql-eval,eval/hf_runner.py,2023-10-16T04:26:01Z,add support for private data
github.com/defog-ai/sql-eval,eval/hf_runner.py,2023-09-28T08:56:58Z,formatting chnages
github.com/defog-ai/sql-eval,eval/hf_runner.py,2023-09-28T08:56:58Z,minor syntactic updates
github.com/defog-ai/sql-eval,eval/hf_runner.py,2023-08-28T03:16:43Z,"fixed GROUP BY bug, added better Llama 2 support, refactored code"
github.com/defog-ai/sql-eval,eval/hf_runner.py,2023-08-16T07:13:33Z,reformatting
github.com/defog-ai/sql-eval,eval/hf_runner.py,2023-08-16T07:13:14Z,refactoring and code cleanup
github.com/defog-ai/sql-eval,eval/hf_runner.py,2023-08-15T17:38:09Z,added check to get more resilience for starcoder and wizardcoder implementations
github.com/defog-ai/sql-eval,eval/hf_runner.py,2023-08-15T08:20:07Z,minor compatibility changes to make new eval work with hf_runner
github.com/defog-ai/sql-eval,eval/hf_runner.py,2023-08-14T09:44:19Z,refactoring and small bugfixes
github.com/defog-ai/sql-eval,eval/hf_runner.py,2023-08-14T09:11:53Z,removed unnecessary print statements
github.com/defog-ai/sql-eval,eval/hf_runner.py,2023-08-14T09:02:12Z,slight modifications to eval functions
github.com/defog-ai/sql-eval,eval/hf_runner.py,2023-08-13T13:04:21Z,black reformatting
github.com/defog-ai/sql-eval,eval/hf_runner.py,2023-08-13T13:03:52Z,made openai prompt and result parsing  more consistent with open-source model
github.com/defog-ai/sql-eval,eval/hf_runner.py,2023-08-11T22:19:26Z,formatting changes
github.com/defog-ai/sql-eval,eval/hf_runner.py,2023-08-11T22:14:44Z,added latency of hf evaluator
github.com/defog-ai/sql-eval,eval/hf_runner.py,2023-08-11T22:09:57Z,added instructions for running HF models
github.com/defog-ai/sql-eval,eval/hf_runner.py,2023-08-11T20:48:06Z,added padding token to tokenizer
github.com/defog-ai/sql-eval,eval/hf_runner.py,2023-08-11T20:45:06Z,added helpful status print statements to hf_runner
github.com/defog-ai/sql-eval,eval/hf_runner.py,2023-08-11T20:30:51Z,minor refactoring nad updates to main.py
github.com/defog-ai/sql-eval,eval/hf_runner.py,2023-08-11T20:20:31Z,created hf runner to improve runtime efficiency
github.com/liucongg/ChatGPTBook,LLMFTProj/predict_lora.py,2023-06-26T07:14:08Z,update code
github.com/BAAI-DCAI/Bunny,bunny/model/builder.py,2024-02-28T14:12:52Z,update README & disable some warnings
github.com/BAAI-DCAI/Bunny,bunny/model/builder.py,2024-02-12T15:03:18Z,migrating to latest phi weights
github.com/BAAI-DCAI/Bunny,bunny/model/builder.py,2024-02-10T16:18:57Z,Update phi-2 link used in Bunny
github.com/BAAI-DCAI/Bunny,bunny/model/builder.py,2024-02-07T15:41:23Z,Initial Commit
github.com/paolorechia/learn-langchain,servers/hf_loader.py,2023-05-13T12:18:41Z,LoRA load example
github.com/paolorechia/learn-langchain,servers/hf_loader.py,2023-05-05T18:20:46Z,"Starcoder (#19)

* Fix merge conflict

* Starcoder example HF 8bit

* Solve conflicts for requirements"
github.com/paolorechia/learn-langchain,servers/hf_loader.py,2023-04-28T12:23:07Z,Adds 'bitsandbytes' dependency
github.com/paolorechia/learn-langchain,servers/hf_loader.py,2023-04-28T12:15:45Z,Fix 16bit HF local model load
github.com/paolorechia/learn-langchain,servers/hf_loader.py,2023-04-25T06:31:33Z,"Load 4 bit (#5)

* Fix

* Embedding Example

* Add experimental support to GPQT 4bit group size 128 model

* Add gpqt repository

* Switch to safetensors instead

* Reorganize repo

* Fix imports

* git ignore update

* Support 13b

* Applies black to servers

* Applies black to langchain_app

* Fix config

* Fix checkpoint paths

* Update README.md"
github.com/paolorechia/learn-langchain,langchain_app/models/alpaca_llm.py,2023-04-30T21:11:25Z,"Support oogabooga web server (#14)

* Refactor code to support different HTTP servers

* Fix langchain passed stop token with http llm

* Add observation to stopping strings; add port to url

* Apply same building logic

* Test web generation

* Add response extractor

* Update readme

* Extend README

* Force stop token removal for ReAct on text generation web UI

* Updates readme, applies black"
github.com/paolorechia/learn-langchain,langchain_app/models/alpaca_llm.py,2023-04-25T06:31:33Z,"Load 4 bit (#5)

* Fix

* Embedding Example

* Add experimental support to GPQT 4bit group size 128 model

* Add gpqt repository

* Switch to safetensors instead

* Reorganize repo

* Fix imports

* git ignore update

* Support 13b

* Applies black to servers

* Applies black to langchain_app

* Fix config

* Fix checkpoint paths

* Update README.md"
github.com/paolorechia/learn-langchain,langchain_app/models/alpaca_llm.py,2023-04-22T23:11:07Z,"React lora (#2)

* Split into modules

* Add prompt human in the loop

* Exp. Embeddings endpoint"
github.com/feizc/MLE-LLaMA,peft/peft_model.py,2023-03-20T08:14:22Z,add package dependency
github.com/feizc/MLE-LLaMA,peft/tuners/lora.py,2023-03-20T08:14:22Z,add package dependency
github.com/feizc/MLE-LLaMA,llama/peft/peft_model.py,2023-03-19T12:17:31Z,add lora
github.com/feizc/MLE-LLaMA,llama/peft/tuners/lora.py,2023-03-19T12:17:31Z,add lora
github.com/feizc/MLE-LLaMA,peft/tuners/prompt_tuning.py,2023-03-20T08:14:22Z,add package dependency
github.com/feizc/MLE-LLaMA,llama/peft/tuners/prompt_tuning.py,2023-03-19T12:17:31Z,add lora
github.com/nbasyl/LLM-FP4,lm_eval/models/huggingface.py,2023-11-22T07:11:19Z,release the source code
github.com/georgian-io/LLM-Finetuning-Hub,inference/fastapi_naive/predictor.py,2023-09-20T20:48:44Z,"polished code, add inference readme"
github.com/georgian-io/LLM-Finetuning-Hub,inference/fastapi_naive/predictor.py,2023-07-31T17:23:14Z,Updated readme
github.com/georgian-io/LLM-Finetuning-Hub,flan-t5/flan_summarization_inference.py,2023-07-26T17:24:46Z,code formatted using black
github.com/georgian-io/LLM-Finetuning-Hub,flan-t5/flan_summarization_inference.py,2023-07-26T17:20:24Z,save metrics dictionary to experiment folder
github.com/georgian-io/LLM-Finetuning-Hub,flan-t5/flan_summarization_inference.py,2023-07-24T14:29:07Z,flan files
github.com/georgian-io/LLM-Finetuning-Hub,flan-t5/flan_classification_inference.py,2023-07-26T17:24:46Z,code formatted using black
github.com/georgian-io/LLM-Finetuning-Hub,flan-t5/flan_classification_inference.py,2023-07-24T14:29:07Z,flan files
github.com/georgian-io/LLM-Finetuning-Hub,inference/text_generation/merge_script.py,2023-09-20T20:48:44Z,"polished code, add inference readme"
github.com/georgian-io/LLM-Finetuning-Hub,inference/text_generation/merge_script.py,2023-08-11T19:31:02Z,added merge scripts
github.com/georgian-io/LLM-Finetuning-Hub,inference/text_generation/merge_script.py,2023-08-04T18:33:49Z,fixed comments
github.com/georgian-io/LLM-Finetuning-Hub,inference/text_generation/merge_script.py,2023-08-04T14:01:25Z,added text generation
github.com/ghimiresunil/LLM-PowerHouse-A-Curated-Guide-for-Large-Language-Models-with-Custom-Training-and-Inferencing,example_codebase/train_inference_peft_lora/inference_peft.py,2023-12-01T02:06:59Z,Update: Branch Cleaning
github.com/horseee/LLM-Pruner,generate.py,2023-08-03T05:20:55Z,add an evaluation script
github.com/horseee/LLM-Pruner,generate.py,2023-05-21T17:34:08Z,"update generate.py. Add script for testing MACs, #param and memory"
github.com/horseee/LLM-Pruner,generate.py,2023-05-21T17:22:59Z,Update readme. Add code for generation
github.com/horseee/LLM-Pruner,LLMPruner/peft/peft_model.py,2023-07-24T07:51:35Z,upload code for evaluation
github.com/horseee/LLM-Pruner,LLMPruner/peft/peft_model.py,2023-05-17T17:15:09Z,add code
github.com/horseee/LLM-Pruner,LLMPruner/peft/tuners/prompt_tuning.py,2023-05-17T17:15:09Z,add code
github.com/horseee/LLM-Pruner,lm-evaluation-harness/lm_eval/models/huggingface.py,2023-07-24T07:51:35Z,upload code for evaluation
github.com/horseee/LLM-Pruner,test_speedup.py,2023-09-27T20:20:21Z,modify linear_dim -> head_dim * num_head
github.com/horseee/LLM-Pruner,test_speedup.py,2023-07-11T08:47:24Z,Add input constructor for testing MACs on GPU
github.com/horseee/LLM-Pruner,test_speedup.py,2023-07-07T07:32:49Z,Support test_speedup.py on CPU
github.com/horseee/LLM-Pruner,test_speedup.py,2023-05-21T17:34:08Z,"update generate.py. Add script for testing MACs, #param and memory"
github.com/zhuyiche/llava-phi,llava_phi/model/builder.py,2024-01-15T05:08:37Z,main
github.com/devsapp/fc-langchain-chatglm6b,src/code/chatglm6b-resource/models/moss_llm.py,2023-06-25T11:01:44Z,"feat: initialize fc-langchain-chatglm6b project

This commit initializes the fc-langchain-chatglm6b project with  dependencies."
github.com/devsapp/fc-langchain-chatglm6b,src/code/chatglm6b-resource/models/chatglm_llm.py,2023-06-25T11:01:44Z,"feat: initialize fc-langchain-chatglm6b project

This commit initializes the fc-langchain-chatglm6b project with  dependencies."
github.com/maszhongming/ReactionMiner,extraction/extractor.py,2024-01-23T15:56:41Z,init upload
github.com/replicate/cog-llama-template,llama_recipes/llama_finetuning.py,2023-11-09T18:59:52Z,apply ruff autofix
github.com/replicate/cog-llama-template,llama_recipes/llama_finetuning.py,2023-11-09T18:59:42Z,format!
github.com/replicate/cog-llama-template,llama_recipes/llama_finetuning.py,2023-09-13T15:07:50Z,"now with gradient checkpointing instead of micro batch size (#33)

* now with gradient checkpointing instead of micro batch size

* words words words"
github.com/replicate/cog-llama-template,llama_recipes/llama_finetuning.py,2023-09-06T15:06:11Z,"Faster, smaller cold starts (#27)

* use slim torch and merge pip installs
* entrypoint scripts aren't currently copied in multistage
* fake training
* Working hotswap loras with transformers
* use exllama for lora
* remove non-exllama code paths
* more fixes - filenames and envvar names
* add schema to makefile
* no system prompt
* 3.11
* configurable fused attn
* factor fused_attn option into config.py
* update all cog.yaml
* hostname logging
* try adding system_prompt in predict.py always, and removing it for non-chat models by setting __signature__
* update makefile to be aware there are two different schemas, and to use symlinks instead of copying when selecting a model
* don't reload lora if it hasn't changed
* fix BUILD_STAGE_DEPS and verify chat
* script to load secrets
* update schemas
* add ./.dockerignore to gitignore, remove all of the shared parts of models/*/.dockerignore, and have make select cat it with the template
* unset lora

---------

Co-authored-by: joe <joe@replicate.com>
Co-authored-by: technillogue <technillogue@gmail.com>"
github.com/replicate/cog-llama-template,llama_recipes/llama_finetuning.py,2023-08-25T18:08:01Z,Add support for qlora
github.com/replicate/cog-llama-template,llama_recipes/llama_finetuning.py,2023-08-24T20:54:09Z,catch empty dataloader and raise error
github.com/replicate/cog-llama-template,llama_recipes/llama_finetuning.py,2023-08-22T18:07:31Z,Expand unit tests and packing support in API
github.com/replicate/cog-llama-template,llama_recipes/llama_finetuning.py,2023-08-21T21:36:35Z,Add packing to train.py
github.com/replicate/cog-llama-template,llama_recipes/llama_finetuning.py,2023-08-21T21:15:01Z,"Switch collators, resize vocab"
github.com/replicate/cog-llama-template,llama_recipes/llama_finetuning.py,2023-08-18T04:13:10Z,adding configurable lora
github.com/replicate/cog-llama-template,llama_recipes/llama_finetuning.py,2023-08-17T21:14:09Z,Add support for validation to train.py
github.com/replicate/cog-llama-template,llama_recipes/llama_finetuning.py,2023-08-14T20:23:08Z,minimally working training
github.com/replicate/cog-llama-template,src/inference_engines/transformers_engine.py,2023-11-09T18:59:42Z,format!
github.com/replicate/cog-llama-template,src/inference_engines/transformers_engine.py,2023-10-24T05:27:03Z,"Engines (#47)

* choo choo

adding vLLM engine

---------

Co-authored-by: Moin Nadeem <moin@replicate.com>

* trivial fix

* Dan/vllm exllama engine (#46)

vllm + exllama, together at last

Co-authored-by: Moin Nadeem <moin@replicate.com>
Co-authored-by: technillogue <technillogue@gmail.com>

---------

Co-authored-by: technillogue <technillogue@gmail.com>
Co-authored-by: Moin Nadeem <Moinnadeem@moinnadeem.com>
Co-authored-by: Moin Nadeem <moin@replicate.com>"
github.com/princeton-nlp/SWE-bench,inference/run_llama.py,2023-11-20T08:28:14Z,Add run_llama.py to inference
github.com/HKUDS/GraphGPT,graphgpt/model/builder.py,2023-10-19T12:39:38Z,upload all code 19
github.com/HKUDS/GraphGPT,graphgpt/model/apply_lora.py,2023-10-19T12:39:38Z,upload all code 19
github.com/DAMO-NLP-SG/chain-of-knowledge,utils/retrieval/wikidata.py,2023-10-03T11:35:39Z,push code
github.com/DAMO-NLP-SG/chain-of-knowledge,utils/retrieval/scienceqa_bio.py,2023-10-03T11:35:39Z,push code
github.com/DAMO-NLP-SG/chain-of-knowledge,utils/retrieval/scienceqa_phy.py,2023-10-03T11:35:39Z,push code
github.com/togethercomputer/StableDiffusion,app/llava/model/builder.py,2023-11-06T14:47:21Z,Dockerfile will load llava dependences and requirements
github.com/DataCanvasIO/LMS,lms/runtime/prune/llm_pruner/LLMPruner/peft/peft_model.py,2023-11-01T10:43:45Z,lms v1.0.0
github.com/DataCanvasIO/LMS,lms/runtime/prune/llm_pruner/LLMPruner/peft/tuners/prompt_tuning.py,2023-11-01T10:43:45Z,lms v1.0.0
github.com/wjn1996/InstructGraph,examples/inference/llama.py,2024-01-17T22:42:41Z,add InstructGraph repo.
github.com/wjn1996/InstructGraph,src/inference/model_utils.py,2024-01-17T22:42:41Z,add InstructGraph repo.
github.com/wjn1996/InstructGraph,examples/demo/hf_text_generation_inference/merge_lora_weights.py,2024-01-17T22:42:41Z,add InstructGraph repo.
github.com/wjn1996/InstructGraph,examples/inference/preference_test.py,2024-02-14T17:57:33Z,update project
github.com/Miraclemarvel55/LLaMA-MOSS-RLHF-LoRA,merge_llama_with_chinese_lora_to_hf.py,2023-05-16T09:06:23Z,llama moss rlhf lora implemented
github.com/OpenSPG/openspg,python/nn4k/nn4k/executor/huggingface/hf_decode_only_executor.py,2024-02-22T06:08:21Z,"feat(nn4k): add huggingface decode only model local sft feature (#1) (#109)

Co-authored-by: xionghuaidong <huaidong.xhd@antgroup.com>"
github.com/modal-labs/modal-examples,06_gpu_and_ml/alpaca/alpaca_lora.py,2024-02-21T15:36:23Z,"Convert examples to all use lifecycle method decorators (#586)

* Convert examples to all use lifecycle method decorators

* Reinstante exc_info arguments on exit methods"
github.com/modal-labs/modal-examples,06_gpu_and_ml/alpaca/alpaca_lora.py,2024-02-12T19:32:23Z,"Update Ruff and remove Black (#578)

* Update Ruff and remove Black

* Format with ruff

* Fix typo

* Edit pyproject.toml and run formatting

* Add ruff-format to pre-commit"
github.com/modal-labs/modal-examples,06_gpu_and_ml/alpaca/alpaca_lora.py,2024-01-07T23:48:32Z,"Example simplification using `@build` (#507)

* Example simplification using __build__

* merge __build__ and __enter__

* add back type annotation that disappeared

* rewrite SDXL

* Rewrite it to use decorators

* Update SDXL Turbo

* Rewrite alpaca-lora

* embeddings/instructor.py – use @build

* Update webcam example"
github.com/modal-labs/modal-examples,06_gpu_and_ml/alpaca/alpaca_lora.py,2023-11-07T16:44:53Z,Fix examples now that we test against Python 3.11 (#490)
github.com/modal-labs/modal-examples,06_gpu_and_ml/alpaca/alpaca_lora.py,2023-08-19T21:06:56Z,Update examples to use .remote (#408)
github.com/modal-labs/modal-examples,06_gpu_and_ml/alpaca/alpaca_lora.py,2023-05-01T18:26:09Z,Add model caching & fix params for Alpaca LoRA (#299)
github.com/modal-labs/modal-examples,06_gpu_and_ml/alpaca/alpaca_lora.py,2023-04-18T18:49:08Z,Use cls for lifecycle classes (#280)
github.com/modal-labs/modal-examples,06_gpu_and_ml/alpaca/alpaca_lora.py,2023-04-06T08:26:34Z,Update deprecated decorators without args to always use () in examples (#269)
github.com/modal-labs/modal-examples,06_gpu_and_ml/alpaca/alpaca_lora.py,2023-04-04T14:48:24Z,ML/GPU subdirectory cleanup (#264)
github.com/modal-labs/modal-examples,06_gpu_and_ml/alpaca/alpaca_lora.py,2023-03-29T18:32:05Z,basic Alpaca LoRA example (#259)
github.com/catid/supercharger,server/model_baize.py,2023-04-11T22:30:27Z,Benchmark and switch to galpaca
github.com/dongyh20/Octopus,octopus/LLaVA/llava/model/builder.py,2024-02-28T08:24:27Z,Modified LLaVA and Otter within Octopus
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-08-24T02:10:48Z,"Adding support for upstage/solar 0 70b 8bit (#60)

* adding support for upstage/SOLAR-0-70b-8bit

* typo fix"
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-08-24T01:56:23Z,adding support for upstage/SOLAR-0-70b-8bit (#59)
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-07-26T22:29:52Z,Fix for loading local models when GPU map is specified
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-07-20T03:22:27Z,Quantize fix for 4bit (#48)
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-07-18T18:13:51Z,Fix stop words
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-07-14T19:28:42Z,"OOM fix by forcing device_map (#44)

* OOM fix by forcing device_map

* minor mention fix"
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-07-14T08:29:03Z,logic fix
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-07-13T23:43:01Z,Add instructcodet5p-16B (#41)
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-07-13T22:41:24Z,Fix for MPT and CodeGen OOM issues (#40)
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-07-13T15:11:46Z,"Add support for LoRA and 4-bit quantization (#36)

* Add support for LoRA and 4-bit quantization

* Fix base model name to be the lora adapter weights

* Added scipy as a dependency for peft

* Install accelerate and peft from source - required for flora

* Update model_utils.py

Co-authored-by: Charles Srisuwananukorn <1967608+csris@users.noreply.github.com>

* Update model_utils.py

Co-authored-by: Charles Srisuwananukorn <1967608+csris@users.noreply.github.com>

* Update model_utils.py

Co-authored-by: Charles Srisuwananukorn <1967608+csris@users.noreply.github.com>

* Update model_utils.py

Co-authored-by: Charles Srisuwananukorn <1967608+csris@users.noreply.github.com>

* Update model_utils.py

Co-authored-by: Charles Srisuwananukorn <1967608+csris@users.noreply.github.com>

* Add specific commits for source installs

---------

Co-authored-by: Charles Srisuwananukorn <1967608+csris@users.noreply.github.com>"
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-07-13T14:07:19Z,Add LLaMA no split module
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-07-13T12:23:59Z,"support llama65b and falcon40b (#38)

* support llama65b and falcon40b

* typo

* update falcon models to use the together fork

---------

Co-authored-by: Jared Baldridge <jrb@together.xyz>"
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-07-13T03:00:32Z,using together fork of T5 model
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-07-13T02:46:28Z,Add more T5-based models
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-07-11T23:28:56Z,remove return_token_type_ids mentions
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-06-29T20:51:35Z,Fix trust_remote_code for tokenizers
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-06-07T23:55:18Z,"Sets fp16 as default dtype, adds dtype support to HF models"
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-06-07T23:21:23Z,"Moved max_memory to model_utils and cleaned up model_path

Moved max_memory inside get_local_huggingface_tokenizer_model to simplify code structure
Set model_path to None by default, thus eliminating the if-else block in HuggingFaceLocalNLPModelInference"
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-06-07T22:12:37Z,"Adds no-return-token-type-ids arg and DecoderLayer

Falcon tokenizer requires return_token_type_ids=False. This can be set by adding the --no-return-token-type-ids flag
DecoderLayer is the no_split_layer used by the Falcon models. Used for multi-card support."
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-06-07T19:02:03Z,"Enable trust_remote_code for Falcon models

The Falcon models require trust_remote_code=True to be set to get config and models"
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-06-07T18:25:24Z,Add support for multi-card inference
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-06-05T21:03:01Z,Fix auth_token (#26)
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-06-05T03:06:55Z,"Remove unnecessary code

Cleans up auth_token code to eliminate possible type collision"
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-06-05T02:15:25Z,Update model_utils.py
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-06-05T02:09:07Z,Update model_utils.py to use auth_token
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-05-26T02:42:52Z,force togethercomputer/GPT-NeoXT-Chat-Base-20B to load in fp16
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-05-09T23:32:48Z,"instead of bailing at the bottom of the ifelse tree, just try loading the model name from hf"
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-05-09T22:26:38Z,adding additional models to model_utils instead of properly refactoring it
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-05-03T08:40:50Z,add openllama
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-05-03T08:31:53Z,correct togethercomputer/Pythia-Chat-Base-7B-v0.16 name
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-05-03T06:52:49Z,add togethercomputer/Pythia-Chat-Base-7B support
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-05-03T05:18:09Z,adding additional hf models. why refactor this when we can just keep adding to the ladder
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-04-20T03:37:02Z,"add stabilityai models

stablelm-base-alpha-7b and stablelm-base-alpha-3b"
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-03-24T02:28:44Z,llm.8bit
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-03-08T09:58:05Z,"WIP: add top_k/repetition_penalty  (#3)

* minor

* add top_k/repetition_penalty"
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-03-07T23:35:50Z,Add dtype to run opt 175m on mac laptop
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-02-27T14:43:04Z,support optiml175
github.com/togethercomputer/Quick_Deployment_HELM,model_utils.py,2023-02-07T11:00:54Z,debug glm
github.com/manyoso/haltt4llm,generate_trivia.py,2023-03-31T20:55:04Z,Add new TruthfulQA trivia set and update results.
github.com/manyoso/haltt4llm,generate_trivia.py,2023-03-30T22:38:00Z,And we have a new winner: GPT4All!
github.com/manyoso/haltt4llm,generate_trivia.py,2023-03-30T17:17:30Z,Minor prompt handling.
github.com/manyoso/haltt4llm,generate_trivia.py,2023-03-30T14:48:45Z,Add ability to generate trivia from local model
github.com/manyoso/haltt4llm,generate_trivia.py,2023-03-29T22:44:46Z,Add the initial work.
github.com/manyoso/haltt4llm,take_test.py,2023-03-30T22:38:00Z,And we have a new winner: GPT4All!
github.com/manyoso/haltt4llm,take_test.py,2023-03-30T17:17:30Z,Minor prompt handling.
github.com/manyoso/haltt4llm,take_test.py,2023-03-30T14:48:45Z,Add ability to generate trivia from local model
github.com/manyoso/haltt4llm,take_test.py,2023-03-29T22:44:46Z,Add the initial work.
github.com/Hritikbansal/sparse_feedback,inference/reranking.py,2023-11-16T19:41:23Z,Merge branch 'main' of https://github.com/Hritikbansal/sparse_feedback
github.com/Hritikbansal/sparse_feedback,inference/reranking.py,2023-11-16T19:41:00Z,add support for roberta (faster/lighter than alpaca)
github.com/Hritikbansal/sparse_feedback,inference/reranking.py,2023-11-13T04:59:14Z,Update reranking.py
github.com/Hritikbansal/sparse_feedback,inference/reranking.py,2023-11-13T01:19:20Z,Update reranking.py
github.com/Hritikbansal/sparse_feedback,inference/reranking.py,2023-08-29T20:09:21Z,Initial commit message
github.com/thcheung/FactLLaMA,generate.py,2023-06-07T08:52:05Z,first commit
github.com/yeyupiaoling/Chinese-LLM-Chat,ChatGLM/merge_lora.py,2023-06-25T08:37:32Z,Guanaco支持直接加载Lora模型推理，支持使用带有history数据微调，ChatGLM支持量化推理
github.com/yeyupiaoling/Chinese-LLM-Chat,ChatGLM/merge_lora.py,2023-05-14T06:25:39Z,修复参数输入和增加文档说明
github.com/yeyupiaoling/Chinese-LLM-Chat,ChatGLM/merge_lora.py,2023-05-11T12:12:35Z,优化代码和增加本地知识检索
github.com/yeyupiaoling/Chinese-LLM-Chat,ChatGLM/merge_lora.py,2023-05-10T15:24:30Z,first commit
github.com/yeyupiaoling/Chinese-LLM-Chat,Guanaco/merge_lora.py,2023-06-25T09:28:20Z,Guanaco支持baichuan-7B
github.com/yeyupiaoling/Chinese-LLM-Chat,Guanaco/merge_lora.py,2023-06-25T08:37:32Z,Guanaco支持直接加载Lora模型推理，支持使用带有history数据微调，ChatGLM支持量化推理
github.com/yeyupiaoling/Chinese-LLM-Chat,Guanaco/merge_lora.py,2023-06-21T15:05:56Z,优化训练和预测
github.com/yeyupiaoling/Chinese-LLM-Chat,Guanaco/merge_lora.py,2023-06-17T03:08:46Z,修改为QLora训练
github.com/yeyupiaoling/Chinese-LLM-Chat,Guanaco/utils/guanaco_predictor.py,2023-06-26T03:55:13Z,支持chatglm2推理
github.com/yeyupiaoling/Chinese-LLM-Chat,Guanaco/utils/guanaco_predictor.py,2023-06-25T09:28:20Z,Guanaco支持baichuan-7B
github.com/yeyupiaoling/Chinese-LLM-Chat,Guanaco/utils/guanaco_predictor.py,2023-06-25T08:37:32Z,Guanaco支持直接加载Lora模型推理，支持使用带有history数据微调，ChatGLM支持量化推理
github.com/yeyupiaoling/Chinese-LLM-Chat,Guanaco/utils/guanaco_predictor.py,2023-06-21T15:05:56Z,优化训练和预测
github.com/yeyupiaoling/Chinese-LLM-Chat,Guanaco/utils/guanaco_predictor.py,2023-06-17T03:08:46Z,修改为QLora训练
github.com/abhinand5/tamil-llama,scripts/train/utils/merge_adapter.py,2023-11-09T12:51:23Z,"Adding Tamil-LLaMA pretrain, finetune and sentencepiece scripts"
github.com/run-llama/modal_finetune_sql,src/inference_utils.py,2023-08-17T05:41:44Z,cr
github.com/DAMO-NLP-SG/VCD,experiments/llava/model/builder.py,2023-11-28T09:36:39Z,merge
github.com/codefuse-ai/MFTCoder,mftcoder_atorch/model/build_model.py,2024-01-04T07:59:42Z,MFTCoder v0.3.0 refactor
github.com/codefuse-ai/MFTCoder,mftcoder_atorch/model/peft/tuner/adalora.py,2024-01-04T07:59:42Z,MFTCoder v0.3.0 refactor
github.com/codefuse-ai/MFTCoder,mftcoder_atorch/model/peft/tuner/unipelt.py,2024-01-04T07:59:42Z,MFTCoder v0.3.0 refactor
github.com/codefuse-ai/MFTCoder,mftcoder_atorch/model/peft/modeling_peft.py,2024-01-04T07:59:42Z,MFTCoder v0.3.0 refactor
github.com/codefuse-ai/MFTCoder,mftcoder_atorch/model/peft/tuner/routelora.py,2024-01-04T07:59:42Z,MFTCoder v0.3.0 refactor
github.com/codefuse-ai/MFTCoder,mftcoder_accelerate/inference/hf_inference.py,2024-01-04T07:59:42Z,MFTCoder v0.3.0 refactor
github.com/codefuse-ai/MFTCoder,mftcoder_atorch/train/trainer/atorch_trainer.py,2024-01-04T07:59:42Z,MFTCoder v0.3.0 refactor
github.com/codefuse-ai/MFTCoder,mftcoder_atorch/utils/merge_base_and_lora_to_hf.py,2024-01-04T07:59:42Z,MFTCoder v0.3.0 refactor
github.com/codefuse-ai/MFTCoder,mftcoder_accelerate/src/pefts/merge_base_and_lora_to_hf.py,2024-01-08T12:29:12Z,"bugfix, fsdp for lora"
github.com/codefuse-ai/MFTCoder,mftcoder_accelerate/src/pefts/merge_base_and_lora_to_hf.py,2024-01-04T07:59:42Z,MFTCoder v0.3.0 refactor
github.com/ssundaram21/dreamsim,dreamsim/model.py,2023-07-27T13:59:50Z,fixed caching bug
github.com/ssundaram21/dreamsim,dreamsim/model.py,2023-07-14T17:09:27Z,Added finetuned single-branch models
github.com/ssundaram21/dreamsim,dreamsim/model.py,2023-06-16T00:07:26Z,Release
github.com/myshell-ai/AIlice,ailice/core/llm/AModelLLAMA.py,2024-02-27T09:09:05Z,1. reduce warnings.
github.com/myshell-ai/AIlice,ailice/core/llm/AModelLLAMA.py,2024-02-23T03:04:58Z,1. removed the peft dependency in the default installation.
github.com/myshell-ai/AIlice,ailice/core/llm/AModelLLAMA.py,2024-01-26T11:24:32Z,1. config huggingface models in config.json.
github.com/myshell-ai/AIlice,ailice/core/llm/AModelLLAMA.py,2024-01-26T09:32:30Z,"1. LLM configuration has been moved to config.json.
2. Support for inference services compatible with openai interface."
github.com/myshell-ai/AIlice,ailice/core/llm/AModelLLAMA.py,2024-01-08T09:27:36Z,"1. systemAsUser is model-related, so we will set it separately for each model in ALLMMeta."
github.com/myshell-ai/AIlice,ailice/core/llm/AModelLLAMA.py,2024-01-08T09:10:03Z,"1. set all the systemAsUser to False, including the one in finetuning. it works better in a finetuned model.
An unexpected thing is that although systemAsUser was set to the default True before, it was actually running as False because the coder-proxy modified the value when it was initialized. Ideally, we want to run as False, but It is not known whether it makes some models unfit."
github.com/myshell-ai/AIlice,ailice/core/llm/AModelLLAMA.py,2023-12-29T15:04:00Z,1. use absolute import.
github.com/myshell-ai/AIlice,ailice/core/llm/AModelLLAMA.py,2023-12-29T12:59:25Z,1. Directory structure adjustment.
github.com/generative-ai-on-aws/generative-ai-on-aws,07_rlhf/wip/trl_neuron.py,2023-12-20T04:16:14Z,[wip] added trn/inf examples
github.com/michael-wzhu/ChatMed,peft/peft_model.py,2023-05-05T14:24:04Z,2023/05/05: update codes & scripts
github.com/michael-wzhu/ChatMed,peft/tuners/prompt_tuning.py,2023-05-05T14:24:04Z,2023/05/05: update codes & scripts
github.com/michael-wzhu/ChatMed,src/chatmed_llama_peft/merge_llama_with_chinese_lora.py,2023-05-05T10:12:05Z,"2023/05/05: update model and readme, and examples"
github.com/rmihaylov/falcontune,falcontune/model/lora.py,2023-06-09T05:24:24Z,Use quant class instead of instance
github.com/rmihaylov/falcontune,falcontune/model/lora.py,2023-05-31T14:39:11Z,Initial commit
github.com/rmihaylov/falcontune,falcontune/model/falcon/model.py,2023-06-10T05:54:30Z,Revert fix
github.com/rmihaylov/falcontune,falcontune/model/falcon/model.py,2023-06-10T05:23:00Z,Fix multi-gpu
github.com/rmihaylov/falcontune,falcontune/model/falcon/model.py,2023-06-02T02:47:30Z,Add function to get decoder layer
github.com/rmihaylov/falcontune,falcontune/model/falcon/model.py,2023-06-02T02:47:30Z,Fix bos/eos/pad tokens
github.com/rmihaylov/falcontune,falcontune/model/falcon/model.py,2023-06-02T02:47:30Z,Update offloading
github.com/rmihaylov/falcontune,falcontune/model/falcon/model.py,2023-06-02T02:47:30Z,Remove inv_freq
github.com/rmihaylov/falcontune,falcontune/model/falcon/model.py,2023-06-02T02:47:30Z,Add cpu offloading
github.com/rmihaylov/falcontune,falcontune/model/falcon/model.py,2023-05-31T14:39:11Z,Initial commit
github.com/IceBear-CreditEase-LLM/aigc-admin,docker/vicuna/fastchat/model/apply_lora.py,2024-01-27T17:11:09Z,初始化项目
github.com/IceBear-CreditEase-LLM/aigc-admin,docker/vicuna/fastchat/model/model_adapter.py,2024-01-27T17:11:09Z,初始化项目
github.com/X-jun-0130/LLM-Pretrain-FineTune,Instructions_FineTune/Bloom_Lora_Sft.py,2023-04-22T12:12:42Z,Lora微调
github.com/trestad/mitigating-reversal-curse,generate.py,2023-11-14T06:14:26Z,first commit
github.com/davendw49/k2,generation/generate.py,2023-06-20T12:35:51Z,Update generate.py
github.com/davendw49/k2,generation/generate.py,2023-06-06T05:11:09Z,update readme
github.com/davendw49/k2,evaluation/run_eval.py,2023-10-13T01:34:53Z,eval
github.com/davendw49/k2,evaluation/run_eval.py,2023-10-12T16:43:08Z,add eval
github.com/davendw49/k2,export_hf_checkpoint.py,2023-06-05T19:16:37Z,init
github.com/davendw49/k2,export_state_dict_checkpoint.py,2023-06-05T19:16:37Z,init
github.com/pminervini/llm-service,chainer/base.py,2023-08-23T08:59:26Z,update
github.com/pminervini/llm-service,chainer/base.py,2023-08-17T12:55:32Z,update
github.com/pminervini/llm-service,chainer/base.py,2023-08-03T09:47:17Z,update
github.com/pminervini/llm-service,chainer/base.py,2023-07-29T08:02:46Z,update
github.com/pminervini/llm-service,chainer/base.py,2023-07-24T10:48:33Z,update
github.com/hppRC/llm-translator,src/demo.py,2023-12-30T15:47:57Z,add scripts
github.com/hppRC/llm-translator,src/misc/upload.py,2023-12-30T15:47:57Z,add scripts
github.com/hppRC/llm-translator,src/run_trained.py,2023-12-30T15:47:57Z,add scripts
github.com/dvlab-research/LLMGA,llmga/llava/model/builder.py,2023-12-07T08:20:07Z,Update builder.py
github.com/dvlab-research/LLMGA,llmga/llava/model/builder.py,2023-11-29T03:28:28Z,LLMGA
github.com/dvlab-research/LLMGA,llmga/diffusers/src/diffusers/models/modeling_utils.py,2023-11-29T03:28:28Z,LLMGA
github.com/Strong-AI-Lab/Logical-and-abstract-reasoning,src/models/hf.py,2024-01-17T10:43:40Z,updated configs and added validation sets
github.com/Strong-AI-Lab/Logical-and-abstract-reasoning,src/models/hf.py,2023-11-23T00:13:42Z,modified tokenizer configuration for compatibility with Transformers v4.35.2
github.com/Strong-AI-Lab/Logical-and-abstract-reasoning,src/models/hf.py,2023-11-22T22:43:34Z,added LLaMA2 and Zephyr models
github.com/Strong-AI-Lab/Logical-and-abstract-reasoning,src/models/hf.py,2023-09-26T08:47:03Z,added LoRA fine-tuning
github.com/Strong-AI-Lab/Logical-and-abstract-reasoning,src/models/hf.py,2023-09-14T09:29:18Z,LoRA fine-tuning
github.com/Strong-AI-Lab/Logical-and-abstract-reasoning,src/models/hf.py,2023-05-31T03:16:28Z,correction label formatting for fine-tuning
github.com/Strong-AI-Lab/Logical-and-abstract-reasoning,src/models/hf.py,2023-05-23T07:18:01Z,added llama and merit configs
github.com/Strong-AI-Lab/Logical-and-abstract-reasoning,src/models/hf.py,2023-05-15T04:58:44Z,added roberta fine-tuning and fixed bug
github.com/Strong-AI-Lab/Logical-and-abstract-reasoning,src/models/hf.py,2023-05-13T09:44:34Z,added Vicuna model and config
github.com/Strong-AI-Lab/Logical-and-abstract-reasoning,src/models/hf.py,2023-05-11T00:02:10Z,Added LLama and Alpca models
github.com/Strong-AI-Lab/Logical-and-abstract-reasoning,src/models/hf.py,2023-05-08T06:05:31Z,fix fine-tuning batch and token issues for MCQA
github.com/Strong-AI-Lab/Logical-and-abstract-reasoning,src/models/hf.py,2023-05-04T02:40:02Z,added fine-tuning script
github.com/Strong-AI-Lab/Logical-and-abstract-reasoning,src/models/hf.py,2023-05-03T04:05:09Z,fix HF generation split between answer and context
github.com/Strong-AI-Lab/Logical-and-abstract-reasoning,src/models/hf.py,2023-05-01T06:35:15Z,Merge branch 'dev' of https://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning into dev
github.com/Strong-AI-Lab/Logical-and-abstract-reasoning,src/models/hf.py,2023-05-01T06:26:58Z,fix truncation in HF completion models
github.com/Strong-AI-Lab/Logical-and-abstract-reasoning,src/models/hf.py,2023-05-01T05:03:18Z,update config for gpt-4 and new dataset
github.com/Strong-AI-Lab/Logical-and-abstract-reasoning,src/models/hf.py,2023-05-01T03:19:10Z,update model config
github.com/Strong-AI-Lab/Logical-and-abstract-reasoning,src/models/hf.py,2023-04-30T07:58:03Z,fix gpu use for MCQA models
github.com/Strong-AI-Lab/Logical-and-abstract-reasoning,src/models/hf.py,2023-04-28T05:55:57Z,better handling of huggingface generation
github.com/Strong-AI-Lab/Logical-and-abstract-reasoning,src/models/hf.py,2023-04-27T23:26:32Z,gpu handling for HF models
github.com/Strong-AI-Lab/Logical-and-abstract-reasoning,src/models/hf.py,2023-04-24T02:11:28Z,configuration for alpaca model
github.com/Strong-AI-Lab/Logical-and-abstract-reasoning,src/models/hf.py,2023-04-23T23:20:19Z,handling of batch inputs
github.com/Strong-AI-Lab/Logical-and-abstract-reasoning,src/models/hf.py,2023-04-20T22:04:34Z,logit correction and simple logging
github.com/Strong-AI-Lab/Logical-and-abstract-reasoning,src/models/hf.py,2023-04-20T11:32:41Z,generic evaluation script and models
github.com/alohachen/Hide-and-Seek,demo_model.py,2023-09-05T09:25:38Z,"Initial commit

Upload the hide and seek model's weight file and the inference demonstration code."
github.com/alohachen/Hide-and-Seek,demo_label.py,2023-09-05T09:25:38Z,"Initial commit

Upload the hide and seek model's weight file and the inference demonstration code."
github.com/StanleyLsx/llms_tool,engines/models.py,2023-12-08T04:07:40Z,支持Yi模型
github.com/StanleyLsx/llms_tool,engines/models.py,2023-11-23T10:06:03Z,1.简化推断代码；2.internlm支持rope
github.com/StanleyLsx/llms_tool,engines/models.py,2023-11-02T04:14:15Z,支持aquila2，以及aquilachat2-7b-16k的NTK
github.com/StanleyLsx/llms_tool,engines/models.py,2023-10-30T08:31:54Z,the Flash Attention method of the HF official currently only supports a few models.
github.com/StanleyLsx/llms_tool,engines/models.py,2023-10-30T08:17:12Z,通过attention_sinks支持StreamingLLM
github.com/StanleyLsx/llms_tool,engines/models.py,2023-10-30T06:03:04Z,modify name
github.com/StanleyLsx/llms_tool,engines/models.py,2023-10-30T03:42:37Z,chatglm3-6b-32k也支持通过rope_ratio来使用Rope
github.com/StanleyLsx/llms_tool,engines/models.py,2023-10-30T03:35:00Z,1.修复判断使用qlora错误的bug；2.将搜找所有全连接层放到model.py文件；3.bnb情况下为chatglm的量化之后再进行GPU分配
github.com/StanleyLsx/llms_tool,engines/models.py,2023-10-26T08:21:02Z,指定加载模型时的精度
github.com/StanleyLsx/llms_tool,engines/models.py,2023-10-25T02:58:24Z,Update models.py
github.com/StanleyLsx/llms_tool,engines/models.py,2023-10-25T02:28:34Z,修改代码使其符合PEP8规范
github.com/StanleyLsx/llms_tool,engines/models.py,2023-10-24T16:24:14Z,支持词表扩充功能
github.com/StanleyLsx/llms_tool,engines/models.py,2023-10-23T09:43:13Z,delete a line which is non sense
github.com/StanleyLsx/llms_tool,engines/models.py,2023-10-23T09:19:11Z,"1.修正一处qlore拼写笔误
2.词表扩展resize就可以了，lm_header会自动变化的，而且这里的操作很多情况都没有考虑到，比如_hf_hook和deep speed zero stage3初始化的支持，可以参考modeling_utils.PreTrainedModel._get_resized_lm_head方法"
github.com/StanleyLsx/llms_tool,engines/models.py,2023-10-12T11:02:01Z,support Mistral model
github.com/StanleyLsx/llms_tool,engines/models.py,2023-10-09T03:36:21Z,增加扩充词表后Embedding初始化方式
github.com/StanleyLsx/llms_tool,engines/models.py,2023-10-08T07:07:58Z,LLama和Falcon两类模型支持Flash Attention2
github.com/StanleyLsx/llms_tool,engines/models.py,2023-09-27T02:34:36Z,Update models.py
github.com/StanleyLsx/llms_tool,engines/models.py,2023-09-25T11:29:17Z,LLAMA的Flash Attention支持(未测试)
github.com/StanleyLsx/llms_tool,engines/models.py,2023-09-22T15:16:26Z,tigerbot通过template prompt支持，更新文档
github.com/StanleyLsx/llms_tool,engines/models.py,2023-09-21T14:40:05Z,推断显存再分配模型
github.com/StanleyLsx/llms_tool,engines/models.py,2023-09-18T07:03:44Z,支持tigerbot微调
github.com/StanleyLsx/llms_tool,engines/models.py,2023-09-07T10:03:15Z,支持使用Rope编码方式的Falcon使用ntk方式
github.com/StanleyLsx/llms_tool,engines/models.py,2023-09-04T10:04:07Z,支持部分模型在推断的时候使用ntk方法
github.com/StanleyLsx/llms_tool,engines/models.py,2023-08-28T02:55:52Z,只支持lora和adalora训练后的权重合并
github.com/StanleyLsx/llms_tool,engines/models.py,2023-08-28T02:40:08Z,除了lora和adalora以外模型不支持merge_and_unload方法
github.com/StanleyLsx/llms_tool,engines/models.py,2023-08-24T09:25:41Z,fix
github.com/StanleyLsx/llms_tool,engines/models.py,2023-08-24T09:23:44Z,支持deepspeed训练
github.com/StanleyLsx/llms_tool,engines/models.py,2023-08-23T08:19:35Z,支持DPO的训练
github.com/StanleyLsx/llms_tool,engines/models.py,2023-08-23T02:39:16Z,增加对xverse模型的支持
github.com/StanleyLsx/llms_tool,engines/models.py,2023-08-22T03:30:19Z,解决PPO训练过程中模型生成的问题
github.com/StanleyLsx/llms_tool,engines/models.py,2023-08-21T10:07:12Z,支持RLHF之PPO训练
github.com/StanleyLsx/llms_tool,engines/models.py,2023-08-17T03:39:03Z,继续完成PPO
github.com/StanleyLsx/llms_tool,engines/models.py,2023-08-16T15:37:13Z,fix
github.com/StanleyLsx/llms_tool,engines/models.py,2023-08-16T14:29:17Z,fix
github.com/StanleyLsx/llms_tool,engines/models.py,2023-08-16T12:09:25Z,将基类中的model加载方法分离以适应PPO模型强化训练
github.com/StanleyLsx/llms_tool,engines/models.py,2023-08-15T11:48:05Z,加载奖励模型进行批量训练
github.com/StanleyLsx/llms_tool,engines/models.py,2023-08-15T09:59:30Z,奖励模型批量测试
github.com/StanleyLsx/llms_tool,engines/models.py,2023-08-10T03:51:29Z,保存奖励模型
github.com/StanleyLsx/llms_tool,engines/models.py,2023-08-09T12:11:51Z,bug-fix
github.com/StanleyLsx/llms_tool,engines/models.py,2023-08-09T10:14:21Z,simplify code
github.com/StanleyLsx/llms_tool,engines/models.py,2023-08-09T05:12:49Z,Update models.py
github.com/StanleyLsx/llms_tool,engines/models.py,2023-08-09T05:07:24Z,bug-fix
github.com/StanleyLsx/llms_tool,engines/models.py,2023-08-09T02:35:56Z,Update models.py
github.com/StanleyLsx/llms_tool,engines/models.py,2023-08-08T16:14:34Z,bug-fix
github.com/StanleyLsx/llms_tool,engines/models.py,2023-08-08T15:53:05Z,cpm量化不用自己定义线性层
github.com/StanleyLsx/llms_tool,engines/models.py,2023-08-06T15:32:54Z,奖励模型训练
github.com/StanleyLsx/llms_tool,engines/models.py,2023-08-03T11:51:49Z,更新通义千问的支持
github.com/StanleyLsx/llms_tool,engines/models.py,2023-07-25T13:53:26Z,初始仓库
github.com/thunlp/ChatEval,FastChat/fastchat/model/apply_lora.py,2023-08-18T12:57:58Z,vicuna demo executable
github.com/thunlp/ChatEval,FastChat/fastchat/model/model_adapter.py,2023-08-18T12:57:58Z,vicuna demo executable
github.com/zhengbw0324/LC-Rec,test.py,2023-11-15T12:36:46Z,Initial commit
github.com/zhengbw0324/LC-Rec,test_ddp.py,2023-11-15T12:36:46Z,Initial commit
github.com/icoz69/StableLLAVA,llava/model/builder.py,2023-12-16T12:10:39Z,Add eval models
github.com/PKU-YuanGroup/Chat-UniVi,ChatUniVi/model/builder.py,2023-11-15T03:07:24Z,init
github.com/FSoft-AI4Code/CodeCapybara,main/train.py,2023-04-26T06:48:20Z,update hf model
github.com/FSoft-AI4Code/CodeCapybara,main/train.py,2023-04-24T13:49:20Z,add docs for training code
github.com/FSoft-AI4Code/CodeCapybara,main/train.py,2023-04-24T10:26:59Z,first commit
github.com/FSoft-AI4Code/CodeCapybara,main/generate.py,2023-04-24T13:02:47Z,Updated README.md
github.com/codefuse-ai/Test-Agent,chat/model/apply_lora.py,2023-10-20T14:14:57Z,init
github.com/codefuse-ai/Test-Agent,chat/model/model_adapter.py,2023-10-20T14:14:57Z,init
github.com/declare-lab/instruct-eval,modeling.py,2023-09-26T17:53:51Z,Fix multiple do_sample arguments
github.com/declare-lab/instruct-eval,modeling.py,2023-07-03T05:23:00Z,Update modeling.py OpenAIModel to use normal API
github.com/declare-lab/instruct-eval,modeling.py,2023-06-30T15:43:15Z,Code formatting with black
github.com/declare-lab/instruct-eval,modeling.py,2023-06-30T15:22:42Z,Support sampling during generation
github.com/declare-lab/instruct-eval,modeling.py,2023-06-30T15:22:42Z,More robust querying for OpenAIModel
github.com/declare-lab/instruct-eval,modeling.py,2023-06-05T09:05:46Z,Rename
github.com/declare-lab/instruct-eval,modeling.py,2023-06-03T09:10:13Z,Fixed error in modeling
github.com/declare-lab/instruct-eval,modeling.py,2023-06-02T20:59:26Z,Update modeling seq_to_seq get_choices
github.com/declare-lab/instruct-eval,modeling.py,2023-06-02T10:19:50Z,Refactored hhh.py
github.com/declare-lab/instruct-eval,modeling.py,2023-05-11T07:16:46Z,Add GPTQModel for LLaMA-65B
github.com/declare-lab/instruct-eval,modeling.py,2023-05-07T15:19:07Z,RWKVModel bugfix
github.com/declare-lab/instruct-eval,modeling.py,2023-05-07T14:27:16Z,Support RWKV
github.com/declare-lab/instruct-eval,modeling.py,2023-05-06T09:52:58Z,"Support causal models with remote code (eg Mosaic, StarCoder)"
github.com/declare-lab/instruct-eval,modeling.py,2023-05-04T17:57:19Z,Support OpenAI models
github.com/declare-lab/instruct-eval,modeling.py,2023-04-17T15:13:09Z,Support lora for seq_to_seq models (eg declare-lab/flan-alpaca-xl-lora)
github.com/declare-lab/instruct-eval,modeling.py,2023-04-17T15:13:09Z,Support llama-lora models with peft
github.com/declare-lab/instruct-eval,modeling.py,2023-04-16T09:01:27Z,Refactor for humaneval
github.com/declare-lab/instruct-eval,modeling.py,2023-04-11T16:25:22Z,"Merge pull request #1 from declare-lab/humaneval

Humaneval"
github.com/declare-lab/instruct-eval,modeling.py,2023-04-09T18:43:40Z,"Add Vicuna-13B, OpenChatKit-20B"
github.com/declare-lab/instruct-eval,modeling.py,2023-04-09T16:01:03Z,Add inference function for human eval
github.com/declare-lab/instruct-eval,modeling.py,2023-04-09T05:13:54Z,Add Koala-13B results for MMLU and BBH
github.com/declare-lab/instruct-eval,modeling.py,2023-04-08T18:28:05Z,Support load_8bit and add 13B llama/alpaca results
github.com/declare-lab/instruct-eval,modeling.py,2023-04-03T15:53:01Z,Refactor select_model for easier debugging
github.com/declare-lab/instruct-eval,modeling.py,2023-03-31T06:14:53Z,Add ChatGLM-6B
github.com/declare-lab/instruct-eval,modeling.py,2023-03-28T18:01:46Z,Add llama and alpaca results (no template works better)
github.com/declare-lab/instruct-eval,modeling.py,2023-03-28T13:50:10Z,Support CausalModel and LlamaModel in modeling.py
github.com/declare-lab/instruct-eval,modeling.py,2023-03-28T09:09:44Z,Refactor model generation to modeling.py
github.com/princeton-nlp/AutoCompressors,train.py,2024-02-19T06:36:54Z,Fix BF16 issue
github.com/princeton-nlp/AutoCompressors,train.py,2023-12-18T20:16:01Z,chore: rm dbg find unused parameters
github.com/princeton-nlp/AutoCompressors,train.py,2023-12-18T20:12:42Z,chore: rm dbg code
github.com/princeton-nlp/AutoCompressors,train.py,2023-12-18T19:56:08Z,fix: wrong llama import module name
github.com/princeton-nlp/AutoCompressors,train.py,2023-10-27T00:08:09Z,Rename Llama architecture and simplify args
github.com/princeton-nlp/AutoCompressors,train.py,2023-10-26T16:09:37Z,committing local changes
github.com/princeton-nlp/AutoCompressors,train.py,2023-10-11T15:44:42Z,Flash Llama + LoRA
github.com/princeton-nlp/AutoCompressors,train.py,2023-07-20T17:39:03Z,Support for training with fast attention
github.com/princeton-nlp/AutoCompressors,train.py,2023-06-27T06:28:24Z,Initial code release
github.com/blcuicall/taoli,src/generate.py,2023-06-09T01:10:04Z,Taoli llama
github.com/blcuicall/taoli,src/generate.py,2023-06-07T10:06:26Z,Chinese llama base
github.com/blcuicall/taoli,export_hf_checkpoint.py,2023-06-09T01:11:15Z,Taoli llama
github.com/blcuicall/taoli,export_hf_checkpoint.py,2023-06-07T10:06:26Z,Chinese llama base
github.com/blcuicall/taoli,export_state_dict_checkpoint.py,2023-06-09T01:11:15Z,Taoli llama
github.com/blcuicall/taoli,export_state_dict_checkpoint.py,2023-06-07T10:06:26Z,Chinese llama base
github.com/kaistAI/LangBridge,evaluation-harness/lm_eval/models/huggingface.py,2024-01-23T12:41:26Z,init
github.com/agiresearch/OpenP5,src/src_llama/generate_llama.py,2024-02-04T23:11:01Z,update the framework
github.com/agiresearch/OpenP5,src/src_llama/.ipynb_checkpoints/recommendation-checkpoint.py,2024-02-04T23:11:01Z,update the framework
github.com/agiresearch/OpenP5,src/src_llama/.ipynb_checkpoints/generate_llama-checkpoint.py,2024-02-04T23:11:01Z,update the framework
github.com/maziarraissi/PetGPT,alpaca-lora/generate.py,2023-04-02T23:18:57Z,first commit
github.com/maziarraissi/PetGPT,alpaca-lora/export_hf_checkpoint.py,2023-04-02T23:18:57Z,first commit
github.com/maziarraissi/PetGPT,alpaca-lora/export_state_dict_checkpoint.py,2023-04-02T23:18:57Z,first commit
github.com/maziarraissi/PetGPT,alpaca-lora/.ipynb_checkpoints/generate-checkpoint.py,2023-04-02T23:18:57Z,first commit
github.com/maziarraissi/PetGPT,alpaca-lora/.ipynb_checkpoints/export_hf_checkpoint-checkpoint.py,2023-04-02T23:18:57Z,first commit
github.com/maziarraissi/PetGPT,alpaca-lora/.ipynb_checkpoints/export_state_dict_checkpoint-checkpoint.py,2023-04-02T23:18:57Z,first commit
github.com/xiaoman-zhang/PMC-VQA,src/MedVInT_TE/peft/peft_model.py,2023-08-03T14:34:33Z,Add files via upload
github.com/xiaoman-zhang/PMC-VQA,src/MedVInT_TE/peft/tuners/prompt_tuning.py,2023-08-03T14:34:33Z,Add files via upload
github.com/databricks/databricks-ml-examples,llm-models/mpt/mpt-7b-8k/04_fine_tune_qlora.py,2024-02-15T06:36:11Z,"Add script for text generation examples (#103)

Add a script to generate the example notebooks for text-generation models"
github.com/databricks/databricks-ml-examples,llm-models/falcon/falcon-40b/06_fine_tune_qlora.py,2023-08-23T22:13:26Z,Inference code
github.com/databricks/databricks-ml-examples,llm-models/falcon/falcon-40b/06_fine_tune_qlora.py,2023-08-23T21:49:54Z,Add Falcon-40B QLoRA example
github.com/databricks/databricks-ml-examples,llm-models/mistral/mistral-7b/06_fine_tune_qlora.py,2023-10-09T06:16:56Z,"Add example notebooks for mistral-7b model (#76)

* Add example notebooks for mistral model

* fix

* Update and address the comments

* address the comments

* add readme

---------

Co-authored-by: lu-wang-dl <lu-wang-dl>"
github.com/databricks/databricks-ml-examples,llm-models/llamav2/llamav2-7b/06_fine_tune_qlora.py,2023-11-29T19:30:50Z,proposed fix form tokenizer not found. (#96)
github.com/databricks/databricks-ml-examples,llm-models/llamav2/llamav2-7b/06_fine_tune_qlora.py,2023-11-22T22:09:00Z,"Removed dataset version dependency and defaulted to latest for llama2 models (#94)

* Quick-Fix for dataset package

* Fixed datasets package deps for all other llama2 models"
github.com/databricks/databricks-ml-examples,llm-models/llamav2/llamav2-7b/06_fine_tune_qlora.py,2023-08-19T00:17:45Z,llama2 7b and llama213b azure instance types
github.com/databricks/databricks-ml-examples,llm-models/llamav2/llamav2-7b/06_fine_tune_qlora.py,2023-08-10T22:00:46Z,Update 06_fine_tune_qlora.py
github.com/databricks/databricks-ml-examples,llm-models/llamav2/llamav2-7b/06_fine_tune_qlora.py,2023-08-09T10:32:09Z,HF transformers fix - https://discuss.huggingface.co/t/help-with-llama-2-finetuning-setup/50035/4
github.com/databricks/databricks-ml-examples,llm-models/llamav2/llamav2-7b/06_fine_tune_qlora.py,2023-08-08T18:08:50Z,Add do_sample
github.com/databricks/databricks-ml-examples,llm-models/llamav2/llamav2-7b/06_fine_tune_qlora.py,2023-08-08T06:37:01Z,Ping accelerate version to avoid bugs
github.com/databricks/databricks-ml-examples,llm-models/llamav2/llamav2-7b/06_fine_tune_qlora.py,2023-08-04T18:29:00Z,Update 06_fine_tune_qlora.py
github.com/databricks/databricks-ml-examples,llm-models/llamav2/llamav2-7b/06_fine_tune_qlora.py,2023-08-04T18:24:34Z,"Update 06_fine_tune_qlora.py

Update the base model for lora fine tune."
github.com/databricks/databricks-ml-examples,llm-models/llamav2/llamav2-7b/06_fine_tune_qlora.py,2023-07-24T16:51:10Z,Update QLORA example (#33)
github.com/databricks/databricks-ml-examples,llm-models/llamav2/llamav2-7b/06_fine_tune_qlora.py,2023-07-19T04:13:57Z,"Update the example (#28)

Co-authored-by: lu-wang-dl <lu-wang-dl>"
github.com/databricks/databricks-ml-examples,llm-models/llamav2/llamav2-7b/06_fine_tune_qlora.py,2023-07-18T17:47:30Z,"Add examples for llama-2 (#24)

* format

* Add llama-2 example

* Add readme

---------

Co-authored-by: lu-wang-dl <lu-wang-dl>"
github.com/databricks/databricks-ml-examples,llm-models/llamav2/llamav2-13b/06_fine_tune_qlora.py,2023-11-22T22:09:00Z,"Removed dataset version dependency and defaulted to latest for llama2 models (#94)

* Quick-Fix for dataset package

* Fixed datasets package deps for all other llama2 models"
github.com/databricks/databricks-ml-examples,llm-models/llamav2/llamav2-13b/06_fine_tune_qlora.py,2023-08-22T20:18:26Z,Recommend AWS instance types
github.com/databricks/databricks-ml-examples,llm-models/llamav2/llamav2-13b/06_fine_tune_qlora.py,2023-08-19T00:17:45Z,llama2 7b and llama213b azure instance types
github.com/databricks/databricks-ml-examples,llm-models/llamav2/llamav2-13b/06_fine_tune_qlora.py,2023-08-16T05:53:46Z,Fix llama 2 13b qlora example
github.com/databricks/databricks-ml-examples,llm-models/llamav2/llamav2-13b/06_fine_tune_qlora.py,2023-08-10T22:02:41Z,Update 06_fine_tune_qlora.py
github.com/databricks/databricks-ml-examples,llm-models/llamav2/llamav2-13b/06_fine_tune_qlora.py,2023-07-31T19:07:56Z,"[ML-33321] Add qlora fine-tuning notebook for llama2-13b (#38)

* Init

* Clean

* A100 comment"
github.com/databricks/databricks-ml-examples,llm-models/llamav2/llamav2-70b/06_fine_tune_qlora.py,2023-11-22T22:09:00Z,"Removed dataset version dependency and defaulted to latest for llama2 models (#94)

* Quick-Fix for dataset package

* Fixed datasets package deps for all other llama2 models"
github.com/databricks/databricks-ml-examples,llm-models/llamav2/llamav2-70b/06_fine_tune_qlora.py,2023-09-07T06:25:53Z,"Add qlora fine tune example for 70b model (#61)

Co-authored-by: lu-wang-dl <lu-wang-dl>"
github.com/databricks/databricks-ml-examples,llm-models/mistral/mistral-7b/06_fine_tune_qlora_marketplace.py,2023-11-01T19:25:36Z,"Add fine tune mistrial_7b from marketplace (#87)

Co-authored-by: lu-wang-dl <lu-wang-dl>"
github.com/databricks/databricks-ml-examples,llm-models/llamav2/llamav2-7b/06_fine_tune_qlora_marketplace.py,2023-11-22T22:09:00Z,"Removed dataset version dependency and defaulted to latest for llama2 models (#94)

* Quick-Fix for dataset package

* Fixed datasets package deps for all other llama2 models"
github.com/databricks/databricks-ml-examples,llm-models/llamav2/llamav2-7b/06_fine_tune_qlora_marketplace.py,2023-10-26T18:45:53Z,"Add QLORA fine tune example for the models from marketplace (#85)

* Add qlora from marketplace

* Update the model serving part

* address the comments

* Add examples for 13b model

---------

Co-authored-by: lu-wang-dl <lu-wang-dl>"
github.com/databricks/databricks-ml-examples,llm-models/llamav2/llamav2-13b/06_fine_tune_qlora_marketplace.py,2023-11-22T22:09:00Z,"Removed dataset version dependency and defaulted to latest for llama2 models (#94)

* Quick-Fix for dataset package

* Fixed datasets package deps for all other llama2 models"
github.com/databricks/databricks-ml-examples,llm-models/llamav2/llamav2-13b/06_fine_tune_qlora_marketplace.py,2023-10-26T18:45:53Z,"Add QLORA fine tune example for the models from marketplace (#85)

* Add qlora from marketplace

* Update the model serving part

* address the comments

* Add examples for 13b model

---------

Co-authored-by: lu-wang-dl <lu-wang-dl>"
github.com/SHI-Labs/VCoder,vcoder_llava/model/builder.py,2023-12-25T09:50:41Z,:tada: Release VCoder
github.com/llava-rlhf/LLaVA-RLHF,Eval/model_vqa.py,2023-11-01T02:19:38Z,add mmhal-bench evaluation script
github.com/llava-rlhf/LLaVA-RLHF,Eval/model_vqa.py,2023-10-19T21:33:16Z,eval sft
github.com/llava-rlhf/LLaVA-RLHF,demo/model_worker.py,2023-10-08T09:44:58Z,add a minimal example for launching demo
github.com/llava-rlhf/LLaVA-RLHF,Eval/model_mmbench.py,2023-10-19T21:33:16Z,eval sft
github.com/llava-rlhf/LLaVA-RLHF,demo/model_builder.py,2023-10-08T09:44:58Z,add a minimal example for launching demo
github.com/llava-rlhf/LLaVA-RLHF,Eval/model_vqa_mmhal.py,2023-11-01T04:35:20Z,update MMHal evaluation code
github.com/llava-rlhf/LLaVA-RLHF,Eval/model_vqa_logit.py,2023-10-19T21:33:16Z,eval sft
github.com/llava-rlhf/LLaVA-RLHF,RLHF/models/reward_model.py,2023-10-14T12:25:27Z,add RLHF code
github.com/jackaduma/Vicuna-LoRA-RLHF-PyTorch,merge_peft_adapter.py,2023-04-24T15:15:41Z,update
github.com/jackaduma/Vicuna-LoRA-RLHF-PyTorch,merge_peft_adapter.py,2023-04-23T16:21:05Z,update
github.com/jackaduma/Vicuna-LoRA-RLHF-PyTorch,merge_peft_adapter.py,2023-04-22T06:06:16Z,add merge_peft_adapter in Vicuna
github.com/zjukg/KnowPAT,inference.py,2023-11-11T08:45:24Z,first commit
github.com/zzlgreat/smart_agent,special_mind/fllama_api.py,2023-09-24T02:29:39Z,add some factors
github.com/zzlgreat/smart_agent,special_mind/fllama_api.py,2023-09-20T06:09:55Z,add some factors
github.com/zzlgreat/smart_agent,special_mind/fllama_api.py,2023-09-18T12:48:33Z,add some factors
github.com/zzlgreat/smart_agent,special_mind/task_class.py,2023-09-06T10:05:39Z,add some sectors
github.com/zzlgreat/smart_agent,special_mind/task_class.py,2023-09-06T09:39:24Z,first commit
github.com/zzlgreat/smart_agent,trainer/func_caller_train.py,2023-09-19T02:41:45Z,add some factors
github.com/mzbac/qlora-fine-tune,inference.py,2023-06-01T16:03:03Z,init
github.com/mzbac/qlora-fine-tune,merge_peft_adapters.py,2023-06-01T16:03:03Z,init
github.com/HC-Guo/Owl,Multiple_Choice/model/lora_llama2.py,2023-10-08T18:56:54Z,Initial open-source owl
github.com/ddzipp/AutoAudit,sandbox/manage.py,2023-07-15T06:07:41Z,change directory names and commit django project: SandBox
github.com/ddzipp/AutoAudit,sandbox/AutoAudit/apps.py,2023-07-15T08:34:58Z,Update apps.py
github.com/ddzipp/AutoAudit,sandbox/AutoAudit/apps.py,2023-07-15T08:30:42Z,Update apps.py
github.com/ddzipp/AutoAudit,sandbox/AutoAudit/apps.py,2023-07-15T08:23:29Z,add integrated dataset directory
github.com/ddzipp/AutoAudit,sandbox/AutoAudit/apps.py,2023-07-15T07:55:26Z,"V0.0.1 (#2)

* change directory names and commit django project: SandBox

* Update apps.py

---------

Co-authored-by: lilBuffaloEric <ericde1920@foxmail.com>
Co-authored-by: lilBuffaloEric <113868733+lilBuffaloEric@users.noreply.github.com>"
github.com/ddzipp/AutoAudit,sandbox/AutoAudit/apps.py,2023-07-15T06:07:41Z,change directory names and commit django project: SandBox
github.com/sdan/selfextend,modeling_utils.py,2024-01-06T04:41:26Z,init
github.com/OpenGVLab/ControlLLM,cllm/services/llama2/llama2.py,2023-12-25T15:58:25Z,ControlLLM initial release
github.com/rui-ye/OpenFedLLM,utils/merge_lora.py,2024-01-29T08:55:29Z,"[rui/eval] include open-ended eval

- MT-Bench (GPT eval)
- Vicuna Bench (GPT eval)
- Advbench (Rule-based eval)"
github.com/rui-ye/OpenFedLLM,evaluation/open_ended/gen_model_answer.py,2024-03-03T15:33:09Z,assign model_name to 'generator' value
github.com/rui-ye/OpenFedLLM,evaluation/open_ended/gen_model_answer.py,2024-03-03T15:13:51Z,[evaluation/open_ended] modify gen_model_answer.py
github.com/rui-ye/OpenFedLLM,evaluation/open_ended/gen_model_answer.py,2024-01-29T08:55:29Z,"[rui/eval] include open-ended eval

- MT-Bench (GPT eval)
- Vicuna Bench (GPT eval)
- Advbench (Rule-based eval)"
github.com/rui-ye/OpenFedLLM,evaluation/open_ended/gen_model_answer_mt.py,2024-02-09T09:05:44Z,"[xyd/fixup] fix logic flow and cleanup

- raise error when meet unsupported arg
- remove commented (thus unused) part
- construct reference model in case of non-peft
  - e.g. for DPO"
github.com/rui-ye/OpenFedLLM,evaluation/open_ended/gen_model_answer_mt.py,2024-01-29T08:55:29Z,"[rui/eval] include open-ended eval

- MT-Bench (GPT eval)
- Vicuna Bench (GPT eval)
- Advbench (Rule-based eval)"
github.com/snap-stanford/MLAgentBench,MLAgentBench/benchmarks/llama-inference/env/inference.py,2023-09-01T22:15:46Z,initial commit
github.com/DaoD/INTERS,evaluation/qdu-tasks/src/modeling.py,2024-03-01T02:27:01Z,Update modeling.py
github.com/DaoD/INTERS,evaluation/qdu-tasks/src/modeling.py,2024-03-01T02:26:43Z,Update modeling.py
github.com/DaoD/INTERS,evaluation/qdu-tasks/src/modeling.py,2024-03-01T02:24:36Z,Update modeling.py
github.com/DaoD/INTERS,evaluation/qdu-tasks/src/modeling.py,2024-02-18T09:52:50Z,Add files via upload
github.com/tsb0601/MMVP,LLaVA/llava/model/builder.py,2024-01-10T19:01:52Z,Initial Commit
github.com/plm353557719/imClumsyPanda,models/loader/loader.py,2023-05-18T14:54:41Z,"llm_model_dict 处理了loader的一些预设行为，如加载位置，模型名称，模型处理器实例, 定义checkpoint名称和远程路径
loader.py: 模型重载
定义 generatorAnswer 增加 AnswerResultStream
   定义generate_with_callback收集器，在每次响应时将队列数据同步到AnswerResult
requirements.txt 变更项目依赖"
github.com/leapingjagg-dev/SLEB,lm-evaluation-harness/lm_eval/models/huggingface.py,2024-02-06T09:52:26Z,now
github.com/sotopia-lab/sotopia,lmlib/serve/lm_inference.py,2024-02-05T19:18:34Z,"bumped langchain and openai version (#25)

* bumped langchain and openai version

* langchain.openai -> langchain_openai

* remove unused type ignore"
github.com/sotopia-lab/sotopia,lmlib/serve/lm_inference.py,2024-01-07T21:49:32Z,Sync private repo with this public repo (#12)
github.com/sotopia-lab/sotopia,lmlib/serve/lm_inference.py,2023-07-23T03:08:59Z,"Fix agentprofile indexing bug (#72)

* 🐛 fix agentprofile indexing bug

* 🏷️ removed unused ignore type

* 🏷️ trying to fix the bugs

* 👷 update workflow and remove mypy ignore"
github.com/sotopia-lab/sotopia,lmlib/serve/lm_inference.py,2023-05-08T00:25:29Z,"lmlib api (#46)

* stream and nonstream api

* fix mypy

* add requirements

* fix untype and rm dataset in this pr"
github.com/sotopia-lab/sotopia,lmlib/serve/lm_inference.py,2023-05-04T19:03:15Z,"Load Locally hosted LLMs (#43)

* fix lint

* scripts

* fix name to pass mypy

* add init

* add init

* 🏷️ added some types and stubs

* 🏷️ add  more types

* 🏷️ ignore transformer errors

* 🐛 fix mypy errors

* 💚 disable warn unused type ignore

---------

Co-authored-by: Hao Zhu <prokilchu@gmail.com>"
github.com/austrian-code-wizard/c3po,src/eval.py,2024-02-16T02:12:39Z,fixing local run args
github.com/austrian-code-wizard/c3po,src/eval.py,2024-02-13T07:58:41Z,post-submission commit
github.com/austrian-code-wizard/c3po,src/eval.py,2024-02-01T04:30:20Z,adding multi feedback and base model
github.com/austrian-code-wizard/c3po,src/eval.py,2024-01-28T09:06:57Z,making answer quality eval invariant to feedback
github.com/austrian-code-wizard/c3po,src/eval.py,2024-01-27T02:11:49Z,improve GPT response handling
github.com/austrian-code-wizard/c3po,src/eval.py,2024-01-27T01:54:23Z,refactored eval
github.com/austrian-code-wizard/c3po,src/eval.py,2024-01-26T05:54:36Z,added result quality eval
github.com/austrian-code-wizard/c3po,src/eval.py,2024-01-26T04:05:25Z,fixing merge
github.com/austrian-code-wizard/c3po,src/eval.py,2024-01-26T03:54:38Z,Merge branch 'main' into annie
github.com/austrian-code-wizard/c3po,src/eval.py,2024-01-26T03:51:43Z,addressed PR feedback
github.com/austrian-code-wizard/c3po,src/eval.py,2024-01-26T01:01:38Z,Basic cot eval
github.com/austrian-code-wizard/c3po,src/eval.py,2024-01-25T18:28:34Z,improved api call robustness
github.com/austrian-code-wizard/c3po,src/eval.py,2024-01-25T10:07:55Z,"Initial CoT sampling, refactoring"
github.com/austrian-code-wizard/c3po,src/eval.py,2024-01-24T18:53:21Z,refactored new feedback
github.com/austrian-code-wizard/c3po,src/eval.py,2024-01-24T00:03:51Z,fixing eval of general prompts
github.com/austrian-code-wizard/c3po,src/eval.py,2024-01-18T23:20:13Z,fixed eval file naming
github.com/austrian-code-wizard/c3po,src/eval.py,2024-01-18T01:44:41Z,added parameter sweep feature
github.com/austrian-code-wizard/c3po,src/eval.py,2024-01-18T01:21:19Z,fixed revision prompt and added filters
github.com/austrian-code-wizard/c3po,src/eval.py,2024-01-16T07:16:14Z,fixed eval bugs
github.com/austrian-code-wizard/c3po,src/eval.py,2024-01-16T02:54:03Z,fixed eval not using OOD data
github.com/austrian-code-wizard/c3po,src/eval.py,2024-01-16T02:36:57Z,fixed eval bugs
github.com/austrian-code-wizard/c3po,src/eval.py,2024-01-16T01:21:51Z,added eval and train
github.com/austrian-code-wizard/c3po,src/eval.py,2024-01-14T00:52:51Z,added models
github.com/austrian-code-wizard/c3po,src/modal/serve.py,2024-02-15T16:48:08Z,allowing modal serve using different methods
github.com/austrian-code-wizard/c3po,src/modal/serve.py,2024-02-14T07:04:44Z,added modal inference endpoint
github.com/OpenMOSS/collie,collie/config.py,2023-09-07T11:32:29Z,Update collie/config.py
github.com/OpenMOSS/collie,collie/config.py,2023-09-07T11:32:23Z,Update collie/config.py
github.com/OpenMOSS/collie,collie/config.py,2023-09-07T11:32:14Z,Update collie/config.py
github.com/OpenMOSS/collie,collie/config.py,2023-09-07T11:32:07Z,Update collie/config.py
github.com/OpenMOSS/collie,collie/config.py,2023-09-07T07:34:52Z,优化collie初始化方式
github.com/OpenMOSS/collie,collie/config.py,2023-09-03T12:55:34Z,修复部分参数加载Bug，从零初始化Pipeline并行，Tensor并行，非并行的Bug
github.com/OpenMOSS/collie,collie/config.py,2023-08-31T06:13:41Z,fix: import PeftConfig directly from peft
github.com/OpenMOSS/collie,collie/config.py,2023-08-22T13:28:41Z,small
github.com/OpenMOSS/collie,collie/config.py,2023-08-22T13:13:23Z,delete setup_deepspeed
github.com/OpenMOSS/collie,collie/config.py,2023-08-22T10:01:50Z,set ds_config's default type to dict
github.com/OpenMOSS/collie,collie/config.py,2023-08-22T08:40:58Z,fix set ds_config
github.com/OpenMOSS/collie,collie/config.py,2023-08-02T06:38:50Z,align train_micro_batch and gradient_accumulate_steps in __post_init__
github.com/OpenMOSS/collie,collie/config.py,2023-08-01T05:53:50Z,add: worker support for dataloader
github.com/OpenMOSS/collie,collie/config.py,2023-07-27T12:21:08Z,support saving config to petrel
github.com/OpenMOSS/collie,collie/config.py,2023-07-26T09:25:50Z,fix: bugs for python3.11
github.com/OpenMOSS/collie,collie/config.py,2023-07-25T04:33:39Z,set checkpointing to model_config
github.com/OpenMOSS/collie,collie/config.py,2023-07-20T03:07:31Z,typo of init_method
github.com/OpenMOSS/collie,collie/config.py,2023-07-06T05:20:41Z,update tutorial-123
github.com/OpenMOSS/collie,collie/config.py,2023-07-05T02:41:03Z,add: peft utils
github.com/OpenMOSS/collie,collie/config.py,2023-07-01T14:59:33Z,typo
github.com/OpenMOSS/collie,collie/config.py,2023-07-01T14:58:29Z,typo
github.com/OpenMOSS/collie,collie/config.py,2023-06-30T14:51:08Z,add: int4 support
github.com/OpenMOSS/collie,collie/config.py,2023-06-28T09:19:47Z,fix: pipelinegeneration
github.com/OpenMOSS/collie,collie/config.py,2023-06-16T11:46:09Z,add: example for further pretraining
github.com/OpenMOSS/collie,collie/config.py,2023-06-12T05:51:15Z,fix imports of collie.trainer; fix docs
github.com/OpenMOSS/collie,collie/config.py,2023-06-07T05:08:24Z,conflict
github.com/OpenMOSS/collie,collie/config.py,2023-06-06T10:24:35Z,add: 添加注释
github.com/OpenMOSS/collie,collie/config.py,2023-06-06T09:41:33Z,add: add peft with zero3
github.com/OpenMOSS/collie,collie/config.py,2023-06-06T09:25:06Z,add: add peft config
github.com/OpenMOSS/collie,collie/config.py,2023-06-05T03:27:46Z,Add callback trigger in trainer; move to cpu when gather at metrics
github.com/OpenMOSS/collie,collie/config.py,2023-06-01T08:23:25Z,Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev
github.com/OpenMOSS/collie,collie/config.py,2023-06-01T08:22:47Z,fix docs; get slice_group using _pg_groups to be compatible with lower versions
github.com/OpenMOSS/collie,collie/config.py,2023-06-01T08:14:10Z,fix: bugs in drivers
github.com/OpenMOSS/collie,collie/config.py,2023-05-31T11:18:55Z,update docs
github.com/OpenMOSS/collie,collie/config.py,2023-05-30T07:33:38Z,fix: bugs in trainer
github.com/OpenMOSS/collie,collie/config.py,2023-05-18T11:12:54Z,add: zero3
github.com/OpenMOSS/collie,collie/config.py,2023-05-18T07:03:46Z,support setattr to model_config
github.com/OpenMOSS/collie,collie/config.py,2023-05-18T06:42:53Z,move arguments.py to collie/config.py
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-12-31T06:51:44Z,fix: only permute kv_cache for pp
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-12-31T05:05:50Z,remove useless comments
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-12-30T15:06:25Z,add comments
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-12-23T13:53:41Z,fix print
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-12-23T13:46:07Z,add resume train
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-10-28T13:49:42Z,update kv_cache and train_epoch_end
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-10-17T14:45:55Z,Update trainer.py
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-10-17T14:44:08Z,"Revert ""fix: adalomo has no loss_scaler but loss_scale""

This reverts commit 1dc252e46522b7e8070bdeda7bb2f4c00e906496."
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-10-17T14:29:31Z,fix: adalomo has no loss_scaler but loss_scale
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-10-17T08:08:06Z,rename _merge_peft  to pp_merge_peft
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-10-17T07:23:09Z,add: adalomo optim
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-10-13T05:40:59Z,refactor: move load_peft to peft_utils
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-09-04T06:06:38Z,misc: format trainer.py by black
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-08-25T05:41:56Z,peft load bug
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-08-23T10:51:57Z,fix config
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-08-23T08:39:15Z,gather on rank 0 to avoid oom
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-08-22T08:47:55Z,Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-08-22T08:47:09Z,complete tests for LoadBestCallback CheckpointCallback and save&load_checkpoint
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-08-10T07:38:05Z,update lomo
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-08-10T07:36:00Z,Merge remote-tracking branch 'upstream/dev' into dev
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-08-09T06:30:46Z,Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-08-09T06:28:09Z,prefix tuning
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-08-07T08:51:30Z,update requments order
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-08-03T11:13:45Z,update lomo
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-08-02T06:38:50Z,align train_micro_batch and gradient_accumulate_steps in __post_init__
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-08-02T03:34:22Z,Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-08-01T06:17:26Z,support p-tuning save&load
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-08-01T05:53:50Z,add: worker support for dataloader
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-08-01T02:54:08Z,Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-08-01T02:54:04Z,support p-tuning save
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-31T12:31:49Z,fix lomo with tp
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-31T05:00:13Z,fix lomo with lr_scheduler
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-30T11:05:49Z,Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-30T11:05:38Z,add: flashv2
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-28T08:28:43Z,Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-28T08:22:38Z,update save_peft and load_peft(lora)
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-28T04:49:05Z,fix: bugs in saveing llama
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-17T09:51:27Z,fix: bugs in moss and trainer
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-16T09:07:52Z,lomo: check dtype in backward
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-14T13:45:48Z,fix bf16+lomo (wip)
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-11T16:09:40Z,fix: bugs in collie
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-11T05:16:45Z,"Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev

Conflicts:
	collie/__init__.py
	collie/models/__init__.py"
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-11T04:45:33Z,Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-11T04:44:40Z,merge peft checkpoint in pp
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-10T13:14:12Z,fix: bugs in wandb
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-10T04:29:54Z,"Merge pull request #74 from KaiLv69/dev

fix bf16+zero3"
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-09T14:32:51Z,fix bug when save zero3 checkpoint with lomo
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-09T14:28:45Z,empty cache after init model
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-09T07:48:58Z,fix: docs
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-07T09:03:46Z,fix merge
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-07T03:16:54Z,prompt tuning for llama
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-06T08:32:23Z,fix: bugs in trainer
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-06T08:20:01Z,fix: bugs in trainer
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-06T07:13:08Z,merge
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-06T07:07:15Z,fix
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-06T07:05:04Z,Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-06T07:04:01Z,update: save petf
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-06T06:59:36Z,fix: zero3 generation
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-06T02:21:09Z,Merge remote-tracking branch 'origin/dev' into dev
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-06T02:21:04Z,fix: bugs for prompt-tuning
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-05T18:48:56Z,complete save_peft and load_peft
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-05T15:26:53Z,merge
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-05T12:43:07Z,fix: bugs
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-05T12:35:36Z,fix: bugs in lora+pp
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-05T02:41:03Z,add: peft utils
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-03T11:14:20Z,add: callbacks for lora
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-01T12:03:42Z,Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-07-01T12:03:39Z,Add on_setup_parallel_model
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-30T14:51:08Z,add: int4 support
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-28T09:19:47Z,fix: pipelinegeneration
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-27T10:14:07Z,fix: bugs in lomo
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-27T06:51:22Z,fix: trainer
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-26T12:51:28Z,Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-26T12:51:25Z,modify: data structure
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-25T12:59:56Z,fit trainer to new lomo
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-23T12:34:45Z,fix: bugs in focs
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-19T08:04:56Z,rename: lomo
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-19T06:52:13Z,fix: finetuning llama example
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-19T04:45:10Z,fix: bugs in overwrite
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-16T17:01:07Z,add: example for expand vocab
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-15T10:40:17Z,Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-15T10:34:22Z,add: example for translation
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-15T02:59:07Z,Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-14T18:34:02Z,fix: generation
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-14T16:33:46Z,Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-14T16:31:20Z,fix: bugs in pipeline
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-14T13:37:16Z,Add attention_mask to moss; optimize progress bar with reset
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-12T05:51:15Z,fix imports of collie.trainer; fix docs
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-11T07:54:38Z,fix: eval_monitor
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-09T12:32:07Z,Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-09T12:21:06Z,Merge branch 'dev' of github.com:OpenLMLab/collie into dev
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-09T12:18:34Z,fix: lr_monitor
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-09T10:26:19Z,Merge branch 'dev' of github.com:OpenLMLab/collie into dev
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-09T10:25:34Z,fix: lr未定义
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-09T09:14:10Z,Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-09T08:38:02Z,Merge branch 'dev' of github.com:OpenLMLab/collie into dev
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-09T08:37:43Z,fix: data_provider
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-09T07:38:15Z,add: 添加记录学习率的monitor
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-09T03:01:48Z,Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev
github.com/OpenMOSS/collie,collie/controller/trainer.py,2023-06-08T12:16:26Z,"Merge branch 'dev' of github.com:OpenLMLab/collie into dev

Conflicts:
	collie/__init__.py
	collie/controller/evaluator.py
	collie/controller/trainer.py
	tests/trainer/_test_trainer_metric_evaluator.py"
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-11-04T14:09:48Z,update docs in evaluator
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-11-04T14:03:22Z,update docs in evaluator
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-09-15T02:44:06Z,fix: bugs with classification in helm style
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-09-14T05:49:16Z,fix: bug with special token of classification in helm style
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-09-14T05:14:39Z,style: format evaluator.py by black
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-08-01T05:53:50Z,add: worker support for dataloader
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-07-30T11:05:38Z,add: flashv2
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-07-12T08:56:00Z,fix: bugs in evaluator
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-07-12T06:44:37Z,update: evaluator for classification
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-07-09T12:54:05Z,fix: bugs in classification training
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-07-09T07:48:58Z,fix: docs
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-07-07T03:16:54Z,prompt tuning for llama
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-07-07T02:12:38Z,add: prompt tuning
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-07-06T07:48:09Z,Merge remote-tracking branch 'origin/dev' into dev
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-07-06T07:48:04Z,fix: bugs in evaluator for prompt tuning
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-07-06T06:59:51Z,fix: zero3 generation
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-07-05T02:41:03Z,add: peft utils
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-06-28T09:19:47Z,fix: pipelinegeneration
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-06-26T12:51:25Z,modify: data structure
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-06-19T08:04:56Z,rename: lomo
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-06-16T17:01:07Z,add: example for expand vocab
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-06-15T10:34:22Z,add: example for translation
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-06-14T18:34:02Z,fix: generation
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-06-14T16:31:20Z,fix: bugs in pipeline
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-06-12T05:51:15Z,fix imports of collie.trainer; fix docs
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-06-11T07:54:38Z,fix: eval_monitor
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-06-09T09:14:10Z,Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-06-09T08:37:43Z,fix: data_provider
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-06-09T03:01:48Z,Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-06-08T12:16:26Z,"Merge branch 'dev' of github.com:OpenLMLab/collie into dev

Conflicts:
	collie/__init__.py
	collie/controller/evaluator.py
	collie/controller/trainer.py
	tests/trainer/_test_trainer_metric_evaluator.py"
github.com/OpenMOSS/collie,collie/controller/evaluator.py,2023-06-08T12:07:00Z,add: dataset for classification
github.com/OpenMOSS/collie,examples/peft/merge_weights.py,2023-10-12T02:40:22Z,refactor: merge_weights.py
github.com/shehuiwojiege/open-llms-next-web,utils.py,2023-12-03T09:28:09Z,'修改utils'
github.com/shehuiwojiege/open-llms-next-web,utils.py,2023-12-03T09:01:08Z,first commit
github.com/liangwq/Chatglm_lora_multi-gpu,webui/web_feadback.py,2023-04-02T03:00:24Z,增加webui交互界面
github.com/liangwq/Chatglm_lora_multi-gpu,postprocess/merge_lora.py,2023-04-10T04:14:03Z,add merge lora
github.com/liangwq/Chatglm_lora_multi-gpu,postprocess/merge_lora.py,2023-04-06T01:47:32Z,merge  lora
github.com/liangwq/Chatglm_lora_multi-gpu,postprocess/merge_lora_basemodel.py,2023-04-10T04:14:03Z,add merge lora
github.com/liangwq/Chatglm_lora_multi-gpu,postprocess/merge_lora_basemodel.py,2023-04-04T11:22:05Z,lora和basemodel合并脚本
github.com/liangwq/Chatglm_lora_multi-gpu,APP_example/lora_sd/inference_cript.py,2023-04-14T13:51:31Z,"add lora sd

新增stable diffusion的lora训练
后期结合chatglm项目用"
github.com/liangwq/Chatglm_lora_multi-gpu,APP_example/auto_prompt_image_genrator/main.py,2023-04-05T03:11:26Z,app fix token bug
github.com/liangwq/Chatglm_lora_multi-gpu,APP_example/auto_prompt_image_genrator/main.py,2023-04-02T15:44:05Z,"增加生成图prompt，然后作图应用

1.新增chatglm生图prompt
2.新增生成图直接sd生成图能力"
github.com/liangwq/Chatglm_lora_multi-gpu,APP_example/chatglm_agent/knowledge_extractor/knowledge_extract_tool.py,2023-07-25T20:40:20Z,add knowledege eatractor tool
github.com/baaivision/JudgeLM,judgelm/model/apply_lora.py,2023-10-26T19:41:48Z,[add] initial commit.
github.com/baaivision/JudgeLM,judgelm/model/model_adapter.py,2023-10-26T19:41:48Z,[add] initial commit.
github.com/SeanLee97/AnglE,scripts/push_llm_model.py,2023-11-02T03:28:54Z,move to scripts
github.com/kaistAI/CoT-Collection,CoT_Finetuning/src/T5.py,2023-09-11T12:28:25Z,[PUSh] backup
github.com/findalexli/SciGraphQA,llava/eval/model_vqa.py,2023-06-11T23:01:07Z,Fix bugs
github.com/findalexli/SciGraphQA,llava/eval/model_vqa.py,2023-06-11T22:05:20Z,Update scripts and docs
github.com/findalexli/SciGraphQA,llava/eval/model_vqa.py,2023-05-01T00:54:16Z,Update eval
github.com/findalexli/SciGraphQA,llava/eval/model_vqa.py,2023-04-18T00:16:37Z,LLaVA initial release
github.com/findalexli/SciGraphQA,llava/serve/model_worker.py,2023-06-11T23:01:07Z,Fix bugs
github.com/findalexli/SciGraphQA,llava/serve/model_worker.py,2023-06-11T22:05:20Z,Update scripts and docs
github.com/findalexli/SciGraphQA,llava/serve/model_worker.py,2023-06-10T03:45:11Z,Update
github.com/findalexli/SciGraphQA,llava/serve/model_worker.py,2023-05-10T17:58:37Z,Improve worker logic
github.com/findalexli/SciGraphQA,llava/serve/model_worker.py,2023-05-07T05:46:19Z,Fix eval prompt
github.com/findalexli/SciGraphQA,llava/serve/model_worker.py,2023-05-06T15:58:22Z,Add MPT inference
github.com/findalexli/SciGraphQA,llava/serve/model_worker.py,2023-05-06T01:48:49Z,Update MPT serving
github.com/findalexli/SciGraphQA,llava/serve/model_worker.py,2023-05-01T01:17:12Z,Fix v0/v1 loading
github.com/findalexli/SciGraphQA,llava/serve/model_worker.py,2023-04-30T04:53:25Z,Optimize for v1
github.com/findalexli/SciGraphQA,llava/serve/model_worker.py,2023-04-19T02:55:21Z,Optimize worker and demo
github.com/findalexli/SciGraphQA,llava/serve/model_worker.py,2023-04-18T00:16:37Z,LLaVA initial release
github.com/NLP-Core-Team/mmlu_ru,mmlu_ru.py,2023-07-19T16:35:36Z,feat: added llama2
github.com/NLP-Core-Team/mmlu_ru,mmlu_ru.py,2023-06-28T13:56:56Z,feat: inital coommit
github.com/geronimi73/qlora-minimal,merge_qlora.py,2023-11-14T06:35:58Z,Update merge_qlora.py
github.com/geronimi73/qlora-minimal,merge_qlora.py,2023-11-14T06:32:12Z,Update merge_qlora.py
github.com/geronimi73/qlora-minimal,merge_qlora.py,2023-11-05T15:02:55Z,check-in please
github.com/kaistAI/InstructIR,eval/eval_instructir_dense.py,2024-02-22T07:05:04Z,Initial commit
github.com/shuxueslpi/chatGLM-6B-QLoRA,inference_qlora.py,2023-06-04T02:51:29Z,QLoRA for chatglm-6b
github.com/shuxueslpi/chatGLM-6B-QLoRA,merge_lora_and_quantize.py,2023-06-28T10:22:08Z,支持chatGLM2-6B
github.com/shuxueslpi/chatGLM-6B-QLoRA,merge_lora_and_quantize.py,2023-06-10T03:18:17Z,新增模型推理性能测试
github.com/shuxueslpi/chatGLM-6B-QLoRA,merge_lora_and_quantize.py,2023-06-07T07:08:39Z,增加训练后lora模型的融合及量化
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-25T08:16:01Z,update webui
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-25T07:47:01Z,update rereank.
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-25T06:32:00Z,update demo.
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-24T12:45:48Z,update rerank model predict.
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-24T10:33:02Z,update expand context chunk logic.
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-15T03:08:23Z,update torch type.
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-11T10:17:40Z,update corpus emb save.
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-11T10:10:41Z,update  sim model.
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-11T10:04:56Z,update  sim model.
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-11T08:53:02Z,update rag demo for bm25 sim model.
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-11T08:51:30Z,update rag demo for bm25 sim model.
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-11T06:57:47Z,update sim model.
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-10T13:15:57Z,update sim model.
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-09T10:34:24Z,update batch rag demo.
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-09T10:29:25Z,update batch rag demo.
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-09T10:13:19Z,update batch rag demo.
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-09T09:16:17Z,update logger.
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-09T09:00:21Z,"update chat with pdf, no history."
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-09T08:18:00Z,update demo.
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-08T13:35:15Z,update logger.
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-08T10:41:36Z,add chunk split.
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-08T10:32:06Z,add chunk split.
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-08T10:25:25Z,add chunk split.
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-08T10:15:33Z,add chunk split.
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-08T09:51:57Z,update sim model
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-08T09:16:27Z,update chatpdf webui.
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-08T07:22:28Z,update chatpdf.
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-06T13:36:32Z,update chatpdf.
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-06T12:30:45Z,update webui.
github.com/shibing624/ChatPDF,chatpdf.py,2024-01-06T09:54:59Z,update sim model.
github.com/shibing624/ChatPDF,chatpdf.py,2023-11-16T04:06:55Z,update emb save.
github.com/shibing624/ChatPDF,chatpdf.py,2023-11-16T03:55:53Z,update sim interface.
github.com/shibing624/ChatPDF,chatpdf.py,2023-08-06T15:43:19Z,update chat webui.
github.com/shibing624/ChatPDF,chatpdf.py,2023-08-06T15:28:16Z,update chat webui.
github.com/shibing624/ChatPDF,chatpdf.py,2023-08-06T15:19:04Z,update gen
github.com/shibing624/ChatPDF,chatpdf.py,2023-08-06T15:15:38Z,update gen config.
github.com/shibing624/ChatPDF,chatpdf.py,2023-08-06T14:58:18Z,update chat stream.
github.com/shibing624/ChatPDF,chatpdf.py,2023-07-25T11:29:19Z,update chat.
github.com/shibing624/ChatPDF,chatpdf.py,2023-07-25T10:05:59Z,update int.
github.com/shibing624/ChatPDF,chatpdf.py,2023-07-25T10:04:45Z,update demo.
github.com/shibing624/ChatPDF,chatpdf.py,2023-07-25T09:28:39Z,update demo.
github.com/shibing624/ChatPDF,chatpdf.py,2023-07-25T09:19:37Z,update chatpdf with custom gen model.
github.com/shibing624/ChatPDF,chatpdf.py,2023-07-18T07:32:09Z,update usage.
github.com/shibing624/ChatPDF,chatpdf.py,2023-07-18T07:29:07Z,update model.
github.com/shibing624/ChatPDF,chatpdf.py,2023-07-17T06:40:42Z,+  mps support for Similarity
github.com/shibing624/ChatPDF,chatpdf.py,2023-07-17T05:49:55Z,+ ChatGLM2-6B support  & bug fix
github.com/shibing624/ChatPDF,chatpdf.py,2023-07-10T16:27:06Z,update gen model.
github.com/shibing624/ChatPDF,chatpdf.py,2023-06-13T08:32:49Z,update chat model.
github.com/shibing624/ChatPDF,chatpdf.py,2023-04-27T07:19:09Z,Merge branch 'main' of github.com:zhongpei/ChatPDF
github.com/shibing624/ChatPDF,chatpdf.py,2023-04-27T06:35:43Z,add options for chatpdf
github.com/shibing624/ChatPDF,chatpdf.py,2023-04-27T03:19:27Z,update web.
github.com/shibing624/ChatPDF,chatpdf.py,2023-04-27T02:19:49Z,add topn for webui
github.com/shibing624/ChatPDF,chatpdf.py,2023-04-23T02:36:01Z,update answer.
github.com/shibing624/ChatPDF,chatpdf.py,2023-04-19T12:35:30Z,update web ui.
github.com/shibing624/ChatPDF,chatpdf.py,2023-04-17T14:01:38Z,update gen res.
github.com/shibing624/ChatPDF,chatpdf.py,2023-04-17T13:58:49Z,update gen res.
github.com/shibing624/ChatPDF,chatpdf.py,2023-04-17T13:53:31Z,update gen.
github.com/shibing624/ChatPDF,chatpdf.py,2023-04-17T13:51:30Z,update gen.
github.com/shibing624/ChatPDF,chatpdf.py,2023-04-17T13:30:34Z,update init.
github.com/shibing624/ChatPDF,chatpdf.py,2023-04-17T13:24:12Z,add chatpdf function.
github.com/jiawei-ren/dreamgaussian4d,diffusers/src/diffusers/models/modeling_utils.py,2023-12-28T08:22:09Z,init
github.com/jiawei-ren/dreamgaussian4d,diffusers/src/diffusers/pipelines/pipeline_utils.py,2023-12-28T08:22:09Z,init
github.com/SAI990323/TALLRec,evaluate.py,2023-06-13T13:46:02Z,update evaluate
github.com/SAI990323/TALLRec,evaluate.py,2023-06-06T09:05:11Z,in batch evaluation
github.com/SAI990323/TALLRec,evaluate.py,2023-06-06T09:01:50Z,multi batch
github.com/SAI990323/TALLRec,evaluate.py,2023-06-04T08:00:34Z,update evaluate.sh
github.com/SAI990323/TALLRec,evaluate.py,2023-04-30T09:44:42Z,update the shell and the evaluate code
github.com/SAI990323/TALLRec,evaluate.py,2023-04-10T06:23:11Z,initial commit
github.com/SAI990323/TALLRec,export_hf_checkpoint.py,2023-04-10T06:23:11Z,initial commit
github.com/SAI990323/TALLRec,export_state_dict_checkpoint.py,2023-04-10T06:23:11Z,initial commit
github.com/Jiuzhouh/Uncertainty-Aware-Language-Agent,run_mmlu_llama2.py,2024-01-25T05:14:49Z,Add files via upload
github.com/Jiuzhouh/Uncertainty-Aware-Language-Agent,run_hotpotqa_llama2.py,2024-01-25T05:14:49Z,Add files via upload
github.com/Jiuzhouh/Uncertainty-Aware-Language-Agent,run_strategyqa_llama2.py,2024-01-25T05:14:49Z,Add files via upload
github.com/Synthintel0/MyGirlGPT,opendan-text-generation-webui/modules/LoRA.py,2023-06-07T05:52:40Z,update LLM Server
github.com/mbzuai-oryx/GeoChat,geochat/model/builder.py,2024-02-27T20:15:10Z,geochat_uploaded
github.com/lc222/BELLE-LORA,generate.py,2023-03-24T12:04:44Z,add generate
github.com/namin/llm-verified-with-monte-carlo-tree-search,huggingface_generate.py,2023-12-26T10:56:26Z,configs
github.com/namin/llm-verified-with-monte-carlo-tree-search,huggingface_generate.py,2023-12-19T21:49:58Z,some tweaks: common can_be_solution and same_for_many_samples option
github.com/namin/llm-verified-with-monte-carlo-tree-search,huggingface_generate.py,2023-12-07T07:08:16Z,restore default
github.com/namin/llm-verified-with-monte-carlo-tree-search,huggingface_generate.py,2023-12-07T06:39:13Z,fix intent
github.com/namin/llm-verified-with-monte-carlo-tree-search,huggingface_generate.py,2023-12-07T05:35:02Z,tweaks
github.com/namin/llm-verified-with-monte-carlo-tree-search,huggingface_generate.py,2023-12-07T03:04:56Z,some stats
github.com/namin/llm-verified-with-monte-carlo-tree-search,huggingface_generate.py,2023-12-07T02:30:23Z,minor indent
github.com/namin/llm-verified-with-monte-carlo-tree-search,huggingface_generate.py,2023-12-07T01:45:21Z,optional beam search
github.com/namin/llm-verified-with-monte-carlo-tree-search,huggingface_generate.py,2023-12-03T00:13:39Z,use diversity as a common lib
github.com/namin/llm-verified-with-monte-carlo-tree-search,huggingface_generate.py,2023-11-21T01:08:33Z,fix: huggingface typo
github.com/Abbey4799/CuteGPT,inference_ft.py,2023-07-20T07:10:37Z,Solve the confusion caused by the default parameters of the preprocessing function; Solve the problem of template inconsistency between training and reasoning
github.com/Abbey4799/CuteGPT,inference_ft.py,2023-07-06T09:06:25Z,update readme and finetuning code
github.com/Abbey4799/CuteGPT,inference_lora.py,2023-07-20T07:10:37Z,Solve the confusion caused by the default parameters of the preprocessing function; Solve the problem of template inconsistency between training and reasoning
github.com/Abbey4799/CuteGPT,inference_lora.py,2023-07-06T09:06:25Z,update readme and finetuning code
github.com/bupticybee/FastLoRAChat,generate.py,2023-04-12T03:26:56Z,:new: now rollout normally
github.com/bupticybee/FastLoRAChat,generate.py,2023-04-09T21:07:59Z,"Update export_hf_checkpoint.py (#302)

* Update export_hf_checkpoint.py

* Update finetune.py

New tokenizer base model for the current dev branch of transformers

* Update generate.py

* Update export_state_dict_checkpoint.py

* Update export_hf_checkpoint.py"
github.com/bupticybee/FastLoRAChat,generate.py,2023-04-04T15:05:20Z,Support streaming output on generate (#263)
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-30T15:57:40Z,Fix server_name
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-30T15:57:40Z,"Added Dockerfile and docker-compose.yml (#207)

* Added Dockerfile for inference

* Added instructions for Dockerfile

* Update README.md

* Update README.md

* Update README.md

* Pass env through Dockerfile

* Added docker compose setup and instructions

* Added more environment options

* Set a safer default mount point

* add docker-compose changes

* Added Dockerfile for inference

* Added instructions for Dockerfile

* Update README.md

* Update README.md

* Update README.md

* Pass env through Dockerfile

* Added docker compose setup and instructions

* Added more environment options

* Set a safer default mount point

* add to gitignore, update to new generate.py

* add docker ignore, simplify docker compose file

* add back missing requirements

* Adjustments to compose and generate.py, added Docker to README.md

* Linting adjust to Black

* Adjusting import linting

* Update README.md

* Update README.md

* Removed comment by original Dockerfile creator.

Comment not necessary.

* cleanup README

Co-authored-by: Francesco Saverio Zuppichini <zuppif@usi.ch>

---------

Co-authored-by: Francesco Saverio Zuppichini <zuppif@usi.ch>
Co-authored-by: Chris Alexiuk <c.s.alexiuk@gmail.com>
Co-authored-by: ElRoberto538 <>
Co-authored-by: Sam Sipe <samsipe@gmail.com>
Co-authored-by: Eric J. Wang <eric.james.wang@gmail.com>"
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-29T23:36:04Z,"Templated prompter (#184)

* Templated prompter

* fix dup import

* Set Verbose False by default

I forgot to disable after testing.

* Fix imports order

* Use Black Formatting

* lint

* Re-introduce lost line

* Cleanup

* template default

* isort

---------

Co-authored-by: Eric Wang <eric.james.wang@gmail.com>"
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-28T16:43:29Z,"Add option to share Gradio demo publicly (#189)

* Add option to share Gradio demo publicly

* gradio_share -> share_gradio

---------

Co-authored-by: Eric Wang <eric.james.wang@gmail.com>"
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-28T15:33:47Z,remove asserts
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-27T17:31:44Z,"Add HF dataset loading, add linters, pyproject.toml (#175)

* add HF dataset loading, add linters, pyproject.toml

- applied markdownlint
- add black, black[jupyter], isort
- fix noqa codes
- add .github workflow linting
- update README.md

* restore default settings

* resume_from_checkpoint

Co-authored-by: AngainorDev <54739135+AngainorDev@users.noreply.github.com>

* Print warning on checkpoint not found

* add HF dataset loading, add linters, pyproject.toml

- applied markdownlint
- add black, black[jupyter], isort
- fix noqa codes
- add .github workflow linting
- update README.md

* Default to local copy and update it

* Typo

* Remove duplicate code block

---------

Co-authored-by: Eric Wang <eric.james.wang@gmail.com>
Co-authored-by: AngainorDev <54739135+AngainorDev@users.noreply.github.com>"
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-24T21:18:42Z,"Use CLI arguments (#159)

* CLI args for finetune

* Update README

* CLI args for generate.py

* reqs.txt

* reorder hyperparams

* lora_target_modules

* cleanup"
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-23T20:54:39Z,"Remove LLaMA download code, as a precaution"
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-23T20:44:45Z,"bos, eos in generate.py"
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-21T21:31:30Z,fix fp16 inference
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-19T22:53:21Z,slider for tokens generated
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-19T18:22:02Z,Remove messy test code
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-19T06:00:18Z,generate.py tweaks
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-18T23:43:53Z,don't share publicly
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-17T22:07:08Z,min beams = 1
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-17T20:53:21Z,Enable inference on CPU and Mac GPU using pytorch support for MPS (#48)
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-17T02:30:27Z,"Update generate.py

Adapting to the input function, a text box for inputting content has been added."
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-16T23:04:06Z,add Gradio interface to generate.py
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-16T19:11:47Z,Catch outdated installs
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-16T19:11:29Z,Update alpaca-lora to use transformers main branch
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-16T16:59:10Z,Expand sampling in generate.py for new test
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-16T07:05:32Z,Add counting test
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-16T00:22:22Z,"generate.py memory, perf updates"
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-15T18:11:26Z,torch.no_grad
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-15T04:41:02Z,add text-davinci-003 to comparisons
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-15T04:33:12Z,Update README.md with new checkpoint details
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-14T22:10:33Z,Ready to go
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-14T00:23:29Z,decapoda
github.com/bupticybee/FastLoRAChat,generate.py,2023-03-13T22:00:05Z,Licenses and whatnot
github.com/bupticybee/FastLoRAChat,export_hf_checkpoint.py,2023-04-17T09:04:14Z,:new: add readme
github.com/bupticybee/FastLoRAChat,export_hf_checkpoint.py,2023-04-17T03:37:04Z,:new: add stuff
github.com/bupticybee/FastLoRAChat,export_hf_checkpoint.py,2023-04-12T03:26:56Z,:new: now rollout normally
github.com/bupticybee/FastLoRAChat,export_hf_checkpoint.py,2023-04-09T21:07:59Z,"Update export_hf_checkpoint.py (#302)

* Update export_hf_checkpoint.py

* Update finetune.py

New tokenizer base model for the current dev branch of transformers

* Update generate.py

* Update export_state_dict_checkpoint.py

* Update export_hf_checkpoint.py"
github.com/bupticybee/FastLoRAChat,export_hf_checkpoint.py,2023-03-28T15:33:47Z,remove asserts
github.com/bupticybee/FastLoRAChat,export_hf_checkpoint.py,2023-03-27T17:31:44Z,"Add HF dataset loading, add linters, pyproject.toml (#175)

* add HF dataset loading, add linters, pyproject.toml

- applied markdownlint
- add black, black[jupyter], isort
- fix noqa codes
- add .github workflow linting
- update README.md

* restore default settings

* resume_from_checkpoint

Co-authored-by: AngainorDev <54739135+AngainorDev@users.noreply.github.com>

* Print warning on checkpoint not found

* add HF dataset loading, add linters, pyproject.toml

- applied markdownlint
- add black, black[jupyter], isort
- fix noqa codes
- add .github workflow linting
- update README.md

* Default to local copy and update it

* Typo

* Remove duplicate code block

---------

Co-authored-by: Eric Wang <eric.james.wang@gmail.com>
Co-authored-by: AngainorDev <54739135+AngainorDev@users.noreply.github.com>"
github.com/bupticybee/FastLoRAChat,export_hf_checkpoint.py,2023-03-23T20:54:39Z,"Remove LLaMA download code, as a precaution"
github.com/bupticybee/FastLoRAChat,export_hf_checkpoint.py,2023-03-18T23:42:58Z,fix HF export script
github.com/bupticybee/FastLoRAChat,export_hf_checkpoint.py,2023-03-18T00:56:10Z,HF export script
github.com/bupticybee/FastLoRAChat,export_state_dict_checkpoint.py,2023-04-09T21:07:59Z,"Update export_hf_checkpoint.py (#302)

* Update export_hf_checkpoint.py

* Update finetune.py

New tokenizer base model for the current dev branch of transformers

* Update generate.py

* Update export_state_dict_checkpoint.py

* Update export_hf_checkpoint.py"
github.com/bupticybee/FastLoRAChat,export_state_dict_checkpoint.py,2023-03-28T15:33:47Z,remove asserts
github.com/bupticybee/FastLoRAChat,export_state_dict_checkpoint.py,2023-03-27T17:31:44Z,"Add HF dataset loading, add linters, pyproject.toml (#175)

* add HF dataset loading, add linters, pyproject.toml

- applied markdownlint
- add black, black[jupyter], isort
- fix noqa codes
- add .github workflow linting
- update README.md

* restore default settings

* resume_from_checkpoint

Co-authored-by: AngainorDev <54739135+AngainorDev@users.noreply.github.com>

* Print warning on checkpoint not found

* add HF dataset loading, add linters, pyproject.toml

- applied markdownlint
- add black, black[jupyter], isort
- fix noqa codes
- add .github workflow linting
- update README.md

* Default to local copy and update it

* Typo

* Remove duplicate code block

---------

Co-authored-by: Eric Wang <eric.james.wang@gmail.com>
Co-authored-by: AngainorDev <54739135+AngainorDev@users.noreply.github.com>"
github.com/bupticybee/FastLoRAChat,export_state_dict_checkpoint.py,2023-03-23T20:54:39Z,"Remove LLaMA download code, as a precaution"
github.com/bupticybee/FastLoRAChat,export_state_dict_checkpoint.py,2023-03-16T19:11:47Z,Catch outdated installs
github.com/bupticybee/FastLoRAChat,export_state_dict_checkpoint.py,2023-03-16T19:11:29Z,Update alpaca-lora to use transformers main branch
github.com/bupticybee/FastLoRAChat,export_state_dict_checkpoint.py,2023-03-16T07:50:24Z,Fix LoRa weight merging
github.com/bupticybee/FastLoRAChat,export_state_dict_checkpoint.py,2023-03-16T00:17:32Z,Add script for converting weights from HF
github.com/BraveGroup/Drive-WM,src/diffusers/models/modeling_utils.py,2023-11-22T10:23:44Z,initialize from diffusers
github.com/BraveGroup/Drive-WM,src/diffusers/pipelines/pipeline_utils.py,2023-11-22T10:23:44Z,initialize from diffusers
github.com/alipay/mobile-agent,code/infer_evaluate.py,2024-01-17T06:16:14Z,new
github.com/alipay/mobile-agent,code/llama-recipes-main/src/llama_recipes/inference/model_utils.py,2024-01-17T06:16:14Z,new
github.com/alipay/mobile-agent,code/llama-recipes-main/examples/hf_text_generation_inference/merge_lora_weights.py,2024-01-17T06:16:14Z,new
github.com/wxjiao/ParroT,train/inference_lora.py,2023-08-23T12:53:08Z,print info for loading lora weights in inference_lora.py
github.com/wxjiao/ParroT,train/inference_lora.py,2023-04-14T05:54:56Z,training scripts of full model and lora
github.com/dyabel/AnyTool,toolbench/model/model_adapter.py,2024-02-23T07:13:06Z,first commit
github.com/dyabel/AnyTool,toolbench/inference/LLM/tool_llama_lora_model.py,2024-02-23T07:13:06Z,first commit
github.com/yongzhuo/chatglm-maths,chatglm_maths/t10_lora_trl_train_ppo.py,2023-04-09T15:31:58Z,"fix ppo of trl(org, lora), padding of outerline,fix predict"
github.com/yongzhuo/chatglm-maths,chatglm_maths/p00_toy_lora_predict_6b.py,2023-06-03T05:08:03Z,fix chatglm-v0
github.com/yongzhuo/chatglm-maths,chatglm_maths/p00_toy_lora_predict_6b.py,2023-04-09T15:31:58Z,"fix ppo of trl(org, lora), padding of outerline,fix predict"
github.com/yongzhuo/chatglm-maths,chatglm_maths/p00_toy_lora_predict_6b.py,2023-03-27T12:02:07Z,"fix mask, padding, use_gMASK, func-savemodel"
github.com/yongzhuo/chatglm-maths,chatglm_maths/p00_toy_lora_predict_6b.py,2023-03-25T03:42:29Z,"add ppo-predict, fix init_weight of no pretrain, fix lora-predict"
github.com/yongzhuo/chatglm-maths,chatglm_maths/p00_toy_lora_predict_6b.py,2023-03-24T16:27:34Z,"add PPO of trl, fix README.md, fix encode(need input_dict), update model/tokenizer"
github.com/yongzhuo/chatglm-maths,chatglm_maths/p00_toy_lora_predict_6b.py,2023-03-23T18:48:03Z,"fix README.md, fix encode(need input_dict), update model/tokenizer"
github.com/pipilurj/G-LLaVA,gllava/model/builder.py,2023-12-22T07:20:51Z,update
github.com/plncmm/guanaco-lora,guanaco-test.py,2023-03-23T16:59:26Z,edit instruction
github.com/plncmm/guanaco-lora,guanaco-test.py,2023-03-23T15:25:13Z,remove comments
github.com/plncmm/guanaco-lora,guanaco-test.py,2023-03-23T14:48:35Z,fix accemts
github.com/plncmm/guanaco-lora,guanaco-test.py,2023-03-23T14:33:58Z,upload train and test scripts
github.com/kubeflow/kfp-tekton,samples/peft-modelmesh-pipeline/peft_model_server.py,2023-06-20T21:08:40Z,"feat(samples): add peft sample with modelmesh (#1258)

* add peft sample with kserve

* lint files"
github.com/kubeflow/kfp-tekton,samples/huggingface-prompt-tuning/prompt-tuning-demo.py,2023-06-16T20:20:37Z,"Upload huggingface demo for tutorial (#1256)

* Add files via upload

* Rename samples/prompt-tuning-demo.py to samples/huggingface-prompt-tuning/prompt-tuning-demo.py"
github.com/zhangnn520/znn_chatglm,src/utils/common.py,2023-04-28T04:49:32Z,reload project
github.com/zhangnn520/znn_chatglm,src/utils/common.py,2023-04-28T04:47:03Z,del project
github.com/zhangnn520/znn_chatglm,src/utils/common.py,2023-04-27T15:12:24Z,update readme and fix bug
github.com/zhangnn520/znn_chatglm,src/utils/common.py,2023-04-27T12:59:28Z,the first upload code
github.com/jiangjiechen/auction-arena,app_modules/utils.py,2023-10-10T01:57:31Z,init
github.com/jacklishufan/InstructAny2Pix,instructany2pix/llm/model/builder.py,2023-12-02T02:31:09Z,init
github.com/MikeGu721/XiezhiBenchmark,Knowledge_Evaluator/xiezhi_evaluate.py,2023-12-04T04:00:59Z,little fix.
github.com/MikeGu721/XiezhiBenchmark,Knowledge_Evaluator/xiezhi_evaluate.py,2023-11-12T07:26:47Z,update some little change.
github.com/xhan77/web-browsing-llama,test_webllama.py,2023-10-11T19:21:06Z,update default adapter
github.com/xhan77/web-browsing-llama,test_webllama.py,2023-10-11T10:54:48Z,init
github.com/MikeGu721/EasyLLM,deploy.py,2023-04-08T07:03:56Z,First Commit
github.com/MikeGu721/EasyLLM,merge_weights.py,2023-04-08T07:03:56Z,First Commit
github.com/MikeGu721/EasyLLM,installs/peft-install/src/peft/peft_model.py,2023-04-08T07:03:56Z,First Commit
github.com/MikeGu721/EasyLLM,installs/peft-install/src/peft/tuners/prompt_tuning.py,2023-04-08T07:03:56Z,First Commit
github.com/camel-ai/camel_chat,camel_chat/model/apply_lora.py,2023-06-19T20:58:21Z,initial commit of training and serving
github.com/jiangxinyang227/LLM-tuning,llama_tuning/lora_ddp/generate.py,2023-04-24T14:53:50Z,init commit
github.com/jiangxinyang227/LLM-tuning,chatglm_tuning/lora_ddp/generate.py,2023-04-24T14:53:50Z,init commit
github.com/jiangxinyang227/LLM-tuning,chatglm_tuning/lora_fsdp/generate.py,2023-04-25T13:34:29Z,add deepspeed and fsdp的训练代码
github.com/jiangxinyang227/LLM-tuning,llama_tuning/lora_deepspeed/generate.py,2023-04-25T13:34:29Z,add deepspeed and fsdp的训练代码
github.com/jiangxinyang227/LLM-tuning,llama_tuning/lora_single_gpu/generate.py,2023-04-24T14:53:50Z,init commit
github.com/jiangxinyang227/LLM-tuning,chatglm_tuning/lora_deepspeed/generate.py,2023-04-25T13:34:29Z,add deepspeed and fsdp的训练代码
github.com/jiangxinyang227/LLM-tuning,chatglm_tuning/lora_shared_ddp/generate.py,2023-04-24T14:53:50Z,init commit
github.com/jiangxinyang227/LLM-tuning,chatglm_tuning/lora_single_gpu/generate.py,2023-04-24T14:53:50Z,init commit
github.com/KyujinHan/Sakura-SOLAR-DPO,merge.py,2023-12-27T17:06:28Z,Add files via upload
github.com/wpydcr/LLM-Kit,modules/model/llm_auto.py,2023-10-07T07:37:50Z,update qwen 7B model and 14B model
github.com/wpydcr/LLM-Kit,modules/model/llm_auto.py,2023-09-14T08:42:48Z,add baichuan2 7B Chat and 13B Chat
github.com/wpydcr/LLM-Kit,modules/model/llm_auto.py,2023-08-31T09:49:35Z,copy handler file and typo fix
github.com/wpydcr/LLM-Kit,modules/model/llm_auto.py,2023-08-30T06:54:02Z,update llm_auto
github.com/wpydcr/LLM-Kit,modules/model/llm_auto.py,2023-08-28T10:34:11Z,add midjourney
github.com/wpydcr/LLM-Kit,modules/model/llm_auto.py,2023-08-28T10:31:53Z,update gitignore
github.com/wpydcr/LLM-Kit,modules/model/llm_auto.py,2023-08-09T10:25:19Z,add qwen-7b-chat
github.com/wpydcr/LLM-Kit,modules/model/llm_auto.py,2023-08-06T15:10:53Z,add chinses llama2 and chatglm2-6b-32k
github.com/wpydcr/LLM-Kit,modules/model/llm_auto.py,2023-07-22T10:01:05Z,new
github.com/runwayIA/alpaca-lora,generate.py,2023-03-17T10:54:38Z,Add files via upload
github.com/runwayIA/alpaca-lora,export_state_dict_checkpoint.py,2023-03-17T10:54:38Z,Add files via upload
github.com/deeppavlov/dream,services/transformers_peft_lm/server.py,2023-06-01T06:26:49Z,"Feat/use configs and function from common (#478)

* efat: move default configs to common

* fix: configs

* fix: use compose_sending_variables"
github.com/deeppavlov/dream,services/transformers_peft_lm/server.py,2023-05-29T10:37:42Z,"Fix/configs and llm resp selection (#476)

* fix: rename goals prompt

* fix: send hyps to llm based resp selector

* fix: send configs and default values

* fix: configs

* fix: extra file

* fix: extra imports"
github.com/deeppavlov/dream,services/transformers_peft_lm/server.py,2023-05-25T11:58:26Z,"Fix/prompt formatting (#471)

* fix: prompts formatting

* fix: transformers get goals"
github.com/deeppavlov/dream,services/transformers_peft_lm/server.py,2023-05-24T15:22:12Z,"Feat/description based skill selector (#463)

* feat: first version

* feat: template prompted skill generates goals from prompt. and saves to shared memory

* feat: template prompted skill saves to human attributes

* fix: imports

* feat; prompt selector

* feat; prompt goals

* feat: description based selector

* feat: use skill selector

* fix: remove imports

* fix: test json

* fix: get attributes

* fix: comma

* fix: logs

* fix: get prompts

* fix: codeswtyle

* fix: prompts_goals in dff

* fix: save prompts

* fix: use skill selector

* feat: save prompt goals

* feat: update prompt goals

* fix: config with prompt selector

* feat: collect goals ok

* feat: select prompted skill

* feat: skills logs

* fix: wait for prompt goals selector

* fix: pipeline conf

* fix: update prompt goals

* fix: store prompt goals

* fix: working

* codestyle

* fix: no extra logs and tests

* fix: cards for goals collector

* fix: use prompt collector

* fix: universal generates goals

* fix: pop keys

* feat: cards for goals collectors

* fix: codestyle

* fix: codestyle

* feat: meta prompt to file

* feat: meta prompt to file

* fix: exatr comma

* fix: generate goals if an endpoint of llm

* fix: codestyle

* fix: jsonify response

* fix: jsonify response

* fix: transformers also can

* fix: transformers also can

* fix: codestyle

* fix: goals for prompts

* fix: goals for prompts from templated

* fix:  print goals

* fix: when add skills

* fix: config

* fix: no prompt usage

* fix: meta prompt formatting

* fix: generate goals

* fix: input params

* fix: codestyle

* fix: correct requests

* fix: working

* feat: goals for demo prompts

* fix: after review"
github.com/deeppavlov/dream,services/transformers_peft_lm/server.py,2023-04-26T16:27:08Z,"fixed cutoff for AI utterance (#426)

* fixed cutoff for AI utterance

* fix for cases with extra ROBOT: etc

* style

* fix for newline"
github.com/deeppavlov/dream,services/transformers_peft_lm/server.py,2023-04-24T13:12:35Z,fix: no \n removing (#402)
github.com/deeppavlov/dream,services/transformers_peft_lm/server.py,2023-04-19T14:07:15Z,"Feat/ru llama distribution (#383)

* feat: ru prompted dist based on llama

* feat: ru persona

* fix: versions

* fix duplicate endpoints, missing keys (#371)

* Make MTL refer to DeepPavlov 1.1 instead of my branch (#366)

* Update Dockerfile

* Update combined_classifier.json

* Update Dockerfile

* Update Dockerfile

* fix: starting openai services without dev.yml (#373)

* fix: reqs

* fix: dockerfile

* fix: pip update

* fix: reqs

* feat: using peft for gusev's model

* fix: reqs

* fix: working load

* feat: separate folder for peft transformers

* fix: revert to dev

* fix: transformers generation

* fix: use peft model

* fix: component yml

* fix: yml configs

* fix: no half precision

* fix: description

* fix: config is optional for some models

* fix: increase timeout

* fix: formatter

* fix: language

* fixed

* fix: rights

* fix: info ymls

* fix: 5sec timeout

* fix: 10sec timeout

* fix: gpu mem

* feat: ru pipeline and dockers

* feat: badlisted words ru

* feat: use fp16 for faster inference

* feat: rank sentences endpoint

* fix: endpoint func

* fix: ping pong

* fix: rannker url

* fix: prompt selector ru

* fix: env ru

* fix: sentence ranker

* fix: no-scripts selector

* fix: timeout 2.0 for ru toxic

* fix: first try for toxic model before ready

* fix: params for language

* fix: language

* fix: timoeout for dialogrpt

* fix: no use of toxic cls

* fix: revert timeout to 1sec

* feat: ru lang

* fix: ru persona

* feat: new prompt

* fix: prompt

* fix: prompt

* fix: prompt

* Update dream_persona.json

* Update dream_persona.json

* Update dream_persona_ru.json

* fix: rename distr

* fix: formatter

---------

Co-authored-by: Maxim Talimanchuk <mtalimanchuk@gmail.com>
Co-authored-by: dimakarp1996 <dimakarp1996@yandex.ru>
Co-authored-by: Fedor Ignatov <ignatov.fedor@gmail.com>
Co-authored-by: Lidia Ostyakova <55363402+lostyakova@users.noreply.github.com>"
github.com/BeyonderXX/TRACE,inference/infer_single.py,2023-09-21T03:44:59Z,Update infer_single.py
github.com/BeyonderXX/TRACE,inference/infer_single.py,2023-09-19T23:10:59Z,Update infer_single.py
github.com/BeyonderXX/TRACE,inference/infer_single.py,2023-09-19T23:00:29Z,Update infer_single.py
github.com/BeyonderXX/TRACE,inference/infer_single.py,2023-09-19T10:50:28Z,Update infer_single.py
github.com/BeyonderXX/TRACE,inference/infer_single.py,2023-09-19T03:53:50Z,fix bugs
github.com/BeyonderXX/TRACE,inference/infer_single.py,2023-09-19T03:15:32Z,Update infer_single.py
github.com/BeyonderXX/TRACE,inference/infer_single.py,2023-09-13T12:48:55Z,fix bugs
github.com/BeyonderXX/TRACE,inference/infer_single.py,2023-09-13T04:22:03Z,Update infer_single.py
github.com/BeyonderXX/TRACE,inference/infer_single.py,2023-09-13T04:18:21Z,fix bugs
github.com/BeyonderXX/TRACE,inference/infer_single.py,2023-09-12T11:58:57Z,Fix train continual bug
github.com/BeyonderXX/TRACE,inference/infer_single.py,2023-09-08T03:16:50Z,fix bugs
github.com/BeyonderXX/TRACE,inference/infer_single.py,2023-09-07T07:19:14Z,Fix inference bug and add sigle gpu inference
github.com/BeyonderXX/TRACE,utils/my_peft/peft_model.py,2023-09-07T14:12:26Z,Add files via upload
github.com/BeyonderXX/TRACE,utils/my_peft/tuners/prompt_tuning.py,2023-09-07T14:12:26Z,Add files via upload
github.com/git-disl/PokeLLMon,poke_env/player/llama_player.py,2024-02-05T20:57:33Z,Update
github.com/git-disl/PokeLLMon,poke_env/player/llama_player.py,2024-01-30T16:40:29Z,Initialization
github.com/git-disl/PokeLLMon,poke_env/player/llama_player.py,2024-01-30T16:39:43Z,Initialization
github.com/git-disl/PokeLLMon,poke_env/player/llama_player.py,2024-01-30T16:39:38Z,Initialization
github.com/NetEase-FuXi/EETQ,examples/models/llama_transformers_example.py,2023-09-12T07:02:38Z,[feat] add cublas gemm kernels
github.com/NetEase-FuXi/EETQ,examples/models/llama_transformers_example.py,2023-09-05T08:41:33Z,[feat] add fused attention
github.com/NetEase-FuXi/EETQ,examples/models/llama_transformers_example.py,2023-08-31T12:36:01Z,[feat] 支持lora模型量化
github.com/NetEase-FuXi/EETQ,examples/models/llama_transformers_example.py,2023-08-30T07:10:57Z,[feat] 从已有的huggingface模型直接转换 & 支持从pt文件加载
github.com/NetEase-FuXi/EETQ,examples/models/llama_transformers_example.py,2023-08-29T08:03:02Z,[feat] add llama13B int8 example
github.com/jamqd/Group-Preference-Optimization,baselines/get_emb/get_embeds.py,2023-10-23T06:47:05Z,update
github.com/whyNLP/Conic10K,src/generate.py,2023-10-24T14:40:01Z,fix argparse bugs
github.com/whyNLP/Conic10K,src/generate.py,2023-10-23T08:39:27Z,reorganize + semantic parsing evaluation
github.com/whyNLP/Conic10K,src/train_clm.py,2023-10-24T14:40:01Z,fix argparse bugs
github.com/whyNLP/Conic10K,src/train_clm.py,2023-10-23T08:39:27Z,reorganize + semantic parsing evaluation
github.com/HosnLS/Hierarchical-Language-Agent,llm-api/modules/LoRA.py,2023-12-04T07:06:28Z,Init
github.com/jialinzhang/chinese-medical-llama2,src/pretrain/run_clm_pt_with_peft.py,2023-09-08T03:36:00Z,"Jialin (#36)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* Jialin (#26) (#27)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* 增加指令微调代码

* 新增数据集大小日志记录，增加eval阶段指标记录

* 修改README.md

* 指令微调脚步增加eval阶段指标记录

* 增加pretrain/merge_tokenizer README.md

* 添加lora_target_modules

* fine tuning 添加 lora_target_modules

* 添加预训练结果，参与训练网络层:q_proj、v_proj、embed_tokens、lm_head

* 设置初始化随机种子

* 删除wandb log"
github.com/jialinzhang/chinese-medical-llama2,src/pretrain/run_clm_pt_with_peft.py,2023-09-07T13:55:21Z,"Jialin (#35)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* Jialin (#26) (#27)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* 增加指令微调代码

* 新增数据集大小日志记录，增加eval阶段指标记录

* 修改README.md

* 指令微调脚步增加eval阶段指标记录

* 增加pretrain/merge_tokenizer README.md

* 添加lora_target_modules

* fine tuning 添加 lora_target_modules

* 添加预训练结果，参与训练网络层:q_proj、v_proj、embed_tokens、lm_head"
github.com/jialinzhang/chinese-medical-llama2,src/pretrain/run_clm_pt_with_peft.py,2023-09-07T12:41:31Z,"Jialin (#33)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* Jialin (#26) (#27)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* 增加指令微调代码

* 新增数据集大小日志记录，增加eval阶段指标记录

* 修改README.md

* 指令微调脚步增加eval阶段指标记录

* 增加pretrain/merge_tokenizer README.md

* 添加lora_target_modules"
github.com/jialinzhang/chinese-medical-llama2,src/pretrain/run_clm_pt_with_peft.py,2023-09-05T12:33:53Z,"Jialin (#29)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* Jialin (#26) (#27)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* 增加指令微调代码

* 新增数据集大小日志记录，增加eval阶段指标记录"
github.com/jialinzhang/chinese-medical-llama2,src/pretrain/run_clm_pt_with_peft.py,2023-09-04T13:23:48Z,"Jialin (#28)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* Jialin (#26) (#27)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* 增加指令微调代码"
github.com/jialinzhang/chinese-medical-llama2,src/pretrain/run_clm_pt_with_peft.py,2023-09-04T00:30:41Z,"Jialin (#26)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数"
github.com/jialinzhang/chinese-medical-llama2,src/pretrain/run_clm_pt_with_peft.py,2023-09-03T12:58:00Z,"Jialin (#24)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录"
github.com/jialinzhang/chinese-medical-llama2,src/pretrain/run_clm_pt_with_peft.py,2023-09-03T07:45:01Z,"Jialin (#18)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程"
github.com/jialinzhang/chinese-medical-llama2,src/pretrain/run_clm_pt_with_peft.py,2023-09-02T03:58:40Z,"Jialin (#13)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算"
github.com/jialinzhang/chinese-medical-llama2,src/pretrain/run_clm_pt_with_peft.py,2023-09-01T12:21:08Z,"Jialin (#12)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch"
github.com/jialinzhang/chinese-medical-llama2,src/pretrain/run_clm_pt_with_peft.py,2023-08-31T12:18:43Z,"Jialin (#11)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py"
github.com/jialinzhang/chinese-medical-llama2,src/pretrain/run_clm_pt_with_peft.py,2023-08-31T02:10:04Z,"Jialin (#10)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化"
github.com/jialinzhang/chinese-medical-llama2,src/pretrain/run_clm_pt_with_peft.py,2023-08-30T11:41:28Z,"Jialin (#9)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突"
github.com/jialinzhang/chinese-medical-llama2,src/pretrain/run_clm_pt_with_peft.py,2023-08-30T03:31:13Z,"Jialin (#8)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py"
github.com/jialinzhang/chinese-medical-llama2,src/pretrain/run_clm_pt_with_peft_ds.py,2023-09-08T03:36:00Z,"Jialin (#36)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* Jialin (#26) (#27)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* 增加指令微调代码

* 新增数据集大小日志记录，增加eval阶段指标记录

* 修改README.md

* 指令微调脚步增加eval阶段指标记录

* 增加pretrain/merge_tokenizer README.md

* 添加lora_target_modules

* fine tuning 添加 lora_target_modules

* 添加预训练结果，参与训练网络层:q_proj、v_proj、embed_tokens、lm_head

* 设置初始化随机种子

* 删除wandb log"
github.com/jialinzhang/chinese-medical-llama2,src/pretrain/run_clm_pt_with_peft_ds.py,2023-09-07T13:55:21Z,"Jialin (#35)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* Jialin (#26) (#27)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* 增加指令微调代码

* 新增数据集大小日志记录，增加eval阶段指标记录

* 修改README.md

* 指令微调脚步增加eval阶段指标记录

* 增加pretrain/merge_tokenizer README.md

* 添加lora_target_modules

* fine tuning 添加 lora_target_modules

* 添加预训练结果，参与训练网络层:q_proj、v_proj、embed_tokens、lm_head"
github.com/jialinzhang/chinese-medical-llama2,src/pretrain/run_clm_pt_with_peft_ds.py,2023-09-07T12:41:31Z,"Jialin (#33)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* Jialin (#26) (#27)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* 增加指令微调代码

* 新增数据集大小日志记录，增加eval阶段指标记录

* 修改README.md

* 指令微调脚步增加eval阶段指标记录

* 增加pretrain/merge_tokenizer README.md

* 添加lora_target_modules"
github.com/jialinzhang/chinese-medical-llama2,src/pretrain/run_clm_pt_with_peft_ds.py,2023-09-05T12:33:53Z,"Jialin (#29)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* Jialin (#26) (#27)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* 增加指令微调代码

* 新增数据集大小日志记录，增加eval阶段指标记录"
github.com/jialinzhang/chinese-medical-llama2,src/pretrain/run_clm_pt_with_peft_ds.py,2023-09-04T13:23:48Z,"Jialin (#28)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* Jialin (#26) (#27)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* 增加指令微调代码"
github.com/jialinzhang/chinese-medical-llama2,src/pretrain/run_clm_pt_with_peft_ds.py,2023-09-04T00:30:41Z,"Jialin (#26)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数"
github.com/jialinzhang/chinese-medical-llama2,src/pretrain/run_clm_pt_with_peft_ds.py,2023-09-03T12:58:00Z,"Jialin (#24)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录"
github.com/jialinzhang/chinese-medical-llama2,src/pretrain/run_clm_pt_with_peft_ds.py,2023-09-03T07:45:01Z,"Jialin (#18)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程"
github.com/jialinzhang/chinese-medical-llama2,src/pretrain/run_clm_pt_with_peft_ds.py,2023-09-03T01:34:51Z,"Jialin (#17)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理"
github.com/jialinzhang/chinese-medical-llama2,src/pretrain/run_clm_pt_with_peft_ds.py,2023-09-03T01:10:44Z,"Jialin (#14)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住"
github.com/jialinzhang/chinese-medical-llama2,src/pretrain/run_clm_pt_with_peft_ds.py,2023-09-02T03:58:40Z,"Jialin (#13)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算"
github.com/jialinzhang/chinese-medical-llama2,src/pretrain/run_clm_pt_with_peft_ds.py,2023-08-31T12:18:43Z,"Jialin (#11)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py"
github.com/jialinzhang/chinese-medical-llama2,src/fine_tuning/run_clm_sft_with_peft_ds.py,2023-09-08T03:36:00Z,"Jialin (#36)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* Jialin (#26) (#27)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* 增加指令微调代码

* 新增数据集大小日志记录，增加eval阶段指标记录

* 修改README.md

* 指令微调脚步增加eval阶段指标记录

* 增加pretrain/merge_tokenizer README.md

* 添加lora_target_modules

* fine tuning 添加 lora_target_modules

* 添加预训练结果，参与训练网络层:q_proj、v_proj、embed_tokens、lm_head

* 设置初始化随机种子

* 删除wandb log"
github.com/jialinzhang/chinese-medical-llama2,src/fine_tuning/run_clm_sft_with_peft_ds.py,2023-09-07T13:55:21Z,"Jialin (#35)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* Jialin (#26) (#27)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* 增加指令微调代码

* 新增数据集大小日志记录，增加eval阶段指标记录

* 修改README.md

* 指令微调脚步增加eval阶段指标记录

* 增加pretrain/merge_tokenizer README.md

* 添加lora_target_modules

* fine tuning 添加 lora_target_modules

* 添加预训练结果，参与训练网络层:q_proj、v_proj、embed_tokens、lm_head"
github.com/jialinzhang/chinese-medical-llama2,src/fine_tuning/run_clm_sft_with_peft_ds.py,2023-09-07T12:45:15Z,"Jialin (#34)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* Jialin (#26) (#27)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* 增加指令微调代码

* 新增数据集大小日志记录，增加eval阶段指标记录

* 修改README.md

* 指令微调脚步增加eval阶段指标记录

* 增加pretrain/merge_tokenizer README.md

* 添加lora_target_modules

* fine tuning 添加 lora_target_modules"
github.com/jialinzhang/chinese-medical-llama2,src/fine_tuning/run_clm_sft_with_peft_ds.py,2023-09-05T14:16:08Z,"Jialin (#31)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* Jialin (#26) (#27)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* 增加指令微调代码

* 新增数据集大小日志记录，增加eval阶段指标记录

* 修改README.md

* 指令微调脚步增加eval阶段指标记录"
github.com/jialinzhang/chinese-medical-llama2,src/fine_tuning/run_clm_sft_with_peft_ds.py,2023-09-04T13:23:48Z,"Jialin (#28)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* Jialin (#26) (#27)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* Jialin (#24) (#25)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#19)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#20)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* Jialin (#18) (#21)

* 调整.gitignorement删除.vscode

* update .gitignore

* torch.distributed.launch -> torchrun

* log增加rank信息

* 调整ddp位置

* collate_fn

* loss require_grad

* loss = loss / accumulation_steps

* rewrite run_clm_pt.py

* 修复ddp_setup位置

* 修复load checkpoint内存不足

* add run_clm_pt_with_peft.py

* 修复amp和float16冲突

* 代码格式化

* add run_clm_pt_with_peft_ds.py

* 修复mini batch

* 修复DeepSpeedEngine.backward损失计算

* 解决deepspeed model_engine.eval卡住

* 分布式评估

* 修复上下文管理器，确保只有主进程进行数据预处理

* 添加torch.distributed.barrier,调整train过程,调整evaluate过程

* 修正wandb日志记录

* 删除按epoch保存模型方式，修正训练参数

* 增加指令微调代码"
github.com/pandinghao/TouYi-LLM,utils.py,2023-11-01T05:31:19Z,final_test
github.com/pandinghao/TouYi-LLM,utils.py,2023-10-31T13:41:59Z,final_test
github.com/pandinghao/TouYi-LLM,utils.py,2023-10-27T17:39:07Z,revise dataset
github.com/pandinghao/TouYi-LLM,utils.py,2023-10-27T14:55:30Z,go
github.com/pandinghao/TouYi-LLM,utils.py,2023-10-24T14:03:31Z,finetune linear and attn
github.com/pandinghao/TouYi-LLM,utils.py,2023-10-22T05:38:12Z,chat_ex
github.com/wuyike2000/Retrieve-Rewrite-Answer,KGQA-MetaQA/rewrite/infer_t5-xl.py,2023-09-17T14:58:30Z,Add files via upload
github.com/eth-sri/language-model-arithmetic,src/model_arithmetic/basic_model_loader.py,2023-11-22T14:59:27Z,initial commit
github.com/vsmolyakov/deep,pytorch/peft_finetuning_llm.py,2023-11-24T01:40:38Z,"peft finetuning

peft finetuning"
github.com/StarRing2022/ChatGPTX-Uni,generate.py,2023-07-17T08:37:28Z,Update generate.py
github.com/StarRing2022/ChatGPTX-Uni,generate.py,2023-04-09T02:39:03Z,Update generate.py
github.com/StarRing2022/ChatGPTX-Uni,generate.py,2023-04-08T08:46:09Z,Add files via upload
github.com/StarRing2022/ChatGPTX-Uni,RingPeft/peft_model.py,2023-07-17T08:22:30Z,Add files via upload
github.com/StarRing2022/ChatGPTX-Uni,RingPeft/tuners/lora.py,2023-07-17T08:23:31Z,Add files via upload
github.com/StarRing2022/ChatGPTX-Uni,RingPeft/tuners/prompt_tuning.py,2023-07-17T08:23:31Z,Add files via upload
github.com/anchen1011/FireAct,models/llama.py,2023-10-08T10:18:49Z,init
github.com/ranchlai/lectures,code/lora/peft-turorial/eval.py,2023-06-12T08:05:27Z,add peft-lora tutorial
github.com/zjunlp/Mol-Instructions,demo/generate.py,2023-06-13T06:12:59Z,Add files via upload
github.com/zjunlp/Mol-Instructions,demo/generate.py,2023-06-12T13:51:54Z,Delete demo directory
github.com/zjunlp/Mol-Instructions,demo/generate.py,2023-06-12T13:51:19Z,Rename generate.py to generate.py
github.com/zjunlp/Mol-Instructions,evaluation/biotext/generate_example.py,2023-10-18T10:27:00Z,Rename generate_many.py to generate_example.py
github.com/zjunlp/Mol-Instructions,evaluation/molecule/generate_example.py,2023-10-17T09:13:17Z,Add files via upload
github.com/Unispac/Visual-Adversarial-Examples-Jailbreak-Large-Language-Models,llava_llama_2/model/builder.py,2023-08-16T07:57:56Z,add support for other models
github.com/ModelTC/EasyLLM,tools/open_hf_infer.py,2024-01-28T13:00:59Z,"Add pyproject.toml and make it a python package

Signed-off-by: lizz <lizz@sensetime.com>"
github.com/ModelTC/EasyLLM,tools/open_hf_infer.py,2023-11-26T10:08:31Z,release: v1.0.0
github.com/ModelTC/EasyLLM,tools/merge_base_model_with_lora.py,2023-11-26T10:08:31Z,release: v1.0.0
github.com/OpenThaiGPT/openthaigpt-finetune,generate.py,2023-11-11T08:33:55Z,Add support for 1.0.0 beta
github.com/OpenThaiGPT/openthaigpt-finetune,generate.py,2023-05-18T12:41:17Z,Init
github.com/OpenThaiGPT/openthaigpt-finetune,export_hf_checkpoint.py,2023-05-18T12:41:17Z,Init
github.com/OpenThaiGPT/openthaigpt-finetune,export_state_dict_checkpoint.py,2023-05-18T12:41:17Z,Init
github.com/heng840/alpaca-lora-chinese,generate.py,2023-03-28T03:23:31Z,Initial commit
github.com/heng840/alpaca-lora-chinese,export_hf_checkpoint.py,2023-03-28T03:23:31Z,Initial commit
github.com/heng840/alpaca-lora-chinese,export_state_dict_checkpoint.py,2023-03-28T03:23:31Z,Initial commit
github.com/alexrs/herd,herd/run_model.py,2023-09-20T10:00:47Z,Restructure to bring scripts out of package
github.com/alexrs/herd,herd/run_model.py,2023-09-20T08:41:18Z,Apply Black formatting and Ruff linting
github.com/alexrs/herd,herd/run_model.py,2023-09-11T16:24:42Z,API working
github.com/alexrs/herd,herd/run_model.py,2023-09-08T14:05:48Z,Fix issue when adapters were not loaded
github.com/alexrs/herd,herd/run_model.py,2023-09-08T10:19:42Z,Experts created and segmented
github.com/alexrs/herd,herd/run_model.py,2023-09-06T10:13:01Z,Expert fine-tuning running
github.com/dmis-lab/self-biorag,retrieval_lm/inference.py,2024-01-25T09:14:05Z,init
github.com/dmis-lab/self-biorag,retrieval_lm/inference.py,2024-01-22T07:40:27Z,init
github.com/git-cloner/llama2-lora-fine-tuning,generate.py,2023-07-28T15:23:24Z,fixed generate.py
github.com/git-cloner/llama2-lora-fine-tuning,generate.py,2023-07-23T11:47:52Z,增加测试代码
github.com/yxli2123/LoftQ,train_clm.py,2023-12-27T14:58:09Z,update train clm
github.com/yxli2123/LoftQ,train_clm.py,2023-12-19T16:30:02Z,upload files
github.com/yxli2123/LoftQ,train_clm.py,2023-12-19T16:07:42Z,rewrite gsm8k code
github.com/yxli2123/LoftQ,train_clm.py,2023-10-30T03:05:24Z,upload summarization code
github.com/yxli2123/LoftQ,train_clm.py,2023-10-23T15:56:02Z,quantize more true quant
github.com/yxli2123/LoftQ,train_clm.py,2023-10-23T15:16:37Z,try new profiling
github.com/yxli2123/LoftQ,train_clm.py,2023-10-16T02:48:33Z,validate gsm8k
github.com/yxli2123/LoftQ,train_clm.py,2023-10-15T23:31:24Z,remove device map
github.com/yxli2123/LoftQ,train_clm.py,2023-10-15T22:38:08Z,update
github.com/yxli2123/LoftQ,train_clm.py,2023-10-15T22:06:49Z,update
github.com/yxli2123/LoftQ,train_clm.py,2023-10-15T21:11:40Z,debugh
github.com/yxli2123/LoftQ,train_clm.py,2023-10-15T19:18:59Z,debug
github.com/yxli2123/LoftQ,train_clm.py,2023-10-15T16:03:01Z,push repo
github.com/yxli2123/LoftQ,train_gsm8k.py,2024-02-05T01:49:32Z,"Update train_gsm8k.py

change the explanation about the full_precision"
github.com/yxli2123/LoftQ,train_gsm8k.py,2023-12-19T16:30:02Z,upload files
github.com/yxli2123/LoftQ,train_gsm8k.py,2023-12-19T16:07:42Z,rewrite gsm8k code
github.com/yxli2123/LoftQ,train_gsm8k.py,2023-10-16T02:48:33Z,validate gsm8k
github.com/yxli2123/LoftQ,train_summarization.py,2023-12-19T16:30:02Z,upload files
github.com/yxli2123/LoftQ,train_summarization.py,2023-12-19T16:07:42Z,rewrite gsm8k code
github.com/yxli2123/LoftQ,train_summarization.py,2023-10-30T03:05:24Z,upload summarization code
github.com/yxli2123/LoftQ,test_gsm8k.py,2023-12-19T16:30:02Z,upload files
github.com/noir55/japanese_llm_simple_webui,llm-webui.py,2024-02-22T13:28:26Z,プロンプトタイプgemmaを追加
github.com/noir55/japanese_llm_simple_webui,llm-webui.py,2023-12-27T14:00:29Z,プロンプトタイプelyzallama2を追加
github.com/noir55/japanese_llm_simple_webui,llm-webui.py,2023-12-23T05:39:30Z,モデルタイプ、プロンプトタイプnekomataを追加
github.com/noir55/japanese_llm_simple_webui,llm-webui.py,2023-12-20T12:50:35Z,プロンプトタイプswallowを追加
github.com/noir55/japanese_llm_simple_webui,llm-webui.py,2023-12-20T12:48:43Z,プロンプトタイプswallowを追加
github.com/noir55/japanese_llm_simple_webui,llm-webui.py,2023-12-17T02:12:06Z,プロンプトタイプmixtralを追加
github.com/noir55/japanese_llm_simple_webui,llm-webui.py,2023-11-23T03:05:12Z,プロンプトタイプchatml追加
github.com/noir55/japanese_llm_simple_webui,llm-webui.py,2023-11-19T14:10:08Z,モデルタイプgeneralの追加
github.com/noir55/japanese_llm_simple_webui,llm-webui.py,2023-08-20T14:08:00Z,モデル定義の追加
github.com/noir55/japanese_llm_simple_webui,llm-webui.py,2023-08-14T14:07:29Z,"Merge pull request #1 from hangingman/main

コードの微修正"
github.com/noir55/japanese_llm_simple_webui,llm-webui.py,2023-08-10T13:31:41Z,Japanese StableLMモデル対応
github.com/noir55/japanese_llm_simple_webui,llm-webui.py,2023-08-07T13:14:26Z,"ifをelifに修正しInvalid MODEL_TYPE ""rinna""のエラーを解消"
github.com/noir55/japanese_llm_simple_webui,llm-webui.py,2023-08-02T15:22:03Z,Rinna 4Bモデル等に対応
github.com/noir55/japanese_llm_simple_webui,llm-webui.py,2023-07-19T16:53:24Z,Llama2対応
github.com/noir55/japanese_llm_simple_webui,llm-webui.py,2023-06-30T15:48:47Z,プロンプト修正
github.com/noir55/japanese_llm_simple_webui,llm-webui.py,2023-06-30T15:26:15Z,4bit量子化対応、対応モデル追加
github.com/noir55/japanese_llm_simple_webui,llm-webui.py,2023-06-20T15:14:59Z,プロンプトタイプqaを追加
github.com/noir55/japanese_llm_simple_webui,llm-webui.py,2023-06-14T14:02:22Z,生成パラメータをWebUI上で設定できる機能を追加
github.com/noir55/japanese_llm_simple_webui,llm-webui.py,2023-06-10T06:59:05Z,対応モデル、プロンプトを追加
github.com/noir55/japanese_llm_simple_webui,llm-webui.py,2023-06-08T11:39:59Z,Rinnaモデルの改行の扱いを修正
github.com/noir55/japanese_llm_simple_webui,llm-webui.py,2023-06-06T14:34:58Z,historyを最初に初期化するよう修正
github.com/noir55/japanese_llm_simple_webui,llm-webui.py,2023-06-05T13:56:05Z,モデルごとの推論方法を統一
github.com/noir55/japanese_llm_simple_webui,llm-webui.py,2023-06-04T16:50:34Z,"gradioのlaunch時に""share=False""オプションを追加"
github.com/noir55/japanese_llm_simple_webui,llm-webui.py,2023-06-04T15:00:42Z,最初のファイル登録
github.com/gersteinlab/Struc-Bench,generate.py,2023-07-08T12:16:17Z,update
github.com/MindSetLib/MS-Education,DeepLearning/NLP/Tasks/LLMfinetuning/scripts/ft_rouge.py,2023-09-19T16:53:49Z,"Llmft (#1)

* First commit

* Fix readme"
github.com/MindSetLib/MS-Education,DeepLearning/NLP/Tasks/LLMfinetuning/scripts/rlhf_detox.py,2023-12-15T12:02:24Z,"Add speech notebooks (#2)

* First commit

* Fix readme

* Add DLA experiments

* Edit README

* Edit README 2"
github.com/MindSetLib/MS-Education,DeepLearning/NLP/Tasks/LLMfinetuning/scripts/rlhf_detox.py,2023-09-19T16:53:49Z,"Llmft (#1)

* First commit

* Fix readme"
github.com/ziweiji/Self_Reflection_Medical,alpaca-lora/loop.py,2023-10-09T08:20:03Z,init
github.com/ziweiji/Self_Reflection_Medical,alpaca-lora/generate.py,2023-10-09T08:20:03Z,init
github.com/HongzheBi/DocQA,model.py,2023-05-23T16:28:32Z,webui
github.com/HongzheBi/DocQA,model.py,2023-05-23T09:19:36Z,webui
github.com/HongzheBi/DocQA,model.py,2023-05-21T17:06:52Z,doc-qa
github.com/poisonwine/Tianchi-LLM-retrieval,LLM.py,2023-11-13T03:12:07Z,Update LLM.py
github.com/poisonwine/Tianchi-LLM-retrieval,LLM.py,2023-11-13T02:26:34Z,Update LLM.py
github.com/poisonwine/Tianchi-LLM-retrieval,LLM.py,2023-11-13T02:21:07Z,"Create LLM.py

llm"
github.com/poisonwine/Tianchi-LLM-retrieval,embeddings.py,2023-11-13T02:28:28Z,"Create embeddings.py

embedding model"
github.com/Akirato/LLM-KG-Reasoning,src/llm_engine.py,2023-04-25T15:25:02Z,New scripts
github.com/Akirato/LLM-KG-Reasoning,src/llm_engine.py,2023-04-08T15:14:04Z,Added scripts and scores
github.com/Akirato/LLM-KG-Reasoning,src/llm_engine.py,2023-04-02T16:43:23Z,Corrected the Alpaca class and call from generate answers
github.com/Akirato/LLM-KG-Reasoning,src/llm_engine.py,2023-04-02T10:58:43Z,Added Alpaca
github.com/Akirato/LLM-KG-Reasoning,src/llm_engine.py,2023-03-31T00:19:37Z,Added LLM classes
github.com/EmoCareAI/ChatPsychiatrist,fastchat/model/apply_lora.py,2023-09-27T08:30:00Z,first commit
github.com/EmoCareAI/ChatPsychiatrist,fastchat/model/model_adapter.py,2023-09-27T08:30:00Z,first commit
github.com/worm128/AI-YinMei-backup,chatglm3.py,2023-12-31T09:38:27Z,相关程序说明备注
github.com/worm128/AI-YinMei-backup,chatglm3.py,2023-12-20T14:14:22Z,初始化ai
github.com/worm128/AI-YinMei-backup,lora-chatglm2.py,2023-12-31T09:38:27Z,相关程序说明备注
github.com/worm128/AI-YinMei-backup,lora-chatglm2.py,2023-12-20T14:14:22Z,初始化ai
github.com/worm128/AI-YinMei-backup,main-local-api.py,2024-01-15T14:12:37Z,对接唱歌
github.com/worm128/AI-YinMei-backup,main-local-api.py,2023-12-31T09:38:27Z,相关程序说明备注
github.com/worm128/AI-YinMei-backup,main-local-api.py,2023-12-20T14:14:22Z,初始化ai
github.com/worm128/AI-YinMei-backup,main-local-input.py,2023-12-31T09:38:27Z,相关程序说明备注
github.com/worm128/AI-YinMei-backup,main-local-input.py,2023-12-25T14:19:46Z,ai联网搜索引擎对接
github.com/worm128/AI-YinMei-backup,main-local-input.py,2023-12-23T05:17:09Z,提交联网功能
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2024-02-05T08:47:14Z,修复正在播放歌单不会清空问题
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2024-02-04T16:38:15Z,修改歌单bug，不断循环播放歌曲
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2024-02-04T14:12:39Z,更改参数
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2024-02-04T13:04:01Z,提交歌曲清单列表展示
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2024-02-04T12:16:02Z,歌曲点播列表上传
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2024-02-04T09:02:18Z,修改弹幕错乱问题
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2024-02-01T13:31:20Z,说明文档更新
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2024-02-01T13:05:43Z,错误日志输出
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2024-02-01T11:25:08Z,随机数修订
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2024-01-29T11:30:35Z,调整文件夹
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2024-01-28T16:36:40Z,控制台日志打印到文件
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2024-01-28T16:33:48Z,控制台日志打印到文件
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2024-01-28T14:39:06Z,升级弹幕样式
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2024-01-27T14:29:14Z,调整搜图能力为百度
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2024-01-27T09:12:38Z,ai回复弹幕显示
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2024-01-24T13:30:01Z,加入回复弹幕
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2024-01-24T12:10:43Z,加入回复弹幕
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2024-01-22T15:49:11Z,加入bert-vits2语音合成
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2024-01-22T03:49:15Z,服务说明文档修改
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2024-01-22T03:09:53Z,禁词调整
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2024-01-21T11:06:07Z,用户提示词加入加强权重
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2024-01-21T08:33:24Z,绘画提示词和唱歌功能上传
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2024-01-16T02:33:26Z,唱歌调整
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2024-01-16T02:32:35Z,唱歌调整
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2024-01-15T14:12:37Z,对接唱歌
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2024-01-03T12:53:45Z,搜图功能更新
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2023-12-31T11:16:46Z,绘画线程加入
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2023-12-31T09:38:27Z,相关程序说明备注
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2023-12-30T10:41:12Z,加入在线绘画能力
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2023-12-25T14:19:46Z,ai联网搜索引擎对接
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2023-12-23T05:19:13Z,提交联网功能
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2023-12-23T05:17:09Z,提交联网功能
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2023-12-23T04:59:36Z,提交ai联网搜索能力
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2023-12-22T11:43:32Z,增加联网查询
github.com/worm128/AI-YinMei-backup,bilibili-live-api.py,2023-12-20T14:14:22Z,初始化ai
github.com/worm128/AI-YinMei-backup,bilibili-live-web.py,2024-01-21T08:37:30Z,鉴黄程序上传
github.com/worm128/AI-YinMei-backup,bilibili-live-local.py,2023-12-31T11:16:46Z,绘画线程加入
github.com/worm128/AI-YinMei-backup,bilibili-live-local.py,2023-12-30T10:41:12Z,加入在线绘画能力
github.com/worm128/AI-YinMei-backup,bilibili-live-local.py,2023-12-25T14:19:46Z,ai联网搜索引擎对接
github.com/worm128/AI-YinMei-backup,bilibili-live-local.py,2023-12-23T05:19:13Z,提交联网功能
github.com/worm128/AI-YinMei-backup,bilibili-live-local.py,2023-12-23T05:17:09Z,提交联网功能
github.com/worm128/AI-YinMei-backup,bilibili-live-local.py,2023-12-23T04:59:36Z,提交ai联网搜索能力
github.com/worm128/AI-YinMei-backup,bilibili-live-local.py,2023-12-22T11:43:32Z,增加联网查询
github.com/worm128/AI-YinMei-backup,bilibili-live-local.py,2023-12-20T14:14:22Z,初始化ai
github.com/worm128/AI-YinMei-backup,text-generation-webui/modules/LoRA.py,2023-12-20T14:14:22Z,初始化ai
github.com/worm128/AI-YinMei-backup,LLaMA-Factory/src/llmtuner/model/adapter.py,2023-12-20T14:14:22Z,初始化ai
github.com/chaoyi-wu/Finetune_LLAMA,Python_Package/peft-llama/src/peft/peft_model.py,2023-03-20T08:45:49Z,changed
github.com/chaoyi-wu/Finetune_LLAMA,Python_Package/peft-llama/src/peft/tuners/lora.py,2023-03-20T08:45:49Z,changed
github.com/chaoyi-wu/Finetune_LLAMA,Python_Package/peft-llama/src/peft/tuners/prompt_tuning.py,2023-03-20T08:45:49Z,changed
github.com/jina-ai/jerboa,jerboa/utils/load_model.py,2023-07-13T11:59:31Z,"fix: allow for hf lora weights (#113)

Co-authored-by: Sebastian Weisshaar <sebastian.weisshaar@jina.ai>"
github.com/jina-ai/jerboa,jerboa/utils/load_model.py,2023-07-13T11:50:24Z,feat: introduce revision parameter
github.com/jina-ai/jerboa,jerboa/utils/load_model.py,2023-07-11T09:46:46Z,"feat: gradio app (#111)

* feat: add artifact, app runs

* feat: change to falcon-40b

* feat: change device map to balanced

* feat: change device map to auto

* feat: load in 8bit

* style: black and ruff applied

* feat: add slider for new tokens

* feat: increase max new tokens to 1024

* style: apply black and ruff

* style: rename directories for artifacts

* style: move configurations to frotn

* build: update poetry to include gradio

* build: include gradio in dependency, optional

* feat: increase temperature to 0.2

* feat: changed temperature

* fix: remove merge conflict

* refactor: remove artifacts directory

* feat: change app to Typer app

* style: black and ruff

* fix: change base model to falcon 7b

* fix: only use lora repo after wandb prefix

* feat: add public link in launch

* docs: explain gradio app in docs

---------

Co-authored-by: Sebastian Weisshaar <sebastian.weisshaar@jina.ai>"
github.com/jina-ai/jerboa,jerboa/utils/load_model.py,2023-07-11T07:15:31Z,"refactor: make general load model function in utils (#105)

* feat: evaluate first two examples

* feat: trust remote code = True

* feat: add quantization

* feat: correct library imports

* refactor: print(x) for evaluation

* feat: take first element of x for generation

* refactor: device allocation at creation of model

* fix: move model to device not tokenizer

* refactor: remove encode from tokenizer

* feat: put model in eval mode

* refactor: comment out kwargs

* refactor: print(x)

* refactor: print model type

* refactor: change arg to kwarg

* refactor: uncommented generate args

* feat: add generation config

* refactor: add generationconfig import

* feat: get configuation from original model

* fix: typo

* feat: set padding token to eos token

* feat: add penalties for repition and length

* refactor: add generation config back

* refactor: increase length penalty

* refactor: change evaluation file

* feat: increased penalties for length and repition

* feat: extreme penalties

* fix: repitition penalty should be float

* feat: increase max length and length penalty

* refactor: evaluate different instructions

* feat: increase max length

* feat: only get target indices

* feat: lower lentgh and repition penalty

* feat: change back to extreme length and repition penalties

* feat: change back to extreme length and repition penalties

* feat: changed penalties

* style: applied black and ruff

* feat: make code understandable

* feat: create new evaluation file for structure

* feat: organized new generation file, hyperparamters for sensible generation

* feat: create evaluation file

* feat: output file for config adjusted results

* feat: create csv file for comparison

* feat: add typer app

* style: applied black

* feat: unify model loading

* fix: specify lora weights correctly

* test: add test for model loading

* feat: quant config and model config to load model

* feat: remove redundant code in finetune.py

* feat: integrated load_model function into finetuning

* fix: remove typo in load model

* fix: remove typo in load model

* feat: include load model function in save_full_models

* fix: specify correct source of lora weights

* feat: incorporate load model into run_code_eval

* refactor: remove old run_code_eval

* style: apply black and ruff

* refactor: remove eval_corrected.jsonl

* fix: remove main point entry

* feat: include source of lora weights in string

* refactor: remove csv creation file

* refactor: remove csv file

* build: include slow marker in pytoml

* ci: exclude slow parts from ci test

* ci: update flag to exclude slow tests

* ci: change flag for slow test

* ci: remove model loading test

* ci: remove no slow flag from pytest ci

---------

Co-authored-by: Sebastian Weisshaar <sebastian.weisshaar@jina.ai>"
github.com/GFNOrg/gfn-lm-tuning,infill_subj_arithmetic/eval_infill.py,2023-10-08T22:52:49Z,complete merge
github.com/Scikud/AnythingButWrappers,Efficient_RedPajama_Finetuning/inference_example.py,2023-05-06T23:10:41Z,inference: Fix path
github.com/Scikud/AnythingButWrappers,Efficient_RedPajama_Finetuning/inference_example.py,2023-05-06T14:41:26Z,"smaller training set, inference example"
github.com/yuta0306/LLM-JDD,demo.py,2023-09-25T08:06:14Z,first commit
github.com/AGI-Edgerunners/RecAlpaca,generate.py,2023-03-29T15:31:30Z,first version
github.com/AGI-Edgerunners/RecAlpaca,export_hf_checkpoint.py,2023-03-29T15:31:30Z,first version
github.com/TIGER-AI-Lab/ImagenHub,src/imagen_hub/infermodels/wuerstchen.py,2024-02-24T22:33:01Z,"Update wuerstchen.py

Modifying Import"
github.com/TIGER-AI-Lab/ImagenHub,src/imagen_hub/infermodels/wuerstchen.py,2024-02-15T19:06:13Z,Modify Wuerstchen by agreeing its argument in infer_one_image function.
github.com/TIGER-AI-Lab/ImagenHub,src/imagen_hub/infermodels/wuerstchen.py,2024-02-14T17:15:37Z,Modify Wuerstchen and __init__
github.com/TIGER-AI-Lab/ImagenHub,src/imagen_hub/infermodels/wuerstchen.py,2024-02-13T03:12:00Z,Modify Wuerstchen by pipeline in diffusers
github.com/TIGER-AI-Lab/ImagenHub,src/imagen_hub/infermodels/wuerstchen.py,2024-02-08T23:38:58Z,Add files via upload
github.com/ukairia777/LLM-Finetuning-tutorial,merge.py,2023-12-30T16:26:46Z,'add_code'
github.com/ukairia777/LLM-Finetuning-tutorial,inference.py,2023-12-30T16:26:46Z,'add_code'
github.com/WangRongsheng/Aurora,app.py,2024-01-23T03:17:01Z,Add files via upload
github.com/WangRongsheng/Aurora,src/llmtuner/model/adapter.py,2023-12-21T09:19:27Z,add src
github.com/Tongyi-EconML/FinQwen,solutions/3_hxjj/app/model/llm/qwen.py,2024-01-04T07:59:03Z,命名统一
github.com/taprosoft/llm_finetuning,merge_lora_checkpoint.py,2023-07-08T02:46:49Z,inital commit
github.com/taprosoft/llm_finetuning,utils/loader/gptq_loader.py,2023-07-08T02:46:49Z,inital commit
github.com/UMass-Foundation-Model/CoVLM,LLaVA/llava/model/builder.py,2023-11-01T14:42:53Z,init commit
github.com/UMass-Foundation-Model/CoVLM,transformers/src/transformers/trainer.py,2023-11-01T14:42:53Z,init commit
github.com/UMass-Foundation-Model/CoVLM,transformers/src/transformers/modeling_utils.py,2023-11-01T14:42:53Z,init commit
github.com/UMass-Foundation-Model/CoVLM,transformers/src/transformers/integrations/peft.py,2023-11-01T14:42:53Z,init commit
github.com/MILVLG/imp,imp_llava/model/builder.py,2024-02-09T09:25:25Z,add training/evaluation code
github.com/bryanchrist/llama2-70b,ppo.py,2023-09-07T21:01:20Z,generated text in 8-bit
github.com/bryanchrist/llama2-70b,ppo.py,2023-08-18T12:41:57Z,building PPO model
github.com/bryanchrist/llama2-70b,ppo.py,2023-08-04T19:55:08Z,fixing deletion of repository
github.com/OSU-NLP-Group/llm-planning-eval,generators/hf_generator.py,2024-02-21T19:31:04Z,Added code. Updated README. 02/21/24/
github.com/OSU-NLP-Group/llm-planning-eval,evaluators/codellama_evaluator.py,2024-02-21T19:31:04Z,Added code. Updated README. 02/21/24/
github.com/DeepSoftwareAnalytics/SoTaNa,webui/generate.py,2023-09-15T09:31:11Z,add webui
github.com/DeepSoftwareAnalytics/SoTaNa,inference/code-generation/inference.py,2023-08-25T14:15:00Z,first commit
github.com/DeepSoftwareAnalytics/SoTaNa,inference/code-summarization/inference.py,2023-08-25T14:15:00Z,first commit
github.com/DeepSoftwareAnalytics/SoTaNa,inference/stackoverflow-question-answering/inference.py,2023-08-25T14:15:00Z,first commit
github.com/china-ai-law-challenge/CAIL2023,ssrd/baseline/eval/inference3_ev.py,2023-09-08T01:39:35Z,add ssrd track
github.com/china-ai-law-challenge/CAIL2023,ssrd/baseline/eval/inference2_inter.py,2023-09-08T01:39:35Z,add ssrd track
github.com/china-ai-law-challenge/CAIL2023,ssrd/baseline/eval/inference4_final_result.py,2023-09-08T01:39:35Z,add ssrd track
github.com/Azure/app-service-linux-docs,HowTo/gRPC/Linux/OpenAI/LangChain/PyServer/venv/Lib/site-packages/transformers/trainer.py,2024-02-29T20:19:37Z,adding windows grpc docs
github.com/choosewhatulike/trainable-agents,FastChat/fastchat/model/apply_lora.py,2023-11-14T13:13:35Z,camera ready
github.com/choosewhatulike/trainable-agents,FastChat/fastchat/model/model_adapter.py,2023-11-14T13:13:35Z,camera ready
github.com/Efficient-Large-Model/VILA,llava/train/train.py,2024-02-23T09:19:39Z,first commit
github.com/Efficient-Large-Model/VILA,export_peft_model.py,2024-02-23T09:19:39Z,first commit
github.com/Efficient-Large-Model/VILA,llava/model/builder.py,2024-02-23T09:19:39Z,first commit
github.com/AlanAnsell/peft,src/peft/auto.py,2023-08-07T14:34:54Z,"[`core`] PEFT refactor + introducing `inject_adapter_in_model` public method (#749)

Refactors a bit the internals of some PEFT models and introduces a new
method inject_adapter_in_model for users that want to pass a bare model
and a peft config to inject adapters in-place into the model. These
changes are totally BC with the previous PEFT versions.

This PR makes things easier for the PEFT integration in transformers
huggingface/transformers#25077

The main goal of the PR is to expose a new API for advanced users that
want to integrate PEFT method without the need to use the PeftModel
wrapper. A simple use case could be someone that wants to inject adapters
into a model and wants to keep the original class of the model without
having to offload that to peft that will create a PeftModel. I have
faced this issue in huggingface/transformers#25077 Among other things,
this PR refactors some internals of PEFT library, while keeping it fully
backward compatible.

To tackle the main motivation I propose to differentiate things between
two type of adapters

1- adapters that are injectable (LoRA, AdaLoRA, IA3)
2- adapters that are not injectable (the rest)

As a first iteration this API would be supported only for the scenario
1- / therefore I decided to create 2 abstract classes to make things
easy to be able to determine if the adapter layer (e.g. LoraLayer) /
adapter module (e.g. LoraModel) does follow the minimal
requirement (i.e. needed attributes, etc.)

Other related changes:

1- Creates a new property method is_prompt_learning to avoid importing
   PromptLearningConfig all the way down
2- Introduces a new object TUNERS_MAPPING, which is a mapping of
   supported pluggable adapters
3- Creates two abstract classes
3.1- BaseTunerLayer: a mixin to check for minimal required attributes
     that a tuner layer should have active_adapter / _is_plugable
3.2- BaseTuner: a higher level module mixin that should be used for any
     injectable adapters in the future.

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
github.com/AlanAnsell/peft,src/peft/auto.py,2023-07-15T12:18:34Z,"[`Auto`] Support `AutoPeftModel` for custom HF models (#707)

* support `AutoPeftModel` for custom HF models

* added documentation."
github.com/AlanAnsell/peft,src/peft/auto.py,2023-07-14T09:07:09Z,"Introducing `AutoPeftModelForxxx` (#694)

* working v1 for LMs

* added tests.

* added documentation.

* fixed ruff issues.

* added `AutoPeftModelForFeatureExtraction` .

* replace with `TypeError`

* address last comments

* added comment."
github.com/AlanAnsell/peft,src/peft/config.py,2023-08-07T14:34:54Z,"[`core`] PEFT refactor + introducing `inject_adapter_in_model` public method (#749)

Refactors a bit the internals of some PEFT models and introduces a new
method inject_adapter_in_model for users that want to pass a bare model
and a peft config to inject adapters in-place into the model. These
changes are totally BC with the previous PEFT versions.

This PR makes things easier for the PEFT integration in transformers
huggingface/transformers#25077

The main goal of the PR is to expose a new API for advanced users that
want to integrate PEFT method without the need to use the PeftModel
wrapper. A simple use case could be someone that wants to inject adapters
into a model and wants to keep the original class of the model without
having to offload that to peft that will create a PeftModel. I have
faced this issue in huggingface/transformers#25077 Among other things,
this PR refactors some internals of PEFT library, while keeping it fully
backward compatible.

To tackle the main motivation I propose to differentiate things between
two type of adapters

1- adapters that are injectable (LoRA, AdaLoRA, IA3)
2- adapters that are not injectable (the rest)

As a first iteration this API would be supported only for the scenario
1- / therefore I decided to create 2 abstract classes to make things
easy to be able to determine if the adapter layer (e.g. LoraLayer) /
adapter module (e.g. LoraModel) does follow the minimal
requirement (i.e. needed attributes, etc.)

Other related changes:

1- Creates a new property method is_prompt_learning to avoid importing
   PromptLearningConfig all the way down
2- Introduces a new object TUNERS_MAPPING, which is a mapping of
   supported pluggable adapters
3- Creates two abstract classes
3.1- BaseTunerLayer: a mixin to check for minimal required attributes
     that a tuner layer should have active_adapter / _is_plugable
3.2- BaseTuner: a higher level module mixin that should be used for any
     injectable adapters in the future.

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
github.com/AlanAnsell/peft,src/peft/helpers.py,2023-08-10T10:14:40Z,"Helper function to update model signature (#784)

Provides helper functions in peft.helpers to update the signature of the
forward or generate method of a PeftModel (or subclass). This can be
useful because the wrapping class may override the docstring and type
annotations of the underlying base model. Applying the helper functions
will restore those, leading to better tab completion, help text, etc.

For the time being, these helper functions are purely optional to use.
At a later stage, we may consider applying them automatically, but that
would require testing to ensure that nothing breaks."
github.com/AlanAnsell/peft,src/peft/peft_model.py,2024-01-24T10:58:37Z,add further documentation
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-11-23T12:35:54Z,bug fix for altered SFT-RigL behaviour; added implementation of SFT-accumulative
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-11-13T14:21:04Z,support non-fp32 dtypes for SFT deltas
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-11-11T15:26:42Z,implement L2 regularisation for SftModel
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-09-08T21:39:19Z,initial implementation of SftModel
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-08-30T15:16:22Z,DOC: PeftModel save_pretrained docstring (#881) (#888)
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-08-29T08:53:14Z,FIX: seq2seq prompt tuning (#439) (#809)
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-08-25T06:12:11Z,"🎉 Add Multitask Prompt Tuning (#400)

* mpt

* fix save

* fix save

* add jupyter notebook

* add jupyter notebook

* add jupyter notebook

* drop shuffling

* drop classify_dataset

* drop classify_dataset

* fix keys

* fix keys

* add comments

* use EXACT_SOURCE_TASK in the example

* formatting

* Fix dict index in embedding retrieval

* run style and quality

* run style and quality

* run style and quality

* style

* final fix

* style

* comment out failing tests

* fix generation tests

* fix style and save test

* all testcases

* fix import

* add license header

* reformat

* fix encoder-decoder models

* fix tests running multiple times

* fix paper name for IA3 and add MPT paper

* Trigger CI

* address the recommended changes

* reformat

* address suggestions

* address suggestions

* revert reformatting

* revert reformatting

---------

Co-authored-by: Alex-Brooks <Alex.Brooks@ibm.com>"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-08-11T21:31:17Z,"GPTQ Integration (#771)

* add gptq lora

* fix peft gptq

* fix condition

* fix test

* remove unused weights

* check type

* style

* change attribute

* remove print

* add exllama

* make style

* refactor + fix tests

* remove print

* remove dep on transformers"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-08-08T12:38:23Z,"Update docstring of PeftModel.from_pretrained (#799)

1. Addresses
https://github.com/huggingface/peft/issues/430#issuecomment-1666312815
2. Reword docstring to not be LoRA-specific"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-08-08T12:35:19Z,"Add adapter error handling (#800)

When a user tries to add a 2nd adapter, Lora and AdaLora make some checks to
ensure the new adapter is compatible with existing adapters. Currently, that
check is performed halfway through the method. This means that if the check
fails, the new adapter is partially applied, leaving the model in a bad state.
The main purpose of this PR is to ensure that the model state is correct after
such a failure is encountered.

Tests were added to catch this potential bug.

While working on this, I also did some related, but not strictly necessary
changes to the add_adapter methods:

- Previously, the peft_config from the PeftModel was passed to the base
  model. This meant that sometimes, the base model would hold a reference
  to PeftModel.peft_config, but not always, as some base models would
  create new dicts. This is problematic, because some code would rely on
  the objects being the same. Now, they are never the same, leading to
  more consistency.
- I think that the check if multiple adapters have biases (which is not
  supported) was accidentally removed by #749. It is added back in.
- Add some type annotations
- Extend docstrings to contain adapter_name"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-08-07T14:34:54Z,"[`core`] PEFT refactor + introducing `inject_adapter_in_model` public method (#749)

Refactors a bit the internals of some PEFT models and introduces a new
method inject_adapter_in_model for users that want to pass a bare model
and a peft config to inject adapters in-place into the model. These
changes are totally BC with the previous PEFT versions.

This PR makes things easier for the PEFT integration in transformers
huggingface/transformers#25077

The main goal of the PR is to expose a new API for advanced users that
want to integrate PEFT method without the need to use the PeftModel
wrapper. A simple use case could be someone that wants to inject adapters
into a model and wants to keep the original class of the model without
having to offload that to peft that will create a PeftModel. I have
faced this issue in huggingface/transformers#25077 Among other things,
this PR refactors some internals of PEFT library, while keeping it fully
backward compatible.

To tackle the main motivation I propose to differentiate things between
two type of adapters

1- adapters that are injectable (LoRA, AdaLoRA, IA3)
2- adapters that are not injectable (the rest)

As a first iteration this API would be supported only for the scenario
1- / therefore I decided to create 2 abstract classes to make things
easy to be able to determine if the adapter layer (e.g. LoraLayer) /
adapter module (e.g. LoraModel) does follow the minimal
requirement (i.e. needed attributes, etc.)

Other related changes:

1- Creates a new property method is_prompt_learning to avoid importing
   PromptLearningConfig all the way down
2- Introduces a new object TUNERS_MAPPING, which is a mapping of
   supported pluggable adapters
3- Creates two abstract classes
3.1- BaseTunerLayer: a mixin to check for minimal required attributes
     that a tuner layer should have active_adapter / _is_plugable
3.2- BaseTuner: a higher level module mixin that should be used for any
     injectable adapters in the future.

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-08-02T14:59:11Z,"Allow passing inputs_embeds instead of input_ids (#757)

Resolves #727

Right now, there is an issue with a few PeftModelForXxx classes when
users pass only inputs_embeds but not input_ids. First of all, the batch
size was being derived on input_ids, now it is derived from
inputs_embeds instead if input_ids is None. Furthermore, a few forward
calls to the base model were not passing the inputs_embeds along, which
resulted in errors down the line. These issues have been fixed now."
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-08-01T13:46:18Z,Support XPU adapter loading   (#737)
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-07-19T12:57:14Z,"[`Patch`] patch trainable params for 4bit layers (#733)

* patch trainable params for 4bit layers

* revert

* added tests.

* added comments.

* addressed final comments"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-07-19T12:29:36Z,"[`Llama2`] Add disabling TP behavior (#728)

* add disabling TP behavior

* add comments

* adapt from new changes of transformers PR"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-07-19T08:59:55Z,revert change (#731)
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-07-19T07:52:25Z,fix the param count when using 4-bit bnb
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-07-19T05:47:15Z,"Fix subfolder issue (#721)

* fix subfolder issue

* added tests"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-07-17T09:58:57Z,better hub kwargs management (#712)
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-07-17T08:02:30Z,"FEAT: Make LoRA work with custom models (#676)

Enable custom models to work with LoRA

This PR enables custom models to work with LoRA in peft by performing a few
changes required for non-transformers models. New tests for linear,
transformers conv1d, and conv2d layers were added.

Not yet contained in this PR:

- support for AdaLoRA and IA³
- documentation
- examples

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-07-15T12:18:34Z,"[`Auto`] Support `AutoPeftModel` for custom HF models (#707)

* support `AutoPeftModel` for custom HF models

* added documentation."
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-07-14T14:28:03Z,"[`Feature`] Save only selected adapters for LoRA (#705)

* v1 working for LoRA

* more checks

* fix prompt learning issues

* fix failing test

* Apply suggestions from code review

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* fixed indentation

* move the check above

* added tests for adaption prompt, enc-dec and feature extraction

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-07-14T14:14:51Z,"[Core] Enhancements and refactoring of LoRA method (#695)

* refactor lora and add utils

1. Refactor LoRA code
2. Add method to delete LoRA adapters
3. Add method to unload the PEFT LoRA model.
4. Add `svd` weighted adapter support.
5. minor fixes

* fixes

* fixes

* Update lora.py

* fixes

* Update lora.py

* docstrings for the added public APIs

* docs

* Update src/peft/tuners/lora.py

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* resolve comments, refactoring and adding tests

* fix the remaining failing tests

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-07-14T12:33:33Z,"[WIP] FIX for disabling adapter, adding tests (#683)

This PR deals with some issues with disabling adapter:

- typo in active.adapter
- prompt encoder could be on wrong device
- when using prompt learning + generate, disabling did not work

For the last point, there is a somewhat ugly fix in place for now,
pending a more comprehensive refactor (a comment was added to that
effect).

Comprehensive tests were added to check that everything works now.

The following tests still not working:

- adaption prompt
- seq2seq with prompt tuning/prompt encoding
- stable diffusion is a little bit flaky but test is hopefully robust enough

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-07-13T12:34:28Z,"add support for Feature Extraction using PEFT (#647)

* add support for embedding with peft

* add example and resolve code quality issues

* update notebook example post fixing the loss

* adding full example with inference notebook

* quality ✨

* add tests, docs, guide and rename task_type to be inline with Hub

* fixes

* fixes

* Apply suggestions from code review

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update peft_model.py

* fixes

* final fixes

* Update _toctree.yml

* fixes and make style and make quality

* deberta exception with checkpointing

* Update docs/source/task_guides/semantic-similarity-lora.md

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Update docs/source/task_guides/semantic-similarity-lora.md

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* resolve comments

* testing prompt learning methods

* Update testing_common.py

* fix the tests

---------

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>
Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>
Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-07-13T09:12:40Z,FIX: base_model_torch_dtype when using model.half() after init (#688)
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-07-13T07:45:50Z,"Add functionality to support IA3 (#578)

* Added initial ia3 code

* Implemented ia3 correctly for feedforward layers; Fixed regex matching

* Fixed module mapping for mt5

* Merged changes from huggingface:main

* Merged changes

* Fixed lora merge conflicts

* Different bloom config

* Added save option for ia3

* Added loading code for ia3

* Added feedforward implementation in utils and seq cls example

* Added feedforward implementation in utils and seq cls example

* Implemented merge, unmerge, enable/disable adapters functionality

* Fixed feedforward during merge

* Debugging Merge

* Removing debug messages

* Cleaned up repo

* Removed non-IA3 changes

* Refactor save and load

* Added support to all models in tests; Added IA3Config for common tests

* Added half-precision support and test for gradient checkpointing; Formatted jupyter notebooks

* Added target modules for new models GPTBigCode and LLama

* Cleaned up code

* Cleaned up code

* Cleaned up example notebook

* Cleaned up  seq2seq notebook

* Corrected function docstrings; refactored find_and_replace

* Corrected function docstrings; refactored find_and_replace

* Added basic docs for IA3

* Added new conceptual guide in source tree for documentation

* Minor fix to documentation

* Minor fixes to docstrings; Added error handling for 4bit quantization; Cleaned unused merge/unmerge methods

* styling changes after merge from main

* Update src/peft/tuners/ia3.py

Remove unused attribute merge_weights

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>

---------

Co-authored-by: Abhishek2304 <abhishekgupta2304@gmail.com>
Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-06-28T07:03:16Z,"style: tentatively add hints for some public function (#614)

* style: tentatively add hints for some public function

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* fix: import annotations to evaluate to str

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* fix: style

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-06-27T21:41:51Z,Update peft_model.py (#644)
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-06-27T12:57:57Z,"feat(model): Allow from_pretrained to accept PeftConfig class (#612)

* feat(model): Allow from_pretrained to accept PeftConfig class

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* tests: add test cases for config construction

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* chore: address comments and run tools

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

* fix: style

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>

---------

Signed-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-06-27T11:26:47Z,"fix ptun and prompt tuning generation issue (#543)

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-06-27T08:27:21Z,"fix Prefix-tuning error in clm Float16 evaluation (#520)

Signed-off-by: Wang, Yi <yi.a.wang@intel.com>"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-06-27T06:15:49Z,"Add seq2seq prompt tuning support (#519)

* Added prompt tuning for seq2seq and corresponding notebook examples

* Added prompt tuning for seq2seq and corresponding notebook examples

* Added prompt tuning for seq2seq and corresponding notebook examples

* Call encoder with get_encoder() and update notebook example

* Style formatting

* Add seq2seq p-tuning support, and improve seq2seq prompt tuning support, enabling the use of generate()

* Fix imports

* Fix imports

* Add co-author.

Co-authored-by: ZhengxiangShi michaelszx117@gmail.com

* Add co-author.

Co-authored-by: ZhengxiangShi <michaelszx117@gmail.com>

---------

Co-authored-by: Thomas SCHILLACI <tschilla@px101.prod.exalead.com>
Co-authored-by: ZhengxiangShi <michaelszx117@gmail.com>"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-06-19T08:49:41Z,"Improve the README when using PEFT (#594)

* add logic

* Update peft_model.py

* fix test failures

* fixes

* fix"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-06-16T11:23:58Z,"feat: Add PeftModelForQuestionAnswering (#473)

* Added first try of supporting QuestionAnswering

* Updated example to be correct

* Added changes from PR 404

* Added missing mapping for task type

* Remove unrelated code

* Run make style"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-06-16T11:04:07Z,"when from_pretrained is called in finetune of lora with flag ""is_trainable"" True, should not call model.eval() (#591)"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-06-16T08:58:51Z,"Fix typo at peft_model.py (#588)

Fix typo on description:
- `imputs_embeds` to `inputs_embeds`"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-06-15T10:23:05Z,"[`core`] Correctly passing the kwargs all over the place (#575)

* v1 of the fix

* forward contrib credits from discussions

* add tests

---------

Co-authored-by: winglian <winglian@users.noreply.github.com>"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-06-15T07:35:43Z,"enable lora for mpt (#576)

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-06-09T10:33:13Z,"[`core`] Add safetensors integration (#553)

* add v1

* clean up

* more improvements

* add device

* final adjustements

* use `EntryNotFoundError`

* better checks

* add tests and final fixes

* make style && make quality

* remove `push_to_hub` because of the release"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-06-07T12:39:17Z,add thousands separator in print_trainable_parameters (#443)
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-06-05T13:14:40Z,add library name to model card (#549)
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-06-01T09:17:05Z,Fixed problem with duplicate same code. (#517)
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-06-01T09:16:38Z,return load_result when load_adapter (#481)
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-06-01T09:09:54Z,"Enable PeftConfig & PeftModel to load from revision (#433)

* Enable PeftConfig to load from revision

* Add revision to PeftModel

* Fix weights download with revision"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-05-31T10:14:27Z,"[`core`] Add gradient checkpointing check (#404)

* add automatic input enable gradients when calling `get_peft_model`

* style

* better check

* add 4bit check"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-05-31T06:08:12Z,Remove merge_weights (#392)
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-05-20T15:47:15Z,"4-bit QLoRA via bitsandbytes (4-bit base model + LoRA) (#476)

* 4bit lora

* 4bit test

* fixing 4bits bugs

* fp4 pass variables

* fix inference datatype and generation config

* updating prep for int8 function to work for 4-bit

* Added FP4 LoRA and FP4 fine-tuning example.

* LinearFP4 -> Linear4bit

* fixes

* Fixed 4-bit example.

* Style changes.

* final changes

---------

Co-authored-by: Artidoro Pagnoni <pagnoni.artidoro@gmail.com>
Co-authored-by: younesbelkada <younesbelkada@gmail.com>"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-05-10T04:39:28Z,"do not use self.device. In FSDP cpu offload mode. self.device is ""CPU"" instead of ""cuda"" (#352)

and there's error like ""Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:1""

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-04-26T12:56:14Z,Use `try` and `finally` in `disable_adapter()` to catch exceptions (#368)
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-04-25T06:54:18Z,"Implement adaption prompt from Llama-Adapter paper (#268)

* Implement adaption prompt from Llama-Adapter paper

* Support multi-adapters

* Refactor adaption prompt to target attn modules instead of layers

* Refactor adaption prompt to be more generic

* Fix adaption prompt not on right device

* Apply suggestions from code review

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>

* Fix style

* Add support for Llama config use_cache=True

* Fix rebase issues

---------

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-04-20T10:46:13Z,"fix lora modules_to_save issue (#343)

* fix lora modules_to_save issue

* fix quality"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-04-08T08:27:08Z,"Merge pull request #283 from huggingface/smangrul/multi-lora-support

fix trainable params setting"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-04-08T06:15:32Z,Update peft_model.py
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-04-07T10:48:22Z,add and fix tests
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-04-07T10:36:58Z,Merge remote-tracking branch 'upstream/main' into fix-half-prec
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-04-06T22:38:10Z,fixing adalora saving and loading
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-04-06T14:01:21Z,😅
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-04-06T13:35:31Z,Merge branch 'main' into smangrul/multi-lora-support
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-04-06T06:00:30Z,"Merge pull request #233 from QingruZhang/main

The Implementation of AdaLoRA (ICLR 2023)"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-04-05T20:52:12Z,Run make style and make quality
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-04-04T20:33:31Z,fix 🐛
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-04-04T14:14:58Z,fixing 🐛
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-04-04T12:14:24Z,fix 🐛
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-04-04T11:34:32Z,Merge branch 'main' into smangrul/multi-lora-support
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-04-04T10:01:47Z,fix half precision forward
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-04-03T16:28:11Z,Fixing a bug where a wrong parameter name is used.
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-04-01T12:54:46Z,"[`core`] Fix offload issue (#248)

* fix offload dir

* remove offload index

* safety checker

* forward contrib credits from previous PR

---------

Co-authored-by: cosimoiaia <cosimoiaia@users.noreply.github.com>"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-03-31T21:41:14Z,fix kwargs
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-03-31T21:30:05Z,clean up docstrings
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-03-30T06:24:45Z,Merge branch 'huggingface:main' into main
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-03-29T13:20:14Z,"Causal LM generation fix for prefix tuning: GPT2 model (#222)

* expand attention mask after preparing generation inputs for prefix tuning

* reformat

* Update src/peft/peft_model.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* reformat as per black

---------

Co-authored-by: Vineet Kumar <vineeku6@in.ibm.com>
Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-03-29T03:19:14Z,merge the conflit'
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-03-28T14:10:06Z,Update peft_model.py
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-03-28T14:02:21Z,Update peft_model.py
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-03-28T13:59:24Z,Update peft_model.py
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-03-28T13:26:24Z,"multi adapter for training and inference

Might have breaking changes"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-03-10T02:36:11Z,changed: 1. replace base_model.prepare_inputs_for_generation and base_model._prepare_encoder_decoder_kwargs_for_generation temporarily
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-03-08T17:36:25Z,"Merge pull request #157 from huggingface/smangrul/lora_fixes_and_updates_wrt_trl

lora fixes and adding 8bitMegredLinear lora"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-03-08T10:41:30Z,"fix count 

num_params should be directly used."
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-03-07T12:29:02Z,adding 8bitMegredLinear lora
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-03-07T08:38:25Z,"Merge pull request #149 from huggingface/smangrul/fixes

minor fixes to the examples"
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-03-07T08:34:19Z,fixing issues and quality ✨
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-03-03T18:13:25Z,support option for encoder only prompts
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-03-02T01:04:48Z,finish the testing and debugging
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-03-01T21:26:07Z,adalora example
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-02-27T03:31:20Z,fix: count params when zero init'd
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-02-25T06:09:07Z,issue#126: torch.load device issue.
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-02-21T18:44:24Z,add util for getting the base model
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-02-17T12:10:45Z,add `disable_adapter` context manager
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-02-10T04:09:02Z,fp32
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-02-09T18:36:25Z,make `save_pretrained` work in a way training could be resumed
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-02-08T13:13:00Z,quality
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-02-08T12:37:15Z,fixes and updating examples
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-02-08T06:48:03Z,add `load_and_dispatch` to `load_pretrained`
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-02-08T03:19:15Z,fix prefix tuning config to remove function field as it cannot be converted to json
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-02-07T09:37:56Z,add support for `generate` when using `prompt_tuning`
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-02-06T18:20:48Z,fix `generate` because of recent transformers release
github.com/AlanAnsell/peft,src/peft/peft_model.py,2023-02-06T13:27:13Z,seq cls examples update
github.com/AlanAnsell/peft,src/peft/tuners/lora/model.py,2023-11-09T17:27:31Z,implement no-Trainer RigL-SFT (and break everything else)
github.com/AlanAnsell/peft,src/peft/tuners/lora/model.py,2023-11-07T13:00:22Z,start implementing cumulative SFT
github.com/AlanAnsell/peft,src/peft/tuners/lora/model.py,2023-09-07T10:14:37Z,"ENH Merge lora module to 8bit model (#875)

Allows merging 8bit weights from bnb.

4bit weight merging was already implemented through the dequantization method
provided by bnb but there is no official dequantization method for 8bit weights.
This PR works by multiplying the weights to an identity matrix using bnb's
quantization aware matmul operation. Empirically, this results in a very small
rounding error."
github.com/AlanAnsell/peft,src/peft/tuners/lora/model.py,2023-09-06T15:31:55Z,"ENH Remove redundant initialization layer calls (#887)

This should lead to a big speedup when initializing LoRA layers.

---------

Co-authored-by: poedator <ruslansv@gmail.com>"
github.com/AlanAnsell/peft,src/peft/tuners/lora/model.py,2023-08-29T09:32:29Z,"MNT: Move tuners to subpackages (#807)

For each tuner, created a sub-module that contains at least:

- config.py for config stuff
- model.py for the actual model/encoder/embedding
- __init__.py so that imports are preserved

Then, when there was a need, further files were created, like layer.py
or utils.py.

Imports were changed to absolute imports everywhere, except for the
sub-packages within a tuner directory, as these packages will always 
stay together in the same place.

For some existing modules, the license comment of the top of the file
was missing, I always added it.

There was a bug in the forward method of 4bit linear lora layers introduced
in #851, for the case that the model is merged AND adapters are disabled.
For that scenario, we need to unmerge first before generating the output,
same as we do for the vanilla Linear layer. This step was missing from the
code previously and is now implemented correctly. Tests were adjusted to
catch that error."
github.com/AlanAnsell/peft,examples/fp4_finetuning/finetune_fp4_opt_bnb_peft.py,2023-06-02T08:07:46Z,"Remove device_map when training 4,8-bit model. (#534)

* Remove device_map when training 4,8-bit model.

* Fix style"
github.com/AlanAnsell/peft,examples/fp4_finetuning/finetune_fp4_opt_bnb_peft.py,2023-05-20T15:47:15Z,"4-bit QLoRA via bitsandbytes (4-bit base model + LoRA) (#476)

* 4bit lora

* 4bit test

* fixing 4bits bugs

* fp4 pass variables

* fix inference datatype and generation config

* updating prep for int8 function to work for 4-bit

* Added FP4 LoRA and FP4 fine-tuning example.

* LinearFP4 -> Linear4bit

* fixes

* Fixed 4-bit example.

* Style changes.

* final changes

---------

Co-authored-by: Artidoro Pagnoni <pagnoni.artidoro@gmail.com>
Co-authored-by: younesbelkada <younesbelkada@gmail.com>"
github.com/AlanAnsell/peft,examples/lora_dreambooth/convert_peft_sd_lora_to_kohya_ss.py,2023-06-27T08:26:54Z,"[Bugfix] Inserted adapter_name to get_peft_model_state_dict function (#626)

* Update train_dreambooth.py

Accelerator init updated from logging_dir to project_dir. Newer versions of accelerate uses project_dir. logging_dir is deprecated

* Bugfix: Adapter name variable inserted, when changing LORA_ADAPTER_NAME it causes error

* Adapter name added as kwarg

* Black code formatted

* Style & Quality check"
github.com/AlanAnsell/peft,examples/lora_dreambooth/convert_peft_sd_lora_to_kohya_ss.py,2023-06-21T14:04:39Z,"Added Civitai LoRAs conversion to PEFT, PEFT LoRAs conversion to webui (#596)

* Fixed kohya_ss to peft lora conversion, added script for backward conversion

* Fixed getting alpha from PEFT

---------

Co-authored-by: Alexander Kovalchuk <a.kovalchuk@prequelapp.com>"
github.com/AlanAnsell/peft,tests/test_hub_features.py,2023-07-19T05:47:15Z,"Fix subfolder issue (#721)

* fix subfolder issue

* added tests"
github.com/AlanAnsell/peft,tests/test_custom_models.py,2023-09-06T15:31:55Z,"ENH Remove redundant initialization layer calls (#887)

This should lead to a big speedup when initializing LoRA layers.

---------

Co-authored-by: poedator <ruslansv@gmail.com>"
github.com/AlanAnsell/peft,tests/test_custom_models.py,2023-08-30T12:40:56Z,"MNT Run tests that were skipped previously (#884)

Some tests were skipped because of an issue with how LoRA weights were
initialized for embeddings. This issue has been fixed for some time now,
so the tests no longer need to be skipped."
github.com/AlanAnsell/peft,tests/test_custom_models.py,2023-08-16T08:57:38Z,"TST: add test about loading custom models (#827)

Prompted by #808, I added a test that shows that loading a trained
custom model works as expected. I only added this to custom models
because it involves a few steps of training and I didn't want to slow
down tests too much. LMK if this should be added to all tests.

In addition, I renamed some custom model tests which had strange
names (probably caused by an overeager query-replace)."
github.com/AlanAnsell/peft,tests/test_custom_models.py,2023-08-08T12:35:19Z,"Add adapter error handling (#800)

When a user tries to add a 2nd adapter, Lora and AdaLora make some checks to
ensure the new adapter is compatible with existing adapters. Currently, that
check is performed halfway through the method. This means that if the check
fails, the new adapter is partially applied, leaving the model in a bad state.
The main purpose of this PR is to ensure that the model state is correct after
such a failure is encountered.

Tests were added to catch this potential bug.

While working on this, I also did some related, but not strictly necessary
changes to the add_adapter methods:

- Previously, the peft_config from the PeftModel was passed to the base
  model. This meant that sometimes, the base model would hold a reference
  to PeftModel.peft_config, but not always, as some base models would
  create new dicts. This is problematic, because some code would rely on
  the objects being the same. Now, they are never the same, leading to
  more consistency.
- I think that the check if multiple adapters have biases (which is not
  supported) was accidentally removed by #749. It is added back in.
- Add some type annotations
- Extend docstrings to contain adapter_name"
github.com/AlanAnsell/peft,tests/test_custom_models.py,2023-07-24T11:23:23Z,"FIX: Disabling adapter works with modules_to_save (#736)

Resolves #493

For LoRA and IA³, there was a bug that even even using the
disable_adapter context, if the module was listed in modules_to_save,
the updated weights would be used instead of the original weights. This
meant that disable_adapter would not return the same results as the base
model without adaptation. This PR fixes the issue and provides a test.

Note: I tried to adjust AdaLoRA too, since it seemed that the same
reasoning should apply there. However, I think that AdaLoRA does not
really support disabling adapters at all. E.g. there is no
disable_adapter_layers method. Therefore, AdaLoRA was not changed."
github.com/AlanAnsell/peft,tests/test_custom_models.py,2023-07-24T08:34:39Z,"ENH: Warn when disabling adapters and bias != 'none' (#741)

For LoRA, given that bias='all' or bias='none', when doing inference
with a model in the disable_adapter context, the output will not be
identical to the output of the base model. This may be surprising to
users. Therefore, a warning is given. Furthermore, the docstring has
been extended to reflect this fact."
github.com/AlanAnsell/peft,tests/test_custom_models.py,2023-07-17T08:02:30Z,"FEAT: Make LoRA work with custom models (#676)

Enable custom models to work with LoRA

This PR enables custom models to work with LoRA in peft by performing a few
changes required for non-transformers models. New tests for linear,
transformers conv1d, and conv2d layers were added.

Not yet contained in this PR:

- support for AdaLoRA and IA³
- documentation
- examples

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/AlanAnsell/peft,tests/test_adaption_prompt.py,2023-07-14T14:28:03Z,"[`Feature`] Save only selected adapters for LoRA (#705)

* v1 working for LoRA

* more checks

* fix prompt learning issues

* fix failing test

* Apply suggestions from code review

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* fixed indentation

* move the check above

* added tests for adaption prompt, enc-dec and feature extraction

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/AlanAnsell/peft,tests/test_adaption_prompt.py,2023-07-14T12:33:33Z,"[WIP] FIX for disabling adapter, adding tests (#683)

This PR deals with some issues with disabling adapter:

- typo in active.adapter
- prompt encoder could be on wrong device
- when using prompt learning + generate, disabling did not work

For the last point, there is a somewhat ugly fix in place for now,
pending a more comprehensive refactor (a comment was added to that
effect).

Comprehensive tests were added to check that everything works now.

The following tests still not working:

- adaption prompt
- seq2seq with prompt tuning/prompt encoding
- stable diffusion is a little bit flaky but test is hopefully robust enough

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/AlanAnsell/peft,tests/test_adaption_prompt.py,2023-06-01T09:14:11Z,"[`Llama-Adapter`] fix half precision inference + add tests (#456)

* fix + add tests

* forward contrib credits from discussions

---------

Co-authored-by: HamidShojanazeri <HamidShojanazeri@users.noreply.github.com>"
github.com/AlanAnsell/peft,tests/test_adaption_prompt.py,2023-04-25T06:54:18Z,"Implement adaption prompt from Llama-Adapter paper (#268)

* Implement adaption prompt from Llama-Adapter paper

* Support multi-adapters

* Refactor adaption prompt to target attn modules instead of layers

* Refactor adaption prompt to be more generic

* Fix adaption prompt not on right device

* Apply suggestions from code review

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>

* Fix style

* Add support for Llama config use_cache=True

* Fix rebase issues

---------

Co-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>"
github.com/AlanAnsell/peft,tests/test_multitask_prompt_tuning.py,2023-08-25T06:12:11Z,"🎉 Add Multitask Prompt Tuning (#400)

* mpt

* fix save

* fix save

* add jupyter notebook

* add jupyter notebook

* add jupyter notebook

* drop shuffling

* drop classify_dataset

* drop classify_dataset

* fix keys

* fix keys

* add comments

* use EXACT_SOURCE_TASK in the example

* formatting

* Fix dict index in embedding retrieval

* run style and quality

* run style and quality

* run style and quality

* style

* final fix

* style

* comment out failing tests

* fix generation tests

* fix style and save test

* all testcases

* fix import

* add license header

* reformat

* fix encoder-decoder models

* fix tests running multiple times

* fix paper name for IA3 and add MPT paper

* Trigger CI

* address the recommended changes

* reformat

* address suggestions

* address suggestions

* revert reformatting

* revert reformatting

---------

Co-authored-by: Alex-Brooks <Alex.Brooks@ibm.com>"
github.com/chs20/RobustVLM,llava/model/builder.py,2024-02-19T18:17:12Z,Initial commit
github.com/kotoba-tech/kotoba-recipes,src/llama_recipes/inference/model_utils.py,2023-11-28T02:49:13Z,initial commit
github.com/kotoba-tech/kotoba-recipes,examples/hf_text_generation_inference/merge_lora_weights.py,2023-11-28T02:49:13Z,initial commit
github.com/yakami129/virtualwife-llm-factory,finetune/framework/llama_factory/src/llmtuner/model/adapter.py,2024-01-13T02:50:41Z,update: 集成llama_factory
github.com/unionai-oss/llm-fine-tuning,redpajama-lora/workflows/llm_inference.py,2023-06-28T06:23:01Z,"modify prompt; modify dataset and model paths

Signed-off-by: Samhita Alla <aallasamhita@gmail.com>"
github.com/unionai-oss/llm-fine-tuning,redpajama-lora/workflows/llm_inference.py,2023-06-02T16:48:26Z,"add fine tuning redpajama using slack data

Signed-off-by: Samhita Alla <aallasamhita@gmail.com>"
github.com/StrongResearch/isc-demos,llama2/llama-recipes/src/llama_recipes/finetuning.py,2023-12-19T23:19:17Z,validation
github.com/StrongResearch/isc-demos,llama2/llama-recipes/src/llama_recipes/inference/model_utils.py,2023-12-19T23:19:17Z,validation
github.com/StrongResearch/isc-demos,llama2/llama-recipes/examples/hf_text_generation_inference/merge_lora_weights.py,2023-12-19T23:19:17Z,validation
github.com/Junjie-Ye/ToolEyes,Code/Inference/models.py,2024-01-12T16:31:24Z,update code
github.com/Junjie-Ye/ToolEyes,Code/model/model_adapter.py,2024-01-12T16:31:24Z,update code
github.com/X-PLUG/mPLUG-HalOwl,HaELM/interface.py,2024-01-29T03:34:16Z,Update HaELM
github.com/X-PLUG/mPLUG-HalOwl,hacl/llava/model/builder.py,2024-01-22T05:56:36Z,commit hacl
github.com/yanqiangmiffy/llm-finetune,bakcup/peft/peft_model.py,2023-11-21T06:06:20Z,update
github.com/yanqiangmiffy/llm-finetune,bakcup/peft/tuners/lora.py,2023-11-21T06:06:20Z,update
github.com/yanqiangmiffy/llm-finetune,bakcup/peft/tuners/prompt_tuning.py,2023-11-21T06:06:20Z,update
github.com/UCSC-VLAA/vllm-safety-benchmark,baselines/llava/model/builder.py,2023-11-24T02:09:04Z,first commit
github.com/UCSC-VLAA/vllm-safety-benchmark,baselines/llava/serve/model_worker.py,2023-11-24T02:09:04Z,first commit
github.com/UCSC-VLAA/vllm-safety-benchmark,baselines/mplug_owl2_utils/model/builder.py,2023-11-24T02:09:04Z,first commit
github.com/SqueezeAILab/KVQuant,gradients/src/transformers/trainer.py,2024-02-06T09:26:27Z,First Commit
github.com/tosiyuki/alpaca-lora-create-news-title,generate_news_title.py,2023-03-19T11:41:38Z,[add]コードなどを追加
github.com/ntunlplab/traditional-chinese-alpaca,code/finetune.py,2023-03-25T10:58:00Z,add code and data
github.com/ntunlplab/traditional-chinese-alpaca,code/inference.py,2023-03-25T10:58:00Z,add code and data
github.com/tenstorrent/tt-buda,pybuda/test/falcon/tests/falcon_modules/falcon.py,2024-02-12T13:08:05Z,"Removing t-streaming redundant config overrides from the rest of PyBuda

(cherry picked from commit 5527a0ecb7dd7045cf669dc61601503bb5ec373b)"
github.com/tenstorrent/tt-buda,pybuda/test/falcon/tests/falcon_modules/falcon.py,2024-02-01T19:21:31Z,"Snapshot of pybuda branch main, squashed from commit f955f2fdd3d4dfc1da7846ae80d10a8855988b5b
Notice regarding the squashed initial commit:

As we transition from our internal development environment to a public
repository, we are committed to ensuring the security and
confidentiality of sensitive information.

To protect against any accidental release of confidential data, we have
made a specific decision regarding the initial commit to our public
repository. We have opted for a squashed commit for the initial public
release, which consolidates all the changes from our internal history
into a single, clean commit. This step ensures that no sensitive
information from our past development is exposed to the public.

We want to emphasize that moving forward, we are fully dedicated to
maintaining a transparent and organized version history for all our
future commits and bug fixes. This approach allows us to maintain a
clean slate for our public repository while ensuring that our ongoing
work is well-documented and traceable.

We appreciate your understanding and support as we make this transition.
Our goal is to provide you with a secure and reliable open-source
project. If you have any questions or concerns, please don't hesitate to
reach out to us.

Thank you for your continued interest and participation in our project."
github.com/IDEA-XL/InstructMol,llava/model/builder.py,2023-11-27T08:01:50Z,support evaluate LLM mol-prop-pred
github.com/IDEA-XL/InstructMol,llava/model/builder.py,2023-11-07T13:19:29Z,support property prediction
github.com/tabtoyou/KoLLaVA,llava/model/builder.py,2023-12-02T18:18:52Z,Update KoLLaVA-v1.5
github.com/tabtoyou/KoLLaVA,llava/model/builder.py,2023-08-04T15:17:33Z,"Update llama-2, qlora"
github.com/HIT-SCIR-SC/QiaoBan,interact.py,2023-09-09T01:42:31Z,add interact.py
github.com/KMnO4-zx/huanhuan-chat,run/api/model.py,2023-08-23T11:06:09Z,Finish api 部署
github.com/KMnO4-zx/huanhuan-chat,run/gui/webui_demo.py,2023-08-14T10:24:13Z,Finish notebook and gui
github.com/KMnO4-zx/huanhuan-chat,run/gui/webui_demo.py,2023-08-13T05:31:06Z,调整目录架构
github.com/codefuse-ai/CodeFuse-MFT-VLM,llava/model/builder.py,2024-03-01T09:37:13Z,update
github.com/codefuse-ai/CodeFuse-MFT-VLM,llava/model/builder.py,2024-01-29T08:23:45Z,opensource
github.com/Sshuoshuo/RAG-competition,app/LLM.py,2024-01-06T08:36:22Z,add submit code
github.com/Sshuoshuo/RAG-competition,app/embeddings.py,2024-01-06T08:36:22Z,add submit code
github.com/williamliujl/LLMRec,generate.py,2023-08-19T04:34:04Z,version_0819
github.com/williamliujl/LLMRec,infer_rec.py,2023-08-19T04:34:04Z,version_0819
github.com/declare-lab/flacuna,peft_flacuna/peft_model.py,2023-07-06T10:20:57Z,Upload files
github.com/declare-lab/flacuna,peft_flacuna/peft_model.py,2023-07-06T10:17:11Z,Upload files
github.com/declare-lab/flacuna,peft_flacuna/peft_model.py,2023-07-06T10:06:21Z,Upload files
github.com/declare-lab/flacuna,fastchat/train/train_lora.py,2023-07-06T10:20:57Z,Upload files
github.com/declare-lab/flacuna,fastchat/train/train_lora.py,2023-07-06T10:17:11Z,Upload files
github.com/declare-lab/flacuna,fastchat/train/train_lora.py,2023-07-06T10:06:21Z,Upload files
github.com/declare-lab/flacuna,fastchat/model/apply_lora.py,2023-07-06T10:20:57Z,Upload files
github.com/declare-lab/flacuna,fastchat/model/apply_lora.py,2023-07-06T10:17:11Z,Upload files
github.com/declare-lab/flacuna,fastchat/model/apply_lora.py,2023-07-06T10:06:21Z,Upload files
github.com/declare-lab/flacuna,peft_flacuna/tuners/prompt_tuning.py,2023-07-06T10:20:57Z,Upload files
github.com/declare-lab/flacuna,peft_flacuna/tuners/prompt_tuning.py,2023-07-06T10:17:11Z,Upload files
github.com/declare-lab/flacuna,peft_flacuna/tuners/prompt_tuning.py,2023-07-06T10:06:21Z,Upload files
github.com/tingxueronghua/ChartLlama-code,model_vqa_lora.py,2023-11-27T11:37:26Z,add inference code
github.com/tingxueronghua/ChartLlama-code,llava/model/builder.py,2023-12-04T03:05:39Z,add full llava code
github.com/tingxueronghua/ChartLlama-code,llava/eval/model_vqa_lora.py,2023-12-04T03:05:39Z,add full llava code
github.com/l294265421/my-alpaca,alpaca_lora/generate.py,2023-04-02T03:35:40Z,automl
github.com/l294265421/my-alpaca,alpaca_lora/generate.py,2023-03-31T16:49:15Z,my alpaca colab
github.com/l294265421/my-alpaca,alpaca_lora/export_hf_checkpoint.py,2023-03-31T16:49:15Z,my alpaca colab
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_llama.py,2023-04-08T03:11:24Z,modify inference to support sampling
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_llama.py,2023-04-07T07:06:09Z,autodl
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_llama.py,2023-04-07T07:00:21Z,autodl
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_llama.py,2023-04-06T09:14:06Z,modify inference to support sampling
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_llama.py,2023-04-05T14:06:49Z,modify inference to support sampling
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_llama.py,2023-04-05T07:03:44Z,modify llama inference
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_llama.py,2023-04-05T07:02:26Z,modify llama inference
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_llama.py,2023-04-05T06:49:19Z,modify llama inference
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_llama.py,2023-04-02T07:37:35Z,autodl
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_llama.py,2023-04-02T04:12:30Z,autodl
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_llama.py,2023-04-02T04:07:49Z,autodl
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_llama.py,2023-04-02T03:40:53Z,automl
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_llama.py,2023-04-02T03:35:40Z,automl
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_alpaca_lora.py,2023-04-08T03:11:24Z,modify inference to support sampling
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_alpaca_lora.py,2023-04-07T04:52:27Z,autodl
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_alpaca_lora.py,2023-04-05T14:06:49Z,modify inference to support sampling
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_alpaca_lora.py,2023-04-02T07:37:35Z,autodl
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_alpaca_lora.py,2023-04-02T04:12:30Z,autodl
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_alpaca_lora.py,2023-04-02T04:07:49Z,autodl
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_alpaca_lora.py,2023-04-02T03:40:53Z,automl
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_alpaca_lora.py,2023-04-02T03:35:40Z,automl
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_llama_gradio.py,2023-04-06T09:14:06Z,modify inference to support sampling
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_llama_gradio.py,2023-04-05T14:06:49Z,modify inference to support sampling
github.com/l294265421/my-alpaca,alpaca_lora/export_state_dict_checkpoint.py,2023-03-31T16:49:15Z,my alpaca colab
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_alpaca_lora_gradio.py,2023-04-08T03:11:24Z,modify inference to support sampling
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_alpaca_lora_gradio.py,2023-04-07T05:15:19Z,autodl
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_alpaca_lora_gradio.py,2023-04-07T05:08:14Z,autodl
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_alpaca_lora_gradio.py,2023-04-07T05:00:00Z,autodl
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_alpaca_lora_gradio.py,2023-04-07T03:05:14Z,autodl
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_alpaca_lora_gradio.py,2023-04-06T09:14:06Z,modify inference to support sampling
github.com/l294265421/my-alpaca,my_alpaca/autodl/inference_alpaca_lora_gradio.py,2023-04-05T14:06:49Z,modify inference to support sampling
github.com/mbzuai-nlp/bactrian-x,generate.py,2023-05-12T07:08:36Z,"add bloom model, support hf args, output examples"
github.com/mbzuai-nlp/bactrian-x,generate.py,2023-05-02T17:04:13Z,"support bloom, use short template, better hyper-parameter,  bug fix"
github.com/mbzuai-nlp/bactrian-x,generate.py,2023-04-24T10:36:05Z,first plenty commit
github.com/mbzuai-nlp/bactrian-x,get_lora_model_answer.py,2023-05-12T07:08:36Z,"add bloom model, support hf args, output examples"
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-12-29T09:31:21Z,fix random seed in gpu environment
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-12-19T06:50:06Z,tweak tokenizing
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-12-15T09:27:42Z,refactoring
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-12-15T08:33:32Z,tweak
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-12-15T00:48:29Z,structure model answers
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-12-15T00:30:02Z,save config as well as result
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-12-14T08:05:41Z,use load_questions
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-12-14T07:29:49Z,declare paths in common.py
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-12-14T02:38:03Z,tweak
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-12-14T01:59:07Z,apply isort
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-28T01:38:25Z,use bfloat16 if available
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-27T05:58:44Z,specify file encoding
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-27T00:48:09Z,fix argument name
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-24T05:31:01Z,use logger instead of print
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-24T05:15:30Z,refactoring
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-22T11:00:19Z,load model configurations from file
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-22T10:11:21Z,fix postprocessing for llm-jp models
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-22T09:58:36Z,fix postprocessing for rinna models
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-22T09:24:31Z,set available GPUs via CUDA_VISIBLE_DEVICES
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-22T09:23:28Z,move argument parsing to main
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-22T09:22:17Z,remove template
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-22T09:20:35Z,simplify data saving
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-22T09:17:35Z,remove unnecessary alies
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-22T09:15:15Z,pass max_new_tokens to generate_response
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-22T09:10:54Z,remove with_prompt option as prompt will be explicitly specified in config file
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-22T09:06:23Z,move the logic to determine temperature to main
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-22T09:03:20Z,wrap only generation process with torch.no_grad
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-22T09:02:21Z,simplify data loading
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-22T08:56:01Z,remove interactive mode
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-22T08:51:26Z,enable eval mode
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-22T08:49:46Z,simplify the peft model initialization
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-22T08:47:00Z,remove print functions to report vocabulary size
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-22T08:45:24Z,remove llama-specific initialization
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-22T08:44:43Z,make tokenizer initalization clear
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-22T08:39:35Z,set appropriate dtype depending on the device
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-22T07:20:03Z,set very small temperature instead of 0 to avoid error
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-22T07:17:55Z,fix random seed
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-22T03:15:51Z,Merge branch 'dev' into refactor
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-21T05:25:07Z,fix the format
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-21T04:09:36Z,Merge remote-tracking branch 'origin/dev' into fix/useless_file
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-21T03:34:45Z,delete more useless files
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-20T07:12:08Z,fix argparse
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-20T02:59:52Z,remove unnecessary lines
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-17T07:59:07Z,fix file gen_model_answer.py
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-17T07:32:31Z,update log language
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-15T15:22:56Z,update llm-jp
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-11T09:25:39Z,support more models
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-10T08:59:03Z,update
github.com/ku-nlp/ja-vicuna-qa-benchmark,llm_judge/gen_model_answer.py,2023-11-10T08:21:59Z,optimize
github.com/FreedomIntelligence/GrammarGPT,generate.py,2023-07-25T03:17:55Z,init commit
github.com/jyLin8100/GenSAM,llava/model/builder.py,2023-12-11T21:54:53Z,GenSAMv1
github.com/google/maxdiffusion,src/maxdiffusion/models/modeling_utils.py,2024-02-16T17:59:56Z,maxdiffusion first commit
github.com/MaverickRen/PixelLM,model/llava/model/builder.py,2024-01-20T05:22:01Z,des
github.com/sehyunkwon/ICTC,step1/llava/model/builder.py,2023-10-28T01:45:27Z,First uploading
github.com/jackaduma/ChatGLM-LoRA-RLHF-PyTorch,merge_peft_adapter.py,2023-04-18T17:21:02Z,merge_peft_adapter
github.com/Dai-shen/LAiW,src/interface.py,2023-09-05T07:43:48Z,add tasks including legal_ar
github.com/Dai-shen/LAiW,src/financial-evaluation/lm_eval/models/huggingface.py,2024-01-02T08:33:50Z,update eval files
github.com/Dai-shen/LAiW,src/financial-evaluation/lm_eval/models/huggingface.py,2023-10-08T01:44:00Z,update src
github.com/Dai-shen/LAiW,src/financial-evaluation/lm_eval/models/huggingface.py,2023-09-05T07:42:33Z,add financial-evaluation
github.com/ohtaman/abci-examples,202310/src/sft.py,2023-10-22T23:26:49Z,Add 202310
github.com/ohtaman/abci-examples,202307/src/finetune.py,2023-07-06T04:47:18Z,Update finetune.py
github.com/ohtaman/abci-examples,202307/src/finetune.py,2023-07-06T01:05:45Z,black
github.com/ohtaman/abci-examples,202307/src/finetune.py,2023-07-05T19:18:07Z,update scripts
github.com/ohtaman/abci-examples,202307/src/finetune.py,2023-07-03T22:28:15Z,add finetune with lora.
github.com/ohtaman/abci-examples,202307/src/finetune.py,2023-07-03T21:29:41Z,add finetune example for 202307.
github.com/ohtaman/abci-examples,202307/src/finetune_lora.py,2023-07-06T04:47:35Z,Update finetune_lora.py
github.com/ohtaman/abci-examples,202307/src/finetune_lora.py,2023-07-06T01:05:45Z,black
github.com/ohtaman/abci-examples,202307/src/finetune_lora.py,2023-07-05T19:18:07Z,update scripts
github.com/ohtaman/abci-examples,202307/src/finetune_lora.py,2023-07-05T19:14:51Z,update script
github.com/ohtaman/abci-examples,202307/src/finetune_lora.py,2023-07-05T03:35:34Z,update
github.com/ohtaman/abci-examples,202307/src/finetune_lora.py,2023-07-03T22:28:15Z,add finetune with lora.
github.com/ohtaman/abci-examples,202310/src/finetune_lora.py,2023-10-23T01:46:56Z,Update scripts.
github.com/ohtaman/abci-examples,202310/src/finetune_lora.py,2023-10-22T23:26:49Z,Add 202310
github.com/ohtaman/abci-examples,202310/src/generate_text_lora.py,2023-10-23T07:41:08Z,Add generate_text_lora
github.com/ohtaman/abci-examples,202307/src/finetune_lora_distribute.py,2023-07-21T21:18:43Z,change to be able to save peft model even if zero3 is used
github.com/ohtaman/abci-examples,202307/src/finetune_lora_distribute.py,2023-07-08T08:44:30Z,add multinode example
github.com/ohtaman/abci-examples,202307/src/finetune_lora_distribute.py,2023-07-06T01:05:45Z,black
github.com/ohtaman/abci-examples,202307/src/finetune_lora_distribute.py,2023-07-05T19:18:07Z,update scripts
github.com/ohtaman/abci-examples,202307/src/finetune_lora_distribute.py,2023-07-05T19:14:51Z,update script
github.com/ohtaman/abci-examples,202307/src/finetune_lora_distribute.py,2023-07-05T19:02:01Z,tmp
github.com/ohtaman/abci-examples,202307/src/finetune_lora_distribute.py,2023-07-05T03:27:02Z,add deepspeed
github.com/ohtaman/abci-examples,202310/src/finetune_lora_distribute.py,2023-10-23T01:46:56Z,Update scripts.
github.com/ohtaman/abci-examples,202310/src/finetune_lora_distribute.py,2023-10-22T23:26:49Z,Add 202310
github.com/ExpertiseModel/MuTAP,llama_util/model_utils.py,2024-01-17T16:07:47Z,Add files via upload
github.com/WangRongsheng/Chinese-LLaMA-Alpaca-Usage,scripts/merge_llama_with_chinese_lora.py,2023-05-15T11:32:49Z,add merge_llama_with_chinese_lora usage
github.com/WangRongsheng/Chinese-LLaMA-Alpaca-Usage,scripts/merge_llama_with_chinese_lora.py,2023-05-15T10:14:34Z,add usage
github.com/taishan1994/qlora-chinese-LLM,chat.py,2023-05-27T08:48:47Z,commit
github.com/taishan1994/qlora-chinese-LLM,model_hub/merge_llama_with_chinese_lora.py,2023-05-27T09:56:13Z,commit
github.com/git-cloner/llama-lora-fine-tuning,generate.py,2023-07-28T15:27:31Z,fiexed generate.py
github.com/git-cloner/llama-lora-fine-tuning,generate.py,2023-06-08T15:00:29Z,add test model
github.com/songyingxin/Novel-GPT,Baichuan-Finetune-Lora/run_one.py,2023-10-09T07:48:54Z,10-9
github.com/AGI-Collective/Robin,robin/model/builder.py,2023-12-01T01:52:18Z,Updated llava to robin
github.com/j-min/VPGen,llama.py,2023-07-25T05:05:30Z,minor refactor
github.com/j-min/VPGen,llama.py,2023-05-25T00:14:56Z,Initial commit
github.com/SUFE-AIFLM-Lab/FinEval,code/evaluators/unify_evaluator.py,2023-08-13T07:21:45Z,Add files via upload
github.com/microsoft/windows-ai-studio-templates,configs/phi-2/inference/utils.py,2023-12-13T19:40:26Z,Adding base only inferencing (#87)
github.com/microsoft/windows-ai-studio-templates,configs/phi-2/inference/utils.py,2023-12-13T03:00:30Z,Remove DataParallel for preview version
github.com/microsoft/windows-ai-studio-templates,configs/phi-2/inference/utils.py,2023-12-12T17:32:22Z,Hello Templates!
github.com/microsoft/windows-ai-studio-templates,configs/phi-1_5/inference/utils.py,2023-12-13T19:40:26Z,Adding base only inferencing (#87)
github.com/microsoft/windows-ai-studio-templates,configs/phi-1_5/inference/utils.py,2023-12-13T03:00:30Z,Remove DataParallel for preview version
github.com/microsoft/windows-ai-studio-templates,configs/phi-1_5/inference/utils.py,2023-12-12T17:32:22Z,Hello Templates!
github.com/microsoft/windows-ai-studio-templates,configs/mistral-7b/inference/utils.py,2023-12-13T19:40:26Z,Adding base only inferencing (#87)
github.com/microsoft/windows-ai-studio-templates,configs/mistral-7b/inference/utils.py,2023-12-13T03:00:30Z,Remove DataParallel for preview version
github.com/microsoft/windows-ai-studio-templates,configs/mistral-7b/inference/utils.py,2023-12-12T17:32:22Z,Hello Templates!
github.com/microsoft/windows-ai-studio-templates,configs/llama-v2-7b/inference/utils.py,2023-12-13T19:40:26Z,Adding base only inferencing (#87)
github.com/microsoft/windows-ai-studio-templates,configs/llama-v2-7b/inference/utils.py,2023-12-13T03:00:30Z,Remove DataParallel for preview version
github.com/microsoft/windows-ai-studio-templates,configs/llama-v2-7b/inference/utils.py,2023-12-12T17:32:22Z,Hello Templates!
github.com/microsoft/windows-ai-studio-templates,configs/zephyr-7b-beta/inference/utils.py,2023-12-13T19:40:26Z,Adding base only inferencing (#87)
github.com/microsoft/windows-ai-studio-templates,configs/zephyr-7b-beta/inference/utils.py,2023-12-13T03:00:30Z,Remove DataParallel for preview version
github.com/microsoft/windows-ai-studio-templates,configs/zephyr-7b-beta/inference/utils.py,2023-12-12T17:32:22Z,Hello Templates!
github.com/079035/WizardLM-mirror,training/src/generate.py,2023-10-10T20:26:49Z,create WizardLM
github.com/079035/WizardLM-mirror,WizardLM/src/infer_wizardlm13b.py,2023-10-10T20:26:49Z,create WizardLM
github.com/SqueezeAILab/KVQuant,gradients/src/transformers/modeling_utils.py,2024-02-06T09:26:27Z,First Commit
github.com/SqueezeAILab/KVQuant,gradients/src/transformers/integrations/peft.py,2024-02-06T09:26:27Z,First Commit
github.com/SqueezeAILab/KVQuant,deployment/transformers/src/transformers/trainer.py,2024-02-16T20:56:11Z,Deployment Support
github.com/SqueezeAILab/KVQuant,deployment/transformers/src/transformers/modeling_utils.py,2024-02-16T20:56:11Z,Deployment Support
github.com/SqueezeAILab/KVQuant,deployment/transformers/src/transformers/integrations/peft.py,2024-02-16T20:56:11Z,Deployment Support
github.com/Ascend/ModelZoo-PyTorch,PyTorch/built-in/foundation/FlagAI/flagai/model/base_model.py,2023-08-18T02:02:56Z,"!5367 [自研][PyTorch][FlagAI Aquila Model]初次提交
* original FlagAI Aquila"
github.com/Ascend/ModelZoo-PyTorch,PyTorch/built-in/foundation/LLaMA-13B/fastchat/model/apply_lora.py,2023-11-24T07:15:55Z,"!5865 更新新版本LLaMA-13B(fsdp训练)
Merge pull request !5865 from zhangbin/master"
github.com/Ascend/ModelZoo-PyTorch,PyTorch/built-in/foundation/LLaMA-13B/transformers_modify/trainer.py,2023-11-24T07:15:55Z,"!5865 更新新版本LLaMA-13B(fsdp训练)
Merge pull request !5865 from zhangbin/master"
github.com/Ascend/ModelZoo-PyTorch,PyTorch/built-in/foundation/LLaMA-13B/transformers_modify/trainer.py,2023-08-25T09:27:31Z,"fix bugs while doing inference, update README.md"
github.com/Ascend/ModelZoo-PyTorch,PyTorch/built-in/foundation/LLaMA-13B/transformers_modify/trainer.py,2023-07-05T06:48:30Z,Add modifications to adapt NPU devices
github.com/Ascend/ModelZoo-PyTorch,PyTorch/built-in/foundation/LLaMA-13B/fastchat/model/model_adapter.py,2023-11-24T07:15:55Z,"!5865 更新新版本LLaMA-13B(fsdp训练)
Merge pull request !5865 from zhangbin/master"
github.com/Ascend/ModelZoo-PyTorch,PyTorch/built-in/foundation/FlagAI/flagai/model/tools/peft/peft_model.py,2023-08-18T02:02:56Z,"!5367 [自研][PyTorch][FlagAI Aquila Model]初次提交
* original FlagAI Aquila"
github.com/Ascend/ModelZoo-PyTorch,PyTorch/built-in/foundation/FlagAI/flagai/model/tools/peft/tuners/prompt_tuning.py,2023-08-18T02:02:56Z,"!5367 [自研][PyTorch][FlagAI Aquila Model]初次提交
* original FlagAI Aquila"
github.com/adamkarvonen/chess_gpt_eval,llama/llama_module.py,2024-01-21T15:06:26Z,Move llama_module into separate folder
github.com/anyscale/endpoint-cookbook,App_FireAct/models/llama.py,2023-10-27T03:33:24Z,FireAct
github.com/suu990901/LLaMA-InfoEntropy-Loss,transformers/trainer.py,2023-11-04T09:18:48Z,init
github.com/suu990901/LLaMA-InfoEntropy-Loss,lm-evaluation-harness/lm_eval/models/huggingface.py,2023-11-04T09:18:48Z,init
github.com/ankur56/ChemLoRA,old_scripts/peft_test1_g4mp2.py,2023-04-19T00:40:35Z,moved files
github.com/ankur56/ChemLoRA,old_scripts/peft_test1_b3lyp.py,2023-04-19T00:40:35Z,moved files
github.com/yangjianxin1/LongQLoRA,component/utils.py,2023-10-26T16:27:01Z,update loading model
github.com/yangjianxin1/LongQLoRA,component/utils.py,2023-10-26T16:21:56Z,add evaluation
github.com/yangjianxin1/LongQLoRA,component/utils.py,2023-10-24T03:29:42Z,init components
github.com/yangjianxin1/LongQLoRA,script/merge_lora.py,2023-11-07T10:49:00Z,update merge_lora.py
github.com/yangjianxin1/LongQLoRA,script/merge_lora.py,2023-10-26T16:37:07Z,merge model params
github.com/Gentopia-AI/Gentopia,gentopia/llm/loaders/bloom.py,2023-06-16T14:32:36Z,"Refine llm.loaders (#12)

* Refine llm.loaders

* fix bug

* fix bug

---------

Co-authored-by: Satan <liuxk2019@mail.sustech.edu.cn>"
github.com/Gentopia-AI/Gentopia,gentopia/llm/loaders/bloom.py,2023-06-16T08:15:02Z,init
github.com/Gentopia-AI/Gentopia,gentopia/llm/loaders/bloom.py,2023-06-16T00:43:44Z,init
github.com/Gentopia-AI/Gentopia,gentopia/llm/loaders/baize.py,2023-06-16T14:32:36Z,"Refine llm.loaders (#12)

* Refine llm.loaders

* fix bug

* fix bug

---------

Co-authored-by: Satan <liuxk2019@mail.sustech.edu.cn>"
github.com/Gentopia-AI/Gentopia,gentopia/llm/loaders/baize.py,2023-06-16T08:15:02Z,init
github.com/Gentopia-AI/Gentopia,gentopia/llm/loaders/baize.py,2023-06-16T00:43:44Z,init
github.com/Gentopia-AI/Gentopia,gentopia/llm/loaders/alpaca.py,2023-06-16T14:32:36Z,"Refine llm.loaders (#12)

* Refine llm.loaders

* fix bug

* fix bug

---------

Co-authored-by: Satan <liuxk2019@mail.sustech.edu.cn>"
github.com/Gentopia-AI/Gentopia,gentopia/llm/loaders/alpaca.py,2023-06-16T08:15:02Z,init
github.com/Gentopia-AI/Gentopia,gentopia/llm/loaders/alpaca.py,2023-06-16T00:43:44Z,init
github.com/Gentopia-AI/Gentopia,gentopia/llm/loaders/replit.py,2023-06-16T08:15:02Z,init
github.com/Gentopia-AI/Gentopia,gentopia/llm/loaders/replit.py,2023-06-16T00:43:44Z,init
github.com/Gentopia-AI/Gentopia,gentopia/llm/loaders/guanaco.py,2023-06-16T14:32:36Z,"Refine llm.loaders (#12)

* Refine llm.loaders

* fix bug

* fix bug

---------

Co-authored-by: Satan <liuxk2019@mail.sustech.edu.cn>"
github.com/Gentopia-AI/Gentopia,gentopia/llm/loaders/guanaco.py,2023-06-16T08:15:02Z,init
github.com/Gentopia-AI/Gentopia,gentopia/llm/loaders/guanaco.py,2023-06-16T00:43:44Z,init
github.com/HillZhang1999/ICD,transformers/src/transformers/trainer.py,2023-12-23T08:01:08Z,add transformers
github.com/HillZhang1999/ICD,transformers/src/transformers/modeling_utils.py,2023-12-23T08:01:08Z,add transformers
github.com/HillZhang1999/ICD,transformers/src/transformers/integrations/peft.py,2023-12-23T08:01:08Z,add transformers
github.com/clcarwin/alpaca-weight,generate_llama.py,2023-03-15T13:03:03Z,init
github.com/clcarwin/alpaca-weight,generate_alpaca.py,2023-03-15T13:03:03Z,init
github.com/clcarwin/alpaca-weight,merge_step1_lora2patch.py,2023-06-13T07:52:05Z,Updates transformers library and fixes missing Seq2SeqTrainerArguments
github.com/clcarwin/alpaca-weight,merge_step1_lora2patch.py,2023-03-15T13:03:03Z,init
github.com/clcarwin/alpaca-weight,generate_llama_with_lora.py,2023-03-15T13:03:03Z,init
github.com/clcarwin/alpaca-weight,merge_step2_patch2alpaca.py,2023-06-13T07:52:05Z,Updates transformers library and fixes missing Seq2SeqTrainerArguments
github.com/clcarwin/alpaca-weight,merge_step2_patch2alpaca.py,2023-03-15T13:03:03Z,init
github.com/AlpinDale/RPTQ-for-LLaMA,lm-evaluation/lm_eval/models/huggingface.py,2023-05-18T22:27:34Z,moved from main
github.com/AlpinDale/RPTQ-for-LLaMA,lm_evaluation/lm_eval/models/huggingface.py,2023-05-04T10:02:06Z,added the repo as a fork
github.com/AlpinDale/RPTQ-for-LLaMA,lm_evaluation/lm_eval/models/huggingface.py,2023-04-03T04:02:17Z,code release
github.com/replicate/cog-llama,predict.py,2023-05-23T22:34:37Z,"Lora llama deepspeed (#12)

* working tensorized peft training & prediction

* I have found and slain the memory leak"
github.com/replicate/cog-llama,predict.py,2023-04-16T19:31:33Z,"Update tensorize (#8)

* working fast tensorization + testing code for posterity"
github.com/replicate/cog-llama,predict.py,2023-04-08T01:06:05Z,a few quality of life modifications
github.com/replicate/cog-llama,predict.py,2023-04-07T23:14:21Z,whitespace
github.com/replicate/cog-llama,predict.py,2023-04-07T23:05:15Z,fixing default
github.com/replicate/cog-llama,predict.py,2023-04-07T22:50:48Z,working tensorized llama training
github.com/replicate/cog-llama,predict.py,2023-04-07T00:24:57Z,working llama fine-tune
github.com/replicate/cog-llama,predict.py,2023-04-06T03:34:56Z,actually working multi-gpu training
github.com/replicate/cog-llama,predict.py,2023-04-05T20:08:04Z,fixed streaming
github.com/replicate/cog-llama,predict.py,2023-04-05T05:14:03Z,black & isort
github.com/replicate/cog-llama,predict.py,2023-04-05T05:10:04Z,tensorizer test
github.com/replicate/cog-llama,predict.py,2023-04-05T04:54:31Z,use builtin function to remove </s>
github.com/replicate/cog-llama,predict.py,2023-04-05T04:54:31Z,"porting daanelson output streaming work

- this is a port from @daanelson work in replicate/cog-llama to T5
https://github.com/replicate/cog-llama/pull/4/files

This patch is for base weights, not tensorized weights"
github.com/replicate/cog-llama,predict.py,2023-04-04T06:10:51Z,working multi-gpu training
github.com/replicate/cog-llama,predict.py,2023-04-03T00:10:03Z,reworked template
github.com/replicate/cog-llama,predict.py,2023-04-02T20:31:26Z,now with ignoring
github.com/replicate/cog-llama,predict.py,2023-03-31T23:54:52Z,refactoring to use cog weights
github.com/replicate/cog-llama,predict.py,2023-03-31T19:46:26Z,hopeful fix to path issues
github.com/replicate/cog-llama,predict.py,2023-03-31T00:31:54Z,fixed predict
github.com/replicate/cog-llama,predict.py,2023-03-31T00:30:07Z,working tensorization
github.com/replicate/cog-llama,predict.py,2023-03-30T23:45:50Z,initial attempt at tensorization
github.com/replicate/cog-llama,predict.py,2023-03-30T21:23:53Z,prepping for tensorization of model
github.com/replicate/cog-llama,predict.py,2023-03-30T18:22:07Z,working train
github.com/replicate/cog-llama,predict.py,2023-03-30T05:56:40Z,working generic train and predict scripts
github.com/replicate/cog-llama,predict.py,2023-03-29T22:58:32Z,"working, scrappy train -> predict handoff"
github.com/replicate/cog-llama,predict.py,2023-03-29T22:00:09Z,wip reformatting
github.com/replicate/cog-llama,predict.py,2023-03-28T22:02:19Z,working training
github.com/replicate/cog-llama,predict.py,2023-03-27T22:33:33Z,not quite working
github.com/Manuel030/alpaca-opt,inference.py,2023-04-06T17:29:11Z,prepare readme
github.com/Manuel030/alpaca-opt,inference.py,2023-04-06T16:52:11Z,use 6.7 version
github.com/Manuel030/alpaca-opt,inference.py,2023-04-04T16:12:49Z,add cpu support and weight mixin
github.com/Manuel030/alpaca-opt,inference.py,2023-04-03T15:37:36Z,change inference parameters
github.com/Manuel030/alpaca-opt,inference.py,2023-03-26T19:40:20Z,run training for opt 6.7b
github.com/Manuel030/alpaca-opt,inference.py,2023-03-16T16:49:04Z,gitignore model outputs
github.com/Manuel030/alpaca-opt,mixin_lora_weights.py,2023-04-06T17:29:11Z,prepare readme
github.com/Manuel030/alpaca-opt,mixin_lora_weights.py,2023-04-06T16:52:11Z,use 6.7 version
github.com/Manuel030/alpaca-opt,mixin_lora_weights.py,2023-04-06T15:55:42Z,support push tu hub
github.com/Manuel030/alpaca-opt,mixin_lora_weights.py,2023-04-04T16:12:49Z,add cpu support and weight mixin
github.com/intel/neural-speed,scripts/huggingface.py,2024-02-05T02:12:09Z,add autoround and remove name in path (#112)
github.com/intel/neural-speed,scripts/huggingface.py,2024-02-04T08:38:52Z,enable pre-commit CI (#113)
github.com/intel/neural-speed,scripts/huggingface.py,2024-01-19T02:30:12Z,miagrate pr: Calculate accuracy of runtime (#46)
github.com/intel/neural-speed,scripts/load_peft_and_merge.py,2023-12-27T06:39:56Z,Update python API and reorg scripts (#16)
github.com/xiaol/Huggingface-RWKV-World,generate_hf_cfg.py,2023-07-15T18:08:01Z,fix equation
github.com/xiaol/Huggingface-RWKV-World,generate_hf_cfg.py,2023-07-15T17:13:52Z,update
github.com/xiaol/Huggingface-RWKV-World,generate_hf_cfg.py,2023-07-15T15:10:02Z,update
github.com/xiaol/Huggingface-RWKV-World,generate_hf_cfg.py,2023-07-15T12:57:58Z,update
github.com/xiaol/Huggingface-RWKV-World,generate_hf_cfg.py,2023-07-15T11:48:24Z,fix bugs
github.com/eval4nlp/SharedTask2023,baselines/model_dict.py,2023-08-28T10:41:06Z,add additional model selection
github.com/eval4nlp/SharedTask2023,baselines/model_dict.py,2023-08-21T15:11:57Z,base_name property no longer required for TheBloke models
github.com/eval4nlp/SharedTask2023,baselines/model_dict.py,2023-08-07T20:55:42Z,tested baselines
github.com/eval4nlp/SharedTask2023,baselines/model_dict.py,2023-08-07T17:52:40Z,adding another baseline
github.com/DataTunerX/datatunerx,cmd/tuning/train.py,2024-02-05T03:42:24Z,"fix(datatunerx): fix go.mod

change finetune-experiment-controller to datatunerx"
github.com/yao8839836/kg-llm,lora_infer_wn11.py,2023-06-24T15:19:01Z,update readme
github.com/yao8839836/kg-llm,lora_infer_wn11.py,2023-06-24T15:07:47Z,update readme
github.com/yao8839836/kg-llm,lora_infer_wn11.py,2023-06-24T14:53:42Z,add Readme
github.com/yao8839836/kg-llm,lora_infer_yago_rel.py,2023-06-24T15:19:01Z,update readme
github.com/yao8839836/kg-llm,lora_infer_yago_rel.py,2023-06-24T14:53:42Z,add Readme
github.com/eth-easl/fmengine,cli/generate.py,2023-12-12T09:44:08Z,formatting
github.com/eth-easl/fmengine,cli/generate.py,2023-12-11T20:40:42Z,"Feat/usability (#24)

* add utils to upload model to HF

* minor

* add peft utils

* lora on all layers

* minor

* fix lora converting

* feat: converts all steps

* minor

* initial peft cli

* peft environments

* minor

* minor"
github.com/Actable-AI/llm-utils,qlora2ct2/convert_qlora2_ct2.py,2023-06-27T07:15:03Z,add convert code
github.com/Actable-AI/llm-utils,qlora2ct2/convert_qlora2_ct2.py,2023-06-27T07:14:10Z,re commit
github.com/Actable-AI/llm-utils,qlora2ct2/convert_qlora2_ct2.py,2023-06-27T07:00:47Z,push convert qlora to ct2 code
github.com/Actable-AI/llm-utils,qlora2ct2/create_dummy_qlora_model.py,2023-06-27T07:15:03Z,add convert code
github.com/Actable-AI/llm-utils,qlora2ct2/create_dummy_qlora_model.py,2023-06-27T07:14:10Z,re commit
github.com/Actable-AI/llm-utils,qlora2ct2/create_dummy_qlora_model.py,2023-06-27T07:00:47Z,push convert qlora to ct2 code
github.com/wegodev2/virtual-prompt-injection,code_injection/evaluation/modeling.py,2023-10-18T21:18:54Z,Initial commit
github.com/camenduru/Video-LLaVA-hf,llava/model/builder.py,2023-11-18T05:51:50Z,m
github.com/hengjiUSTC/learn-llm,merge_lora.py,2024-01-12T09:05:14Z,add dpo
github.com/hengjiUSTC/learn-llm,merge_lora.py,2024-01-09T14:52:44Z,update tokenzier and model loader
github.com/hengjiUSTC/learn-llm,merge_lora.py,2024-01-09T09:55:21Z,fix inference and merge bugs
github.com/hengjiUSTC/learn-llm,merge_lora.py,2024-01-08T09:36:53Z,update merge for tokenizer
github.com/hengjiUSTC/learn-llm,merge_lora.py,2024-01-05T03:20:41Z,add lora merge and inference
github.com/valohai/mistral-example,inference-mistral.py,2023-11-20T12:08:49Z,"Prepare for public (#5)

* fix dataprep error / prompt prep in inference

* undo changes in inference parser

* Update data-preprocess self.data_path argument

* Add os.path.dirname to data_path

* data-preprocess: change load dataset

* data-preprocess: change the prompt mr->target sentence

* finetune-mistral: change hyperparams for trainer

* inference-mistral: Make the whole prompt as parameter / Add postproc method

* valohai.yaml: update environment for trial users

* inference-mistral: delete duplication

* Fix lint error

---------

Co-authored-by: sofiacharnota <sofiacharnota@gmail.com>
Co-authored-by: Aarni Koskela <akx@iki.fi>"
github.com/valohai/mistral-example,inference-mistral.py,2023-11-07T22:21:28Z,"Additional fixes (#4)


Co-authored-by: sofiacharnota <sofiacharnota@gmail.com>"
github.com/valohai/mistral-example,inference-mistral.py,2023-11-06T11:37:18Z,inference-mistral: clean up and add some logging
github.com/valohai/mistral-example,inference-mistral.py,2023-11-06T11:37:18Z,inference-mistral: remove unused code
github.com/valohai/mistral-example,inference-mistral.py,2023-11-06T11:37:18Z,Wrap entry points in functions to avoid leaky globals
github.com/valohai/mistral-example,inference-mistral.py,2023-11-06T10:55:42Z,"Apply code style, again"
github.com/valohai/mistral-example,inference-mistral.py,2023-11-02T19:00:31Z,Prepare for public use / Test new docker image
github.com/valohai/mistral-example,inference-mistral.py,2023-11-02T10:15:02Z,Apply code style
github.com/valohai/mistral-example,inference-mistral.py,2023-10-31T14:34:35Z,Add readme \ refactoring
github.com/valohai/mistral-example,inference-mistral.py,2023-10-31T11:16:12Z,minor
github.com/FartyPants/Playground,script.py,2024-01-05T05:28:42Z,"Update script.py

removed exllama v1"
github.com/FartyPants/Playground,script.py,2023-12-24T04:10:47Z,"PEFT unload ? Does it actually work? Nope

I'm pretty sure something is broken in PEFT unload because it keeps the adapters in memory"
github.com/FartyPants/Playground,script.py,2023-10-24T03:07:09Z,Add files via upload
github.com/FartyPants/Playground,script.py,2023-10-09T17:35:21Z,Add files via upload
github.com/FartyPants/Playground,script.py,2023-10-06T05:29:41Z,Add files via upload
github.com/FartyPants/Playground,script.py,2023-09-29T20:49:11Z,Add files via upload
github.com/FartyPants/Playground,script.py,2023-09-19T21:24:27Z,Add files via upload
github.com/FartyPants/Playground,script.py,2023-09-18T06:53:35Z,Add files via upload
github.com/FartyPants/Playground,script.py,2023-08-27T14:40:11Z,Update script.py
github.com/FartyPants/Playground,script.py,2023-07-06T01:44:42Z,Added token view
github.com/FartyPants/Playground,script.py,2023-07-03T07:13:36Z,Add files via upload
github.com/FartyPants/Playground,script.py,2023-07-03T01:33:07Z,Add files via upload
github.com/FartyPants/Playground,script.py,2023-06-26T01:59:15Z,ABility to save note to Lora
github.com/FartyPants/Playground,script.py,2023-06-25T20:09:13Z,More fixes after last update
github.com/FartyPants/Playground,script.py,2023-06-25T19:06:50Z,Keeping up with oobabooga changes
github.com/FartyPants/Playground,script.py,2023-06-23T18:51:55Z,Added ability to scale LORA
github.com/FartyPants/Playground,script.py,2023-06-21T04:20:31Z,minor error
github.com/FartyPants/Playground,script.py,2023-06-20T18:25:15Z,Add files via upload
github.com/FartyPants/Playground,script.py,2023-06-14T17:53:49Z,Added LORA switch
github.com/FartyPants/Playground,script.py,2023-06-14T15:31:32Z,Add files via upload
github.com/FartyPants/Playground,script.py,2023-06-14T07:54:13Z,Perma Memory and Memory Limit
github.com/FartyPants/Playground,script.py,2023-06-10T02:30:02Z,"a few changes, OpenAssistant prompt added"
github.com/FartyPants/Playground,script.py,2023-06-09T19:52:20Z,The correct playground file
github.com/FartyPants/Playground,script.py,2023-06-09T06:58:06Z,Added generate from selection
github.com/FartyPants/Playground,script.py,2023-06-07T20:48:40Z,Add files via upload
github.com/FartyPants/Playground,script.py,2023-06-07T19:11:14Z,Add files via upload
github.com/poteminr/instruct-ner,instruction_ner/inference_instruct.py,2024-02-13T17:58:18Z,"refactoring, add metrics for extended instructions"
github.com/poteminr/instruct-ner,instruction_ner/inference_instruct.py,2024-02-09T13:28:23Z,"refactoring, add extended instructions for conll2003 rudrec"
github.com/poteminr/instruct-ner,instruction_ner/inference_instruct.py,2024-02-04T15:50:05Z,refactoring + add RWKV-LM (HF)
github.com/poteminr/instruct-ner,instruction_ner/inference_instruct.py,2024-01-07T14:10:31Z,add calculations of overall metrics
github.com/poteminr/instruct-ner,instruction_ner/inference_instruct.py,2023-12-27T18:16:06Z,update inferece script for multiconer2023
github.com/poteminr/instruct-ner,instruction_ner/inference_instruct.py,2023-12-23T18:05:07Z,add mistral to inference
github.com/poteminr/instruct-ner,instruction_ner/inference_instruct.py,2023-09-28T06:21:04Z,add conll2003 dataset
github.com/poteminr/instruct-ner,instruction_ner/inference_instruct.py,2023-09-18T17:12:48Z,add batch_size param for inference
github.com/poteminr/instruct-ner,instruction_ner/inference_instruct.py,2023-08-16T14:19:04Z,extend inference for new dataset
github.com/poteminr/instruct-ner,instruction_ner/inference_instruct.py,2023-08-15T16:03:41Z,fix inference scripts (temporarily only for rudrec dataset)
github.com/poteminr/instruct-ner,instruction_ner/inference_instruct.py,2023-08-13T12:12:35Z,fix import paths
github.com/poteminr/instruct-ner,instruction_ner/inference_instruct.py,2023-07-26T07:52:58Z,"huge refactoring according to google pyguide, add compute metric for trainer"
github.com/poteminr/instruct-ner,instruction_ner/inference_instruct.py,2023-07-22T16:01:39Z,add llama_cpp inference script
github.com/poteminr/instruct-ner,instruction_ner/inference_instruct.py,2023-07-22T09:55:22Z,refactor dataset and increase speed of llm inference
github.com/poteminr/instruct-ner,instruction_ner/inference_instruct.py,2023-07-21T16:24:02Z,small refactoring
github.com/poteminr/instruct-ner,instruction_ner/inference_instruct.py,2023-07-20T16:09:53Z,"add code for f1 metric, update inference"
github.com/poteminr/instruct-ner,instruction_ner/inference_instruct.py,2023-07-20T10:25:47Z,add extract_classes from string for f1 metric calculation
github.com/poteminr/instruct-ner,instruction_ner/inference_instruct.py,2023-07-20T09:48:25Z,update inference script and fix bug with max_instances
github.com/YerayL/FinChina-SA,src/inference.py,2023-08-12T06:04:28Z,Add files via upload
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-11-24T06:10:52Z,"Refactoring Trainer, adds `save_only_model` arg and simplifying FSDP integration (#27652)

* add code changes

1. Refactor FSDP
2. Add `--save_only_model` option: When checkpointing, whether to only save the model, or also the optimizer, scheduler & rng state.
3. Bump up the minimum `accelerate` version to `0.21.0`

* quality

* fix quality?

* Revert ""fix quality?""

This reverts commit 149330a6abc078827be274db84c8a2d26a76eba1.

* fix fsdp doc strings

* fix quality

* Update src/transformers/training_args.py

Co-authored-by: Zach Mueller <muellerzr@gmail.com>

* please fix the quality issue 😅

* Apply suggestions from code review

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* address comment

* simplify conditional check as per the comment

* update documentation

---------

Co-authored-by: Zach Mueller <muellerzr@gmail.com>
Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-11-21T16:09:35Z,"remove the deprecated method `init_git_repo` (#27617)

* remove deprecated method `init_git_repo`

* make style"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-11-14T20:31:04Z,"Track the number of tokens seen to metrics (#27274)

* Add tokens seen

* Address comments, add to TrainingArgs

* Update log

* Apply suggestions from code review

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Use self.args

* Fix docstring

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

---------

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-11-14T19:54:44Z,"Have seq2seq just use gather (#27025)

* Have seq2seq just use gather

* Change

* Reset after

* Make slow

* Apply suggestions from code review

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Clean

* Simplify and just use gather

* Update tests/trainer/test_trainer_seq2seq.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* gather always for seq2seq

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>
Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-11-07T21:40:00Z,"Allow scheduler parameters (#26480)

* Allow for scheduler kwargs

* Formatting

* Arguments checks, passing the tests

* Black failed somehow

---------

Co-authored-by: Pierre <pierre@avatarin.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-11-02T20:08:03Z,"Fixed base model class name extraction from PeftModels (#27162)

* Fixed base model class name extraction from PeftModels

* Changes to first unwrap the model then extract the base model name

* Changed base_model to base_model.model to stay consistent with peft model abstractions"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-11-02T10:27:13Z,"Reproducible checkpoint for npu (#27208)

* save NPU's RNG states when saving a checkpoint and set after all the
data skip phase when resuming training.

* re-trigger ci

* re-trigger ci"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-11-01T18:42:38Z,"Enable split_batches through TrainingArguments (#26798)

* Enable split_batches through TrainingArguments

* Extra dispatch_batches

* Keep as default false

* Add to docstring

* Add to docstring

* Remove the capturewarnings change

* Comma"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-10-31T15:03:59Z,"[FEAT] Add Neftune into transformers Trainer (#27141)

* add v1 neftune

* use `unwrap_model` instead

* add test + docs

* Apply suggestions from code review

Co-authored-by: Zach Mueller <muellerzr@gmail.com>

* more details

* fixup

* Update docs/source/en/main_classes/trainer.md

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* refactor a bit

* more elaborated test

* fix unwrap issue

---------

Co-authored-by: Zach Mueller <muellerzr@gmail.com>
Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-10-30T11:55:03Z,"remove the obsolete code related to fairscale FSDP (#26651)

* remove the obsolete code related to fairscale FSDP

* apple review suggestion"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-10-30T11:41:48Z,"[`Trainer` / `GC`] Add `gradient_checkpointing_kwargs` in trainer and training arguments (#27068)

* add `gradient_checkpointing_kwargs` in trainer and training arguments

* add comment

* add test - currently failing

* now tests pass"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-10-26T16:13:19Z,"Save TB logs as part of push_to_hub (#27022)

* Support runs/

* Upload runs folder as part of push to hub

* Add a test

* Add to test deps

* Update with proposed solution from Slack

* Ensure that repo gets deleted in tests"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-10-26T15:46:17Z,"Correct docstrings and a typo in comments (#27047)

* docs(training_args): correct docstrings

Correct docstrings of these methods in `TrainingArguments`:

- `set_save`
- `set_logging`

* docs(training_args): adjust words in docstrings

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* docs(trainer): correct a typo in comments

---------

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-10-26T09:20:11Z,"Bring back `set_epoch` for Accelerate-based dataloaders (#26850)

* Working tests!

* Fix sampler

* Fix

* Update src/transformers/trainer.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Fix check

* Clean

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-10-16T13:29:47Z,"fix resume_from_checkpoint bug (#26739)

* fix resume_from_checkpoint bug

* update code"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-10-12T08:28:40Z,"Add many missing spaces in adjacent strings (#26751)

Add missing spaces in adjacent strings"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-10-06T14:03:11Z,"remove SharedDDP as it is deprecated (#25702)

* remove SharedDDP as it was drepracated

* apply review suggestion

* make style

* Oops,forgot to remove the compute_loss context manager in Seq2SeqTrainer.

* remove the unnecessary conditional statement

* keep the logic of IPEX

* clean code

* mix precision setup & make fixup

---------

Co-authored-by: statelesshz <jihuazhong1@huawei.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-10-04T13:13:37Z,"Docstring check (#26052)

* Fix number of minimal calls to the Hub with peft integration

* Alternate design

* And this way?

* Revert

* Nits to fix

* Add util

* Print when changes are made

* Add list to ignore

* Add more rules

* Manual fixes

* deal with kwargs

* deal with enum defaults

* avoid many digits for floats

* Manual fixes

* Fix regex

* Fix regex

* Auto fix

* Style

* Apply script

* Add ignored list

* Add check that templates are filled

* Adding to CI checks

* Add back semi-fix

* Ignore more objects

* More auto-fixes

* Ignore missing objects

* Remove temp semi-fix

* Fixes

* Update src/transformers/models/pvt/configuration_pvt.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update utils/check_docstrings.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update src/transformers/utils/quantization_config.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Deal with float defaults

* Fix small defaults

* Address review comment

* Treat

* Post-rebase cleanup

* Address review comment

* Update src/transformers/models/deprecated/mctct/configuration_mctct.py

Co-authored-by: Lysandre Debut <lysandre.debut@reseau.eseo.fr>

* Address review comment

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre.debut@reseau.eseo.fr>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-10-04T12:57:11Z,Extend Trainer to enable Ascend NPU to use the fused Adamw optimizer when training (#26194)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-09-26T17:27:09Z,"Add torch `RMSProp` optimizer (#26425)

add rmsprop"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-09-22T01:33:29Z,"[QUICK FIX LINK] Update trainer.py (#26293)

* Update trainer.py

Fix link

* Update src/transformers/trainer.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update trainer.py

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-09-20T15:38:59Z,"[`Trainer`] Refactor trainer + bnb logic (#26248)

* refactor trainer + bnb logic

* remove logger.info

* oops"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-09-20T04:56:16Z,"FSDP tests and checkpointing fixes (#26180)

* add fsdp tests

* Update test_fsdp.py

* Update test_fsdp.py

* fixes

* checks

* Update trainer.py

* fix

* fixes for saving/resuming checkpoints

* fixes

* add tests and delete debug statements

* fixing tests

* Update test_fsdp.py

* fix tests

* fix tests

* minor nits

* fix code style and quality

* refactor and modularize test code

* reduce the time of tests

* reduce the test time

* fix test

* reduce test time

* reduce test time

* fix failing tests

* fix

* Apply suggestions from code review

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* resolve comments

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-09-18T15:40:11Z,refactor decay_parameters production into its own function (#26152)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-09-14T09:57:47Z,"Fix eval accumulation when `accelerate` > 0.20.3 (#26060)

As mentioned in: https://github.com/huggingface/transformers/issues/25641

Eval accumulation will never happen with `accelerate > 0.20.3`, so this change ensures that `sync_gradients` is ignored if accelerate is > 0.20.3"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-09-12T17:01:22Z,"enable optuna multi-objectives feature (#25969)

* enable optuna multi-objectives feature

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

* Apply suggestions from code review

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* update hpo doc

* update docstring

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

* extend direction to List[str] type

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

* Update src/transformers/integrations/integration_utils.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

---------

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>
Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>
Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-09-11T11:56:36Z,"only main process should call _save on deepspeed zero3 (#25959)

only main process should call _save when deepspeed zero3"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-09-07T19:00:22Z,"Try to fix training Loss inconsistent after resume from old checkpoint (#25872)

* fix loss inconsistent after resume  #25340

* fix typo

* clean code

* reformatted code

* adjust code according to comments

* adjust check_dataloader_randomsampler location

* return sampler only

* handle sampler is None

* Update src/transformers/trainer_pt_utils.py

thanks @amyeroberts

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

---------

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-09-07T16:17:30Z,"Add `tgs` speed metrics (#25858)

* Add tgs metrics

* bugfix and black formatting

* workaround for tokens counting

* formating and bugfix

* Fix

* Add opt-in for tgs metrics

* make style and fix error

* Fix doc

* fix docbuild

* hf-doc-build

* fix

* test

* Update src/transformers/training_args.py

renaming

Co-authored-by: Zach Mueller <muellerzr@gmail.com>

* Update src/transformers/training_args.py

renaming

Co-authored-by: Zach Mueller <muellerzr@gmail.com>

* Fix some symbol

* test

* Update src/transformers/trainer_utils.py

match nameing patterns

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/training_args.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/trainer.py

nice

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Fix reviews

* Fix

* Fix black

---------

Co-authored-by: Zach Mueller <muellerzr@gmail.com>
Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-09-07T04:22:53Z,"Fix err with FSDP (#25991)

* Fix err

* Use version check"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-09-05T17:01:20Z,"deepspeed resume from ckpt fixes and adding support for deepspeed optimizer and HF scheduler (#25863)

* Add support for deepspeed optimizer and HF scheduler

* fix bug

* fix the import

* fix issue with deepspeed scheduler saving for hf optim + hf scheduler scenario

* fix loading of hf scheduler when loading deepspeed checkpoint

* fix import of `DeepSpeedSchedulerWrapper`

* add tests

* add the comment and skip the failing tests

* address comment"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-09-01T15:24:12Z,"Revert frozen training arguments (#25903)

* Revert frozen training arguments

* TODO"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-09-01T09:50:42Z,"fix FSDP model resume optimizer & scheduler (#25852)

* fix FSDP resume optimizer & scheduler

* improve trainer code quality

---------

Co-authored-by: machi04 <machi04@meituan.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-08-31T09:47:53Z,"fix ds z3 checkpointing when  `stage3_gather_16bit_weights_on_model_save=False` (#25817)

* fix ds z3 checkpointing when  `stage3_gather_16bit_weights_on_model_save=False`

* refactoring"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-08-29T14:06:41Z,"Error with checking args.eval_accumulation_steps to gather tensors (#25819)

* Update trainer.py (error with checking steps in args.eval_accumulation_steps to gather tensors)

While the deprecated code has the correct check (line 3772): 
""if args.eval_accumulation_steps is not None and (step + 1) % args.eval_accumulation_steps == 0:""

The current code does not (line 3196):
""if args.eval_accumulation_steps is not None and self.accelerator.sync_gradients:""

We need to check ""(step + 1) % args.eval_accumulation_steps == 0"". Hence, the line 3196 should be modified to:
""if args.eval_accumulation_steps is not None and (step + 1) % args.eval_accumulation_steps == 0 and self.accelerator.sync_gradients:""

* Fix error with checking args.eval_accumulation_steps to gather tensors"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-08-29T07:22:14Z,"Arde/fsdp activation checkpointing (#25771)

* add FSDP config option to enable activation-checkpointing

* update docs

* add checks and remove redundant code

* fix formatting error"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-08-25T15:13:34Z,"🚨🚨🚨 [`Refactor`] Move third-party related utility files into `integrations/` folder 🚨🚨🚨 (#25599)

* move deepspeed to `lib_integrations.deepspeed`

* more refactor

* oops

* fix slow tests

* Fix docs

* fix docs

* addess feedback

* address feedback

* final modifs for PEFT

* fixup

* ok now

* trigger CI

* trigger CI again

* Update docs/source/en/main_classes/deepspeed.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* import from `integrations`

* address feedback

* revert removal of `deepspeed` module

* revert removal of `deepspeed` module

* fix conflicts

* ooops

* oops

* add deprecation warning

* place it on the top

* put `FutureWarning`

* fix conflicts with not_doctested.txt

* add back `bitsandbytes` module with a depr warning

* fix

* fix

* fixup

* oops

* fix doctests

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-08-18T17:08:03Z,"[`PEFT`] Peft integration alternative design  (#25077)

* a draft version

* v2 integration

* fix

* make it more generic and works for IA3

* add set adapter and multiple adapters support

* fixup

* adapt a bit

* oops

* oops

* oops

* adapt more

* fix

* add more refactor

* now works with model class

* change it to instance method as it causes issues with `jit`.

* add CR

* change method name

* add `add_adapter` method

* clean up

* Update src/transformers/adapters/peft_mixin.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* add moe utils

* fixup

* Update src/transformers/adapters/peft_mixin.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* adapt

* oops

* fixup

* add is_peft_available

* remove `requires_backend`

* trainer compatibility

* fixup + docstring

* more details

* trigger CI

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/modeling_utils.py

* fixup + is_main_process

* added `save_peft_format` in save_pretrained

* up

* fix nits here and there

* nits here and there.

* docs

* revert `encoding=""utf-8""`

* comment

* added slow tests before the PEFT release.

* fixup and nits

* let's be on the safe zone

* added more comments

* v1 docs

* add remaining docs

* Apply suggestions from code review

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* move to `lib_integrations`

* fixup

* this time fixup

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* address final comments

* refactor to use `token`

* add PEFT to DockerFile for slow tests.

* added pipeline support.

---------

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-08-17T18:48:58Z,"add warning for 8bit optimizers (#25575)

* add warning for 8bit optimizers

* protect import"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-08-17T16:23:34Z,"add util for ram efficient loading of model when using fsdp (#25107)

* add util for ram efficient loading of model when using fsdp

* make fix-copies

* fixes 😅

* docs

* making it further easier to use

* rename the function

* refactor to handle fsdp ram efficiency in `from_pretrained`

* fixes

* fixes

* fixes

* update

* fixes

* revert `load_pretrained_model_only_on_rank0`

* resolve `load_from_checkpoint`"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-08-17T15:44:01Z,"Revert ""change version (#25387)"" (#25573)

This reverts commit 3a05e010e0c7e8abd3e5357dd4e89e28cc69003e."
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-08-17T06:10:33Z,Update trainer.py (#25553)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-08-16T16:19:51Z,More frozen args (#25540)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-08-10T20:06:29Z,"GPTQ integration (#25062)

* GTPQ integration

* Add tests for gptq

* support for more quantization model

* fix style

* typo

* fix method

* Update src/transformers/modeling_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* add dataclass and fix quantization_method

* fix doc

* Update tests/quantization/gptq/test_gptq.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* modify dataclass

* add gtpqconfig import

* fix typo

* fix tests

* remove dataset as req arg

* remove tokenizer import

* add offload cpu quantization test

* fix check dataset

* modify dockerfile

* protect trainer

* style

* test for config

* add more log

* overwrite torch_dtype

* draft doc

* modify quantization_config docstring

* fix class name in docstring

* Apply suggestions from code review

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* more warning

* fix 8bit kwargs tests

* peft compatibility

* remove var

* fix is_gptq_quantized

* remove is_gptq_quantized

* fix wrap

* Update src/transformers/modeling_utils.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* add exllama

* skip test

* overwrite float16

* style

* fix skip test

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* fix docsting formatting

* add doc

* better test

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-08-10T15:07:32Z,"Fix issue with ratio evaluation steps and auto find batch size (#25436)

* Fully rebased solution

* 500"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-08-09T07:31:24Z,rm useless condition since the previous condition contains it. (#25403)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-08-08T17:05:41Z,change version (#25387)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-08-07T15:47:22Z,"Migrate Trainer from `Repository` to `upload_folder` (#25095)

* First draft

* Deal with progress bars

* Update src/transformers/utils/hub.py

Co-authored-by: Lucain <lucainp@gmail.com>

* Address review comments

* Forgot one

* Pin hf_hub

* Add argument for push all and fix tests

* Fix tests

* Address review comments

---------

Co-authored-by: Lucain <lucainp@gmail.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-08-02T07:29:00Z,Fix set of model parallel in the Trainer when no GPUs are available (#25239)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-07-28T09:40:08Z,"Fix `.push_to_hub` and cleanup `get_full_repo_name` usage (#25120)

* Fix .push_to_hub and cleanup get_full_repo_name usage

* Do not rely on Python bool conversion magic

* request changes"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-07-27T12:34:02Z,fix delete all checkpoints when save_total_limit is set to 1 (#25136)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-07-27T01:41:43Z,fix deepspeed load best model at end when the model gets sharded (#25057)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-07-24T14:53:10Z,"compute_loss in trainer failing to label shift for PEFT model when label smoothing enabled. (#25044)

* added PeftModelForCausalLM to MODEL_FOR_CAUSAL_LM_MAPPING_NAMES dict

* check for PEFT model in compute_loss section

---------

Co-authored-by: Nathan Brake <nbrake3@mmm.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-07-24T13:27:19Z,"Add dispatch_batches to training arguments (#25038)

* Dispatch batches

* Copy items"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-07-21T14:30:17Z,Use main_input_name for include_inputs_for_metrics (#24993)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-07-21T12:22:48Z,"fsdp fixes and enhancements (#24980)

* fix fsdp prepare to remove the warnings and fix excess memory usage

* Update training_args.py

* parity for FSDP+XLA

* Update trainer.py"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-07-21T06:47:26Z,"fix fsdp checkpointing issues (#24926)

* fix fsdp load

* Update trainer.py

* remove saving duplicate state_dict"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-07-17T18:45:59Z,"Remove deprecated codes (#24837)

* remove `xpu_backend` training argument

* always call `contextlib.nullcontext()` since transformers updated to
python3.8

* these codes will not be executed"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-07-12T15:47:21Z,"Rm duplicate pad_across_processes (#24780)

Rm duplicate"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-07-12T14:01:51Z,"Fix pad across processes dim in trainer and not being able to set the timeout (#24775)

* dim, and rm copy

* Don't rm copy for now

* Oops

* pad index

* Should be a working test

* Tickle down ddp timeout

* Put fix back in now that testing locally is done

* Better comment specifying timeout

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-07-12T09:49:12Z,"Fix eval_accumulation_steps leading to incorrect metrics (#24756)

Fix eval steps"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-07-11T20:37:04Z,"Fix lr scheduler not being reset on reruns (#24758)

* Try this

* Solved!

* Rm extranious

* Rm extranious

* self

* Args'

* Check for if we created the lr scheduler

* Move comment

* Clean"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-07-11T15:21:29Z,Docs: add `kwargs` type to fix formatting (#24733)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-07-06T18:12:16Z,"Fix integration with Accelerate and failing test (#24691)

Fix integration"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-07-06T09:33:25Z,"DeepSpeed/FSDP ckpt saving utils fixes and FSDP training args fixes (#24591)

* update ds and fsdp ckpt logic

* refactoring

* fix 🐛

* resolve comment

* fix issue with overriding of the fsdp config set by accelerate"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-29T18:37:44Z,"fix peft ckpts not being pushed to hub  (#24578)

* fix push to hub for peft ckpts

* oops"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-27T17:28:26Z,"Fix LR scheduler based on bs from auto bs finder (#24521)

* One solution

* args -> self"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-27T14:09:38Z,set model to training mode before accelerate.prepare (#24520)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-27T12:33:21Z,"use accelerate autocast in jit eval path, since mix precision logic is… (#24460)

use accelerate autocast in jit eval path, since mix precision logic is in accelerator currently

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-26T17:38:29Z,"Fix 'local_rank' AttiributeError in Trainer class (#24297)

fix attribute error"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-26T12:15:37Z,"deepspeed z1/z2 state dict fix (#24489)

* deepspeed z2/z1 state_dict bloating fix

* update

* version check"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-26T12:07:27Z,"when resume from peft checkpoint, the model should be trainable (#24463)"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-23T12:33:57Z,fixes issue when saving fsdp via accelerate's FSDP plugin (#24446)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-23T12:13:07Z,"fix the grad_acc issue at epoch boundaries (#24415)

* fix the grad_acc issue at epoch boundaries

Co-Authored-By: Zach Mueller <7831895+muellerzr@users.noreply.github.com>

* add contributors.

Co-authored-by: sumpster

* address comments

---------

Co-authored-by: Zach Mueller <7831895+muellerzr@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-23T11:35:04Z,"[`Trainer`] Fix `.to` call on 4bit models (#24444)

* fix `.to` call on 4bit models

* better check"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-22T18:46:20Z,Clarify batch size displayed when using DataParallel (#24430)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-22T18:28:25Z,"Refactor hyperparameter search backends (#24384)

* Refactor hyperparameter search backends

* Simpler refactoring without abstract base class

* black

* review comments:
specify name in class
use methods instead of callable class attributes
name constant better

* review comments: safer bool checking, log multiple available backends

* test ALL_HYPERPARAMETER_SEARCH_BACKENDS vs HPSearchBackend in unit test, not module. format with black.

* copyright"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-21T11:24:41Z,"[Trainer] Fix optimizer step on PyTorch TPU (#24389)

* update optimizer step for tpu

* add comment"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-20T11:57:08Z,"Fix resuming PeftModel checkpoints in Trainer  (#24274)

* Fix resuming checkpoints for PeftModels

Fix an error occurred when resuming a PeftModel from a training checkpoint. That was caused since PeftModel.pre_trained saves only adapter-related data while _load_from_checkpoint was expecting a torch sved model. This PR fix this issue and allows the adapter checkpoint to be loaded.

Resolves: #24252

* fix last comment

* fix nits

---------

Co-authored-by: younesbelkada <younesbelkada@gmail.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-16T19:14:03Z,"Adding ddp_broadcast_buffers argument to Trainer (#24326)

adding ddp_broadcast_buffers argument"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-16T14:38:23Z,"Byebye pytorch 1.9 (#24080)

byebye

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-15T13:17:09Z,"deepspeed init during eval fix (#24298)

* deepspeed init during eval fix

* commit suggestions

Co-Authored-By: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-14T16:44:09Z,"Clean up old Accelerate checks (#24279)

* Clean up old Accelerate checks

* Put back imports"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-13T19:19:15Z,"update FSDP save and load logic (#24249)

* update fsdp save and load logic

* fix

* see if this resolves the failing tests"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-12T17:26:17Z,Finish dataloader integration (#24201)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-12T15:23:37Z,"🚨🚨🚨 Replace DataLoader logic for Accelerate in Trainer, remove unneeded tests 🚨🚨🚨 (#24028)

* Working integration

* Fix failing test

* Revert label host logic

* Bring it back!"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-09T12:24:53Z,"fix bugs with trainer (#24134)

* fix the deepspeed test failures

* apex fix

* FSDP save ckpt fix

* Update src/transformers/trainer.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-08T13:38:30Z,"[`Trainer`] Correct behavior of `_load_best_model` for PEFT models (#24103)

* v1

* some refactor

- add ST format as well

* fix

* add `ADAPTER_WEIGHTS_NAME` & `ADAPTER_SAFE_WEIGHTS_NAME`"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-07T19:33:13Z,"fix accelerator prepare during eval only mode (#24014)

* fix mixed precision prep during eval only mode

* update to address comments

* update to reflect the changes in accelerate"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-07T19:31:32Z,"Do not prepare lr scheduler as it as the right number of steps (#24088)

* Do not prepare lr scheduler as it as the right number of steps

* Trigger CI

* Trigger CI

* Trigger CI

* Add fake comment

* Remove fake comment

* Trigger CI please!"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-07T16:38:04Z,"fix executable batch size issue (#24067)

* fix executable batch size issue

* fix

* undo"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-07T12:30:55Z,"Support PEFT models when saving the model using trainer (#24073)

* support PEFT models when saving the model using trainer

* fixup"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-05T12:28:10Z,"fix trainer slow tests related to hyperparam search (#24011)

* fix trainer slow tests

* commit 2"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-06-02T12:53:48Z,"Trainer: fixed evaluate raising `KeyError` for ReduceLROnPlateau (#23952)

Trainer: fixed KeyError on evaluate for ReduceLROnPlateau

Co-authored-by: Claudius Kienle <claudius.kienle@artiminds.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-05-31T17:34:55Z,"remove the extra `accelerator.prepare`  (#23914)

remove the extra `accelerator.prepare` that slipped in with multiple update from main 😅"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-05-31T11:54:26Z,Fix Trainer when model is loaded on a different GPU (#23792)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-05-31T09:46:22Z,"accelerate deepspeed and gradient accumulation integrate (#23236)

* mixed precision support via accelerate

* fix issues

* fix for the sharded ddp case

* fix flax and tf failing tests

* `refactor the place to create `Accelerator` object

* move ddp prep to accelerate

* fix 😅

* resolving comments

* move fsdp handling to accelerate

* fixex

* fix saving

* shift torch dynamo handling to accelerate

* shift deepspeed integration and save & load utils to accelerate

* fix accelerate launcher support

* oops

* fix 🐛

* save ckpt fix

* Trigger CI

* nasty 🐛 😅

* as deepspeed needs grad_acc fixes, transfer grad_acc to accelerate

* make tests happy

* quality ✨

* loss tracked needs to account for grad_acc

* fixing the deepspeed tests

* quality ✨

* 😅😅😅

* tests 😡

* quality ✨

* Trigger CI

* resolve comments and fix the issue with the previous merge from branch

* Trigger CI

* accelerate took over deepspeed integration

---------

Co-authored-by: Stas Bekman <stas@stason.org>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-05-31T09:38:20Z,Fix last instances of kbit -> quantized (#23797)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-05-31T09:12:07Z,"shift torch dynamo handling to accelerate (#23168)

* mixed precision support via accelerate

* fix issues

* fix for the sharded ddp case

* fix flax and tf failing tests

* `refactor the place to create `Accelerator` object

* move ddp prep to accelerate

* fix 😅

* resolving comments

* move fsdp handling to accelerate

* fixex

* fix saving

* shift torch dynamo handling to accelerate"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/trainer.py,2023-05-31T08:40:46Z,"move fsdp handling to accelerate (#23158)

* mixed precision support via accelerate

* fix issues

* fix for the sharded ddp case

* fix flax and tf failing tests

* `refactor the place to create `Accelerator` object

* move ddp prep to accelerate

* fix 😅

* resolving comments

* move fsdp handling to accelerate

* fixex

* fix saving"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-11-24T06:10:52Z,"Refactoring Trainer, adds `save_only_model` arg and simplifying FSDP integration (#27652)

* add code changes

1. Refactor FSDP
2. Add `--save_only_model` option: When checkpointing, whether to only save the model, or also the optimizer, scheduler & rng state.
3. Bump up the minimum `accelerate` version to `0.21.0`

* quality

* fix quality?

* Revert ""fix quality?""

This reverts commit 149330a6abc078827be274db84c8a2d26a76eba1.

* fix fsdp doc strings

* fix quality

* Update src/transformers/training_args.py

Co-authored-by: Zach Mueller <muellerzr@gmail.com>

* please fix the quality issue 😅

* Apply suggestions from code review

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* address comment

* simplify conditional check as per the comment

* update documentation

---------

Co-authored-by: Zach Mueller <muellerzr@gmail.com>
Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-11-21T17:51:48Z,"Fix `resize_token_embeddings` (#26861) (#26865)

* Fix `resize_token_embeddings` about `requires_grad`

The method `resize_token_embeddings` should keep `requires_grad`
unchanged for all parameters in embeddings.

Previously, `resize_token_embeddings` always set `requires_grad`
to `True`. After fixed, `resize_token_embeddings` copy the
`requires_grad` attribute in the old embeddings."
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-11-21T10:03:30Z,"[`core` / `gradient_checkpointing`] add support for old GC method (#27610)

* add support for old GC method

* add also disable

* up

* oops"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-11-20T15:45:55Z,"[`FA-2`] Add fa2 support for `from_config` (#26914)

* add fa2 support for from_config

* Update test_modeling_common.py"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-11-16T16:43:19Z,"[`Styling`] stylify using ruff (#27144)

* try to stylify using ruff

* might need to remove these changes?

* use ruf format andruff check

* use isinstance instead of type comparision

* use # fmt: skip

* use # fmt: skip

* nits

* soem styling changes

* update ci job

* nits isinstance

* more files update

* nits

* more nits

* small nits

* check and format

* revert wrong changes

* actually use formatter instead of checker

* nits

* well docbuilder is overwriting this commit

* revert notebook changes

* try to nuke docbuilder

* style

* fix feature exrtaction test

* remve `indent-width = 4`

* fixup

* more nits

* update the ruff version that we use

* style

* nuke docbuilder styling

* leve the print for detected changes

* nits

* Remove file I/O

Co-authored-by: charliermarsh
 <charlie.r.marsh@gmail.com>

* style

* nits

* revert notebook changes

* Add # fmt skip when possible

* Add # fmt skip when possible

* Fix

* More `  # fmt: skip` usage

* More `  # fmt: skip` usage

* More `  # fmt: skip` usage

* NIts

* more fixes

* fix tapas

* Another way to skip

* Recommended way

* Fix two more fiels

* Remove asynch
Remove asynch

---------

Co-authored-by: charliermarsh <charlie.r.marsh@gmail.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-11-16T15:35:40Z,"Raise error when quantizing a quantized model (#27500)

add error msg"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-11-15T19:58:08Z,"Fix offload disk for loading derivated model checkpoint into base model (#27253)

* fix

* style

* add test"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-11-02T11:03:51Z,"[`core` / `Quantization`] Fix for 8bit serialization tests (#27234)

* fix for 8bit serialization

* added regression tests.

* fixup"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-11-01T18:25:23Z,"Fix CPU offload + disk offload tests (#27204)

Fix disk offload tests + weight sharing issues"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-11-01T17:09:21Z,"Add exllamav2 better (#27111)

* add_ xllamav2 arg

* add test

* style

* add check

* add doc

* replace by use_exllama_v2

* fix tests

* fix doc

* style

* better condition

* fix logic

* add deprecate msg

* deprecate exllama

* remove disable_exllama from the linter

* remove

* fix warning

* Revert the commits deprecating exllama

* deprecate disable_exllama for use_exllama

* fix

* fix loading attribute

* better handling of args

* remove disable_exllama from init and linter

* Apply suggestions from code review

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* better arg

* fix warning

* Apply suggestions from code review

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* switch to dict

* Apply suggestions from code review

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* style

* nits

* style

* better tests

* style

---------

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-11-01T08:06:31Z,"[`core` / `Quantization` ] AWQ integration (#27045)

* working v1

* oops

* Update src/transformers/modeling_utils.py

Co-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>

* fixup

* oops

* push

* more changes

* add docs

* some fixes

* fix copies

* add v1 doc

* added installation guide

* relax constraints

* revert

* attempt llm-awq

* oops

* oops

* fixup

* raise error when incorrect cuda compute capability

* nit

* add instructions for llm-awq

* fixup

* fix copies

* fixup and docs

* change

* few changes + add demo

* add v1 tests

* add autoawq in dockerfile

* finalize

* Update tests/quantization/autoawq/test_awq.py

* fix test

* fix

* fix issue

* Update src/transformers/integrations/awq.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update docs/source/en/main_classes/quantization.md

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update docs/source/en/main_classes/quantization.md

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update src/transformers/integrations/awq.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update src/transformers/integrations/awq.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* add link to example script

* Update docs/source/en/main_classes/quantization.md

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* add more content

* add more details

* add link to quantization docs

* camel case + change backend class name

* change to string

* fixup

* raise errors if libs not installed

* change to `bits` and `group_size`

* nit

* nit

* Apply suggestions from code review

Co-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>

* disable training

* address some comments and fix nits

* fix

* final nits and fix tests

* adapt to our new runners

* make fix-copies

* Update src/transformers/utils/quantization_config.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/utils/quantization_config.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/integrations/awq.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* Update src/transformers/integrations/awq.py

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* move to top

* add conversion test

* final nit

* add more elaborated test

---------

Co-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>
Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>
Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-10-31T18:16:49Z,"Safetensors serialization by default (#27064)

* Safetensors serialization by default

* First pass on the tests

* Second pass on the tests

* Third pass on the tests

* Fix TF weight loading from TF-format safetensors

* Specific encoder-decoder fixes for weight crossloading

* Add VisionEncoderDecoder fixes for TF too

* Change filename test for pt-to-tf

* One missing fix for TFVisionEncoderDecoder

* Fix the other crossload test

* Support for flax + updated tests

* Apply suggestions from code review

Co-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>

* Sanchit's comments

* Sanchit's comments 2

* Nico's comments

* Fix tests

* cleanup

* Apply suggestions from code review

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

---------

Co-authored-by: Matt <rocketknight1@gmail.com>
Co-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>
Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-10-31T13:45:23Z,"Add support for loading GPTQ models on CPU (#26719)

* Add support for loading GPTQ models on CPU

Right now, we can only load the GPTQ Quantized model on the CUDA
device. The attribute `gptq_supports_cpu` checks if the current
auto_gptq version is the one which has the cpu support for the
model or not.
The larger variants of the model are hard to load/run/trace on
the GPU and that's the rationale behind adding this attribute.

Signed-Off By: Vivek Khandelwal <vivek@nod-labs.com>

* Update quantization.md

* Update quantization.md

* Update quantization.md"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-10-30T20:08:29Z,"Fix import of torch.utils.checkpoint (#27155)

* Fix import

* Apply suggestions from code review

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

---------

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-10-27T14:15:22Z,"[`core`/ `gradient_checkpointing`] Refactor GC - part 2 (#27073)

* fix

* more fixes

* fix other models

* fix long t5

* use `gradient_checkpointing_func` instead

* fix copies

* set `gradient_checkpointing_func` as a private attribute and retrieve previous behaviour

* Update src/transformers/modeling_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* replace it with `is_gradient_checkpointing_set`

* remove default

* Update src/transformers/modeling_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* fixup

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-10-27T13:49:20Z,"Fix no split modules underlying modules (#27090)

* fix no split

* style

* remove comm

* Update src/transformers/modeling_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* rename modules

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-10-27T12:32:54Z,Provide alternative when warning on use_auth_token (#27105)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-10-27T09:23:06Z,"Revert ""add exllamav2 arg"" (#27102)

Revert ""add exllamav2 arg (#26437)""

This reverts commit 8214d6e7b1d6ac25859ad745ccebdf73434e166d."
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-10-26T14:15:05Z,"add exllamav2 arg (#26437)

* add_ xllamav2 arg

* add test

* style

* add check

* add doc

* replace by use_exllama_v2

* fix tests

* fix doc

* style

* better condition

* fix logic

* add deprecate msg"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-10-26T09:21:04Z,"Bump`flash_attn` version to `2.1` (#27079)

* pin FA-2 to `2.1`

* fix on modeling"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-10-25T10:16:15Z,"[`core`] Refactor of `gradient_checkpointing` (#27020)

* v1

* fix

* remove `create_custom_forward`

* fixup

* fixup

* add test and fix all failing GC tests

* remove all remaining `create_custom_forward` methods

* fix idefics bug

* fixup

* replace with `__call__`

* add comment

* quality"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-10-24T17:05:37Z,"Fix config silent copy in from_pretrained (#27043)

* Fix config modeling utils

* fix more

* fix attn mask bug

* Update src/transformers/modeling_utils.py"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-10-24T13:10:23Z,"Add fuyu device map (#26949)

* add _no_split_modules

* style

* fix _no_split_modules

* add doc"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-10-23T12:25:48Z,"Change default `max_shard_size` to smaller value (#26942)

* Update modeling_utils.py

* fixup

* let's change it to 5GB

* fix"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-10-16T17:56:53Z,"🚨🚨🚨 [`Quantization`] Store the original dtype in the config as a private attribute 🚨🚨🚨 (#26761)

* First step

* fix

* add adjustements for gptq

* change to `_pre_quantization_dtype`

* Update src/transformers/modeling_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* fix serialization

* Apply suggestions from code review

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* fixup

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-10-16T13:29:01Z,"Make fsdp ram efficient loading optional (#26631)

make fsdp ram efficient loading optional"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-10-13T10:56:50Z,"[`core`] Fix fa-2 import (#26785)

* fix fa-2 import

* nit"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-10-12T08:28:40Z,"Add many missing spaces in adjacent strings (#26751)

Add missing spaces in adjacent strings"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-10-05T12:44:31Z,"[`core`] fix silent bug `keep_in_fp32` modules (#26589)

* fix silent bug `keep_in_fp32` modules

* final fix

* added a common test.

* Trigger CI

* revert"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-10-03T12:53:09Z,"[`PEFT`] Final fixes (#26559)

* fix issues with PEFT

* logger warning futurewarning issues

* fixup

* adapt from suggestions

* oops

* rm test"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-10-03T06:55:39Z,"[RFC, Logging] Change warning to info (#26545)

[Logging] Change warning to info"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-10-02T12:59:24Z,"[`PEFT`] Protect `adapter_kwargs` check (#26537)

Update modeling_utils.py"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-10-02T09:23:03Z,"[`PEFT`] Pass token when calling `find_adapter_config` (#26488)

* try

* nit

* nits"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-09-28T09:13:03Z,"[`PEFT`] introducing `adapter_kwargs` for loading adapters from different Hub location (`subfolder`, `revision`) than the base model (#26270)

* make use of adapter_revision

* v1 adapter kwargs

* fix CI

* fix CI

* fix CI

* fixup

* add BC

* Update src/transformers/integrations/peft.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* fixup

* change it to error

* Update src/transformers/modeling_utils.py

* Update src/transformers/modeling_utils.py

* fixup

* change

* Update src/transformers/integrations/peft.py

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-09-27T14:45:31Z,"[`PEFT`] Fix PEFT multi adapters support (#26407)

* fix PEFT multi adapters support

* refactor a bit

* save pretrained + BC + added tests

* Update src/transformers/integrations/peft.py

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* add more tests

* add suggestion

* final changes

* adapt a bit

* fixup

* Update src/transformers/integrations/peft.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* adapt from suggestions

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-09-22T15:42:10Z,"[`core` ] Integrate Flash attention 2 in most used models (#25598)

* v1

* oops

* working v1

* fixup

* add some TODOs

* fixup

* padding support + try with module replacement

* nit

* alternative design

* oops

* add `use_cache` support for llama

* v1 falcon

* nit

* a bit of refactor

* nit

* nits nits

* add v1 padding support falcon (even though it seemed to work before)

* nit

* falcon works

* fixup

* v1 tests

* nit

* fix generation llama flash

* update tests

* fix tests + nits

* fix copies

* fix nit

* test- padding mask

* stype

* add more mem efficient support

* Update src/transformers/modeling_utils.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* fixup

* nit

* fixup

* remove it from config when saving

* fixup

* revert docstring

* add more checks

* use values

* oops

* new version

* fixup

* add same trick for falcon

* nit

* add another test

* change tests

* fix issues with GC and also falcon

* fixup

* oops

* Update src/transformers/models/falcon/modeling_falcon.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* add init_rope

* updates

* fix copies

* fixup

* fixup

* more clarification

* fixup

* right padding tests

* add docs

* add FA in docker image

* more clarifications

* add some figures

* add todo

* rectify comment

* Change to FA2

* Update docs/source/en/perf_infer_gpu_one.md

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* split in two lines

* change test name

* add more tests

* some clean up

* remove `rearrange` deps

* add more docs

* revert changes on dockerfile

* Revert ""revert changes on dockerfile""

This reverts commit 8d72a66b4b9b771abc3f15a9b9506b4246d62d8e.

* revert changes on dockerfile

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <hi@lysand.re>

* address some comments

* docs

* use inheritance

* Update src/transformers/testing_utils.py

Co-authored-by: Lysandre Debut <hi@lysand.re>

* fixup

* Apply suggestions from code review

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* Update src/transformers/modeling_utils.py

* final comments

* clean up

* style

* add cast + warning for PEFT models

* fixup

---------

Co-authored-by: Felix Marty <9808326+fxmarty@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>
Co-authored-by: Lysandre Debut <hi@lysand.re>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-09-21T10:00:03Z,"Keep relevant weights in fp32 when `model._keep_in_fp32_modules` is set even when `accelerate` is not installed (#26225)

* fix bug where weight would not be kept in fp32

* nit

* address review comments

* fix test"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-09-20T04:56:16Z,"FSDP tests and checkpointing fixes (#26180)

* add fsdp tests

* Update test_fsdp.py

* Update test_fsdp.py

* fixes

* checks

* Update trainer.py

* fix

* fixes for saving/resuming checkpoints

* fixes

* add tests and delete debug statements

* fixing tests

* Update test_fsdp.py

* fix tests

* fix tests

* minor nits

* fix code style and quality

* refactor and modularize test code

* reduce the time of tests

* reduce the test time

* fix test

* reduce test time

* reduce test time

* fix failing tests

* fix

* Apply suggestions from code review

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* resolve comments

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-09-19T19:44:41Z,"[FIX] resize_token_embeddings (#26102)

* fix roundup command

* add test for resize_token_embeddings

* Update tests/test_modeling_common.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* style

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-09-19T19:04:56Z,"DeepSpeed ZeRO-3 handling when resizing embedding layers (#26259)

* fix failing deepspeed slow tests

* fixes"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-09-15T15:53:39Z,"Fix pad to multiple of (#25732)

* nits

* update the test

* nits

* update

* fix bark

* fix bark tests and allow padding to multiple of without new tokens"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-09-14T11:01:58Z,"[`PEFT`] Fix PEFT + gradient checkpointing (#25846)

* fix PEFT + gradient checkpointing

* add disable RG

* polish tests

* fix comment

* Revert ""fix comment""

This reverts commit b85386f50d2b104bac522e823c47b7e232116a47.

* final explanations and tests"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-09-13T12:12:35Z,"[`core`] fix 4bit `num_parameters` (#26132)

* fix 4bit `num_parameters`

* stronger check"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-09-13T04:56:37Z,safeguard torch distributed check (#26056)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-09-08T19:13:33Z,"Skip warning if tracing with dynamo (#25581)

* Ignore warning if tracing with dynamo

* fix import error

* separate to function

* add test"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-09-07T09:10:40Z,fix _resize_token_embeddings will set lm head size to 0 when enabled deepspeed zero3 (#26024)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-09-06T15:45:47Z,"modify context length for GPTQ + version bump (#25899)

* add new arg for gptq

* add tests

* add min version autogptq

* fix order

* skip test

* fix

* Update src/transformers/modeling_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* fix style

* change model path

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-09-05T09:37:54Z,"nn.Identity is not required to be compatible with PyTorch < 1.1.0 as the minimum PyTorch version we currently support is 1.10.0 (#25974)

nn.Identity is not required to be compatible with PyTorch < 1.1.0 as the
minimum PyTorch version we currently support is 1.10.0"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-08-31T21:38:14Z,"remove torch_dtype override (#25894)

* remove torch_dtype override

* style

* Update src/transformers/modeling_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-08-30T15:00:36Z,fix max_memory for bnb (#25842)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-08-29T19:10:46Z,Generate: models with custom `generate()` return `True` in `can_generate()` (#25838)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-08-29T11:32:19Z,"Resolving Attribute error when using the FSDP ram efficient feature (#25820)

fix bug"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-08-25T15:46:56Z,"fix a typo in docsting (#25759)

* fix a typo in docsting

* Update src/transformers/modeling_utils.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

---------

Co-authored-by: statelesshz <jihuazhong1@huawei.com>
Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-08-25T15:13:34Z,"🚨🚨🚨 [`Refactor`] Move third-party related utility files into `integrations/` folder 🚨🚨🚨 (#25599)

* move deepspeed to `lib_integrations.deepspeed`

* more refactor

* oops

* fix slow tests

* Fix docs

* fix docs

* addess feedback

* address feedback

* final modifs for PEFT

* fixup

* ok now

* trigger CI

* trigger CI again

* Update docs/source/en/main_classes/deepspeed.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* import from `integrations`

* address feedback

* revert removal of `deepspeed` module

* revert removal of `deepspeed` module

* fix conflicts

* ooops

* oops

* add deprecation warning

* place it on the top

* put `FutureWarning`

* fix conflicts with not_doctested.txt

* add back `bitsandbytes` module with a depr warning

* fix

* fix

* fixup

* oops

* fix doctests

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-08-24T16:48:41Z,"[`from_pretrained`] Fix failing PEFT tests (#25733)

fix failing PEFT tests"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-08-24T14:18:39Z,"[`from_pretrained`]  Simpler code for peft (#25726)

* refactor complicated from pretrained for peft

* nits

* more nits

* Update src/transformers/modeling_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* make tests happy

* fixup after merge

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-08-24T12:56:11Z,"Fix number of minimal calls to the Hub with peft integration (#25715)

* Fix number of minimal calls to the Hub with peft integration

* Alternate design

* And this way?

* Revert

* Address comments"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-08-24T09:14:27Z,"docs: Resolve typos in warning text (#25711)

Resolve typos in warning text"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-08-24T06:00:42Z,fix ram efficient fsdp init (#25686)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-08-18T21:30:29Z,"reattach hooks when using `resize_token_embeddings` (#25596)

* reattach hooks

* fix style"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-08-18T17:08:03Z,"[`PEFT`] Peft integration alternative design  (#25077)

* a draft version

* v2 integration

* fix

* make it more generic and works for IA3

* add set adapter and multiple adapters support

* fixup

* adapt a bit

* oops

* oops

* oops

* adapt more

* fix

* add more refactor

* now works with model class

* change it to instance method as it causes issues with `jit`.

* add CR

* change method name

* add `add_adapter` method

* clean up

* Update src/transformers/adapters/peft_mixin.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* add moe utils

* fixup

* Update src/transformers/adapters/peft_mixin.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* adapt

* oops

* fixup

* add is_peft_available

* remove `requires_backend`

* trainer compatibility

* fixup + docstring

* more details

* trigger CI

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/modeling_utils.py

* fixup + is_main_process

* added `save_peft_format` in save_pretrained

* up

* fix nits here and there

* nits here and there.

* docs

* revert `encoding=""utf-8""`

* comment

* added slow tests before the PEFT release.

* fixup and nits

* let's be on the safe zone

* added more comments

* v1 docs

* add remaining docs

* Apply suggestions from code review

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* move to `lib_integrations`

* fixup

* this time fixup

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* address final comments

* refactor to use `token`

* add PEFT to DockerFile for slow tests.

* added pipeline support.

---------

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-08-18T08:32:46Z,"Added missing parenthesis in call to is_fsdp_enabled (#25585)

Calling function is_fsdp_enabled instead of checking if it is not None"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-08-17T16:23:34Z,"add util for ram efficient loading of model when using fsdp (#25107)

* add util for ram efficient loading of model when using fsdp

* make fix-copies

* fixes 😅

* docs

* making it further easier to use

* rename the function

* refactor to handle fsdp ram efficiency in `from_pretrained`

* fixes

* fixes

* fixes

* update

* fixes

* revert `load_pretrained_model_only_on_rank0`

* resolve `load_from_checkpoint`"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-08-17T15:44:01Z,"Revert ""change version (#25387)"" (#25573)

This reverts commit 3a05e010e0c7e8abd3e5357dd4e89e28cc69003e."
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-08-17T15:19:54Z,"Inconsistency in PreTrainedModel.resize_token_embeddings When ZeRO3 Is Enabled (#25394)

* Inconsistency in PreTrainedModel.resize_token_embeddings

This PR addresses https://github.com/huggingface/transformers/issues/25241.

In previous implementation when ZeRO stage 3 was enbaled, resize_token_embeddings would create independent PyTorch weights on each device. Here we ensure that new embeddings are created with DeepSpeed init, and are properly partitioned accros devices.

* formatting with black

* adding the removed comments back in

---------

Co-authored-by: Sina Moeini <smoeini@amazon.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-08-17T15:00:32Z,"[`resize_embedding`] Introduce `pad_to_multiple_of` and guidance (#25088)

* fix

* revert cahnges and update resizing of embedding layer

* use wraning

* fixup

* more styling nits

* fix all tests that overload the embedding tests

* 👀👀 remove breakpoint

* remove useless overload + overload correctly where needed

* resize lm head with new vocab size

* reverse not necessary changes

* style

* fix CIs!

* fix last CI tests, adapt bark and Marian

* fixup"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-08-10T20:06:29Z,"GPTQ integration (#25062)

* GTPQ integration

* Add tests for gptq

* support for more quantization model

* fix style

* typo

* fix method

* Update src/transformers/modeling_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* add dataclass and fix quantization_method

* fix doc

* Update tests/quantization/gptq/test_gptq.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* modify dataclass

* add gtpqconfig import

* fix typo

* fix tests

* remove dataset as req arg

* remove tokenizer import

* add offload cpu quantization test

* fix check dataset

* modify dockerfile

* protect trainer

* style

* test for config

* add more log

* overwrite torch_dtype

* draft doc

* modify quantization_config docstring

* fix class name in docstring

* Apply suggestions from code review

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* more warning

* fix 8bit kwargs tests

* peft compatibility

* remove var

* fix is_gptq_quantized

* remove is_gptq_quantized

* fix wrap

* Update src/transformers/modeling_utils.py

Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>

* add exllama

* skip test

* overwrite float16

* style

* fix skip test

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* fix docsting formatting

* add doc

* better test

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-08-10T09:54:26Z,Generate: Load generation config when `device_map` is passed (#25413)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-08-08T17:05:41Z,change version (#25387)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-08-08T08:49:21Z,"Add warning for missing attention mask when pad tokens are detected (#25345)

* Add attention mask and pad token warning to many of the models

* Remove changes under examples/research_projects

These files are not maintained by HG.

* Skip the warning check during torch.fx or JIT tracing

* Switch ordering for the warning and input shape assignment

This ordering is a little cleaner for some of the cases.

* Add missing line break in one of the files"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-08-04T11:42:05Z,"Move usage of deprecated logging.warn to logging.warning (#25310)

The former spelling is deprecated and has been discouraged for a
while. The latter spelling seems to be more common in this project
anyway, so this change ought to be safe.

Fixes https://github.com/huggingface/transformers/issues/25283"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-08-02T16:11:15Z,"[MMS] Fix mms (#25267)

* [MMS] Fix mms

* [MMS] Fix mms

* fix mms loading

* Apply suggestions from code review

* make style

* Update tests/models/wav2vec2/test_modeling_wav2vec2.py"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-08-01T08:56:52Z,"[`Docs`/`quantization`] Clearer explanation on how things works under the hood. + remove outdated info (#25216)

* clearer explanation on how things works under the hood.

* Update docs/source/en/main_classes/quantization.md

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

* Update docs/source/en/main_classes/quantization.md

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* add `load_in_4bit` in `from_pretrained`

---------

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>
Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-07-31T15:25:09Z,"[`PreTrainedModel`] Wrap `cuda` and `to` method correctly (#25206)

wrap `cuda` and `to` method correctly"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-07-28T12:17:24Z,override .cuda() to check if model is already quantized (#25166)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-07-27T13:09:27Z,"Clarify 4/8 bit loading log message (#25134)

* clarify 4/8 bit loading log message

* make style"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-07-26T13:09:59Z,"update `use_auth_token` -> `token` (#25083)

* update

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-07-25T22:08:45Z,"fix tied_params for meta tensor (#25101)

* fix tied_params for meta tensor

* remove duplicate"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-07-21T19:39:28Z,"improve from_pretrained for zero3 multi gpus mode (#24964)

* improve from_pretrained for zero3 multi gpus mode

* Add check if torch.distributed.is_initialized

* Revert torch.distributed

---------

Co-authored-by: Stas Bekman <stas@stason.org>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-07-11T16:40:21Z,Skip keys not in the state dict when finding mismatched weights (#24749)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-07-11T15:21:29Z,Docs: add `kwargs` type to fix formatting (#24733)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-06-30T12:19:39Z,"Show a warning for missing attention masks when pad_token_id is not None (#24510)

* Adding warning messages to BERT for missing attention masks

These warning messages when there are pad tokens within the input ids and
no attention masks are given. The warning message should only show up once.

* Adding warning messages to BERT for missing attention masks

These warning messages are shown when the pad_token_id is not None
and no attention masks are given. The warning message should only
show up once.

* Ran fix copies to copy over the changes to some of the other models

* Add logger.warning_once.cache_clear() to the test

* Shows warning when there are no attention masks and input_ids start/end with pad tokens

* Using warning_once() instead and fix indexing in input_ids check

---------

Co-authored-by: JB Lau <hckyn@voyager2.local>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-06-28T05:22:39Z,"⚠️ Time to say goodbye to py37 (#24091)

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-06-27T18:45:40Z,"Clean load keys (#24505)

* Preliminary work on some models

* Fix test load missing and make sure nonpersistent buffers are tested

* Always ignore nonpersistent buffers if in state_dict

* Treat models

* More models

* Treat remaining models

* Fix quality

* Fix tests

* Remove draft

* This test is not needed anymore

* Fix copies

* Fix last test

* Newly added models

* Fix last tests

* Address review comments"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-06-22T13:40:38Z,"[`bnb`] Fix bnb serialization issue with new release (#24416)

* fix bnb issue

* fixup

* revert and do simple patching instead

* add more details"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-06-21T17:24:11Z,"Explicit arguments in `from_pretrained` (#24306)

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

* fix

---------

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-06-16T14:55:42Z,"Tied weights load (#24310)

* Use tied weight keys

* More

* Fix tied weight missing warning

* Only give info on unexpected keys with different classes

* Deal with empty archs

* Fix tests

* Refine test"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-06-15T16:31:38Z,"Make `can_generate` as class method (#24299)

fix

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-06-14T16:44:09Z,"Clean up old Accelerate checks (#24279)

* Clean up old Accelerate checks

* Put back imports"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-06-13T15:38:39Z,"Tied params cleanup (#24211)

* First test

* Add info for all models

* style

* Repo consistency

* Fix last model and cleanup prints

* Repo consistency

* Use consistent function for detecting tied weights"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-06-13T13:07:00Z,Improving error message when using `use_safetensors=True`. (#24232)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-06-12T15:31:06Z,"Fix `_load_pretrained_model` (#24200)

Fix test"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-06-06T13:12:46Z,"Add check for tied parameters (#24029)

* Add check for tied parameters

* Fix style

* fix style

* Fix versioning

* Change if to elif"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-06-01T17:21:22Z,"Modify device_map behavior when loading a model using from_pretrained (#23922)

* Modify device map behavior for 4/8 bits model

* Remove device_map arg for training 4/8 bit model

* Remove index

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Add Exceptions

* Modify comment

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Fix formatting

* Get current device with accelerate

* Revert ""Get current device with accelerate""

This reverts commit 46f00799103bbe15bd58762ba029aab35363c4f7.

* Fix Exception

* Modify quantization doc

* Fix error

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-05-31T19:32:21Z,Skip device placement for past key values in decoder models (#23919)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-05-31T14:40:07Z,"[`bnb`] add warning when no linear  (#23894)

* add warning for gpt2-like models

* more details

* adapt from suggestions"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-05-31T13:42:30Z,"Support shared tensors (#23871)

* Suport shared storage

* Really be sure we have the same storage

* Make style

* - Refactor storage identifier mechanism
 - Group everything into a single for loop

* Make style

* PR

* make style

* Update src/transformers/pytorch_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-05-31T09:38:20Z,Fix last instances of kbit -> quantized (#23797)
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-05-30T15:12:14Z,"[from_pretrained] imporve the error message when `_no_split_modules` is not defined (#23861)

* Better warning

* Update src/transformers/modeling_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* format line

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/modeling_utils.py,2023-05-25T11:48:48Z,"Fix `pip install --upgrade accelerate` command in modeling_utils.py (#23747)

Fix command in modeling_utils.py"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/integrations/peft.py,2023-11-14T09:32:57Z,"[`Peft`] `modules_to_save` support for peft integration (#27466)

* `modules_to_save` support for peft integration

* Update docs/source/en/peft.md

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>

* slightly elaborate test

---------

Co-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/integrations/peft.py,2023-11-13T13:20:54Z,"Remove-auth-token (#27060)

* don't use `use_auth_token`internally

* let's use token everywhere

* fixup"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/integrations/peft.py,2023-10-03T12:53:09Z,"[`PEFT`] Final fixes (#26559)

* fix issues with PEFT

* logger warning futurewarning issues

* fixup

* adapt from suggestions

* oops

* rm test"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/integrations/peft.py,2023-09-28T09:13:03Z,"[`PEFT`] introducing `adapter_kwargs` for loading adapters from different Hub location (`subfolder`, `revision`) than the base model (#26270)

* make use of adapter_revision

* v1 adapter kwargs

* fix CI

* fix CI

* fix CI

* fixup

* add BC

* Update src/transformers/integrations/peft.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

* fixup

* change it to error

* Update src/transformers/modeling_utils.py

* Update src/transformers/modeling_utils.py

* fixup

* change

* Update src/transformers/integrations/peft.py

---------

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/integrations/peft.py,2023-09-27T14:45:31Z,"[`PEFT`] Fix PEFT multi adapters support (#26407)

* fix PEFT multi adapters support

* refactor a bit

* save pretrained + BC + added tests

* Update src/transformers/integrations/peft.py

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>

* add more tests

* add suggestion

* final changes

* adapt a bit

* fixup

* Update src/transformers/integrations/peft.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* adapt from suggestions

---------

Co-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/integrations/peft.py,2023-09-15T16:22:01Z,"[PEFT] Allow PEFT model dict to be loaded (#25721)

* Allow PEFT model dict to be loaded

* make style

* make style

* Apply suggestions from code review

* address comments

* fixup

* final change

* added tests

* fix test

* better logic for handling if adapter has been loaded

* Update tests/peft_integration/test_peft_integration.py

Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>

---------

Co-authored-by: younesbelkada <younesbelkada@gmail.com>
Co-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>
Co-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/integrations/peft.py,2023-08-30T10:56:05Z,"minor typo fix in PeftAdapterMixin docs (#25829)

fix minor documentation typo"
github.com/kssteven418/SqueezeLLM-gradients,src/transformers/integrations/peft.py,2023-08-25T15:13:34Z,"🚨🚨🚨 [`Refactor`] Move third-party related utility files into `integrations/` folder 🚨🚨🚨 (#25599)

* move deepspeed to `lib_integrations.deepspeed`

* more refactor

* oops

* fix slow tests

* Fix docs

* fix docs

* addess feedback

* address feedback

* final modifs for PEFT

* fixup

* ok now

* trigger CI

* trigger CI again

* Update docs/source/en/main_classes/deepspeed.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* import from `integrations`

* address feedback

* revert removal of `deepspeed` module

* revert removal of `deepspeed` module

* fix conflicts

* ooops

* oops

* add deprecation warning

* place it on the top

* put `FutureWarning`

* fix conflicts with not_doctested.txt

* add back `bitsandbytes` module with a depr warning

* fix

* fix

* fixup

* oops

* fix doctests

---------

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>"
github.com/mzbac/llama2-fine-tune,merge_peft_adapters.py,2023-08-13T14:24:09Z,Update merge_peft_adapters.py
github.com/mzbac/llama2-fine-tune,merge_peft_adapters.py,2023-08-13T14:10:45Z,init
github.com/wordweb/langchain-ChatGLM-and-TigerBot,models/loader/loader.py,2023-06-12T06:12:06Z,langchain-chatglm up
github.com/wordweb/langchain-ChatGLM-and-TigerBot,models/loader/loader.py,2023-06-08T11:56:22Z,Merge remote-tracking branch 'origin/main'
github.com/OpenGVLab/LAMM,src/model/llava/model/builder.py,2023-12-22T07:36:37Z,llava1.5
github.com/OpenGVLab/LAMM,src/ChEF/models/test_llava15.py,2023-12-22T07:36:37Z,llava1.5
github.com/ymcui/Chinese-Mixtral,scripts/merge_mixtral_with_chinese_lora_low_mem.py,2024-01-29T02:55:06Z,"v1.0: release Chinese-Mixtral and Chinese-Mixtral-Instruct (#1)

* doc: init doc

* Update README.md

* doc: update gguf model perf.

* doc: finish quant perf.

* doc: update intro

* doc: add longbench info

* doc: update iq2_xs, iq2_xxs perf.

* doc: add chinese-mixtral baidu links

* doc: add ppl v.s. ctx figure

* add merge_lora script

* Update merge_mixtral_with_chinese_lora_low_mem.py

clean up old naming

* doc: update template

* doc: update numbers, mixtral arch

* llamacpp: add chat script

* Update chinese-mixtral-ppl.png

* doc: add perf.

* doc: update baidu link

* doc: update gpt-4 rating

* doc: finsh perf.

* script: change default temp.

* doc: add gpt-4 score

* doc: update context desc.

* doc: init en readme

* doc: minor fixes

* update based on Codacy

---------

Co-authored-by: ymcui <16095339+ymcui@users.noreply.github.com>
Co-authored-by: iMountTai <2506700016@qq.com>"
github.com/ArtificialZeng/baichuan-speedup,pyfastllm/fastllm/convert.py,2023-08-10T05:04:50Z,add c++ code
github.com/telexyz/GPT4VN,chatbot.py,2023-05-21T14:01:08Z,Update chatbot.py
github.com/telexyz/GPT4VN,chatbot.py,2023-04-09T00:58:22Z,update
github.com/telexyz/GPT4VN,chatbot.py,2023-04-09T00:56:12Z,update
github.com/telexyz/GPT4VN,chatbot.py,2023-04-05T03:11:50Z,thay dữ liệu finetune mới nhất
github.com/telexyz/GPT4VN,chatbot.py,2023-04-03T11:10:39Z,chatbot use best model
github.com/telexyz/GPT4VN,chatbot.py,2023-04-03T09:32:28Z,huấn luyện chỉ dẫn và chạy chatbot trên toàn bộ tập dữ liệu
github.com/telexyz/GPT4VN,chatbot.py,2023-04-02T15:41:20Z,another way to extract response
github.com/telexyz/GPT4VN,chatbot.py,2023-04-02T15:21:28Z,update
github.com/telexyz/GPT4VN,chatbot.py,2023-04-02T07:42:45Z,refine
github.com/telexyz/GPT4VN,chatbot.py,2023-03-31T12:17:11Z,load in 8bit to save vram
github.com/telexyz/GPT4VN,chatbot.py,2023-03-31T12:08:30Z,move make_prompt to separate file
github.com/telexyz/GPT4VN,chatbot.py,2023-03-31T12:06:53Z,Sử dụng model vietai gpt-j 6b
github.com/telexyz/GPT4VN,chatbot.py,2023-03-31T11:58:10Z,update
github.com/ictnlp/TruthX,transformers/src/transformers/trainer.py,2024-02-27T13:46:43Z,first commit
github.com/ictnlp/TruthX,transformers/src/transformers/modeling_utils.py,2024-02-27T13:46:43Z,first commit
github.com/ictnlp/TruthX,transformers/src/transformers/integrations/peft.py,2024-02-27T13:46:43Z,first commit
github.com/BUPT-LawLLM/LawLLM,evaluator.py,2023-11-15T19:11:18Z,Add files via upload
github.com/FederatedAI/FATE-LLM,python/fate_llm/model_zoo/pellm/parameter_efficient_llm.py,2023-08-30T06:33:51Z,"LLM-1.3 Update
1. FedIPR:
- backdoor watermark dataset
- feature-based water mark modules: conv and layernorm
- built-in feature-based watermark model: alexnet, resnet18, distilbert, gpt2
- fedipr trainer
2. Offsite-tuning
- offsite-tuning models: gpt2 family and bloom-7b
- offsite-tuning trainer

Signed-off-by: cwj <talkingwallace@sohu.com>"
github.com/FederatedAI/FATE-LLM,python/fate_llm/model_zoo/pellm/parameter_efficient_llm.py,2023-05-29T12:36:58Z,"update fate_llm v1.1: support chatglm

Signed-off-by: mgqa34 <mgq3374541@163.com>"
github.com/parity-asia/hackathon-2023-winter,projects/37-MonteCarlo/src/poc-ai-script/adhoc_unit_test.py,2023-12-20T19:24:40Z,update readme
github.com/cmagganas/CoverLetter-GenAI-adapter,app/app.py,2023-04-27T17:09:34Z,Create app.py
github.com/bigcode-project/astraios,peft/src/peft/auto.py,2023-12-14T14:00:17Z,init
github.com/bigcode-project/astraios,peft/src/peft/config.py,2023-12-14T14:00:17Z,init
github.com/bigcode-project/astraios,peft/src/peft/helpers.py,2023-12-14T14:00:17Z,init
github.com/bigcode-project/astraios,peft/src/peft/peft_model.py,2023-12-14T14:00:17Z,init
github.com/bigcode-project/astraios,peft/src/peft/tuners/ia3.py,2023-12-14T14:00:17Z,init
github.com/bigcode-project/astraios,peft/src/peft/tuners/lora.py,2023-12-14T14:00:17Z,init
github.com/bigcode-project/astraios,peft/src/peft/tuners/bottleneck.py,2023-12-14T14:00:17Z,init
github.com/bigcode-project/astraios,peft/src/peft/tuners/prompt_tuning.py,2023-12-14T14:00:17Z,init
github.com/bigcode-project/astraios,peft/examples/fp4_finetuning/finetune_fp4_opt_bnb_peft.py,2023-12-14T14:00:17Z,init
github.com/bigcode-project/astraios,peft/examples/lora_dreambooth/convert_peft_sd_lora_to_kohya_ss.py,2023-12-14T14:00:17Z,init
github.com/bigcode-project/astraios,peft/tests/test_hub_features.py,2023-12-14T14:00:17Z,init
github.com/bigcode-project/astraios,peft/tests/test_custom_models.py,2023-12-14T14:00:17Z,init
github.com/bigcode-project/astraios,peft/tests/test_adaption_prompt.py,2023-12-14T14:00:17Z,init
github.com/bigcode-project/astraios,peft/tests/test_multitask_prompt_tuning.py,2023-12-14T14:00:17Z,init
github.com/liyucheng09/Selective_Context,qa_manager.py,2023-06-04T16:26:46Z,fix a lot of bugs in adding new models
github.com/liyucheng09/Selective_Context,qa_manager.py,2023-05-19T22:20:59Z,add open-sourced LLMs in experiments
github.com/liyucheng09/Selective_Context,qa_manager.py,2023-04-24T14:58:38Z,stable version with all element for the paper
github.com/liyucheng09/Selective_Context,qa_manager.py,2023-04-19T12:57:44Z,"add Filelock in `context_manager.py`
Fix bug in `load_articles`
Enhance robustness in `qa_manager.py`
add results parsing in `utils.py`"
github.com/liyucheng09/Selective_Context,qa_manager.py,2023-04-17T22:00:00Z,"add reconstruction task
seperate Evaluator from TaskManager"
github.com/liyucheng09/Selective_Context,qa_manager.py,2023-04-17T15:18:02Z,"error handle in `qa_manager.py`
dataset_statistic checker in `utils.py`"
github.com/liyucheng09/Selective_Context,qa_manager.py,2023-04-12T23:45:17Z,"tokenizer bug fixed
spacy.noun_chunk doesn't align to GPT tokenizer

add num_article variable to context_manager"
github.com/liyucheng09/Selective_Context,qa_manager.py,2023-04-12T21:05:34Z,"add slicing to context_manager
bug fix"
github.com/liyucheng09/Selective_Context,qa_manager.py,2023-04-12T18:12:19Z,add checkpoints supports
github.com/liyucheng09/Selective_Context,qa_manager.py,2023-04-12T16:57:55Z,add `qa` task and `phrase` level masking
github.com/liyucheng09/Selective_Context,qa_manager.py,2023-04-11T14:50:22Z,bugs fix
github.com/liyucheng09/Selective_Context,qa_manager.py,2023-04-07T14:37:40Z,context_manager and qa_manager
github.com/lfy79001/TableQAKit,TableQAKit/llama/model.py,2023-07-31T03:13:35Z,增加icl，更改部分代码
github.com/huggingface/lighteval,src/lighteval/models/adapter_model.py,2024-02-07T14:38:46Z,"Adding inference endpoints models (#12)

This PR:
    uses Requests instead of passing tuples (which are more error prones) in the Datasets
    introduces an Abstract Model class which defines the minimum functions we need to have in a model for it to be lighteval compatible
    cleans up the BaseModel code
    introduces inference endpoints models

Inference endpoints models are these ones: https://huggingface.co/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints
Not to be confused with TGI models (which need a local deployment)


---------

Co-authored-by: Nathan Habib <30601243+NathanHB@users.noreply.github.com>"
github.com/huggingface/lighteval,src/lighteval/models/adapter_model.py,2024-01-26T14:38:32Z,"Init

Co-authored-by: Nathan Habib <nathan.habib@huggingface.co>
Co-authored-by: Thom Wolf <thom@huggingface.co>"
github.com/mikeybellissimo/LoRA-MPT,src/generate.py,2023-06-06T21:06:13Z,uncommented generate.py usage of finetuned weights
github.com/mikeybellissimo/LoRA-MPT,src/generate.py,2023-06-06T16:12:30Z,updated ReadME
github.com/mikeybellissimo/LoRA-MPT,src/generate.py,2023-06-05T17:42:19Z,Enabled it to be built as pip
github.com/mikeybellissimo/LoRA-MPT,src/upload_to_hub.py,2023-06-07T21:46:57Z,Got eval working correctly
github.com/mikeybellissimo/LoRA-MPT,src/export_state_dict_checkpoint.py,2023-06-05T17:42:19Z,Enabled it to be built as pip
github.com/linto-ai/linto-stt,whisper/stt/processing/load_model.py,2024-02-16T16:34:14Z,"Support more model formats, and add log about precision (ct2/faster_whisper)"
github.com/linto-ai/linto-stt,whisper/stt/processing/load_model.py,2023-11-30T16:49:13Z,fix coding style
github.com/linto-ai/linto-stt,whisper/stt/processing/load_model.py,2023-11-29T17:49:00Z,Isolate what is specific to Whisper in a folder
github.com/JD-P/minihf,vae_infer.py,2023-10-22T04:56:01Z,Add classifier free guidance
github.com/JD-P/minihf,vae_infer.py,2023-10-13T02:25:04Z,Fix samplers so they generate good prose
github.com/JD-P/minihf,vae_infer.py,2023-10-11T13:03:32Z,Update training code and add tuned AdaVAE samplers
github.com/JD-P/minihf,vae_infer.py,2023-10-03T14:46:42Z,Quick fix indices for default bigvae generation
github.com/JD-P/minihf,vae_infer.py,2023-10-03T10:54:08Z,Make default model Mistral
github.com/JD-P/minihf,vae_infer.py,2023-10-03T10:48:59Z,Add BigVAE inference code
github.com/JD-P/minihf,lora_tune.py,2023-06-14T08:13:43Z,Clear CUDA cache when tuning reward model
github.com/JD-P/minihf,lora_tune.py,2023-06-05T23:16:06Z,"Tune in 8-bit, with gradient checkpointing, on multiple GPUs"
github.com/JD-P/minihf,lora_tune.py,2023-06-05T23:14:33Z,Make inference server actually use tuned model
github.com/JD-P/minihf,lora_tune.py,2023-06-05T13:23:54Z,Add ability to lora tune evaluator model
github.com/JD-P/minihf,minihf_infer.py,2023-12-10T13:01:13Z,Add 1st draft of rewrite button
github.com/JD-P/minihf,minihf_infer.py,2023-12-02T16:17:53Z,"Fix node updates on edit, make base roll settings control gen"
github.com/JD-P/minihf,minihf_infer.py,2023-11-27T23:22:32Z,Add AdaVAE support
github.com/JD-P/minihf,minihf_infer.py,2023-11-03T20:19:40Z,Switch default model over to Mistral
github.com/JD-P/minihf,minihf_infer.py,2023-07-19T05:00:26Z,Add generator LoRA support
github.com/JD-P/minihf,minihf_infer.py,2023-07-14T20:56:59Z,Use RiversHaveWings/minihf_evaluator_openllama_7b as evaluator
github.com/JD-P/minihf,minihf_infer.py,2023-07-12T07:29:54Z,Generate using batch size 1
github.com/JD-P/minihf,minihf_infer.py,2023-07-04T18:07:49Z,"Use OpenLLaMA 7B as generator, LoRA of it as evaluator"
github.com/JD-P/minihf,minihf_infer.py,2023-07-03T08:03:46Z,Make /generate produce multiple gens based on weave_beam_width
github.com/JD-P/minihf,minihf_infer.py,2023-07-01T03:06:37Z,Add toggle to use weave gen or manual loom
github.com/JD-P/minihf,minihf_infer.py,2023-06-29T07:29:06Z,"Expose ability to do normal gen, misc fixes

- Separate reroll button into crayon (normal gen) and pen (weave)
- Fix bug where generation from root node didn't work
- Change rendering decision where nodes without scores were shown as 100% probable"
github.com/JD-P/minihf,minihf_infer.py,2023-06-25T14:53:39Z,Expose weave hyperparams and fix editor for root node
github.com/JD-P/minihf,minihf_infer.py,2023-06-20T15:03:37Z,Refactor MiniHF client data structure to be a tree
github.com/JD-P/minihf,minihf_infer.py,2023-06-14T08:13:43Z,Clear CUDA cache when tuning reward model
github.com/JD-P/minihf,minihf_infer.py,2023-06-13T08:00:11Z,Fix reward model loading
github.com/JD-P/minihf,minihf_infer.py,2023-06-13T01:08:32Z,Fix context handling bugs in evaluator
github.com/JD-P/minihf,minihf_infer.py,2023-06-05T23:14:33Z,Make inference server actually use tuned model
github.com/JD-P/minihf,minihf_infer.py,2023-06-05T13:23:54Z,Add ability to lora tune evaluator model
github.com/JD-P/minihf,minihf_infer.py,2023-06-04T07:59:57Z,Merge in Crowson's weave algorithm and zero-shot evaluation
github.com/JD-P/minihf,minihf_infer.py,2023-05-28T10:21:21Z,Don't try to open directories in a zip as files
github.com/JD-P/minihf,minihf_infer.py,2023-05-28T05:06:59Z,Fix reward head tuning on Mac OS X
github.com/JD-P/minihf,minihf_infer.py,2023-05-27T09:12:07Z,Fix bug where only new text segment is stored during expansion
github.com/JD-P/minihf,minihf_infer.py,2023-05-27T02:38:07Z,Fix overflow in fp16 when computing mean embedding during training
github.com/JD-P/minihf,minihf_infer.py,2023-05-26T07:48:30Z,Exclude padding tokens from mean embedding when training reward models
github.com/JD-P/minihf,minihf_infer.py,2023-05-26T06:11:10Z,Create reward_heads directory during training if not exists
github.com/JD-P/minihf,minihf_infer.py,2023-05-26T05:34:35Z,Add ability to train reward head with zip upload within interface
github.com/JD-P/minihf,minihf_infer.py,2023-05-25T23:29:12Z,Make the MLP actually an MLP
github.com/JD-P/minihf,minihf_infer.py,2023-05-25T19:55:44Z,Initial commit
github.com/JD-P/minihf,find_ae_scale.py,2023-10-05T22:37:09Z,Add ae_scale finder
github.com/JD-P/minihf,sft_generator.py,2023-07-26T03:16:08Z,Decode text files read from user datasets
github.com/JD-P/minihf,sft_generator.py,2023-07-11T05:00:00Z,Change default SFT generator quantization to 4 bits
github.com/JD-P/minihf,sft_generator.py,2023-07-11T03:55:02Z,Actually load the (roughly) 10 megabyte minimum data for finetune
github.com/JD-P/minihf,sft_generator.py,2023-07-10T13:00:23Z,First draft of generator supervised finetune script
github.com/JD-P/minihf,sft_evaluator.py,2023-07-14T20:56:59Z,Use RiversHaveWings/minihf_evaluator_openllama_7b as evaluator
github.com/JD-P/minihf,sft_evaluator.py,2023-07-12T00:52:15Z,Rename make_evaluator.py to sft_evaluator.py
github.com/JD-P/minihf,rlaif_generator.py,2023-11-24T22:10:51Z,Fix gradient checkpointing in RLAIF script
github.com/JD-P/minihf,rlaif_generator.py,2023-07-26T21:54:40Z,Add multi-gpu RLAIF tuning
github.com/JD-P/minihf,rlaif_generator.py,2023-07-21T22:24:47Z,Add RLAIF tuning
github.com/JD-P/minihf,train_vae_router.py,2023-10-03T10:54:08Z,Make default model Mistral
github.com/JD-P/minihf,train_vae_router.py,2023-10-03T10:48:59Z,Add BigVAE inference code
github.com/JD-P/minihf,train_vae_router.py,2023-09-29T17:59:56Z,Add MoE VAE training code
github.com/JD-P/minihf,train_vae_overlap.py,2023-10-11T13:03:32Z,Update training code and add tuned AdaVAE samplers
github.com/JD-P/minihf,train_vae_overlap.py,2023-10-03T10:54:08Z,Make default model Mistral
github.com/JD-P/minihf,train_vae_overlap.py,2023-10-03T10:48:59Z,Add BigVAE inference code
github.com/JD-P/minihf,train_vae_overlap.py,2023-09-29T17:59:56Z,Add MoE VAE training code
github.com/JD-P/minihf,rlaif_generator_dpo.py,2023-12-02T16:26:33Z,Use evaluator logit scale argument
github.com/JD-P/minihf,rlaif_generator_dpo.py,2023-12-02T16:14:39Z,Add the Direct Preference Optimization RLAIF generator script
github.com/DRSY/EMO,continual_finetuning/merge_lora.py,2023-10-22T07:14:27Z,update code structure
github.com/khaimt/qa_expert,train/merge_weight.py,2023-11-12T16:50:13Z,upload reame and fix some minor things
github.com/khaimt/qa_expert,train/merge_weight.py,2023-11-03T01:59:29Z,change added_tokens --> add_special_tokens
github.com/khaimt/qa_expert,train/merge_weight.py,2023-10-13T16:16:53Z,update readme and inference
github.com/khaimt/qa_expert,train/merge_weight.py,2023-10-12T13:30:20Z,change the structure
github.com/jayli/langchain-ChatGLM,models/chatglm_llm.py,2023-05-27T19:21:21Z,add all
github.com/jayli/langchain-ChatGLM,models/chatglm_llm.py,2023-05-27T05:32:34Z,add all
github.com/jayli/langchain-ChatGLM,models/chatglm_llm.py,2023-05-27T04:23:27Z,init commit
github.com/jayli/langchain-ChatGLM,models/loader/loader.py,2023-05-27T04:23:27Z,init commit
github.com/seudl/JurisLMs,ailawyer/demo/generate.py,2023-06-16T08:06:05Z,fix
github.com/seudl/JurisLMs,ailawyer/demo/generate.py,2023-05-20T10:21:13Z,模型推理和部署
github.com/seudl/JurisLMs,ailawyer/demo/web_demo_llama_13B.py,2023-05-20T10:21:13Z,模型推理和部署
github.com/seudl/JurisLMs,ailawyer/utils/merge_llama_with_chinese_lora_to_hf.py,2023-05-30T16:38:31Z,add data and test
github.com/debjitpaul/refiner,src/refiner.py,2023-04-24T20:57:14Z,add lora
github.com/debjitpaul/refiner,src/refiner.py,2023-04-24T17:26:42Z,added new code
github.com/DemoGit4LIANG/Chat2Anything,Chat2Anything/fastchat/model/apply_lora.py,2023-08-04T05:19:14Z,v0.1
github.com/brando90/ultimate-utils,tutorials_for_myself/my_hf_hugging_face_pg/opt/opt_6.7b/finetune_opt_bnb_peft.py,2023-02-16T01:16:12Z,opt 6.7B fine tuning
github.com/grasses/PromptCARE,soft_prompt/training/trainer.py,2023-11-28T16:39:26Z,add new version
github.com/LC1332/Luotuo-QA,app/infer.py,2023-04-24T14:43:01Z,add continue_generate
github.com/LC1332/Luotuo-QA,app/infer.py,2023-04-22T13:32:22Z,add app/infer.ipynb
github.com/LC1332/Luotuo-QA,app/infer.py,2023-04-22T03:59:33Z,clean app code
github.com/declare-lab/red-instruct,starling_training/fastchat/model/apply_lora.py,2023-08-22T02:03:59Z,add codes and data
github.com/declare-lab/red-instruct,starling_training/fastchat/model/model_adapter.py,2023-08-22T02:03:59Z,add codes and data
github.com/YeonwooSung/ai_book,LLMs/llama2/llama2-fine-tune/merge_peft_adapters.py,2024-03-02T12:08:04Z,feat: Add scripts for finetuning llama-2
github.com/lighttransport/japanese-llama-experiment,sandbox/eval_checkpoint.py,2023-10-31T19:03:34Z,Pass tokenized inputs object.
github.com/lighttransport/japanese-llama-experiment,sandbox/eval_checkpoint.py,2023-10-31T17:15:33Z,eval from checkpoint weights.
github.com/lighttransport/japanese-llama-experiment,sandbox/eval_checkpoint.py,2023-10-28T11:41:53Z,update incr pretrain script.
github.com/lighttransport/japanese-llama-experiment,10_incremental_pretrain/publish_checkpoint.py,2023-11-05T17:38:05Z,hf -> llama.cpp GGUF convert.
github.com/lighttransport/japanese-llama-experiment,10_incremental_pretrain/publish_checkpoint.py,2023-11-05T13:17:48Z,Publish script for checkpoint data.
github.com/lizhongyi123/llama2_chat_fine,merge.py,2023-11-09T09:50:15Z,redme
github.com/lizhongyi123/llama2_chat_fine,merge.py,2023-11-08T17:15:08Z,1
github.com/lizhongyi123/llama2_chat_fine,inference/model_utils.py,2023-11-08T17:15:08Z,1
github.com/pengwei-iie/Llama2-Chinese,examples/chat_gradio_no_merge.py,2023-10-18T15:20:13Z,message
github.com/pengwei-iie/Llama2-Chinese,train/merge_peft_model/merge_peft_adapter.py,2023-10-18T15:20:13Z,message
github.com/pengwei-iie/Llama2-Chinese,train/merge_peft_model/merge_muilt_peft_adapter.py,2023-10-18T15:20:13Z,message
github.com/ssbuild/deep_training,src/deep_training/trainer/hf/trainer.py,2023-11-13T09:25:18Z,"fix hf trainer

Signed-off-by: ssbuild <462304@qq.cn>"
github.com/ssbuild/deep_training,src/deep_training/trainer/hf/trainer.py,2023-10-10T06:25:40Z,"fix baichuan gradient_checkpointing

Signed-off-by: ssbuild <462304@qq.cn>"
github.com/ssbuild/deep_training,src/deep_training/trainer/hf/trainer.py,2023-10-10T03:18:39Z,"update

Signed-off-by: ssbuild <462304@qq.cn>"
github.com/ssbuild/deep_training,src/deep_training/trainer/hf/trainer.py,2023-10-10T02:26:08Z,"fix some new bug

Signed-off-by: ssbuild <462304@qq.cn>"
github.com/ssbuild/deep_training,src/deep_training/trainer/hf/trainer.py,2023-10-09T11:40:49Z,"fix hf and acc

Signed-off-by: ssbuild <462304@qq.com>"
github.com/ssbuild/deep_training,src/deep_training/trainer/hf/trainer.py,2023-09-27T00:52:35Z,"fix hf trainer save min weight

Signed-off-by: ssbuild <462304@qq.cn>"
github.com/ssbuild/deep_training,src/deep_training/trainer/hf/trainer.py,2023-09-26T16:48:27Z,"update

Signed-off-by: ssbuild <462304@qq.com>"
github.com/ssbuild/deep_training,src/deep_training/trainer/hf/trainer.py,2023-09-26T16:28:17Z,"fix hf trainer

Signed-off-by: ssbuild <462304@qq.com>"
github.com/ssbuild/deep_training,src/deep_training/trainer/hf/trainer.py,2023-09-26T06:04:24Z,"0.2.4

Signed-off-by: ssbuild <462304@qq.cn>"
github.com/ssbuild/deep_training,src/deep_training/trainer/hf/trainer.py,2023-09-26T04:26:54Z,"support hf

Signed-off-by: ssbuild <462304@qq.cn>"
github.com/ssbuild/deep_training,src/deep_training/trainer/hf/trainer.py,2023-09-26T02:41:16Z,"support for hf trainer

Signed-off-by: ssbuild <462304@qq.cn>"
github.com/ssbuild/deep_training,src/deep_training/trainer/hf/trainer.py,2023-09-26T01:28:53Z,"update hf

Signed-off-by: ssbuild <462304@qq.cn>"
github.com/ssbuild/deep_training,src/deep_training/trainer/hf/trainer.py,2023-09-25T16:36:48Z,"support hf

Signed-off-by: ssbuild <462304@qq.com>"
github.com/ssbuild/deep_training,src/deep_training/trainer/hf/trainer.py,2023-09-25T09:20:27Z,"update

Signed-off-by: ssbuild <462304@qq.cn>"
github.com/ssbuild/deep_training,src/deep_training/trainer/hf/trainer.py,2023-08-23T04:03:02Z,"0.2.0

Signed-off-by: ssbuild <462304@qq.cn>"
github.com/ssbuild/deep_training,src/deep_training/trainer/hf/trainer.py,2023-07-31T06:53:06Z,"internal change

Signed-off-by: ssbuild <462304@qq.cn>"
github.com/ssbuild/deep_training,src/deep_training/nlp/models/petl/lora/model.py,2023-11-13T04:26:07Z,"0.2.8

Signed-off-by: ssbuild <462304@qq.cn>"
github.com/ssbuild/deep_training,src/deep_training/nlp/models/petl/lora/model.py,2023-11-13T01:40:21Z,"0.3.0 rc

Signed-off-by: ssbuild <462304@qq.cn>"
github.com/AbaciNLP/InvestLM,inference.py,2023-09-14T13:53:40Z,create project
github.com/cauyxy/bilivideos,llm-train/inference.py,2023-06-22T14:31:28Z,finish llm-fintunig
github.com/cauyxy/bilivideos,llm-train/merge_weights.py,2023-06-22T14:31:28Z,finish llm-fintunig
github.com/avocardio/Zicklein,predict.py,2023-05-20T22:01:24Z,"Added scripts, data, sources and assets"
github.com/avocardio/Zicklein,generate.py,2023-05-20T22:01:24Z,"Added scripts, data, sources and assets"
github.com/avocardio/Zicklein,export_hf_checkpoint.py,2023-05-20T22:01:24Z,"Added scripts, data, sources and assets"
github.com/avocardio/Zicklein,export_state_dict_checkpoint.py,2023-05-20T22:01:24Z,"Added scripts, data, sources and assets"
github.com/xufangzhi/Symbol-LLM,demo-webui/modules/LoRA.py,2024-01-02T07:57:53Z,update
github.com/zhiweihu1103/AgriMa,src/llmtuner/model/adapter.py,2024-02-21T11:07:25Z,Add files via upload
github.com/SYSU-MUCFC-FinTech-Research-Center/ZhiLu,scripts/merge_llama_with_chinese_lora_low_mem.py,2023-10-28T03:56:16Z,Update merge_llama_with_chinese_lora_low_mem.py
github.com/SYSU-MUCFC-FinTech-Research-Center/ZhiLu,scripts/merge_llama_with_chinese_lora_low_mem.py,2023-10-28T03:18:20Z,add merge-scripts
github.com/ZetangForward/Detox-CoT,train_cmd/peft_model_clm.py,2024-02-20T07:03:40Z,cmd
github.com/ZetangForward/Detox-CoT,utils/continuation_inference.py,2024-02-20T07:03:40Z,cmd
github.com/ZetangForward/Detox-CoT,utils/continuation_inference.py,2023-11-09T14:41:45Z,add span-cnn
github.com/huangb23/VTimeLLM,vtimellm/model/builder.py,2024-01-01T13:28:42Z,vtimellm_chatglm training
github.com/huangb23/VTimeLLM,vtimellm/model/builder.py,2023-12-04T15:32:58Z,inference
github.com/runpod/serverless-workers,workers/Dolly-Tuner/rp_handler.py,2023-04-23T22:56:03Z,no pre-loaded model
github.com/runpod/serverless-workers,workers/Dolly-Tuner/rp_handler.py,2023-04-22T18:46:15Z,Update rp_handler.py
github.com/runpod/serverless-workers,workers/Dolly-Tuner/rp_handler.py,2023-04-22T18:44:25Z,Update rp_handler.py
github.com/runpod/serverless-workers,workers/Dolly-Tuner/rp_handler.py,2023-04-22T17:27:09Z,upload trained model when done
github.com/runpod/serverless-workers,workers/Dolly-Tuner/rp_handler.py,2023-04-22T17:01:29Z,Update rp_handler.py
github.com/runpod/serverless-workers,workers/Dolly-Tuner/rp_handler.py,2023-04-22T16:56:01Z,Update rp_handler.py
github.com/runpod/serverless-workers,workers/Dolly-Tuner/rp_handler.py,2023-04-22T16:51:14Z,Update rp_handler.py
github.com/runpod/serverless-workers,workers/Dolly-Tuner/rp_handler.py,2023-04-22T16:43:35Z,feat: added Dolly-Tuner
github.com/hkust-nlp/PEM_composition,AlpacaLoRAcomposition/generate.py,2023-11-26T09:40:14Z,Release Alpaca-LoRA composition exps code
github.com/hkust-nlp/PEM_composition,AlpacaLoRAcomposition/generate_in_batch.py,2023-11-26T09:40:14Z,Release Alpaca-LoRA composition exps code
github.com/hkust-nlp/PEM_composition,AlpacaLoRAcomposition/export_hf_checkpoint.py,2023-11-26T09:40:14Z,Release Alpaca-LoRA composition exps code
github.com/hkust-nlp/PEM_composition,AlpacaLoRAcomposition/export_state_dict_checkpoint.py,2023-11-26T09:40:14Z,Release Alpaca-LoRA composition exps code
github.com/serp-ai/LLaMA-8bit-LoRA,merge_adapter_weights.py,2023-03-22T10:10:30Z,Replace automodel and tokenizer with llama version
github.com/serp-ai/LLaMA-8bit-LoRA,merge_adapter_weights.py,2023-03-20T22:23:47Z,Initial commit
github.com/weijiang2023/algmon-kb,script/fine_tune.py,2024-02-01T09:05:39Z,ADD script for download lookbooks
github.com/A-baoYang/LLM-Finetune-Guide,efficient-finetune/lora/serve/model.py,2023-05-01T04:12:58Z,[feature] Add Efficient Finetune Method: LoRA
github.com/A-baoYang/LLM-Finetune-Guide,efficient-finetune/lora/peftv2/peft_model.py,2024-02-07T02:59:40Z,[LLaMA2] Add lora for sft LLaMA2
github.com/A-baoYang/LLM-Finetune-Guide,efficient-finetune/lora/peftv2/tuners/lora.py,2024-02-07T02:59:40Z,[LLaMA2] Add lora for sft LLaMA2
github.com/A-baoYang/LLM-Finetune-Guide,efficient-finetune/lora/validation/generate.py,2023-05-02T10:08:46Z,[feature] Add LoRA validation code & cpp format convert code
github.com/A-baoYang/LLM-Finetune-Guide,cpp_format_convert/merge_llama_with_chinese_lora.py,2023-07-10T05:59:27Z,[feature] Arranged Chinese-LLaMA-Alpaca model merge method and llama.cpp ggjt format convert methods
github.com/A-baoYang/LLM-Finetune-Guide,cpp_format_convert/merge_llama_with_chinese_lora.py,2023-05-02T10:08:46Z,[feature] Add LoRA validation code & cpp format convert code
github.com/A-baoYang/LLM-Finetune-Guide,efficient-finetune/lora/validation/batch_generate.py,2023-08-27T11:05:54Z,[mix] Runnable updates
github.com/A-baoYang/LLM-Finetune-Guide,efficient-finetune/lora/validation/batch_generate.py,2023-05-02T10:08:46Z,[feature] Add LoRA validation code & cpp format convert code
github.com/A-baoYang/LLM-Finetune-Guide,efficient-finetune/lora/peftv2/tuners/prompt_tuning.py,2024-02-07T02:59:40Z,[LLaMA2] Add lora for sft LLaMA2
github.com/mu-cai/ViP-LLaVA,llava/model/builder.py,2023-12-03T23:54:12Z,Initial commit
github.com/XiPotatonium/chatbot-api,src/model/llama/__init__.py,2023-05-29T10:06:54Z,feat(blip2chatglm&chatglm): new generation code
github.com/XiPotatonium/chatbot-api,src/model/llama/__init__.py,2023-05-08T12:55:13Z,"Squashed commit of the following:

commit 614367c8518a697987ba25b6443de96f5c70c66d
Author: shwu <xipotatonium@outlook.com>
Date:   Mon May 8 12:53:33 2023 +0000

    doc

commit 668f5ce22b1de0f791e5f13cb5254ef04be7c4ea
Author: shwu <xipotatonium@outlook.com>
Date:   Thu Apr 20 18:00:04 2023 +0800

    feat: more lora models

    * InstructGLM
    * raw llama
    * fix api and more generation configs
    * new examples

commit 4dbc483dade0914514d0be8bf106e42dbbeb3688
Author: Yongliang Shen <syl@zju.edu.cn>
Date:   Wed Apr 19 20:59:17 2023 +0800

    update API

commit d197f51776d7dd00bdfb31a73846b0dca2a16cdb
Merge: 26919f1 f0816b3
Author: Yongliang Shen <syl@zju.edu.cn>
Date:   Tue Apr 18 20:13:58 2023 +0800

    Merge branch 'main' of https://github.com/XiPotatonium/chatbot-api into dev

commit 26919f1aa55a2d82a35f981f1932521279d99661
Author: shwu <xipotatonium@outlook.com>
Date:   Tue Apr 18 20:05:37 2023 +0800

    refactor: refactor ChatModel interface"
github.com/XiPotatonium/chatbot-api,src/model/llama/__init__.py,2023-04-08T16:59:27Z,feat: llama and chatglm
github.com/XiPotatonium/chatbot-api,src/model/chatglm/__init__.py,2023-05-29T10:06:54Z,feat(blip2chatglm&chatglm): new generation code
github.com/XiPotatonium/chatbot-api,src/model/chatglm/__init__.py,2023-05-08T12:55:13Z,"Squashed commit of the following:

commit 614367c8518a697987ba25b6443de96f5c70c66d
Author: shwu <xipotatonium@outlook.com>
Date:   Mon May 8 12:53:33 2023 +0000

    doc

commit 668f5ce22b1de0f791e5f13cb5254ef04be7c4ea
Author: shwu <xipotatonium@outlook.com>
Date:   Thu Apr 20 18:00:04 2023 +0800

    feat: more lora models

    * InstructGLM
    * raw llama
    * fix api and more generation configs
    * new examples

commit 4dbc483dade0914514d0be8bf106e42dbbeb3688
Author: Yongliang Shen <syl@zju.edu.cn>
Date:   Wed Apr 19 20:59:17 2023 +0800

    update API

commit d197f51776d7dd00bdfb31a73846b0dca2a16cdb
Merge: 26919f1 f0816b3
Author: Yongliang Shen <syl@zju.edu.cn>
Date:   Tue Apr 18 20:13:58 2023 +0800

    Merge branch 'main' of https://github.com/XiPotatonium/chatbot-api into dev

commit 26919f1aa55a2d82a35f981f1932521279d99661
Author: shwu <xipotatonium@outlook.com>
Date:   Tue Apr 18 20:05:37 2023 +0800

    refactor: refactor ChatModel interface"
github.com/XiPotatonium/chatbot-api,src/model/chatglm/__init__.py,2023-04-15T10:58:18Z,refactor: for new blip2chatglm
github.com/XiPotatonium/chatbot-api,src/model/chatglm/__init__.py,2023-04-11T12:16:26Z,feat: add model scheduling
github.com/XiPotatonium/chatbot-api,src/model/chatglm/__init__.py,2023-04-09T03:52:10Z,fix(iter_message):
github.com/XiPotatonium/chatbot-api,src/model/chatglm/__init__.py,2023-04-08T16:59:27Z,feat: llama and chatglm
github.com/XiPotatonium/chatbot-api,src/model/blip2chatglm/__init__.py,2023-05-29T10:06:54Z,feat(blip2chatglm&chatglm): new generation code
github.com/XiPotatonium/chatbot-api,src/model/blip2chatglm/__init__.py,2023-05-08T12:55:13Z,"Squashed commit of the following:

commit 614367c8518a697987ba25b6443de96f5c70c66d
Author: shwu <xipotatonium@outlook.com>
Date:   Mon May 8 12:53:33 2023 +0000

    doc

commit 668f5ce22b1de0f791e5f13cb5254ef04be7c4ea
Author: shwu <xipotatonium@outlook.com>
Date:   Thu Apr 20 18:00:04 2023 +0800

    feat: more lora models

    * InstructGLM
    * raw llama
    * fix api and more generation configs
    * new examples

commit 4dbc483dade0914514d0be8bf106e42dbbeb3688
Author: Yongliang Shen <syl@zju.edu.cn>
Date:   Wed Apr 19 20:59:17 2023 +0800

    update API

commit d197f51776d7dd00bdfb31a73846b0dca2a16cdb
Merge: 26919f1 f0816b3
Author: Yongliang Shen <syl@zju.edu.cn>
Date:   Tue Apr 18 20:13:58 2023 +0800

    Merge branch 'main' of https://github.com/XiPotatonium/chatbot-api into dev

commit 26919f1aa55a2d82a35f981f1932521279d99661
Author: shwu <xipotatonium@outlook.com>
Date:   Tue Apr 18 20:05:37 2023 +0800

    refactor: refactor ChatModel interface"
github.com/XiPotatonium/chatbot-api,src/model/blip2chatglm/__init__.py,2023-04-15T10:58:18Z,refactor: for new blip2chatglm
github.com/XiPotatonium/chatbot-api,src/model/blip2chatglm/__init__.py,2023-04-11T12:16:26Z,feat: add model scheduling
github.com/XiPotatonium/chatbot-api,src/model/blip2chatglm/__init__.py,2023-04-09T03:52:10Z,fix(iter_message):
github.com/XiPotatonium/chatbot-api,src/model/blip2chatglm/__init__.py,2023-04-08T16:59:27Z,feat: llama and chatglm
github.com/XiPotatonium/chatbot-api,src/model/blip2chatglm/__init__.py,2023-04-08T14:36:11Z,feat: basically ok
github.com/XiPotatonium/chatbot-api,src/model/blip2chatglm/__init__.py,2023-04-07T07:48:34Z,feat: connection ok
github.com/usail-hkust/LLMTSCS,finetune/merge_lora.py,2024-02-13T16:02:57Z,update LLMLight
github.com/invoke-ai/invoke-training,src/invoke_training/_shared/stable_diffusion/lora_checkpoint_utils.py,2024-01-29T15:20:35Z,Move _shared/ up a directory.
github.com/kukrishna/evidence_inspector,backend/serve_seq2seq_qlora.py,2023-08-28T02:35:35Z,added intro modal showing config of backend models
github.com/kukrishna/evidence_inspector,backend/serve_seq2seq_qlora.py,2023-08-25T04:10:15Z,initial commit
github.com/JuneYaooo/medical_kb_chatbot,finetune/pulse/src/trainer.py,2023-06-28T13:40:55Z,v0
github.com/JuneYaooo/medical_kb_chatbot,loader/models/loader/loader.py,2023-07-03T03:08:31Z,fix: input not on same device
github.com/JuneYaooo/medical_kb_chatbot,loader/models/loader/loader.py,2023-06-28T13:40:55Z,v0
github.com/opendatalab/HA-DPO,ha_dpo/models/llava-v1_5/train_dpo.py,2024-01-30T07:29:53Z,init commit
github.com/opendatalab/HA-DPO,ha_dpo/models/minigpt4/merge_peft_adapter.py,2024-01-30T07:29:53Z,init commit
github.com/opendatalab/HA-DPO,ha_dpo/models/llava-v1_5/llava/model/builder.py,2024-01-30T07:29:53Z,init commit
github.com/opendatalab/HA-DPO,ha_dpo/models/instructblip/merge_peft_adapter.py,2024-01-30T07:29:53Z,init commit
github.com/OpenMOSS/Say-I-Dont-Know,src/Inference/infer_llama.py,2024-01-29T16:32:17Z,add training methods
github.com/OpenMOSS/Say-I-Dont-Know,src/llama_recipes/inference/model_utils.py,2024-01-29T16:32:17Z,add training methods
github.com/batmen-lab/BioMANIA,src/models/model.py,2024-01-27T23:01:43Z,"update openai version, fix details in chitchat"
github.com/batmen-lab/BioMANIA,src/models/model.py,2024-01-18T16:30:23Z,update instruction generation setting
github.com/batmen-lab/BioMANIA,src/models/model.py,2024-01-16T23:33:07Z,minor update for prompt and new libs
github.com/batmen-lab/BioMANIA,src/models/model.py,2023-12-06T15:56:44Z,minor update before v1.1.9
github.com/batmen-lab/BioMANIA,src/models/model.py,2023-12-01T14:00:43Z,v1.1.7 update
github.com/batmen-lab/BioMANIA,src/models/model.py,2023-11-06T13:27:51Z,add chat2jupyter for converting chat json into jupyter notebook
github.com/batmen-lab/BioMANIA,src/models/model.py,2023-11-03T14:45:20Z,add source code
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/machine_learning/hugging_face_peft_test.py,2023-03-23T02:48:15Z,"[Chore] hugging_face_test.py was moved from sw_dev/python/rnd/test/language_processing to sw_dev/python/rnd/test/machine_learning.
[Update] The examples for Hugging Face PEFT library were moved from sw_dev/python/rnd/test/machine_learning/hugging_face_test.py to sw_dev/python/rnd/test/machine_learning/hugging_face_peft_test.py."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2024-02-27T11:11:47Z,"[Update] An example for Gemma model was initially added.
[Update] An example for Transformer Reinforcement Learning (TRL) library to train small LLMs (sLLMs) was initially added."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2024-02-21T12:45:57Z,[Update] Examples for DINO and DPT models were implemented in hugging_face_transformers_test.py.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2024-02-16T06:16:19Z,[Update] Placeholders for examples of Hugging Face TRL library were added.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2024-02-08T03:52:31Z,"[Update] Several examples for stable diffusion models of Stability AI were implemented.
[Update] Information about Transformer Reinforcement Learning (TRL) library was moved from hugging_face_test.py to hugging_face_transformers_test.py."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2024-01-31T08:31:27Z,[Update] A test was added to fine-tune a ResNet model using Dog/Cat dataset in pytorch_transfer_learning.py.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2024-01-28T13:07:24Z,"[Update] An example of Zephyr-7B was added in hugging_face_transformers_test.py, but was not tested.
[Update] The information about Transformer Reinforcement Learning (TRL) was described in hugging_face_test.py."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2024-01-11T04:04:23Z,"[Update] An example for retrieval-augmented generation (RAG) was added in hugging_face_transformers_test.py.
[Update] Information about TableMASTER-mmocr was removed from mmocr_usage_guide.txt."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-12-24T02:40:32Z,"[Update] A few examples for Mistral-7B and Mixtral-8x7B models were added.
[Update] The installation of TensorFlow 2 were updated."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-12-23T12:07:02Z,"[Update] An example of ORCA-2 model was added.
[Update] The examples for ViT, ViLT, BEiT, LayoutLM, and Donut models were merged respectively.
[Update] The information about data preparation, training, evaluation, visualization, and model export were supplemented in paddle_ocr_usage_guide.txt."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-11-24T08:49:41Z,"[Update] A customized version of ViT model was implemented and tested, which doesn't have classification token and head."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-11-20T04:48:59Z,"[Update] A few examples for Falcon model were initially implemented.
[Update] A few examples for StarCoder and Replit models were implemented, but yet tested."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-11-10T01:27:50Z,[Update] LLMs Yi-6B & Yi-34B were tested.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-11-06T12:24:53Z,"[Update] The forecasts of transformers.TimeSeriesTransformerForPrediction were evaluated by MASE, MAPE, sMAPE.
[Update] Metrics for evaluating ML models' performance in evaluate library were tested."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-11-03T10:46:04Z,"[Update] Two examples for phi-1 & phi-1.5 models were initially added.
[Update] A few examples for Kosmos-2 model were initially added."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-10-12T12:13:43Z,[Update] A few examples for Probabilistic time series transformer model were added.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-10-07T08:51:44Z,[Update] Several examples for Perceiver IO model were initially committed.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-09-26T10:56:44Z,"[Update] A simple example for Code Llama model was initially added.
[Update] A few commands were explained to profile Python scripts."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-09-21T12:31:58Z,[Update] A simple example for OpenLLaMA models was implemented.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-09-04T03:59:25Z,"[New] A few examples for PID, MPC, LQR using python-control library was initially added.
[Update] A simple example for trajectory transformer models was added in hugging_face_transformers_test.py."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-09-02T12:30:02Z,"[New] An example for model predictive control (MPC) was initially committed.
[Update] A test for CodeParrot model was initially implemented in hugging_face_transformers_test.py."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-08-31T06:01:24Z,"[Update] Memory footprint and computing performance (FLOPS) Hugging Face transformers models was measured.
[Update] A simple test for Hugging Face datasets was added."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-08-04T08:13:21Z,[Update] An example for OpenFlamingo library was added.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-07-25T12:07:07Z,"[Update] A few examples for Llama 2 model were added in hugging_face_transformers_test.py.
[Update] Model parallelism was tested based on Hugging Face Accelerate library in hugging_face_transformers_test.py.
[Update] Information about Hugging Face Accelerate library was reinforced."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-07-22T06:48:12Z,"[Update] Cross references about transformer and ViT models were reinforced.
[Chore] vit_test.py was moved from sw_dev/python/rnd/test/machine_learning/vit_test.py to sw_dev/python/rnd/test/machine_vision/vit_test.py.
[Update] The installation of node.js was explained."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-07-17T01:20:26Z,"[Update] Several examples for CodeBERT, CodeBERTa, CodeT5+, CodeGen2, & CodeGen2.5 models were added.
[Update] An example of SpeechT5 model was divied into 3 examples: ASR, TTS, & speech-to-speech."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-07-12T07:54:25Z,"[Update] An example for Decision Transformer was initially committed.
[Update] A few examples for NVIDIA Megatron-LM, ASR, TTS models were implemented.
[Update] An example for SegFormer model was initially added."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-06-30T14:20:26Z,"[Update] The method, chain of thoughts (CoT) was tested on two LLMs, LLaMA & MPT."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-06-27T04:15:37Z,"[Update] A few examples for MPT & TVLT models were implemented in hugging_face_transformers_test.py.
[Chore] table_generation_usage_guide.txt was moved to SWLP repository."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-06-22T14:49:33Z,"[Update] Three language models were tested: LLaMA, Galactica, & OPT models."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-06-15T07:51:37Z,[Update] The example of LLaMA model was reinforced.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-05-26T18:49:16Z,[Update] A test for data parallelism was implemented in PyTorch library.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-05-05T03:23:29Z,"[Update] Two simple tests for two Facebook's language models, OPT and Galactica were initially implemented."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-04-30T14:34:59Z,[New] A simple tutorial for MMSegmentation library was initially committed.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-04-06T03:27:45Z,"[Update] A couple of simple examples for LLaMa model were implemented, but they were not tested due to library version issue."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-04-04T11:52:41Z,"[Update] Several examples for CodeT5 and CodeGen models were implemented to generate code.
[Update] Several examples for GIT and BLIP models for vision-and-language modeling models were implemented.
[Update] A few examples for TaPEx model were implemented to understand tables."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-04-01T14:10:48Z,"[New] A couple of examples for GPT4All models were added, but they were not correctly working."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-04-01T06:26:08Z,"[New] Two simple examples for PaLM and PaLM+RLHF models were implemented, but the models were not trained in the examples."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-30T08:06:47Z,"[New] Several examples for table processing (Table Transformer, TATR), OCR (TrOCR), speech processing (SpeechT5) were initially added to hugging_face_transformers_test.py."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-29T05:25:41Z,[Update] Several examples for speech recognition (whisper) & synthesis (tacotron2 & fastspeech2) were newly implemented in hugging_face_transformers_test.py.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-26T07:02:13Z,"[New] A few examples for ALIGN model were implemented in hugging_face_transformers_test.py.
[Update] A couple of examples for CLIP model were added in hugging_face_transformers_test.py.
[Update] Useful information about Transformer architectures was described."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-24T06:24:31Z,[Update] A simple example about dataclass in Python was added.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-22T14:48:31Z,"[Update] A couple of examples of seq-to-seq models for LoRA & Prefix Tuning were implemented in hugging_face_test.py.
[Update] An example for CLIP model was implemented in hugging_face_transformers_test.py.
[Update] A few examples for Whisper model were implemented in hugging_face_transformers_test.py.
[New] A usage guide for Hugging Face library was added."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-21T07:10:38Z,"[Update] A simple example of Parameter-Efficient Fine-Tuning (PEFT) library was added to hugging_face_test.py.
[Update] A simple test of tokenizers was added to hugging_face_transformers_test.py."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-18T11:41:29Z,[Update] A couple of examples for diffusion models of StabilityAI and CompVis were added.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-18T05:09:16Z,[Update] A few examples for Flan-T5 model were reinforced in hugging_face_transformers_test.py.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-16T11:52:13Z,[Update] Information about Hugging Face models was supplemented.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-15T12:08:24Z,[Update] An example for question answering using GPT-neo was added.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-14T02:43:49Z,[Update] A few examples for BLOOM and Flan-T5 models were implemented.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-09T01:51:43Z,[Update] A test for KLUE BERT models was implemented.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-09T01:49:32Z,"[Update] A few examples of text summarization for Korean & English were implemented, but their results were not good.
[Update] A few examples for T5 model were added."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-07T17:44:21Z,[Update] A few tests were added for GPT & BERT models.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-03-01T14:37:43Z,[New] A few guides were initially committed for Hugging Face Hub library.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-01-19T10:11:50Z,"[Update] Several examples for vision, vision and language models were added in HuggingFace library."
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2023-01-19T08:06:44Z,[Update] Several examples for LayoutLM & Donut models were implemented in HugggingFace library.
github.com/sangwook236/SWDT,sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py,2022-04-01T07:00:58Z,[Chore] transformers_test.py was renamed to hugging_face_transformers_test.py.
github.com/liuqidong07/MOELoRA-peft,src/MLoRA/main.py,2023-10-13T08:43:26Z,commit the code
github.com/liuqidong07/MOELoRA-peft,src/MLoRA/main_offline.py,2023-10-13T08:43:26Z,commit the code
github.com/liuqidong07/MOELoRA-peft,src/MLoRA/peft/peft_model.py,2023-10-13T08:43:26Z,commit the code
github.com/liuqidong07/MOELoRA-peft,src/MLoRA/peft/tuners/prompt_tuning.py,2023-10-13T08:43:26Z,commit the code
github.com/liuqidong07/MOELoRA-peft,src/MLoRA/test.py,2023-10-13T08:43:26Z,commit the code
github.com/taishan1994/baichuan-Qlora-Tuning,predict.py,2023-06-22T11:44:42Z,commit
github.com/victor-iyi/rlhf-trl,scripts/evaluate.py,2023-06-14T07:37:34Z,Update README.md
github.com/victor-iyi/rlhf-trl,scripts/evaluate.py,2023-06-14T06:54:12Z,Remove default arguments
github.com/victor-iyi/rlhf-trl,scripts/evaluate.py,2023-06-13T22:48:24Z,Add training and evaluation scripts
github.com/SmithaUpadhyaya/fashion_image_caption,app.py,2023-06-27T13:37:38Z,Modify streamlit app and include demo gif file in README
github.com/SmithaUpadhyaya/fashion_image_caption,app.py,2023-06-26T16:35:58Z,Streamlit deploy code for HFSpaces
github.com/ChiYeungLaw/LLaMa-EasyFT,src/generate.py,2023-04-14T03:34:20Z,initial
github.com/yysirs/ChatDoc,models/moss_llm.py,2023-05-26T03:18:50Z,push code
github.com/yysirs/ChatDoc,models/chatglm_llm.py,2023-05-26T03:18:50Z,push code
github.com/hppRC/llm-lora-classification,src/models.py,2023-07-17T03:42:57Z,:tada: Initial commit
github.com/weiyifan1023/Neeko,src/model.py,2024-02-20T10:39:58Z,add moelora
github.com/weiyifan1023/Neeko,src/model.py,2024-01-21T06:24:27Z,first commit
github.com/weiyifan1023/Neeko,eval/infer.py,2024-02-21T08:52:02Z,eval
github.com/weiyifan1023/Neeko,moelora/peft_model.py,2024-02-20T10:39:58Z,add moelora
github.com/weiyifan1023/Neeko,moelora/tuners/prompt_tuning.py,2024-02-20T10:39:58Z,add moelora
github.com/hdong920/LESS,src/lm_eval/models/huggingface.py,2024-02-14T15:34:36Z,train and eval code
github.com/song-wx/SIFT,exp/mmlu/eval_mmlu.py,2023-12-18T13:37:16Z,first add exp
github.com/HanGuo97/lq-lora,run_oasst1.py,2023-11-21T01:42:48Z,initial
github.com/GCYZSL/MoLA,src/mola_lora_hacked.py,2024-02-13T15:31:45Z,MoLA
github.com/GCYZSL/MoLA,evaluation_scienceqa.py,2024-02-13T15:31:45Z,MoLA
github.com/GCYZSL/MoLA,src/mola_peft_model_hacked.py,2024-02-13T15:31:45Z,MoLA
github.com/zhiqix/NL2GQL,Smaller_LLM.py,2023-11-06T13:40:08Z,add final_version
github.com/wwxu21/CUT,merge.py,2023-12-22T07:47:13Z,src code
github.com/wwxu21/CUT,modeling_llama_unlikelihood.py,2024-01-09T05:46:35Z,fix batch size issue
github.com/wwxu21/CUT,modeling_llama_unlikelihood.py,2023-12-22T07:43:44Z,src code
github.com/taishan1994/PPO_Chinese_Generate,roberta-chinese.py,2023-04-24T10:38:40Z,commit
github.com/taishan1994/PPO_Chinese_Generate,roberta-chinese.py,2023-04-24T09:51:25Z,commit
github.com/morecry/CharacterChat,model/merge_lora.py,2023-08-20T09:37:24Z,First Commit
github.com/rjmacarthy/quintus,src/inference/local/model.py,2023-05-18T14:05:57Z,fix folders
github.com/rjmacarthy/quintus,src/inference/local/model.py,2023-05-18T14:05:21Z,gitignore cache
github.com/rjmacarthy/quintus,src/inference/local/model.py,2023-05-18T14:03:19Z,rename lib t src
github.com/rjmacarthy/quintus,src/inference/local/model.py,2023-04-13T09:40:10Z,add pip support
github.com/rjmacarthy/quintus,src/inference/local/model.py,2023-04-12T15:59:29Z,add main.py for openai inference
github.com/wuhy68/Parameter-Efficient-MoE,merge_moe_lora.py,2024-02-18T02:44:39Z,merge
github.com/Yiwei98/TDG,code/peft_NAT/peft_model.py,2024-01-10T13:26:06Z,add dataset
github.com/Yiwei98/TDG,code/peft_NAT/tuners/lora.py,2024-01-10T13:26:06Z,add dataset
github.com/Yiwei98/TDG,code/generate_batch_adaptive.py,2024-01-10T13:26:06Z,add dataset
github.com/Yiwei98/TDG,code/peft_NAT/tuners/prompt_tuning.py,2024-01-10T13:26:06Z,add dataset
github.com/mlrun/functions,huggingface_auto_trainer/huggingface_auto_trainer.py,2023-08-27T10:00:42Z,"[huggingface-auto-trainer] add llm fine tuner (#651)

* huggingface-auto-trainer

* after yoni's review

* Update huggingface_auto_trainer/item.yaml

Co-authored-by: Yonatan Shelach <92271540+yonishelach@users.noreply.github.com>

* fixes for demo

---------

Co-authored-by: Yonatan Shelach <92271540+yonishelach@users.noreply.github.com>"
github.com/Percent-BFD/neurips_submission,neurips_submission_1/helper.py,2023-10-27T06:10:09Z,fist commit
github.com/Percent-BFD/neurips_submission,neurips_submission_2/helper.py,2023-10-27T06:10:09Z,fist commit
github.com/LLM360/Analysis360,eval/harness/lm_eval/models/huggingface.py,2024-02-06T15:19:23Z,fixes
github.com/VITA-Group/LLaGA,model/builder.py,2024-02-12T06:04:00Z,init commit
github.com/Q-Future/Q-Align,q_align/model/builder.py,2024-01-06T18:02:50Z,lora fine-tune on one_align enabled
github.com/Q-Future/Q-Align,q_align/model/builder.py,2024-01-03T11:42:52Z,quick fix
github.com/Q-Future/Q-Align,q_align/model/builder.py,2023-12-20T15:30:24Z,bump
github.com/flurryunicorn/virtualgf-gpt,opendan-text-generation-webui/modules/LoRA.py,2023-06-07T05:52:40Z,update LLM Server
github.com/insightbuilder/python_de_learners_data,code_script_notebooks/projects/exploring_modal/openLlama_onModal.py,2023-06-04T10:00:05Z,ready for samantha
github.com/huggingface/optimum-habana,examples/text-generation/utils.py,2024-02-23T08:29:15Z,Enable internal kv bucket in llama (#720)
github.com/huggingface/optimum-habana,examples/text-generation/utils.py,2024-02-19T15:43:10Z,Enable torch_compile mode for distributed (#659)
github.com/huggingface/optimum-habana,examples/text-generation/utils.py,2024-02-03T01:53:30Z,Bring back workaround for Falcon with SynapseAI 1.13 (#685)
github.com/huggingface/optimum-habana,examples/text-generation/utils.py,2024-01-25T19:01:23Z,Upgrade to Synapse 1.14 (#664)
github.com/huggingface/optimum-habana,examples/text-generation/utils.py,2024-01-24T19:02:28Z,"Fix error in PR#654 (#661)

* Falcon changes for 1.14.0

1) fix for invalid input shape with DS 0.12.4
2) revert --skip_hash_with_views changes
3) add falcon to model_on_meta()

* address comments

* README change

* make PR#654 backward compatible for other models

* address comments"
github.com/huggingface/optimum-habana,examples/text-generation/utils.py,2024-01-24T01:14:37Z,"Hqt (#648)

* enable HQT"
github.com/huggingface/optimum-habana,examples/text-generation/utils.py,2024-01-24T01:00:52Z,"Falcon changes for v1.14.0 release (#654)

* Falcon changes for 1.14.0

1) fix for invalid input shape with DS 0.12.4
2) revert --skip_hash_with_views changes
3) add falcon to model_on_meta()

* address comments

* README change"
github.com/huggingface/optimum-habana,examples/text-generation/utils.py,2024-01-23T18:23:59Z,Run Llama2 with torch.compile on Gaudi2 (#616)
github.com/huggingface/optimum-habana,examples/text-generation/utils.py,2023-12-29T15:32:51Z,Update generation config to enable flash attention for inference (#609)
github.com/huggingface/optimum-habana,examples/text-generation/utils.py,2023-12-18T08:40:56Z,Dyn prompt after refactor (#543)
github.com/huggingface/optimum-habana,examples/text-generation/utils.py,2023-12-11T13:46:44Z,Refactoring LLama Attention and mlp layers (#589)
github.com/huggingface/optimum-habana,examples/text-generation/utils.py,2023-12-08T09:08:31Z, Fix hash_with_views error (#587)
github.com/huggingface/optimum-habana,examples/text-generation/utils.py,2023-11-27T18:44:57Z,Add fallback for PEFT when the base model doesn't exist (#557)
github.com/huggingface/optimum-habana,examples/text-generation/utils.py,2023-11-23T16:03:06Z,Add Llama2 fp8 inference (#542)
github.com/huggingface/optimum-habana,examples/text-generation/utils.py,2023-11-21T08:32:41Z,Automate skip_hash_with_views for text generation with Falcon (#544)
github.com/huggingface/optimum-habana,examples/text-generation/utils.py,2023-11-20T20:14:30Z,Fix loading on meta device for PEFT models with DS-inference (#528)
github.com/huggingface/optimum-habana,examples/text-generation/utils.py,2023-11-18T15:29:27Z,Add hash_with_views arg for Falcon inference perf (#534)
github.com/huggingface/optimum-habana,examples/text-generation/utils.py,2023-11-17T19:05:03Z,Refactor run generation (#523)
github.com/huggingface/optimum-habana,examples/trl/merge_peft_adapter.py,2024-01-16T11:55:53Z,Restructure example/trl/stack_llama_2 for generic DPO  (#635)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2024-02-14T07:06:32Z,To fix LLAMA-V2-70B-FT-HF (8x) for eager mode (#709)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2024-02-12T06:34:09Z,"Upgrade to Transformers 4.37 (#651)

Co-authored-by: Sayantan Sarkar <sasarkar@habana.ai>
Co-authored-by: Libin Tang <litang@habana.ai>
Co-authored-by: Jimin Ha <jha@habana.ai>
Co-authored-by: Yeonsil Yoon <yyoon@habana.ai>
Co-authored-by: Sayantan Sarkar <supersarkar@gmail.com>"
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2024-01-23T18:09:33Z,Add changes to support FSDP (#598)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2024-01-03T18:11:00Z,Adding support for bf16_full_eval (#610)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-12-29T15:24:37Z,Fix crash if gaudi_config is not passed to GaudiTrainer (#613)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-12-19T08:15:17Z,"In peft, only the trainable parameters need to be saved (#576)"
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-12-14T09:06:11Z,Integrate Habana flash attention to Llama2-70B finetune (#596)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-12-12T10:42:29Z,Support for FlashAttention in Llama2 (#584)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-11-30T22:58:23Z,Upgrade to SynapseAI 1.13 (#563)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-11-30T21:54:03Z,Add workaround for GPT2 gaudi config for autocast
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-11-24T13:23:44Z,Remove HMP from optimum-habana (#349)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-11-21T21:23:00Z,Fix save DS scheduler (#549)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-11-17T09:32:40Z,Add infra to enable/disable dynamic shapes feature through gaudi_config (#513)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-11-16T16:48:52Z,"Fix for attn_softmax_bf16 when generation_config is None (#531)

* Enable llama attention softmax in bf16

* apply attn_softmax_bf16 only for llama

* separte 2 if blocks

* Fix CI tests error from DummyModel

* Fix for attn_softmax_bf16 when generation_config is None

* Fix typo"
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-11-15T17:24:39Z,Enable llama attention softmax in bf16 (#521)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-11-07T09:04:34Z,Upgrade to Transformers 4.34 (#475)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-11-02T16:42:05Z,Add maximum hpugraphs and disable_tensor_cache arguments to GaudiTrainer (#493)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-10-26T12:02:26Z,Enable/disable gradient_checkpointing as per training_args.gradient_checkpointing value (#484)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-10-21T19:17:11Z,Update GPT2-XL notebook (#481)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-10-09T07:55:39Z,Remove `sharded_ddp` (#456)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-09-27T17:35:39Z,No need to wrap DDP when using Fast DDP (#430)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-09-25T08:09:08Z,Makes the record_shapes of the profiler changeable. (#418)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-09-19T20:03:11Z,Upgrade to Transformers 4.33 (#412)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-09-12T17:10:39Z,Add support for autocast custom ops in `GaudiTrainer` (#308)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-08-24T20:09:11Z,Upgrade to Accelerate v0.22 (#362)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-08-23T09:09:30Z,Upgrade to Transformers v4.32 (#354)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-08-17T09:43:58Z,Upgrade to Synapse 1.11 (#333)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-08-13T09:36:01Z,Upgrade to Transformers v4.31 (#312)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-07-28T01:02:16Z,"Revert ""Upgrade to Transformers v4.31"""
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-07-26T08:40:17Z,Fix tests
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-07-25T10:37:54Z,Make style
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-07-25T10:09:19Z,Add support for DeepSpeed
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-07-24T16:41:50Z,[skip ci] Update `GaudiTrainer` and `GaudiTrainingArguments`
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-06-28T22:08:12Z,Add Bridgetower example (#283)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-06-22T15:28:08Z,Enable/Optimize flan t5 xxl on deepspeed z3 (#257)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-06-16T17:03:57Z,Add support for HPU graphs for training and Fast DDP (#200)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-06-16T06:48:42Z,Improve DeepSpeed support (#272)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-06-15T20:20:25Z,Add test and doc for Torch Autocast (#269)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-06-14T07:09:11Z,Enable usage of PyTorch autocast on Gaudi during training (#226)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-06-07T23:21:46Z,Fix profiling in GaudiTrainer (#260)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-06-06T20:58:57Z,Added an option to remove log/save/evaluate time from throughput calculation (#237)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-05-30T21:45:29Z,Enable profiling (#250)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-04-26T08:19:22Z,Enable asynchronous data copy to get a better performance (#211)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-04-17T17:21:44Z,Upgrade to SynapseAI 1.9.0 (#193)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-04-15T19:12:59Z,Add mark_step between fwd and bwd for better performance (#189)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-04-14T23:06:09Z,Upgrade to Transformers 4.28 (#202)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-03-16T09:58:57Z,Update to Transformers 4.27 (#181)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-03-09T16:57:07Z,Enable HPU graphs for distributed runs and generation (#179)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-03-05T22:37:31Z,Add copyrights (#180)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-02-25T17:59:07Z,Improve data sampling for training in lazy mode (#152)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-02-25T10:14:52Z,Deprecate the usage of `hmp_opt_level` rather than remove it (#173)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-02-22T08:08:12Z,Remove O1/O2 in Gaudi config (#169)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-02-12T22:05:20Z,Update CI baseline for speech recognition (#164)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-02-08T22:54:34Z,Update to SynapseAI v1.8.0 (#160)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-02-08T08:22:36Z,Reformat files (#161)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-02-01T17:48:10Z,Update the CLM and MLM examples (#156)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-01-31T20:53:29Z,Add support for inference through HPU graphs in GaudiTrainer (#151)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2023-01-24T22:29:00Z,Update to Transformers 4.26 (#143)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2022-12-29T22:22:24Z,Add support for multi-node training (#116)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2022-12-13T00:25:01Z,Enable DeepSpeed activation checkpointing (#142)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2022-12-10T10:38:03Z,Update to Diffusers 0.10.0 (#139)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2022-12-02T10:14:44Z,Update to Transformers 4.25.1 (#136)
github.com/huggingface/optimum-habana,optimum/habana/transformers/trainer.py,2022-11-30T23:06:04Z,Add support for Stable Diffusion (#131)
github.com/huggingface/optimum-habana,optimum/habana/trl/trainer/dpo_trainer.py,2024-02-12T06:34:09Z,"Upgrade to Transformers 4.37 (#651)

Co-authored-by: Sayantan Sarkar <sasarkar@habana.ai>
Co-authored-by: Libin Tang <litang@habana.ai>
Co-authored-by: Jimin Ha <jha@habana.ai>
Co-authored-by: Yeonsil Yoon <yyoon@habana.ai>
Co-authored-by: Sayantan Sarkar <supersarkar@gmail.com>"
github.com/huggingface/optimum-habana,optimum/habana/trl/trainer/dpo_trainer.py,2024-01-09T14:18:03Z,Fix dpo graph compile error in evaluation (#630)
github.com/huggingface/optimum-habana,optimum/habana/trl/trainer/dpo_trainer.py,2023-12-25T23:59:12Z,"add DPO and SFT of TRL support in Gaudi and example (#601)

* add DPO and SFT of TRL support in Gaudi and example

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

* upgrade SFTTrainer/DPO trainer and stack_llama_2 example to v0.7.6

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

---------

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>"
github.com/huggingface/optimum-habana,optimum/habana/trl/trainer/sft_trainer.py,2024-02-12T06:34:09Z,"Upgrade to Transformers 4.37 (#651)

Co-authored-by: Sayantan Sarkar <sasarkar@habana.ai>
Co-authored-by: Libin Tang <litang@habana.ai>
Co-authored-by: Jimin Ha <jha@habana.ai>
Co-authored-by: Yeonsil Yoon <yyoon@habana.ai>
Co-authored-by: Sayantan Sarkar <supersarkar@gmail.com>"
github.com/huggingface/optimum-habana,optimum/habana/trl/trainer/sft_trainer.py,2024-01-09T14:18:03Z,Fix dpo graph compile error in evaluation (#630)
github.com/huggingface/optimum-habana,optimum/habana/trl/trainer/sft_trainer.py,2023-12-25T23:59:12Z,"add DPO and SFT of TRL support in Gaudi and example (#601)

* add DPO and SFT of TRL support in Gaudi and example

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

* upgrade SFTTrainer/DPO trainer and stack_llama_2 example to v0.7.6

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

---------

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>"
github.com/vermouthdky/SimTeG,src/model/lms/lm_modeling.py,2023-07-01T03:08:11Z,"tem save, update with tape"
github.com/vermouthdky/SimTeG,src/model/lms/lm_modeling.py,2023-06-23T05:20:49Z,tem save
github.com/vermouthdky/SimTeG,src/model/lms/lm_modeling.py,2023-06-09T08:01:51Z,tem save
github.com/vermouthdky/SimTeG,src/model/lms/lm_modeling.py,2023-06-07T12:02:23Z,tem save
github.com/vermouthdky/SimTeG,src/model/lms/lm_modeling.py,2023-05-17T08:50:12Z,tem save
github.com/vermouthdky/SimTeG,src/model/lms/lm_modeling.py,2023-05-17T05:27:29Z,tem save
github.com/vermouthdky/SimTeG,src/model/lms/lm_modeling.py,2023-05-09T09:24:02Z,optimize optuna
github.com/vermouthdky/SimTeG,src/model/lms/lm_modeling.py,2023-05-08T07:00:00Z,tem save
github.com/vermouthdky/SimTeG,src/model/lms/lm_modeling.py,2023-04-26T03:29:00Z,tem save
github.com/vermouthdky/SimTeG,src/model/lms/lm_modeling.py,2023-04-21T06:55:16Z,tem save
github.com/vermouthdky/SimTeG,src/model/lms/lm_modeling.py,2023-03-30T03:26:12Z,add huggingface trainer
github.com/vermouthdky/SimTeG,src/model/lms/lm_modeling.py,2023-03-10T07:28:34Z,tem save
github.com/vermouthdky/SimTeG,src/model/lms/lm_modeling.py,2023-03-08T06:54:19Z,reformat train.sh
github.com/vermouthdky/SimTeG,src/model/lms/lm_modeling.py,2023-03-05T03:09:49Z,tem save
github.com/vermouthdky/SimTeG,src/model/lms/link_lm_modeling.py,2023-07-01T03:08:11Z,"tem save, update with tape"
github.com/vermouthdky/SimTeG,src/model/lms/link_lm_modeling.py,2023-06-13T06:43:16Z,tem save
github.com/vermouthdky/SimTeG,src/model/lms/link_lm_modeling.py,2023-06-07T12:02:23Z,tem save
github.com/vermouthdky/SimTeG,src/model/lms/link_lm_modeling.py,2023-05-29T09:50:42Z,tem save
github.com/vermouthdky/SimTeG,src/model/lms/link_lm_modeling.py,2023-05-24T05:51:56Z,link prediction task
github.com/huu4ontocord/MDEL,lora-x/experimental/qlora_lomo.py,2023-08-20T16:33:22Z,Rename lora-x/qlora_lomo.py to lora-x/experimental/qlora_lomo.py
github.com/jmhessel/caption_contest_corpus,llama2/inference.py,2023-09-02T05:24:32Z,updated inference to account for prompt delim arg
github.com/jmhessel/caption_contest_corpus,llama2/inference.py,2023-08-31T21:40:28Z,"v0.1, initial commit"
github.com/XiPotatonium/chatbot-webui,modules/model/llama/__init__.py,2023-05-08T12:52:12Z,refactor: better chat history
github.com/XiPotatonium/chatbot-webui,modules/model/llama/__init__.py,2023-04-15T03:52:20Z,refactor(blip2zh): new blip2zh modeling
github.com/XiPotatonium/chatbot-webui,modules/model/llama/__init__.py,2023-03-31T06:32:26Z,feat: chatgpt support
github.com/XiPotatonium/chatbot-webui,modules/model/llama/__init__.py,2023-03-29T10:44:07Z,"Squashed commit of the following:

commit 65dc9d36bba4c2b3c4e39f6a5003f2fcb5db2037
Author: shwu <xipotatonium@outlook.com>
Date:   Wed Mar 29 10:43:35 2023 +0000

    refactor: do not use history"
github.com/XiPotatonium/chatbot-webui,modules/model/llama/__init__.py,2023-03-28T06:11:01Z,feat(model & ui): support stream
github.com/XiPotatonium/chatbot-webui,modules/model/llama/__init__.py,2023-03-25T12:35:33Z,feat(ui): simplify ui
github.com/XiPotatonium/chatbot-webui,modules/model/llama/__init__.py,2023-03-22T12:30:37Z,"feat(all): llama and belle

* add llama and belle
* use configs
* refactor history"
github.com/XiPotatonium/chatbot-webui,modules/model/blip2chatglm/__init__.py,2023-05-08T12:52:12Z,refactor: better chat history
github.com/XiPotatonium/chatbot-webui,modules/model/blip2chatglm/__init__.py,2023-04-15T03:52:20Z,refactor(blip2zh): new blip2zh modeling
github.com/XiPotatonium/chatbot-webui,modules/model/blip2chatglm/__init__.py,2023-03-29T10:44:07Z,"Squashed commit of the following:

commit 65dc9d36bba4c2b3c4e39f6a5003f2fcb5db2037
Author: shwu <xipotatonium@outlook.com>
Date:   Wed Mar 29 10:43:35 2023 +0000

    refactor: do not use history"
github.com/XiPotatonium/chatbot-webui,modules/model/blip2chatglm/__init__.py,2023-03-28T13:25:42Z,feat: blip2chatglm
github.com/feizc/Visual-ChatGLM,fine_tune.py,2023-03-21T09:23:03Z,basic frame
github.com/feizc/Visual-ChatGLM,peft/peft_model.py,2023-03-21T09:23:03Z,basic frame
github.com/feizc/Visual-ChatGLM,peft/tuners/lora.py,2023-03-21T09:23:03Z,basic frame
github.com/feizc/Visual-ChatGLM,peft/tuners/prompt_tuning.py,2023-03-21T09:23:03Z,basic frame
github.com/yuhuixu1993/qa-lora,peft_utils.py,2023-10-08T09:00:45Z,更新 peft_utils.py
github.com/yuhuixu1993/qa-lora,peft_utils.py,2023-10-05T16:47:48Z,fixing lint issues
github.com/yuhuixu1993/qa-lora,peft_utils.py,2023-10-02T00:24:38Z,"更新 peft_utils.py

fix bugs"
github.com/yuhuixu1993/qa-lora,peft_utils.py,2023-10-01T21:33:52Z,Add missing parenthesis in peft_utils.py
github.com/yuhuixu1993/qa-lora,peft_utils.py,2023-09-27T03:26:25Z,Add files via upload
github.com/alexrame/rewardedsoups,llama/utils/ppo_utils.py,2023-06-08T07:20:10Z,First commit
github.com/alexrame/rewardedsoups,llama/utils/inference_utils.py,2023-06-08T07:20:10Z,First commit
github.com/mlrun/demo-llm-tuning,src/serving.py,2023-07-12T06:06:21Z,Updated model to falcon (#8)
github.com/mlrun/demo-llm-tuning,src/serving.py,2023-06-07T11:44:12Z,code documentation and formatting
github.com/mlrun/demo-llm-tuning,src/serving.py,2023-05-24T20:53:56Z,moving from cluster to new repo
github.com/X-PLUG/Multi-LLM-Agent,GLPFT/utils/model_adapter.py,2024-02-01T04:01:17Z,v0
github.com/X-PLUG/Multi-LLM-Agent,GLPFT/inference_utils/toolbench/inference.py,2024-02-01T04:01:17Z,v0
github.com/X-PLUG/Multi-LLM-Agent,GLPFT/inference_utils/toolbench/infer_pipeline.py,2024-02-01T04:01:17Z,v0
github.com/X-PLUG/Multi-LLM-Agent,GLPFT/inference_utils/toolbench/inference_lora.py,2024-02-01T04:01:17Z,v0
github.com/X-PLUG/Multi-LLM-Agent,ToolBench-multiLLM/toolbench/model/model_adapter.py,2024-02-01T04:01:17Z,v0
github.com/X-PLUG/Multi-LLM-Agent,ToolBench-multiLLM/toolbench/inference/LLM/collab_agent_model.py,2024-02-01T04:01:17Z,v0
github.com/X-PLUG/Multi-LLM-Agent,ToolBench-multiLLM/toolbench/inference/LLM/tool_llama_lora_model.py,2024-02-01T04:01:17Z,v0
github.com/Jiuzhouh/PiVe,unified_verifier/inference.py,2023-05-21T11:09:37Z,Add files via upload
github.com/TIGER-AI-Lab/TIGERScore,tigerscore/finetune/trainer.py,2023-11-10T17:11:16Z,Clean Code
github.com/TIGER-AI-Lab/TIGERScore,tigerscore/finetune/trainer.py,2023-10-23T02:14:34Z,"update Readme
y"
github.com/TIGER-AI-Lab/TIGERScore,tigerscore/finetune/test_llama.py,2023-11-10T17:11:16Z,Clean Code
github.com/TIGER-AI-Lab/TIGERScore,tigerscore/finetune/test_llama.py,2023-10-23T02:14:34Z,"update Readme
y"
github.com/ntropy-network/enrichment_models,enrichment_models/llms/llama.py,2023-05-31T22:00:32Z,publish benchmark
github.com/DeepLink-org/AIChipBenchmark,llm/Alpaca-Lora/trainer.py,2023-09-01T06:01:30Z,Add tgs
github.com/princeton-nlp/QuRating,training/train_language_model.py,2024-02-27T19:41:08Z,Add training scripts and instructions
github.com/princeton-nlp/QuRating,training/train_preference_model.py,2024-02-27T19:41:08Z,Add training scripts and instructions
github.com/princeton-nlp/QuRating,eval/lm-evaluation-harness/lm_eval/models/huggingface.py,2024-02-16T02:43:46Z,"lm-evaluation-harness v0.3.0 with minor modifications (e.g., use modeling_flash_llama)"
github.com/Enderfga/FineRewards,fastchat/model/apply_lora.py,2023-05-30T13:20:29Z,FineRewards
github.com/AdaCheng/EgoThink,models/test_llava.py,2023-12-18T08:41:44Z,update code
github.com/xxm1668/ChatGLM-Efficient-LORA,src/inference_rm.py,2023-06-19T02:32:00Z,first commit
github.com/xxm1668/ChatGLM-Efficient-LORA,src/inference_sft.py,2023-06-19T02:32:00Z,first commit
github.com/xxm1668/ChatGLM-Efficient-LORA,src/inference_sft_deepspeed.py,2023-06-19T02:32:00Z,first commit
github.com/Jiacheng-Zhu-AIML/AsymmetryLoRA,LoRASYM_peft/local_lorasym_all.py,2024-02-26T05:00:11Z,Keep initializing thigns
github.com/Jiacheng-Zhu-AIML/AsymmetryLoRA,LoRASYM_peft/local_lorasym_all.py,2024-02-26T03:49:31Z,initializing the files
github.com/Jiacheng-Zhu-AIML/AsymmetryLoRA,LoRASYM_peft/local_peft_model_all.py,2024-02-26T05:00:11Z,Keep initializing thigns
github.com/Jiacheng-Zhu-AIML/AsymmetryLoRA,LoRASYM_peft/local_peft_model_all.py,2024-02-26T03:49:31Z,initializing the files
github.com/WuNein/vllm4mteb,run_array_decoder_lora.py,2023-11-26T03:01:41Z,add vllm
github.com/WuNein/vllm4mteb,run_array_decoder_lora.py,2023-11-26T02:58:05Z,init
github.com/WuNein/vllm4mteb,run_array_decoder_qwen.py,2024-01-18T07:23:00Z,fix
github.com/WuNein/vllm4mteb,run_array_decoder_qwen.py,2023-11-26T03:01:41Z,add vllm
github.com/WuNein/vllm4mteb,run_array_decoder_qwen.py,2023-11-26T02:58:05Z,init
github.com/WuNein/vllm4mteb,run_array_decoder_vllm.py,2023-11-26T03:01:41Z,add vllm
github.com/UX-Decoder/LLaVA-Grounding,llava/model/builder.py,2023-12-04T17:01:16Z,first commit
github.com/UX-Decoder/LLaVA-Grounding,llava/eval/LLaVA_G_Eval.py,2023-12-04T19:08:11Z,Demo-related code. And readme.
github.com/IvanaXu/TianChiProj,2023.07-1.CCKS2023_1/KnowLM-main/examples/generate_lora.py,2023-07-21T10:27:55Z,2023.07 #38
github.com/IvanaXu/TianChiProj,2023.07-1.CCKS2023_1/KnowLM-main/examples/generate_lora_web.py,2023-07-21T10:27:55Z,2023.07 #38
github.com/IvanaXu/TianChiProj,2023.07-1.CCKS2023_1/KnowLM-main/tools/export_hf_checkpoint.py,2023-07-21T10:27:55Z,2023.07 #38
github.com/IvanaXu/TianChiProj,2023.07-1.CCKS2023_1/EasyInstruct-main/easyinstruct/engines/llama_engine.py,2023-07-21T10:27:55Z,2023.07 #38
github.com/IvanaXu/TianChiProj,2023.07-5.CCKS2023_PromptCBLUE_12/PromptCBLUE-main/src/ft_chatglm_lora/main.py,2023-07-18T07:31:03Z,2023.07-5.CCKS2023_PromptCBLUE_12
github.com/IvanaXu/TianChiProj,2023.07-5.CCKS2023_PromptCBLUE_12/PromptCBLUE-main/src/ft_chatglm_lora/peft/peft_model.py,2023-07-18T07:31:03Z,2023.07-5.CCKS2023_PromptCBLUE_12
github.com/IvanaXu/TianChiProj,2023.07-5.CCKS2023_PromptCBLUE_12/PromptCBLUE-main/src/ft_chatglm_lora/peft/tuners/prompt_tuning.py,2023-07-18T07:31:03Z,2023.07-5.CCKS2023_PromptCBLUE_12
github.com/mindspore-lab/mindnlp,mindnlp/peft/peft_model.py,2023-12-15T13:14:48Z,Tuning `gpt_bigcode-santacoder` with peft lora (#805)
github.com/mindspore-lab/mindnlp,mindnlp/peft/peft_model.py,2023-09-14T01:57:42Z,"add PEFT:LORA support (#657)

Co-authored-by: cjl99 <cj2559@163.com>"
github.com/mindspore-lab/mindnlp,mindnlp/peft/peft_model.py,2023-08-21T11:56:50Z,"Basic PEFT support. (#633)

Co-authored-by: cjl99 <cj2559@163.com>"
github.com/mindspore-lab/mindnlp,mindnlp/peft/peft_model.py,2023-07-13T10:52:36Z,add 'peft' dir & download files to current work dir (#600)
github.com/mindspore-lab/mindnlp,mindnlp/peft/tuners/lora.py,2023-12-03T16:02:37Z,"add mt5 model & rename 'gamma, beta, embedding_table' to 'weight, bias' (#768)"
github.com/mindspore-lab/mindnlp,mindnlp/peft/tuners/lora.py,2023-10-18T12:40:59Z,refactor models to transformers for huggingface compatible (#698)
github.com/mindspore-lab/mindnlp,mindnlp/peft/tuners/lora.py,2023-09-14T01:57:42Z,"add PEFT:LORA support (#657)

Co-authored-by: cjl99 <cj2559@163.com>"
github.com/mindspore-lab/mindnlp,mindnlp/peft/tuners/lora.py,2023-08-21T11:56:50Z,"Basic PEFT support. (#633)

Co-authored-by: cjl99 <cj2559@163.com>"
github.com/mindspore-lab/mindnlp,mindnlp/peft/tuners/lora.py,2023-07-13T10:52:36Z,add 'peft' dir & download files to current work dir (#600)
github.com/jyFengGoGo/InstructDet,modules/llava/llava/model/builder.py,2024-01-30T02:22:06Z,"release code,dataset"
github.com/jyFengGoGo/InstructDet,modules/fastchat/fastchat/model/apply_lora.py,2024-01-30T02:22:06Z,"release code,dataset"
github.com/DLCV-BUAA/TinyLLaVABench,tinyllava/model/builder.py,2024-02-25T11:18:45Z,fix typos when load 2.0B model
github.com/DLCV-BUAA/TinyLLaVABench,tinyllava/model/builder.py,2024-02-23T10:27:04Z,add eval code
github.com/PrasannS/rlhf-length-biases,scripts/dpo_eval.py,2023-11-29T18:06:26Z,"Lots of new robustness, reliability experiments."
github.com/PrasannS/rlhf-length-biases,scripts/generate_outs.py,2024-02-16T16:11:37Z,"New analysis, code cleanup"
github.com/PrasannS/rlhf-length-biases,scripts/generate_outs.py,2024-01-29T12:29:31Z,"Lots more code, contrastive dataset setup"
github.com/PrasannS/rlhf-length-biases,scripts/generate_outs.py,2024-01-17T15:18:22Z,lots of new stuff
github.com/PrasannS/rlhf-length-biases,scripts/generate_outs.py,2023-12-25T17:34:37Z,"More updates, ready to do some more eval"
github.com/PrasannS/rlhf-length-biases,scripts/generate_outs.py,2023-12-25T14:46:03Z,Lots more analysis / updates
github.com/PrasannS/rlhf-length-biases,scripts/generate_outs.py,2023-12-12T17:26:24Z,Many new rollout / analysis goodies
github.com/PrasannS/rlhf-length-biases,scripts/generate_outs.py,2023-11-16T22:53:02Z,More RM stuff
github.com/PrasannS/rlhf-length-biases,scripts/generate_outs.py,2023-10-06T01:55:29Z,"""renamed to scripts"""
github.com/PrasannS/rlhf-length-biases,rm-attacks/debug_utils.py,2023-08-23T17:11:26Z,Analysis experiments
github.com/PrasannS/rlhf-length-biases,scripts/ultra_cross_eval.py,2023-11-29T18:06:26Z,"Lots of new robustness, reliability experiments."
github.com/PrasannS/rlhf-length-biases,scripts/ultra_cross_eval.py,2023-11-16T22:53:02Z,More RM stuff
github.com/PrasannS/rlhf-length-biases,scripts/ultra_cross_eval.py,2023-11-14T15:32:09Z,Scoring code
github.com/PrasannS/rlhf-length-biases,scripts/ultra_cross_eval.py,2023-11-06T17:37:22Z,"further analysis, set up synthetic experiment"
github.com/PrasannS/rlhf-length-biases,scripts/stack_cross_eval.py,2023-11-04T05:12:28Z,Change magnitude default behaviour
github.com/PrasannS/rlhf-length-biases,scripts/stack_cross_eval.py,2023-10-23T20:13:47Z,Stack cross-rm evaluation code
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/rmcode.py,2024-02-16T16:11:37Z,"New analysis, code cleanup"
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/rmcode.py,2024-01-29T12:29:31Z,"Lots more code, contrastive dataset setup"
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/rmcode.py,2024-01-17T15:18:22Z,lots of new stuff
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/rmcode.py,2023-12-10T03:00:40Z,"Multi-checkpoint/customizable rollouts, DPOvsRM"
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/rmcode.py,2023-11-29T18:06:26Z,"Lots of new robustness, reliability experiments."
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/rmcode.py,2023-11-16T22:53:02Z,More RM stuff
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/rmcode.py,2023-11-14T15:32:09Z,Scoring code
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/rmcode.py,2023-11-06T17:37:22Z,"further analysis, set up synthetic experiment"
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/rmcode.py,2023-11-04T05:12:28Z,Change magnitude default behaviour
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/rmcode.py,2023-10-23T20:13:47Z,Stack cross-rm evaluation code
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/rmcode.py,2023-10-23T16:29:56Z,"Moing files around a bit, RM analysis"
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/rmcode.py,2023-10-23T01:52:49Z,Working RM code with absolute magnitudes
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/rmcode.py,2023-10-22T21:44:44Z,"Code for ultra-feedback data, magnitude loss"
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/rmcode.py,2023-10-05T22:04:29Z,Remove unnecessary code
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/rmcode.py,2023-09-09T20:27:38Z,truncation logic added
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/rmcode.py,2023-09-09T04:23:15Z,Data Carto Setup / Analysis
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/rmcode.py,2023-09-07T18:39:05Z,"RM code, RL Fix"
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/rmcode.py,2023-09-06T20:19:08Z,Code unification logic RLHF
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/rmcode.py,2023-09-01T18:37:48Z,"More training jobs, apf setup"
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/rmcode.py,2023-08-30T21:11:51Z,Small adjustments
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/rmcode.py,2023-08-30T14:10:27Z,"More experiments, TFRM stuff"
github.com/PrasannS/rlhf-length-biases,scripts/merge_peft_adapter.py,2023-10-23T20:13:47Z,Stack cross-rm evaluation code
github.com/PrasannS/rlhf-length-biases,scripts/merge_peft_adapter.py,2023-10-06T01:55:29Z,"""renamed to scripts"""
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/debug_utils.py,2024-01-21T18:07:08Z,more updates
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/debug_utils.py,2024-01-17T15:18:22Z,lots of new stuff
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/debug_utils.py,2023-11-29T18:06:26Z,"Lots of new robustness, reliability experiments."
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/debug_utils.py,2023-11-16T22:53:02Z,More RM stuff
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/debug_utils.py,2023-11-14T15:32:09Z,Scoring code
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/debug_utils.py,2023-09-06T16:04:36Z,"More RM code, more  PPO runs"
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/debug_utils.py,2023-09-01T18:37:48Z,"More training jobs, apf setup"
github.com/PrasannS/rlhf-length-biases,rlhfutils/rlhfutils/debug_utils.py,2023-08-31T02:29:37Z,Getting up some intrinsic stuff
github.com/andersonbcdefg/dpo-lora,models.py,2023-09-30T23:18:33Z,i refactored like everything to make DPO work with DDP...
github.com/andersonbcdefg/dpo-lora,models.py,2023-09-30T21:38:53Z,add ddp to see what happens
github.com/andersonbcdefg/dpo-lora,models.py,2023-09-30T19:08:49Z,fix quantization thing
github.com/andersonbcdefg/dpo-lora,models.py,2023-09-30T18:56:11Z,"add mistral, see if that works"
github.com/andersonbcdefg/dpo-lora,models.py,2023-09-29T21:34:10Z,first commit
github.com/Macielyoung/Baichuan-QLora,generation/generate.py,2023-07-09T14:43:49Z,update readme
github.com/Macielyoung/Baichuan-QLora,generation/generate.py,2023-07-02T07:44:54Z,add repo
github.com/Macielyoung/Baichuan-QLora,generation/qlora_eval.py,2023-07-02T07:44:54Z,add repo
github.com/Macielyoung/Baichuan-QLora,generation/multi_qlora_eval.py,2023-07-09T14:43:49Z,update readme
github.com/UCSC-VLAA/Sight-Beyond-Text,llava/model/builder.py,2023-09-13T23:59:00Z,initial commit
github.com/UCSC-VLAA/Sight-Beyond-Text,llava/eval/mmlu/mmlu_modeling.py,2023-09-13T23:59:00Z,initial commit
github.com/tmylla/HackMentor,chat.py,2023-07-05T15:14:59Z,"init commit, pre-hackmentor"
github.com/tmylla/HackMentor,eval/infer.py,2023-07-05T15:14:59Z,"init commit, pre-hackmentor"
github.com/tmylla/HackMentor,eval/ZenoEval/infer.py,2023-07-05T15:14:59Z,"init commit, pre-hackmentor"
github.com/tmylla/HackMentor,eval/ZenoEval/modeling.py,2023-07-05T15:14:59Z,"init commit, pre-hackmentor"
github.com/tmylla/HackMentor,train/fastchat/model/apply_lora.py,2023-09-30T17:16:34Z,code released & readme updated
github.com/claws-lab/XLingEval,correctness/MedAlpaca/inferer.py,2023-10-20T15:53:04Z,Add the correctness experiment files for MedAlpaca
github.com/claws-lab/XLingEval,consistency/Medalpaca/inferer.py,2023-10-22T21:59:45Z,Update Consistency on Medalpaca
github.com/claws-lab/XLingEval,consistency/Medalpaca/inferer.py,2023-10-22T20:53:15Z,Add Medalpaca for consistency experiments
github.com/claws-lab/XLingEval,verifiability/Medalpaca/inferer.py,2023-10-22T22:02:37Z,Add Medalpaca model to Verifiability
github.com/chuanyangjin/MMToM-QA,BIP-ALM/inverse_symbolic_planner.py,2024-01-11T22:52:47Z,Initial commit
github.com/Wusiwei0410/SciMMIR,src/LLM_models/MyLLaVA/builder.py,2024-01-25T06:03:58Z,add codes
github.com/Wusiwei0410/SciMMIR,src/LLM_models/Mymplug_owl2/builder.py,2024-01-25T06:03:58Z,add codes
github.com/intel/llm-on-ray,inference/deepspeed_predictor.py,2024-02-28T07:10:56Z,"Enable mllm api (#107)

* Necessary codes fixing and enable openai API in webui

Signed-off-by: Xue, Chendi <chendi.xue@intel.com>

* Fix ruff error

Signed-off-by: Xue, Chendi <chendi.xue@intel.com>

* add new MLLM predcitor to LLM-on-Ray

Signed-off-by: Xue, Chendi <chendi.xue@intel.com>

* Add langchain and openai test

Signed-off-by: Xue, Chendi <chendi.xue@intel.com>

* Fix formatting

Signed-off-by: Xue, Chendi <chendi.xue@intel.com>

* Fix issues per Carson's request

Signed-off-by: Xue, Chendi <chendi.xue@intel.com>

* MLLM predictor update per Carson's request

Signed-off-by: Xue, Chendi <chendi.xue@intel.com>

* Fix parsing issue

Signed-off-by: Xue, Chendi <chendi.xue@intel.com>

* Modifications for UI format

* Update codes per second review comments

Signed-off-by: Xue, Chendi <chendi.xue@intel.com>

* Add two example for inference with openai API and langchain API

Signed-off-by: Xue, Chendi <chendi.xue@intel.com>

* Add start_ui fixing

Signed-off-by: Xue, Chendi <chendi.xue@intel.com>

* using os env for OPENAI_API_KEY and OPENAI_BASE_URL

Signed-off-by: Xue, Chendi <chendi.xue@intel.com>

* update format

Signed-off-by: Xue, Chendi <chendi.xue@intel.com>

---------

Signed-off-by: Xue, Chendi <chendi.xue@intel.com>
Signed-off-by: Chendi.Xue <chendi.xue@intel.com>
Co-authored-by: KepingYan <keping.yan@intel.com>"
github.com/intel/llm-on-ray,inference/deepspeed_predictor.py,2024-02-07T02:11:55Z,"[Inference] Fix auth token and add models starcoder and llama2 (#39)

* add starcoder and enable llama2

* nit

* nit

* revert

* add token

* dedup

* add token to from_pretrained

* pass auth token to from_pretrained

* nit

* add auth tokens

* lint

* fix lint

* nit

* deepspeed not support starcoder

* nit

* remove from ci

* remove direct auth token

* add back ci workflow temporarily

* remove from ci

* add load environment and enable 2 models again

* add dir

* add load environment and enable 2 models again

* change proxy

* revert proxy

* change proxy

* revert proxy

* remove 2 models from ci

---------

Signed-off-by: Yizhong Zhang <zyzzxycj@163.com>"
github.com/intel/llm-on-ray,inference/deepspeed_predictor.py,2024-02-05T12:29:34Z,"Support vllm for openai api, refactor openai non-streaming api (#86)

* add vllm support for openai mode

* fix streaming response

* refactor openai call function

* fix tokens' length

* fix tokens' length for vllm

* run ci

* modify

* remove return_shape param, add data class

* modify

* modify

* address comments

* modify

* minimal modify after testing again"
github.com/intel/llm-on-ray,inference/deepspeed_predictor.py,2024-01-22T08:32:41Z,"[Serving] Support multiple prompts for generate (#62)

* support multiple prompts

* fix error response

* fix

* edit type

* update

* update"
github.com/intel/llm-on-ray,inference/deepspeed_predictor.py,2024-01-18T07:43:11Z,"Add vllm Predictor (#20)

* add vllm_predictor

* add tests skeleton

* add tests skeleton

* add pytest.ini

* wip

* complete, debug wip

* nit

* nit

* nit

* complete generate supporting str and List[str]

* add model

* add streaming

* remove tests

* Add install-vllm-cpu script

* nit

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>

* nit

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>

* nit

* fix package inference

* update install script and add doc

* nit

* nit

* nit

* add dtype support

* nit

* nit

* nit

* add ci

* nit

* nit

* add libpthread-stubs0-dev

* fix install-vllm-cpu

* fix

* revert inference.inference_config

* debug ci

* debug ci

* debug ci

* debug ci

* debug ci

* debug ci

* debug ci

* debug ci

* update

---------

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>"
github.com/intel/llm-on-ray,inference/deepspeed_predictor.py,2024-01-11T13:05:03Z,"[Lint] add lint (#34)

* add pre-commit, port from https://github.com/intel-sandbox/llm-ray/pull/185

* move lint check to github

* modify permission

* test

* test

* test

* test

* move to ubuntu-latest

* fix lint

* fix parameter error

* Recover lines that should not be deleted

* test lint in ci

* done

* add needs in ci

* move hpu tokenizer"
github.com/intel/llm-on-ray,inference/deepspeed_predictor.py,2024-01-09T08:45:21Z,"make ui demo independent (#29)

* make ui demo independent

* fix

* upd path in doc"
github.com/intel/llm-on-ray,inference/deepspeed_predictor.py,2023-12-21T06:05:27Z,"Sync with internal (cherry-picked from 644488 to 25118e3) (#9)

* merged [common] unified conf to yaml

Signed-off-by: Jiafu Zhang <jiafu.zhang@intel.com>

* reconstruct config by moving ipex and precision to ipex struct (#168)

* reconstruct config by moving ipex and precision to ipex struct

Signed-off-by: Jiafu Zhang <jiafu.zhang@intel.com>

* reconstruct config by moving ipex and precision to ipex struct

Signed-off-by: Jiafu Zhang <jiafu.zhang@intel.com>

---------

Signed-off-by: Jiafu Zhang <jiafu.zhang@intel.com>

* [Inference] Add Neural-chat inference support (#149)

* add neural chat inference

* change transformers from 4.31 to 4.35

* update prompt

* nit

* trigger ci

* remove from ci

* add auth token to all models

* revert

* merged [Inference] Add Neural-chat inference support

Signed-off-by: Jiafu Zhang <jiafu.zhang@intel.com>

* remove llama-2-7b in inferencce ci since ipex failed to optimize it

Signed-off-by: Jiafu Zhang <jiafu.zhang@intel.com>

* Add the HFTonkenizer patch for Model-References (#169)

Add the HFTokenizer patch
Add the pretrain_module to invoke different pretrain module

Signed-off-by: yuanwu <yuan.wu@intel.com>

* Abstract common features into Predictor (#166)

* fix bug of precess config; use tokenizer.__call__

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* init

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* create utils & move tokenizer to predictor

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* fix

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* change inferCfg to infer_conf; simplify code

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* fix

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* replace inferenceConfig with infer_conf

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* fix deepspeed

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* further simplify

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* move actor_options to utils

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* fix

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* remove input len

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* remove input len follow-up

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

---------

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* Update the Dockerfile.optimum.habana (#184)

Updathe the dockerfile
Fix the HABANA_VISIBLE_MODULES  envs issue

Signed-off-by: yuanwu <yuan.wu@intel.com>

* Update the Pretrain ReadME (#186)

Signed-off-by: yuanwu <yuan.wu@intel.com>

* Move the dockerfiles of pretrain into pretrain/docker (#187)

Delete the useless dp dockerfile

Change the nvidia GPU dockerfile name, because it use the same
dockerfile for both megatron-deepspeed and huggingface trainer

refactor the folder path of pretrain

Signed-off-by: yuanwu <yuan.wu@intel.com>

* renamed workflow_orders to workflow_orders_on_pr

Signed-off-by: Jiafu Zhang <jiafu.zhang@intel.com>

---------

Signed-off-by: Jiafu Zhang <jiafu.zhang@intel.com>
Signed-off-by: yuanwu <yuan.wu@intel.com>
Signed-off-by: Zhi Lin <zhi.lin@intel.com>
Co-authored-by: harborn <gangsheng.wu@intel.com>
Co-authored-by: Yizhong Zhang <zyzzxycj@163.com>
Co-authored-by: yuanwu2017 <yuan.wu@intel.com>
Co-authored-by: Zhi Lin <zhi.lin@intel.com>"
github.com/intel/llm-on-ray,inference/deepspeed_predictor.py,2023-11-30T05:36:40Z,"change the way to build project by using pyproject.toml (#143)

* change the way to build project by using pyproject.toml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* change the way to build project by using pyproject.toml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* change the way to build project by using pyproject.toml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* change the way to build project by using pyproject.toml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* change the way to build project by using pyproject.toml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* change the way to build project by using pyproject.toml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* remove unused import and variable

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

---------

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>"
github.com/intel/llm-on-ray,inference/deepspeed_predictor.py,2023-11-28T08:22:51Z,"add gaudi support for single process serving (#117)

* add gaudi support for single process serving

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* rollback deepspeed predictor & deal with hpu

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* optimize logic of ipex and hpu graph

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* add dockerfile and readme for habana

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* move import inside to avoid deepspeed import

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* fix deepspeed test

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* add comments

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* fix

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* add resource requirement for HPU

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* fix

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* change device_name to device.type

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* update base image

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* fix

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* fix model_id

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* deduplicate code

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* use_hpu_graphs default to True and print warning

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* import ipex in actor if ipex is enabled

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* add dep in Dockerfile

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* import when necessary

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

---------

Signed-off-by: Zhi Lin <zhi.lin@intel.com>"
github.com/intel/llm-on-ray,inference/deepspeed_predictor.py,2023-11-23T08:18:03Z,"refactor argument passing and model configs by using yaml (#127)

* refactoring

Signed-off-by: Jiafu Zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* debugging gpt-j-6b

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

---------

Signed-off-by: Jiafu Zhang <jiafu.zhang@intel.com>
Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>"
github.com/intel/llm-on-ray,inference/deepspeed_predictor.py,2023-11-16T07:34:03Z,"Ipex upgrade (#121)

* merge and update

* upd

* update asyncio.sleep time"
github.com/intel/llm-on-ray,inference/deepspeed_predictor.py,2023-11-15T01:27:18Z,"[Enhancement] Upgrade Ray from 2.5 to 3.0 dev and Remove RayDP (#112)

* Upgrade Ray from 2.5 to 3.0 dev

* remove raydp

* replace Checkpoint to TorchCheckpoint"
github.com/intel/llm-on-ray,inference/deepspeed_predictor.py,2023-11-14T02:02:03Z,"deltatuner integration (#87)

* add dependency

* enable delta model in common lib

* add sample best structure for deltatuner

* add workflow file

* update peft version

* update deltatuner version

* enable empty config for peft and deltatuner

* path fix

* add version in requirements file

* update params doc

* added models in the CICD configure file

* added the best model structures

* enable transformer predictor

* enable deepspeed predictor

* refine args

* update workflow file

* using latest deltatuner

* remove external best structure

* update best structure path

* empty matrix fix

* add default params

* update version

* fix workflow yml

* update version

* fix workflow yml

* fix workflow yml

* fix path in workflow yml

* fix workflow yml

* update version

* update version

* refine input model to peft model

* refine workflow yml

* remove tokenizer & add the condition on lora

* reduce workflow checks

* fix workflow yml

* fix regular exp

* fix eof

* fix eof

* fix format

* update models

* update models

---------

Co-authored-by: tianyil1 <tianyi.liu@intel.com>"
github.com/intel/llm-on-ray,inference/deepspeed_predictor.py,2023-11-13T03:20:15Z,"Remove Dialogue actor and refactor streaming (#111)

* remove dialogue actor and refactor streaming

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* add to_device

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* fix

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* fix deepspeed streaming

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* fix

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

---------

Signed-off-by: Zhi Lin <zhi.lin@intel.com>"
github.com/intel/llm-on-ray,inference/deepspeed_predictor.py,2023-11-09T02:02:13Z,"support gpu device (#71)

* support gpu device

* support gpu for inference

* set cpus_per_worker

* set gpus_per_worker

* support pvc in Inference"
github.com/intel/llm-on-ray,inference/deepspeed_predictor.py,2023-11-02T10:31:45Z,"Adapt the latest code for ui demo | add llama2 model (#73)

* Adapt the latest code for ui demo

* fix bug about progress bar

* modify trust_remote_code"
github.com/intel/llm-on-ray,inference/deepspeed_predictor.py,2023-11-01T08:03:43Z,"Fix deepspeed streaming_generate (#98)

* Fix deepspeed streaming_generate

* add dummy streamer

* nit

* nit

* add --streaming_response to CI

* rewrite DummyStreamer

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>

* nit

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>

* update

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>

* update

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>

* disable mpt

---------

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>"
github.com/intel/llm-on-ray,inference/deepspeed_predictor.py,2023-09-15T03:51:44Z,"Enable DeepSpeed for CPU inference (#42)

* Add deepspeed_inference.py

* update

* fix input

* update

* update scaling config

* Add streaming_generate

* Add DeepSpeedPredictor to model serve

* update README

* Add deepspeed_inference_example

* remove test

* fix return value bug

* Fix DeepSpeedPredictor

* fix generate

* remove examples and debug print

* remove *.sh

* Add deepspeed requirements

* enable DeepSpeed in CI

* update Dockerfile

* add device support

* update

* update

* update

* update

* update

* update

* update

* nit

* nit

* update

* add install oneapi and start-ray-cluster scripts

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>

* remove sudo for docker

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>

* fix path

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>

* nit

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>

* nit

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>

* nit

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>

* nit

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>

* nit

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>

* update

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>

* update deepspeed version

---------

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>"
github.com/intel/llm-on-ray,inference/transformer_predictor.py,2024-02-07T02:11:55Z,"[Inference] Fix auth token and add models starcoder and llama2 (#39)

* add starcoder and enable llama2

* nit

* nit

* revert

* add token

* dedup

* add token to from_pretrained

* pass auth token to from_pretrained

* nit

* add auth tokens

* lint

* fix lint

* nit

* deepspeed not support starcoder

* nit

* remove from ci

* remove direct auth token

* add back ci workflow temporarily

* remove from ci

* add load environment and enable 2 models again

* add dir

* add load environment and enable 2 models again

* change proxy

* revert proxy

* change proxy

* revert proxy

* remove 2 models from ci

---------

Signed-off-by: Yizhong Zhang <zyzzxycj@163.com>"
github.com/intel/llm-on-ray,inference/transformer_predictor.py,2024-02-05T12:29:34Z,"Support vllm for openai api, refactor openai non-streaming api (#86)

* add vllm support for openai mode

* fix streaming response

* refactor openai call function

* fix tokens' length

* fix tokens' length for vllm

* run ci

* modify

* remove return_shape param, add data class

* modify

* modify

* address comments

* modify

* minimal modify after testing again"
github.com/intel/llm-on-ray,inference/transformer_predictor.py,2024-01-22T08:32:41Z,"[Serving] Support multiple prompts for generate (#62)

* support multiple prompts

* fix error response

* fix

* edit type

* update

* update"
github.com/intel/llm-on-ray,inference/transformer_predictor.py,2024-01-18T07:43:11Z,"Add vllm Predictor (#20)

* add vllm_predictor

* add tests skeleton

* add tests skeleton

* add pytest.ini

* wip

* complete, debug wip

* nit

* nit

* nit

* complete generate supporting str and List[str]

* add model

* add streaming

* remove tests

* Add install-vllm-cpu script

* nit

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>

* nit

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>

* nit

* fix package inference

* update install script and add doc

* nit

* nit

* nit

* add dtype support

* nit

* nit

* nit

* add ci

* nit

* nit

* add libpthread-stubs0-dev

* fix install-vllm-cpu

* fix

* revert inference.inference_config

* debug ci

* debug ci

* debug ci

* debug ci

* debug ci

* debug ci

* debug ci

* debug ci

* update

---------

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>"
github.com/intel/llm-on-ray,inference/transformer_predictor.py,2024-01-11T13:05:03Z,"[Lint] add lint (#34)

* add pre-commit, port from https://github.com/intel-sandbox/llm-ray/pull/185

* move lint check to github

* modify permission

* test

* test

* test

* test

* move to ubuntu-latest

* fix lint

* fix parameter error

* Recover lines that should not be deleted

* test lint in ci

* done

* add needs in ci

* move hpu tokenizer"
github.com/intel/llm-on-ray,inference/transformer_predictor.py,2024-01-09T08:45:21Z,"make ui demo independent (#29)

* make ui demo independent

* fix

* upd path in doc"
github.com/intel/llm-on-ray,inference/transformer_predictor.py,2023-12-21T06:05:27Z,"Sync with internal (cherry-picked from 644488 to 25118e3) (#9)

* merged [common] unified conf to yaml

Signed-off-by: Jiafu Zhang <jiafu.zhang@intel.com>

* reconstruct config by moving ipex and precision to ipex struct (#168)

* reconstruct config by moving ipex and precision to ipex struct

Signed-off-by: Jiafu Zhang <jiafu.zhang@intel.com>

* reconstruct config by moving ipex and precision to ipex struct

Signed-off-by: Jiafu Zhang <jiafu.zhang@intel.com>

---------

Signed-off-by: Jiafu Zhang <jiafu.zhang@intel.com>

* [Inference] Add Neural-chat inference support (#149)

* add neural chat inference

* change transformers from 4.31 to 4.35

* update prompt

* nit

* trigger ci

* remove from ci

* add auth token to all models

* revert

* merged [Inference] Add Neural-chat inference support

Signed-off-by: Jiafu Zhang <jiafu.zhang@intel.com>

* remove llama-2-7b in inferencce ci since ipex failed to optimize it

Signed-off-by: Jiafu Zhang <jiafu.zhang@intel.com>

* Add the HFTonkenizer patch for Model-References (#169)

Add the HFTokenizer patch
Add the pretrain_module to invoke different pretrain module

Signed-off-by: yuanwu <yuan.wu@intel.com>

* Abstract common features into Predictor (#166)

* fix bug of precess config; use tokenizer.__call__

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* init

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* create utils & move tokenizer to predictor

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* fix

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* change inferCfg to infer_conf; simplify code

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* fix

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* replace inferenceConfig with infer_conf

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* fix deepspeed

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* further simplify

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* move actor_options to utils

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* fix

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* remove input len

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* remove input len follow-up

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

---------

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* Update the Dockerfile.optimum.habana (#184)

Updathe the dockerfile
Fix the HABANA_VISIBLE_MODULES  envs issue

Signed-off-by: yuanwu <yuan.wu@intel.com>

* Update the Pretrain ReadME (#186)

Signed-off-by: yuanwu <yuan.wu@intel.com>

* Move the dockerfiles of pretrain into pretrain/docker (#187)

Delete the useless dp dockerfile

Change the nvidia GPU dockerfile name, because it use the same
dockerfile for both megatron-deepspeed and huggingface trainer

refactor the folder path of pretrain

Signed-off-by: yuanwu <yuan.wu@intel.com>

* renamed workflow_orders to workflow_orders_on_pr

Signed-off-by: Jiafu Zhang <jiafu.zhang@intel.com>

---------

Signed-off-by: Jiafu Zhang <jiafu.zhang@intel.com>
Signed-off-by: yuanwu <yuan.wu@intel.com>
Signed-off-by: Zhi Lin <zhi.lin@intel.com>
Co-authored-by: harborn <gangsheng.wu@intel.com>
Co-authored-by: Yizhong Zhang <zyzzxycj@163.com>
Co-authored-by: yuanwu2017 <yuan.wu@intel.com>
Co-authored-by: Zhi Lin <zhi.lin@intel.com>"
github.com/intel/llm-on-ray,inference/transformer_predictor.py,2023-12-06T11:41:35Z,"integrate bigl-llm[cpu] (#150)

* change the way to build project by using pyproject.toml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* change the way to build project by using pyproject.toml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* change the way to build project by using pyproject.toml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* change the way to build project by using pyproject.toml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* change the way to build project by using pyproject.toml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* change the way to build project by using pyproject.toml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* remove unused import and variable

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* integrate bigdl-llm[cpu]

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* fix deepspeed path

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* fix deepspeed path

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* add ci for bigdl

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* add ci for bigdl

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* add ci for bigdl

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* add ci for bigdl

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* add ci for bigdl

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* add ci for bigdl

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* add ci for bigdl

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* add ci for bigdl

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* restore dict() method since deepspeed has lower pydantic version deps

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* separate Dockerfiles for ci efficiency

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* remove load_in_8bit since our GPU doesn't support bitsandbytes

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* remove ds_report and oneapi related commands in bigdl-cpu dockerfile

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* remove ds_report and oneapi related commands in bigdl-cpu dockerfile

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* escape inference with deepspeed for bigdl model

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* added hpus_per_worker to inference config template

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* rename model_source to bigdl and updated docs

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

---------

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>"
github.com/intel/llm-on-ray,inference/transformer_predictor.py,2023-11-28T08:22:51Z,"add gaudi support for single process serving (#117)

* add gaudi support for single process serving

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* rollback deepspeed predictor & deal with hpu

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* optimize logic of ipex and hpu graph

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* add dockerfile and readme for habana

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* move import inside to avoid deepspeed import

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* fix deepspeed test

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* add comments

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* fix

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* add resource requirement for HPU

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* fix

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* change device_name to device.type

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* update base image

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* fix

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* fix model_id

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* deduplicate code

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* use_hpu_graphs default to True and print warning

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* import ipex in actor if ipex is enabled

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* add dep in Dockerfile

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* import when necessary

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

---------

Signed-off-by: Zhi Lin <zhi.lin@intel.com>"
github.com/intel/llm-on-ray,inference/transformer_predictor.py,2023-11-23T08:18:03Z,"refactor argument passing and model configs by using yaml (#127)

* refactoring

Signed-off-by: Jiafu Zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* refactor argument passing and model configs by using yaml

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

* debugging gpt-j-6b

Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>

---------

Signed-off-by: Jiafu Zhang <jiafu.zhang@intel.com>
Signed-off-by: jiafu zhang <jiafu.zhang@intel.com>"
github.com/intel/llm-on-ray,inference/transformer_predictor.py,2023-11-16T07:34:03Z,"Ipex upgrade (#121)

* merge and update

* upd

* update asyncio.sleep time"
github.com/intel/llm-on-ray,inference/transformer_predictor.py,2023-11-14T02:02:03Z,"deltatuner integration (#87)

* add dependency

* enable delta model in common lib

* add sample best structure for deltatuner

* add workflow file

* update peft version

* update deltatuner version

* enable empty config for peft and deltatuner

* path fix

* add version in requirements file

* update params doc

* added models in the CICD configure file

* added the best model structures

* enable transformer predictor

* enable deepspeed predictor

* refine args

* update workflow file

* using latest deltatuner

* remove external best structure

* update best structure path

* empty matrix fix

* add default params

* update version

* fix workflow yml

* update version

* fix workflow yml

* fix workflow yml

* fix path in workflow yml

* fix workflow yml

* update version

* update version

* refine input model to peft model

* refine workflow yml

* remove tokenizer & add the condition on lora

* reduce workflow checks

* fix workflow yml

* fix regular exp

* fix eof

* fix eof

* fix format

* update models

* update models

---------

Co-authored-by: tianyil1 <tianyi.liu@intel.com>"
github.com/intel/llm-on-ray,inference/transformer_predictor.py,2023-11-13T03:20:15Z,"Remove Dialogue actor and refactor streaming (#111)

* remove dialogue actor and refactor streaming

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* add to_device

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* fix

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* fix deepspeed streaming

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

* fix

Signed-off-by: Zhi Lin <zhi.lin@intel.com>

---------

Signed-off-by: Zhi Lin <zhi.lin@intel.com>"
github.com/intel/llm-on-ray,inference/transformer_predictor.py,2023-11-09T02:02:13Z,"support gpu device (#71)

* support gpu device

* support gpu for inference

* set cpus_per_worker

* set gpus_per_worker

* support pvc in Inference"
github.com/intel/llm-on-ray,inference/transformer_predictor.py,2023-11-02T10:31:45Z,"Adapt the latest code for ui demo | add llama2 model (#73)

* Adapt the latest code for ui demo

* fix bug about progress bar

* modify trust_remote_code"
github.com/intel/llm-on-ray,inference/transformer_predictor.py,2023-09-15T03:51:44Z,"Enable DeepSpeed for CPU inference (#42)

* Add deepspeed_inference.py

* update

* fix input

* update

* update scaling config

* Add streaming_generate

* Add DeepSpeedPredictor to model serve

* update README

* Add deepspeed_inference_example

* remove test

* fix return value bug

* Fix DeepSpeedPredictor

* fix generate

* remove examples and debug print

* remove *.sh

* Add deepspeed requirements

* enable DeepSpeed in CI

* update Dockerfile

* add device support

* update

* update

* update

* update

* update

* update

* update

* nit

* nit

* update

* add install oneapi and start-ray-cluster scripts

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>

* remove sudo for docker

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>

* fix path

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>

* nit

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>

* nit

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>

* nit

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>

* nit

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>

* nit

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>

* update

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>

* update deepspeed version

---------

Signed-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>"
github.com/TongjiFinLab/CFGPT,code/test/eval-generate.py,2023-11-01T07:04:42Z,update
github.com/HKUNLP/multilingual-transfer,finetune.py,2023-06-13T06:17:53Z,Add files via upload
github.com/HKUNLP/multilingual-transfer,evaluation.py,2023-06-13T06:17:53Z,Add files via upload
github.com/volotat/Anagnorisis,llm_engine.py,2024-02-24T22:41:30Z,v 0.0.6
github.com/volotat/Anagnorisis,llm_engine.py,2024-01-13T00:41:59Z,wiki rendering
github.com/volotat/Anagnorisis,llm_engine.py,2024-01-06T09:58:56Z,v0.0.4
github.com/volotat/Anagnorisis,llm_engine.py,2023-12-16T00:16:39Z,Version 0.0.3
github.com/volotat/Anagnorisis,llm_engine.py,2023-12-09T21:57:29Z,v 0.0.2
github.com/volotat/Anagnorisis,llm_engine.py,2023-10-08T21:32:06Z,initial commit
github.com/volotat/Anagnorisis,src/fine_tune_page.py,2024-02-24T22:41:30Z,v 0.0.6
github.com/volotat/Anagnorisis,src/fine_tune_page.py,2024-01-13T00:41:59Z,wiki rendering
github.com/volotat/Anagnorisis,src/fine_tune_page.py,2023-12-22T23:32:58Z,fine-tunning improvements
github.com/volotat/Anagnorisis,src/fine_tune_page.py,2023-12-09T21:57:29Z,v 0.0.2
github.com/volotat/Anagnorisis,src/fine_tune_page.py,2023-10-08T21:32:06Z,initial commit
github.com/fengredrum/finetune-whisper-lora,eval_lora.py,2023-10-12T01:47:11Z,Setup LoRA and kbit parameters
github.com/fengredrum/finetune-whisper-lora,eval_lora.py,2023-09-13T03:20:08Z,Update eval_lora.py
github.com/fengredrum/finetune-whisper-lora,eval_lora.py,2023-09-11T06:25:06Z,Add ArgumentParser for evaluation
github.com/fengredrum/finetune-whisper-lora,eval_lora.py,2023-09-10T09:12:28Z,Update eval_lora.py
github.com/fengredrum/finetune-whisper-lora,eval_lora.py,2023-09-10T06:59:31Z,Update eval_lora.py for no stream mode
github.com/fengredrum/finetune-whisper-lora,eval_lora.py,2023-09-07T04:39:34Z,Add datasets support
github.com/fengredrum/finetune-whisper-lora,eval_lora.py,2023-08-29T14:53:17Z,Finetune lora with stream
github.com/fengredrum/finetune-whisper-lora,eval_lora.py,2023-08-07T03:11:42Z,Update eval_lora.py
github.com/fengredrum/finetune-whisper-lora,eval_lora.py,2023-08-06T14:08:34Z,Create eval_lora.py
github.com/fengredrum/finetune-whisper-lora,convert_model/using_optimum/vanilla_onnx_convert.py,2023-10-10T09:48:35Z,Add LoRa support
github.com/fengredrum/finetune-whisper-lora,convert_model/using_optimum/vanilla_onnx_convert.py,2023-09-27T01:28:43Z,Reorganize directory
github.com/georgechen1827/ChatGLM-text-embedding,example_with_embeddings/models.py,2023-04-09T04:13:20Z,add finetuned model and results
github.com/FreedomIntelligence/ALLaVA,allava/model/builder.py,2024-02-20T18:42:07Z,init
github.com/linhduongtuan/doctorwithbloom,chat.py,2023-03-29T07:39:37Z,Update chat.py
github.com/linhduongtuan/doctorwithbloom,chat.py,2023-03-29T07:35:36Z,Update chat.py
github.com/linhduongtuan/doctorwithbloom,chat.py,2023-03-28T14:53:42Z,Update chat.py
github.com/linhduongtuan/doctorwithbloom,chat.py,2023-03-28T14:10:56Z,Add files via upload
github.com/linhduongtuan/doctorwithbloom,generate.py,2023-03-28T19:03:10Z,Update generate.py
github.com/linhduongtuan/doctorwithbloom,generate.py,2023-03-28T18:53:53Z,Update generate.py
github.com/linhduongtuan/doctorwithbloom,generate.py,2023-03-28T18:43:23Z,Add files via upload
github.com/linhduongtuan/doctorwithbloom,generate.py,2023-03-28T13:19:55Z,Add files via upload
github.com/linhduongtuan/doctorwithbloom,export_hf_checkpoint.py,2023-03-28T18:43:23Z,Add files via upload
github.com/linhduongtuan/doctorwithbloom,export_hf_checkpoint.py,2023-03-28T13:19:55Z,Add files via upload
github.com/linhduongtuan/doctorwithbloom,export_state_dict_checkpoint.py,2023-03-28T18:43:23Z,Add files via upload
github.com/linhduongtuan/doctorwithbloom,export_state_dict_checkpoint.py,2023-03-28T13:19:55Z,Add files via upload
github.com/lawrence-cj/LLaMA-DiffFit,generate.py,2023-05-02T05:59:21Z,train and inference code update
github.com/lawrence-cj/LLaMA-DiffFit,generate.py,2023-05-01T15:34:18Z,Initial commit
github.com/lawrence-cj/LLaMA-DiffFit,peft/src/peft/peft_model.py,2023-05-01T15:34:18Z,Initial commit
github.com/lawrence-cj/LLaMA-DiffFit,peft/src/peft/tuners/prompt_tuning.py,2023-05-01T15:34:18Z,Initial commit
github.com/jackaduma/Recurrent-LLM,models/baichuan_hf.py,2023-06-20T06:05:04Z,update Baichuan LLM
github.com/jackaduma/Recurrent-LLM,models/baichuan_hf.py,2023-06-20T06:02:53Z,update Baichuan LLM :  it's so bad on writing novel.
github.com/jackaduma/Recurrent-LLM,models/baichuan_hf.py,2023-06-16T12:17:36Z,support Baichuan LLM
github.com/Umi7899/langchain-ChatGLM-My,models/loader/loader.py,2023-06-26T06:44:36Z,Initial commit
github.com/CSHaitao/ChatGLM_mutli_gpu_tuning,infer_lora.py,2023-05-15T14:30:18Z,Update infer_lora.py
github.com/CSHaitao/ChatGLM_mutli_gpu_tuning,infer_lora.py,2023-05-09T06:28:56Z,update
github.com/cascip/ChatAlpaca,utils/apply_lora.py,2023-06-01T00:46:05Z,new data and models released
github.com/pacman100/DHS-LLM-Workshop,chat_assistant/dpo/utils.py,2024-02-02T11:06:26Z,support for merge and unload in DPO
github.com/pacman100/DHS-LLM-Workshop,chat_assistant/dpo/utils.py,2024-02-02T07:48:33Z,Support unsloth
github.com/pacman100/DHS-LLM-Workshop,chat_assistant/dpo/utils.py,2024-02-02T07:09:36Z,"fix 4-bit quantization misspelling

Co-Authored-By: zhewei li <43416079+we1k@users.noreply.github.com>"
github.com/pacman100/DHS-LLM-Workshop,chat_assistant/dpo/utils.py,2024-02-01T08:12:17Z,Update utils.py
github.com/pacman100/DHS-LLM-Workshop,chat_assistant/dpo/utils.py,2024-02-01T08:10:29Z,Code refactor and addition of DPO finetuning script
github.com/pacman100/DHS-LLM-Workshop,5_Module/chat_gradio_app/app.py,2023-07-31T22:33:00Z,add content
github.com/pacman100/DHS-LLM-Workshop,5_Module/chat_gradio_app/app.py,2023-07-29T10:46:48Z,Update app.py
github.com/pacman100/DHS-LLM-Workshop,5_Module/chat_gradio_app/app.py,2023-07-29T08:40:27Z,Update app.py
github.com/pacman100/DHS-LLM-Workshop,5_Module/chat_gradio_app/app.py,2023-07-29T08:19:58Z,add gradio app
github.com/usail-hkust/UrbanKGent,utils/llama_lora_app.py,2024-02-17T03:46:59Z,Update llama_lora_app.py
github.com/usail-hkust/UrbanKGent,utils/llama_lora_app.py,2024-02-15T02:17:58Z,Add files via upload
github.com/PKU-RL/LLaMA-Rider,skills/llm_planner.py,2023-11-05T07:01:08Z,first commit
github.com/mobiusml/hqq,examples/lora/train_hqq_lora_example.py,2024-01-08T17:16:20Z,v0.1.2 (LoRA + optimizer V2 + refactor)
github.com/mobiusml/hqq,examples/lora/train_hqq_lora_example.py,2024-01-04T15:47:26Z,Add LoRA + optimizer_v2
github.com/zjunlp/IEPile,src/model/adapter.py,2024-02-23T15:18:48Z,update
github.com/zjunlp/IEPile,src/model/adapter.py,2024-02-22T16:52:50Z,init
github.com/vmware/vSphere-machine-learning-extension,examples/model_training/llm/bloom_fine_tune/merge_peft_adapters.py,2023-08-14T10:24:30Z,Add Bloom fine-tune example
github.com/git-cloner/Llama2-chinese,generate.py,2023-07-29T15:44:34Z,init commit
github.com/git-cloner/Llama2-chinese,api_stream.py,2023-07-30T04:37:09Z,increase max_token_size
github.com/git-cloner/Llama2-chinese,api_stream.py,2023-07-29T15:44:34Z,init commit
github.com/git-cloner/Llama2-chinese,inference/model_utils.py,2023-07-29T15:44:34Z,init commit
github.com/git-cloner/Llama2-chinese,inference/safety_utils.py,2023-07-29T15:44:34Z,init commit
github.com/git-cloner/Llama2-chinese,inference/chat_completion.py,2023-07-29T15:44:34Z,init commit
github.com/git-cloner/Llama2-chinese,inference/hf-text-generation-inference/merge_lora_weights.py,2023-07-29T15:44:34Z,init commit
github.com/andysingal/llm-course,transformers/AlpaGasus2-QLoRA/evaluation/AlpaGasus-Evaluation/response_data/generate.py,2023-08-31T08:35:18Z,Add files via upload
github.com/feizc/Visual-LLaMA,finetune.py,2023-04-16T07:20:49Z,debug for issue #2
github.com/feizc/Visual-LLaMA,finetune.py,2023-04-05T08:15:06Z,lora for llm
github.com/feizc/Visual-LLaMA,finetune.py,2023-04-05T06:59:14Z,support lora
github.com/feizc/Visual-LLaMA,finetune.py,2023-04-04T07:25:57Z,Add files via upload
github.com/hf-lin/ChatMusician,model/train/merge.py,2023-12-13T18:35:30Z,add files
github.com/hf-lin/ChatMusician,model/infer/predict.py,2023-12-13T18:35:30Z,add files
github.com/A-baoYang/alpaca-7b-chinese,serve/model.py,2023-04-24T14:26:43Z,"[feature] serving add support on AutoModel, BloomForCasualLM"
github.com/A-baoYang/alpaca-7b-chinese,serve/model.py,2023-03-31T04:07:23Z,[other] Fix path to avoid confusing
github.com/A-baoYang/alpaca-7b-chinese,serve/model.py,2023-03-30T06:13:53Z,[other] dataset fix & path update
github.com/A-baoYang/alpaca-7b-chinese,serve/model.py,2023-03-29T09:30:31Z,[serve] Update API & UIUI& UII serving
github.com/A-baoYang/alpaca-7b-chinese,validation/generate.py,2023-04-24T14:25:37Z,[feature] Add validation scripts (inference)
github.com/A-baoYang/alpaca-7b-chinese,validation/batch_generate.py,2023-04-24T14:25:37Z,[feature] Add validation scripts (inference)
github.com/aws-samples/aws-ai-ml-workshop-kr,genai/jumpstart/text_to_text/flan_t5_xl_with_LoRA/run_clm.py,2023-06-26T22:26:50Z,genai-initialize
github.com/aws-samples/aws-ai-ml-workshop-kr,genai/jumpstart/text_to_text/flan_t5_xl_with_LoRA/inference.py,2023-06-26T22:26:50Z,genai-initialize
github.com/RupertLuo/Valley,valley/inference/run_valley.py,2023-11-09T08:56:14Z,fix some bugs
github.com/RupertLuo/Valley,valley/inference/run_valley.py,2023-09-22T05:03:03Z,Fix projector no grad bug. Thanks to @TonyXuQAQ
github.com/RupertLuo/Valley,valley/inference/run_valley.py,2023-08-15T02:42:53Z,update valley cn
github.com/RupertLuo/Valley,valley/inference/run_valley.py,2023-08-14T08:03:12Z,update chinese valley
github.com/RupertLuo/Valley,valley/inference/run_valley.py,2023-08-14T05:47:56Z,update code
github.com/RupertLuo/Valley,valley/inference/run_valley.py,2023-06-28T04:17:45Z,upload train code and shell conv code
github.com/RupertLuo/Valley,valley/inference/run_valley.py,2023-06-13T06:28:24Z,update model delta weight apply/make code
github.com/RupertLuo/Valley,valley/inference/run_valley.py,2023-06-12T13:05:53Z,update readme
github.com/RupertLuo/Valley,valley/inference/run_valley.py,2023-06-09T10:51:49Z,update valley inference code
github.com/ParisNeo/lollms_bindings_zoo,TGI/special/llava_tools.py,2024-01-17T21:41:08Z,upgraded zoo
github.com/ParisNeo/lollms_bindings_zoo,bs_exllamav2/special/llava_tools.py,2024-01-17T23:16:18Z,exllama is back
github.com/ParisNeo/lollms_bindings_zoo,hugging_face/special/llava_tools.py,2023-11-21T00:06:52Z,llava advanced
github.com/foldl/chatllm.cpp,convert.py,2024-02-25T06:21:20Z,support CodeFuse-DeepSeek
github.com/foldl/chatllm.cpp,convert.py,2024-02-24T12:27:43Z,support of Gemma.
github.com/foldl/chatllm.cpp,convert.py,2024-02-18T10:05:58Z,all QWen1.5 models are now supported.
github.com/foldl/chatllm.cpp,convert.py,2024-02-16T04:10:27Z,support of Qwen 1.5
github.com/foldl/chatllm.cpp,convert.py,2024-02-14T01:46:48Z,add support of Persimmon
github.com/foldl/chatllm.cpp,convert.py,2024-02-06T06:18:02Z,support of MiniCPM
github.com/foldl/chatllm.cpp,convert.py,2024-02-04T03:04:41Z,support of InterLM2
github.com/foldl/chatllm.cpp,convert.py,2024-01-30T11:49:26Z,support BCE-ReRanker
github.com/foldl/chatllm.cpp,convert.py,2024-01-30T05:52:52Z,add Unigram tokenizer
github.com/foldl/chatllm.cpp,convert.py,2024-01-29T09:41:50Z,support of BCE-Embedding
github.com/foldl/chatllm.cpp,convert.py,2024-01-28T13:56:42Z,work on XLMRobertaModel
github.com/foldl/chatllm.cpp,convert.py,2024-01-24T14:40:19Z,support Orion model
github.com/foldl/chatllm.cpp,convert.py,2024-01-22T00:16:58Z,support of NeuralBeagle14 7B & refactor.
github.com/foldl/chatllm.cpp,convert.py,2024-01-17T10:13:46Z,support StableCode 3B
github.com/foldl/chatllm.cpp,convert.py,2024-01-15T07:04:07Z,support Dolphin (Phi2)
github.com/foldl/chatllm.cpp,convert.py,2024-01-12T09:43:00Z,"support of Phi-2 latest revision

eb8bbd1d37d258ea74fb082c53346d33056a83d4"
github.com/foldl/chatllm.cpp,convert.py,2024-01-12T06:24:59Z,support of InternLM 20B; convertor updated for QWen.
github.com/foldl/chatllm.cpp,convert.py,2024-01-10T09:57:12Z,implement `ntkmixed` for BlueLM
github.com/foldl/chatllm.cpp,convert.py,2024-01-10T01:51:08Z,"support of BlueLM

32k context is not supported yet."
github.com/foldl/chatllm.cpp,convert.py,2024-01-09T03:33:46Z,support of TigerBot
github.com/foldl/chatllm.cpp,convert.py,2024-01-08T10:01:47Z,support of QWen
github.com/foldl/chatllm.cpp,convert.py,2024-01-07T07:31:34Z,fix converter for Baichuan2 7B
github.com/foldl/chatllm.cpp,convert.py,2024-01-06T08:06:51Z,support of Mixtral.
github.com/foldl/chatllm.cpp,convert.py,2024-01-06T04:56:07Z,support of OpenChat. incremental `convert.py`.
github.com/foldl/chatllm.cpp,convert.py,2024-01-05T04:24:33Z,Support WizardMath-7B
github.com/foldl/chatllm.cpp,convert.py,2024-01-04T09:07:45Z,initial support of Mistral
github.com/foldl/chatllm.cpp,convert.py,2024-01-04T04:13:30Z,"support WizardLM 7B, 13B"
github.com/foldl/chatllm.cpp,convert.py,2024-01-04T03:37:42Z,support of WizardCoder-Python-7B
github.com/foldl/chatllm.cpp,convert.py,2024-01-03T09:33:00Z,support Phi: lots of updates to make it happen.
github.com/foldl/chatllm.cpp,convert.py,2023-12-27T02:59:34Z,support of Yi.
github.com/foldl/chatllm.cpp,convert.py,2023-12-19T04:27:45Z,add support of CodeLlaMA
github.com/foldl/chatllm.cpp,convert.py,2023-12-11T06:35:58Z,InternLM v1.1 is supported too
github.com/foldl/chatllm.cpp,convert.py,2023-12-11T04:35:04Z,support of InternLM (elder version)
github.com/foldl/chatllm.cpp,convert.py,2023-12-10T10:46:55Z,support of CodeGeeX2
github.com/foldl/chatllm.cpp,convert.py,2023-12-10T07:23:16Z,support of ChatGLM3
github.com/foldl/chatllm.cpp,convert.py,2023-12-09T09:05:40Z,support of Baichuan-2
github.com/foldl/chatllm.cpp,convert.py,2023-12-09T06:23:21Z,support of DeepSeek Coder
github.com/foldl/chatllm.cpp,convert.py,2023-12-08T10:55:17Z,redesign tokenizer
github.com/foldl/chatllm.cpp,convert.py,2023-12-04T10:10:28Z,"sync with ggml rev a399be0977

1. use ggml revision: a399be09771d9ac56fad7f81dae40df391286a62
2. Known issues: GLM/GLM2 is broken because of  `ggml_view_2d`
   (ggerganov/ggml#4322)."
github.com/foldl/chatllm.cpp,convert.py,2023-12-02T09:51:54Z,init
github.com/blazerye/DrugAssist,merge_model.py,2023-12-27T07:57:45Z,"initial commit

initial commit"
github.com/blazerye/DrugAssist,gradio_service.py,2023-12-27T07:57:45Z,"initial commit

initial commit"
github.com/sshh12/multi_token,multi_token/inference.py,2023-10-20T01:45:12Z,tweaks
github.com/sshh12/multi_token,multi_token/inference.py,2023-10-20T01:03:23Z,tweaks
github.com/sshh12/multi_token,multi_token/inference.py,2023-10-18T03:37:20Z,tweak
github.com/yegcjs/DiffusionLLM,src/utils.py,2023-08-24T15:00:43Z,first commit
github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety,llama2/inference/model_utils.py,2023-10-07T05:07:55Z,public release
github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety,llama2/safety_evaluation/question_inference.py,2023-10-07T05:07:55Z,public release
github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety,llama2/safety_evaluation/eval_utils/model_utils.py,2023-10-07T05:07:55Z,public release
github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety,llama2/utility_evaluation/mt_bench/gen_model_answer.py,2023-10-07T05:07:55Z,public release
github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety,llama2/inference/hf-text-generation-inference/merge_lora_weights.py,2023-10-07T05:07:55Z,public release
github.com/cassanof/finetuning-harness,scripts/load_and_push_to_hub.py,2023-09-26T23:01:18Z,clean
github.com/hplt-project/monolingual-multilingual-instruction-tuning,loraft/generate.py,2023-10-29T14:02:19Z,update scripts
github.com/kuutsav/llm-toys,llm_toys/hf/peft/auto.py,2023-07-15T14:30:57Z,updated hf refs
github.com/kuutsav/llm-toys,llm_toys/tasks/predict.py,2023-07-16T13:24:22Z,updated dialogue summarization and theme detection
github.com/kuutsav/llm-toys,llm_toys/tasks/predict.py,2023-07-15T18:43:41Z,refactoring + README update
github.com/kuutsav/llm-toys,llm_toys/tasks/predict.py,2023-07-15T14:52:35Z,updates related to pypi issues
github.com/kuutsav/llm-toys,llm_toys/tasks/predict.py,2023-07-15T14:30:57Z,updated hf refs
github.com/kuutsav/llm-toys,llm_toys/tasks/predict.py,2023-07-15T07:06:01Z,trying device compatibility
github.com/kuutsav/llm-toys,llm_toys/tasks/predict.py,2023-07-15T05:22:28Z,added 3b and 7b models for paraphrasing and changing the tone
github.com/kuutsav/llm-toys,llm_toys/hf/peft/peft_model.py,2023-07-15T19:10:59Z,updated readme and pre-commit checks
github.com/kuutsav/llm-toys,llm_toys/hf/peft/peft_model.py,2023-07-15T14:30:57Z,updated hf refs
github.com/kuutsav/llm-toys,llm_toys/hf/peft/tuners/lora.py,2023-07-15T14:30:57Z,updated hf refs
github.com/kuutsav/llm-toys,llm_toys/hf/transformers/trainer.py,2023-07-15T19:10:59Z,updated readme and pre-commit checks
github.com/kuutsav/llm-toys,llm_toys/hf/transformers/trainer.py,2023-07-15T14:30:57Z,updated hf refs
github.com/kuutsav/llm-toys,llm_toys/hf/peft/tuners/prompt_tuning.py,2023-07-15T14:30:57Z,updated hf refs
github.com/AgainstEntropy/kanji,train_lora/train_lcm_distill_lora_sd.py,2024-02-26T04:28:46Z,"update README, add StreamDiffusionIO"
github.com/LCS2-IIITD/DaSLaM,LLAMA13B/Context/ppo_train_13B.py,2023-10-22T05:12:09Z,Adding code for RL finetuning
github.com/teticio/llama-squad,model.py,2023-08-10T18:37:54Z,add app
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-03-03T12:50:14Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-03-03T12:43:28Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-03-03T12:31:00Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-03-03T12:26:11Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-03-03T12:23:01Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-03-03T12:16:10Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-03-03T12:03:38Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-03-03T11:56:13Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-03-03T11:52:04Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-03-03T11:50:08Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-03-03T11:40:32Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-03-03T11:15:22Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-03-03T10:23:25Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-03-03T10:14:46Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-03-03T10:13:46Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-03-03T10:07:42Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-03-01T09:30:12Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-03-01T09:16:36Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-03-01T07:56:17Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-03-01T07:27:23Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-03-01T07:23:22Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-03-01T07:19:09Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-03-01T06:55:34Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-02-27T03:12:46Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-02-27T03:04:22Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-02-26T09:37:10Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-02-26T03:56:20Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-02-21T02:22:06Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-02-21T02:18:20Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-02-21T02:07:53Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-02-20T14:59:50Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-02-20T14:50:13Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-02-19T02:15:46Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-01-24T07:40:01Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-01-24T07:34:16Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-01-23T08:43:41Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-01-22T12:32:11Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-01-22T12:31:16Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-01-22T12:27:06Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-01-22T11:12:02Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-01-22T10:58:24Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-01-22T09:25:20Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-01-22T09:24:56Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-01-22T08:43:26Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-01-11T07:27:14Z,Refactor backend parameters in init_model function
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-01-05T12:32:39Z,Fix input_tokens_count in async_vllm_chat
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-01-05T11:59:56Z,bakcend parameters auto setup in vllm
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-01-05T09:07:40Z,Add SingleOutputMeta class to SingleOutput in utils module
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2024-01-01T11:35:31Z,Refactor code to support chat templates in tokenizer
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-29T10:43:42Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-29T10:11:01Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-29T10:04:53Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-29T08:45:05Z,Add support  streaming chat in qianwen saas
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-27T07:06:43Z,Add prob variable to async_vllm_chat function
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-26T14:17:01Z,Add import math and fix formatting in stream_chat function
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-26T14:13:45Z,Add stopping criteria and tokenization improvements in stream_chat of auto
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-26T08:19:07Z,Refactor stream_chat and get_meta in auto
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-26T08:12:08Z,Add trust_remote_code parameter to AutoTokenizer.from_pretrained()
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-26T08:09:13Z,Refactor init_model function in auto to remove unsupported backend and unused code
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-25T15:51:42Z,Add get_meta  to saas model
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-25T08:32:44Z,Fix backend parameter assignment in auto/__init__.py and update test_model_meta.py
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-25T08:30:27Z,Refactor backend parameters in auto/__init__.py
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-25T08:16:34Z,Refactor async_get_meta function to include additional model information
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-25T07:38:31Z,Remove unnecessary parameter from async_get_meta function
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-25T07:22:50Z,"Add async_get_meta method and  get_meta method which
can show model meta info"
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-23T12:40:43Z,Fix print statement formatting in async_vllm_chat function
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-20T04:32:26Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-14T07:52:47Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-14T07:45:38Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-14T07:24:15Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-14T06:59:10Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-14T06:35:52Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-14T06:04:02Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-14T06:02:21Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-14T05:52:35Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-14T05:31:26Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-14T05:23:17Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-14T05:12:45Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-14T04:57:04Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-14T04:15:44Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-14T04:05:57Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-14T03:55:30Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-14T03:51:33Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-14T01:26:43Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-14T00:57:03Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-14T00:41:13Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-13T14:57:38Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-13T14:41:12Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-13T14:25:09Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-13T12:43:48Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-13T12:42:20Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-13T12:39:17Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-13T05:22:11Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-13T04:15:15Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-13T04:08:26Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-06T04:52:00Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-02T11:38:40Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-02T10:07:02Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-02T10:01:29Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/auto/__init__.py,2023-12-02T09:48:10Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/qwen/__init__.py,2024-03-01T06:55:34Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/qwen/__init__.py,2023-12-26T07:58:43Z,Add get_meta method to models
github.com/allwefantasy/byzer-llm,src/byzerllm/qwen/__init__.py,2023-12-26T00:32:52Z,make sure the client works fine with saas/proprietary model
github.com/allwefantasy/byzer-llm,src/byzerllm/qwen/__init__.py,2023-12-02T05:24:41Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/qwen/__init__.py,2023-12-02T05:22:52Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/qwen/__init__.py,2023-12-02T05:22:32Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2024-03-01T06:55:34Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-12-26T07:58:43Z,Add get_meta method to models
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-12-26T00:32:52Z,make sure the client works fine with saas/proprietary model
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-09-12T05:40:39Z,default using int4 quantization
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-09-12T03:30:07Z,refactor quantization config
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-09-11T10:21:24Z,"llama, llama2, and falcon support 4bit and 8bit configurations when quantization is turned on"
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-08-11T10:34:21Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-25T05:18:06Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-25T05:17:30Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-25T05:14:29Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-25T04:58:05Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-25T03:58:50Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-25T02:49:37Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-25T02:47:19Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-25T02:31:27Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-25T01:37:41Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-25T01:32:17Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-25T01:21:05Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-24T11:16:49Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-24T11:04:17Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-24T10:50:57Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-24T10:38:51Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-24T10:32:23Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-24T10:21:51Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-24T09:50:00Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-24T04:04:14Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-21T07:32:44Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-21T04:16:25Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-21T03:56:41Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-21T03:50:52Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-21T03:43:34Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-21T03:42:09Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-21T03:40:00Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-21T03:15:06Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-20T09:05:51Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-20T08:50:51Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-20T08:24:07Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-20T08:18:35Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-20T08:16:22Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-20T06:54:50Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-16T03:17:55Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-16T01:34:39Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-10T09:16:23Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama/__init__.py,2023-07-10T03:00:50Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama2/__init__.py,2024-03-01T06:55:34Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama2/__init__.py,2023-12-26T07:58:43Z,Add get_meta method to models
github.com/allwefantasy/byzer-llm,src/byzerllm/llama2/__init__.py,2023-12-26T00:32:52Z,make sure the client works fine with saas/proprietary model
github.com/allwefantasy/byzer-llm,src/byzerllm/llama2/__init__.py,2023-12-20T07:52:09Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama2/__init__.py,2023-12-20T07:46:49Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama2/__init__.py,2023-10-18T10:31:17Z,remove to_bettertransformer in llama2 temporal
github.com/allwefantasy/byzer-llm,src/byzerllm/llama2/__init__.py,2023-09-12T05:40:39Z,default using int4 quantization
github.com/allwefantasy/byzer-llm,src/byzerllm/llama2/__init__.py,2023-09-12T03:30:07Z,refactor quantization config
github.com/allwefantasy/byzer-llm,src/byzerllm/llama2/__init__.py,2023-09-11T10:21:24Z,"llama, llama2, and falcon support 4bit and 8bit configurations when quantization is turned on"
github.com/allwefantasy/byzer-llm,src/byzerllm/llama2/__init__.py,2023-08-31T04:04:14Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama2/__init__.py,2023-08-05T03:51:03Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/llama2/__init__.py,2023-08-03T02:08:50Z,support llama2 model
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-12-26T07:58:43Z,Add get_meta method to models
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-09-12T05:40:39Z,default using int4 quantization
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-09-12T03:30:07Z,refactor quantization config
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-09-11T10:21:24Z,"llama, llama2, and falcon support 4bit and 8bit configurations when quantization is turned on"
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-08-31T04:04:14Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-08-26T01:09:21Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-07-21T07:32:44Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-07-20T00:59:15Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-07-19T06:12:51Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-07-17T06:23:37Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-07-17T04:53:27Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-07-17T04:36:56Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-07-16T03:24:31Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-07-16T02:41:56Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-07-16T02:35:00Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-07-16T01:49:57Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-07-16T01:39:13Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-07-16T01:34:39Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-07-16T01:10:10Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-07-15T12:39:52Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-07-15T12:37:13Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-07-15T12:32:13Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-07-15T12:21:44Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-07-15T11:56:41Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-07-14T14:28:22Z,upate
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-07-11T08:52:19Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-07-10T09:16:23Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/falcon/__init__.py,2023-07-10T03:00:50Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/zephyr/__init__.py,2024-03-01T06:55:34Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/zephyr/__init__.py,2023-11-14T12:17:08Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/baichuan/__init__.py,2023-12-26T07:58:43Z,Add get_meta method to models
github.com/allwefantasy/byzer-llm,src/byzerllm/baichuan/__init__.py,2023-12-25T15:51:42Z,Add get_meta  to saas model
github.com/allwefantasy/byzer-llm,src/byzerllm/baichuan/__init__.py,2023-09-12T08:55:36Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/baichuan/__init__.py,2023-07-16T01:34:39Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/baichuan/__init__.py,2023-07-10T03:00:50Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/chatglm2/__init__.py,2024-01-02T12:17:46Z,Fix model meta setup and chat template usage
github.com/allwefantasy/byzer-llm,src/byzerllm/chatglm2/__init__.py,2023-12-26T07:58:43Z,Add get_meta method to models
github.com/allwefantasy/byzer-llm,src/byzerllm/chatglm2/__init__.py,2023-08-31T04:12:25Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/chatglm2/__init__.py,2023-07-16T01:34:39Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/chatglm2/__init__.py,2023-07-10T03:00:50Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/visualglm/__init__.py,2023-07-16T01:34:39Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/visualglm/__init__.py,2023-07-10T03:00:50Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/utils/sft/merge_lora.py,2023-09-13T09:14:59Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/utils/sft/merge_lora.py,2023-09-13T09:07:24Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/utils/sft/merge_lora.py,2023-09-13T08:52:41Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/utils/sft/merge_lora.py,2023-09-13T08:42:02Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/utils/sft/merge_lora.py,2023-09-13T08:38:44Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/utils/sft/merge_lora.py,2023-09-13T08:33:44Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/utils/sft/merge_lora.py,2023-09-13T08:28:53Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/utils/sft/merge_lora.py,2023-09-13T08:12:47Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/utils/sft/merge_lora.py,2023-09-12T08:34:55Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/utils/sft/merge_lora.py,2023-09-11T11:29:56Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/utils/sft/merge_lora.py,2023-09-11T11:24:31Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/utils/sft/merge_lora.py,2023-09-11T11:10:35Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/qwen_vl_chat/__init__.py,2024-01-05T13:16:43Z,Update output format in stream_chat function
github.com/allwefantasy/byzer-llm,src/byzerllm/qwen_vl_chat/__init__.py,2024-01-05T01:42:41Z,Refactor input history parsing in stream_chat function
github.com/allwefantasy/byzer-llm,src/byzerllm/qwen_vl_chat/__init__.py,2024-01-05T01:37:55Z,optimize vl-chat
github.com/allwefantasy/byzer-llm,src/byzerllm/qwen_vl_chat/__init__.py,2024-01-04T11:12:35Z,"Add tiktoken to demo-requirements.txt and requirements.txt, update CUDA version to 12.2.2, and remove unused imports in __init__.py"
github.com/allwefantasy/byzer-llm,src/byzerllm/qwen_vl_chat/__init__.py,2023-12-26T07:58:43Z,Add get_meta method to models
github.com/allwefantasy/byzer-llm,src/byzerllm/qwen_vl_chat/__init__.py,2023-08-26T08:55:00Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/qwen_vl_chat/__init__.py,2023-08-26T08:55:00Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/qwen_vl_chat/__init__.py,2023-08-26T08:55:00Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/qwen_vl_chat/__init__.py,2023-08-26T08:55:00Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/qwen_vl_chat/__init__.py,2023-08-26T08:55:00Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/qwen_vl_chat/__init__.py,2023-08-26T08:55:00Z,update
github.com/allwefantasy/byzer-llm,src/byzerllm/qwen_vl_chat/__init__.py,2023-08-26T08:55:00Z,add qwen_vl_chat
github.com/huggingface/amused,training/generate_images.py,2023-12-08T01:13:00Z,"some things

more

more

commit running scripts

more

remove commit files"
github.com/huggingface/amused,training/generate_images.py,2023-12-01T08:59:43Z,fix
github.com/huggingface/amused,training/generate_images.py,2023-12-01T07:43:00Z,fix
github.com/huggingface/amused,training/generate_images.py,2023-12-01T07:10:48Z,misc
github.com/openfeedback/superhf,experiments/evaluations/model_loading.py,2023-09-29T04:37:59Z,Add ability to mock RM on local mps
github.com/openfeedback/superhf,experiments/evaluations/model_loading.py,2023-09-29T04:00:20Z,Fix model loading for mac apple silicon
github.com/openfeedback/superhf,experiments/evaluations/model_loading.py,2023-05-10T21:34:54Z,Add tokenizer saving and left padding lm
github.com/openfeedback/superhf,experiments/evaluations/model_loading.py,2023-05-10T21:11:52Z,Load both lms and rms in bfloat16 not just rm
github.com/openfeedback/superhf,experiments/evaluations/model_loading.py,2023-05-10T00:32:57Z,First attempt at running evals
github.com/openfeedback/superhf,experiments/evaluations/model_loading.py,2023-05-09T06:32:17Z,Default revision None
github.com/openfeedback/superhf,experiments/evaluations/model_loading.py,2023-05-09T06:24:24Z,Support loading a particular peft adapter revision
github.com/openfeedback/superhf,experiments/evaluations/model_loading.py,2023-05-09T06:21:33Z,Reimplement peft model loading into our model loader
github.com/openfeedback/superhf,experiments/evaluations/model_loading.py,2023-05-07T17:55:30Z,Free memory
github.com/openfeedback/superhf,experiments/evaluations/model_loading.py,2023-05-06T23:46:10Z,Add pad token to eval tokenizer if not set
github.com/openfeedback/superhf,experiments/evaluations/model_loading.py,2023-05-06T06:51:45Z,Fix for misnamed Llama tokenizer config class
github.com/openfeedback/superhf,experiments/evaluations/model_loading.py,2023-05-06T06:44:25Z,Finish the model loader main stuff
github.com/openfeedback/superhf,experiments/evaluations/model_loading.py,2023-05-06T05:48:47Z,Initial model_loading
github.com/laiviet/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-06-13T06:25:11Z,Update dataset definition and task definition
github.com/laiviet/lm-evaluation-harness,lm_eval/models/huggingface.py,2023-05-28T17:36:39Z,Init
github.com/crowsonkb/LDLM,train_vae.py,2023-08-04T19:18:58Z,Cast input_ids to long in train_vae.py
github.com/crowsonkb/LDLM,train_vae.py,2023-08-02T19:04:32Z,Initial commit
github.com/crowsonkb/LDLM,train_ldlm.py,2023-08-02T19:04:32Z,Initial commit
github.com/crowsonkb/LDLM,infer_ldlm.py,2023-08-04T06:02:56Z,Fix indexing bug in inference code
github.com/crowsonkb/LDLM,infer_ldlm.py,2023-08-04T05:56:50Z,Use 25 steps of DPM++ 2M to sample
github.com/crowsonkb/LDLM,infer_ldlm.py,2023-08-03T17:09:49Z,Fix inference script bugs
github.com/crowsonkb/LDLM,infer_ldlm.py,2023-08-02T19:04:32Z,Initial commit
github.com/zejunwang1/chatglm_tuning,cli_demo.py,2023-05-17T01:12:14Z,upgrade
github.com/zejunwang1/chatglm_tuning,cli_demo.py,2023-05-16T01:08:32Z,initial commit
github.com/zxy556677/EasyGen,fastchat/model/apply_lora.py,2023-10-08T10:16:30Z,Add files via upload
github.com/zxy556677/EasyGen,fastchat/model/model_adapter.py,2023-10-08T10:16:30Z,Add files via upload
github.com/zxy556677/EasyGen,fastchat/serve/inference_llama.py,2023-10-18T08:21:38Z,Update inference_llama.py
github.com/zxy556677/EasyGen,fastchat/serve/inference_llama.py,2023-10-17T13:10:22Z,Update inference_llama.py
github.com/zxy556677/EasyGen,fastchat/serve/inference_llama.py,2023-10-08T10:15:44Z,Add files via upload
github.com/zxy556677/EasyGen,fastchat/serve/inference_easygen.py,2023-10-18T08:10:57Z,Update inference_easygen.py
github.com/zxy556677/EasyGen,fastchat/serve/inference_easygen.py,2023-10-18T08:06:29Z,Add files via upload
github.com/amitpuri/Ask-picturize-it,notebooks/huggingface/sagemaker/24_train_bloom_peft_lora/scripts/run_clm.py,2023-07-05T18:08:28Z,update
github.com/swj0419/in-context-pretraining,lib/huggingface_pytorch-transformers_main/src/transformers/trainer.py,2024-01-13T23:12:12Z,lib
github.com/swj0419/in-context-pretraining,lib/huggingface_pytorch-transformers_main/src/transformers/modeling_utils.py,2024-01-13T23:12:12Z,lib
github.com/swj0419/in-context-pretraining,lib/huggingface_pytorch-transformers_main/src/transformers/integrations/peft.py,2024-01-13T23:12:12Z,lib
github.com/aldraus/quilt-llava,llava/model/builder.py,2024-02-05T06:50:25Z,llava model
github.com/StarRing2022/RingRWKV,example/generate_hf.py,2023-07-17T06:23:22Z,Add files via upload
github.com/yongzhuo/LLM-SFT,llm_sft/ft_lora/post_api.py,2023-06-12T14:36:24Z,"init, add SFT of chatglm, llama, bloomz, qlora"
github.com/yongzhuo/LLM-SFT,llm_sft/ft_qlora/post_api.py,2023-06-12T14:36:24Z,"init, add SFT of chatglm, llama, bloomz, qlora"
github.com/yongzhuo/LLM-SFT,llm_sft/ft_lora/evaluation.py,2023-06-12T14:36:24Z,"init, add SFT of chatglm, llama, bloomz, qlora"
github.com/yongzhuo/LLM-SFT,llm_sft/ft_qlora/evaluation.py,2023-06-12T14:36:24Z,"init, add SFT of chatglm, llama, bloomz, qlora"
github.com/Abbey4799/CELLO,code/evaluators/vicuna_else.py,2023-09-17T04:28:57Z,upload CELLO
github.com/Abbey4799/CELLO,code/evaluators/wizardlm13b.py,2023-09-17T04:28:57Z,upload CELLO
github.com/Abbey4799/CELLO,code/evaluators/cutegpt_lora.py,2023-09-17T04:28:57Z,upload CELLO
github.com/Abbey4799/CELLO,code/evaluators/baichuan13b_chat.py,2023-09-17T04:28:57Z,upload CELLO
github.com/l294265421/multi-turn-alpaca,data/original_data/AlpacaDataCleaned/eval/eval.py,2023-04-10T07:54:03Z,training multi-turn alpaca
github.com/l294265421/multi-turn-alpaca,multi_turn_alpaca/training_model/alpaca_chatbot.py,2023-04-21T06:39:58Z,update chatbot
github.com/l294265421/multi-turn-alpaca,multi_turn_alpaca/training_model/alpaca_chatbot.py,2023-04-17T06:19:11Z,update readme
github.com/l294265421/multi-turn-alpaca,multi_turn_alpaca/training_model/alpaca_chatbot.py,2023-04-13T13:01:42Z,run inference background
github.com/l294265421/multi-turn-alpaca,multi_turn_alpaca/training_model/alpaca_chatbot.py,2023-04-13T03:55:02Z,add CDial-GPT dataset
github.com/l294265421/multi-turn-alpaca,multi_turn_alpaca/training_model/alpaca_chatbot.py,2023-04-12T14:17:19Z,improve chatbot
github.com/l294265421/multi-turn-alpaca,multi_turn_alpaca/training_model/alpaca_chatbot.py,2023-04-12T13:49:23Z,add chatbot
github.com/l294265421/multi-turn-alpaca,multi_turn_alpaca/training_model/alpaca_chatbot.py,2023-04-12T11:43:21Z,add alpaca chatbot
github.com/ducdauge/sft-llm,eval/utils.py,2024-01-29T11:12:35Z,cleaned
github.com/SAI990323/Grounding4Rec,inference.py,2023-10-29T08:40:58Z,fix bugs
github.com/SAI990323/Grounding4Rec,inference.py,2023-08-19T08:42:08Z,initial commit
github.com/kotoba-tech/kotomamba,src/llama_recipes/inference/model_utils.py,2023-12-23T01:50:33Z,feat: integrate kotoba-recipes
github.com/FSoft-AI4Code/RepoPilot,scripts/merge_model.py,2023-12-18T03:15:52Z,add local vllm and instruction tuning script
github.com/taishan1994/chinese_llm_pretrained,test_pretrained_model.py,2023-06-25T07:42:54Z,commit
github.com/BangLab-UdeM-Mila/NLP4MatSci-HoneyBee,generate.py,2024-01-13T14:15:58Z,update generate.py
github.com/alex000kim/ML-Pipeline-With-DVC-SkyPilot-HuggingFace,src/merge_model.py,2023-09-01T18:03:28Z,init
github.com/vilm-ai/vietcuna,launch.py,2023-07-08T11:03:09Z,update vietcuna-7b-alpha settings
github.com/vilm-ai/vietcuna,launch.py,2023-07-08T11:01:45Z,update vietcuna-7b-alpha
github.com/vilm-ai/vietcuna,launch.py,2023-06-21T17:41:32Z,fix 4bit
github.com/vilm-ai/vietcuna,launch.py,2023-06-19T15:07:50Z,Update launch.py
github.com/vilm-ai/vietcuna,launch.py,2023-06-17T17:34:39Z,update vietcuna3b
github.com/vilm-ai/vietcuna,launch.py,2023-06-17T17:33:42Z,update vicuna-3b
github.com/vilm-ai/vietcuna,launch.py,2023-06-16T14:34:19Z,Update launch.py
github.com/vilm-ai/vietcuna,launch.py,2023-06-16T14:24:34Z,Create launch.py
github.com/EvilFreelancer/ruGPT-3.5-training,convert_to_native.py,2023-10-08T11:03:35Z,Layers converter added
github.com/EvilFreelancer/ruGPT-3.5-training,test_lora.py,2023-10-08T10:59:10Z,Testing scripts added
github.com/EvilFreelancer/ruGPT-3.5-training,test_gigasaiga.py,2023-10-08T10:59:10Z,Testing scripts added
github.com/eltociear/qa-lora,peft_utils.py,2023-09-27T03:26:25Z,Add files via upload
github.com/bigai-nlco/LSTP-Chat,eval/utils/builder_utils.py,2024-02-26T04:21:58Z,init
github.com/ChenDelong1999/polite-flamingo,polite_flamingo/src/factory.py,2023-07-06T10:45:41Z,update_readme
github.com/HiBugEnterprise/HiBug-6B,models/loader/loader.py,2023-08-10T10:50:21Z,init commit
github.com/pleisto/yuren-baichuan-7b,tools/merge_lora/src/merge_lora/__main__.py,2023-06-27T08:48:36Z,fix: lint and github action
github.com/pleisto/yuren-baichuan-7b,tools/merge_lora/src/merge_lora/__main__.py,2023-06-27T16:27:46Z,add webui
github.com/pleisto/yuren-baichuan-7b,tools/merge_lora/src/merge_lora/__main__.py,2023-06-26T00:37:16Z,first commit
github.com/pleisto/yuren-baichuan-7b,apps/yuren_trainer/src/yuren_trainer/text_sft.py,2023-06-27T08:48:36Z,fix: lint and github action
github.com/pleisto/yuren-baichuan-7b,apps/yuren_trainer/src/yuren_trainer/text_sft.py,2023-06-27T16:27:46Z,add webui
github.com/pleisto/yuren-baichuan-7b,apps/yuren_trainer/src/yuren_trainer/text_sft.py,2023-06-26T00:37:16Z,first commit
github.com/pleisto/yuren-baichuan-7b,apps/yuren_trainer/src/yuren_trainer/peft_trainer.py,2023-06-27T08:48:36Z,fix: lint and github action
github.com/pleisto/yuren-baichuan-7b,apps/yuren_trainer/src/yuren_trainer/peft_trainer.py,2023-06-26T00:37:16Z,first commit
github.com/ImKeTT/Alpaca-Light,generate.py,2023-03-28T10:02:34Z,first commit
github.com/IBM/SALMON,inference/demo.py,2023-10-09T13:45:13Z,clean the code of step 1
github.com/IBM/SALMON,inference/demo.py,2023-10-09T11:51:29Z,add demo code
github.com/IBM/SALMON,training/models/reward_model.py,2023-10-09T15:34:31Z,add training code
github.com/IBM/SALMON,training/step1_synthetic_preference_collection/synthetic_preference.py,2023-10-09T13:45:13Z,clean the code of step 1
github.com/ruleGreen/SAFARI,lora/peft/peft_model.py,2023-12-02T03:56:04Z,init commit
github.com/ruleGreen/SAFARI,lora/peft/tuners/prompt_tuning.py,2023-12-02T03:56:04Z,init commit
github.com/Montinger/Transformer-Workbench,LoRA-from-scratch/Train-QLoRA-with-PEFT.py,2023-12-10T16:02:53Z,Added LoRA-from scratch code and results
github.com/Montinger/Transformer-Workbench,LoRA-from-scratch/Load-LoRA-Weights-PEFT.py,2023-12-10T16:02:53Z,Added LoRA-from scratch code and results
github.com/WangRongsheng/Knowledge-Base-LLMs-QA,models/moss_llm.py,2023-05-21T17:07:40Z,init Knowledge-Base-LLMs-QA
github.com/WangRongsheng/Knowledge-Base-LLMs-QA,models/chatglm_llm.py,2023-05-21T17:07:40Z,init Knowledge-Base-LLMs-QA
github.com/Neph0s/InCharacter,code/ChatHaruhi/ChatGLM2GPT.py,2024-01-09T11:38:51Z,first commit
github.com/Neph0s/InCharacter,code/ChatHaruhi/BaiChuan2GPT.py,2024-01-09T11:38:51Z,first commit
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-25T01:36:08Z,Remove an assert
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-23T20:43:45Z,Seperate method use cases
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-23T19:42:40Z,"Simplify code regarding saving, loading adapters dict"
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-22T18:53:07Z,Remove unnecessary code
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-21T17:50:26Z,Removal of commented sections
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-20T11:11:28Z,Automatically set adapters for from_pretrained
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-12T12:37:20Z,Merge
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-12T11:35:32Z,Make public
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-12T11:27:30Z,Make public
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-12T01:43:18Z,Add 'set_global_scaling_weight'
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-10T20:44:03Z,Merge branch 'master' into save_all_adapters
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-10T20:40:31Z,Formatting
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-10T20:40:15Z,Add method to override and specify adapters
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-10T20:18:52Z,Add hf_hub_subdir to loading fns
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-10T19:51:49Z,Fix removal of a fn
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-10T19:05:34Z,Make function public
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-09T21:56:17Z,Add sketch impl
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-09T19:39:57Z,"Add scaling pass value to config, default is now 0"
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-09T19:38:01Z,Add loading from hf hub
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-07T18:19:57Z,Update type ignore
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-07T18:16:50Z,Update ci
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-07T18:13:50Z,Update for mypy
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-07T15:20:44Z,Ensure use_cache is false
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-04T22:43:11Z,Move a util fn
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-02T19:47:28Z,Ensure non instantiation
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-01T17:10:38Z,Patch the generate override fn
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-01T15:20:18Z,Patch .generate to ensure frozen
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-01T13:16:59Z,Add more verbose print
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-02-01T10:34:08Z,"Update the saving, loading safetensors system"
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-31T23:37:06Z,Add class hack for type hints
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-31T20:31:39Z,Fix via ruff
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-31T20:27:38Z,Merge branch 'master' into seqlen_scalings
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-31T20:16:00Z,Update verbose impl
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-31T20:14:47Z,Update verbose impl
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-31T20:14:38Z,Update verbose impl
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-31T20:11:37Z,Extend output
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-31T20:11:15Z,Implement load scalings log
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-31T20:09:48Z,Implement load scalings log
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-31T18:59:37Z,Pass seq len in payload
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-30T10:23:25Z,Fix some bugs
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-30T00:02:38Z,Simplify implementation
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-30T00:02:38Z,Simplify implementation
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-29T22:17:40Z,Add manual setting of scalings
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-29T17:54:28Z,Reimplement topk
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-28T18:17:27Z,Update based on master state
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-27T22:43:33Z,Remove requires_grad in temp scalings during class pass
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-27T19:30:14Z,Typing update
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-27T19:13:18Z,Move bound name correctly
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-27T18:59:57Z,Remove generate
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-27T18:50:43Z,Use hooks
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-26T23:16:54Z,Properly insert generate
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-26T22:34:55Z,Fix recursion err
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-26T22:00:14Z,*MAJOR CHANGE* Remove all global state
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-25T12:30:53Z,Update impl of disable
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-22T20:32:55Z,Remove top_k_lora
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-22T15:41:52Z,Remove need for looping over batch *major change*
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-20T20:03:32Z,Update the API
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-20T19:47:10Z,Specify use trainable in config
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-20T19:28:32Z,Fix small bug with adapters name
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-20T19:23:10Z,Update the kwargs default
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-20T19:15:00Z,Break the recursion
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-20T18:49:41Z,Save the adapters if enabled
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-20T18:19:38Z,Allow config to enable trainable adapters
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-20T11:23:06Z,Calculate seq lens with new option
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-20T11:14:36Z,Remove the old comments
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-20T11:11:57Z,Just formatting
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-19T21:36:30Z,Use new attention mask seq length
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-19T21:35:14Z,"Fixed sequence length, and a few other things"
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-19T11:13:55Z,FIxed some issues
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-18T23:31:49Z,"Ruff fixes, bugfix for topklora"
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-18T23:11:30Z,Fix bug with device and config
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-18T23:11:29Z,"Implement, debug from_pretrained, also update config saving"
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-18T23:11:15Z,Use the new dummy scalings
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-18T20:59:51Z,Clean up the code
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-18T20:58:37Z,Fix circular imports
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-18T20:47:56Z,"Enable, disable the xlora layers to not use dummy scalings"
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-18T20:33:25Z,Clean up the code
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-18T18:44:32Z,Fixed issue of lost gradients with multiple batches
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-18T15:46:41Z,Fixed method to freeze LoRA layers
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-18T01:38:24Z,Fix docstring
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-18T01:29:34Z,Improve tqdm loading behavior
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-17T22:03:51Z,Update the verbose print
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-17T22:02:33Z,"Add, update the xlora conversion API"
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-17T21:58:58Z,Fix docstring
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-17T21:57:28Z,Add scalings logging back
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-17T20:55:16Z,Reversion and bugfix
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-16T17:54:12Z,Fix layerwise ID number
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-16T01:05:18Z,Update docs
github.com/EricLBuehler/xlora,src/xlora/xlora.py,2024-01-15T19:21:58Z,Execute the rename
github.com/liucongg/MolChat,predict_glm.py,2023-05-12T06:01:12Z,update code
github.com/liucongg/MolChat,predict_llama.py,2023-05-12T06:01:12Z,update code
github.com/vmware-labs/research-and-development-artificial-intelligence-lab,instruction-tuning/peft-seq2seq/merge_weights.py,2023-06-05T20:03:14Z,Add copyright & spdx lines
github.com/vmware-labs/research-and-development-artificial-intelligence-lab,instruction-tuning/peft-seq2seq/merge_weights.py,2023-06-01T20:45:47Z,added LLM Peft-Seq2seq instruction tuning code
github.com/brightjade/SimCKP,models/modeling_utils.py,2023-10-09T02:51:39Z,Initial commit
github.com/tiansztiansz/langchain-chatyuan,models/moss_llm.py,2023-05-20T15:04:13Z,提交
github.com/tiansztiansz/langchain-chatyuan,models/chatglm_llm.py,2023-05-20T15:04:13Z,提交
github.com/taishan1994/Chinese-LLaMA-Alpaca-LoRA-Tuning,model_hub/merge_llama_with_chinese_lora.py,2023-05-24T03:53:39Z,commit
github.com/totallylegitco/healthinsurance-llm,test_new_model.py,2023-06-17T16:00:46Z,Progress.
github.com/totallylegitco/healthinsurance-llm,test_new_model.py,2023-06-17T15:46:46Z,Refactor
github.com/totallylegitco/healthinsurance-llm,test_new_model.py,2023-06-03T19:01:12Z,"Handle UTF8 chars.

Fix

Fix"
github.com/totallylegitco/healthinsurance-llm,test_new_model.py,2023-06-03T19:01:11Z,"Fix pipeline usage

Fix

Fix"
github.com/totallylegitco/healthinsurance-llm,test_new_model.py,2023-06-03T19:01:08Z,"Don't use quantization for testing the new model and also (I know it's bad to use our train data for test but just for introspection) print out what the new model looks like.

Fix

Fix

Fix"
github.com/totallylegitco/healthinsurance-llm,test_new_model.py,2023-06-03T01:43:03Z,Update the README with the latest of trying to run training on an AGX (no luck for now)
github.com/AUGMXNT/shisa,eval/merge-lora.py,2023-11-28T07:39:38Z,more eval stuff
github.com/AUGMXNT/shisa,train/qwen/qmerge.py,2023-11-11T16:10:12Z,for merging fine tune
github.com/PKU-YuanGroup/Peer-review-in-LLMs,llm_judge/config/apply_lora.py,2024-02-07T02:29:57Z,Initial commit
github.com/PKU-YuanGroup/Peer-review-in-LLMs,llm_judge/config/model_adapter.py,2024-02-07T02:29:57Z,Initial commit
github.com/HKUDS/GraphEdit,LLM/scripts/apply_lora.py,2024-02-21T05:52:33Z,update
github.com/HKUDS/GraphEdit,LLM/graphedit/model/apply_lora.py,2024-02-22T14:08:17Z,update
github.com/HKUDS/GraphEdit,LLM/graphedit/model/apply_lora.py,2024-02-21T05:52:33Z,update
github.com/HKUDS/GraphEdit,LLM/graphedit/model/model_adapter.py,2024-02-22T14:08:17Z,update
github.com/HKUDS/GraphEdit,LLM/graphedit/model/model_adapter.py,2024-02-21T05:52:33Z,update
github.com/williamliujl/CMExam,src/LoRA/infer.py,2023-08-26T14:30:27Z,add infer
github.com/williamliujl/CMExam,src/LoRA/infer.py,2023-08-25T04:14:37Z,add infer
github.com/williamliujl/CMExam,src/LoRA/generate.py,2023-08-26T14:32:23Z,add generate.py
github.com/boom-R123/ChatWK,merge_lora.py,2023-07-22T12:50:56Z,ChatWK version 0.5
github.com/fecet/alpaca-lora-Chinese,utils.py,2023-03-27T17:10:29Z,upd
github.com/hppRC/simple-simcse-ja,src/experiment.py,2023-10-04T10:38:15Z,Update README.md
github.com/hppRC/simple-simcse-ja,src/experiment.py,2023-09-28T08:23:54Z,format
github.com/hppRC/simple-simcse-ja,src/experiment.py,2023-09-24T14:47:48Z,:+1: Add results
github.com/hppRC/simple-simcse-ja,src/experiment.py,2023-03-11T03:14:00Z,chore
github.com/hppRC/simple-simcse-ja,src/experiment.py,2023-03-02T01:03:15Z,chore
github.com/WeihaoTan/TWOSOME,twosome/virtualhome/policy_v1.py,2023-10-25T02:12:38Z,reach for to close
github.com/WeihaoTan/TWOSOME,twosome/virtualhome/policy_v1.py,2023-09-30T08:58:13Z,update basemodel
github.com/WeihaoTan/TWOSOME,twosome/virtualhome/policy_v1.py,2023-09-15T03:56:01Z,fix critic bug
github.com/WeihaoTan/TWOSOME,twosome/virtualhome/policy_v1.py,2023-09-14T02:50:17Z,fix bugs
github.com/WeihaoTan/TWOSOME,twosome/virtualhome/policy_v1.py,2023-06-23T12:30:46Z,init
github.com/WeihaoTan/TWOSOME,twosome/virtualhome/policy_v2.py,2023-10-25T02:12:38Z,reach for to close
github.com/WeihaoTan/TWOSOME,twosome/virtualhome/policy_v2.py,2023-09-30T08:58:13Z,update basemodel
github.com/WeihaoTan/TWOSOME,twosome/virtualhome/policy_v2.py,2023-09-23T08:05:23Z,fix bugs
github.com/WeihaoTan/TWOSOME,twosome/virtualhome/policy_v2.py,2023-09-15T03:56:01Z,fix critic bug
github.com/WeihaoTan/TWOSOME,twosome/virtualhome/policy_v2.py,2023-09-14T02:50:17Z,fix bugs
github.com/WeihaoTan/TWOSOME,twosome/virtualhome/policy_v2.py,2023-06-23T12:30:46Z,init
github.com/WeihaoTan/TWOSOME,twosome/overcooked/policy_pomdp.py,2023-10-01T11:18:06Z,update save config
github.com/WeihaoTan/TWOSOME,twosome/overcooked/policy_pomdp.py,2023-09-30T08:58:13Z,update basemodel
github.com/WeihaoTan/TWOSOME,twosome/overcooked/policy_pomdp.py,2023-09-15T03:56:01Z,fix critic bug
github.com/WeihaoTan/TWOSOME,twosome/overcooked/policy_pomdp.py,2023-06-23T12:30:46Z,init
github.com/centerforaisafety/HarmBench,adversarial_training/alignment-handbook/scripts/run_dpo.py,2024-02-27T03:30:43Z,HarmBench 1.0 update
github.com/FartyPants/VirtualLora,script.py,2024-02-28T08:15:38Z,Removed add_lora_exllama
github.com/FartyPants/VirtualLora,script.py,2023-12-24T04:29:11Z,Deleteing adapters manually
github.com/FartyPants/VirtualLora,script.py,2023-12-24T04:08:30Z,Trying to go around weird Unload adapters in PEFT
github.com/FartyPants/VirtualLora,script.py,2023-11-07T19:35:27Z,Add files via upload
github.com/FartyPants/VirtualLora,script.py,2023-10-18T20:14:57Z,Add files via upload
github.com/FartyPants/VirtualLora,script.py,2023-10-17T04:07:47Z,Add files via upload
github.com/FartyPants/VirtualLora,script.py,2023-10-17T04:03:31Z,Add files via upload
github.com/FartyPants/VirtualLora,script.py,2023-10-16T02:46:51Z,Add files via upload
github.com/FartyPants/VirtualLora,script.py,2023-10-16T01:40:35Z,Add files via upload
github.com/GreenBitAI/low_bit_llama,model.py,2023-10-05T14:51:04Z,fix repeat memory allocation issue
github.com/GreenBitAI/low_bit_llama,model.py,2023-09-29T09:40:50Z,fix quantization version mismatch issue
github.com/GreenBitAI/low_bit_llama,model.py,2023-09-28T23:10:18Z,fix quantization version mismatch issue
github.com/GreenBitAI/low_bit_llama,model.py,2023-08-16T01:02:01Z,add 2-bit openllama 3B
github.com/GreenBitAI/low_bit_llama,model.py,2023-07-17T14:47:59Z,update for instruction-tuned model
github.com/GreenBitAI/low_bit_llama,model.py,2023-07-09T17:41:24Z,initial commit
github.com/zhangnn520/chinese_llama_alpaca_lora,inference_hf.py,2023-04-20T02:53:25Z,the first upload
github.com/zhangnn520/chinese_llama_alpaca_lora,scripts/merge_llama_with_chinese_lora_to_hf.py,2023-04-20T02:53:25Z,the first upload
github.com/18907305772/FuseLLM,FuseChat/train/model/apply_lora.py,2024-02-26T12:16:14Z,Update FuseChat project.
github.com/18907305772/FuseLLM,FuseChat/train/model/model_adapter.py,2024-02-26T12:16:14Z,Update FuseChat project.
github.com/UW-Madison-Lee-Lab/CoBSAT,load_models/call_qwen.py,2024-01-26T17:43:35Z,update
github.com/UW-Madison-Lee-Lab/CoBSAT,load_models/call_qwen.py,2024-01-25T23:05:31Z,sync
github.com/UW-Madison-Lee-Lab/CoBSAT,load_models/call_qwen.py,2024-01-25T18:11:00Z,update
github.com/UW-Madison-Lee-Lab/CoBSAT,load_models/call_qwen.py,2024-01-25T17:31:20Z,update finetuned model inference + evaluation
github.com/UW-Madison-Lee-Lab/CoBSAT,load_models/call_qwen.py,2024-01-25T16:53:52Z,add qwen to evaluation
github.com/UW-Madison-Lee-Lab/CoBSAT,load_models/call_qwen.py,2024-01-25T03:15:55Z,update
github.com/UW-Madison-Lee-Lab/CoBSAT,load_models/call_qwen.py,2024-01-25T02:56:52Z,update finetuning
github.com/UW-Madison-Lee-Lab/CoBSAT,load_models/call_qwen.py,2024-01-24T22:27:20Z,sync
github.com/UW-Madison-Lee-Lab/CoBSAT,load_models/call_qwen.py,2024-01-24T22:13:44Z,fix the bug
github.com/UW-Madison-Lee-Lab/CoBSAT,load_models/call_qwen.py,2024-01-24T21:31:22Z,update history saving
github.com/UW-Madison-Lee-Lab/CoBSAT,load_models/call_qwen.py,2024-01-24T17:27:34Z,fix the bug
github.com/UW-Madison-Lee-Lab/CoBSAT,load_models/call_qwen.py,2024-01-24T07:54:02Z,implemented cot
github.com/UW-Madison-Lee-Lab/CoBSAT,load_models/call_qwen.py,2024-01-24T05:25:32Z,finished implementing caption experiment
github.com/UW-Madison-Lee-Lab/CoBSAT,load_models/call_qwen.py,2024-01-24T04:52:19Z,update new prompt
github.com/UW-Madison-Lee-Lab/CoBSAT,load_models/call_qwen.py,2024-01-23T18:08:28Z,organized the instruction dict
github.com/UW-Madison-Lee-Lab/CoBSAT,load_models/call_qwen.py,2024-01-05T09:43:12Z,update the default shots and qwen prompt
github.com/UW-Madison-Lee-Lab/CoBSAT,load_models/call_qwen.py,2024-01-05T09:30:00Z,half implemented evaluation
github.com/UW-Madison-Lee-Lab/CoBSAT,load_models/call_qwen.py,2024-01-05T09:26:06Z,update the prompts
github.com/UW-Madison-Lee-Lab/CoBSAT,load_models/call_qwen.py,2024-01-04T08:36:19Z,update prompts
github.com/UW-Madison-Lee-Lab/CoBSAT,load_models/call_qwen.py,2024-01-04T08:07:31Z,updated prompt for qwen
github.com/UW-Madison-Lee-Lab/CoBSAT,load_models/call_qwen.py,2024-01-04T06:27:20Z,update
github.com/UW-Madison-Lee-Lab/CoBSAT,load_models/call_qwen.py,2024-01-03T06:48:57Z,update load_model
github.com/UW-Madison-Lee-Lab/CoBSAT,load_models/call_qwen.py,2023-12-26T09:56:05Z,reimplement text_inference
github.com/UW-Madison-Lee-Lab/CoBSAT,load_models/call_qwen.py,2023-12-26T08:59:13Z,implement qwen
github.com/UW-Madison-Lee-Lab/CoBSAT,load_models/call_qwen.py,2023-12-26T05:47:28Z,update code
github.com/georgepullen/Merge-LoRA-Into-GGUF,merge_lora_into_gguf.py,2023-09-06T18:07:37Z,Update merge_lora_into_gguf.py
github.com/georgepullen/Merge-LoRA-Into-GGUF,merge_lora_into_gguf.py,2023-09-06T16:31:49Z,Update and rename merge_lora_into_ggml.py to merge_lora_into_gguf.py
github.com/xxm1668/Chatglm_tune_gpus,infer_lora.py,2023-05-15T08:58:49Z,微调推理
github.com/xxm1668/Chatglm_tune_gpus,infer_lora.py,2023-05-15T08:57:35Z,微调推理
github.com/xxm1668/Chatglm_tune_gpus,infer_lora.py,2023-05-15T08:40:59Z,微调
github.com/photomz/BabyDoctor,llava/model/merger.py,2023-07-30T00:24:55Z,stuff
github.com/official-elinas/zeus-llm-trainer,generate.py,2023-05-09T00:48:43Z,Update to latest
github.com/official-elinas/zeus-llm-trainer,generate.py,2023-03-16T23:04:06Z,add Gradio interface to generate.py
github.com/official-elinas/zeus-llm-trainer,generate.py,2023-03-16T19:11:47Z,Catch outdated installs
github.com/official-elinas/zeus-llm-trainer,generate.py,2023-03-16T19:11:29Z,Update alpaca-lora to use transformers main branch
github.com/official-elinas/zeus-llm-trainer,generate.py,2023-03-16T16:59:10Z,Expand sampling in generate.py for new test
github.com/official-elinas/zeus-llm-trainer,generate.py,2023-03-16T07:05:32Z,Add counting test
github.com/official-elinas/zeus-llm-trainer,generate.py,2023-03-16T00:22:22Z,"generate.py memory, perf updates"
github.com/official-elinas/zeus-llm-trainer,generate.py,2023-03-15T18:11:26Z,torch.no_grad
github.com/official-elinas/zeus-llm-trainer,generate.py,2023-03-15T04:41:02Z,add text-davinci-003 to comparisons
github.com/official-elinas/zeus-llm-trainer,generate.py,2023-03-15T04:33:12Z,Update README.md with new checkpoint details
github.com/official-elinas/zeus-llm-trainer,generate.py,2023-03-14T22:10:33Z,Ready to go
github.com/official-elinas/zeus-llm-trainer,generate.py,2023-03-14T00:23:29Z,decapoda
github.com/official-elinas/zeus-llm-trainer,generate.py,2023-03-13T22:00:05Z,Licenses and whatnot
github.com/official-elinas/zeus-llm-trainer,scripts/export_hf_checkpoint.py,2023-05-14T20:32:04Z,move scripts to scripts dir
github.com/official-elinas/zeus-llm-trainer,scripts/merge_lora_hf_checkpoint.py,2023-05-14T20:32:04Z,move scripts to scripts dir
github.com/official-elinas/zeus-llm-trainer,scripts/export_state_dict_checkpoint.py,2023-05-14T20:32:04Z,move scripts to scripts dir
github.com/tpdmason/TabbyML,python/tabby/trainer.py,2023-06-13T19:48:27Z,feat: cleanup trainer with new data format
github.com/pipilurj/MLLM-protector,llava/model/builder.py,2024-02-28T17:21:45Z,submit code
github.com/huxiaosheng123/open-llama2,scripts/scripts/inference/gradio_demo.py,2023-08-31T16:23:16Z,update code
github.com/huxiaosheng123/open-llama2,scripts/scripts/inference/inference_hf_tool.py,2023-08-31T16:23:16Z,update code
github.com/huxiaosheng123/open-llama2,scripts/scripts/openai_server_demo/openai_api_server.py,2023-08-31T16:23:16Z,update code
github.com/huxiaosheng123/open-llama2,scripts/scripts/merge_llama2_with_chinese_lora_low_mem.py,2023-08-31T16:23:16Z,update code
github.com/EdVince/whisper-trtllm,transformers/src/transformers/trainer.py,2023-08-22T07:50:00Z,add:transformers
github.com/EdVince/whisper-trtllm,transformers/src/transformers/modeling_utils.py,2023-08-22T07:50:00Z,add:transformers
github.com/EdVince/whisper-trtllm,transformers/src/transformers/lib_integrations/peft/peft_mixin.py,2023-08-22T07:50:00Z,add:transformers
github.com/HKUDS/HiGPT,higpt/model/builder.py,2024-02-23T13:50:20Z,up
github.com/HKUDS/HiGPT,higpt/model/apply_lora.py,2024-02-23T13:50:20Z,up
github.com/X-D-Lab/KarmaVLM,llava/model/builder.py,2024-02-25T12:09:23Z,init
github.com/RBDash-Team/RBDash,rbdash/model/builder.py,2023-12-20T01:33:52Z,[feat] Add all files
