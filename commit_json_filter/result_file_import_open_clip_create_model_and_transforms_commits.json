[
    {
        "repo_url": "github.com/google-research/google-research",
        "filepath": "agile_modeling/load_clip.py",
        "commit_date": "2024-01-23T00:22:24Z",
        "message": "Open-sourcing the code for \"CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor\".\nhttps://arxiv.org/abs/2312.07661\n\nPiperOrigin-RevId: 600601383"
    },
    {
        "repo_url": "github.com/google-research/google-research",
        "filepath": "agile_modeling/load_clip.py",
        "commit_date": "2023-09-15T18:25:10Z",
        "message": "Steps 1-3 (up to initial model training) ported to external colab.\n\nPiperOrigin-RevId: 565729930"
    },
    {
        "repo_url": "github.com/langchain-ai/langchain",
        "filepath": "libs/experimental/langchain_experimental/open_clip/open_clip.py",
        "commit_date": "2024-02-24T02:24:16Z",
        "message": "experimental: docstrings update (#18048)\n\nAdded missed docstrings. Formatted docsctrings to the consistent format."
    },
    {
        "repo_url": "github.com/langchain-ai/langchain",
        "filepath": "libs/experimental/langchain_experimental/open_clip/open_clip.py",
        "commit_date": "2024-01-02T20:09:45Z",
        "message": "langchain[patch], experimental[patch]: replace langchain.schema imports (#15410)\n\nImport from core instead.\n\nRan:\n```bash\ngit grep -l 'from langchain.schema\\.output_parser' | xargs -L 1 sed -i '' \"s/from\\ langchain\\.schema\\.output_parser/from\\ langchain_core.output_parsers/g\"\ngit grep -l 'from langchain.schema\\.messages' | xargs -L 1 sed -i '' \"s/from\\ langchain\\.schema\\.messages/from\\ langchain_core.messages/g\"\ngit grep -l 'from langchain.schema\\.document' | xargs -L 1 sed -i '' \"s/from\\ langchain\\.schema\\.document/from\\ langchain_core.documents/g\"\ngit grep -l 'from langchain.schema\\.runnable' | xargs -L 1 sed -i '' \"s/from\\ langchain\\.schema\\.runnable/from\\ langchain_core.runnables/g\"\ngit grep -l 'from langchain.schema\\.vectorstore' | xargs -L 1 sed -i '' \"s/from\\ langchain\\.schema\\.vectorstore/from\\ langchain_core.vectorstores/g\"\ngit grep -l 'from langchain.schema\\.language_model' | xargs -L 1 sed -i '' \"s/from\\ langchain\\.schema\\.language_model/from\\ langchain_core.language_models/g\"\ngit grep -l 'from langchain.schema\\.embeddings' | xargs -L 1 sed -i '' \"s/from\\ langchain\\.schema\\.embeddings/from\\ langchain_core.embeddings/g\"\ngit grep -l 'from langchain.schema\\.storage' | xargs -L 1 sed -i '' \"s/from\\ langchain\\.schema\\.storage/from\\ langchain_core.stores/g\"\ngit checkout master libs/langchain/tests/unit_tests/schema/\nmake format\ncd libs/experimental\nmake format\ncd ../langchain\nmake format\n```"
    },
    {
        "repo_url": "github.com/langchain-ai/langchain",
        "filepath": "libs/experimental/langchain_experimental/open_clip/open_clip.py",
        "commit_date": "2023-12-05T21:36:38Z",
        "message": "Multi-modal RAG template (#14186)\n\n* OpenCLIP embeddings\n* GPT-4V\n\n---------\n\nCo-authored-by: Erick Friis <erick@langchain.dev>"
    },
    {
        "repo_url": "github.com/langchain-ai/langchain",
        "filepath": "libs/experimental/langchain_experimental/open_clip/open_clip.py",
        "commit_date": "2023-12-01T23:13:20Z",
        "message": "Update Open CLIP embd (#14155)\n\nPrior default model required a large amt of RAM and often crashed\nJupyter ntbk kernel."
    },
    {
        "repo_url": "github.com/langchain-ai/langchain",
        "filepath": "libs/experimental/langchain_experimental/open_clip/open_clip.py",
        "commit_date": "2023-11-10T17:43:10Z",
        "message": "Add Chroma multimodal cookbook (#12952)\n\nPending:\n* https://github.com/chroma-core/chroma/pull/1294\n* https://github.com/chroma-core/chroma/pull/1293\n\n---------\n\nCo-authored-by: Erick Friis <erick@langchain.dev>\nCo-authored-by: Bagatur <baskaryan@gmail.com>"
    },
    {
        "repo_url": "github.com/Stability-AI/generative-models",
        "filepath": "sgm/modules/encoders/modules.py",
        "commit_date": "2023-07-26T08:30:21Z",
        "message": "Revert \"Replace most print()s with logging calls (#42)\" (#65)\n\nThis reverts commit 6f6d3f8716bda7d9c74405a7d1d8e4beaa74f9d2."
    },
    {
        "repo_url": "github.com/Stability-AI/generative-models",
        "filepath": "sgm/modules/encoders/modules.py",
        "commit_date": "2023-07-25T13:21:30Z",
        "message": "Replace most print()s with logging calls (#42)"
    },
    {
        "repo_url": "github.com/AUTOMATIC1111/stable-diffusion-webui",
        "filepath": "modules/sd_disable_initialization.py",
        "commit_date": "2023-12-14T07:15:22Z",
        "message": "Merge pull request #14216 from wfjsw/state-dict-ref-comparison\n\nchange state dict comparison to ref compare"
    },
    {
        "repo_url": "github.com/AUTOMATIC1111/stable-diffusion-webui",
        "filepath": "modules/sd_disable_initialization.py",
        "commit_date": "2023-08-16T09:11:01Z",
        "message": "send weights to target device instead of CPU memory"
    },
    {
        "repo_url": "github.com/AUTOMATIC1111/stable-diffusion-webui",
        "filepath": "modules/sd_disable_initialization.py",
        "commit_date": "2023-07-24T19:08:08Z",
        "message": "Use less RAM when creating models"
    },
    {
        "repo_url": "github.com/AUTOMATIC1111/stable-diffusion-webui",
        "filepath": "modules/sd_disable_initialization.py",
        "commit_date": "2023-02-05T08:21:00Z",
        "message": "make it possible to load SD1 checkpoints without CLIP"
    },
    {
        "repo_url": "github.com/AUTOMATIC1111/stable-diffusion-webui",
        "filepath": "modules/sd_disable_initialization.py",
        "commit_date": "2023-01-22T05:20:48Z",
        "message": "fix missing field for aesthetic embedding extension"
    },
    {
        "repo_url": "github.com/AUTOMATIC1111/stable-diffusion-webui",
        "filepath": "modules/sd_disable_initialization.py",
        "commit_date": "2023-01-11T15:54:13Z",
        "message": "fix for an error caused by skipping initialization, for realsies this time: TypeError: expected str, bytes or os.PathLike object, not NoneType"
    },
    {
        "repo_url": "github.com/AUTOMATIC1111/stable-diffusion-webui",
        "filepath": "modules/sd_disable_initialization.py",
        "commit_date": "2023-01-10T14:46:59Z",
        "message": "add support for transformers==4.25.1\nadd fallback for when quick model creation fails"
    },
    {
        "repo_url": "github.com/AUTOMATIC1111/stable-diffusion-webui",
        "filepath": "modules/sd_disable_initialization.py",
        "commit_date": "2023-01-10T13:51:04Z",
        "message": "add more stuff to ignore when creating model from config\nprevent .vae.safetensors files from being listed as stable diffusion models"
    },
    {
        "repo_url": "github.com/AUTOMATIC1111/stable-diffusion-webui",
        "filepath": "modules/sd_disable_initialization.py",
        "commit_date": "2023-01-10T11:08:29Z",
        "message": "disable torch weight initialization and CLIP downloading/reading checkpoint to speedup creating sd model from config"
    },
    {
        "repo_url": "github.com/hpcaitech/ColossalAI",
        "filepath": "examples/images/diffusion/ldm/modules/encoders/modules.py",
        "commit_date": "2023-09-19T06:20:26Z",
        "message": "[misc] update pre-commit and run all files (#4752)\n\n* [misc] update pre-commit\n\n* [misc] run pre-commit\n\n* [misc] remove useless configuration files\n\n* [misc] ignore cuda for clang-format"
    },
    {
        "repo_url": "github.com/hpcaitech/ColossalAI",
        "filepath": "examples/images/diffusion/ldm/modules/encoders/modules.py",
        "commit_date": "2022-11-08T06:39:35Z",
        "message": "add ColoDiffusion codes: /ldm/module/, /ldm/data/, /scripts/test/"
    },
    {
        "repo_url": "github.com/brycedrennan/imaginAIry",
        "filepath": "imaginairy/modules/encoders.py",
        "commit_date": "2023-12-15T22:32:01Z",
        "message": "docs: add docstrings\n\nWrote an openai script and custom prompt to generate them."
    },
    {
        "repo_url": "github.com/brycedrennan/imaginAIry",
        "filepath": "imaginairy/modules/encoders.py",
        "commit_date": "2023-09-30T06:01:50Z",
        "message": "style: speed up linting and autoformatting. fix lints"
    },
    {
        "repo_url": "github.com/brycedrennan/imaginAIry",
        "filepath": "imaginairy/modules/encoders.py",
        "commit_date": "2022-11-24T08:50:57Z",
        "message": "feature: Stable Diffusion 2.0\n\nworking: CUDA and MacOS\nworking: 512p model with all samplers\nworking: inpainting with all samplers\nworking: 768p model with ddim sampler"
    },
    {
        "repo_url": "github.com/Sygil-Dev/Sygil-WebUI",
        "filepath": "scripts/img2txt.py",
        "commit_date": "2023-06-23T02:58:24Z",
        "message": "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci"
    },
    {
        "repo_url": "github.com/Sygil-Dev/Sygil-WebUI",
        "filepath": "scripts/img2txt.py",
        "commit_date": "2022-11-03T07:05:54Z",
        "message": "Initial conversion of the sd_util.py file into a package with smaller modules in it."
    },
    {
        "repo_url": "github.com/Sygil-Dev/Sygil-WebUI",
        "filepath": "scripts/img2txt.py",
        "commit_date": "2022-11-01T17:25:43Z",
        "message": "Cleaned some commented code that is no longer needed."
    },
    {
        "repo_url": "github.com/Sygil-Dev/Sygil-WebUI",
        "filepath": "scripts/img2txt.py",
        "commit_date": "2022-10-31T20:38:31Z",
        "message": "Added support for webp images in the img2txt tab"
    },
    {
        "repo_url": "github.com/Sygil-Dev/Sygil-WebUI",
        "filepath": "scripts/img2txt.py",
        "commit_date": "2022-10-27T01:40:17Z",
        "message": "Moved the generate button to the top of the page so it doesn't get pushed down and disappear when there are many images or the expanders are not collapsed."
    },
    {
        "repo_url": "github.com/Sygil-Dev/Sygil-WebUI",
        "filepath": "scripts/img2txt.py",
        "commit_date": "2022-10-24T11:45:09Z",
        "message": "Added logging back to the img2txt tab."
    },
    {
        "repo_url": "github.com/Sygil-Dev/Sygil-WebUI",
        "filepath": "scripts/img2txt.py",
        "commit_date": "2022-10-24T07:22:36Z",
        "message": "Improved the img2txt tab by having more tags."
    },
    {
        "repo_url": "github.com/Sygil-Dev/Sygil-WebUI",
        "filepath": "scripts/img2txt.py",
        "commit_date": "2022-10-24T07:21:54Z",
        "message": "Added option to make the grid format the same as the individual image format.\nThere is a new option called grid quality now that allow you to specify on the settings page the quality for the grid image."
    },
    {
        "repo_url": "github.com/Sygil-Dev/Sygil-WebUI",
        "filepath": "scripts/img2txt.py",
        "commit_date": "2022-10-24T01:54:10Z",
        "message": "More renaming and changed to links related to the organization, docs and repo names."
    },
    {
        "repo_url": "github.com/Sygil-Dev/Sygil-WebUI",
        "filepath": "scripts/img2txt.py",
        "commit_date": "2022-10-24T00:17:50Z",
        "message": "More renaming and changes to links related to the organization, docs an repo names."
    },
    {
        "repo_url": "github.com/Sygil-Dev/Sygil-WebUI",
        "filepath": "scripts/img2txt.py",
        "commit_date": "2022-10-23T23:22:40Z",
        "message": "Fixed links so they point to the new repo and organization names."
    },
    {
        "repo_url": "github.com/Sygil-Dev/Sygil-WebUI",
        "filepath": "scripts/img2txt.py",
        "commit_date": "2022-10-22T20:30:12Z",
        "message": "Made sure the log variable is still around on the img2txt after its being cleared."
    },
    {
        "repo_url": "github.com/Sygil-Dev/Sygil-WebUI",
        "filepath": "scripts/img2txt.py",
        "commit_date": "2022-10-15T16:32:15Z",
        "message": "Replaced most if not all print statements with loguru to make the console output easier to understand and just look better overall."
    },
    {
        "repo_url": "github.com/Sygil-Dev/Sygil-WebUI",
        "filepath": "scripts/img2txt.py",
        "commit_date": "2022-10-06T07:03:45Z",
        "message": "Added extra models back to img2txt."
    },
    {
        "repo_url": "github.com/Sygil-Dev/Sygil-WebUI",
        "filepath": "scripts/img2txt.py",
        "commit_date": "2022-10-04T10:43:36Z",
        "message": "Update img2txt.py\n\n// is not a comment"
    },
    {
        "repo_url": "github.com/Sygil-Dev/Sygil-WebUI",
        "filepath": "scripts/img2txt.py",
        "commit_date": "2022-10-02T02:19:04Z",
        "message": "Merge remote-tracking branch 'origin/dev' into dev"
    },
    {
        "repo_url": "github.com/Sygil-Dev/Sygil-WebUI",
        "filepath": "scripts/img2txt.py",
        "commit_date": "2022-10-01T22:39:12Z",
        "message": "Added WIP code for img2txt to get information dynamically from artstation."
    },
    {
        "repo_url": "github.com/Sygil-Dev/Sygil-WebUI",
        "filepath": "scripts/img2txt.py",
        "commit_date": "2022-09-30T20:04:08Z",
        "message": "Add more print statements to provide more info on whats happening behind the scenes."
    },
    {
        "repo_url": "github.com/Sygil-Dev/Sygil-WebUI",
        "filepath": "scripts/img2txt.py",
        "commit_date": "2022-09-30T15:47:30Z",
        "message": "Added open clip dependency which is needed for some CLIP models."
    },
    {
        "repo_url": "github.com/Sygil-Dev/Sygil-WebUI",
        "filepath": "scripts/img2txt.py",
        "commit_date": "2022-09-29T15:52:46Z",
        "message": "Img2txt working now but needs more than 8GB og VRAM to work. Will be trying to improve it as the next step."
    },
    {
        "repo_url": "github.com/Sygil-Dev/Sygil-WebUI",
        "filepath": "scripts/img2txt.py",
        "commit_date": "2022-09-28T19:37:15Z",
        "message": "Img2txt dependencies and necessary files. (#1354)"
    },
    {
        "repo_url": "github.com/Sygil-Dev/Sygil-WebUI",
        "filepath": "scripts/img2txt.py",
        "commit_date": "2022-09-28T16:33:54Z",
        "message": "Changed the default streamlit import for hydralit as we will be using hydralit as replacement for the default streamlit library, hydralit provides better control over css as well as having a lot more options. (#1352)"
    },
    {
        "repo_url": "github.com/NVIDIA/NeMo",
        "filepath": "scripts/fid-eval-text2img/compute_clip_score.py",
        "commit_date": "2024-01-20T01:59:23Z",
        "message": "Final multimodal PR with our recent developments on MM side (#8127)\n\n* Hotfix (#7501) (#7568)\n\nSigned-off-by: Jan Baczek <jbaczek@nvidia.com>\nCo-authored-by: jbaczek <45043825+jbaczek@users.noreply.github.com>\n\n* Avoid duplicated checkpoint save (#7555) (#7566)\n\nSigned-off-by: Miko\u0142aj B\u0142a\u017c <mblaz@nvidia.com>\nCo-authored-by: mikolajblaz <mikolajblaz@users.noreply.github.com>\n\n* Cache FP8 weight and transpose only at the first micro-batch in each validation and test routine (#7470) (#7483)\n\n* Cache weight and transpose only in the first batch in all training, val, and test runs\n\n\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Add an option to disable manual GC in validation (#7467) (#7476)\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\n\n* Remove PUBLICATIONS.md, point to github.io NeMo page instead (#7694) (#7695)\n\n* update publications section to point to blog website page\n\n\n\n* add hyphen\n\n\n\n* use double backquotes for code formatting\n\n\n\n---------\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\nSigned-off-by: Elena Rastorgueva <80532067+erastorgueva-nv@users.noreply.github.com>\nCo-authored-by: Elena Rastorgueva <80532067+erastorgueva-nv@users.noreply.github.com>\n\n* Fix multi rank finetune for ASR (#7684) (#7699)\n\n* Fix multi rank finetune for ASR\n\n\n\n* Actually add time\n\n\n\n* Actually add time\n\n\n\n---------\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\n\n* Update docs: readme, getting started, ASR intro (#7679)\n\n* [TTS] Add dataset to path of logged artifacts (#7462)\n\n* [TTS] Add dataset to path of logged artifacts\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Revert axis name back to Audio Frames\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* move install info to INSTALLATION.md\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* tidy up links\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix sft dataset truncation (#7464)\n\n* Add fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Automatic Lip Reading Recognition (ALR) - ASR/CV (Visual ASR) (#7330)\n\n* striding_conv1d_k5 and dw_striding_conv1d_k5 subsampling\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* transpose conv1d inputs\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* Update subsampling.py\n\nchange striding_conv1d_k5 to striding_conv1d\n\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\n\n* cv branch\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* video manifest\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* add collection classes\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add test_step_outputs\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* correct manifest bug when having only audio or only videos\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* correct manifest bug when having only audio or only videos\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* clean references\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* freeze unfreeze transcribe cv models\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* correct manifest get_full_path bug\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* update for PR\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* guard torchvision\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update nemo/collections/cv/data/video_to_text_dataset.py\n\nCo-authored-by: Igor Gitman <igor.a.gitman@gmail.com>\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\n\n* _video_speech_collate_fn in cv/data/video_to_text.py\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* add self.out = None to asr subsampling\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* Update nemo/collections/cv/data/video_to_text_dataset.py\n\nCo-authored-by: Igor Gitman <igor.a.gitman@gmail.com>\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\n\n* cv -> multimodal/speech_cv branch\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Igor Gitman <igor.a.gitman@gmail.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* HF StarCoder to NeMo conversion script (#7421)\n\n* Script to convert HF StarCoder checkpoint to NeMo\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* StarCoder conversion test\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Fix test\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Catch up with save_to changes\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Don't abbreviate args for clarity\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Configurable precision: BF16 vs FP32\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* fix bug when loading dist ckpt in peft (#7452)\n\nSigned-off-by: Hongbin Liu <hongbinl@nvidia.com>\nCo-authored-by: Hongbin Liu <hongbinl@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix adding positional embeddings in-place in transformer module (#7440)\n\nSigned-off-by: Tamerlan Tabolov <tktabolov@gmail.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix (#7478)\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* add sleep (#7498) (#7499)\n\n* add sleep\n\n\n\n* add sleep onto config instead\n\n\n\n* add comment\n\n\n\n---------\n\nSigned-off-by: Gerald Shen <geshen@nvidia.com>\nCo-authored-by: Gerald Shen <119401249+gshennvm@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix exp manager check for sleep (#7503) (#7504)\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* bugfix: trainer.accelerator=auto from None. (#7492) (#7493)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [doc] fix broken link (#7481)\n\nSigned-off-by: Stas Bekman <stas00@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [TTS] Read audio as int32 to avoid flac read errors (#7477)\n\n* [TTS] Read audio as int32 to avoid flac read errors\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Add comment about read failures\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Add dataset 'AISHELL-3' from OpenSLR for training mandarin TTS (#7409)\n\n* Add dataset 'AISHELL-3' from OpenSLR for training mandarin TTS\n* Train 'AISHELL-3' dataset with multi-speakers\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update get_data.py\n\nupdate copyright header\n\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* Update get_data.py\n\nadded a disclaimer\n\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add new configuration file for AISHELL3 with multispeaker of fastpitch\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* dllogger - log on rank 0 only (#7513)\n\nSigned-off-by: Stas Bekman <stas00@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix TTS FastPitch tutorial (#7494) (#7516)\n\n* Fix\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix get_dist() tensor dimension (#7506) (#7515)\n\nSigned-off-by: Jocelyn Huang <jocelynh@nvidia.com>\nCo-authored-by: Jocelyn <jocelynh@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* bugfix: specify trainer.strategy=auto when devices=1 (#7509) (#7512)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* fix (#7511)\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [TTS] Fix FastPitch data prep tutorial (#7524)\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* add italian tokenization (#7486)\n\n* add italian tokenization\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add more ipa lexicon it\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix error deletion\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* add test\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Replace None strategy with auto in tutorial notebooks (#7521) (#7527)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* unpin setuptools (#7534) (#7535)\n\nSigned-off-by: fayejf <36722593+fayejf@users.noreply.github.com>\nCo-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* remove auto generated examples (#7510)\n\n* explicitly remove autogenerated examples for data parallel evaluation\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* mark autogenrated and remove it for test\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: arendu <adithyare@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Add the `strategy` argument to `MegatronGPTModel.generate()` (#7264)\n\nIt is passed as an explicit argument rather than through\n`**strategy_args` so as to ensure someone cannot accidentally pass other\narguments that would end up being ignored.\n\nIt is a keyword-only argument to ensure that if in the future we want to\nupdate the signature to `**strategy_args`, we can do it without breaking\ncode.\n\nSigned-off-by: Olivier Delalleau <507137+odelalleau@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix PTL2.0 related ASR bugs in r1.21.0: Val metrics logging, None dataloader issue (#7531) (#7533)\n\n* fix none dataloader issue ptl2\n\n\n\n* ptl2.0 logging fixes for rnnt_models\n\n\n\n---------\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\nCo-authored-by: Kunal Dhawan <kunaldhawan97@gmail.com>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* gpus -> devices (#7542) (#7545)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Update FFMPEG version to fix issue with torchaudio (#7551) (#7553)\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* PEFT GPT & T5 Refactor (#7308)\n\n* initial implementation of add_adapters API\n\n* correct type hint\n\n* Add config in add_adapters for save and load (@author bobchen)\n\n* Remove AdapterConfig to avoid import error\n\n* Add AdaterConfig back and move adaptermixin to sft model\n\n* Add NLPSaveRestoreConnector as default in NLPModel.restore_from\n\n* Add restore_from_nemo_with_adapter and test script\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* rename t5 file and classes to be consistent with GPT\n\n* add t5 sft dataset\n\n* add support for single-file format with T5SFTDataset\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Various small changes to make T5 SFT work like GPT SFT\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add adapter evaluation test script\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add MultiAdaterConfig for ia3 and fix builder issue\n\n* Make ptuning for T5SFTModel work using mixin\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add IA3_Adapter for AdapterName\n\n* Add adapter name for ptuning and attention adapter\n\n* Make test script GPT/T5 agnostic\n\n* Add layer selection feature\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Integrate adapter name and config\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update gpt peft tuning script to new API\n\n* add t5 peft tuning script with new API\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix IA3 layer selection issue\n\n* Override state_dict on SFT model instead of mixin\n\n* Add load adapter by adapter config\n\n* move peft config map away from example script\n\n* auto get config from nemo adapter\n\n* Move PEFTConfig to new file\n\n* fix ckpt save/load for t5\n\n* name change: add_adapters -> add_adapter\n\n* variable name change\n\n* update t5 script\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix t5 issues\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add weight tying\n\n* update gpt tuning script\n\n* PEFT-API proposal\n\n* Fix according to comments\n\n* update tuning scripts\n\n* move merge_cfg_with to mixin class since it applies to both gpt and t5 and requires the model class for restore\n\n* Add mcore_gpt support for NLPAdapterMixin\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix typo\n\n* variable name change to distinguish \"peft\" and \"adapter\"\n\n* override `load_adapters` to support `add_adapter` name change\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update tuning and eval script for adapter save/load\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add Ptuning on first stage only\n\n* add lora tutorial for review\n\n* Fix layer selection for mcore\n\n* add landing page\n\n* fix resume training\n\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add mcore condition in sharded_state_dict to make sft work\n\n* Update lora_tutorial.md\n\nFirst edit of this file for PEFT documentation for NeMO\n\nSigned-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\n\n* rename Adapter to AttentionAdapter to avoid confusion in doc\n\n* Change load_adapters to load .nemo\n\n* add quick start guide\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add load_adapters with .ckpt\n\n* Remove setup_complete changes in load_adapters\n\n* update landing page\n\n* remove typo\n\n* Updated quick_start.md per Chen Cui\n\nSigned-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\n\n* Add inference config merger and tutorial\n\n* Add doc string for NLPAdapterModelMixin and deprecated warning on MegatronGPTPEFTModel\n\n* add supported_methods.md and update other documentations\n\n* Update supported_methods.md\n\nminor updates.\n\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\n\n* Update landing_page.md\n\nminor update.\n\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\n\n* Modify doc string for NLPAdapterModelMixin\n\n* Add doc string add_adapters in NLPAdapterModelMixin\n\n* rename canonical adapters\n\n* remove mcore hard dependency\n\n* [PATCH] move microbatch calculator to nemo from apex\n\n* remove apex dependency in gpt and t5 sft models\n\n* remove apex dependency in gpt model\n\n* render doc strings\n\n* fix\n\n* Add missing virtual_tokens on ptuning\n\n* fix docstrings\n\n* update gpt-style model coverage in docs\n\n* update docstring\n\n* Remove pdb\n\n* add lightning_fabric to make docstring rendering work\n\n* Add Ptuning missing key\n\n* try docstring rendering\n\n* Fix ptuning issue\n\n* update gpt t5 peft tuning and eval scripts\n\n* typos\n\n* update eval config\n\n* fix bug relating to apex dependency removal\n\n* typo\n\n* make predict step behave the same as test step\n\n* make lora tutorial work in notebook\n\n* cosmetics\n\n* update yaml scripts\n\n* mcore_gpt attribute optional\n\n* typo\n\n* update eval scripts and fix T5 eval bugs\n\n* add NLPDDPStrategyNotebook and trainer builder logic to use it\n\n* update lora notebook to use new trainer builder\n\n* fix microbatch calculator bug for inference after training\n\n* Convert markdown files to RST and incorporate with doc\n\n* typo\n\n* revise language\n\n* remove extra cell\n\n* remove unnecessary inheritance\n\n* remove old tests\n\n* move layer selection default so logging messages make sense\n\n* remove `save_adapters` as adapter weights are saved automatically during training\n\n* initialize weights from a checkpoint instead of randomly\n\n* multiple fields can form a context (#7147)\n\n* list of context fields and flexible prompt template\n\nSigned-off-by: arendu <adithya.r@gmail.com>\n\n* list of fields for context\n\nSigned-off-by: arendu <adithya.r@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Add multiple truncation fields and middle truncation\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Compatible to old ckpt\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix tokenize detokenize issue\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Remove detokenization, add truncation augmentation\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Resolve comments\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Remove unused import\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* revert eos\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Add tokenizer space_sensitive attribute\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix error\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Fix erorr and use re\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Change assert logic\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Follow adi suggestion\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Remove merge function\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add example and comment\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Remove context_key and add comment\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Remove random truncation\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix template none\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-off-by: arendu <adithya.r@gmail.com>\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nSigned-off-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\n* revert config changes\n\n* remove accidental breakpoint\n\n* support TP>1 loading\n\n* infer adapter type from checkpoint in during eval\n\n* breakup add adapter\n\n* enable interpolation of train_ds and validation_ds\n\n* update metric calc script to conform to single-file eval format\n\n* remove extraneous print\n\n* update lora notebook for updated merge_inference_cfg\n\n* Update nlp_adapter_mixins.py\n\nvariable name change\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* turn off grad scaler for PP to match old scripts\n\n* remove PEFTSaveRestoreConnector since functionality all covered by the new mixin class\n\n* remove resume_from_checkpoint check since covered in #7335\n\n* revert changes made in eval config interpolation\n\n* more interpolation\n\n* typo\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove dup line\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* code style warnings\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix config mistake\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* add copyright header\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* fix code check warnings\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* revert changes to remove apex dependency (mixed apex+nemo microbatch calculator broke some CI tests)\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add more deprecation notices\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* update deprecation notices\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* update deprecation notices\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* consolidate peft and sft scripts\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* update CI tests\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* notebook branch points to main to prepare for merge\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* fix gpt and t5 validation with any metric other than loss\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* support pre-extracted checkpoints\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n---------\n\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\nSigned-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\nSigned-off-by: arendu <adithya.r@gmail.com>\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nSigned-off-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\nSigned-off-by: Chen Cui <chcui@nvidia.com>\nCo-authored-by: Chen Cui <chcui@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Marc Romeyn <marcromeyn@gmail.com>\nCo-authored-by: jasonwan <jasonwan@nvidia.com>\nCo-authored-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\nCo-authored-by: Adi Renduchintala <adithyare@nvidia.com>\nCo-authored-by: Yuanzhe Dong <yudong@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* fix a typo (#7496)\n\nSigned-off-by: BestJuly <chntaoli@163.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [TTS] remove curly braces from ${BRANCH} in jupyer notebook cell. (#7554) (#7560)\n\n* remove curly braces.\n* remove installation of pynini.\n---------\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* add youtube embed url (#7570)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Remap speakers to continuous range of speaker_id for dataset AISHELL3 (#7536)\n\n* Remap speakers to continuous range of speaker_id for dataset AISHELL3\n* Add new key/value pair to record raw speaker for AISHELL3 dataset\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* fix validation_step_outputs initialization for multi-dataloader (#7546) (#7572)\n\n* added correct validation_step_outputs initialization for mutli-dataloader\n\n\n\n* changed kernel for display\n\n\n\n* Update logic for validation and test step outputs\n\n\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* revert multidataloader changes in multilang ASR notebook\n\n\n\n---------\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Kunal Dhawan <kunaldhawan97@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Append output of val step to self.validation_step_outputs (#7530) (#7532)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [TTS] fixed trainer's accelerator and strategy. (#7569) (#7574)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Append val/test output to instance variable in EncDecSpeakerLabelModel (#7562) (#7573)\n\n* Append val/test output to the instance variable in EncDecSpeakerLabelModel\n\n\n\n* Handle test case in evaluation_step\n\n\n\n* Replace type with isinstance\n\n\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix CustomProgressBar for resume (#7427) (#7522)\n\n* Fix CustomProgress Bar for resume and multiple epochs\n\n\n\n* Edit num_training_batches\n\n\n\n* Use max_steps as total for progress bar for resume\n\n\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* fix typos in nfa and speech enhancement tutorials (#7580) (#7583)\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\nCo-authored-by: Elena Rastorgueva <80532067+erastorgueva-nv@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Add strategy as ddp_find_unused_parameters_true for glue_benchmark.py (#7454) (#7461)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* update strategy (#7577) (#7578)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix typos (#7581)\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Change hifigan finetune strategy to ddp_find_unused_parameters_true (#7579) (#7584)\n\n* Change strategy to auto\n\n\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [BugFix] Add missing quotes for auto strategy in tutorial notebooks (#7541) (#7548)\n\n* Add missing quotes for auto strategy\n\n\n\n* Revert trainer.gpus to trainer.devices in Self_Supervised_Pre_Training.ipynb\n\n\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nSigned-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* add build os key (#7596) (#7599)\n\n* add build os key\n\n\n\n* add tools\n\n\n\n* update to stable version\n\n\n\n---------\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* StarCoder SFT test + bump PyT NGC image to 23.09 (#7540)\n\n* Add SFT StarCoder test\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Remove _modify_config call as it is covered in load_from_nemo just below\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Test with pyt:23.09 container\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n---------\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* defaults changed (#7600)\n\n* defaults changed\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* typo\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* update\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n---------\n\nSigned-off-by: arendu <adithyare@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* add ItalianPhonemesTokenizer (#7587)\n\n* add ItalianPhonemesTokenizer\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix Italian phonemes\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add test\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n---------\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* best ckpt fix (#7564) (#7588)\n\nSigned-off-by: dimapihtar <dpihtar@gmail.com>\nCo-authored-by: Dmytro Pykhtar <37850217+dimapihtar@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Add files via upload (#7598)\n\nspecifies the branch\n\nSigned-off-by: George <37293288+Jorjeous@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix validation in G2PModel and ThutmoseTaggerModel (#7597) (#7606)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain (#7576) (#7586)\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Safeguard nemo_text_processing installation on ARM (#7485)\n\n* safeguard nemo_text_processing installing\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update check\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n---------\n\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Bound transformers version in requirements (#7620)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* fix llama2 70b lora tuning bug (#7622)\n\n* fix llama2 70b lora tuning bug\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* Update peft_config.py\n\nbrackets\n\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\n\n---------\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\nCo-authored-by: Adi Renduchintala <adithyare@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix import error no module name model_utils (#7629)\n\nSigned-off-by: Mehadi Hasan Menon <mehadihasan80@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* add fc large ls models (#7641)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao Koluguri <nithinraok>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* bugfix: trainer.gpus, trainer.strategy, trainer.accelerator (#7621) (#7642)\n\n* [TTS] bugfix for Tacotron2 tutorial due to PTL 2.0\n* trainer.gpus -> trainer.devices\n* fixed related tutorial bugs\n---------\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* fix ssl models ptl monitor val through logging (#7608) (#7614)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix metrics for SE tutorial (#7604) (#7612)\n\nSigned-off-by: Ante Jukic\u0301 <ajukic@nvidia.com>\nCo-authored-by: anteju <108555623+anteju@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Add ddp_find_unused_parameters=True and change accelerator to auto (#7623) (#7644)\n\n* Add ddp_find_unused_parameters=True and change acclerator to auto\n\n\n\n* Add ddp_find_unused_parameters True for normalization_as_tagging_train.py\n\n\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix py3.11 dataclasses issue  (#7616)\n\n* Fix py3.11 dataclasses issue  (#7582)\n\n* Update ASR configs to support Python 3.11\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Update TTS configs to support Python 3.11\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Guard MeCab and Ipadic\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix remaining ASR dataclasses\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Fix remaining ASR dataclasses\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Fix scripts\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Update name to ConfidenceMethodConfig\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain (#7576) (#7586)\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Safeguard nemo_text_processing installation on ARM (#7485)\n\n* safeguard nemo_text_processing installing\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update check\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n---------\n\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Fix changes to confidence measure\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Jason <jasoli@nvidia.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix issues with Dockerfile (#7650) (#7652)\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [ASR] RNN-T greedy decoding max_frames fix for alignment and confidence (#7635)\n\n* decoding and test fix\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [ASR] Fix type error in jasper (#7636) (#7653)\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nCo-authored-by: Ryan Langman <rlangman@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [TTS] Add STFT and SI-SDR loss to audio codec recipe (#7468)\n\n* [TTS] Add STFT and SI-SDR loss to audio codec recipe\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Fix STFT resolution\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Fix training metric logging\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Add docstring to mel and stft losses\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* add outline of asr quickstart info to asr/intro.rst\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* add CLI, LM and real-time transcription sections\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Create per.py (#7538)\n\n* Move model precision copy (#7336)\n\n* move cfg precision set to megatron base model\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* remove copy from other models\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* modify attribute not arg\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* fix gpt model test for ptl 2.0\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* rename function and add docstring\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* replace precision to dtype conditionals with func call\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* unnecessary function and cfg reset\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* set default value\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* fix precision lookup in a few more places\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* rename mapping function\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* ununsed import\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* save torch datatype to model\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* set weights precision wrt amp o2\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* Revert \"set weights precision wrt amp o2\"\n\nThis reverts commit 313a4bfe5eb69d771a6d2433898c0685836aef5c.\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* revert half precision at inference attempt\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* move autocast dtype to base model\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* move params dtype to base model, enable fp16 O2 inf\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* unused imports\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n---------\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix PEFT checkpoint loading (#7388)\n\n* Fix PEFT checkpoint loading\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Use distributed optimizer support for multiple dtypes (#7359)\n\n* Update distopt wrapper with multiple dtype support\n\nRemove manual handling of separate FP32 optimizer.\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Use distopt support for contiguous buffers with multiple dtypes\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Fix typo\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Separate distopt buckets for first GPT layer and non-overlapped params\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Add distopt logic for int dtypes\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Update Apex commit\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Remove unused variables\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Update Apex commit in README and Jenkensfile\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Debug Dockerfile and Jenkinsfile\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n---------\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* minor fix for llama ckpt conversion script (#7387)\n\n* minor fix for llama ckpt conversion script\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* Update Jenkinsfile\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* remove fast_swiglu configuration\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n---------\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix wrong calling of librosa.get_duration() in notebook (#7376)\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [PATCH] PEFT import mcore (#7393)\n\n* [PATCH] PEFT import mcore\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Create per.py\n\nScript for calculation Punctuation Error Rate and related rates (correct rate, deletions rate, etc.)\n\nSigned-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [TTS] Added a callback for logging initial data (#7384)\n\nSigned-off-by: Ante Jukic\u0301 <ajukic@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update Core Commit (#7402)\n\n* Update Core Commit\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n* update commit\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n---------\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Use cfg attribute in bert (#7394)\n\n* use cfg attribute instead of arg\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* use torch_dtype in place of cfg.precision\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* move precision copy before super constructor\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* use trainer arg\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n---------\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add support for bias conversion in Swiglu models (#7386)\n\n* Add support for bias conversion in Swiglu models\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Add support for auto extracting tokenizer model\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add support for auto extracting tokenizer model\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Fix issue with missing tokenizer\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Refactor\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Refactor\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update save_to and restore_from for dist checkpointing (#7343)\n\n* add dist ckpt to save to, in progress\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* move dist ckpt\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* clean up\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update restore from, need to figure out how to initialize distributed\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* launch distrib if needed when restoring dist ckpt\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* when using mcore we can change tp pp on the fly\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* add load_from_checkpoint support for dist ckpt\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update llama convert script to save dist .nemo\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* fix load dist ckpt\n\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* setup TE TP groups if needed\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* setup te tp groups if needed\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* remove import\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n---------\n\nSigned-off-by: eharper <eharper@nvidia.com>\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: jasonwan <jasonwan@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* fix forward for with mcore=false (#7403)\n\nSigned-off-by: Jimmy Zhang <jiemingz@nvidia.com>\nCo-authored-by: Jimmy Zhang <jiemingz@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix logging to remove 's/it' from progress bar in Megatron models and add train_step_timing (#7374)\n\n* Add CustomProgressBar class to exp_manager and trainer callbacks\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix the progress bar to reflect total microbatch cnt\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* Modify CustomProgressBar class\n\n1) Modify CustomProgressBar class to update progress bar per global_step instead of per microbatch\n2) Add the callback to other megatron training/finetuning files that are not using MegatronTrainerBuilder\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* Add CustomProgressBar callback to tuning files\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Set Activation Checkpointing Defaults (#7404)\n\n* Set Activation Checkpointing Defaults\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* check for None\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* make loss mask default to false (#7407)\n\nSigned-off-by: eharper <eharper@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add dummy userbuffer config files (#7408)\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* add missing ubconf files (#7412)\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* New tutorial on Speech Data Explorer (#7405)\n\n* Added Google Colab based tutorial on Speech Data Explorer\n\nSigned-off-by: George Zelenfroynd <gzelenfroind@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update ptl training ckpt conversion script to work with dist ckpt (#7416)\n\n* update ptl convert script\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* don't break legacy\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: eharper <eharper@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Allow disabling sanity checking when num_sanity_val_steps=0 (#7413)\n\n* Allow disabling sanity checking when num_sanity_val_steps=0\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* Update num_sanity_val_steps to be a multiple of num_microbatches\n\nSigned-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nSigned-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add comprehensive error messages (#7261)\n\nSigned-off-by: Anton Peganov <apeganov@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* check NEMO_PATH (#7418)\n\nSigned-off-by: Nikolay Karpov <karpnv@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* layer selection for ia3 (#7417)\n\n* layer selection for ia3\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: arendu <adithyare@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix missing pip package 'einops' (#7397)\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix failure of pyaudio in Google Colab (#7396)\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update README.md: output_path --> output_manifest_filepath (#7442)\n\nSigned-off-by: Samuele Cornell <cornellsamuele@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add rope dynamic linear scaling (#7437)\n\n* Add dynamic linear scaling\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix None dataloader issue in PTL2.0 (#7455)\n\n* Fix None dataloader issue in PTL2.0\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* updating values of self._validation_dl and self._test_dl as well\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* updating values of self._validation_dl and self._test_dl as well\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [ASR] Confidence measure -> method renames (#7434)\n\n* measure -> method\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add steps for document of getting dataset 'SF Bilingual Speech' (#7378)\n\n* Add steps for document of getting dataset 'SF Bilingual Speech'\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update datasets.rst\n\nadded a link from a tutorial demonstrating detailed data prep steps.\n\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* RNN-T confidence and alignment bugfix (#7381)\n\n* new frame_confidence and alignments lists are now always created after the while loop\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* tests added\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix resume from checkpoint in exp_manager (#7424) (#7426)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix checking of cuda/cpu device for inputs of Decoder (#7444)\n\n* Fix checking of cuda/cpu device for inputs of Decoder\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update tacotron2.py\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: Jason <jasoli@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix failure of ljspeech's get_data.py (#7430)\n\n* Fix failure of ljspeech's get_data.py\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [TTS] Fix audio codec type checks (#7373)\n\n* [TTS] Fix audio codec type checks\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Fix audio codec tests\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [\u2026"
    },
    {
        "repo_url": "github.com/brycedrennan/imaginAIry",
        "filepath": "imaginairy/modules/sgm/encoders/modules.py",
        "commit_date": "2023-12-15T22:32:01Z",
        "message": "docs: add docstrings\n\nWrote an openai script and custom prompt to generate them."
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2024-02-21T21:37:25Z",
        "message": "[ENH] Remove ONNX Logspam (#1747)\n\n## Description of changes\n\nAfter 1.17, ONNXRuntime produces scary warnings on mac platforms,\nbecause it tries to put our default embedding function into the CoreML\nexecution environment, where it doesn't fit.\n\nThis PR suppresses warnings from ONNX within the default embedding\nfunction so that users don't see scary warnings.\n\n## Test plan\n\nLocally tested via the `start_here` notebook.\n\n## Documentation Changes\nN/A"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2024-01-17T17:13:57Z",
        "message": "[WIP] [ENH] add exponential backoff and jitter to embedding calls (#1526)\n\nThis is a WIP, closes https://github.com/chroma-core/chroma/issues/1524\n\n*Summarize the changes made by this PR.*\n - Improvements & Bug fixes\n\t - Use `tenacity` to add exponential backoff and jitter\n - New functionality\n- control the parameters of the exponential backoff and jitter and allow\nthe user to use their own wait functions from `tenacity`'s API\n\n## Test plan\n*How are these changes tested?*\n\n- [x] Tests pass locally with `pytest` for python, `yarn test` for js\n\n## Documentation Changes\nNone"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2024-01-11T21:13:27Z",
        "message": "[BUG]Type errors in embading function #1169 (#1517)\n\n## Description of changes\n\n- Added correct type annotations for these methods #1169 \n\n## Test plan\n*How are these changes tested?*\n\n- [ ] Tests pass locally with `pytest` for python, `yarn test` for js\n\n## Documentation Changes\n*Are all docstrings for user-facing APIs updated if required? Do we need\nto make documentation changes in the [docs\nrepository](https://github.com/chroma-core/docs)?*\n\n---------\n\nCo-authored-by: Ran <rccalman@gmail.com>\nCo-authored-by: Ben Eggers <64657842+beggers@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2024-01-05T18:14:56Z",
        "message": "Replace ONNXMiniLM_L6_V2._init_model_and_tokenizer with tokenizer and model cached properties (#1194)\n\n## Description of changes\n\n*Summarize the changes made by this PR.*\n - Improvements & Bug fixes\n- Fixes #1193: race condition in\n`ONNXMiniLM_L6_V2._init_model_and_tokenizer`\n\n## Test plan\n*How are these changes tested?*\n\n- [x] Tests pass locally with `pytest` for python, `yarn test` for js"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2024-01-04T17:11:40Z",
        "message": "[CLN] Import json at top-level in embedding_functions (#1562)\n\n## Description of changes\n\n*Summarize the changes made by this PR.*\n - Improvements & Bug fixes\n\t - Move `import json` out of Amazon Bedrock EF and to top-level imports\n\n## Test plan\n*How are these changes tested?*\n\n- [x] Tests pass locally with `pytest` for python, `yarn test` for js\n\n## Documentation Changes\n*Are all docstrings for user-facing APIs updated if required? Do we need\nto make documentation changes in the [docs\nrepository](https://github.com/chroma-core/docs)?*"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-12-23T21:46:19Z",
        "message": "identity to equality check (#1566)\n\n## Description of changes\n\nChanges identity `is` to equality `==` check\n\nCo-authored-by: Jeffrey Huber <jeff@trychroma.com>"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-12-20T23:26:50Z",
        "message": "Add Amazon Bedrock Embedding function (#1361)\n\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/embeddings.html\n\n## Description of changes\n\n - New functionality\n\t - Support Amazon Bedrock embedding function\n\n## Test plan\n\n- [ ] Tests pass locally with `pytest` for python, `yarn test` for js\n\nTested locally by given profile_name with appropreate `~/.aws/config`\n\n```py\n>>> import boto3\n>>> from chromadb.utils.embedding_functions import AmazonBedrockEmbeddingFunction\n>>> session = boto3.Session(profile_name=\"myprofile\", region_name=\"us-east-1\")\n>>> ef = AmazonBedrockEmbeddingFunction(session=session)\n>>> ef([\"Hello Bedrock\"])\n[[-0.73046875, 0.390625, 0.24511719, 0.111816406, 0.83203125, 0.79296875,...,]]\n```\n\n## Documentation Changes\nWritten docstrings as much as possible.\n\n---------\n\nCo-authored-by: Ben Eggers <64657842+beggers@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-12-19T21:56:33Z",
        "message": "[ENH]: SHA256 sum check of Chroma's onnx model. (#1493)\n\nRefs: #883\n\n## Description of changes\n\n*Summarize the changes made by this PR.*\n - Improvements & Bug fixes\n- Verify ONNX all-MiniLM-L6 model model download from s3 with static\nSHA256 (within the python code)\n\n## Test plan\n*How are these changes tested?*\n\n- [x] Tests pass locally with `pytest` for python\n\n## Documentation Changes\nN/A"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-12-15T01:19:37Z",
        "message": "Gemini (#1520)\n\nThis adds a Google Gemini embedding function and an RAG chat example \n\nTODO\n- [x] JS support\n- [x] Docs PR"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-11-29T18:53:09Z",
        "message": "[ENH]: Embedding Function - Hugging Face Text Embedding Server (#1371)\n\nRefs: [Feature Request]: Hugging Face text embedding inference custom\nembedding #1367\n\n## Description of changes\n\n*Summarize the changes made by this PR.*\n - New functionality\n\t - New Embedding Function for HF Text Embedding Server\n\t - Added sample docker compose to run things locally\n\t - Added example notebook\n\n## Test plan\n*How are these changes tested?*\n\n- [x] Tests pass locally with `pytest` for python\n\n## Documentation Changes\nTBD\n\nhttps://github.com/huggingface/text-embeddings-inference"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-11-28T22:58:15Z",
        "message": "Remove redundant check for \"requests\" module (#1427)\n\n`requests` is imported in line 17, and hence required:\n\n\nhttps://github.com/chroma-core/chroma/blob/33289e8c5b0b5d65132a2995ab7199e83eaeacdf/chromadb/utils/embedding_functions.py#L17"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-11-17T18:01:17Z",
        "message": "Pass input_type to cohere embedding models (#1407)\n\n## Description of changes\n\n*Summarize the changes made by this PR.*\n - Improvements & Bug fixes\n\t - Fixes https://github.com/chroma-core/chroma/issues/1385\n - New functionality\n\t - ...\n\n## Test plan\n*How are these changes tested?*\n\nGot a Cohere API key and repro'd the issue locally. With this change,\ncalling the embedding function no longer breaks.\n\n- [x] Tests pass locally with `pytest` for python, `yarn test` for js\n\n## Documentation Changes\n*Are all docstrings for user-facing APIs updated if required? Do we need\nto make documentation changes in the [docs\nrepository](https://github.com/chroma-core/docs)?*"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-11-17T06:32:03Z",
        "message": "feat: add Jina AI embedding function (#1324)\n\n## Description of changes\n\nHey Chroma team!\n\nWe just launched [Jina Embeddings](https://jina.ai/embeddings/) and\nwould love to add a possibilty for the community to use it with\nJinaEmbeddingFunctions.\n\nThanks!\n\n## Documentation Changes\nLink to docs PR: https://github.com/chroma-core/docs/pull/153\n\n---------\n\nSigned-off-by: Joan Fontanals Martinez <joan.martinez@jina.ai>"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-11-16T05:55:58Z",
        "message": "ENH: Allow default headers to be passed to OpenAI API (#1397)\n\n## Description of changes\n\n*Summarize the changes made by this PR.*\n - Improvements & Bug fixes\n- Allows users to pass custom headers to OpenAI API, enabling\nintermediary proxies with different authentication methods.\n - New functionality\n- New optional `default_headers` input at the `OpenAIEmbeddingFunction`\nclass.\n\n## Test plan\n*How are these changes tested?*\n\n- [x] Tests pass locally with `pytest` for python, `yarn test` for js\n\n## Documentation Changes\n*Are all docstrings for user-facing APIs updated if required? Do we need\nto make documentation changes in the [docs\nrepository](https://github.com/chroma-core/docs)?*\n\nSince this is a relatively specific feature, I believe it won't require\nan usage example in the docs.\n\nCo-authored-by: Gustavo Antoniassi <gustavo.antoniassi@ifood.com.br>"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-11-08T22:22:58Z",
        "message": "support of OpenAI package v1.X.X for utils.OpenAIEmbeddingFunction, deployment_id parameter for openai v0.X.X (#1338)\n\n## Description of changes\n- Add support of OpenAI package v1.X.X for utils.OpenAIEmbeddingFunction\n- Add Azure OpenAI Deployment ID parameter for openai v0.X.X lib in\nutils.OpenAIEmbeddingFunction\n\n## Test plan\n*How are these changes tested?*\n\nTested as dependency of https://github.com/Nayjest/ai-microcore with\nAzure & openai packages v0.28.1 & v1.0.1, v1.1.0"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-11-07T22:07:53Z",
        "message": "[ENH] Multimodal Embedding Functions (#1345)\n\n## Description of changes\n\nThis PR introduces multi-modal embeddings into Chroma. \n- It adds the generic `EmbeddingFunction` which can take various data\ntypes. Existing functions take the `Documents` type.\n- Adds `Images` as a type (numpy NDArray taking ints or floats)\n- Add `OpenCLIPEmbeddingFunction` which is an\n`EmbeddingFunction[Union[Documents, Images]]`\n\n## Test\n\nIntegration tests pass. \n\nA new test for multimodal embedding functions:\n[chromadb/test/ef/test_multimodal_ef.py](https://github.com/chroma-core/chroma/blob/86a9e2620352ee0b2844bc3233f9e001cc4aa3d9/chromadb/test/ef/test_multimodal_ef.py)\n\n## Documentation\n\nSee https://github.com/chroma-core/chroma/pull/1294\n\n## TODOs\n- [x] Tests\n- [x] ~Wiring through FastAPI~ Nothing to wire through\n- [x] Documentation\n- [x] Telemetry\n- [ ] JavaScript"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-11-07T18:00:00Z",
        "message": "Revert \"[ENH] Multimodal Embeddings\" (#1344)\n\nReverts chroma-core/chroma#1293"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-11-07T17:57:00Z",
        "message": "[ENH] Multimodal Embeddings (#1293)\n\n## Description of changes\n\nThis PR introduces multi-modal embeddings into Chroma. \n- It adds the generic `EmbeddingFunction` which can take various data\ntypes. Existing functions take the `Documents` type.\n- Adds `Images` as a type (numpy NDArray taking ints or floats)\n- Add `OpenCLIPEmbeddingFunction` which is an\n`EmbeddingFunction[Union[Documents, Images]]`\n\n## Test\n\nIntegration tests pass. \n\nA new test for multimodal embedding functions:\n[chromadb/test/ef/test_multimodal_ef.py](https://github.com/chroma-core/chroma/blob/86a9e2620352ee0b2844bc3233f9e001cc4aa3d9/chromadb/test/ef/test_multimodal_ef.py)\n\n## Documentation\n\nSee https://github.com/chroma-core/chroma/pull/1294\n\n## TODOs\n- [x] Tests\n- [x] ~Wiring through FastAPI~ Nothing to wire through\n- [x] Documentation\n- [x] Telemetry\n- [ ] ~JavaScript~"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-09-22T22:49:58Z",
        "message": "[ENH] Metric batching and more metrics (#1163)\n\n## Description of changes\nThis PR accomplishes two things:\n- Adds batching to metrics to decrease load to Posthog\n- Adds more metric instrumentation\n\nEach `TelemetryEvent` type now has a `batch_size` member defining how\nmany of that Event to include in a batch. `TelemetryEvent`s with\n`batch_size > 1` must also define `can_batch()` and `batch()` methods to\ndo the actual batching -- our posthog client can't do this itself since\ndifferent `TelemetryEvent`s use different count fields. The Posthog\nclient combines events until they hit their `batch_size` then fires them\noff as one event.\n\nNB: this means we can drop up to `batch_size` events -- since we only\nbatch `add()` calls right now this seems fine, though we may want to\naddress it in the future.\n\nAs for the additional telemetry, I pretty much copied Anton's draft\nhttps://github.com/chroma-core/chroma/pull/859 with some minor changes.\n\nOther considerations: Maybe we should implement `can_batch()` and\n`batch()` on all events, even those which don't currently use them? I'd\nprefer not to leave dead code hanging around but happy to go either way.\n\nI created a ticket for the type ignores:\nhttps://github.com/chroma-core/chroma/issues/1169\n\n## Test plan\npytest passes modulo a couple unrelated failures\n\nWith `print(event.properties)` in posthog client's `_direct_capture()`:\n```\n>>> import chromadb\n>>> client = chromadb.Client()\n{'batch_size': 1}\n>>> collection = client.create_collection(\"sample_collection\")\n{'batch_size': 1, 'collection_uuid': 'bb19d790-4ec7-436c-b781-46dab047625d', 'embedding_function': 'ONNXMiniLM_L6_V2'}\n>>> collection.add(\n...     documents=[\"This is document1\", \"This is document2\"], # we embed for you, or bring your own\n...     metadatas=[{\"source\": \"notion\"}, {\"source\": \"google-docs\"}], # filter on arbitrary metadata!\n...     ids=[\"doc1\", \"doc2\"], # must be unique for each doc \n... )\n{'batch_size': 1, 'collection_uuid': 'bb19d790-4ec7-436c-b781-46dab047625d', 'add_amount': 2, 'with_documents': 2, 'with_metadata': 2}\n>>> for i in range(50):\n...   collection.add(documents=[str(i)], ids=[str(i)])\n... \n{'batch_size': 20, 'collection_uuid': 'bb19d790-4ec7-436c-b781-46dab047625d', 'add_amount': 20, 'with_documents': 20, 'with_metadata': 0}\n{'batch_size': 20, 'collection_uuid': 'bb19d790-4ec7-436c-b781-46dab047625d', 'add_amount': 20, 'with_documents': 20, 'with_metadata': 0}\n>>> for i in range(50):\n...   collection.add(documents=[str(i) + ' ' + str(n) for n in range(20)], ids=[str(i) + ' ' + str(n) for n in range(20)])\n... \n{'batch_size': 20, 'collection_uuid': 'bb19d790-4ec7-436c-b781-46dab047625d', 'add_amount': 210, 'with_documents': 210, 'with_metadata': 0}\n{'batch_size': 20, 'collection_uuid': 'bb19d790-4ec7-436c-b781-46dab047625d', 'add_amount': 400, 'with_documents': 400, 'with_metadata': 0}\n{'batch_size': 20, 'collection_uuid': 'bb19d790-4ec7-436c-b781-46dab047625d', 'add_amount': 400, 'with_documents': 400, 'with_metadata': 0}\n```\n\n## Documentation Changes\nhttps://github.com/chroma-core/docs/pull/139\n\nhttps://github.com/chroma-core/docs/commit/a4fd57d4d2cc3cae00cbb4a9245b938e2f0d1842"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-08-29T17:18:53Z",
        "message": "[ENH] Added providers to onnx runtime session (#1006)\n\nRefs:  https://onnxruntime.ai/docs/api/python/api_summary.html\n\nRefs: #1004\n\n## Description of changes\n\n*Summarize the changes made by this PR.*\n - Improvements & Bug fixes\n- In multi-provider envs Onnyx Runtime requires that providers are\nspecified (with order of preference) as input to the InferrenceSession\n\n## Test plan\nIdeally, we'll need a GPU runners for testing -\nhttps://github.com/github/roadmap/issues/505\n\n## Documentation Changes\nNo change to docs is required.\n\n> Note: Sorry for the unsigned commit, I tried using GH Codespaces for\nthis one."
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-08-15T01:18:11Z",
        "message": "Fix model download for ONNX embedder (#976)\n\n## Description of changes\nThe current function is looking for the tar.gz file instead of checking\nif the folder already exists, so if the tar.gz gets deleted after\nextraction, it downloads it again.. This PR resolves this and checks for\nthe model in the extracted folder before attempting to download or\nextract again.\n\n## Test plan\nBy using it\n\n## Documentation Changes\nI didn't find any documentation about how this does the download."
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-07-28T21:35:31Z",
        "message": "fix: update api request (#890)\n\n## Description of changes\n\n*Summarize the changes made by this PR.*\n - Improvements & Bug fixes\n\t - Update the API endpoint for Google VertexEmbedding\n - New functionality\n\t - ...\n\n## Test plan\n*How are these changes tested?*\nAt the moment, there is any unit test for embedding function. However I\nhave tested the new code change locally and it worked.\n\n## Documentation Changes\n*Are all docstrings for user-facing APIs updated if required? Do we need\nto make documentation changes in the [docs\nrepository](https://github.com/chroma-core/docs)?*\n\n1. The default model_name should be textembedding-gecko instead of\ntextembedding-gecko-001\n2. The _api_url should be changed to\n3. The json payload should take in only 1 string instead of array of\nstrings. Thus I made a for loop to call the api endpoint in the event\nthere are arrays of text documents passed into the _call() function"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-07-27T23:09:43Z",
        "message": "add azure openai api_version param (#832)\n\n## Description of changes\n\n*Summarize the changes made by this PR.*\n - Improvements & Bug fixes\nFix the bug that causes functionality to be unavailable when using\nazure-openai due to missing api version parameter.\nopenai examples:\nhttps://github.com/openai/openai-cookbook/blob/main/examples/azure/embeddings.ipynb\n - New functionality\n\t - ...\n\n## Test plan\n*How are these changes tested?*\n\n## Documentation Changes\n*Are all docstrings for user-facing APIs updated if required? Do we need\nto make documentation changes in the [docs\nrepository](https://github.com/chroma-core/docs)?*\n\nfix https://github.com/chroma-core/chroma/issues/698\n\n---------\n\nCo-authored-by: litong <you@example.com>"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-07-27T23:06:23Z",
        "message": "Update chromadb/utils/embedding_functions.py (#816)\n\nThis is a copy of https://github.com/kevinlu1248/chroma/pull/7, written\nby Sweep, an AI junior dev.\n\n## Description of changes\nThis PR adds documentation for the HuggingFace embeddings feature in the\nchroma repository. The HuggingFaceEmbeddingFunction class is already\nimplemented in the `chromadb/utils/embedding_functions.py` file, but it\nis currently undocumented. This PR adds docstrings to the class and\nupdates the README.md file to explain how to use the class and what it\ndoes.\n\n## Changes Made\n- Added docstrings to the HuggingFaceEmbeddingFunction class in\n`chromadb/utils/embedding_functions.py`\n- Updated the README.md file to include a section about the\nHuggingFaceEmbeddingFunction class, explaining its usage and providing\nan example\n- Mentioned that the requests package is required and provided the\ncommand to install it\n\n---------\n\nCo-authored-by: sweep-ai[bot] <128439645+sweep-ai[bot]@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-06-28T17:16:06Z",
        "message": "Normalized embeddings (#737)\n\n## Description of changes\nAdded support to the use of normalize embeddings in the\n`SentenceTransformerEmbeddingFunction` class by adding a new attribute\nto the class and use it when calling the `encode` function.\n\n## Test plan\nI was planning to add a new test case where an object was created with\n`normalize_embeddings=True` but I haven't found any test which is\ncurrently testing `SentenceTransformerEmbeddingFunction` (I guess that\nshould be in the folder `test/utils`).\n\n## Documentation Changes\nThe document talking about [embeddings] should be changed. In the\nsection *Sentence Transformers* it should be mentioned the possiblitiy\nof using an optional parameter `normalize_embeddings`. A similar text to\nthe one in the SentenceTransformer documentation could be used (or\nadapted):\n\n> If set to True, returned vectors will have length 1. In that case, the\nfaster dot-product (util.dot_score) instead of cosine similarity can be\nused."
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-06-21T21:01:34Z",
        "message": "fix wrong return type (#613)\n\n## Description of changes\nRelated issue: #594\n*Summarize the changes made by this PR.*\n - Improvements & Bug fixes\n- HuggingFaceEmbeddingFunction: fix wrong return type from List to\nList[List]\n- GoogleVertexEmbeddingFunction: fix wrong return type of empty dict to\nempty List\n\n## Test plan\n*How are these changes tested?*\n\n## Documentation Changes\n*Are all docstrings for user-facing APIs updated if required? Do we need\nto make documentation changes in the [docs\nrepository](https://github.com/chroma-core/docs)?*\n\n---------\n\nCo-authored-by: Tegar Dani Pratama <tegar.dani@hukumonline.com>\nCo-authored-by: Jeffrey Huber <jeff@trychroma.com>"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-05-31T21:23:10Z",
        "message": "Add a thin client (#610)\n\n## Description of changes\n*Summarize the changes made by this PR.*\n - Improvements & Bug fixes\n\t - Typing cleanup\n- Changes embedding function defaults to work through the default params\nas opposed to via None so that None now means - NO embedding function as\nopposed to use the default. This is technically a breaking change.\n - New functionality\n- Adds a thin client that restricts what you can create to the REST api\nclient and builds it separately so it can be published to its own pypi\npackage. The thin client restricts the default embedding function to be\nNone always - forcing manual specification of the embedding function\nwhile using the thin client.\n- The thin client is built with its own pyproject.toml with a limited\nset of dependencies and a is_thin_client.py file that acts as a compile\nflag. The build script stages the toml, places the two files in the\nright place, performs the build and then tears down the changes.\n\t \nAddresses #289 \n\n## Test plan\nThe existing tests should cover this configuration. We can add CI for\nthe thin-client in the future.\n\n## Documentation Changes\nWe will add a section on the docs that explains the thin client and its\nlimitations."
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-05-19T22:07:41Z",
        "message": "Add Google Vertex Embedding Function (#528)\n\nAdd Google Vertex Embedding Function\n\n\nhttps://console.cloud.google.com/vertex-ai/publishers/google/model-garden/textembedding-gecko?project=noted-victory-383712\n\n## Description of changes\nAdded Google Vertex Embedding Function\n\n## Test plan\nBy providing the API key and project ID.\n\n## Documentation Changes\nHas as much documentation as the others in that file.\n\n---------\n\nCo-authored-by: Jeffrey Huber <jeff@trychroma.com>"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-05-19T22:06:46Z",
        "message": "Add task instruction pairing to InstructorEmbeddingFunction (#556)\n\n## Description of changes\n\n- Add an optional `instruction` constructor parameter to\nInstructorEmbeddingFunction to allow `instruction` and Document pairs to\nbe encoded.\n\n## Test plan\n\n\n## Documentation Changes\nAdded examples to the Alternative Embedding notebook.\n\nNot sure if this is a good implementation, since you'll need a separate\nCollection for each instruction you want to use (or reassign\n`self._instruction`), but at least the change is pretty minimal. For my\nuse case, two instructions are enough (one for storing, one for\nretrieving). For a scenario where you need lots of different\ninstructions, perhaps \"Represent the <Science|Financial|Political|etc.>\narticle: \", another solution is needed.\n\nFeature Request #546\n\n---------\n\nCo-authored-by: Jeffrey Huber <jeff@trychroma.com>"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-05-19T07:30:04Z",
        "message": "Switch to ONNX model for default embedding model (#267)\n\n## Description of changes\n\n*Summarize the changes made by this PR.* \n - Improvements & Bug fixes\n\t - None\n - New functionality\n- Adds a ONNX port of sentence-transformers all-MiniLM-L6-v2 in order to\nremove the dependencies on pytorch, sentence-transformers,\nsentence-piece and other heavy depdencies. This reduced the on disk\nenvironment size needed for dependencies to run chroma from ~900MB to\n~300MB\n- The ONNX port and verification of its accuracy live in\nhttps://github.com/chroma-core/onnx-embedding\n- The ONNX model is hosted on S3 after being generated in the above repo\n- The implementation here runs the model and applies mean-pooling using\nnumpy since that's the final layer.\n- The embedding model will download the model using tqdm to provide the\nsame download experience as before\n\t - If the model is cached it will not be downloaded. \n\t - In contrast to before, the model is now ONLY downloaded if used!\n- The net-new dependencies are onnxruntime and tokenizers, both are\nlightweight.\n\t - Updated the default model to be this one instead of ST.\n- We create a new DefaultEmbeddingFunction which aliases the ONNX\nembedding function\n\n## Test plan\nAdded a test to test multiple batches with the new model\n\n## Documentation Changes\n*Are all docstrings for user-facing APIs updated if required? Do we need\nto make documentation changes in the [docs\nrepository](https://github.com/chroma-core/docs)?*\nWe will need to change the documentation here\nhttps://docs.trychroma.com/embeddings#default-sentence-transformers to\nhighlight this fact."
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-05-19T05:11:55Z",
        "message": "Added api_base and api_type to the OpenAIEmbeddingFunction  (#517)\n\nAdded api_base and api_type to the OpenAIEmbeddingFunction to support\noutside the base openai deployment like Azure etc\n\n## Description of changes\n\nThis PR adds support for deploying the OpenAIEmbeddingFunction on\nplatforms other than the base OpenAI deployment, such as Azure, by\nallowing users to specify the `api_base` and `api_type` parameters.\n\n### Improvements & Bug fixes\n- None\n\n### New functionality\n- Added `api_base` and `api_type` as optional parameters to the\n`OpenAIEmbeddingFunction` to support deployment on other platforms like\nAzure, etc.\n\n## Test plan\n\nTo test the changes made in this PR, the following steps can be taken:\n\n### Test against Azure\n1. Set up the OpenAIEmbeddingFunction with the new `api_base` and\n`api_type` parameters.\n2. Deploy the embedding model on the desired platform (e.g., Azure).\n3. Run sample embeddings and test the new deployment against the Azure\ndeployment instance.\n\n### Test against Base Open AI\n1. Set up the OpenAIEmbeddingFunction with just the `api_key and without\nthe new `api_base` and `api_type` parameters.\n2. Run sample embeddings and test the new deployment against the OpenAI\ninstance.\n\n## Documentation Changes\n\nAll docstrings for user-facing APIs needs to be updated to reflect the\nnew `api_base` and `api_type` parameters. Additionally, documentation in\nthe [docs repository](https://github.com/chroma-core/docs) should be\nupdated to provide guidance on how to use the new parameters for\ndeploying the OpenAIEmbeddingFunction on platforms other than the base\nOpenAI deployment. This change will be submitted as a separate pull\nrequest.\n\n---------\n\nCo-authored-by: Jeff Huber <jeff@trychroma.com>"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-05-18T04:49:54Z",
        "message": "Add organization_id to openai embeddings (#548)\n\n## Description of changes\nThis related to #547 issue.\n - New functionality added:\n\t - add optional organization_id parameter to OpenAIEmbeddingFunction.\n\n## Test plan\n- Create the class object without organization_id and no errors\nexpected.\n- Create the class object with organization_id.\n\n## Documentation Changes\n*Are all docstrings for user-facing APIs updated if required? Do we need\nto make documentation changes in the [docs\nrepository](https://github.com/chroma-core/docs)?*\n- The class optional parameters are not mentioned explicitly."
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-05-12T23:43:17Z",
        "message": "Add Text2VecEmbeddingFunction (#417)\n\nAdd Text2VecEmbeddingFunction for Chinese sentence embedding.\n\nCo-authored-by: hammadb <hammad@trychroma.com>"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-05-10T05:52:12Z",
        "message": "Add precommit hooks (#483)\n\nAdds precommit hooks based on #433 to our repository. Only one file here is new - the configuration for the hooks, everything else is linting/formatting fixes. We do not run the typechecker globally since that would be quite lengthy to clean up - instead we will have to incrementally clean up type check issues as we go."
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-05-09T18:23:22Z",
        "message": "Add Google PaLM embedding function. (#445)\n\n* Add Google PaLM embedding function.\n\n* Address feedback by @markmcd.\n\n* Default model, import failure string\n\n---------\n\nCo-authored-by: atroyn <anton.troynikov@gmail.com>"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-05-06T04:03:15Z",
        "message": "Team/hypothesis tests (#474)\n\nMerges the team/hypothesis-tests branch to main. Which adds a robust property-based testing suite to Chroma. lfg."
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-04-18T00:59:45Z",
        "message": "make api_key in OpenAIEmbeddingFunction optional (#320)\n\n* make api_key in OpenAIEmbeddingFunction optional\n\n* make optional api_key actually work\n\n* Early error\n\n---------\n\nCo-authored-by: atroyn <anton.troynikov@gmail.com>"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-04-18T00:54:53Z",
        "message": "[bugfix] Ensure Openai batch embeddings are sorted by index (#344)\n\n* Ensure openai batch embeddings are sorted by index\n\n* Use lambda\n\n---------\n\nCo-authored-by: atroyn <anton.troynikov@gmail.com>"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-04-11T21:04:25Z",
        "message": "Adds option to pass compute device like cpu, cuda, cuda:1 to SentenceTransformerEmbeddingFunction"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-04-03T19:37:16Z",
        "message": "Add flake8 linter, address linter issues (#287)\n\nAdds the flake8 linter configured to run with black in vscode"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-03-24T03:07:13Z",
        "message": "added embedding function for the Instructor models."
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-03-17T05:46:48Z",
        "message": "Merge pull request #206 from danielgross/patch-2\n\nIn my experience T5 is much better, add comment with that as an option."
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-03-16T02:42:15Z",
        "message": "use requests Session object to keep requests imported"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-03-16T01:48:52Z",
        "message": "add wrapper for hugging faces embedding api"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-03-07T19:45:19Z",
        "message": "In my experience T5 is much better, add comment with that as an option."
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-03-03T18:40:46Z",
        "message": "Unified error messages (#199)\n\nUnifies error messages across the project to make them more useful and consistent\nImproves the error message for invalid collection names\n\n---------\n\nCo-authored-by: Hammad Bashir <HammadB@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-02-17T18:06:08Z",
        "message": "Add support for cohere embeddings (#141)\n\n* Add support for cohere embeddings\n\n* Keep newlines"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-02-15T20:13:22Z",
        "message": "Remove newlines to improve performance\n\nPer OpenAI's get_embedding function here: https://github.com/openai/openai-python/blob/main/openai/embeddings_utils.py"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-02-14T03:45:38Z",
        "message": "OpenAI Embeddings (#117)\n\n* Adding OpenAI embedding functions\n\n* OpenAI"
    },
    {
        "repo_url": "github.com/chroma-core/chroma",
        "filepath": "chromadb/utils/embedding_functions.py",
        "commit_date": "2023-02-11T05:58:44Z",
        "message": "Add embedding functions (#19)\n\n* Pass in embedding function.\n\n* Embedding function\n\n* Text-only queries\n\n* Add correct result and types for get (#15)\n\n* Reactor get result to client (#16)\n\n* Add query result (#17)\n\n* fix\n\n* Nit\n\n---------\n\nCo-authored-by: Hammad Bashir <HammadB@users.noreply.github.com>\nCo-authored-by: Jeffrey Huber <jeff@trychroma.com>"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2023-04-20T14:05:29Z",
        "message": "remove einops exts for better pytorch 2.0 compile compatibility"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2023-03-07T20:41:55Z",
        "message": "use .to(device) to avoid copy, within one_unet_in_gpu context"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2023-03-05T18:45:44Z",
        "message": "always rederive the predicted noise from the clipped x0 for ddim + predict noise objective"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2023-02-12T01:18:40Z",
        "message": "fix for self conditioning in diffusion prior network https://github.com/lucidrains/DALLE2-pytorch/issues/273"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-12-23T21:23:09Z",
        "message": "default ddim sampling eta to 0"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-12-15T18:54:21Z",
        "message": "extra insurance in case eos id is not there"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-10-29T01:21:07Z",
        "message": "bring in prediction of v objective, combining the findings from progressive distillation paper and imagen-video to the eventual extension of dalle2 to make-a-video"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-10-23T22:12:09Z",
        "message": "fix openclipadapter to be able to use latest open sourced sota model"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-10-19T16:26:06Z",
        "message": "fix a dtype conversion issue for the diffusion timesteps in the diffusion prior, thanks to @JiaHeng-DLUT"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-09-29T16:09:47Z",
        "message": "fix for use with larger openai clip models by extracting dimension of last layernorm in clip"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-09-19T17:27:14Z",
        "message": "handle open clip adapter image size being a tuple"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-09-01T16:40:47Z",
        "message": "in ddim, noise should be predicted after x0 is maybe clipped, thanks to @lukovnikov for pointing this out in another repository"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-09-01T00:19:05Z",
        "message": "fix bug with misnamed variable in diffusion prior network"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-08-28T17:11:37Z",
        "message": "add two asserts to diffusion prior to ensure matching image embedding dimensions for clip, diffusion prior network, and what was set on diffusion prior"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-08-23T15:29:01Z",
        "message": "fix classifier free guidance for diffusion prior, thanks to @jaykim9870 for spotting the issue"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-08-20T17:56:01Z",
        "message": "cast attention matrix back to original dtype pre-softmax in attention"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-08-14T18:34:45Z",
        "message": "add weight standardization behind feature flag, which may potentially work well with group norm"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-08-13T17:03:40Z",
        "message": "make it so diffusion prior p_sample_loop returns unnormalized image embeddings"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-08-12T22:25:03Z",
        "message": "dry up some code around handling unet outputs with learned variance"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-08-12T19:30:03Z",
        "message": "fix self conditioning shape in diffusion prior"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-08-12T19:20:51Z",
        "message": "make self conditioning technique work with diffusion prior"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-08-12T18:36:08Z",
        "message": "bet on the new self-conditioning technique out of geoffrey hintons group"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-08-03T02:21:44Z",
        "message": "add gradient checkpointing for all resnet blocks"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-30T16:02:31Z",
        "message": "make open clip available for use with dalle2 pytorch"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-29T19:48:20Z",
        "message": "add cosine sim for self attention as well, as a setting"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-29T19:41:02Z",
        "message": "change up epsilon in layernorm the case of using fp16, thanks to @Veldrovive for figuring out this stabilizes training"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-29T18:12:18Z",
        "message": "allow for cosine sim cross attention, modify linear attention in attempt to resolve issue on fp16"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-28T17:17:43Z",
        "message": "make sure entire readme runs without errors"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-28T15:33:51Z",
        "message": "fix readme and a small bug in DALLE2 class"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-27T19:27:38Z",
        "message": "rescale values in linear attention to mitigate overflows in fp16 setting"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-26T17:46:04Z",
        "message": "add upsample combiner feature for the unets"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-20T02:07:50Z",
        "message": "Improved upsampler training (#181)\n\nSampling is now possible without the first decoder unet\n\nNon-training unets are deleted in the decoder trainer since they are never used and it is harder merge the models is they have keys in this state dict\n\nFixed a mistake where clip was not re-added after saving"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-19T18:31:56Z",
        "message": "@jacobwjs reports dynamic thresholding works very well and 0.95 is a better value"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-19T16:26:55Z",
        "message": "complete inpainting ability using inpaint_image and inpaint_mask passed into sample function for decoder"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-19T02:04:26Z",
        "message": "fix a bug with ddim and predict x0 objective"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-16T00:29:10Z",
        "message": "offer way to turn off initial cross embed convolutional module, for debugging upsampler artifacts"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-14T22:28:34Z",
        "message": "takes care of a grad strides error at https://github.com/lucidrains/DALLE2-pytorch/issues/196 thanks to @YUHANG-Ma"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-14T19:49:43Z",
        "message": "protect against random cropping for base unet"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-14T17:27:04Z",
        "message": "let the neural network peek at the low resolution conditioning one last time before making prediction, for upsamplers"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-14T03:59:43Z",
        "message": "just always use nearest neighbor interpolation when resizing for low resolution conditioning, for https://github.com/lucidrains/DALLE2-pytorch/pull/181"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-13T20:12:30Z",
        "message": "allow for using classifier free guidance for some unets but not others, by passing in a tuple of cond_scale during sampling for decoder, just in case it is causing issues for upsamplers"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-13T19:56:02Z",
        "message": "hack around some inplace error, also make sure for openai clip text encoding, only tokens after eos_id is masked out"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-13T17:11:49Z",
        "message": "fix a potential bug with conditioning with blurred low resolution image, blur should be applied only 50% of the time"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-13T16:48:40Z",
        "message": "fix issue with ddim and normalization of lowres conditioning image"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-13T14:56:30Z",
        "message": "only use the stable layernorm for final output norm in transformer"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-13T00:49:16Z",
        "message": "add yet another transformer stability measure"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-13T00:33:14Z",
        "message": "add learned padding tokens, same strategy as dalle1, for diffusion prior, and get rid of masking in causal transformer"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-13T00:08:12Z",
        "message": "add setting to attend to all text encodings regardless of padding, for diffusion prior"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-12T23:00:19Z",
        "message": "make sure text encodings being passed in has the correct batch dimension"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-12T22:40:31Z",
        "message": "remove text masking altogether in favor of deriving from text encodings (padded text encodings must be pad value of 0.)"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-12T22:33:13Z",
        "message": "protect against bad text mask being passed into decoder"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-12T22:01:46Z",
        "message": "one more fix for text mask, if the length of the text encoding exceeds max_text_len, add an assert for better error msg"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-12T19:54:59Z",
        "message": "generate text mask within the unet and diffusion prior itself from the text encodings, if not given"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-11T23:07:23Z",
        "message": "add PixelShuffleUpsample thanks to @MalumaDev and @marunine for running the experiment and verifyng absence of checkboard artifacts"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-11T20:22:06Z",
        "message": "zero init final projection in unet, since openai and @crowsonkb are both doing it"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-11T17:56:19Z",
        "message": "make it so even if text mask is omitted, it will be derived based on whether text encodings are all 0s or not, simplify dataloading"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-10T16:44:38Z",
        "message": "allow for final l2norm clamping of the sampled image embed"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-10T02:01:37Z",
        "message": "fix misnamed variable, thanks to @nousr"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-10T01:38:40Z",
        "message": "do not noise for the last step in ddim"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-10T00:31:54Z",
        "message": "fix for small validation bug for sampling steps"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-10T00:28:14Z",
        "message": "more informative error for something that tripped me up"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-10T00:25:34Z",
        "message": "complete ddim integration of diffusion prior as well as decoder for each unet, feature complete for https://github.com/lucidrains/DALLE2-pytorch/issues/157"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-09T01:17:29Z",
        "message": "just force it so researcher can never pass in an image that is less than the size that is required for CLIP or CoCa"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-08T18:44:43Z",
        "message": "allow for control over use of nearest interp method of downsampling low res conditioning, in addition to being able to turn it off"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-07T16:41:49Z",
        "message": "fix a potential issue in the low resolution conditioner, when downsampling and then upsampling using resize right, thanks to @marunine"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-07T14:43:41Z",
        "message": "fix condition_on_text_encodings in dalle2 orchestrator class, fix readme"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-07T02:05:50Z",
        "message": "more shots in the dark regarding fp16 with learned variance for deepspeed issue"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-06T15:27:34Z",
        "message": "attempting to fix issue with deepspeed fp16 seeing overflowing gradient"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-06T01:01:22Z",
        "message": "cast long as float before deriving sinusoidal pos emb"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-05T23:53:58Z",
        "message": "remove forcing of softmax in f32, in case it is interfering with deepspeed"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-05T21:27:04Z",
        "message": "bring in two tricks from the cogview paper for reducing the chances of overflow, for attention and layernorm"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-01T17:22:07Z",
        "message": "add ability to specify full self attention on specific stages in the unet"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-01T16:35:39Z",
        "message": "allow for returning low resolution conditioning image on forward through decoder with return_lowres_cond_image flag"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-01T16:21:47Z",
        "message": "bring back convtranspose2d upsampling, allow for nearest upsample with hyperparam, change kernel size of last conv to 1, make configurable, cleanup"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-07-01T00:03:16Z",
        "message": "blur sigma for upsampling training was 0.6 in the paper, make that the default value"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-06-29T15:16:58Z",
        "message": "add skip connections for all intermediate resnet blocks, also add an extra resnet block for memory efficient version of unet, time condition for both initial resnet block and last one before output"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-06-27T04:59:55Z",
        "message": "bring in the skip connection scaling factor, used by imagen in their unets, cite original paper using it"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-06-27T04:07:42Z",
        "message": "fix remaining issues with deriving cond_on_text_encodings from child unet settings"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-06-26T23:12:32Z",
        "message": "bug fixes for text conditioning update (#175)"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-06-26T19:45:05Z",
        "message": "nevermind, do not enforce text encodings on first unet"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-06-26T19:32:17Z",
        "message": "remove unnecessary decoder setting, and if not unconditional, always make sure the first unet is condition-able on text"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-06-25T23:37:06Z",
        "message": "autoswitch tqdm for notebooks (#171)\n\navoids printing the `tqdm` progress bar to a newline in notebooks when detected"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-06-20T16:43:31Z",
        "message": "be able to turn off p2 loss reweighting for upsamplers"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-06-20T16:05:08Z",
        "message": "in paper, blur sigma was 0.6"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-06-20T15:56:37Z",
        "message": "allow for setting beta schedules of unets differently in the decoder, as what was used in the paper was cosine, cosine, linear"
    },
    {
        "repo_url": "github.com/lucidrains/DALLE2-pytorch",
        "filepath": "dalle2_pytorch/dalle2_pytorch.py",
        "commit_date": "2022-06-19T16:25:54Z",
        "message": "Distributed Training of the Decoder (#121)\n\n* Converted decoder trainer to use accelerate\n\n* Fixed issue where metric evaluation would hang on distributed mode\n\n* Implemented functional saving\nLoading still fails due to some issue with the optimizer\n\n* Fixed issue with loading decoders\n\n* Fixed issue with tracker config\n\n* Fixed issue with amp\nUpdated logging to be more logical\n\n* Saving checkpoint now saves position in training as well\nFixed an issue with running out of gpu space due to loading weights into the gpu twice\n\n* Fixed ema for distributed training\n\n* Fixed isue where get_pkg_version was reintroduced\n\n* Changed decoder trainer to upload config as a file\n\nFixed issue where loading best would error"
    },
    {
        "repo_url": "github.com/open-mmlab/mmsegmentation",
        "filepath": "projects/CAT-Seg/cat_seg/models/clip_ovseg.py",
        "commit_date": "2023-08-09T15:57:30Z",
        "message": "[Project] Support CAT-Seg from CVPR2023 (#3098)\n\nThanks for your contribution and we appreciate it a lot. The following\ninstructions would make your pull request more healthy and more easily\nget feedback. If you do not understand some items, don't worry, just\nmake the pull request and seek help from maintainers.\n\n## Motivation\n\nSupport CAT-Seg open-vocabulary semantic segmentation (CVPR2023).\n\n## Modification\n\nSupport CAT-Seg open-vocabulary semantic segmentation (CVPR2023).\n- [x] Support CAT-Seg model training.\n- [x] CLIP model based `backbone` (R101 & Swin-B), aggregation layers\nbased `neck`, and `decoder` head.\n  - [x] Provide customized coco-stuff164k_384x384 training configs.\n- [x] Language model supports for `open vocabulary` (OV) tasks. \n  - [x] Support CLIP-based pretrained language model (LM) inference.\n  - [x] Add commonly used prompts templates. \n- [x] Add README tutorials.\n- [x] Add zero-shot testing scripts.\n\n**Working on the following tasks.**\n- [x] Add unit test.\n\n## BC-breaking (Optional)\n\nDoes the modification introduce changes that break the\nbackward-compatibility of the downstream repos?\nIf so, please describe how it breaks the compatibility and how the\ndownstream projects should modify their code to keep compatibility with\nthis PR.\n\n## Use cases (Optional)\n\nIf this PR introduces a new feature, it is better to list some use cases\nhere, and update the documentation.\n\n## Checklist\n\n1. Pre-commit or other linting tools are used to fix the potential lint\nissues.\n2. The modification is covered by complete unit tests. If not, please\nadd more unit test to ensure the correctness.\n3. If the modification has potential influence on downstream projects,\nthis PR should be tested with downstream projects, like MMDet or\nMMDet3D.\n4. The documentation has been modified accordingly, like docstring or\nexample tutorials.\n\n---------\n\nCo-authored-by: xiexinch <xiexinch@outlook.com>"
    },
    {
        "repo_url": "github.com/modelscope/modelscope",
        "filepath": "modelscope/models/multi_modal/videocomposer/clip.py",
        "commit_date": "2023-08-15T04:01:03Z",
        "message": "VideoComposer: Compositional Video Synthesis with Motion Controllability (#431)\n\n* VideoComposer: Compositional Video Synthesis with Motion Controllability\n\n* videocomposer pipeline\n\n* pre commit\n\n* delete xformers"
    },
    {
        "repo_url": "github.com/Sygil-Dev/Sygil-WebUI",
        "filepath": "webui/streamlit/scripts/img2txt.py",
        "commit_date": "2023-06-23T02:58:24Z",
        "message": "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci"
    },
    {
        "repo_url": "github.com/Sygil-Dev/Sygil-WebUI",
        "filepath": "webui/streamlit/scripts/img2txt.py",
        "commit_date": "2023-01-31T21:30:59Z",
        "message": "Started to move all the streamlit code and dependencies to its own folder in `webui/streamlit` and moving the backend to use nataili, this should help make it self-contained and reduce the amount of code we have in other places which should also make it so the code is easier to understand and read."
    },
    {
        "repo_url": "github.com/brycedrennan/imaginAIry",
        "filepath": "tests/test_guidance.py",
        "commit_date": "2022-11-26T03:23:06Z",
        "message": "Fix typos\n\nFound via `codespell -S ./imaginairy/vendored`"
    },
    {
        "repo_url": "github.com/marqo-ai/marqo",
        "filepath": "src/marqo/s2_inference/clip_utils.py",
        "commit_date": "2024-01-07T23:34:40Z",
        "message": "Improve error handling and API error codes (#656)\n\nCo-authored-by: Farshid Zavareh <farshid@marqo.ai>"
    },
    {
        "repo_url": "github.com/marqo-ai/marqo",
        "filepath": "src/marqo/s2_inference/clip_utils.py",
        "commit_date": "2023-07-26T07:35:54Z",
        "message": "Improve image download validation and resource management (#551)\n\nExplicitly close streaming HTTP connections and PIL images. Ensure _id is never treated as an image URL. Error out if _id is specified as a tensor field by the user."
    },
    {
        "repo_url": "github.com/marqo-ai/marqo",
        "filepath": "src/marqo/s2_inference/clip_utils.py",
        "commit_date": "2023-06-26T13:10:20Z",
        "message": "download integrity (#502)\n\n* catch automodel.pretrained error\n\n* finished open_clip part\n\n* finished open_clip part\n\n* catch mainline\n\n* finish the test\n\n* finish the test\n\n* raise internal errors now.\n\n* update string\n\n* update error message\n\n* update error message\n\n* update error message\n\n* update error message\n\n* update error catching\n\n* update error catching for loading clip model into openclip\n\n* update error handling in s2_inference\n\n* update error handling in s2_inference\n\n* catch mainline\n\n* catch mainline"
    },
    {
        "repo_url": "github.com/marqo-ai/marqo",
        "filepath": "src/marqo/s2_inference/clip_utils.py",
        "commit_date": "2023-06-23T06:31:19Z",
        "message": "Consolidating default device to CUDA when available (#508)\n\n* preliminary replacement work\n\n* changed a few cpus to cuda\n\n* add defaults in tensor search, bulk search, add docs, also added env var\n\n* 1st sweep of removing device params, on parallel now\n\n* transferred default calc from add docs to orch\n\n* replaced defaults in clip_utils.py\n\n* replaced defaults in s2_inference, clip_utils, onnx_utils\n\n* removed defaults everywhere\n\n* added device to vectorise, CLIP calls\n\n* updated device in other models, aux functions\n\n* added device for vectorise and encoding tests\n\n* added device to everything before test_add_documents\n\n* more test fixes\n\n* fixed more test, hardcoded cpu for _float_tensor_to_list\n\n* hardcoded cpu for _float_tensor_to_list\n\n* removed debug message for best available device\n\n* changed comment on vector text search\n\n* all tensor_search tests pass\n\n* fixed errors raised\n\n* added new unit tests, changed env var name\n\n* util env var, replace with empty dict\n\n* updated bulk search default and utils tests\n\n* separated on start method, added search test\n\n* updated validation and more tests\n\n* added unit tests for all internal func validations\n\n* added tests for add docs mp and batch request\n\n* removed validation from Model. Added to SBERT and HF models instead\n\n* added tests to fail if no default orch, search, bulk search\n\n* removed Model test\n\n* moved validation back to Model parent class\n\n* added Model no device test back in"
    },
    {
        "repo_url": "github.com/marqo-ai/marqo",
        "filepath": "src/marqo/s2_inference/clip_utils.py",
        "commit_date": "2023-06-22T06:09:05Z",
        "message": "Return metrics (#506)\n\n* basic Request level metrics/telemetry\n\n* add unit tests for telemetry\n\n* add request metrics to add/search APIs\n\n* python 3.8 fixes\n\n* simplify threaded metrics in download_images\n\n* fix bug\n\n* simplify tests\n\n* fix None in tests\n\n* make tests extra clean\n\n* make metric names more user friendly\n\n* add image download for search. Other improvements\n\n* fix unit tests for clip_utils\n\n* remove 'vector_inference'\n\n* search.create_vectors -> search.vector_inference_full_pipeline\n\n* preprocess -> processing_before_opensearch\n\n* better naming for RequestMetric(s)"
    },
    {
        "repo_url": "github.com/marqo-ai/marqo",
        "filepath": "src/marqo/s2_inference/clip_utils.py",
        "commit_date": "2023-06-13T00:07:26Z",
        "message": "remove autocast for cpu (#491)\n\n* remove autocast for cpu\n\n* add tests\n\n* add tests"
    },
    {
        "repo_url": "github.com/marqo-ai/marqo",
        "filepath": "src/marqo/s2_inference/clip_utils.py",
        "commit_date": "2023-05-22T03:01:11Z",
        "message": "Refactor `tensor_search::search` to use shared functions from `tensor_search::bulk_search` (#469)\n\n* refactor tensor_search/search\n\n* move SearchQuery.context and SearchQuery.scoreModifiers to pydantic\n\n* fix removed SearchContext object\n\n* fix tests\n\n* fix inference for model auth tests\n\n* PR fixes\n\n* add more tests\n\n* multi modal test +1"
    },
    {
        "repo_url": "github.com/marqo-ai/marqo",
        "filepath": "src/marqo/s2_inference/clip_utils.py",
        "commit_date": "2023-05-19T07:08:17Z",
        "message": "custom hf model loading with authentication  (#474)\n\n* custom hf model loading with authentication for hf model\n\n* custom hf model loading with authentication for hf model\n\n* custom hf model loading with authentication for hf model\n\n* custom hf model loading with authentication for hf model\n\n* custom hf model loading with authentication for hf model\n\n* pass the loading from huggingface repo directly test\n\n* test for loading model\n\n* only need to test variants\n\n* add more tests\n\n* add more tests\n\n* fix a test and remove pring\n\n* fix a test and remove pring\n\n* fix a test and remove pring\n\n* fix a test and remove pring\n\n* fix a test and remove pring\n\n* fix a test and remove pring\n\n* fix a test and remove pring\n\n* fix a test and remove pring\n\n* use repo_id instead of name now\n\n* use repo_id instead of name now\n\n* revise from jack\n\n* revise from pandu\n\n* revise from pandu\n\n* update code\n\n* catch automodel.pretrained error\n\n* catch automodel.pretrained error\n\n* catch automodel.pretrained error\n\n* catch automodel.pretrained error\n\n* catch automodel.pretrained error\n\n* catch automodel.pretrained error\n\n* reivise\n\n* add extra tests\n\n* bug fix"
    },
    {
        "repo_url": "github.com/marqo-ai/marqo",
        "filepath": "src/marqo/s2_inference/clip_utils.py",
        "commit_date": "2023-05-11T00:35:53Z",
        "message": "Stateless model auth (#460)\n\n* added model auth\n\n* Added objects for structure\n\n* introducing AddDocsParams object for addDocuments calls\n\nuntested\n\n* added pydantic plugin advice to dev guide\n\n* refactored test_add_docs to use AddDocsParams class\n\n* updated tensor search docstring\n\n* refactored tests to work with new AddDocsParams object\n\n* Commit for refactoring done to test_add_documents_use_existing_tensors.py\n\n* refactored delete_documents to use the add_documents wrapper\n\n* added transitional add_docs wrapper\n\n* added transition add docs wrappers to add_documents calls in these test cases\n\n* added transition add docs wrappers to add_documents calls in these test cases\n\n* test updated to use new add_docs param object\n\n* cleaning up tests with issues\n\n* made progress integrating s3\n\n* add docs, s3 works\n\n* search seems to work (it at least passes through the auth info OK)\n\n* added boto to reqs add hf skeleton funcs\n\n* added changes\n\n* fixed test_bulk_search_different_models_separate_vectorise_calls\n\n* created test_model_auth_s3 ()\n\n* test_model_auth_s3() asserts boto3 client instantiation\n\n* made assertions about the presence of the model file\n\n* added hf loading tests\n\n* added tests for hf, s3 search\n\n* added test for s3/hf mismatch\n\n* test refactor\n\n* added cuda test\n\n* add_docs parses model auth str\n\n* corrected add_docs derivatives test\n\n* removed unused vectorise params\n\n* fixed bug, not passing through auth in multimodal_combination\n\n* added test for no creds\n\n* added test_bad_creds_error_s3\n\n* test for access to non existent hf repo\n\n* Added test_after_downloading_search_doesnt_redownload\n\n* fixed checking loaded models\n\n* added from_s3 tests\n\n* added from_hf tests\n\n* added test_custom_clip_utils.py\n\nneed to fix tests\n\n* added call to search before add docs parallel\n\n* Added search call before parallel add_docs\n\n* fixed casing issue in SearchQuery\n\n* fixed tests\n\n* completed custom clip utils test\n\n* updated version\n\n* improved error msg, added bulk search tests\n\n* made custom clip tests stricter\n\n* added device to model auth cuda setup\n\n* added tests for CLIP._download_from_repo\n\n* added CLIP.load() tests\n\n* added OPEN_CLIP.load() tests\n\n* fixed private model and test\n\n* corrected HF auth and location inheritance\n\n* removed test_put_documents_orchestrator() as put_documents is deprecated\n\n* corrected version"
    },
    {
        "repo_url": "github.com/marqo-ai/marqo",
        "filepath": "src/marqo/s2_inference/clip_utils.py",
        "commit_date": "2023-02-28T03:18:22Z",
        "message": "[feature] Multimodal tensor combination (#332)\n\n* draft PR\n\n* add test\n\n* delete comments and prints\n\n* new design\n\n* adding test\n\n* add assertion\n\n* change comments\n\n* add more args\n\n* add more test\n\n* remove print\n\n* catch mainline\n\n* catch mainline\n\n* catch mainline\n\n* catch mainline\n\n* catch mainline\n\n* catch mainline\n\n* adding test\n\n* adding test\n\n* change _infer_opensearch_data_type\n\n* support dictionary\n\n* fix text\n\n* add test\n\n* revise parameters\n\n* add to image repo\n\n* revise again.\n\n* revised\n\n* add batch downloading.\n\n* revise test\n\n* add new test\n\n* remove space\n\n* revised.\n\n* revised.\n\n* revised.\n\n* revised.\n\n* update index info\n\n* update index info\n\n* update validation\n\n* update test for new api\n\n* updated\n\n* updated\n\n* add validate mappings\n\n* add todo\n\n* revised\n\n* revised\n\n* revised\n\n* revised\n\n* revised\n\n* revised\n\n* revised\n\n* add test\n\n* add test\n\n* add test\n\n* Mappings for add_docs (#355)\n\n* Update CONTRIBUTING.md\n\n* Update CONTRIBUTING.md\n\n* Adding mappings validation\n\n* mappings validation within add_documents\n\n* added mappings to endpoint and orchestrator: untested\n\n* add test\n\n* add more tests\n\n* add more tests\n\n* add more tests\n\n* finalise\n\n* add open_search test\n\n* update all the error messages.\n\n* update all the error messages.\n\n---------\n\nCo-authored-by: pandu-k <107458762+pandu-k@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/marqo-ai/marqo",
        "filepath": "src/marqo/s2_inference/clip_utils.py",
        "commit_date": "2023-02-27T05:04:57Z",
        "message": "Update clip_utils.py (#351)\n\nupdate custom open_clip models to use open_clip base tokenizer instead of clip"
    },
    {
        "repo_url": "github.com/marqo-ai/marqo",
        "filepath": "src/marqo/s2_inference/clip_utils.py",
        "commit_date": "2023-02-20T11:09:11Z",
        "message": "Image download headings (#336)\n\n* add header for authentication\n\n* update load image path tests\n\n* remove headers as a required argument\n\n* update tests and image utils\n\n* fixed bugs\n\n* Changed headers type to dict internally\n\n* Image download headers propagated for search()\n\n* changed remaining image_download_headers param defaults to None\n\n---------\n\nCo-authored-by: Tom Hamer <tom@marqo.ai>"
    },
    {
        "repo_url": "github.com/marqo-ai/marqo",
        "filepath": "src/marqo/s2_inference/clip_utils.py",
        "commit_date": "2023-02-16T23:42:55Z",
        "message": "Fp16 clip update (#331)\n\n* update fp16 model and add tests\n\n* update fp16 model and add tests"
    },
    {
        "repo_url": "github.com/marqo-ai/marqo",
        "filepath": "src/marqo/s2_inference/clip_utils.py",
        "commit_date": "2023-02-14T00:39:38Z",
        "message": "[features] Open clip update (#305)\n\n* update open_clip models\n\n* update open_clip models\n\n* update open_clip models\n\n* update open_clip models\n\n* update open_clip models\n\n* add auto_cast to open_clip\n\n* add auto_cast to open_clip\n\n* convert to float32 at output to normalize it.\n\n* convert to float32 at output to normalize it.\n\n* add onnx32/open_clip/ViT-L-14/openai and\nonnx16/open_clip/ViT-L-14/openai"
    },
    {
        "repo_url": "github.com/marqo-ai/marqo",
        "filepath": "src/marqo/s2_inference/clip_utils.py",
        "commit_date": "2023-02-13T23:17:02Z",
        "message": "Broaden catch download error (#321)\n\n* RequestException is excepted, which is the super class of all requests errors\n\n* Fixed unit tests"
    },
    {
        "repo_url": "github.com/marqo-ai/marqo",
        "filepath": "src/marqo/s2_inference/clip_utils.py",
        "commit_date": "2023-02-10T04:30:25Z",
        "message": "Concurrent image downloads  (#281)\n\n* added changed files from branch concurrent download img 2\n\n* refactored model registry/multilingual clip to prevent circular import\n\n* Fixed bug where a bounding box is returned rather than highlights\n\n* added test cases\n\n* removed unnecessary extra call\n\n* remove unused circular reference\n\n* added timeout of 3 seconds to image downloads\n\n* added timeout and unit tests\n\n* added timeout test to clip_utils\n\n* Widened the net of image download errors\n\n* Specific image retrieval error is passed to user"
    },
    {
        "repo_url": "github.com/marqo-ai/marqo",
        "filepath": "src/marqo/s2_inference/clip_utils.py",
        "commit_date": "2023-01-10T23:00:56Z",
        "message": "add unit test for multilingual clip"
    },
    {
        "repo_url": "github.com/marqo-ai/marqo",
        "filepath": "src/marqo/s2_inference/clip_utils.py",
        "commit_date": "2023-01-06T07:28:22Z",
        "message": "Onnx refactor: adding open_clip onnx support, decouple image preprocess (#255)\n\n* merge model properties\n\n* add transform and open_clip support for onnx_clip\n\n* add transform and open_clip support for onnx_clip\n\n* add transform and open_clip support for onnx_clip\n\n* add transform and open_clip support for onnx_clip\n\n* add transform and open_clip support for onnx_clip\n\n* add transform and open_clip support for onnx_clip\n\n* update open_clip tokenizer function\n\n* update open_clip tokenizer function\n\n* add \"onnx32/open_clip/ViT-L-14/laionb_s32b_b82k\" and \"onnx16/open_clip/ViT-L-14/laionb_s32b_b82k\"\n\n* add \"onnx32/open_clip/ViT-L-14/laionb_s32b_b82k\" and \"onnx16/open_clip/ViT-L-14/laionb_s32b_b82k\"\n\n* add \"onnx32/open_clip/ViT-L-14/laionb_s32b_b82k\" and \"onnx16/open_clip/ViT-L-14/laionb_s32b_b82k\"\n\n* add \"onnx32/open_clip/ViT-L-14/laionb_s32b_b82k\" and \"onnx16/open_clip/ViT-L-14/laionb_s32b_b82k\"\n\n* add \"onnx32/open_clip/ViT-L-14/laionb_s32b_b82k\" and \"onnx16/open_clip/ViT-L-14/laionb_s32b_b82k\"\n\n* add \"onnx32/open_clip/ViT-L-14/laionb_s32b_b82k\" and \"onnx16/open_clip/ViT-L-14/laionb_s32b_b82k\"\n\n* add \"onnx32/open_clip/ViT-L-14-336/openai\"\n\n* add \"onnx32/open_clip/ViT-L-14-336/openai\"\n\n* onnx/open_clip/ViT-B-32/openai\n\n* onnx/open_clip/ViT-B-32/laion400m_e31\n\n* onnx/open_clip/ViT-B-32/laion400m_e31\n\n* onnx/open_clip/ViT-B-32/laion400m_e31\n\n* 'onnx32/open_clip/ViT-B-32-quickgelu/laion400m_e31'\n\n* 'onnx32/open_clip/ViT-B-32-quickgelu/laion400m_e32'\n\n* 'onnx32/open_clip/ViT-B-16/openai'\n\n* ViT-B-16/laion400m_e31\n\n* ViT-B-16/laion400m_e32\n\n* onnx32/open_clip/ViT-B-16-plus-240/laion400m_e31\n\n* ViT-B-16-plus-240/laion400m_e32\n\n* onnx32/open_clip/ViT-H-14/laion2b_s32b_b79k\n\n* adding zip, unzip logic\n\n* adding zip, unzip logic\n\n* adding zip, unzip logic\n\n* adding zip, unzip logic\n\n* adding zip, unzip logic\n\n* adding zip, unzip logic\n\n* adding zip, unzip logic\n\n* adding zip, unzip logic\n\n* adding zip, unzip logic\n\n* 'onnx32/open_clip/ViT-g-14/laion2b_s12b_b42k'\n\n* finish all the open_clip onnx model card\n\n* add test for onnx/open_clip\n\n* add test for onnx/open_clip\n\n* clear loaded models in encoding test\n\n* clear loaded models in encoding test\n\n* clear loaded models in encoding test\n\n* revise space between functions\n\n* revise space between functions"
    },
    {
        "repo_url": "github.com/marqo-ai/marqo",
        "filepath": "src/marqo/s2_inference/clip_utils.py",
        "commit_date": "2022-12-13T22:45:12Z",
        "message": "Visual search update release (#214)\n\n* Visual search update (#210)\n\n* add parametrisation for chunking, overlapping boxes and combined model+boxes\n\n* add more tests\n\n* integrate yolox and simple grid changes\n\n* helper functions for opencv and yolox\n\n* add yolox patch class and helper functions\n\n* update to use opencv\n\n* update model cache and add logging\n\n* fix model caching and device selection\n\n* device conflicts\n\n* add attention based bb generation\n\n* update bboxes and test yolo\n\n* refactor and add attention based ViT for bb determination\n\n* create dino specific utils file for vit attention\n\n* include dino files\n\n* split the image file into seperate utils\n\n* refactor, split into seperate files and use a proper base class\n\n* add more tests\n\n* update and add more tests\n\n* refactor and clean up\n\n* add more packages\n\n* clean up and refactor\n\n* update tests\n\n* docker and cloud versions\n\n* change\n\n* fix device for owl\n\n* update types and dco strings\n\n* rename file\n\n* rename file\n\n* move tests\n\n* pytorch utils test\n\n* add another error for model loading\n\n* update functions to handle some edge cases and update types\n\n* add more tests\n\n* update the yolox utils to download the proper model\n\n* add yolox specific tests\n\n* update reqs and setup to be on the latest\n\n* update dockerfile to be same as new one\n\n* clean up\n\n* add the example and app\n\n* update file locations\n\n* update demo\n\n* bump model version\n\n* update demo\n\n* minor text edits\n\n* update demo\n\n* update demo\n\n* better error handling\n\n* update tests\n\n* change to PIL error\n\n* add more error types\n\n* small fixes for errors\n\n* error handling\n\n* update tests\n\n* remove models\n\n* clean up, doc strings and formatting\n\n* update tests\n\n* minor formatting\n\nCo-authored-by: Jesse Clark <jesse@s2search.io>\n\n* update some function names after merge\n\n* clean up and rename\n\n* Pinning tox ci (#211)\n\n* Branch aware ci tests (#209)\n\n* set the MQ_API_TEST_BRANCH to the current branch\n\n* setting github ref to just branch name\n\n* Adding quotes around env var\n\n* fixed syntax error\n\n* parsing the github.ref string\n\n* exporting just before running\n\n* added image_to_test input\n\n* fix: typo\n\n* default image to test is now explicit\n\n* updated documentation for image_to_test var\n\n* Update unit_test_CI.yml\n\n* tox pinned to 3.26\n\nCo-authored-by: pandu-k <107458762+pandu-k@users.noreply.github.com>\nCo-authored-by: Pandu Oliver Kerr <pandu@s2search.io>\n\n* Update Dockerfile\n\nremove space\n\n* clean up\n\n* move to headless opencv\n\n* tidying up\n\n* update error\n\n* minor edits\n\n* fix PR feedback and add more descriptions in the docstrings\n\n* use literal type\n\n* clean up, make the names more descriptive\n\n* change logging level to debug for some messages\n\nCo-authored-by: Jesse Clark <jesse@s2search.io>\nCo-authored-by: pandu-k <107458762+pandu-k@users.noreply.github.com>\nCo-authored-by: Pandu Oliver Kerr <pandu@s2search.io>"
    },
    {
        "repo_url": "github.com/marqo-ai/marqo",
        "filepath": "src/marqo/s2_inference/clip_utils.py",
        "commit_date": "2022-12-02T06:30:39Z",
        "message": "use UnidentifiedImageError if it cannot be loaded from URL source"
    },
    {
        "repo_url": "github.com/marqo-ai/marqo",
        "filepath": "src/marqo/s2_inference/clip_utils.py",
        "commit_date": "2022-11-29T01:36:55Z",
        "message": "Check HTTP status for remote image paths"
    },
    {
        "repo_url": "github.com/marqo-ai/marqo",
        "filepath": "src/marqo/s2_inference/clip_utils.py",
        "commit_date": "2022-08-26T04:27:24Z",
        "message": "split the image load to be used elsewhere"
    },
    {
        "repo_url": "github.com/modelscope/modelscope",
        "filepath": "modelscope/models/multi_modal/clip_interrogator/model.py",
        "commit_date": "2024-02-21T06:24:24Z",
        "message": "Fix word pubicly -> publicly (#748)"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-08-04T20:41:56Z",
        "message": "fix: cutouts generation (#138)\n\n* fix: cutouts generation\n\n* fix: cutouts generation\n\n* fix: cutouts generation\n\n* fix: cutouts generation\n\n* fix: cutouts generation\n\n* fix: cutouts generation\n\n* fix: cutouts generation\n\n* fix: cutouts generation\n\n* fix: cutouts generation\n\n* fix: cutouts generation\n\n* fix: cutouts generation\n\n* fix: cutouts generation\n\n* fix: cutouts generation\n\n* fix: cutouts generation\n\n* fix: cutouts generation\n\n* fix: cutouts generation\n\n* style: fix overload and cli autocomplete\n\n* fix: cutouts generation\n\n* fix: cutouts generation\n\nCo-authored-by: Jina Dev Bot <dev-bot@jina.ai>"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-08-03T18:26:48Z",
        "message": "feat: add option for visualizing cuts (#130)\n\n\n* style: fix overload and cli autocomplete\n\n* feat: add option for visualizing cuts\n\nCo-authored-by: Jina Dev Bot <dev-bot@jina.ai>"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-08-02T19:04:56Z",
        "message": "fix: make eval schedule string safe (#126)"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-31T16:06:00Z",
        "message": "refactor: disable wandb by default (#117)\n\n\n\nCo-authored-by: Jina Dev Bot <dev-bot@jina.ai>"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-30T22:32:33Z",
        "message": "refactor: default no sha for faster load (#110)"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-30T14:51:41Z",
        "message": "feat: add name var substitute (#105)\n\n* feat: add name var substitute\n\n* refactor: ipython display in tabs\n\n* style: fix overload and cli autocomplete\n\nCo-authored-by: Jina Dev Bot <dev-bot@jina.ai>"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-30T11:09:47Z",
        "message": "fix: save gif and progress correctly on batch size (#104)"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-28T09:28:25Z",
        "message": "feat: allow scheduling on prompt weight (#96)\n\n* fix: add use checkpoint\n\n* feat: allow scheduling on prompt weight\n\n* feat: allow scheduling on prompt weight\n\n* feat: allow scheduling on prompt weight\n\n* feat: allow scheduling on prompt weight\n\n* feat: allow scheduling on prompt weight\n\n* style: fix overload and cli autocomplete\n\n* feat: allow scheduling on prompt weight\n\nCo-authored-by: Jina Dev Bot <dev-bot@jina.ai>"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-27T12:44:13Z",
        "message": "feat: 0.8 add prompt scheduling (#91)"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-25T10:33:19Z",
        "message": "fix: ic_cut_gray in docstring (#83)\n\n* fix: ic_cut_gray in docstring\n\n* style: fix overload and cli autocomplete\n\n* fix: ic_cut_gray in docstring\n\n* fix: ic_cut_gray in docstring\n\nCo-authored-by: Jina Dev Bot <dev-bot@jina.ai>"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-24T19:55:54Z",
        "message": "refactor: optimize loss compute logic (#78)"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-24T13:01:40Z",
        "message": "feat: allow customized remote url for diff models"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-24T12:19:40Z",
        "message": "feat: output gif for intermediate results (#76)"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-24T09:39:24Z",
        "message": "fix: set strict to false (#75)"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-21T22:45:57Z",
        "message": "fix: display thread problem (#68)\n\n* fix: display thread problem\n\n* fix: display thread problem\n\n* revert: text clip on gpu\n\n* fix: #67\n\n* style: fix overload and cli autocomplete\n\n* fix: #67\n\nCo-authored-by: Jina Dev Bot <dev-bot@jina.ai>"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-21T16:19:25Z",
        "message": "fix: open clip loader device placement (#64)"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-21T14:20:44Z",
        "message": "feat: move text transformer to cpu (#62)"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-20T11:34:15Z",
        "message": "refactor: do plot and persist in another thread (#55)"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-16T20:09:34Z",
        "message": "fix: save filename on done (#37)"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-16T11:26:35Z",
        "message": "refactor: core (#36)\n\n* refactor: core\n\n* refactor: core\n\n* refactor: remove git clone install\n\n* refactor: remove git clone install\n\n* refactor: remove git clone install"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-13T10:45:49Z",
        "message": "feat: add complete tag (#26)\n\n* fix: update help text\n\n* fix: update help text"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-13T10:08:32Z",
        "message": "feat: add all scheduler (#25)\n\n* feat: add all scheduler\n\n* style: fix overload and cli autocomplete\n\n* feat: add all scheduler\n\n* feat: add all scheduler\n\n* fix: update help text\n\n* fix: update help text\n\n* fix: update help text\n\nCo-authored-by: Jina Dev Bot <dev-bot@jina.ai>"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-12T14:48:40Z",
        "message": "feat: add comics diffusion model (#21)\n\n* feat: add comics diffusion model\n\n* feat: add comics diffusion model"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-12T12:16:31Z",
        "message": "feat: 0.3 (#19)\n\n* refactor: improve final result instruction\n\n* feat: support more diffusion models\n\n* fix: secondary model scheduling\n\n* refactor: create api\n\n* refactor: create api\n\n* refactor: create api\n\n* refactor: create api\n\n* refactor: create api\n\n* style: fix overload and cli autocomplete\n\n* refactor: create api\n\n* style: fix overload and cli autocomplete\n\n* refactor: create api\n\n* style: fix overload and cli autocomplete\n\n* feat: add recommended size\n\n* feat: add recommended size\n\n* feat: add recommended size\n\nCo-authored-by: Jina Dev Bot <dev-bot@jina.ai>"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-11T21:58:22Z",
        "message": "feat: add secondary model scheduling (#15)\n\n* chore: fix readme\n\n* style: fix overload and cli autocomplete\n\n* feat: add secondary model scheduling\n\n* feat: add secondary model scheduling\n\n* feat: add secondary model scheduling\n\n* style: fix overload and cli autocomplete\n\n* ci: add tests ci\n\n* ci: add tests ci\n\n* refactor: remove wget\n\n* refactor: remove wget\n\n* refactor: remove wget\n\n* refactor: remove wget\n\n* refactor: remove wget\n\nCo-authored-by: Jina Dev Bot <dev-bot@jina.ai>"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-11T16:30:44Z",
        "message": "feat: add clip model scheduling (#14)\n\n* feat: add clip model scheduling\n\n* feat: support customized model config\n\n* feat: support customized model config\n\n* feat: support customized model config\n\n* feat: support customized model config\n\n* feat: add cheatsheet\n\n* feat: add cheatsheet\n\n* feat: add cheatsheet\n\n* feat: add cheatsheet\n\n* feat: add cheatsheet\n\n* feat: add cheatsheet\n\n* feat: add cheatsheet\n\n* feat: add cheatsheet"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-10T20:00:31Z",
        "message": "feat: ipython deps is optional fix cpu support"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-10T19:42:54Z",
        "message": "feat: ipython deps is optional fix cpu support"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-10T19:26:31Z",
        "message": "feat: ipython deps is optional fix cpu support"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-09T19:04:11Z",
        "message": "feat: 0.1 (#10)\n\n* fix: upstream #116\n\n* refactor: replace clip with open-clip\n\n* feat: add custom models support and symmetry\n\n* style: reformat with blacl\n\n* style: reformat with blacl\n\n* fix: broken compression on race condition #9\n\n* fix: use uuid instead of seed as da name\n\n* docs: update readme\n\n* docs: update readme\n\n* docs: update readme\n\n* fix: update guided diffusion upstream\n\n* fix: update guided diffusion upstream\n\n* fix: update guided diffusion upstream\n\n* fix: update guided diffusion upstream\n\n* fix: update guided diffusion upstream\n\n* fix: update guided diffusion upstream\n\n* fix: update guided diffusion upstream\n\n* fix: update guided diffusion upstream\n\n* fix: update guided diffusion upstream\n\n* fix: update guided diffusion upstream\n\n* fix: update guided diffusion upstream\n\n* feat: add spell checker\n\n* feat: add spell checker\n\n* feat: add spell checker\n\n* feat: add spell checker\n\n* feat: add spell checker\n\n* feat: add spell checker\n\n* feat: add spell checker\n\n* feat: add spell checker\n\n* feat: add spell checker\n\n* feat: add spell checker\n\n* feat: add spell checker\n\n* feat: add spell checker\n\n* feat: add spell checker\n\n* feat: add spell checker\n\n* feat: add spell checker\n\n* feat: add spell checker\n\n* feat: add spell checker\n\n* feat: add spell checker"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-08T20:28:55Z",
        "message": "feat: add open clip pretrained models"
    },
    {
        "repo_url": "github.com/jina-ai/discoart",
        "filepath": "discoart/helper.py",
        "commit_date": "2022-07-08T20:13:59Z",
        "message": "feat: add open clip pretrained models"
    },
    {
        "repo_url": "github.com/microsoft/onnxruntime",
        "filepath": "onnxruntime/python/tools/transformers/models/stable_diffusion/test/check_image.py",
        "commit_date": "2024-01-31T01:39:27Z",
        "message": "Save stablediffusion and open-clip in pipeline cache (#19314)\n\n### Description\n1. save the model to pipeline cache\n2. lower the similarly bar to 97\n3. publish the generated image that we can check it once the test fails\n\n\n### Motivation and Context\nReduce model downloads"
    },
    {
        "repo_url": "github.com/microsoft/onnxruntime",
        "filepath": "onnxruntime/python/tools/transformers/models/stable_diffusion/test/check_image.py",
        "commit_date": "2024-01-29T17:33:58Z",
        "message": "Add VP test in Stable diffusion pipeline (#19300)\n\n### Description\n1. Add visual parity test based on openai clip model\n2. Add trigger rules\n\n### Motivation and Context\n1. check generated image is expected\n2. reduce unnecessary triggers"
    },
    {
        "repo_url": "github.com/NVIDIA/NeMo",
        "filepath": "nemo/collections/multimodal/modules/stable_diffusion/encoders/modules.py",
        "commit_date": "2024-01-20T01:59:23Z",
        "message": "Final multimodal PR with our recent developments on MM side (#8127)\n\n* Hotfix (#7501) (#7568)\n\nSigned-off-by: Jan Baczek <jbaczek@nvidia.com>\nCo-authored-by: jbaczek <45043825+jbaczek@users.noreply.github.com>\n\n* Avoid duplicated checkpoint save (#7555) (#7566)\n\nSigned-off-by: Miko\u0142aj B\u0142a\u017c <mblaz@nvidia.com>\nCo-authored-by: mikolajblaz <mikolajblaz@users.noreply.github.com>\n\n* Cache FP8 weight and transpose only at the first micro-batch in each validation and test routine (#7470) (#7483)\n\n* Cache weight and transpose only in the first batch in all training, val, and test runs\n\n\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Add an option to disable manual GC in validation (#7467) (#7476)\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\n\n* Remove PUBLICATIONS.md, point to github.io NeMo page instead (#7694) (#7695)\n\n* update publications section to point to blog website page\n\n\n\n* add hyphen\n\n\n\n* use double backquotes for code formatting\n\n\n\n---------\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\nSigned-off-by: Elena Rastorgueva <80532067+erastorgueva-nv@users.noreply.github.com>\nCo-authored-by: Elena Rastorgueva <80532067+erastorgueva-nv@users.noreply.github.com>\n\n* Fix multi rank finetune for ASR (#7684) (#7699)\n\n* Fix multi rank finetune for ASR\n\n\n\n* Actually add time\n\n\n\n* Actually add time\n\n\n\n---------\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\n\n* Update docs: readme, getting started, ASR intro (#7679)\n\n* [TTS] Add dataset to path of logged artifacts (#7462)\n\n* [TTS] Add dataset to path of logged artifacts\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Revert axis name back to Audio Frames\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* move install info to INSTALLATION.md\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* tidy up links\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix sft dataset truncation (#7464)\n\n* Add fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Automatic Lip Reading Recognition (ALR) - ASR/CV (Visual ASR) (#7330)\n\n* striding_conv1d_k5 and dw_striding_conv1d_k5 subsampling\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* transpose conv1d inputs\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* Update subsampling.py\n\nchange striding_conv1d_k5 to striding_conv1d\n\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\n\n* cv branch\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* video manifest\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* add collection classes\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add test_step_outputs\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* correct manifest bug when having only audio or only videos\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* correct manifest bug when having only audio or only videos\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* clean references\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* freeze unfreeze transcribe cv models\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* correct manifest get_full_path bug\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* update for PR\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* guard torchvision\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update nemo/collections/cv/data/video_to_text_dataset.py\n\nCo-authored-by: Igor Gitman <igor.a.gitman@gmail.com>\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\n\n* _video_speech_collate_fn in cv/data/video_to_text.py\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* add self.out = None to asr subsampling\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* Update nemo/collections/cv/data/video_to_text_dataset.py\n\nCo-authored-by: Igor Gitman <igor.a.gitman@gmail.com>\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\n\n* cv -> multimodal/speech_cv branch\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Igor Gitman <igor.a.gitman@gmail.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* HF StarCoder to NeMo conversion script (#7421)\n\n* Script to convert HF StarCoder checkpoint to NeMo\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* StarCoder conversion test\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Fix test\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Catch up with save_to changes\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Don't abbreviate args for clarity\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Configurable precision: BF16 vs FP32\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* fix bug when loading dist ckpt in peft (#7452)\n\nSigned-off-by: Hongbin Liu <hongbinl@nvidia.com>\nCo-authored-by: Hongbin Liu <hongbinl@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix adding positional embeddings in-place in transformer module (#7440)\n\nSigned-off-by: Tamerlan Tabolov <tktabolov@gmail.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix (#7478)\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* add sleep (#7498) (#7499)\n\n* add sleep\n\n\n\n* add sleep onto config instead\n\n\n\n* add comment\n\n\n\n---------\n\nSigned-off-by: Gerald Shen <geshen@nvidia.com>\nCo-authored-by: Gerald Shen <119401249+gshennvm@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix exp manager check for sleep (#7503) (#7504)\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* bugfix: trainer.accelerator=auto from None. (#7492) (#7493)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [doc] fix broken link (#7481)\n\nSigned-off-by: Stas Bekman <stas00@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [TTS] Read audio as int32 to avoid flac read errors (#7477)\n\n* [TTS] Read audio as int32 to avoid flac read errors\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Add comment about read failures\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Add dataset 'AISHELL-3' from OpenSLR for training mandarin TTS (#7409)\n\n* Add dataset 'AISHELL-3' from OpenSLR for training mandarin TTS\n* Train 'AISHELL-3' dataset with multi-speakers\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update get_data.py\n\nupdate copyright header\n\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* Update get_data.py\n\nadded a disclaimer\n\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add new configuration file for AISHELL3 with multispeaker of fastpitch\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* dllogger - log on rank 0 only (#7513)\n\nSigned-off-by: Stas Bekman <stas00@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix TTS FastPitch tutorial (#7494) (#7516)\n\n* Fix\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix get_dist() tensor dimension (#7506) (#7515)\n\nSigned-off-by: Jocelyn Huang <jocelynh@nvidia.com>\nCo-authored-by: Jocelyn <jocelynh@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* bugfix: specify trainer.strategy=auto when devices=1 (#7509) (#7512)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* fix (#7511)\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [TTS] Fix FastPitch data prep tutorial (#7524)\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* add italian tokenization (#7486)\n\n* add italian tokenization\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add more ipa lexicon it\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix error deletion\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* add test\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Replace None strategy with auto in tutorial notebooks (#7521) (#7527)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* unpin setuptools (#7534) (#7535)\n\nSigned-off-by: fayejf <36722593+fayejf@users.noreply.github.com>\nCo-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* remove auto generated examples (#7510)\n\n* explicitly remove autogenerated examples for data parallel evaluation\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* mark autogenrated and remove it for test\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: arendu <adithyare@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Add the `strategy` argument to `MegatronGPTModel.generate()` (#7264)\n\nIt is passed as an explicit argument rather than through\n`**strategy_args` so as to ensure someone cannot accidentally pass other\narguments that would end up being ignored.\n\nIt is a keyword-only argument to ensure that if in the future we want to\nupdate the signature to `**strategy_args`, we can do it without breaking\ncode.\n\nSigned-off-by: Olivier Delalleau <507137+odelalleau@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix PTL2.0 related ASR bugs in r1.21.0: Val metrics logging, None dataloader issue (#7531) (#7533)\n\n* fix none dataloader issue ptl2\n\n\n\n* ptl2.0 logging fixes for rnnt_models\n\n\n\n---------\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\nCo-authored-by: Kunal Dhawan <kunaldhawan97@gmail.com>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* gpus -> devices (#7542) (#7545)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Update FFMPEG version to fix issue with torchaudio (#7551) (#7553)\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* PEFT GPT & T5 Refactor (#7308)\n\n* initial implementation of add_adapters API\n\n* correct type hint\n\n* Add config in add_adapters for save and load (@author bobchen)\n\n* Remove AdapterConfig to avoid import error\n\n* Add AdaterConfig back and move adaptermixin to sft model\n\n* Add NLPSaveRestoreConnector as default in NLPModel.restore_from\n\n* Add restore_from_nemo_with_adapter and test script\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* rename t5 file and classes to be consistent with GPT\n\n* add t5 sft dataset\n\n* add support for single-file format with T5SFTDataset\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Various small changes to make T5 SFT work like GPT SFT\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add adapter evaluation test script\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add MultiAdaterConfig for ia3 and fix builder issue\n\n* Make ptuning for T5SFTModel work using mixin\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add IA3_Adapter for AdapterName\n\n* Add adapter name for ptuning and attention adapter\n\n* Make test script GPT/T5 agnostic\n\n* Add layer selection feature\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Integrate adapter name and config\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update gpt peft tuning script to new API\n\n* add t5 peft tuning script with new API\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix IA3 layer selection issue\n\n* Override state_dict on SFT model instead of mixin\n\n* Add load adapter by adapter config\n\n* move peft config map away from example script\n\n* auto get config from nemo adapter\n\n* Move PEFTConfig to new file\n\n* fix ckpt save/load for t5\n\n* name change: add_adapters -> add_adapter\n\n* variable name change\n\n* update t5 script\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix t5 issues\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add weight tying\n\n* update gpt tuning script\n\n* PEFT-API proposal\n\n* Fix according to comments\n\n* update tuning scripts\n\n* move merge_cfg_with to mixin class since it applies to both gpt and t5 and requires the model class for restore\n\n* Add mcore_gpt support for NLPAdapterMixin\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix typo\n\n* variable name change to distinguish \"peft\" and \"adapter\"\n\n* override `load_adapters` to support `add_adapter` name change\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update tuning and eval script for adapter save/load\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add Ptuning on first stage only\n\n* add lora tutorial for review\n\n* Fix layer selection for mcore\n\n* add landing page\n\n* fix resume training\n\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add mcore condition in sharded_state_dict to make sft work\n\n* Update lora_tutorial.md\n\nFirst edit of this file for PEFT documentation for NeMO\n\nSigned-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\n\n* rename Adapter to AttentionAdapter to avoid confusion in doc\n\n* Change load_adapters to load .nemo\n\n* add quick start guide\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add load_adapters with .ckpt\n\n* Remove setup_complete changes in load_adapters\n\n* update landing page\n\n* remove typo\n\n* Updated quick_start.md per Chen Cui\n\nSigned-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\n\n* Add inference config merger and tutorial\n\n* Add doc string for NLPAdapterModelMixin and deprecated warning on MegatronGPTPEFTModel\n\n* add supported_methods.md and update other documentations\n\n* Update supported_methods.md\n\nminor updates.\n\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\n\n* Update landing_page.md\n\nminor update.\n\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\n\n* Modify doc string for NLPAdapterModelMixin\n\n* Add doc string add_adapters in NLPAdapterModelMixin\n\n* rename canonical adapters\n\n* remove mcore hard dependency\n\n* [PATCH] move microbatch calculator to nemo from apex\n\n* remove apex dependency in gpt and t5 sft models\n\n* remove apex dependency in gpt model\n\n* render doc strings\n\n* fix\n\n* Add missing virtual_tokens on ptuning\n\n* fix docstrings\n\n* update gpt-style model coverage in docs\n\n* update docstring\n\n* Remove pdb\n\n* add lightning_fabric to make docstring rendering work\n\n* Add Ptuning missing key\n\n* try docstring rendering\n\n* Fix ptuning issue\n\n* update gpt t5 peft tuning and eval scripts\n\n* typos\n\n* update eval config\n\n* fix bug relating to apex dependency removal\n\n* typo\n\n* make predict step behave the same as test step\n\n* make lora tutorial work in notebook\n\n* cosmetics\n\n* update yaml scripts\n\n* mcore_gpt attribute optional\n\n* typo\n\n* update eval scripts and fix T5 eval bugs\n\n* add NLPDDPStrategyNotebook and trainer builder logic to use it\n\n* update lora notebook to use new trainer builder\n\n* fix microbatch calculator bug for inference after training\n\n* Convert markdown files to RST and incorporate with doc\n\n* typo\n\n* revise language\n\n* remove extra cell\n\n* remove unnecessary inheritance\n\n* remove old tests\n\n* move layer selection default so logging messages make sense\n\n* remove `save_adapters` as adapter weights are saved automatically during training\n\n* initialize weights from a checkpoint instead of randomly\n\n* multiple fields can form a context (#7147)\n\n* list of context fields and flexible prompt template\n\nSigned-off-by: arendu <adithya.r@gmail.com>\n\n* list of fields for context\n\nSigned-off-by: arendu <adithya.r@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Add multiple truncation fields and middle truncation\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Compatible to old ckpt\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix tokenize detokenize issue\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Remove detokenization, add truncation augmentation\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Resolve comments\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Remove unused import\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* revert eos\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Add tokenizer space_sensitive attribute\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix error\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Fix erorr and use re\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Change assert logic\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Follow adi suggestion\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Remove merge function\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add example and comment\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Remove context_key and add comment\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Remove random truncation\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix template none\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-off-by: arendu <adithya.r@gmail.com>\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nSigned-off-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\n* revert config changes\n\n* remove accidental breakpoint\n\n* support TP>1 loading\n\n* infer adapter type from checkpoint in during eval\n\n* breakup add adapter\n\n* enable interpolation of train_ds and validation_ds\n\n* update metric calc script to conform to single-file eval format\n\n* remove extraneous print\n\n* update lora notebook for updated merge_inference_cfg\n\n* Update nlp_adapter_mixins.py\n\nvariable name change\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* turn off grad scaler for PP to match old scripts\n\n* remove PEFTSaveRestoreConnector since functionality all covered by the new mixin class\n\n* remove resume_from_checkpoint check since covered in #7335\n\n* revert changes made in eval config interpolation\n\n* more interpolation\n\n* typo\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove dup line\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* code style warnings\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix config mistake\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* add copyright header\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* fix code check warnings\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* revert changes to remove apex dependency (mixed apex+nemo microbatch calculator broke some CI tests)\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add more deprecation notices\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* update deprecation notices\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* update deprecation notices\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* consolidate peft and sft scripts\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* update CI tests\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* notebook branch points to main to prepare for merge\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* fix gpt and t5 validation with any metric other than loss\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* support pre-extracted checkpoints\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n---------\n\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\nSigned-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\nSigned-off-by: arendu <adithya.r@gmail.com>\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nSigned-off-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\nSigned-off-by: Chen Cui <chcui@nvidia.com>\nCo-authored-by: Chen Cui <chcui@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Marc Romeyn <marcromeyn@gmail.com>\nCo-authored-by: jasonwan <jasonwan@nvidia.com>\nCo-authored-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\nCo-authored-by: Adi Renduchintala <adithyare@nvidia.com>\nCo-authored-by: Yuanzhe Dong <yudong@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* fix a typo (#7496)\n\nSigned-off-by: BestJuly <chntaoli@163.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [TTS] remove curly braces from ${BRANCH} in jupyer notebook cell. (#7554) (#7560)\n\n* remove curly braces.\n* remove installation of pynini.\n---------\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* add youtube embed url (#7570)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Remap speakers to continuous range of speaker_id for dataset AISHELL3 (#7536)\n\n* Remap speakers to continuous range of speaker_id for dataset AISHELL3\n* Add new key/value pair to record raw speaker for AISHELL3 dataset\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* fix validation_step_outputs initialization for multi-dataloader (#7546) (#7572)\n\n* added correct validation_step_outputs initialization for mutli-dataloader\n\n\n\n* changed kernel for display\n\n\n\n* Update logic for validation and test step outputs\n\n\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* revert multidataloader changes in multilang ASR notebook\n\n\n\n---------\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Kunal Dhawan <kunaldhawan97@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Append output of val step to self.validation_step_outputs (#7530) (#7532)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [TTS] fixed trainer's accelerator and strategy. (#7569) (#7574)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Append val/test output to instance variable in EncDecSpeakerLabelModel (#7562) (#7573)\n\n* Append val/test output to the instance variable in EncDecSpeakerLabelModel\n\n\n\n* Handle test case in evaluation_step\n\n\n\n* Replace type with isinstance\n\n\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix CustomProgressBar for resume (#7427) (#7522)\n\n* Fix CustomProgress Bar for resume and multiple epochs\n\n\n\n* Edit num_training_batches\n\n\n\n* Use max_steps as total for progress bar for resume\n\n\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* fix typos in nfa and speech enhancement tutorials (#7580) (#7583)\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\nCo-authored-by: Elena Rastorgueva <80532067+erastorgueva-nv@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Add strategy as ddp_find_unused_parameters_true for glue_benchmark.py (#7454) (#7461)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* update strategy (#7577) (#7578)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix typos (#7581)\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Change hifigan finetune strategy to ddp_find_unused_parameters_true (#7579) (#7584)\n\n* Change strategy to auto\n\n\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [BugFix] Add missing quotes for auto strategy in tutorial notebooks (#7541) (#7548)\n\n* Add missing quotes for auto strategy\n\n\n\n* Revert trainer.gpus to trainer.devices in Self_Supervised_Pre_Training.ipynb\n\n\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nSigned-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* add build os key (#7596) (#7599)\n\n* add build os key\n\n\n\n* add tools\n\n\n\n* update to stable version\n\n\n\n---------\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* StarCoder SFT test + bump PyT NGC image to 23.09 (#7540)\n\n* Add SFT StarCoder test\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Remove _modify_config call as it is covered in load_from_nemo just below\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Test with pyt:23.09 container\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n---------\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* defaults changed (#7600)\n\n* defaults changed\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* typo\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* update\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n---------\n\nSigned-off-by: arendu <adithyare@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* add ItalianPhonemesTokenizer (#7587)\n\n* add ItalianPhonemesTokenizer\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix Italian phonemes\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add test\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n---------\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* best ckpt fix (#7564) (#7588)\n\nSigned-off-by: dimapihtar <dpihtar@gmail.com>\nCo-authored-by: Dmytro Pykhtar <37850217+dimapihtar@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Add files via upload (#7598)\n\nspecifies the branch\n\nSigned-off-by: George <37293288+Jorjeous@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix validation in G2PModel and ThutmoseTaggerModel (#7597) (#7606)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain (#7576) (#7586)\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Safeguard nemo_text_processing installation on ARM (#7485)\n\n* safeguard nemo_text_processing installing\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update check\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n---------\n\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Bound transformers version in requirements (#7620)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* fix llama2 70b lora tuning bug (#7622)\n\n* fix llama2 70b lora tuning bug\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* Update peft_config.py\n\nbrackets\n\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\n\n---------\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\nCo-authored-by: Adi Renduchintala <adithyare@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix import error no module name model_utils (#7629)\n\nSigned-off-by: Mehadi Hasan Menon <mehadihasan80@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* add fc large ls models (#7641)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao Koluguri <nithinraok>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* bugfix: trainer.gpus, trainer.strategy, trainer.accelerator (#7621) (#7642)\n\n* [TTS] bugfix for Tacotron2 tutorial due to PTL 2.0\n* trainer.gpus -> trainer.devices\n* fixed related tutorial bugs\n---------\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* fix ssl models ptl monitor val through logging (#7608) (#7614)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix metrics for SE tutorial (#7604) (#7612)\n\nSigned-off-by: Ante Jukic\u0301 <ajukic@nvidia.com>\nCo-authored-by: anteju <108555623+anteju@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Add ddp_find_unused_parameters=True and change accelerator to auto (#7623) (#7644)\n\n* Add ddp_find_unused_parameters=True and change acclerator to auto\n\n\n\n* Add ddp_find_unused_parameters True for normalization_as_tagging_train.py\n\n\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix py3.11 dataclasses issue  (#7616)\n\n* Fix py3.11 dataclasses issue  (#7582)\n\n* Update ASR configs to support Python 3.11\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Update TTS configs to support Python 3.11\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Guard MeCab and Ipadic\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix remaining ASR dataclasses\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Fix remaining ASR dataclasses\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Fix scripts\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Update name to ConfidenceMethodConfig\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain (#7576) (#7586)\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Safeguard nemo_text_processing installation on ARM (#7485)\n\n* safeguard nemo_text_processing installing\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update check\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n---------\n\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Fix changes to confidence measure\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Jason <jasoli@nvidia.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix issues with Dockerfile (#7650) (#7652)\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [ASR] RNN-T greedy decoding max_frames fix for alignment and confidence (#7635)\n\n* decoding and test fix\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [ASR] Fix type error in jasper (#7636) (#7653)\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nCo-authored-by: Ryan Langman <rlangman@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [TTS] Add STFT and SI-SDR loss to audio codec recipe (#7468)\n\n* [TTS] Add STFT and SI-SDR loss to audio codec recipe\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Fix STFT resolution\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Fix training metric logging\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Add docstring to mel and stft losses\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* add outline of asr quickstart info to asr/intro.rst\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* add CLI, LM and real-time transcription sections\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Create per.py (#7538)\n\n* Move model precision copy (#7336)\n\n* move cfg precision set to megatron base model\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* remove copy from other models\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* modify attribute not arg\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* fix gpt model test for ptl 2.0\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* rename function and add docstring\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* replace precision to dtype conditionals with func call\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* unnecessary function and cfg reset\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* set default value\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* fix precision lookup in a few more places\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* rename mapping function\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* ununsed import\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* save torch datatype to model\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* set weights precision wrt amp o2\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* Revert \"set weights precision wrt amp o2\"\n\nThis reverts commit 313a4bfe5eb69d771a6d2433898c0685836aef5c.\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* revert half precision at inference attempt\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* move autocast dtype to base model\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* move params dtype to base model, enable fp16 O2 inf\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* unused imports\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n---------\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix PEFT checkpoint loading (#7388)\n\n* Fix PEFT checkpoint loading\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Use distributed optimizer support for multiple dtypes (#7359)\n\n* Update distopt wrapper with multiple dtype support\n\nRemove manual handling of separate FP32 optimizer.\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Use distopt support for contiguous buffers with multiple dtypes\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Fix typo\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Separate distopt buckets for first GPT layer and non-overlapped params\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Add distopt logic for int dtypes\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Update Apex commit\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Remove unused variables\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Update Apex commit in README and Jenkensfile\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Debug Dockerfile and Jenkinsfile\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n---------\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* minor fix for llama ckpt conversion script (#7387)\n\n* minor fix for llama ckpt conversion script\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* Update Jenkinsfile\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* remove fast_swiglu configuration\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n---------\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix wrong calling of librosa.get_duration() in notebook (#7376)\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [PATCH] PEFT import mcore (#7393)\n\n* [PATCH] PEFT import mcore\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Create per.py\n\nScript for calculation Punctuation Error Rate and related rates (correct rate, deletions rate, etc.)\n\nSigned-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [TTS] Added a callback for logging initial data (#7384)\n\nSigned-off-by: Ante Jukic\u0301 <ajukic@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update Core Commit (#7402)\n\n* Update Core Commit\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n* update commit\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n---------\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Use cfg attribute in bert (#7394)\n\n* use cfg attribute instead of arg\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* use torch_dtype in place of cfg.precision\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* move precision copy before super constructor\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* use trainer arg\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n---------\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add support for bias conversion in Swiglu models (#7386)\n\n* Add support for bias conversion in Swiglu models\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Add support for auto extracting tokenizer model\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add support for auto extracting tokenizer model\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Fix issue with missing tokenizer\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Refactor\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Refactor\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update save_to and restore_from for dist checkpointing (#7343)\n\n* add dist ckpt to save to, in progress\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* move dist ckpt\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* clean up\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update restore from, need to figure out how to initialize distributed\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* launch distrib if needed when restoring dist ckpt\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* when using mcore we can change tp pp on the fly\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* add load_from_checkpoint support for dist ckpt\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update llama convert script to save dist .nemo\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* fix load dist ckpt\n\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* setup TE TP groups if needed\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* setup te tp groups if needed\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* remove import\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n---------\n\nSigned-off-by: eharper <eharper@nvidia.com>\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: jasonwan <jasonwan@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* fix forward for with mcore=false (#7403)\n\nSigned-off-by: Jimmy Zhang <jiemingz@nvidia.com>\nCo-authored-by: Jimmy Zhang <jiemingz@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix logging to remove 's/it' from progress bar in Megatron models and add train_step_timing (#7374)\n\n* Add CustomProgressBar class to exp_manager and trainer callbacks\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix the progress bar to reflect total microbatch cnt\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* Modify CustomProgressBar class\n\n1) Modify CustomProgressBar class to update progress bar per global_step instead of per microbatch\n2) Add the callback to other megatron training/finetuning files that are not using MegatronTrainerBuilder\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* Add CustomProgressBar callback to tuning files\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Set Activation Checkpointing Defaults (#7404)\n\n* Set Activation Checkpointing Defaults\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* check for None\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* make loss mask default to false (#7407)\n\nSigned-off-by: eharper <eharper@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add dummy userbuffer config files (#7408)\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* add missing ubconf files (#7412)\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* New tutorial on Speech Data Explorer (#7405)\n\n* Added Google Colab based tutorial on Speech Data Explorer\n\nSigned-off-by: George Zelenfroynd <gzelenfroind@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update ptl training ckpt conversion script to work with dist ckpt (#7416)\n\n* update ptl convert script\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* don't break legacy\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: eharper <eharper@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Allow disabling sanity checking when num_sanity_val_steps=0 (#7413)\n\n* Allow disabling sanity checking when num_sanity_val_steps=0\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* Update num_sanity_val_steps to be a multiple of num_microbatches\n\nSigned-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nSigned-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add comprehensive error messages (#7261)\n\nSigned-off-by: Anton Peganov <apeganov@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* check NEMO_PATH (#7418)\n\nSigned-off-by: Nikolay Karpov <karpnv@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* layer selection for ia3 (#7417)\n\n* layer selection for ia3\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: arendu <adithyare@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix missing pip package 'einops' (#7397)\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix failure of pyaudio in Google Colab (#7396)\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update README.md: output_path --> output_manifest_filepath (#7442)\n\nSigned-off-by: Samuele Cornell <cornellsamuele@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add rope dynamic linear scaling (#7437)\n\n* Add dynamic linear scaling\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix None dataloader issue in PTL2.0 (#7455)\n\n* Fix None dataloader issue in PTL2.0\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* updating values of self._validation_dl and self._test_dl as well\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* updating values of self._validation_dl and self._test_dl as well\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [ASR] Confidence measure -> method renames (#7434)\n\n* measure -> method\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add steps for document of getting dataset 'SF Bilingual Speech' (#7378)\n\n* Add steps for document of getting dataset 'SF Bilingual Speech'\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update datasets.rst\n\nadded a link from a tutorial demonstrating detailed data prep steps.\n\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* RNN-T confidence and alignment bugfix (#7381)\n\n* new frame_confidence and alignments lists are now always created after the while loop\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* tests added\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix resume from checkpoint in exp_manager (#7424) (#7426)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix checking of cuda/cpu device for inputs of Decoder (#7444)\n\n* Fix checking of cuda/cpu device for inputs of Decoder\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update tacotron2.py\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: Jason <jasoli@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix failure of ljspeech's get_data.py (#7430)\n\n* Fix failure of ljspeech's get_data.py\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [TTS] Fix audio codec type checks (#7373)\n\n* [TTS] Fix audio codec type checks\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Fix audio codec tests\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [\u2026"
    },
    {
        "repo_url": "github.com/NVIDIA/NeMo",
        "filepath": "nemo/collections/multimodal/modules/stable_diffusion/encoders/modules.py",
        "commit_date": "2024-01-11T06:32:19Z",
        "message": "Add All Multimodal Source Code Part 2: Text to image, x to nerf (#7970)\n\n* Update README.md: output_path --> output_manifest_filepath (#7442)\n\nSigned-off-by: Samuele Cornell <cornellsamuele@gmail.com>\n\n* Updating FlashAttention API to match FlashAttentionV2\n\n* Multiple fixes for mm\n\n* Fix CI inductor issue and update to torch compile\n\n* Remove suppress error\n\n* Fix when conversion config uses fp16 and it complains about precision plugin\n\n* Fixing FAv2 API usage\n\n* Initial release of content filtering model\n\n* Added synthetic dataloader for precached and online mode\n\n* Mingyuanm/dreambooth opt\n\n* Add llama2 support in neva training\n\n* Fix sampler length\n\n* Fix all precision issues in nemo multimodal\n\n* Add rope dynamic linear scaling (#7437)\n\n* Add dynamic linear scaling\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>\n\n* Fix None dataloader issue in PTL2.0 (#7455)\n\n* Fix None dataloader issue in PTL2.0\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* updating values of self._validation_dl and self._test_dl as well\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* updating values of self._validation_dl and self._test_dl as well\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* [ASR] Confidence measure -> method renames (#7434)\n\n* measure -> method\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Add steps for document of getting dataset 'SF Bilingual Speech' (#7378)\n\n* Add steps for document of getting dataset 'SF Bilingual Speech'\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update datasets.rst\n\nadded a link from a tutorial demonstrating detailed data prep steps.\n\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* RNN-T confidence and alignment bugfix (#7381)\n\n* new frame_confidence and alignments lists are now always created after the while loop\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* tests added\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* Fix resume from checkpoint in exp_manager (#7424) (#7426)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\n\n* Fix checking of cuda/cpu device for inputs of Decoder (#7444)\n\n* Fix checking of cuda/cpu device for inputs of Decoder\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update tacotron2.py\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: Jason <jasoli@nvidia.com>\n\n* Fix failure of ljspeech's get_data.py (#7430)\n\n* Fix failure of ljspeech's get_data.py\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* [TTS] Fix audio codec type checks (#7373)\n\n* [TTS] Fix audio codec type checks\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Fix audio codec tests\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Add dataset to path of logged artifacts (#7462)\n\n* [TTS] Add dataset to path of logged artifacts\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Revert axis name back to Audio Frames\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* Fix sft dataset truncation (#7464)\n\n* Add fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Automatic Lip Reading Recognition (ALR) - ASR/CV (Visual ASR) (#7330)\n\n* striding_conv1d_k5 and dw_striding_conv1d_k5 subsampling\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* transpose conv1d inputs\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* Update subsampling.py\n\nchange striding_conv1d_k5 to striding_conv1d\n\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\n\n* cv branch\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* video manifest\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* add collection classes\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add test_step_outputs\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* correct manifest bug when having only audio or only videos\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* correct manifest bug when having only audio or only videos\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* clean references\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* freeze unfreeze transcribe cv models\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* correct manifest get_full_path bug\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* update for PR\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* guard torchvision\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update nemo/collections/cv/data/video_to_text_dataset.py\n\nCo-authored-by: Igor Gitman <igor.a.gitman@gmail.com>\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\n\n* _video_speech_collate_fn in cv/data/video_to_text.py\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* add self.out = None to asr subsampling\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* Update nemo/collections/cv/data/video_to_text_dataset.py\n\nCo-authored-by: Igor Gitman <igor.a.gitman@gmail.com>\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\n\n* cv -> multimodal/speech_cv branch\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Igor Gitman <igor.a.gitman@gmail.com>\n\n* HF StarCoder to NeMo conversion script (#7421)\n\n* Script to convert HF StarCoder checkpoint to NeMo\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* StarCoder conversion test\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Fix test\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Catch up with save_to changes\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Don't abbreviate args for clarity\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Configurable precision: BF16 vs FP32\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* fix bug when loading dist ckpt in peft (#7452)\n\nSigned-off-by: Hongbin Liu <hongbinl@nvidia.com>\nCo-authored-by: Hongbin Liu <hongbinl@nvidia.com>\n\n* Fix adding positional embeddings in-place in transformer module (#7440)\n\nSigned-off-by: Tamerlan Tabolov <tktabolov@gmail.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\n* Fix (#7478)\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* add sleep (#7498) (#7499)\n\n* add sleep\n\n\n\n* add sleep onto config instead\n\n\n\n* add comment\n\n\n\n---------\n\nSigned-off-by: Gerald Shen <geshen@nvidia.com>\nCo-authored-by: Gerald Shen <119401249+gshennvm@users.noreply.github.com>\n\n* Fix exp manager check for sleep (#7503) (#7504)\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\n\n* bugfix: trainer.accelerator=auto from None. (#7492) (#7493)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\n\n* [doc] fix broken link (#7481)\n\nSigned-off-by: Stas Bekman <stas00@users.noreply.github.com>\n\n* [TTS] Read audio as int32 to avoid flac read errors (#7477)\n\n* [TTS] Read audio as int32 to avoid flac read errors\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Add comment about read failures\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* Add dataset 'AISHELL-3' from OpenSLR for training mandarin TTS (#7409)\n\n* Add dataset 'AISHELL-3' from OpenSLR for training mandarin TTS\n* Train 'AISHELL-3' dataset with multi-speakers\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update get_data.py\n\nupdate copyright header\n\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* Update get_data.py\n\nadded a disclaimer\n\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add new configuration file for AISHELL3 with multispeaker of fastpitch\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* dllogger - log on rank 0 only (#7513)\n\nSigned-off-by: Stas Bekman <stas00@users.noreply.github.com>\n\n* Fix TTS FastPitch tutorial (#7494) (#7516)\n\n* Fix\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\n* Fix get_dist() tensor dimension (#7506) (#7515)\n\nSigned-off-by: Jocelyn Huang <jocelynh@nvidia.com>\nCo-authored-by: Jocelyn <jocelynh@nvidia.com>\n\n* bugfix: specify trainer.strategy=auto when devices=1 (#7509) (#7512)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\n\n* fix (#7511)\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n* [TTS] Fix FastPitch data prep tutorial (#7524)\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* add italian tokenization (#7486)\n\n* add italian tokenization\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add more ipa lexicon it\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix error deletion\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* add test\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Replace None strategy with auto in tutorial notebooks (#7521) (#7527)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* unpin setuptools (#7534) (#7535)\n\nSigned-off-by: fayejf <36722593+fayejf@users.noreply.github.com>\nCo-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>\n\n* remove auto generated examples (#7510)\n\n* explicitly remove autogenerated examples for data parallel evaluation\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* mark autogenrated and remove it for test\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: arendu <adithyare@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Add the `strategy` argument to `MegatronGPTModel.generate()` (#7264)\n\nIt is passed as an explicit argument rather than through\n`**strategy_args` so as to ensure someone cannot accidentally pass other\narguments that would end up being ignored.\n\nIt is a keyword-only argument to ensure that if in the future we want to\nupdate the signature to `**strategy_args`, we can do it without breaking\ncode.\n\nSigned-off-by: Olivier Delalleau <507137+odelalleau@users.noreply.github.com>\n\n* Fix PTL2.0 related ASR bugs in r1.21.0: Val metrics logging, None dataloader issue (#7531) (#7533)\n\n* fix none dataloader issue ptl2\n\n\n\n* ptl2.0 logging fixes for rnnt_models\n\n\n\n---------\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\nCo-authored-by: Kunal Dhawan <kunaldhawan97@gmail.com>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\n\n* gpus -> devices (#7542) (#7545)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\n\n* Update FFMPEG version to fix issue with torchaudio (#7551) (#7553)\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\n\n* PEFT GPT & T5 Refactor (#7308)\n\n* initial implementation of add_adapters API\n\n* correct type hint\n\n* Add config in add_adapters for save and load (@author bobchen)\n\n* Remove AdapterConfig to avoid import error\n\n* Add AdaterConfig back and move adaptermixin to sft model\n\n* Add NLPSaveRestoreConnector as default in NLPModel.restore_from\n\n* Add restore_from_nemo_with_adapter and test script\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* rename t5 file and classes to be consistent with GPT\n\n* add t5 sft dataset\n\n* add support for single-file format with T5SFTDataset\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Various small changes to make T5 SFT work like GPT SFT\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add adapter evaluation test script\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add MultiAdaterConfig for ia3 and fix builder issue\n\n* Make ptuning for T5SFTModel work using mixin\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add IA3_Adapter for AdapterName\n\n* Add adapter name for ptuning and attention adapter\n\n* Make test script GPT/T5 agnostic\n\n* Add layer selection feature\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Integrate adapter name and config\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update gpt peft tuning script to new API\n\n* add t5 peft tuning script with new API\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix IA3 layer selection issue\n\n* Override state_dict on SFT model instead of mixin\n\n* Add load adapter by adapter config\n\n* move peft config map away from example script\n\n* auto get config from nemo adapter\n\n* Move PEFTConfig to new file\n\n* fix ckpt save/load for t5\n\n* name change: add_adapters -> add_adapter\n\n* variable name change\n\n* update t5 script\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix t5 issues\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add weight tying\n\n* update gpt tuning script\n\n* PEFT-API proposal\n\n* Fix according to comments\n\n* update tuning scripts\n\n* move merge_cfg_with to mixin class since it applies to both gpt and t5 and requires the model class for restore\n\n* Add mcore_gpt support for NLPAdapterMixin\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix typo\n\n* variable name change to distinguish \"peft\" and \"adapter\"\n\n* override `load_adapters` to support `add_adapter` name change\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update tuning and eval script for adapter save/load\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add Ptuning on first stage only\n\n* add lora tutorial for review\n\n* Fix layer selection for mcore\n\n* add landing page\n\n* fix resume training\n\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add mcore condition in sharded_state_dict to make sft work\n\n* Update lora_tutorial.md\n\nFirst edit of this file for PEFT documentation for NeMO\n\nSigned-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\n\n* rename Adapter to AttentionAdapter to avoid confusion in doc\n\n* Change load_adapters to load .nemo\n\n* add quick start guide\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add load_adapters with .ckpt\n\n* Remove setup_complete changes in load_adapters\n\n* update landing page\n\n* remove typo\n\n* Updated quick_start.md per Chen Cui\n\nSigned-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\n\n* Add inference config merger and tutorial\n\n* Add doc string for NLPAdapterModelMixin and deprecated warning on MegatronGPTPEFTModel\n\n* add supported_methods.md and update other documentations\n\n* Update supported_methods.md\n\nminor updates.\n\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\n\n* Update landing_page.md\n\nminor update.\n\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\n\n* Modify doc string for NLPAdapterModelMixin\n\n* Add doc string add_adapters in NLPAdapterModelMixin\n\n* rename canonical adapters\n\n* remove mcore hard dependency\n\n* [PATCH] move microbatch calculator to nemo from apex\n\n* remove apex dependency in gpt and t5 sft models\n\n* remove apex dependency in gpt model\n\n* render doc strings\n\n* fix\n\n* Add missing virtual_tokens on ptuning\n\n* fix docstrings\n\n* update gpt-style model coverage in docs\n\n* update docstring\n\n* Remove pdb\n\n* add lightning_fabric to make docstring rendering work\n\n* Add Ptuning missing key\n\n* try docstring rendering\n\n* Fix ptuning issue\n\n* update gpt t5 peft tuning and eval scripts\n\n* typos\n\n* update eval config\n\n* fix bug relating to apex dependency removal\n\n* typo\n\n* make predict step behave the same as test step\n\n* make lora tutorial work in notebook\n\n* cosmetics\n\n* update yaml scripts\n\n* mcore_gpt attribute optional\n\n* typo\n\n* update eval scripts and fix T5 eval bugs\n\n* add NLPDDPStrategyNotebook and trainer builder logic to use it\n\n* update lora notebook to use new trainer builder\n\n* fix microbatch calculator bug for inference after training\n\n* Convert markdown files to RST and incorporate with doc\n\n* typo\n\n* revise language\n\n* remove extra cell\n\n* remove unnecessary inheritance\n\n* remove old tests\n\n* move layer selection default so logging messages make sense\n\n* remove `save_adapters` as adapter weights are saved automatically during training\n\n* initialize weights from a checkpoint instead of randomly\n\n* multiple fields can form a context (#7147)\n\n* list of context fields and flexible prompt template\n\nSigned-off-by: arendu <adithya.r@gmail.com>\n\n* list of fields for context\n\nSigned-off-by: arendu <adithya.r@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Add multiple truncation fields and middle truncation\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Compatible to old ckpt\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix tokenize detokenize issue\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Remove detokenization, add truncation augmentation\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Resolve comments\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Remove unused import\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* revert eos\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Add tokenizer space_sensitive attribute\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix error\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Fix erorr and use re\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Change assert logic\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Follow adi suggestion\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Remove merge function\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add example and comment\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Remove context_key and add comment\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Remove random truncation\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix template none\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-off-by: arendu <adithya.r@gmail.com>\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nSigned-off-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\n* revert config changes\n\n* remove accidental breakpoint\n\n* support TP>1 loading\n\n* infer adapter type from checkpoint in during eval\n\n* breakup add adapter\n\n* enable interpolation of train_ds and validation_ds\n\n* update metric calc script to conform to single-file eval format\n\n* remove extraneous print\n\n* update lora notebook for updated merge_inference_cfg\n\n* Update nlp_adapter_mixins.py\n\nvariable name change\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* turn off grad scaler for PP to match old scripts\n\n* remove PEFTSaveRestoreConnector since functionality all covered by the new mixin class\n\n* remove resume_from_checkpoint check since covered in #7335\n\n* revert changes made in eval config interpolation\n\n* more interpolation\n\n* typo\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove dup line\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* code style warnings\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix config mistake\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* add copyright header\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* fix code check warnings\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* revert changes to remove apex dependency (mixed apex+nemo microbatch calculator broke some CI tests)\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add more deprecation notices\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* update deprecation notices\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* update deprecation notices\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* consolidate peft and sft scripts\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* update CI tests\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* notebook branch points to main to prepare for merge\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* fix gpt and t5 validation with any metric other than loss\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* support pre-extracted checkpoints\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n---------\n\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\nSigned-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\nSigned-off-by: arendu <adithya.r@gmail.com>\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nSigned-off-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\nSigned-off-by: Chen Cui <chcui@nvidia.com>\nCo-authored-by: Chen Cui <chcui@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Marc Romeyn <marcromeyn@gmail.com>\nCo-authored-by: jasonwan <jasonwan@nvidia.com>\nCo-authored-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\nCo-authored-by: Adi Renduchintala <adithyare@nvidia.com>\nCo-authored-by: Yuanzhe Dong <yudong@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\n* fix a typo (#7496)\n\nSigned-off-by: BestJuly <chntaoli@163.com>\n\n* [TTS] remove curly braces from ${BRANCH} in jupyer notebook cell. (#7554) (#7560)\n\n* remove curly braces.\n* remove installation of pynini.\n---------\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\n\n* add youtube embed url (#7570)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\n\n* Remap speakers to continuous range of speaker_id for dataset AISHELL3 (#7536)\n\n* Remap speakers to continuous range of speaker_id for dataset AISHELL3\n* Add new key/value pair to record raw speaker for AISHELL3 dataset\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* fix validation_step_outputs initialization for multi-dataloader (#7546) (#7572)\n\n* added correct validation_step_outputs initialization for mutli-dataloader\n\n\n\n* changed kernel for display\n\n\n\n* Update logic for validation and test step outputs\n\n\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* revert multidataloader changes in multilang ASR notebook\n\n\n\n---------\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Kunal Dhawan <kunaldhawan97@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Append output of val step to self.validation_step_outputs (#7530) (#7532)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* [TTS] fixed trainer's accelerator and strategy. (#7569) (#7574)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\n\n* Append val/test output to instance variable in EncDecSpeakerLabelModel (#7562) (#7573)\n\n* Append val/test output to the instance variable in EncDecSpeakerLabelModel\n\n\n\n* Handle test case in evaluation_step\n\n\n\n* Replace type with isinstance\n\n\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* Fix CustomProgressBar for resume (#7427) (#7522)\n\n* Fix CustomProgress Bar for resume and multiple epochs\n\n\n\n* Edit num_training_batches\n\n\n\n* Use max_steps as total for progress bar for resume\n\n\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* fix typos in nfa and speech enhancement tutorials (#7580) (#7583)\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\nCo-authored-by: Elena Rastorgueva <80532067+erastorgueva-nv@users.noreply.github.com>\n\n* Add strategy as ddp_find_unused_parameters_true for glue_benchmark.py (#7454) (#7461)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* update strategy (#7577) (#7578)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\n\n* Fix typos (#7581)\n\n* Change hifigan finetune strategy to ddp_find_unused_parameters_true (#7579) (#7584)\n\n* Change strategy to auto\n\n\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\n* [BugFix] Add missing quotes for auto strategy in tutorial notebooks (#7541) (#7548)\n\n* Add missing quotes for auto strategy\n\n\n\n* Revert trainer.gpus to trainer.devices in Self_Supervised_Pre_Training.ipynb\n\n\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nSigned-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* add build os key (#7596) (#7599)\n\n* add build os key\n\n\n\n* add tools\n\n\n\n* update to stable version\n\n\n\n---------\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\n\n* StarCoder SFT test + bump PyT NGC image to 23.09 (#7540)\n\n* Add SFT StarCoder test\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Remove _modify_config call as it is covered in load_from_nemo just below\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Test with pyt:23.09 container\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n---------\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* defaults changed (#7600)\n\n* defaults changed\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* typo\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* update\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n---------\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* add ItalianPhonemesTokenizer (#7587)\n\n* add ItalianPhonemesTokenizer\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix Italian phonemes\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add test\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n---------\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* best ckpt fix (#7564) (#7588)\n\nSigned-off-by: dimapihtar <dpihtar@gmail.com>\nCo-authored-by: Dmytro Pykhtar <37850217+dimapihtar@users.noreply.github.com>\n\n* Add files via upload (#7598)\n\nspecifies the branch\n\nSigned-off-by: George <37293288+Jorjeous@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* Fix validation in G2PModel and ThutmoseTaggerModel (#7597) (#7606)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain (#7576) (#7586)\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Safeguard nemo_text_processing installation on ARM (#7485)\n\n* safeguard nemo_text_processing installing\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update check\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n---------\n\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Bound transformers version in requirements (#7620)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* fix llama2 70b lora tuning bug (#7622)\n\n* fix llama2 70b lora tuning bug\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* Update peft_config.py\n\nbrackets\n\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\n\n---------\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\nCo-authored-by: Adi Renduchintala <adithyare@nvidia.com>\n\n* Fix import error no module name model_utils (#7629)\n\nSigned-off-by: Mehadi Hasan Menon <mehadihasan80@gmail.com>\n\n* add fc large ls models (#7641)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao Koluguri <nithinraok>\n\n* bugfix: trainer.gpus, trainer.strategy, trainer.accelerator (#7621) (#7642)\n\n* [TTS] bugfix for Tacotron2 tutorial due to PTL 2.0\n* trainer.gpus -> trainer.devices\n* fixed related tutorial bugs\n---------\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* fix ssl models ptl monitor val through logging (#7608) (#7614)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* Fix metrics for SE tutorial (#7604) (#7612)\n\nSigned-off-by: Ante Jukic\u0301 <ajukic@nvidia.com>\nCo-authored-by: anteju <108555623+anteju@users.noreply.github.com>\n\n* Add ddp_find_unused_parameters=True and change accelerator to auto (#7623) (#7644)\n\n* Add ddp_find_unused_parameters=True and change acclerator to auto\n\n\n\n* Add ddp_find_unused_parameters True for normalization_as_tagging_train.py\n\n\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* Fix py3.11 dataclasses issue  (#7616)\n\n* Fix py3.11 dataclasses issue  (#7582)\n\n* Update ASR configs to support Python 3.11\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Update TTS configs to support Python 3.11\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Guard MeCab and Ipadic\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix remaining ASR dataclasses\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Fix remaining ASR dataclasses\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Fix scripts\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Update name to ConfidenceMethodConfig\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain (#7576) (#7586)\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Safeguard nemo_text_processing installation on ARM (#7485)\n\n* safeguard nemo_text_processing installing\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update check\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n---------\n\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Fix changes to confidence measure\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Jason <jasoli@nvidia.com>\n\n* [Stable Diffusion/ControlNet] Enable O2 training for SD and Fix ControlNet CI failure\n\n* Mingyuanm/dreambooth fix\n\n* Fix NeMo CI Infer Issue\n\n* DreamFusion\n\n* Move neva export changes\n\n* Add Imagen Synthetic Dataloader\n\n* Add VITWrapper and export stuff to wrapper\n\n* Update neva with megatron-core support\n\n* Fix issues with Dockerfile (#7650) (#7652)\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\n\n* [ASR] RNN-T greedy decoding max_frames fix for alignment and confidence (#7635)\n\n* decoding and test fix\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* [ASR] Fix type error in jasper (#7636) (#7653)\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nCo-authored-by: Ryan Langman <rlangman@nvidia.com>\n\n* [TTS] Add STFT and SI-SDR loss to audio codec recipe (#7468)\n\n* [TTS] Add STFT and SI-SDR loss to audio codec recipe\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Fix STFT resolution\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Fix training metric logging\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Add docstring to mel and stft losses\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Create per.py (#7538)\n\n* Move model precision copy (#7336)\n\n* move cfg precision set to megatron base model\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* remove copy from other models\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* modify attribute not arg\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* fix gpt model test for ptl 2.0\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* rename function and add docstring\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* replace precision to dtype conditionals with func call\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* unnecessary function and cfg reset\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* set default value\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* fix precision lookup in a few more places\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* rename mapping function\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* ununsed import\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* save torch datatype to model\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* set weights precision wrt amp o2\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* Revert \"set weights precision wrt amp o2\"\n\nThis reverts commit 313a4bfe5eb69d771a6d2433898c0685836aef5c.\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* revert half precision at inference attempt\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* move autocast dtype to base model\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* move params dtype to base model, enable fp16 O2 inf\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* unused imports\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n---------\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix PEFT checkpoint loading (#7388)\n\n* Fix PEFT checkpoint loading\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Use distributed optimizer support for multiple dtypes (#7359)\n\n* Update distopt wrapper with multiple dtype support\n\nRemove manual handling of separate FP32 optimizer.\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Use distopt support for contiguous buffers with multiple dtypes\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Fix typo\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Separate distopt buckets for first GPT layer and non-overlapped params\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Add distopt logic for int dtypes\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Update Apex commit\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Remove unused variables\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Update Apex commit in README and Jenkensfile\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Debug Dockerfile and Jenkinsfile\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n---------\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* minor fix for llama ckpt conversion script (#7387)\n\n* minor fix for llama ckpt conversion script\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* Update Jenkinsfile\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* remove fast_swiglu configuration\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n---------\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix wrong calling of librosa.get_duration() in notebook (#7376)\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [PATCH] PEFT import mcore (#7393)\n\n* [PATCH] PEFT import mcore\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Create per.py\n\nScript for calculation Punctuation Error Rate and related rates (correct rate, deletions rate, etc.)\n\nSigned-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [TTS] Added a callback for logging initial data (#7384)\n\nSigned-off-by: Ante Jukic\u0301 <ajukic@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update Core Commit (#7402)\n\n* Update Core Commit\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n* update commit\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n---------\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Use cfg attribute in bert (#7394)\n\n* use cfg attribute instead of arg\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* use torch_dtype in place of cfg.precision\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* move precision copy before super constructor\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* use trainer arg\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n---------\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add support for bias conversion in Swiglu models (#7386)\n\n* Add support for bias conversion in Swiglu models\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Add support for auto extracting tokenizer model\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add support for auto extracting tokenizer model\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Fix issue with missing tokenizer\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Refactor\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Refactor\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update save_to and restore_from for dist checkpointing (#7343)\n\n* add dist ckpt to save to, in progress\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* move dist ckpt\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* clean up\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update restore from, need to figure out how to initialize distributed\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* launch distrib if needed when restoring dist ckpt\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* when using mcore we can change tp pp on the fly\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* add load_from_checkpoint support for dist ckpt\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update llama convert script to save dist .nemo\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* fix load dist ckpt\n\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* setup TE TP groups if needed\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* setup te tp groups if needed\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* remove import\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n---------\n\nSigned-off-by: eharper <eharper@nvidia.com>\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: jasonwan <jasonwan@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* fix forward for with mcore=false (#7403)\n\nSigned-off-by: Jimmy Zhang <jiemingz@nvidia.com>\nCo-authored-by: Jimmy Zhang <jiemingz@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix logging to remove 's/it' from progress bar in Megatron models and add train_step_timing (#7374)\n\n* Add CustomProgressBar class to exp_manager and trainer callbacks\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix the progress bar to reflect total microbatch cnt\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* Modify CustomProgressBar class\n\n1) Modify CustomProgressBar class to update progress bar per global_step instead of per microbatch\n2) Add the callback to other megatron training/finetuning files that are not using MegatronTrainerBuilder\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* Add CustomProgressBar callback to tuning files\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Set Activation Checkpointing Defaults (#7404)\n\n* Set Activation Checkpointing Defaults\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* check for None\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* make loss mask default to false (#7407)\n\nSigned-off-by: eharper <eharper@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add dummy userbuffer config files (#7408)\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* add missing ubconf files (#7412)\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* New tutorial on Speech Data Explorer (#7405)\n\n* Added Google Colab based tutorial on Speech Data Explorer\n\nSigned-off-by: George Zelenfroynd <gzelenfroind@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update ptl training ckpt conversion script to work with dist ckpt (#7416)\n\n* update ptl convert script\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* don't break legacy\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: eharper <eharper@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Allow disabling sanity checking when num_sanity_val_steps=0 (#7413)\n\n* Allow disabling sanity checking when num_sanity_val_steps=0\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* Update num_sanity_val_steps to be a multiple of num_microbatches\n\nSigned-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nSigned-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add comprehensive error messages (#7261)\n\nSigned-off-by: Anton Peganov <apeganov@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* check NEMO_PATH (#7418)\n\nSigned-off-by: Nikolay Karpov <karpnv@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* layer selection for ia3 (#7417)\n\n* layer selection for ia3\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: arendu <adithyare@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix missing pip package 'einops' (#7397)\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix failure of pyaudio in Google Colab (#7396)\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update README.md: output_path --> output_manifest_filepath (#7442)\n\nSigned-off-by: Samuele Cornell <cornellsamuele@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add rope dynamic linear scaling (#7437)\n\n* Add dynamic linear scaling\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix None dataloader issue in PTL2.0 (#7455)\n\n* Fix None dataloader issue in PTL2.0\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* updating values of self._validation_dl and self._test_dl as well\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* updating values of self._validation_dl and self._test_dl as well\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [ASR] Confidence measure -> method renames (#7434)\n\n* measure -> method\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add steps for document of getting dataset 'SF Bilingual Speech' (#7378)\n\n* Add steps for document of getting dataset 'SF Bilingual Speech'\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update datasets.rst\n\nadded a link from a tutorial demonstrating detailed data prep steps.\n\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* RNN-T confidence and alignment bugfix (#7381)\n\n* new frame_confidence and alignments lists are now always created after the while loop\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* tests added\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix resume from checkpoint in exp_manager (#7424) (#7426)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix checking of cuda/cpu device for inputs of Decoder (#7444)\n\n* Fix checking of cuda/cpu device for inputs of Decoder\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update tacotron2.py\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: Jason <jasoli@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix failure of ljspeech's get_data.py (#7430)\n\n* Fix failure of ljspeech's get_data.py\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [TTS] Fix audio codec type checks (#7373)\n\n* [TTS] Fix audio codec type checks\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Fix audio codec tests\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [TTS] Add dataset to path of logged artifacts (#7462)\n\n* [TTS] Add dataset to path of logged artifacts\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Revert axis name back to Audio Frames\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix sft dataset truncation (#7464)\n\n* Add fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-of\u2026"
    },
    {
        "repo_url": "github.com/modelscope/modelscope",
        "filepath": "modelscope/models/cv/anydoor/ldm/modules/encoders/modules.py",
        "commit_date": "2023-12-25T11:01:50Z",
        "message": "Add AnyDoor support (#688)\n\n* support anydoor\n\n* add dinov2\n\n* fix bug\n\n* convert rgb\n\n* update anydoor_pipeline and add docstr"
    },
    {
        "repo_url": "github.com/mlfoundations/open_flamingo",
        "filepath": "open_flamingo/eval/rices.py",
        "commit_date": "2023-08-02T19:54:36Z",
        "message": "Evaluation updates: add RICES + prompt ensembling (#220)\n\n* added rices for picking demos for captioning and vqa\n\n* add caching\n\n* script to cache RICES features; RICES DDP\n\n* RICES for Hatefulmemes\n\n* make RICES a command line argument\n\n* remove DDP\n\n* refactor classification\n\n* add prompt ensembling\n\n* enforce correct class ordering\n\n* refactor cached classification\n\n* fix classification caching\n\n* customize rices encoder\n\n* scripts for vqav2 test-dev json; code cleanup\n\n* move cache script to scripts/\n\n* add vizwiz testdev helper to script\n\n* fixed seeding in dist eval among other things\n\n* add vizwiz test questions to data\n\n* added rices instructions and fixed fill script\n\n* applied linter\n\n---------\n\nCo-authored-by: Anas Awadalla <anasa2@uw.edu>"
    },
    {
        "repo_url": "github.com/mlfoundations/open_flamingo",
        "filepath": "open_flamingo/eval/rices.py",
        "commit_date": "2023-07-01T16:51:14Z",
        "message": "Fix accident where I pushed RICES code to main (#212)"
    },
    {
        "repo_url": "github.com/mlfoundations/open_flamingo",
        "filepath": "open_flamingo/eval/rices.py",
        "commit_date": "2023-07-01T16:43:31Z",
        "message": "added rices for captioning and vqa"
    },
    {
        "repo_url": "github.com/lllyasviel/stable-diffusion-webui-forge",
        "filepath": "modules/sd_disable_initialization.py",
        "commit_date": "2023-12-06T00:00:48Z",
        "message": "change state dict comparison to ref compare"
    },
    {
        "repo_url": "github.com/lllyasviel/stable-diffusion-webui-forge",
        "filepath": "modules/sd_disable_initialization.py",
        "commit_date": "2023-08-16T09:11:01Z",
        "message": "send weights to target device instead of CPU memory"
    },
    {
        "repo_url": "github.com/lllyasviel/stable-diffusion-webui-forge",
        "filepath": "modules/sd_disable_initialization.py",
        "commit_date": "2023-07-24T19:08:08Z",
        "message": "Use less RAM when creating models"
    },
    {
        "repo_url": "github.com/lllyasviel/stable-diffusion-webui-forge",
        "filepath": "modules/sd_disable_initialization.py",
        "commit_date": "2023-02-05T08:21:00Z",
        "message": "make it possible to load SD1 checkpoints without CLIP"
    },
    {
        "repo_url": "github.com/lllyasviel/stable-diffusion-webui-forge",
        "filepath": "modules/sd_disable_initialization.py",
        "commit_date": "2023-01-22T05:20:48Z",
        "message": "fix missing field for aesthetic embedding extension"
    },
    {
        "repo_url": "github.com/lllyasviel/stable-diffusion-webui-forge",
        "filepath": "modules/sd_disable_initialization.py",
        "commit_date": "2023-01-11T15:54:13Z",
        "message": "fix for an error caused by skipping initialization, for realsies this time: TypeError: expected str, bytes or os.PathLike object, not NoneType"
    },
    {
        "repo_url": "github.com/lllyasviel/stable-diffusion-webui-forge",
        "filepath": "modules/sd_disable_initialization.py",
        "commit_date": "2023-01-10T14:46:59Z",
        "message": "add support for transformers==4.25.1\nadd fallback for when quick model creation fails"
    },
    {
        "repo_url": "github.com/lllyasviel/stable-diffusion-webui-forge",
        "filepath": "modules/sd_disable_initialization.py",
        "commit_date": "2023-01-10T13:51:04Z",
        "message": "add more stuff to ignore when creating model from config\nprevent .vae.safetensors files from being listed as stable diffusion models"
    },
    {
        "repo_url": "github.com/lllyasviel/stable-diffusion-webui-forge",
        "filepath": "modules/sd_disable_initialization.py",
        "commit_date": "2023-01-10T11:08:29Z",
        "message": "disable torch weight initialization and CLIP downloading/reading checkpoint to speedup creating sd model from config"
    },
    {
        "repo_url": "github.com/NVIDIA/NeMo",
        "filepath": "examples/multimodal/vision_language_foundation/clip/convert_external_clip_to_nemo.py",
        "commit_date": "2024-01-11T06:32:19Z",
        "message": "Add All Multimodal Source Code Part 2: Text to image, x to nerf (#7970)\n\n* Update README.md: output_path --> output_manifest_filepath (#7442)\n\nSigned-off-by: Samuele Cornell <cornellsamuele@gmail.com>\n\n* Updating FlashAttention API to match FlashAttentionV2\n\n* Multiple fixes for mm\n\n* Fix CI inductor issue and update to torch compile\n\n* Remove suppress error\n\n* Fix when conversion config uses fp16 and it complains about precision plugin\n\n* Fixing FAv2 API usage\n\n* Initial release of content filtering model\n\n* Added synthetic dataloader for precached and online mode\n\n* Mingyuanm/dreambooth opt\n\n* Add llama2 support in neva training\n\n* Fix sampler length\n\n* Fix all precision issues in nemo multimodal\n\n* Add rope dynamic linear scaling (#7437)\n\n* Add dynamic linear scaling\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>\n\n* Fix None dataloader issue in PTL2.0 (#7455)\n\n* Fix None dataloader issue in PTL2.0\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* updating values of self._validation_dl and self._test_dl as well\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* updating values of self._validation_dl and self._test_dl as well\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* [ASR] Confidence measure -> method renames (#7434)\n\n* measure -> method\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Add steps for document of getting dataset 'SF Bilingual Speech' (#7378)\n\n* Add steps for document of getting dataset 'SF Bilingual Speech'\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update datasets.rst\n\nadded a link from a tutorial demonstrating detailed data prep steps.\n\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* RNN-T confidence and alignment bugfix (#7381)\n\n* new frame_confidence and alignments lists are now always created after the while loop\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* tests added\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* Fix resume from checkpoint in exp_manager (#7424) (#7426)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\n\n* Fix checking of cuda/cpu device for inputs of Decoder (#7444)\n\n* Fix checking of cuda/cpu device for inputs of Decoder\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update tacotron2.py\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: Jason <jasoli@nvidia.com>\n\n* Fix failure of ljspeech's get_data.py (#7430)\n\n* Fix failure of ljspeech's get_data.py\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* [TTS] Fix audio codec type checks (#7373)\n\n* [TTS] Fix audio codec type checks\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Fix audio codec tests\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Add dataset to path of logged artifacts (#7462)\n\n* [TTS] Add dataset to path of logged artifacts\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Revert axis name back to Audio Frames\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* Fix sft dataset truncation (#7464)\n\n* Add fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Automatic Lip Reading Recognition (ALR) - ASR/CV (Visual ASR) (#7330)\n\n* striding_conv1d_k5 and dw_striding_conv1d_k5 subsampling\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* transpose conv1d inputs\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* Update subsampling.py\n\nchange striding_conv1d_k5 to striding_conv1d\n\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\n\n* cv branch\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* video manifest\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* add collection classes\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add test_step_outputs\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* correct manifest bug when having only audio or only videos\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* correct manifest bug when having only audio or only videos\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* clean references\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* freeze unfreeze transcribe cv models\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* correct manifest get_full_path bug\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* update for PR\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* guard torchvision\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update nemo/collections/cv/data/video_to_text_dataset.py\n\nCo-authored-by: Igor Gitman <igor.a.gitman@gmail.com>\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\n\n* _video_speech_collate_fn in cv/data/video_to_text.py\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* add self.out = None to asr subsampling\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* Update nemo/collections/cv/data/video_to_text_dataset.py\n\nCo-authored-by: Igor Gitman <igor.a.gitman@gmail.com>\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\n\n* cv -> multimodal/speech_cv branch\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Igor Gitman <igor.a.gitman@gmail.com>\n\n* HF StarCoder to NeMo conversion script (#7421)\n\n* Script to convert HF StarCoder checkpoint to NeMo\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* StarCoder conversion test\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Fix test\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Catch up with save_to changes\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Don't abbreviate args for clarity\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Configurable precision: BF16 vs FP32\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* fix bug when loading dist ckpt in peft (#7452)\n\nSigned-off-by: Hongbin Liu <hongbinl@nvidia.com>\nCo-authored-by: Hongbin Liu <hongbinl@nvidia.com>\n\n* Fix adding positional embeddings in-place in transformer module (#7440)\n\nSigned-off-by: Tamerlan Tabolov <tktabolov@gmail.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\n* Fix (#7478)\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* add sleep (#7498) (#7499)\n\n* add sleep\n\n\n\n* add sleep onto config instead\n\n\n\n* add comment\n\n\n\n---------\n\nSigned-off-by: Gerald Shen <geshen@nvidia.com>\nCo-authored-by: Gerald Shen <119401249+gshennvm@users.noreply.github.com>\n\n* Fix exp manager check for sleep (#7503) (#7504)\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\n\n* bugfix: trainer.accelerator=auto from None. (#7492) (#7493)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\n\n* [doc] fix broken link (#7481)\n\nSigned-off-by: Stas Bekman <stas00@users.noreply.github.com>\n\n* [TTS] Read audio as int32 to avoid flac read errors (#7477)\n\n* [TTS] Read audio as int32 to avoid flac read errors\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Add comment about read failures\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* Add dataset 'AISHELL-3' from OpenSLR for training mandarin TTS (#7409)\n\n* Add dataset 'AISHELL-3' from OpenSLR for training mandarin TTS\n* Train 'AISHELL-3' dataset with multi-speakers\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update get_data.py\n\nupdate copyright header\n\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* Update get_data.py\n\nadded a disclaimer\n\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add new configuration file for AISHELL3 with multispeaker of fastpitch\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* dllogger - log on rank 0 only (#7513)\n\nSigned-off-by: Stas Bekman <stas00@users.noreply.github.com>\n\n* Fix TTS FastPitch tutorial (#7494) (#7516)\n\n* Fix\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\n* Fix get_dist() tensor dimension (#7506) (#7515)\n\nSigned-off-by: Jocelyn Huang <jocelynh@nvidia.com>\nCo-authored-by: Jocelyn <jocelynh@nvidia.com>\n\n* bugfix: specify trainer.strategy=auto when devices=1 (#7509) (#7512)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\n\n* fix (#7511)\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n* [TTS] Fix FastPitch data prep tutorial (#7524)\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* add italian tokenization (#7486)\n\n* add italian tokenization\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add more ipa lexicon it\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix error deletion\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* add test\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Replace None strategy with auto in tutorial notebooks (#7521) (#7527)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* unpin setuptools (#7534) (#7535)\n\nSigned-off-by: fayejf <36722593+fayejf@users.noreply.github.com>\nCo-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>\n\n* remove auto generated examples (#7510)\n\n* explicitly remove autogenerated examples for data parallel evaluation\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* mark autogenrated and remove it for test\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: arendu <adithyare@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Add the `strategy` argument to `MegatronGPTModel.generate()` (#7264)\n\nIt is passed as an explicit argument rather than through\n`**strategy_args` so as to ensure someone cannot accidentally pass other\narguments that would end up being ignored.\n\nIt is a keyword-only argument to ensure that if in the future we want to\nupdate the signature to `**strategy_args`, we can do it without breaking\ncode.\n\nSigned-off-by: Olivier Delalleau <507137+odelalleau@users.noreply.github.com>\n\n* Fix PTL2.0 related ASR bugs in r1.21.0: Val metrics logging, None dataloader issue (#7531) (#7533)\n\n* fix none dataloader issue ptl2\n\n\n\n* ptl2.0 logging fixes for rnnt_models\n\n\n\n---------\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\nCo-authored-by: Kunal Dhawan <kunaldhawan97@gmail.com>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\n\n* gpus -> devices (#7542) (#7545)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\n\n* Update FFMPEG version to fix issue with torchaudio (#7551) (#7553)\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\n\n* PEFT GPT & T5 Refactor (#7308)\n\n* initial implementation of add_adapters API\n\n* correct type hint\n\n* Add config in add_adapters for save and load (@author bobchen)\n\n* Remove AdapterConfig to avoid import error\n\n* Add AdaterConfig back and move adaptermixin to sft model\n\n* Add NLPSaveRestoreConnector as default in NLPModel.restore_from\n\n* Add restore_from_nemo_with_adapter and test script\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* rename t5 file and classes to be consistent with GPT\n\n* add t5 sft dataset\n\n* add support for single-file format with T5SFTDataset\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Various small changes to make T5 SFT work like GPT SFT\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add adapter evaluation test script\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add MultiAdaterConfig for ia3 and fix builder issue\n\n* Make ptuning for T5SFTModel work using mixin\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add IA3_Adapter for AdapterName\n\n* Add adapter name for ptuning and attention adapter\n\n* Make test script GPT/T5 agnostic\n\n* Add layer selection feature\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Integrate adapter name and config\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update gpt peft tuning script to new API\n\n* add t5 peft tuning script with new API\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix IA3 layer selection issue\n\n* Override state_dict on SFT model instead of mixin\n\n* Add load adapter by adapter config\n\n* move peft config map away from example script\n\n* auto get config from nemo adapter\n\n* Move PEFTConfig to new file\n\n* fix ckpt save/load for t5\n\n* name change: add_adapters -> add_adapter\n\n* variable name change\n\n* update t5 script\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix t5 issues\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add weight tying\n\n* update gpt tuning script\n\n* PEFT-API proposal\n\n* Fix according to comments\n\n* update tuning scripts\n\n* move merge_cfg_with to mixin class since it applies to both gpt and t5 and requires the model class for restore\n\n* Add mcore_gpt support for NLPAdapterMixin\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix typo\n\n* variable name change to distinguish \"peft\" and \"adapter\"\n\n* override `load_adapters` to support `add_adapter` name change\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update tuning and eval script for adapter save/load\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add Ptuning on first stage only\n\n* add lora tutorial for review\n\n* Fix layer selection for mcore\n\n* add landing page\n\n* fix resume training\n\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add mcore condition in sharded_state_dict to make sft work\n\n* Update lora_tutorial.md\n\nFirst edit of this file for PEFT documentation for NeMO\n\nSigned-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\n\n* rename Adapter to AttentionAdapter to avoid confusion in doc\n\n* Change load_adapters to load .nemo\n\n* add quick start guide\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add load_adapters with .ckpt\n\n* Remove setup_complete changes in load_adapters\n\n* update landing page\n\n* remove typo\n\n* Updated quick_start.md per Chen Cui\n\nSigned-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\n\n* Add inference config merger and tutorial\n\n* Add doc string for NLPAdapterModelMixin and deprecated warning on MegatronGPTPEFTModel\n\n* add supported_methods.md and update other documentations\n\n* Update supported_methods.md\n\nminor updates.\n\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\n\n* Update landing_page.md\n\nminor update.\n\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\n\n* Modify doc string for NLPAdapterModelMixin\n\n* Add doc string add_adapters in NLPAdapterModelMixin\n\n* rename canonical adapters\n\n* remove mcore hard dependency\n\n* [PATCH] move microbatch calculator to nemo from apex\n\n* remove apex dependency in gpt and t5 sft models\n\n* remove apex dependency in gpt model\n\n* render doc strings\n\n* fix\n\n* Add missing virtual_tokens on ptuning\n\n* fix docstrings\n\n* update gpt-style model coverage in docs\n\n* update docstring\n\n* Remove pdb\n\n* add lightning_fabric to make docstring rendering work\n\n* Add Ptuning missing key\n\n* try docstring rendering\n\n* Fix ptuning issue\n\n* update gpt t5 peft tuning and eval scripts\n\n* typos\n\n* update eval config\n\n* fix bug relating to apex dependency removal\n\n* typo\n\n* make predict step behave the same as test step\n\n* make lora tutorial work in notebook\n\n* cosmetics\n\n* update yaml scripts\n\n* mcore_gpt attribute optional\n\n* typo\n\n* update eval scripts and fix T5 eval bugs\n\n* add NLPDDPStrategyNotebook and trainer builder logic to use it\n\n* update lora notebook to use new trainer builder\n\n* fix microbatch calculator bug for inference after training\n\n* Convert markdown files to RST and incorporate with doc\n\n* typo\n\n* revise language\n\n* remove extra cell\n\n* remove unnecessary inheritance\n\n* remove old tests\n\n* move layer selection default so logging messages make sense\n\n* remove `save_adapters` as adapter weights are saved automatically during training\n\n* initialize weights from a checkpoint instead of randomly\n\n* multiple fields can form a context (#7147)\n\n* list of context fields and flexible prompt template\n\nSigned-off-by: arendu <adithya.r@gmail.com>\n\n* list of fields for context\n\nSigned-off-by: arendu <adithya.r@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Add multiple truncation fields and middle truncation\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Compatible to old ckpt\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix tokenize detokenize issue\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Remove detokenization, add truncation augmentation\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Resolve comments\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Remove unused import\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* revert eos\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Add tokenizer space_sensitive attribute\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix error\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Fix erorr and use re\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Change assert logic\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Follow adi suggestion\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Remove merge function\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add example and comment\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Remove context_key and add comment\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Remove random truncation\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix template none\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-off-by: arendu <adithya.r@gmail.com>\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nSigned-off-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\n* revert config changes\n\n* remove accidental breakpoint\n\n* support TP>1 loading\n\n* infer adapter type from checkpoint in during eval\n\n* breakup add adapter\n\n* enable interpolation of train_ds and validation_ds\n\n* update metric calc script to conform to single-file eval format\n\n* remove extraneous print\n\n* update lora notebook for updated merge_inference_cfg\n\n* Update nlp_adapter_mixins.py\n\nvariable name change\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* turn off grad scaler for PP to match old scripts\n\n* remove PEFTSaveRestoreConnector since functionality all covered by the new mixin class\n\n* remove resume_from_checkpoint check since covered in #7335\n\n* revert changes made in eval config interpolation\n\n* more interpolation\n\n* typo\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove dup line\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* code style warnings\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix config mistake\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* add copyright header\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* fix code check warnings\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* revert changes to remove apex dependency (mixed apex+nemo microbatch calculator broke some CI tests)\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add more deprecation notices\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* update deprecation notices\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* update deprecation notices\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* consolidate peft and sft scripts\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* update CI tests\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* notebook branch points to main to prepare for merge\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* fix gpt and t5 validation with any metric other than loss\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* support pre-extracted checkpoints\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n---------\n\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\nSigned-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\nSigned-off-by: arendu <adithya.r@gmail.com>\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nSigned-off-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\nSigned-off-by: Chen Cui <chcui@nvidia.com>\nCo-authored-by: Chen Cui <chcui@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Marc Romeyn <marcromeyn@gmail.com>\nCo-authored-by: jasonwan <jasonwan@nvidia.com>\nCo-authored-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\nCo-authored-by: Adi Renduchintala <adithyare@nvidia.com>\nCo-authored-by: Yuanzhe Dong <yudong@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\n* fix a typo (#7496)\n\nSigned-off-by: BestJuly <chntaoli@163.com>\n\n* [TTS] remove curly braces from ${BRANCH} in jupyer notebook cell. (#7554) (#7560)\n\n* remove curly braces.\n* remove installation of pynini.\n---------\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\n\n* add youtube embed url (#7570)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\n\n* Remap speakers to continuous range of speaker_id for dataset AISHELL3 (#7536)\n\n* Remap speakers to continuous range of speaker_id for dataset AISHELL3\n* Add new key/value pair to record raw speaker for AISHELL3 dataset\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* fix validation_step_outputs initialization for multi-dataloader (#7546) (#7572)\n\n* added correct validation_step_outputs initialization for mutli-dataloader\n\n\n\n* changed kernel for display\n\n\n\n* Update logic for validation and test step outputs\n\n\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* revert multidataloader changes in multilang ASR notebook\n\n\n\n---------\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Kunal Dhawan <kunaldhawan97@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Append output of val step to self.validation_step_outputs (#7530) (#7532)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* [TTS] fixed trainer's accelerator and strategy. (#7569) (#7574)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\n\n* Append val/test output to instance variable in EncDecSpeakerLabelModel (#7562) (#7573)\n\n* Append val/test output to the instance variable in EncDecSpeakerLabelModel\n\n\n\n* Handle test case in evaluation_step\n\n\n\n* Replace type with isinstance\n\n\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* Fix CustomProgressBar for resume (#7427) (#7522)\n\n* Fix CustomProgress Bar for resume and multiple epochs\n\n\n\n* Edit num_training_batches\n\n\n\n* Use max_steps as total for progress bar for resume\n\n\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* fix typos in nfa and speech enhancement tutorials (#7580) (#7583)\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\nCo-authored-by: Elena Rastorgueva <80532067+erastorgueva-nv@users.noreply.github.com>\n\n* Add strategy as ddp_find_unused_parameters_true for glue_benchmark.py (#7454) (#7461)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* update strategy (#7577) (#7578)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\n\n* Fix typos (#7581)\n\n* Change hifigan finetune strategy to ddp_find_unused_parameters_true (#7579) (#7584)\n\n* Change strategy to auto\n\n\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\n* [BugFix] Add missing quotes for auto strategy in tutorial notebooks (#7541) (#7548)\n\n* Add missing quotes for auto strategy\n\n\n\n* Revert trainer.gpus to trainer.devices in Self_Supervised_Pre_Training.ipynb\n\n\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nSigned-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* add build os key (#7596) (#7599)\n\n* add build os key\n\n\n\n* add tools\n\n\n\n* update to stable version\n\n\n\n---------\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\n\n* StarCoder SFT test + bump PyT NGC image to 23.09 (#7540)\n\n* Add SFT StarCoder test\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Remove _modify_config call as it is covered in load_from_nemo just below\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Test with pyt:23.09 container\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n---------\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* defaults changed (#7600)\n\n* defaults changed\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* typo\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* update\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n---------\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* add ItalianPhonemesTokenizer (#7587)\n\n* add ItalianPhonemesTokenizer\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix Italian phonemes\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add test\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n---------\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* best ckpt fix (#7564) (#7588)\n\nSigned-off-by: dimapihtar <dpihtar@gmail.com>\nCo-authored-by: Dmytro Pykhtar <37850217+dimapihtar@users.noreply.github.com>\n\n* Add files via upload (#7598)\n\nspecifies the branch\n\nSigned-off-by: George <37293288+Jorjeous@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* Fix validation in G2PModel and ThutmoseTaggerModel (#7597) (#7606)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain (#7576) (#7586)\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Safeguard nemo_text_processing installation on ARM (#7485)\n\n* safeguard nemo_text_processing installing\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update check\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n---------\n\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Bound transformers version in requirements (#7620)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* fix llama2 70b lora tuning bug (#7622)\n\n* fix llama2 70b lora tuning bug\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* Update peft_config.py\n\nbrackets\n\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\n\n---------\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\nCo-authored-by: Adi Renduchintala <adithyare@nvidia.com>\n\n* Fix import error no module name model_utils (#7629)\n\nSigned-off-by: Mehadi Hasan Menon <mehadihasan80@gmail.com>\n\n* add fc large ls models (#7641)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao Koluguri <nithinraok>\n\n* bugfix: trainer.gpus, trainer.strategy, trainer.accelerator (#7621) (#7642)\n\n* [TTS] bugfix for Tacotron2 tutorial due to PTL 2.0\n* trainer.gpus -> trainer.devices\n* fixed related tutorial bugs\n---------\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* fix ssl models ptl monitor val through logging (#7608) (#7614)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* Fix metrics for SE tutorial (#7604) (#7612)\n\nSigned-off-by: Ante Jukic\u0301 <ajukic@nvidia.com>\nCo-authored-by: anteju <108555623+anteju@users.noreply.github.com>\n\n* Add ddp_find_unused_parameters=True and change accelerator to auto (#7623) (#7644)\n\n* Add ddp_find_unused_parameters=True and change acclerator to auto\n\n\n\n* Add ddp_find_unused_parameters True for normalization_as_tagging_train.py\n\n\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* Fix py3.11 dataclasses issue  (#7616)\n\n* Fix py3.11 dataclasses issue  (#7582)\n\n* Update ASR configs to support Python 3.11\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Update TTS configs to support Python 3.11\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Guard MeCab and Ipadic\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix remaining ASR dataclasses\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Fix remaining ASR dataclasses\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Fix scripts\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Update name to ConfidenceMethodConfig\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain (#7576) (#7586)\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Safeguard nemo_text_processing installation on ARM (#7485)\n\n* safeguard nemo_text_processing installing\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update check\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n---------\n\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Fix changes to confidence measure\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Jason <jasoli@nvidia.com>\n\n* [Stable Diffusion/ControlNet] Enable O2 training for SD and Fix ControlNet CI failure\n\n* Mingyuanm/dreambooth fix\n\n* Fix NeMo CI Infer Issue\n\n* DreamFusion\n\n* Move neva export changes\n\n* Add Imagen Synthetic Dataloader\n\n* Add VITWrapper and export stuff to wrapper\n\n* Update neva with megatron-core support\n\n* Fix issues with Dockerfile (#7650) (#7652)\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\n\n* [ASR] RNN-T greedy decoding max_frames fix for alignment and confidence (#7635)\n\n* decoding and test fix\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* [ASR] Fix type error in jasper (#7636) (#7653)\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nCo-authored-by: Ryan Langman <rlangman@nvidia.com>\n\n* [TTS] Add STFT and SI-SDR loss to audio codec recipe (#7468)\n\n* [TTS] Add STFT and SI-SDR loss to audio codec recipe\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Fix STFT resolution\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Fix training metric logging\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Add docstring to mel and stft losses\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Create per.py (#7538)\n\n* Move model precision copy (#7336)\n\n* move cfg precision set to megatron base model\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* remove copy from other models\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* modify attribute not arg\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* fix gpt model test for ptl 2.0\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* rename function and add docstring\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* replace precision to dtype conditionals with func call\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* unnecessary function and cfg reset\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* set default value\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* fix precision lookup in a few more places\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* rename mapping function\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* ununsed import\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* save torch datatype to model\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* set weights precision wrt amp o2\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* Revert \"set weights precision wrt amp o2\"\n\nThis reverts commit 313a4bfe5eb69d771a6d2433898c0685836aef5c.\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* revert half precision at inference attempt\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* move autocast dtype to base model\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* move params dtype to base model, enable fp16 O2 inf\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* unused imports\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n---------\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix PEFT checkpoint loading (#7388)\n\n* Fix PEFT checkpoint loading\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Use distributed optimizer support for multiple dtypes (#7359)\n\n* Update distopt wrapper with multiple dtype support\n\nRemove manual handling of separate FP32 optimizer.\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Use distopt support for contiguous buffers with multiple dtypes\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Fix typo\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Separate distopt buckets for first GPT layer and non-overlapped params\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Add distopt logic for int dtypes\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Update Apex commit\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Remove unused variables\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Update Apex commit in README and Jenkensfile\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Debug Dockerfile and Jenkinsfile\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n---------\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* minor fix for llama ckpt conversion script (#7387)\n\n* minor fix for llama ckpt conversion script\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* Update Jenkinsfile\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* remove fast_swiglu configuration\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n---------\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix wrong calling of librosa.get_duration() in notebook (#7376)\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [PATCH] PEFT import mcore (#7393)\n\n* [PATCH] PEFT import mcore\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Create per.py\n\nScript for calculation Punctuation Error Rate and related rates (correct rate, deletions rate, etc.)\n\nSigned-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [TTS] Added a callback for logging initial data (#7384)\n\nSigned-off-by: Ante Jukic\u0301 <ajukic@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update Core Commit (#7402)\n\n* Update Core Commit\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n* update commit\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n---------\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Use cfg attribute in bert (#7394)\n\n* use cfg attribute instead of arg\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* use torch_dtype in place of cfg.precision\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* move precision copy before super constructor\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* use trainer arg\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n---------\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add support for bias conversion in Swiglu models (#7386)\n\n* Add support for bias conversion in Swiglu models\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Add support for auto extracting tokenizer model\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add support for auto extracting tokenizer model\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Fix issue with missing tokenizer\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Refactor\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Refactor\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update save_to and restore_from for dist checkpointing (#7343)\n\n* add dist ckpt to save to, in progress\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* move dist ckpt\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* clean up\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update restore from, need to figure out how to initialize distributed\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* launch distrib if needed when restoring dist ckpt\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* when using mcore we can change tp pp on the fly\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* add load_from_checkpoint support for dist ckpt\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update llama convert script to save dist .nemo\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* fix load dist ckpt\n\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* setup TE TP groups if needed\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* setup te tp groups if needed\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* remove import\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n---------\n\nSigned-off-by: eharper <eharper@nvidia.com>\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: jasonwan <jasonwan@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* fix forward for with mcore=false (#7403)\n\nSigned-off-by: Jimmy Zhang <jiemingz@nvidia.com>\nCo-authored-by: Jimmy Zhang <jiemingz@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix logging to remove 's/it' from progress bar in Megatron models and add train_step_timing (#7374)\n\n* Add CustomProgressBar class to exp_manager and trainer callbacks\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix the progress bar to reflect total microbatch cnt\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* Modify CustomProgressBar class\n\n1) Modify CustomProgressBar class to update progress bar per global_step instead of per microbatch\n2) Add the callback to other megatron training/finetuning files that are not using MegatronTrainerBuilder\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* Add CustomProgressBar callback to tuning files\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Set Activation Checkpointing Defaults (#7404)\n\n* Set Activation Checkpointing Defaults\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* check for None\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* make loss mask default to false (#7407)\n\nSigned-off-by: eharper <eharper@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add dummy userbuffer config files (#7408)\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* add missing ubconf files (#7412)\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* New tutorial on Speech Data Explorer (#7405)\n\n* Added Google Colab based tutorial on Speech Data Explorer\n\nSigned-off-by: George Zelenfroynd <gzelenfroind@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update ptl training ckpt conversion script to work with dist ckpt (#7416)\n\n* update ptl convert script\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* don't break legacy\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: eharper <eharper@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Allow disabling sanity checking when num_sanity_val_steps=0 (#7413)\n\n* Allow disabling sanity checking when num_sanity_val_steps=0\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* Update num_sanity_val_steps to be a multiple of num_microbatches\n\nSigned-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nSigned-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add comprehensive error messages (#7261)\n\nSigned-off-by: Anton Peganov <apeganov@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* check NEMO_PATH (#7418)\n\nSigned-off-by: Nikolay Karpov <karpnv@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* layer selection for ia3 (#7417)\n\n* layer selection for ia3\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: arendu <adithyare@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix missing pip package 'einops' (#7397)\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix failure of pyaudio in Google Colab (#7396)\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update README.md: output_path --> output_manifest_filepath (#7442)\n\nSigned-off-by: Samuele Cornell <cornellsamuele@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add rope dynamic linear scaling (#7437)\n\n* Add dynamic linear scaling\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix None dataloader issue in PTL2.0 (#7455)\n\n* Fix None dataloader issue in PTL2.0\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* updating values of self._validation_dl and self._test_dl as well\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* updating values of self._validation_dl and self._test_dl as well\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [ASR] Confidence measure -> method renames (#7434)\n\n* measure -> method\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add steps for document of getting dataset 'SF Bilingual Speech' (#7378)\n\n* Add steps for document of getting dataset 'SF Bilingual Speech'\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update datasets.rst\n\nadded a link from a tutorial demonstrating detailed data prep steps.\n\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* RNN-T confidence and alignment bugfix (#7381)\n\n* new frame_confidence and alignments lists are now always created after the while loop\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* tests added\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix resume from checkpoint in exp_manager (#7424) (#7426)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix checking of cuda/cpu device for inputs of Decoder (#7444)\n\n* Fix checking of cuda/cpu device for inputs of Decoder\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update tacotron2.py\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: Jason <jasoli@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix failure of ljspeech's get_data.py (#7430)\n\n* Fix failure of ljspeech's get_data.py\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [TTS] Fix audio codec type checks (#7373)\n\n* [TTS] Fix audio codec type checks\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Fix audio codec tests\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [TTS] Add dataset to path of logged artifacts (#7462)\n\n* [TTS] Add dataset to path of logged artifacts\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Revert axis name back to Audio Frames\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix sft dataset truncation (#7464)\n\n* Add fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-of\u2026"
    },
    {
        "repo_url": "github.com/NVIDIA/NeMo",
        "filepath": "examples/multimodal/vision_language_foundation/clip/convert_external_clip_to_nemo.py",
        "commit_date": "2023-12-13T02:12:55Z",
        "message": "Add All Multimodal Source Code (#7791)\n\n* Add comprehensive error messages (#7261)\n\nSigned-off-by: Anton Peganov <apeganov@nvidia.com>\n\n* check NEMO_PATH (#7418)\n\nSigned-off-by: Nikolay Karpov <karpnv@gmail.com>\n\n* layer selection for ia3 (#7417)\n\n* layer selection for ia3\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: arendu <adithyare@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Fix missing pip package 'einops' (#7397)\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Fix failure of pyaudio in Google Colab (#7396)\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update README.md: output_path --> output_manifest_filepath (#7442)\n\nSigned-off-by: Samuele Cornell <cornellsamuele@gmail.com>\n\n* Updating FlashAttention API to match FlashAttentionV2\n\n* Multiple fixes for mm\n\n* Fix CI inductor issue and update to torch compile\n\n* Remove suppress error\n\n* Fix when conversion config uses fp16 and it complains about precision plugin\n\n* Fixing FAv2 API usage\n\n* Initial release of content filtering model\n\n* Added synthetic dataloader for precached and online mode\n\n* Mingyuanm/dreambooth opt\n\n* Add llama2 support in neva training\n\n* Fix sampler length\n\n* Fix all precision issues in nemo multimodal\n\n* Add rope dynamic linear scaling (#7437)\n\n* Add dynamic linear scaling\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>\n\n* Fix None dataloader issue in PTL2.0 (#7455)\n\n* Fix None dataloader issue in PTL2.0\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* updating values of self._validation_dl and self._test_dl as well\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* updating values of self._validation_dl and self._test_dl as well\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* [ASR] Confidence measure -> method renames (#7434)\n\n* measure -> method\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Add steps for document of getting dataset 'SF Bilingual Speech' (#7378)\n\n* Add steps for document of getting dataset 'SF Bilingual Speech'\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update datasets.rst\n\nadded a link from a tutorial demonstrating detailed data prep steps.\n\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* RNN-T confidence and alignment bugfix (#7381)\n\n* new frame_confidence and alignments lists are now always created after the while loop\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* tests added\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* Fix resume from checkpoint in exp_manager (#7424) (#7426)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\n\n* Fix checking of cuda/cpu device for inputs of Decoder (#7444)\n\n* Fix checking of cuda/cpu device for inputs of Decoder\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update tacotron2.py\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: Jason <jasoli@nvidia.com>\n\n* Fix failure of ljspeech's get_data.py (#7430)\n\n* Fix failure of ljspeech's get_data.py\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* [TTS] Fix audio codec type checks (#7373)\n\n* [TTS] Fix audio codec type checks\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Fix audio codec tests\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Add dataset to path of logged artifacts (#7462)\n\n* [TTS] Add dataset to path of logged artifacts\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Revert axis name back to Audio Frames\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* Fix sft dataset truncation (#7464)\n\n* Add fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Automatic Lip Reading Recognition (ALR) - ASR/CV (Visual ASR) (#7330)\n\n* striding_conv1d_k5 and dw_striding_conv1d_k5 subsampling\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* transpose conv1d inputs\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* Update subsampling.py\n\nchange striding_conv1d_k5 to striding_conv1d\n\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\n\n* cv branch\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* video manifest\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* add collection classes\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add test_step_outputs\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* correct manifest bug when having only audio or only videos\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* correct manifest bug when having only audio or only videos\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* clean references\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* freeze unfreeze transcribe cv models\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* correct manifest get_full_path bug\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* update for PR\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* guard torchvision\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update nemo/collections/cv/data/video_to_text_dataset.py\n\nCo-authored-by: Igor Gitman <igor.a.gitman@gmail.com>\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\n\n* _video_speech_collate_fn in cv/data/video_to_text.py\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* add self.out = None to asr subsampling\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* Update nemo/collections/cv/data/video_to_text_dataset.py\n\nCo-authored-by: Igor Gitman <igor.a.gitman@gmail.com>\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\n\n* cv -> multimodal/speech_cv branch\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Igor Gitman <igor.a.gitman@gmail.com>\n\n* HF StarCoder to NeMo conversion script (#7421)\n\n* Script to convert HF StarCoder checkpoint to NeMo\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* StarCoder conversion test\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Fix test\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Catch up with save_to changes\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Don't abbreviate args for clarity\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Configurable precision: BF16 vs FP32\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* fix bug when loading dist ckpt in peft (#7452)\n\nSigned-off-by: Hongbin Liu <hongbinl@nvidia.com>\nCo-authored-by: Hongbin Liu <hongbinl@nvidia.com>\n\n* Fix adding positional embeddings in-place in transformer module (#7440)\n\nSigned-off-by: Tamerlan Tabolov <tktabolov@gmail.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\n* Fix (#7478)\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* add sleep (#7498) (#7499)\n\n* add sleep\n\n\n\n* add sleep onto config instead\n\n\n\n* add comment\n\n\n\n---------\n\nSigned-off-by: Gerald Shen <geshen@nvidia.com>\nCo-authored-by: Gerald Shen <119401249+gshennvm@users.noreply.github.com>\n\n* Fix exp manager check for sleep (#7503) (#7504)\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\n\n* bugfix: trainer.accelerator=auto from None. (#7492) (#7493)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\n\n* [doc] fix broken link (#7481)\n\nSigned-off-by: Stas Bekman <stas00@users.noreply.github.com>\n\n* [TTS] Read audio as int32 to avoid flac read errors (#7477)\n\n* [TTS] Read audio as int32 to avoid flac read errors\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Add comment about read failures\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* Add dataset 'AISHELL-3' from OpenSLR for training mandarin TTS (#7409)\n\n* Add dataset 'AISHELL-3' from OpenSLR for training mandarin TTS\n* Train 'AISHELL-3' dataset with multi-speakers\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update get_data.py\n\nupdate copyright header\n\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* Update get_data.py\n\nadded a disclaimer\n\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add new configuration file for AISHELL3 with multispeaker of fastpitch\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* dllogger - log on rank 0 only (#7513)\n\nSigned-off-by: Stas Bekman <stas00@users.noreply.github.com>\n\n* Fix TTS FastPitch tutorial (#7494) (#7516)\n\n* Fix\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\n* Fix get_dist() tensor dimension (#7506) (#7515)\n\nSigned-off-by: Jocelyn Huang <jocelynh@nvidia.com>\nCo-authored-by: Jocelyn <jocelynh@nvidia.com>\n\n* bugfix: specify trainer.strategy=auto when devices=1 (#7509) (#7512)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\n\n* fix (#7511)\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n* [TTS] Fix FastPitch data prep tutorial (#7524)\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* add italian tokenization (#7486)\n\n* add italian tokenization\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add more ipa lexicon it\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix error deletion\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* add test\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Replace None strategy with auto in tutorial notebooks (#7521) (#7527)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* unpin setuptools (#7534) (#7535)\n\nSigned-off-by: fayejf <36722593+fayejf@users.noreply.github.com>\nCo-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>\n\n* remove auto generated examples (#7510)\n\n* explicitly remove autogenerated examples for data parallel evaluation\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* mark autogenrated and remove it for test\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: arendu <adithyare@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Add the `strategy` argument to `MegatronGPTModel.generate()` (#7264)\n\nIt is passed as an explicit argument rather than through\n`**strategy_args` so as to ensure someone cannot accidentally pass other\narguments that would end up being ignored.\n\nIt is a keyword-only argument to ensure that if in the future we want to\nupdate the signature to `**strategy_args`, we can do it without breaking\ncode.\n\nSigned-off-by: Olivier Delalleau <507137+odelalleau@users.noreply.github.com>\n\n* Fix PTL2.0 related ASR bugs in r1.21.0: Val metrics logging, None dataloader issue (#7531) (#7533)\n\n* fix none dataloader issue ptl2\n\n\n\n* ptl2.0 logging fixes for rnnt_models\n\n\n\n---------\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\nCo-authored-by: Kunal Dhawan <kunaldhawan97@gmail.com>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\n\n* gpus -> devices (#7542) (#7545)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\n\n* Update FFMPEG version to fix issue with torchaudio (#7551) (#7553)\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\n\n* PEFT GPT & T5 Refactor (#7308)\n\n* initial implementation of add_adapters API\n\n* correct type hint\n\n* Add config in add_adapters for save and load (@author bobchen)\n\n* Remove AdapterConfig to avoid import error\n\n* Add AdaterConfig back and move adaptermixin to sft model\n\n* Add NLPSaveRestoreConnector as default in NLPModel.restore_from\n\n* Add restore_from_nemo_with_adapter and test script\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* rename t5 file and classes to be consistent with GPT\n\n* add t5 sft dataset\n\n* add support for single-file format with T5SFTDataset\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Various small changes to make T5 SFT work like GPT SFT\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add adapter evaluation test script\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add MultiAdaterConfig for ia3 and fix builder issue\n\n* Make ptuning for T5SFTModel work using mixin\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add IA3_Adapter for AdapterName\n\n* Add adapter name for ptuning and attention adapter\n\n* Make test script GPT/T5 agnostic\n\n* Add layer selection feature\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Integrate adapter name and config\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update gpt peft tuning script to new API\n\n* add t5 peft tuning script with new API\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix IA3 layer selection issue\n\n* Override state_dict on SFT model instead of mixin\n\n* Add load adapter by adapter config\n\n* move peft config map away from example script\n\n* auto get config from nemo adapter\n\n* Move PEFTConfig to new file\n\n* fix ckpt save/load for t5\n\n* name change: add_adapters -> add_adapter\n\n* variable name change\n\n* update t5 script\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix t5 issues\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add weight tying\n\n* update gpt tuning script\n\n* PEFT-API proposal\n\n* Fix according to comments\n\n* update tuning scripts\n\n* move merge_cfg_with to mixin class since it applies to both gpt and t5 and requires the model class for restore\n\n* Add mcore_gpt support for NLPAdapterMixin\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix typo\n\n* variable name change to distinguish \"peft\" and \"adapter\"\n\n* override `load_adapters` to support `add_adapter` name change\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update tuning and eval script for adapter save/load\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add Ptuning on first stage only\n\n* add lora tutorial for review\n\n* Fix layer selection for mcore\n\n* add landing page\n\n* fix resume training\n\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add mcore condition in sharded_state_dict to make sft work\n\n* Update lora_tutorial.md\n\nFirst edit of this file for PEFT documentation for NeMO\n\nSigned-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\n\n* rename Adapter to AttentionAdapter to avoid confusion in doc\n\n* Change load_adapters to load .nemo\n\n* add quick start guide\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add load_adapters with .ckpt\n\n* Remove setup_complete changes in load_adapters\n\n* update landing page\n\n* remove typo\n\n* Updated quick_start.md per Chen Cui\n\nSigned-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\n\n* Add inference config merger and tutorial\n\n* Add doc string for NLPAdapterModelMixin and deprecated warning on MegatronGPTPEFTModel\n\n* add supported_methods.md and update other documentations\n\n* Update supported_methods.md\n\nminor updates.\n\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\n\n* Update landing_page.md\n\nminor update.\n\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\n\n* Modify doc string for NLPAdapterModelMixin\n\n* Add doc string add_adapters in NLPAdapterModelMixin\n\n* rename canonical adapters\n\n* remove mcore hard dependency\n\n* [PATCH] move microbatch calculator to nemo from apex\n\n* remove apex dependency in gpt and t5 sft models\n\n* remove apex dependency in gpt model\n\n* render doc strings\n\n* fix\n\n* Add missing virtual_tokens on ptuning\n\n* fix docstrings\n\n* update gpt-style model coverage in docs\n\n* update docstring\n\n* Remove pdb\n\n* add lightning_fabric to make docstring rendering work\n\n* Add Ptuning missing key\n\n* try docstring rendering\n\n* Fix ptuning issue\n\n* update gpt t5 peft tuning and eval scripts\n\n* typos\n\n* update eval config\n\n* fix bug relating to apex dependency removal\n\n* typo\n\n* make predict step behave the same as test step\n\n* make lora tutorial work in notebook\n\n* cosmetics\n\n* update yaml scripts\n\n* mcore_gpt attribute optional\n\n* typo\n\n* update eval scripts and fix T5 eval bugs\n\n* add NLPDDPStrategyNotebook and trainer builder logic to use it\n\n* update lora notebook to use new trainer builder\n\n* fix microbatch calculator bug for inference after training\n\n* Convert markdown files to RST and incorporate with doc\n\n* typo\n\n* revise language\n\n* remove extra cell\n\n* remove unnecessary inheritance\n\n* remove old tests\n\n* move layer selection default so logging messages make sense\n\n* remove `save_adapters` as adapter weights are saved automatically during training\n\n* initialize weights from a checkpoint instead of randomly\n\n* multiple fields can form a context (#7147)\n\n* list of context fields and flexible prompt template\n\nSigned-off-by: arendu <adithya.r@gmail.com>\n\n* list of fields for context\n\nSigned-off-by: arendu <adithya.r@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Add multiple truncation fields and middle truncation\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Compatible to old ckpt\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix tokenize detokenize issue\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Remove detokenization, add truncation augmentation\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Resolve comments\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Remove unused import\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* revert eos\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Add tokenizer space_sensitive attribute\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix error\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Fix erorr and use re\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Change assert logic\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Follow adi suggestion\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Remove merge function\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add example and comment\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Remove context_key and add comment\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Remove random truncation\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix template none\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-off-by: arendu <adithya.r@gmail.com>\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nSigned-off-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\n* revert config changes\n\n* remove accidental breakpoint\n\n* support TP>1 loading\n\n* infer adapter type from checkpoint in during eval\n\n* breakup add adapter\n\n* enable interpolation of train_ds and validation_ds\n\n* update metric calc script to conform to single-file eval format\n\n* remove extraneous print\n\n* update lora notebook for updated merge_inference_cfg\n\n* Update nlp_adapter_mixins.py\n\nvariable name change\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* turn off grad scaler for PP to match old scripts\n\n* remove PEFTSaveRestoreConnector since functionality all covered by the new mixin class\n\n* remove resume_from_checkpoint check since covered in #7335\n\n* revert changes made in eval config interpolation\n\n* more interpolation\n\n* typo\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove dup line\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* code style warnings\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix config mistake\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* add copyright header\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* fix code check warnings\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* revert changes to remove apex dependency (mixed apex+nemo microbatch calculator broke some CI tests)\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add more deprecation notices\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* update deprecation notices\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* update deprecation notices\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* consolidate peft and sft scripts\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* update CI tests\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* notebook branch points to main to prepare for merge\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* fix gpt and t5 validation with any metric other than loss\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* support pre-extracted checkpoints\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n---------\n\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\nSigned-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\nSigned-off-by: arendu <adithya.r@gmail.com>\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nSigned-off-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\nSigned-off-by: Chen Cui <chcui@nvidia.com>\nCo-authored-by: Chen Cui <chcui@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Marc Romeyn <marcromeyn@gmail.com>\nCo-authored-by: jasonwan <jasonwan@nvidia.com>\nCo-authored-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\nCo-authored-by: Adi Renduchintala <adithyare@nvidia.com>\nCo-authored-by: Yuanzhe Dong <yudong@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\n* fix a typo (#7496)\n\nSigned-off-by: BestJuly <chntaoli@163.com>\n\n* [TTS] remove curly braces from ${BRANCH} in jupyer notebook cell. (#7554) (#7560)\n\n* remove curly braces.\n* remove installation of pynini.\n---------\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\n\n* add youtube embed url (#7570)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\n\n* Remap speakers to continuous range of speaker_id for dataset AISHELL3 (#7536)\n\n* Remap speakers to continuous range of speaker_id for dataset AISHELL3\n* Add new key/value pair to record raw speaker for AISHELL3 dataset\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* fix validation_step_outputs initialization for multi-dataloader (#7546) (#7572)\n\n* added correct validation_step_outputs initialization for mutli-dataloader\n\n\n\n* changed kernel for display\n\n\n\n* Update logic for validation and test step outputs\n\n\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* revert multidataloader changes in multilang ASR notebook\n\n\n\n---------\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Kunal Dhawan <kunaldhawan97@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Append output of val step to self.validation_step_outputs (#7530) (#7532)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* [TTS] fixed trainer's accelerator and strategy. (#7569) (#7574)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\n\n* Append val/test output to instance variable in EncDecSpeakerLabelModel (#7562) (#7573)\n\n* Append val/test output to the instance variable in EncDecSpeakerLabelModel\n\n\n\n* Handle test case in evaluation_step\n\n\n\n* Replace type with isinstance\n\n\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* Fix CustomProgressBar for resume (#7427) (#7522)\n\n* Fix CustomProgress Bar for resume and multiple epochs\n\n\n\n* Edit num_training_batches\n\n\n\n* Use max_steps as total for progress bar for resume\n\n\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* fix typos in nfa and speech enhancement tutorials (#7580) (#7583)\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\nCo-authored-by: Elena Rastorgueva <80532067+erastorgueva-nv@users.noreply.github.com>\n\n* Add strategy as ddp_find_unused_parameters_true for glue_benchmark.py (#7454) (#7461)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* update strategy (#7577) (#7578)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\n\n* Fix typos (#7581)\n\n* Change hifigan finetune strategy to ddp_find_unused_parameters_true (#7579) (#7584)\n\n* Change strategy to auto\n\n\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\n* [BugFix] Add missing quotes for auto strategy in tutorial notebooks (#7541) (#7548)\n\n* Add missing quotes for auto strategy\n\n\n\n* Revert trainer.gpus to trainer.devices in Self_Supervised_Pre_Training.ipynb\n\n\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nSigned-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* add build os key (#7596) (#7599)\n\n* add build os key\n\n\n\n* add tools\n\n\n\n* update to stable version\n\n\n\n---------\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\n\n* StarCoder SFT test + bump PyT NGC image to 23.09 (#7540)\n\n* Add SFT StarCoder test\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Remove _modify_config call as it is covered in load_from_nemo just below\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Test with pyt:23.09 container\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n---------\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* defaults changed (#7600)\n\n* defaults changed\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* typo\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* update\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n---------\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* add ItalianPhonemesTokenizer (#7587)\n\n* add ItalianPhonemesTokenizer\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix Italian phonemes\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add test\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n---------\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* best ckpt fix (#7564) (#7588)\n\nSigned-off-by: dimapihtar <dpihtar@gmail.com>\nCo-authored-by: Dmytro Pykhtar <37850217+dimapihtar@users.noreply.github.com>\n\n* Add files via upload (#7598)\n\nspecifies the branch\n\nSigned-off-by: George <37293288+Jorjeous@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* Fix validation in G2PModel and ThutmoseTaggerModel (#7597) (#7606)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain (#7576) (#7586)\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Safeguard nemo_text_processing installation on ARM (#7485)\n\n* safeguard nemo_text_processing installing\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update check\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n---------\n\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Bound transformers version in requirements (#7620)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* fix llama2 70b lora tuning bug (#7622)\n\n* fix llama2 70b lora tuning bug\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* Update peft_config.py\n\nbrackets\n\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\n\n---------\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\nCo-authored-by: Adi Renduchintala <adithyare@nvidia.com>\n\n* Fix import error no module name model_utils (#7629)\n\nSigned-off-by: Mehadi Hasan Menon <mehadihasan80@gmail.com>\n\n* add fc large ls models (#7641)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao Koluguri <nithinraok>\n\n* bugfix: trainer.gpus, trainer.strategy, trainer.accelerator (#7621) (#7642)\n\n* [TTS] bugfix for Tacotron2 tutorial due to PTL 2.0\n* trainer.gpus -> trainer.devices\n* fixed related tutorial bugs\n---------\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* fix ssl models ptl monitor val through logging (#7608) (#7614)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* Fix metrics for SE tutorial (#7604) (#7612)\n\nSigned-off-by: Ante Jukic\u0301 <ajukic@nvidia.com>\nCo-authored-by: anteju <108555623+anteju@users.noreply.github.com>\n\n* Add ddp_find_unused_parameters=True and change accelerator to auto (#7623) (#7644)\n\n* Add ddp_find_unused_parameters=True and change acclerator to auto\n\n\n\n* Add ddp_find_unused_parameters True for normalization_as_tagging_train.py\n\n\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* Fix py3.11 dataclasses issue  (#7616)\n\n* Fix py3.11 dataclasses issue  (#7582)\n\n* Update ASR configs to support Python 3.11\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Update TTS configs to support Python 3.11\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Guard MeCab and Ipadic\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix remaining ASR dataclasses\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Fix remaining ASR dataclasses\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Fix scripts\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Update name to ConfidenceMethodConfig\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain (#7576) (#7586)\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Safeguard nemo_text_processing installation on ARM (#7485)\n\n* safeguard nemo_text_processing installing\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update check\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n---------\n\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Fix changes to confidence measure\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Jason <jasoli@nvidia.com>\n\n* [Stable Diffusion/ControlNet] Enable O2 training for SD and Fix ControlNet CI failure\n\n* Mingyuanm/dreambooth fix\n\n* Fix NeMo CI Infer Issue\n\n* DreamFusion\n\n* Move neva export changes\n\n* Add Imagen Synthetic Dataloader\n\n* Add VITWrapper and export stuff to wrapper\n\n* Update neva with megatron-core support\n\n* Fix issues with Dockerfile (#7650) (#7652)\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\n\n* [ASR] RNN-T greedy decoding max_frames fix for alignment and confidence (#7635)\n\n* decoding and test fix\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* [ASR] Fix type error in jasper (#7636) (#7653)\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nCo-authored-by: Ryan Langman <rlangman@nvidia.com>\n\n* [TTS] Add STFT and SI-SDR loss to audio codec recipe (#7468)\n\n* [TTS] Add STFT and SI-SDR loss to audio codec recipe\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Fix STFT resolution\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Fix training metric logging\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Add docstring to mel and stft losses\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Create per.py (#7538)\n\n* Move model precision copy (#7336)\n\n* move cfg precision set to megatron base model\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* remove copy from other models\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* modify attribute not arg\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* fix gpt model test for ptl 2.0\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* rename function and add docstring\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* replace precision to dtype conditionals with func call\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* unnecessary function and cfg reset\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* set default value\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* fix precision lookup in a few more places\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* rename mapping function\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* ununsed import\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* save torch datatype to model\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* set weights precision wrt amp o2\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* Revert \"set weights precision wrt amp o2\"\n\nThis reverts commit 313a4bfe5eb69d771a6d2433898c0685836aef5c.\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* revert half precision at inference attempt\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* move autocast dtype to base model\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* move params dtype to base model, enable fp16 O2 inf\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* unused imports\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n---------\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix PEFT checkpoint loading (#7388)\n\n* Fix PEFT checkpoint loading\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Use distributed optimizer support for multiple dtypes (#7359)\n\n* Update distopt wrapper with multiple dtype support\n\nRemove manual handling of separate FP32 optimizer.\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Use distopt support for contiguous buffers with multiple dtypes\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Fix typo\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Separate distopt buckets for first GPT layer and non-overlapped params\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Add distopt logic for int dtypes\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Update Apex commit\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Remove unused variables\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Update Apex commit in README and Jenkensfile\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Debug Dockerfile and Jenkinsfile\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n---------\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* minor fix for llama ckpt conversion script (#7387)\n\n* minor fix for llama ckpt conversion script\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* Update Jenkinsfile\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* remove fast_swiglu configuration\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n---------\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix wrong calling of librosa.get_duration() in notebook (#7376)\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [PATCH] PEFT import mcore (#7393)\n\n* [PATCH] PEFT import mcore\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Create per.py\n\nScript for calculation Punctuation Error Rate and related rates (correct rate, deletions rate, etc.)\n\nSigned-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [TTS] Added a callback for logging initial data (#7384)\n\nSigned-off-by: Ante Jukic\u0301 <ajukic@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update Core Commit (#7402)\n\n* Update Core Commit\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n* update commit\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n---------\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Use cfg attribute in bert (#7394)\n\n* use cfg attribute instead of arg\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* use torch_dtype in place of cfg.precision\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* move precision copy before super constructor\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* use trainer arg\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n---------\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add support for bias conversion in Swiglu models (#7386)\n\n* Add support for bias conversion in Swiglu models\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Add support for auto extracting tokenizer model\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add support for auto extracting tokenizer model\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Fix issue with missing tokenizer\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Refactor\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Refactor\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update save_to and restore_from for dist checkpointing (#7343)\n\n* add dist ckpt to save to, in progress\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* move dist ckpt\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* clean up\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update restore from, need to figure out how to initialize distributed\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* launch distrib if needed when restoring dist ckpt\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* when using mcore we can change tp pp on the fly\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* add load_from_checkpoint support for dist ckpt\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update llama convert script to save dist .nemo\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* fix load dist ckpt\n\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* setup TE TP groups if needed\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* setup te tp groups if needed\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* remove import\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n---------\n\nSigned-off-by: eharper <eharper@nvidia.com>\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: jasonwan <jasonwan@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* fix forward for with mcore=false (#7403)\n\nSigned-off-by: Jimmy Zhang <jiemingz@nvidia.com>\nCo-authored-by: Jimmy Zhang <jiemingz@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix logging to remove 's/it' from progress bar in Megatron models and add train_step_timing (#7374)\n\n* Add CustomProgressBar class to exp_manager and trainer callbacks\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix the progress bar to reflect total microbatch cnt\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* Modify CustomProgressBar class\n\n1) Modify CustomProgressBar class to update progress bar per global_step instead of per microbatch\n2) Add the callback to other megatron training/finetuning files that are not using MegatronTrainerBuilder\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* Add CustomProgressBar callback to tuning files\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Set Activation Checkpointing Defaults (#7404)\n\n* Set Activation Checkpointing Defaults\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* check for None\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* make loss mask default to false (#7407)\n\nSigned-off-by: eharper <eharper@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add dummy userbuffer config files (#7408)\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* add missing ubconf files (#7412)\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* New tutorial on Speech Data Explorer (#7405)\n\n* Added Google Colab based tutorial on Speech Data Explorer\n\nSigned-off-by: George Zelenfroynd <gzelenfroind@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update ptl training ckpt conversion script to work with dist ckpt (#7416)\n\n* update ptl convert script\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* don't break legacy\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: eharper <eharper@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Allow disabling sanity checking when num_sanity_val_steps=0 (#7413)\n\n* Allow disabling sanity checking when num_sanity_val_steps=0\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* Update num_sanity_val_steps to be a multiple of num_microbatches\n\nSigned-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nSigned-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add comprehensive error messages (#7261)\n\nSigned-off-by: Anton Peganov <apeganov@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* check NEMO_PATH (#7418)\n\nSigned-off-by: Nikolay Karpov <karpnv@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* layer selection for ia3 (#7417)\n\n* layer selection for ia3\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: arendu <adithyare@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix missing pip package 'einops' (#7397)\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix failure of pyaudio in Google Colab (#7396)\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update README.md: output_path --> output_manifest_filepath (#7442)\n\nSigned-off-by: Samuele Cornell <cornellsamuele@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add rope dynamic linear scaling (#7437)\n\n* Add dynamic linear scaling\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix None dataloader issue in PTL2.0 (#7455)\n\n* Fix None dataloader issue in PTL2.0\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* updating values of self._validation_dl and self._test_dl as well\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* updating values of self._validation_dl and self._test_dl as well\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [ASR] Confidence measure -> method renames (#7434)\n\n* measure -> method\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add steps for document of getting dataset 'SF Bilingual Speech' (#7378)\n\n* Add steps for document of getting dataset 'SF Bilingual Speech'\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update datasets.rst\n\nadded a link from a tutorial demonstrating detailed data prep steps.\n\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* RNN-T confidence and alignment bugfix (#7381)\n\n* new frame_confidence and alignments lists are now always created after the while loop\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* tests added\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix resume from checkpoint in exp_manager (#7424) (#7426)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix checking of cuda/cpu device for inputs of Decoder (#7444)\n\n* Fix checking of cuda/cpu device for inputs of Decoder\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update tacotron2.py\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: Jason <jasoli@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix failure of ljspeech's get_data.py (#7430)\n\n* Fix failure of ljspeech's get_data.py\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [TTS] Fix audio codec type checks (#7373)\n\n* [TTS] Fix audio codec type checks\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Fix audio codec tests\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nSigne\u2026"
    },
    {
        "repo_url": "github.com/modelscope/modelscope",
        "filepath": "modelscope/models/multi_modal/video_to_video/modules/embedder.py",
        "commit_date": "2023-08-21T10:44:14Z",
        "message": "add video2video (#486)\n\n* add video2video\n\n* fix bugs of pre-commit\n\n* update some files\n\n* fix video write module\n\n* fix max_frames"
    },
    {
        "repo_url": "github.com/modelscope/modelscope",
        "filepath": "modelscope/models/multi_modal/image_to_video/modules/embedder.py",
        "commit_date": "2023-08-22T02:57:23Z",
        "message": "fix video output of image2video (#488)\n\n* fix video output\n\n* fix logger.error\n\n* fix log error"
    },
    {
        "repo_url": "github.com/mlfoundations/open_flamingo",
        "filepath": "open_flamingo/src/factory.py",
        "commit_date": "2023-08-21T16:17:25Z",
        "message": "Add Support for Specifying Custom `cache_dir` (#245)\n\n* Add support for custom cache_dir\n\n* Lint"
    },
    {
        "repo_url": "github.com/mlfoundations/open_flamingo",
        "filepath": "open_flamingo/src/factory.py",
        "commit_date": "2023-03-26T22:49:44Z",
        "message": "added new eval script that samples from query set (#117)\n\n* added newe eval script that samples from query set\n\n* ran code formatter"
    },
    {
        "repo_url": "github.com/mlfoundations/open_flamingo",
        "filepath": "open_flamingo/src/factory.py",
        "commit_date": "2023-03-19T04:21:18Z",
        "message": "Release fixes (#112)\n\n* improved args and fixed readme example\n\n* fixed data args\n\n* added logging steps arg\n\n* fixed transformers dependency\n\n* fixed transformers dependency\n\n* fix proj vector"
    },
    {
        "repo_url": "github.com/mlfoundations/open_flamingo",
        "filepath": "open_flamingo/src/factory.py",
        "commit_date": "2023-03-12T22:09:37Z",
        "message": "Reintroduce documentation and GitHub workflow changes (#109)\n\n* removed old data code\n\n* ran formatter on data.py"
    },
    {
        "repo_url": "github.com/mlfoundations/open_flamingo",
        "filepath": "open_flamingo/src/factory.py",
        "commit_date": "2023-03-12T21:14:56Z",
        "message": "Revert \"General cleaning up of the repo for Monday's release (#102)\" (#104)\n\nThis reverts commit 50b8023c7e951bf537e5ec0e95c8c4e835dd0ee0."
    },
    {
        "repo_url": "github.com/mlfoundations/open_flamingo",
        "filepath": "open_flamingo/src/factory.py",
        "commit_date": "2023-03-12T06:14:59Z",
        "message": "General cleaning up of the repo for Monday's release (#102)\n\n* removed extra imports and old code\n\n* delete old demo\n\n* add more details to readme\n\n* Add linter action\n\n* Update pylint.yml\n\n* Update pylint.yml\n\n* Create\n\n* Update code-formatter.yml\n\n* Delete pylint.yml\n\n* updated unit tests to use tiny models\n\n* Update tests.yml\n\n* Update tests.yml to install open_flamingo\n\n* Update tests.yml\n\n* Added requests to dev requiremnets\n\n* Update tests.yml\n\n* fixed enviornment for workflow\n\n* added examples to readme\n\n* added flamingo model image\n\n* more readme fixes"
    },
    {
        "repo_url": "github.com/mlfoundations/open_flamingo",
        "filepath": "open_flamingo/src/factory.py",
        "commit_date": "2023-02-15T03:42:28Z",
        "message": "preforward hooks don't take kwargs, so switch to multiple forwards"
    },
    {
        "repo_url": "github.com/mlfoundations/open_flamingo",
        "filepath": "open_flamingo/src/factory.py",
        "commit_date": "2023-02-11T07:54:04Z",
        "message": "support more eleuther models out of the box"
    },
    {
        "repo_url": "github.com/mlfoundations/open_flamingo",
        "filepath": "open_flamingo/src/factory.py",
        "commit_date": "2023-02-11T05:17:52Z",
        "message": "add any hf lm; tested for gpt-neo, opt"
    },
    {
        "repo_url": "github.com/mlfoundations/open_flamingo",
        "filepath": "open_flamingo/src/factory.py",
        "commit_date": "2023-02-09T07:52:13Z",
        "message": "almost pile-compatible: need to reconcile different hidden sizes"
    },
    {
        "repo_url": "github.com/mlfoundations/open_flamingo",
        "filepath": "open_flamingo/src/factory.py",
        "commit_date": "2023-02-08T18:40:21Z",
        "message": "perceiver flag, enforce consistent notation for shapes"
    },
    {
        "repo_url": "github.com/mlfoundations/open_flamingo",
        "filepath": "open_flamingo/src/factory.py",
        "commit_date": "2023-02-07T02:32:35Z",
        "message": "Updated codebase to use LAION+PILE approach (#55)\n\n* pile co-training experiment\n\n* removed resampler\n\n* changed repo structure\n\n* fix train args\n\n* update eval code & add more experiment args to train script\n\n* added evaluation docs\n\n* updated stability scripts\n\n* fixed eval code"
    },
    {
        "repo_url": "github.com/mlfoundations/open_flamingo",
        "filepath": "open_flamingo/src/factory.py",
        "commit_date": "2023-01-09T08:13:04Z",
        "message": "A big codebase restructure (#42)\n\nRestructured codebase and added interleaved format"
    },
    {
        "repo_url": "github.com/microsoft/Cream",
        "filepath": "TinyCLIP/inference.py",
        "commit_date": "2023-10-25T15:21:08Z",
        "message": "[TinyCLIP] Inference Example (#196)\n\n* [TinyCLIP] Inference Example"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama.py",
        "commit_date": "2023-11-18T16:56:22Z",
        "message": "Move base python path from LLaMA2-Accessory/accessory to LLaMA2-Accessory"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama.py",
        "commit_date": "2023-10-17T17:12:30Z",
        "message": "support simplified env, fix demo bug"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama.py",
        "commit_date": "2023-08-04T11:05:54Z",
        "message": "Support loading models trained with different model_parallel_world_size. (#16)\n\n* temp save\n\n* quick fix of demo memory issue\n\n* Refactor tensor creation dtype / device control.\n\nThis commit makes two changes during model creation:\n1. Decouples promote_trainable_params_to_fp32 from model __init__. This\n   is to avoid casting to fp32 to save memory in inference-only mode\n   (#4).\n2. Use a context manager to manage default tensor type change. In the\n   previous version, the default tensor type is reset to\n   torch.FloatTensor after creating the vision model, which is\n   technically incorrect and should be the previous default tensor type\n   instead. We implement our own context manager because the official\n   context managers seem to be incomplete at this time (PyTorch 2.0.1):\n   No dtype manager is provided and set_default_device is ineffective to\n   the torch.Tensor calls which are used in fairscale.\n\n* Change CLIP dtype management in llama.py\n\nIt is probably safer to keep CLIP at its original precision (e.g., fp16)\nregardless of the autocast setting: Some casting (e.g., from fp16 to\nbf16) may be lossy and can potentially harm the pre-trained model.\n\nKeep the changes to llama.py only at this moment since a lot of copy-\npasted codes may be refactored in the future (#3).\n\n* Respect args.precision when saving checkpoints.\n\n* Support checkpoint merge\n\nCheckpoint merge is suported in misc/tensor_parallel.py. Merge requires\nthat the checkpoint_mp_world_size % mp_world_size == 0. Support for\nsplit (i.e., when mp_world_size % checkpoint_mp_world_size == 0) and\nredistribute (for general mp_world_size and checkpoint_mp_world_size\nvalues) will be added in the future.\n\nAlso changing multi_turn demo to use the new loading function with merge\nsupport.\n\n* move printing trainable params\n\n* move training model creation back to cpu\n\nCloses #15, #13"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama.py",
        "commit_date": "2023-07-26T12:46:58Z",
        "message": "70b inference (#2)\n\nsupport kv cache; change attention dispatch; rewrite multi-turn demo"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama.py",
        "commit_date": "2023-07-25T06:23:44Z",
        "message": "refactor data configs to configs/data; add llama adapter"
    },
    {
        "repo_url": "github.com/mlfoundations/open_clip",
        "filepath": "tests/util_test.py",
        "commit_date": "2023-05-12T18:35:41Z",
        "message": "Add EVA models (via timm backbone), torch.compile support, more (#500)\n\n* Add EVA models (via timm backbone), torch.compile support, pure bf16/fp16 mode, safetensors push support\n\n* Fix optional type refinement for torchscript\n\n* Back torchcompile changes out of factory, needs to be closer to use for various reasons\n\n* Fix output_dict + jit regression, remove native OpenAI jit load as it's not working reliably in PyTorch 2.0, always extract state-dict, load model, re-jit (if enabled)"
    },
    {
        "repo_url": "github.com/mlfoundations/open_clip",
        "filepath": "tests/util_test.py",
        "commit_date": "2023-01-29T00:41:42Z",
        "message": "Add coca trained (#307) (#308)\n\n* Add coca trained (#307)\n\n* initial setup\n\n* add coca loss\n\n* remove loss from the model\n\n* fix loss\n\n* add underscores\n\n* name changes\n\n* add cross attention to Residual and CustomResidual\n\n* fix if\n\n* \u00e4dd transformer 'decoder'\n\n* minor fix\n\n* looks better\n\n* initlize coca model structure\n\n* clean\n\n* typo and format\n\n* checkpoint signature\n\n* adjust multimodal decoder and add CoCaTransformer\n\n* keep older logic\n\n* remove chunk\n\n* typo\n\n* fix\n\n* make chunk dim explicit\n\n* adjust cfg names\n\n* add attentionalpooling\n\n* add attentional pooling to coca\n\n* small change\n\n* add cocatransformer variants and AttentionPooling\n\n* remoive older attention pooler\n\n* adapt embed text to coca text transformer\n\n* rm coca layers\n\n* rename and remove useless CoCa models\n\n* make attentionpooler pooler only\n\n* refactor for one transformer only\n\n* coca forward works\n\n* separatae context and n_queries\n\n* add inital coca_base config\n\n* remove config\n\n* small loss change\n\n* init training file\n\n* make variable order right\n\n* remove print\n\n* uniform names\n\n* renaming\n\n* add coca funcs to init\n\n* add coca config and exclude from testing\n\n* add and comment simple test (no trained model)\n\n* add L2 norm\n\n* make L2 same as in clip\n\n* remove unused temperature\n\n* type\n\n* clean\n\n* fix config\n\n* make rename and move cfg\n\n* rename\n\n* temptative add coca to factory\n\n* fix config\n\n* update config\n\n* embed contrastive cls token in model\n\n* remove unused arg\n\n* import create_loss\n\n* make factory accept coca\n\n* make caption loss distributed\n\n* make loss customizable\n\n* pass loss trhough training_epoch\n\n* add coca specific params to params\n\n* removed decoder unused parameters\n\n* remove unused attributes\n\n* adjust coca_config\n\n* fix config and remove unused parameters\n\n* remove comment\n\n* remove more comments\n\n* rename attention pooler\n\n* rename TransformerDecoder\n\n* make AttentionalPooler clearer\n\n* add local loss logic to cocaloss\n\n* only create loss if train in data\n\n* remove wrong file\n\n* fix attentional pooler call\n\n* not ready for testing\n\n* really not ready for testing\n\n* eof lien\n\n* uniform names\n\n* add possible generative loss to evaluate\n\n* change _build function names\n\n* remove wrong import\n\n* remove local_loss from captioning loss\n\n* indexing error\n\n* finish renaming\n\n* adjust configs\n\n* add training test for coca\n\n* simplify captioning loss\n\n* remove hf\n\n* fix evaluate and loss\n\n* remove print\n\n* move projection\n\n* add coca vit 32 config\n\n* test on new config\n\n* adjust coca_base config\n\n* remove coca from test_inference\n\n* maybe fix regression test\n\n* make logits and labels contiguous\n\n* simpler logic\n\n* make contiguous after transpose\n\n* last test\n\n* try fix loss\n\n* CoCa PR: loss fix + rename file\n\n* wait for feedback on this\n\n* cleanup\n\n* CoCa PR: add set_grad_checkpointing + fix checkpoint API\n\n* CoCa PR: fix eval (which uses encode_x instead of forward)\n\n* move making space for CLS token into encode_text\n\n* rever zs changes + fix\n\nCo-authored-by: gpucce <g.puccetti92@gmail.com>\nCo-authored-by: gpucce <g.puccetti@gmail.com>\nCo-authored-by: iejmac <iejmac@ip-172-31-44-155.ec2.internal>\n\n* Add coca to CI\n\n* Add coca to CI pr\n\n* simplify encode_iamge (#313)\n\nCo-authored-by: Romain Beaumont <romain.rom1@gmail.com>\n\n* Add cls mask (#312)\n\n* buil_cls_mask\n\n* add cls_mask to encode_text\n\n* add model properties\n\nCo-authored-by: Romain Beaumont <romain.rom1@gmail.com>\nCo-authored-by: gpucce <g.puccetti@gmail.com>\n\n* Ignore pad tokens in captioning loss (#316)\n\n* add ignore_index\n\n* just need to pick right index\n\nCo-authored-by: gpucce <g.puccetti@gmail.com>\n\n* add `generate` to coca model (#314)\n\n* add initial generative support\n\n* make generation context_length independend\n\n* remove kwargs\n\n* last positional embeddings for CLS\n\n* typo\n\n* fix mask len\n\n* add comment\n\n* remove unused args\n\n* simpler logic for input shorter than context length\n\nCo-authored-by: gpucce <g.puccetti@gmail.com>\n\n* use `TextEncoder` in coca `encode_image` (#321)\n\n* use self.text in encode image\n\n* unused var\n\n* rever aAtention and CustoResidualAttentionBlock\n\n* remove whiteline\n\n* add dict output\n\n* bintegrate self.text attributes\n\n* HF compatibility\n\n* better config and minor fixes\n\n* clean\n\n* remove eembed_cls option from HF\n\n* use cls_token_position\n\n* fix cls masking\n\n* resize labels\n\n* text -> self.text\n\n* split loss logging\n\n* add total loss\n\n* minor logs formatting\n\n* fix generate\n\n* simpler logic\n\n* disentangle proj for HF too\n\n* adjust config\n\n* only norm cls\n\n* move attn_pool to VisionTransformer\n\n* adjust coca_base config\n\n* fix grad checkpointing in MultimodalTransformer\n\nCo-authored-by: gpucce <g.puccetti@gmail.com>\nCo-authored-by: iejMac <kilianmaciej6@gmail.com>\n\n* Get some basic PEP changes out of the way\n\n* Add tests bis (#355)\n\n* make jit compilable\n\n* redundant annotation\n\n* less tests\n\n* less annotations\n\n* even less annotations\n\n* fix name check in ci\n\n* some annotations back\n\n* make it simpler\n\n* make hf simpler too\n\n* better jit support with tests\n\n* remove extra line\n\n* add customtextclip\n\n* more jit tests\n\n* missing assert\n\n* add eval\n\n* typo\n\n* rever forward changes\n\n* clean coca model\n\n* more cleaning\n\n* last cleaning\n\n* train.py: fix is_clip when doing distributed (#364)\n\n* add README (#365)\n\n* add README\n\n* multimodal_cfg info\n\n* multimodal\n\n* remove output_dict argument (#368)\n\n* remove output_dict argument\n\n* cleaner\n\n* do same thing for _encode_image (#366)\n\n* do same thing for _encode_image\n\n* encoder\n\n* try this\n\n* adjust inference tests\n\n* fix syntax\n\n* True not None\n\n* dumb\n\n* CoCa/forward: remove unused output_dict param\n\n* Revert \"do same thing for _encode_image (#366)\"\n\nThis reverts commit de343fb73e9512c63bcbf3d902359c652580aef0.\n\n* refactor\n\n* white space\n\n* remove extra layer norm\n\n* move to_logits into decoder\n\n* leave for later\n\n* better torchscript\n\n* annotate hf too\n\n* Add CoCa-ViT-L/14 config (#379)\n\n* Remove dead LN code, refactor attn_pool conditional for more clarity, minor formatting tweaks\n\n* latent_dim to embed_dim\n\n* remove extra cfg\n\n* A bit more cleanup, keep context_length as context len, 'num_pos' to incl extra tokens. None type check for embed_cls instead of getattr\n\n* CoCa: add B/32 pretrained (#389)\n\n* add B/32 pretrained\n\n* fix\n\n* no capital\n\n* slash\n\n* remove coca from ci.yml\n\n---------\n\nCo-authored-by: gpucce <g.puccetti92@gmail.com>\nCo-authored-by: gpucce <g.puccetti@gmail.com>\nCo-authored-by: iejmac <iejmac@ip-172-31-44-155.ec2.internal>\nCo-authored-by: iejMac <kilianmaciej6@gmail.com>\nCo-authored-by: Ross Wightman <rwightman@gmail.com>"
    },
    {
        "repo_url": "github.com/mlfoundations/open_clip",
        "filepath": "tests/util_test.py",
        "commit_date": "2022-12-09T00:40:09Z",
        "message": "CI: on-the-fly data generation for regression test determinism (#260)\n\n* CI on-the-fly data generation for regression tests\n\n* delete old reg test data\n\n* util_test.py option to create test data for specific git revision\nskip instead of fail regression tests if data not found\nregister pytest markers in pytest.ini\n\n* use sha instead of branch name if on a detached HEAD\nfixed typo\n\n* CI: save model list util_test to ensure no model names leak from test to reference\n\n* CI: use marker instead of name filter to collect model names from pytest\n\n* CI: use manual revision for checkout action\n\n* util_test: avoid nested exception to make sure working tree is being restored on failure\n\n* cache venv with minor version specific python\nensure durations file exists\n\n* only pop stash if changes were stashed\n\n* keep naming for env and durations cache the same\n\nCo-authored-by: Romain Beaumont <romain.rom1@gmail.com>"
    },
    {
        "repo_url": "github.com/mlfoundations/open_clip",
        "filepath": "tests/util_test.py",
        "commit_date": "2022-11-20T00:03:34Z",
        "message": "create HF models without pretrained weights (#235)\n\nHF model inference tests on random models"
    },
    {
        "repo_url": "github.com/mlfoundations/open_clip",
        "filepath": "tests/util_test.py",
        "commit_date": "2022-11-17T17:32:24Z",
        "message": "Parallelize CI tests (#228)\n\n* parallelize CI tests\n\n* disable macos testing for now, as it's slow\n\n* typo\n\n* util_test only import argparse if called as script"
    },
    {
        "repo_url": "github.com/mlcommons/training",
        "filepath": "stable_diffusion/ldm/models/clip_encoder.py",
        "commit_date": "2023-06-15T00:15:33Z",
        "message": "Added Stable Diffusion (SD) benchmark (#656)"
    },
    {
        "repo_url": "github.com/mlfoundations/open_clip",
        "filepath": "tests/test_inference.py",
        "commit_date": "2023-01-29T00:41:42Z",
        "message": "Add coca trained (#307) (#308)\n\n* Add coca trained (#307)\n\n* initial setup\n\n* add coca loss\n\n* remove loss from the model\n\n* fix loss\n\n* add underscores\n\n* name changes\n\n* add cross attention to Residual and CustomResidual\n\n* fix if\n\n* \u00e4dd transformer 'decoder'\n\n* minor fix\n\n* looks better\n\n* initlize coca model structure\n\n* clean\n\n* typo and format\n\n* checkpoint signature\n\n* adjust multimodal decoder and add CoCaTransformer\n\n* keep older logic\n\n* remove chunk\n\n* typo\n\n* fix\n\n* make chunk dim explicit\n\n* adjust cfg names\n\n* add attentionalpooling\n\n* add attentional pooling to coca\n\n* small change\n\n* add cocatransformer variants and AttentionPooling\n\n* remoive older attention pooler\n\n* adapt embed text to coca text transformer\n\n* rm coca layers\n\n* rename and remove useless CoCa models\n\n* make attentionpooler pooler only\n\n* refactor for one transformer only\n\n* coca forward works\n\n* separatae context and n_queries\n\n* add inital coca_base config\n\n* remove config\n\n* small loss change\n\n* init training file\n\n* make variable order right\n\n* remove print\n\n* uniform names\n\n* renaming\n\n* add coca funcs to init\n\n* add coca config and exclude from testing\n\n* add and comment simple test (no trained model)\n\n* add L2 norm\n\n* make L2 same as in clip\n\n* remove unused temperature\n\n* type\n\n* clean\n\n* fix config\n\n* make rename and move cfg\n\n* rename\n\n* temptative add coca to factory\n\n* fix config\n\n* update config\n\n* embed contrastive cls token in model\n\n* remove unused arg\n\n* import create_loss\n\n* make factory accept coca\n\n* make caption loss distributed\n\n* make loss customizable\n\n* pass loss trhough training_epoch\n\n* add coca specific params to params\n\n* removed decoder unused parameters\n\n* remove unused attributes\n\n* adjust coca_config\n\n* fix config and remove unused parameters\n\n* remove comment\n\n* remove more comments\n\n* rename attention pooler\n\n* rename TransformerDecoder\n\n* make AttentionalPooler clearer\n\n* add local loss logic to cocaloss\n\n* only create loss if train in data\n\n* remove wrong file\n\n* fix attentional pooler call\n\n* not ready for testing\n\n* really not ready for testing\n\n* eof lien\n\n* uniform names\n\n* add possible generative loss to evaluate\n\n* change _build function names\n\n* remove wrong import\n\n* remove local_loss from captioning loss\n\n* indexing error\n\n* finish renaming\n\n* adjust configs\n\n* add training test for coca\n\n* simplify captioning loss\n\n* remove hf\n\n* fix evaluate and loss\n\n* remove print\n\n* move projection\n\n* add coca vit 32 config\n\n* test on new config\n\n* adjust coca_base config\n\n* remove coca from test_inference\n\n* maybe fix regression test\n\n* make logits and labels contiguous\n\n* simpler logic\n\n* make contiguous after transpose\n\n* last test\n\n* try fix loss\n\n* CoCa PR: loss fix + rename file\n\n* wait for feedback on this\n\n* cleanup\n\n* CoCa PR: add set_grad_checkpointing + fix checkpoint API\n\n* CoCa PR: fix eval (which uses encode_x instead of forward)\n\n* move making space for CLS token into encode_text\n\n* rever zs changes + fix\n\nCo-authored-by: gpucce <g.puccetti92@gmail.com>\nCo-authored-by: gpucce <g.puccetti@gmail.com>\nCo-authored-by: iejmac <iejmac@ip-172-31-44-155.ec2.internal>\n\n* Add coca to CI\n\n* Add coca to CI pr\n\n* simplify encode_iamge (#313)\n\nCo-authored-by: Romain Beaumont <romain.rom1@gmail.com>\n\n* Add cls mask (#312)\n\n* buil_cls_mask\n\n* add cls_mask to encode_text\n\n* add model properties\n\nCo-authored-by: Romain Beaumont <romain.rom1@gmail.com>\nCo-authored-by: gpucce <g.puccetti@gmail.com>\n\n* Ignore pad tokens in captioning loss (#316)\n\n* add ignore_index\n\n* just need to pick right index\n\nCo-authored-by: gpucce <g.puccetti@gmail.com>\n\n* add `generate` to coca model (#314)\n\n* add initial generative support\n\n* make generation context_length independend\n\n* remove kwargs\n\n* last positional embeddings for CLS\n\n* typo\n\n* fix mask len\n\n* add comment\n\n* remove unused args\n\n* simpler logic for input shorter than context length\n\nCo-authored-by: gpucce <g.puccetti@gmail.com>\n\n* use `TextEncoder` in coca `encode_image` (#321)\n\n* use self.text in encode image\n\n* unused var\n\n* rever aAtention and CustoResidualAttentionBlock\n\n* remove whiteline\n\n* add dict output\n\n* bintegrate self.text attributes\n\n* HF compatibility\n\n* better config and minor fixes\n\n* clean\n\n* remove eembed_cls option from HF\n\n* use cls_token_position\n\n* fix cls masking\n\n* resize labels\n\n* text -> self.text\n\n* split loss logging\n\n* add total loss\n\n* minor logs formatting\n\n* fix generate\n\n* simpler logic\n\n* disentangle proj for HF too\n\n* adjust config\n\n* only norm cls\n\n* move attn_pool to VisionTransformer\n\n* adjust coca_base config\n\n* fix grad checkpointing in MultimodalTransformer\n\nCo-authored-by: gpucce <g.puccetti@gmail.com>\nCo-authored-by: iejMac <kilianmaciej6@gmail.com>\n\n* Get some basic PEP changes out of the way\n\n* Add tests bis (#355)\n\n* make jit compilable\n\n* redundant annotation\n\n* less tests\n\n* less annotations\n\n* even less annotations\n\n* fix name check in ci\n\n* some annotations back\n\n* make it simpler\n\n* make hf simpler too\n\n* better jit support with tests\n\n* remove extra line\n\n* add customtextclip\n\n* more jit tests\n\n* missing assert\n\n* add eval\n\n* typo\n\n* rever forward changes\n\n* clean coca model\n\n* more cleaning\n\n* last cleaning\n\n* train.py: fix is_clip when doing distributed (#364)\n\n* add README (#365)\n\n* add README\n\n* multimodal_cfg info\n\n* multimodal\n\n* remove output_dict argument (#368)\n\n* remove output_dict argument\n\n* cleaner\n\n* do same thing for _encode_image (#366)\n\n* do same thing for _encode_image\n\n* encoder\n\n* try this\n\n* adjust inference tests\n\n* fix syntax\n\n* True not None\n\n* dumb\n\n* CoCa/forward: remove unused output_dict param\n\n* Revert \"do same thing for _encode_image (#366)\"\n\nThis reverts commit de343fb73e9512c63bcbf3d902359c652580aef0.\n\n* refactor\n\n* white space\n\n* remove extra layer norm\n\n* move to_logits into decoder\n\n* leave for later\n\n* better torchscript\n\n* annotate hf too\n\n* Add CoCa-ViT-L/14 config (#379)\n\n* Remove dead LN code, refactor attn_pool conditional for more clarity, minor formatting tweaks\n\n* latent_dim to embed_dim\n\n* remove extra cfg\n\n* A bit more cleanup, keep context_length as context len, 'num_pos' to incl extra tokens. None type check for embed_cls instead of getattr\n\n* CoCa: add B/32 pretrained (#389)\n\n* add B/32 pretrained\n\n* fix\n\n* no capital\n\n* slash\n\n* remove coca from ci.yml\n\n---------\n\nCo-authored-by: gpucce <g.puccetti92@gmail.com>\nCo-authored-by: gpucce <g.puccetti@gmail.com>\nCo-authored-by: iejmac <iejmac@ip-172-31-44-155.ec2.internal>\nCo-authored-by: iejMac <kilianmaciej6@gmail.com>\nCo-authored-by: Ross Wightman <rwightman@gmail.com>"
    },
    {
        "repo_url": "github.com/mlfoundations/open_clip",
        "filepath": "tests/test_inference.py",
        "commit_date": "2022-12-29T07:17:23Z",
        "message": "Add xxlarge convnext and fix timm import for current pre-release of timm"
    },
    {
        "repo_url": "github.com/mlfoundations/open_clip",
        "filepath": "tests/test_inference.py",
        "commit_date": "2022-12-09T00:40:09Z",
        "message": "CI: on-the-fly data generation for regression test determinism (#260)\n\n* CI on-the-fly data generation for regression tests\n\n* delete old reg test data\n\n* util_test.py option to create test data for specific git revision\nskip instead of fail regression tests if data not found\nregister pytest markers in pytest.ini\n\n* use sha instead of branch name if on a detached HEAD\nfixed typo\n\n* CI: save model list util_test to ensure no model names leak from test to reference\n\n* CI: use marker instead of name filter to collect model names from pytest\n\n* CI: use manual revision for checkout action\n\n* util_test: avoid nested exception to make sure working tree is being restored on failure\n\n* cache venv with minor version specific python\nensure durations file exists\n\n* only pop stash if changes were stashed\n\n* keep naming for env and durations cache the same\n\nCo-authored-by: Romain Beaumont <romain.rom1@gmail.com>"
    },
    {
        "repo_url": "github.com/mlfoundations/open_clip",
        "filepath": "tests/test_inference.py",
        "commit_date": "2022-11-21T22:03:27Z",
        "message": "hf_model.py: support encoder-decoder models (#239)\n\n* hf_model.py: support encoder-decoder models\n\n* populate configs with some values\n\n* update\n\n* add test data and missing deps\n\n* make context_length blank for mt5\n\n* add mt5 xl\n\n* remove xl from tests\n\nCo-authored-by: Romain Beaumont <romain.rom1@gmail.com>"
    },
    {
        "repo_url": "github.com/mlfoundations/open_clip",
        "filepath": "tests/test_inference.py",
        "commit_date": "2022-11-20T00:03:34Z",
        "message": "create HF models without pretrained weights (#235)\n\nHF model inference tests on random models"
    },
    {
        "repo_url": "github.com/mlfoundations/open_clip",
        "filepath": "tests/test_inference_simple.py",
        "commit_date": "2023-01-30T22:38:24Z",
        "message": "Cover jit and force_custom_text in simple tests"
    },
    {
        "repo_url": "github.com/mlfoundations/open_clip",
        "filepath": "tests/test_inference_simple.py",
        "commit_date": "2022-11-13T10:51:10Z",
        "message": "Add roberta base ViT-B/32 pretrained. (#221)\n\n* Add roberta base ViT-B/32 pretrained.\n\nA ViT-B/32 with roberta base encoder with a 61.7% top-1 ImageNet-1k zero-shot was trained on stability. See model details here https://huggingface.co/laion/CLIP-ViT-B-32-roberta-base-laion2B-s12B-b32k\nThis is the first openclip model using a HF text tower. It has better performance on a range of tasks compared to the standard text encoder, see [metrics](https://huggingface.co/laion/CLIP-ViT-B-32-roberta-base-laion2B-s12B-b32k/blob/main/unknown.png)\n\n* test"
    },
    {
        "repo_url": "github.com/mlfoundations/open_clip",
        "filepath": "tests/test_inference_simple.py",
        "commit_date": "2022-11-10T10:23:24Z",
        "message": "training/params.py: remove hf params and get them from model config (#215)\n\n* zero_shot.py: set correct tokenizer based on args\n\n* training/params.py: remove HF specific params, get those automatically from config\n\n* set tokenizer in PreTrainedTextEncoder\n\n* fix CsvDataset\n\n* take tokenizer name from tokenizer\n\n* Fix\n\n* add tok name to test\n\n* None\n\n* update\n\n* no need to store this anymore\n\n* upadte README\n\n* add tokenizer attribute\n\n* remove useless code\n\n* update example\n\n* use model.tokenizer in test inference simple\n\n* trying getattr\n\n* syntax\n\n* get_tokenizer\n\n* factory fix\n\n* fix test\n\n* fix\n\n* fix'\n\n* add get-tokenizer to README + update exmaple\n\n* update README\n\n* zero-shot get_tokenizer\n\nCo-authored-by: Romain Beaumont <romain.rom1@gmail.com>\nCo-authored-by: iejmac <iejmac@gpu-st-p4d-24xlarge-4.hpc-1click-sandbox.pcluster>"
    },
    {
        "repo_url": "github.com/mlfoundations/open_clip",
        "filepath": "tests/test_inference_simple.py",
        "commit_date": "2022-11-07T18:58:43Z",
        "message": "Implement simple training test. (#203)\n\nOnly runs the training, no actual check except no crashes.\n\nFor #198"
    },
    {
        "repo_url": "github.com/mlfoundations/open_clip",
        "filepath": "tests/test_download_pretrained.py",
        "commit_date": "2023-01-23T23:59:33Z",
        "message": "Fetch from Hugging Face Hub using hf_hub: prefix"
    },
    {
        "repo_url": "github.com/mlfoundations/open_clip",
        "filepath": "tests/test_download_pretrained.py",
        "commit_date": "2022-11-07T13:34:49Z",
        "message": "Add checksum verification for pretrained model weights downloaded from mlfoundations github releases url (#145)"
    },
    {
        "repo_url": "github.com/mlcommons/training",
        "filepath": "stable_diffusion/ldm/modules/encoders/modules.py",
        "commit_date": "2023-06-21T22:07:16Z",
        "message": "Added Stable Diffusion (SD) benchmark - Part 2 (#661)"
    },
    {
        "repo_url": "github.com/mlcommons/training",
        "filepath": "stable_diffusion/ldm/modules/encoders/modules.py",
        "commit_date": "2023-06-15T00:15:33Z",
        "message": "Added Stable Diffusion (SD) benchmark (#656)"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_ens.py",
        "commit_date": "2023-11-28T08:12:22Z",
        "message": "Fix SPHINX inference memory with image input (#116)\n\n* fix embedding precision w/ image to save memory\n\n* pin gradio version"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_ens.py",
        "commit_date": "2023-11-23T17:14:43Z",
        "message": "fix sphinx sphinx-1k sphinx-2k quant (#113)"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_ens.py",
        "commit_date": "2023-11-18T16:56:22Z",
        "message": "Move base python path from LLaMA2-Accessory/accessory to LLaMA2-Accessory"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_ens.py",
        "commit_date": "2023-11-07T15:48:47Z",
        "message": "llama ens 10 ens5p2 pose demo"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_ens.py",
        "commit_date": "2023-10-17T17:12:30Z",
        "message": "support simplified env, fix demo bug"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_ens.py",
        "commit_date": "2023-10-16T17:24:11Z",
        "message": "update demo & set llama_ens default to 13B"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_ens.py",
        "commit_date": "2023-10-14T14:06:04Z",
        "message": "sort data list during dataset init for efficiency"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_peft.py",
        "commit_date": "2023-11-18T16:56:22Z",
        "message": "Move base python path from LLaMA2-Accessory/accessory to LLaMA2-Accessory"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_peft.py",
        "commit_date": "2023-10-17T17:12:30Z",
        "message": "support simplified env, fix demo bug"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_peft.py",
        "commit_date": "2023-08-04T11:05:54Z",
        "message": "Support loading models trained with different model_parallel_world_size. (#16)\n\n* temp save\n\n* quick fix of demo memory issue\n\n* Refactor tensor creation dtype / device control.\n\nThis commit makes two changes during model creation:\n1. Decouples promote_trainable_params_to_fp32 from model __init__. This\n   is to avoid casting to fp32 to save memory in inference-only mode\n   (#4).\n2. Use a context manager to manage default tensor type change. In the\n   previous version, the default tensor type is reset to\n   torch.FloatTensor after creating the vision model, which is\n   technically incorrect and should be the previous default tensor type\n   instead. We implement our own context manager because the official\n   context managers seem to be incomplete at this time (PyTorch 2.0.1):\n   No dtype manager is provided and set_default_device is ineffective to\n   the torch.Tensor calls which are used in fairscale.\n\n* Change CLIP dtype management in llama.py\n\nIt is probably safer to keep CLIP at its original precision (e.g., fp16)\nregardless of the autocast setting: Some casting (e.g., from fp16 to\nbf16) may be lossy and can potentially harm the pre-trained model.\n\nKeep the changes to llama.py only at this moment since a lot of copy-\npasted codes may be refactored in the future (#3).\n\n* Respect args.precision when saving checkpoints.\n\n* Support checkpoint merge\n\nCheckpoint merge is suported in misc/tensor_parallel.py. Merge requires\nthat the checkpoint_mp_world_size % mp_world_size == 0. Support for\nsplit (i.e., when mp_world_size % checkpoint_mp_world_size == 0) and\nredistribute (for general mp_world_size and checkpoint_mp_world_size\nvalues) will be added in the future.\n\nAlso changing multi_turn demo to use the new loading function with merge\nsupport.\n\n* move printing trainable params\n\n* move training model creation back to cpu\n\nCloses #15, #13"
    },
    {
        "repo_url": "github.com/modelscope/modelscope",
        "filepath": "modelscope/models/multi_modal/video_synthesis/text_to_video_synthesis_model.py",
        "commit_date": "2024-02-23T09:05:15Z",
        "message": "fix text_to_video_synthesis_model device (#751)\n\nCo-authored-by: slin000111 <zhaoshilin2015@sina.con>"
    },
    {
        "repo_url": "github.com/modelscope/modelscope",
        "filepath": "modelscope/models/multi_modal/video_synthesis/text_to_video_synthesis_model.py",
        "commit_date": "2023-08-15T04:01:03Z",
        "message": "VideoComposer: Compositional Video Synthesis with Motion Controllability (#431)\n\n* VideoComposer: Compositional Video Synthesis with Motion Controllability\n\n* videocomposer pipeline\n\n* pre commit\n\n* delete xformers"
    },
    {
        "repo_url": "github.com/modelscope/modelscope",
        "filepath": "modelscope/models/multi_modal/video_synthesis/text_to_video_synthesis_model.py",
        "commit_date": "2023-07-14T08:22:10Z",
        "message": "add parameters height and width for text-to-video \n\nLink: https://code.alibaba-inc.com/Ali-MaaS/MaaS-lib/codereview/13171907"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_ens5.py",
        "commit_date": "2023-11-28T08:12:22Z",
        "message": "Fix SPHINX inference memory with image input (#116)\n\n* fix embedding precision w/ image to save memory\n\n* pin gradio version"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_ens5.py",
        "commit_date": "2023-11-23T17:14:43Z",
        "message": "fix sphinx sphinx-1k sphinx-2k quant (#113)"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_ens5.py",
        "commit_date": "2023-11-18T16:56:22Z",
        "message": "Move base python path from LLaMA2-Accessory/accessory to LLaMA2-Accessory"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_ens5.py",
        "commit_date": "2023-11-07T15:48:47Z",
        "message": "llama ens 10 ens5p2 pose demo"
    },
    {
        "repo_url": "github.com/openvinotoolkit/anomalib",
        "filepath": "src/anomalib/models/image/winclip/torch_model.py",
        "commit_date": "2024-02-27T13:43:29Z",
        "message": "\ud83d\udd28 Rename DynamicBufferModule to DynamicBufferMixin (#1776)\n\n* Rename DynamicBufferModule to DynamicBufferMixin\n\n* Add unit tests for DynamicBufferMixin\n\n* Add logs to the changelog\n\n* Add minor changes to the comments"
    },
    {
        "repo_url": "github.com/openvinotoolkit/anomalib",
        "filepath": "src/anomalib/models/image/winclip/torch_model.py",
        "commit_date": "2024-01-24T12:19:17Z",
        "message": "\ud83d\ude80 V1 (#1663)\n\n* Merge feature/lightning-version-upgrade to feature/custom-trainer (#1297)\n\nUpgrade to Lightning 2.0.5 (#1221)\n\n* Adapt code and configs to PL 2.0.5\n\n* Pre-commit checks.\n\n* Fix a function call.\n\n* Fix function calls.\n\n* pytorch_lightning -> lightning.pytorch\n\n* Add lightning to requirements\n\n---------\n\nCo-authored-by: Weilin Xu <mzweilin@gmail.com>\nCo-authored-by: Samet <samet.akcay@intel.com>\n\n* Partially restore tests (#1298)\n\n* Upgrade to Lightning 2.0.5 (#1221)\n\n* Adapt code and configs to PL 2.0.5\n\n* Pre-commit checks.\n\n* Fix a function call.\n\n* Fix function calls.\n\n* pytorch_lightning -> lightning.pytorch\n\n* Add lightning to requirements\n\n---------\n\nCo-authored-by: Samet <samet.akcay@intel.com>\n\n* partially restore tests\n\n* Address PR comments\n\n---------\n\nCo-authored-by: Weilin Xu <mzweilin@gmail.com>\nCo-authored-by: Samet <samet.akcay@intel.com>\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\n\n* [Custom Trainer] Remove future annotations (#1299)\n\n* remove __future__\n\n* Update changelog\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\n\n* [Custom Trainer] Refactor postprocessing (#1302)\n\n* remove __future__\n\n* Update changelog\n\n* \ud83d\ude9a Trainer -> AnomalibTrainer\n\n* add post-processor\n\n* Refactor callback\n\n* Remove handler\n\n* Address PR comments\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\n\n* [Custom Trainer] refactor normalization callbacks (#1310)\n\n* remove __future__\n\n* Update changelog\n\n* \ud83d\ude9a Trainer -> AnomalibTrainer\n\n* add post-processor\n\n* Refactor callback\n\n* Refactor normalization callback\n\n* Match convention\n\n* Refactor imports\n\n* Address PR comments\n\n* Fix path\n\n* Refactor callbacks\n\n* Fix module path\n\n---------\n\n* [Custom Trainer] Refactor thresholding (#1311)\n\n* remove __future__\n\n* Update changelog\n\n* \ud83d\ude9a Trainer -> AnomalibTrainer\n\n* add post-processor\n\n* Refactor callback\n\n* Refactor normalization callback\n\n* Refactor thresholding\n\n* Match convention\n\n* Refactor imports\n\n* Address PR comments\n\n* Fix path\n\n* Refactor callbacks\n\n* Fix module path\n\n* Address PR comments\n\n* Apply suggestions from code review\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n---------\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* [Custom Trainer] Refactor Metrics (#1312)\n\n* remove __future__\n\n* Update changelog\n\n* \ud83d\ude9a Trainer -> AnomalibTrainer\n\n* add post-processor\n\n* Refactor callback\n\n* Refactor normalization callback\n\n* Refactor thresholding\n\n* Refactor metrics configuration\n\n* Match convention\n\n* Refactor imports\n\n* Address PR comments\n\n* Fix path\n\n* Refactor callbacks\n\n* Fix module path\n\n* Address PR comments\n\n* Address PR comments\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\n\n* [Custom Trainer] Refactor visualization callback (#1313)\n\n* remove __future__\n\n* Update changelog\n\n* \ud83d\ude9a Trainer -> AnomalibTrainer\n\n* add post-processor\n\n* Refactor callback\n\n* Refactor normalization callback\n\n* Refactor thresholding\n\n* Refactor metrics configuration\n\n* Move visualizer callbacks into trainer\n\n* Match convention\n\n* Refactor imports\n\n* Address PR comments\n\n* Fix path\n\n* Refactor callbacks\n\n* Fix module path\n\n* Address PR comments\n\n* Remove comment\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\n\n* [Custom Trainer] Initial engine design (#1348)\n\n* Initial engine design\n\n* Address PR comments\n\n* Circular import + trainer->engine\n\n* Update src/anomalib/engine/engine.py\n\n* revert import in __init__\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* [Custom Trainer] Partially fix tests (#1359)\n\n* Partially fix test\n\n* Address PR comments\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\n\n* [Custom Trainer] Add CLI support (#1381)\n\n* Support trainer methods\n\n* support yaml serialization\n\n* add hpo command\n\n* Add benchmark + train entrypoint\n\n* Add export arguments\n\n* Partially address PR comments\n\n* Add export subcommands + refactor\n\n* Address PR comments\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\n\n* [Custom Trainer] Partially restore tests (#1391)\n\n* Fix tests\n\n* Patch get experiment logger\n\n* Sort imports\n\n* Add stfpm config\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\n\n* Add new ruff rules (#1390)\n\n* Add rules to pyproject.toml file\n\n* Only include padim and stfpm in tests\n\n* Fix notebook tests\n\n* Fix notebook tests\n\n* Code quality/enable rules (#1394)\n\n* pyflakes\n\n* pycodestyle\n\n* pep8-naming (`N`)\n\n* Enable Ruff rules - Part III (#1397)\n\n* pyflakes\n\n* pycodestyle\n\n* pep8-naming (`N`)\n\n* pyupgrade (`UP`)\n\n* flake8-bandit (`S`)\n\n* Enabled UP, ANN, S, BLE, FBT, B\n\n* Fix typo\n\n* Revert F1AdaptiveThreshold parent classes\n\n* Fix some of the tests\n\n* ignore boolean-positional-value-in-call (FBT003)\n\n* Addressed pr comments\n\n* Remove duplicated lines\n\n* Enable Ruff Rules - Part 4 (#1402)\n\n* pyflakes\n\n* pycodestyle\n\n* pep8-naming (`N`)\n\n* pyupgrade (`UP`)\n\n* flake8-bandit (`S`)\n\n* Enabled UP, ANN, S, BLE, FBT, B\n\n* Fix typo\n\n* Revert F1AdaptiveThreshold parent classes\n\n* Fix some of the tests\n\n* ignore boolean-positional-value-in-call (FBT003)\n\n* Addressed pr comments\n\n* Remove duplicated lines\n\n* flake8-builtins (`A`) and flake8-commas (`COM`)\n\n* flake8-comprehensions (`C4`)\n\n* flake8-datatimez (`DTZ`), flake8-debugger (`T10`), flake8-errmsg (`EM`)\n\n* [Custom Trainer] Switch to manual optimization for ganomaly (#1404)\n\n* implement manual optimizers for ganomaly\n\n* cleanup\n\n* Enable Ruff Rules - Part 5 (#1403)\n\n* pyflakes\n\n* pycodestyle\n\n* pep8-naming (`N`)\n\n* pyupgrade (`UP`)\n\n* flake8-bandit (`S`)\n\n* Enabled UP, ANN, S, BLE, FBT, B\n\n* Fix typo\n\n* Revert F1AdaptiveThreshold parent classes\n\n* Fix some of the tests\n\n* ignore boolean-positional-value-in-call (FBT003)\n\n* Addressed pr comments\n\n* Remove duplicated lines\n\n* flake8-builtins (`A`) and flake8-commas (`COM`)\n\n* flake8-comprehensions (`C4`)\n\n* flake8-datatimez (`DTZ`), flake8-debugger (`T10`), flake8-errmsg (`EM`)\n\n* flake8-pie (`PIE`)\n\n* flake8-raise (`RSE`)\n\n* flake8-return (`RET`)\n\n* flake8-self (`SLF`)\n\n* flake8-simplify (`SIM`)\n\n* flake8-unsused-arguments (`ARG`)\n\n* Update src/anomalib/models/components/flow/all_in_one_block.py\n\nCo-authored-by: Ashwin Vaidya <ashwinnitinvaidya@gmail.com>\n\n* addressed pr comments\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinnitinvaidya@gmail.com>\n\n* [Custom Trainer] Add import checks (#1393)\n\n* Add checks\n\n* Add checks for wandb\n\n* move exception handling to method\n\n* fix pre-commit issue\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\n\n* [Custom Trainer] Remove max epochs > 1 (#1400)\n\nRemove max epochs>1 from default param list\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\n\n* [Custom Trainer] Add default values (#1395)\n\n* Add checks\n\n* Add default values to datasets + padim model\n\n* update default values\n\n* Remove merge artifact\n\n* Change gt path\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\n\n* Enable Ruff Rules - Part 6 (#1407)\n\n* pyflakes\n\n* pycodestyle\n\n* pep8-naming (`N`)\n\n* pyupgrade (`UP`)\n\n* flake8-bandit (`S`)\n\n* Enabled UP, ANN, S, BLE, FBT, B\n\n* Fix typo\n\n* Revert F1AdaptiveThreshold parent classes\n\n* Fix some of the tests\n\n* ignore boolean-positional-value-in-call (FBT003)\n\n* Addressed pr comments\n\n* Remove duplicated lines\n\n* flake8-builtins (`A`) and flake8-commas (`COM`)\n\n* flake8-comprehensions (`C4`)\n\n* flake8-datatimez (`DTZ`), flake8-debugger (`T10`), flake8-errmsg (`EM`)\n\n* flake8-pie (`PIE`)\n\n* flake8-raise (`RSE`)\n\n* flake8-return (`RET`)\n\n* flake8-self (`SLF`)\n\n* flake8-simplify (`SIM`)\n\n* flake8-unsused-arguments (`ARG`)\n\n* flake8-use-pathlib (`PTH`)\n\n* eradicate (`ERA`)\n\n* pylint (`PL`)\n\n* tryceratos (`TRY`), flynt (`FLY`), Perflint (`PERF`)\n\n* NumPy-specific rules (`NPY`)\n\n* Ruff-specific rules (`RUF`)\n\n* Remove pylint ignore comments\n\n* Fix tests\n\n* Fix tests\n\n* Enable Ruff Rules - Part 7 (#1408)\n\n* pyflakes\n\n* pycodestyle\n\n* pep8-naming (`N`)\n\n* pyupgrade (`UP`)\n\n* flake8-bandit (`S`)\n\n* Enabled UP, ANN, S, BLE, FBT, B\n\n* Fix typo\n\n* Revert F1AdaptiveThreshold parent classes\n\n* Fix some of the tests\n\n* ignore boolean-positional-value-in-call (FBT003)\n\n* Addressed pr comments\n\n* Remove duplicated lines\n\n* flake8-builtins (`A`) and flake8-commas (`COM`)\n\n* flake8-comprehensions (`C4`)\n\n* flake8-datatimez (`DTZ`), flake8-debugger (`T10`), flake8-errmsg (`EM`)\n\n* flake8-pie (`PIE`)\n\n* flake8-raise (`RSE`)\n\n* flake8-return (`RET`)\n\n* flake8-self (`SLF`)\n\n* flake8-simplify (`SIM`)\n\n* flake8-unsused-arguments (`ARG`)\n\n* flake8-use-pathlib (`PTH`)\n\n* eradicate (`ERA`)\n\n* pylint (`PL`)\n\n* tryceratos (`TRY`), flynt (`FLY`), Perflint (`PERF`)\n\n* NumPy-specific rules (`NPY`)\n\n* Ruff-specific rules (`RUF`)\n\n* Remove pylint ignore comments\n\n* Fix tests\n\n* Fix tests\n\n* mccabe (`C90`)\n\n* pygrep-hooks (`PGH`)\n\n* flake8-todos (`TD`)\n\n* flake8-fixme (`FIX`)\n\n* pandas-vet (`PD`)\n\n* Fix random_split tests\n\n* Fix pre-commit\n\n* Fixed the logger test\n\n* Fix the typos in todos\n\n* Enable Ruff Rules - Part 8 (#1412)\n\n* pyflakes\n\n* pycodestyle\n\n* pep8-naming (`N`)\n\n* pyupgrade (`UP`)\n\n* flake8-bandit (`S`)\n\n* Enabled UP, ANN, S, BLE, FBT, B\n\n* Fix typo\n\n* Revert F1AdaptiveThreshold parent classes\n\n* Fix some of the tests\n\n* ignore boolean-positional-value-in-call (FBT003)\n\n* Addressed pr comments\n\n* Remove duplicated lines\n\n* flake8-builtins (`A`) and flake8-commas (`COM`)\n\n* flake8-comprehensions (`C4`)\n\n* flake8-datatimez (`DTZ`), flake8-debugger (`T10`), flake8-errmsg (`EM`)\n\n* flake8-pie (`PIE`)\n\n* flake8-raise (`RSE`)\n\n* flake8-return (`RET`)\n\n* flake8-self (`SLF`)\n\n* flake8-simplify (`SIM`)\n\n* flake8-unsused-arguments (`ARG`)\n\n* flake8-use-pathlib (`PTH`)\n\n* eradicate (`ERA`)\n\n* pylint (`PL`)\n\n* tryceratos (`TRY`), flynt (`FLY`), Perflint (`PERF`)\n\n* NumPy-specific rules (`NPY`)\n\n* Ruff-specific rules (`RUF`)\n\n* Remove pylint ignore comments\n\n* Fix tests\n\n* Fix tests\n\n* mccabe (`C90`)\n\n* pygrep-hooks (`PGH`)\n\n* flake8-todos (`TD`)\n\n* flake8-fixme (`FIX`)\n\n* pandas-vet (`PD`)\n\n* ignore ANN101 ANN102 and ANN103\n\n* Fix random_split tests\n\n* Fix pre-commit\n\n* ANN partly done\n\n* Remove kwargs: Any\n\n* flake8-annotations (`ANN`)\n\n* Enabled tests\n\n* Revert padim configs\n\n* Enable Ruff Rules - Part 9 (#1419)\n\n* pyflakes\n\n* pycodestyle\n\n* pep8-naming (`N`)\n\n* pyupgrade (`UP`)\n\n* flake8-bandit (`S`)\n\n* Enabled UP, ANN, S, BLE, FBT, B\n\n* Fix typo\n\n* Revert F1AdaptiveThreshold parent classes\n\n* Fix some of the tests\n\n* ignore boolean-positional-value-in-call (FBT003)\n\n* Addressed pr comments\n\n* Remove duplicated lines\n\n* flake8-builtins (`A`) and flake8-commas (`COM`)\n\n* flake8-comprehensions (`C4`)\n\n* flake8-datatimez (`DTZ`), flake8-debugger (`T10`), flake8-errmsg (`EM`)\n\n* flake8-pie (`PIE`)\n\n* flake8-raise (`RSE`)\n\n* flake8-return (`RET`)\n\n* flake8-self (`SLF`)\n\n* flake8-simplify (`SIM`)\n\n* flake8-unsused-arguments (`ARG`)\n\n* flake8-use-pathlib (`PTH`)\n\n* eradicate (`ERA`)\n\n* pylint (`PL`)\n\n* tryceratos (`TRY`), flynt (`FLY`), Perflint (`PERF`)\n\n* NumPy-specific rules (`NPY`)\n\n* Ruff-specific rules (`RUF`)\n\n* Remove pylint ignore comments\n\n* Fix tests\n\n* Fix tests\n\n* mccabe (`C90`)\n\n* pygrep-hooks (`PGH`)\n\n* flake8-todos (`TD`)\n\n* flake8-fixme (`FIX`)\n\n* pandas-vet (`PD`)\n\n* ignore ANN101 ANN102 and ANN103\n\n* Fix random_split tests\n\n* Fix pre-commit\n\n* ANN partly done\n\n* Remove kwargs: Any\n\n* flake8-annotations (`ANN`)\n\n* Enabled tests\n\n* Revert padim configs\n\n* Add auto fixes\n\n* Fix docstrings\n\n* Update src/anomalib/utils/metrics/binning.py\n\nCo-authored-by: Ashwin Vaidya <ashwinnitinvaidya@gmail.com>\n\n* Update src/anomalib/models/dfkde/lightning_model.py\n\nCo-authored-by: Ashwin Vaidya <ashwinnitinvaidya@gmail.com>\n\n* Update src/anomalib/models/rkde/feature_extractor.py\n\nCo-authored-by: Ashwin Vaidya <ashwinnitinvaidya@gmail.com>\n\n* Fixed pre-commit\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinnitinvaidya@gmail.com>\n\n* Merge main into feature/custom_trainer (#1420)\n\n* Address tiler issues (#1411)\n\n* fix tiler\n\n* deprecate random tile locations\n\n* restore random tiling in tile method\n\n* check tiling section in config\n\n* disable tiling for ganomalu\n\n* pad -> padding\n\n* Refactor Reverse Distillation to match official code (#1389)\n\n* Non-mandatory early stopping\n\n* Added conv4 and bn4 to OCBE\n\n* Loss as in the official code (flattened arrays)\n\n* Added comment on how to use torchvision model as an encoder to reproduce results in the paper\n\n* Remove early stop from config, change default anomaly_map_mode to add\n\n* pre-commit fix\n\n* Updated results\n\n* Update src/anomalib/models/reverse_distillation/README.md\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Update src/anomalib/models/reverse_distillation/README.md\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Update src/anomalib/models/reverse_distillation/README.md\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Remove early_stopping\n\n* Update src/anomalib/models/reverse_distillation/lightning_model.py\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Easier to read code\n\n---------\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n---------\n\nCo-authored-by: Dick Ameln <dick.ameln@intel.com>\nCo-authored-by: abc-125 <63813435+abc-125@users.noreply.github.com>\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\n\n* Enable Ruff Rules - Part 10 (#1423)\n\n* pyflakes\n\n* pycodestyle\n\n* pep8-naming (`N`)\n\n* pyupgrade (`UP`)\n\n* flake8-bandit (`S`)\n\n* Enabled UP, ANN, S, BLE, FBT, B\n\n* Fix typo\n\n* Revert F1AdaptiveThreshold parent classes\n\n* Fix some of the tests\n\n* ignore boolean-positional-value-in-call (FBT003)\n\n* Addressed pr comments\n\n* Remove duplicated lines\n\n* flake8-builtins (`A`) and flake8-commas (`COM`)\n\n* flake8-comprehensions (`C4`)\n\n* flake8-datatimez (`DTZ`), flake8-debugger (`T10`), flake8-errmsg (`EM`)\n\n* flake8-pie (`PIE`)\n\n* flake8-raise (`RSE`)\n\n* flake8-return (`RET`)\n\n* flake8-self (`SLF`)\n\n* flake8-simplify (`SIM`)\n\n* flake8-unsused-arguments (`ARG`)\n\n* flake8-use-pathlib (`PTH`)\n\n* eradicate (`ERA`)\n\n* pylint (`PL`)\n\n* tryceratos (`TRY`), flynt (`FLY`), Perflint (`PERF`)\n\n* NumPy-specific rules (`NPY`)\n\n* Ruff-specific rules (`RUF`)\n\n* Remove pylint ignore comments\n\n* Fix tests\n\n* Fix tests\n\n* mccabe (`C90`)\n\n* pygrep-hooks (`PGH`)\n\n* flake8-todos (`TD`)\n\n* flake8-fixme (`FIX`)\n\n* pandas-vet (`PD`)\n\n* ignore ANN101 ANN102 and ANN103\n\n* Fix random_split tests\n\n* Fix pre-commit\n\n* ANN partly done\n\n* Remove kwargs: Any\n\n* flake8-annotations (`ANN`)\n\n* Enabled tests\n\n* Revert padim configs\n\n* Add auto fixes\n\n* Fix docstrings\n\n* Enabled \"PLW2901\",  # `for` loop variable `row` overwritten by assignment target\n\n* Enable Ruff Rules - Part 11 (#1425)\n\n* pyflakes\n\n* pycodestyle\n\n* pep8-naming (`N`)\n\n* pyupgrade (`UP`)\n\n* flake8-bandit (`S`)\n\n* Enabled UP, ANN, S, BLE, FBT, B\n\n* Fix typo\n\n* Revert F1AdaptiveThreshold parent classes\n\n* Fix some of the tests\n\n* ignore boolean-positional-value-in-call (FBT003)\n\n* Addressed pr comments\n\n* Remove duplicated lines\n\n* flake8-builtins (`A`) and flake8-commas (`COM`)\n\n* flake8-comprehensions (`C4`)\n\n* flake8-datatimez (`DTZ`), flake8-debugger (`T10`), flake8-errmsg (`EM`)\n\n* flake8-pie (`PIE`)\n\n* flake8-raise (`RSE`)\n\n* flake8-return (`RET`)\n\n* flake8-self (`SLF`)\n\n* flake8-simplify (`SIM`)\n\n* flake8-unsused-arguments (`ARG`)\n\n* flake8-use-pathlib (`PTH`)\n\n* eradicate (`ERA`)\n\n* pylint (`PL`)\n\n* tryceratos (`TRY`), flynt (`FLY`), Perflint (`PERF`)\n\n* NumPy-specific rules (`NPY`)\n\n* Ruff-specific rules (`RUF`)\n\n* Remove pylint ignore comments\n\n* Fix tests\n\n* Fix tests\n\n* mccabe (`C90`)\n\n* pygrep-hooks (`PGH`)\n\n* flake8-todos (`TD`)\n\n* flake8-fixme (`FIX`)\n\n* pandas-vet (`PD`)\n\n* ignore ANN101 ANN102 and ANN103\n\n* Fix random_split tests\n\n* Fix pre-commit\n\n* ANN partly done\n\n* Remove kwargs: Any\n\n* flake8-annotations (`ANN`)\n\n* Enabled tests\n\n* Revert padim configs\n\n* Add auto fixes\n\n* Fix docstrings\n\n* Enabled \"PLW2901\",  # `for` loop variable `row` overwritten by assignment target\n\n* Add google style to pydocstyle\n\n* Remove dashed line from Returns\n\n* Remove dashed line from Args\n\n* Remove dashed line from Example and Raises\n\n* Removed left-over dashed lines\n\n* Cleanup pyproject.toml file\n\n* [Custom Trainer] Add a verbose help output structure to the CLI (#1396)\n\n* Add Verbosity Help-Formatter class\n\n* Add Help-Formatter unit-tests\n\n* Fix some strings\n\n* Fix pre-commit ruff stuff\n\n* Fix help_formatter's pre-commit\n\n* Add new configs (#1418)\n\n* Add new configs\n\n* Add draem config\n\n* Fix docstring\n\n* Update src/anomalib/models/cflow/lightning_model.py\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Linting\n\n* Remove --- from docstrings\n\n* Remove any from return type\n\n* Fix linting issues from feature/custom_trainer\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Add CLI Tests (#1426)\n\n* Add new configs\n\n* Add draem config\n\n* Stash cli tests\n\n* Stash cli tests with minor changes\n\n* Stash changes\n\n* Fix reverse distillation\n\n* Fix EfficientAD\n\n* Match ai_vad params to config params\n\n* Fix ucsd and ai_vad configs\n\n* Uncomment validation step\n\n* Refactor directory structure\n\n* Rename method\n\n* use uscd for aivad\n\n* fix ucsd path + modify model checkpoint callback for tests\n\n* fix dfkde config\n\n* Restructure tests + fix normalization test\n\n* Revert file\n\n* add v1 to tox\n\n* Skip testing ai_vad\n\n* Increase train and test size\n\n* use mvtec dataset\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\n\n* Restructure test directories. (#1438)\n\n* Restructured the test directories\n\n* Fixed typo\n\n* Fix imports\n\n* Fix config path in export tests\n\n* Replace black with ruff formatter (#1439)\n\n* [Custom Trainer] Refactor export (#1440)\n\n* Refactor export\n\n* Fix entrypoint tests\n\n* remove match statements\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\n\n* Add ruff checks to tests (#1455)\n\n* Fix tests + add ruff check to tests\n\n* Limit gradio version\n\n* Path->str\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\n\n* Add dummy image dataset generator helper class (#1444)\n\n* Created tests/v1 directory\n\n* update license year\n\n* Add beantech generator\n\n* Refactor mvtec-ad and beantech\n\n* Add visa dataset generator\n\n* Add DummyImageGenerator\n\n* Use DummyImageGenerator in dummy mvtec dataset generator\n\n* Use DummyImageGenerator in dummy mvtec dataset generator\n\n* Use DummyImageGenerator in dummy mvtec3d dataset generator\n\n* Restructured the test directories\n\n* Fixed typo\n\n* Fix imports\n\n* Fix config path in export tests\n\n* Add kolektor dataset\n\n* add ucsdped generator\n\n* Fix tests\n\n* Revert conftest.py\n\n* add method for generating avenue dataset\n\n* Revert conftest.py\n\n* add method for generating shanghaitech dataset\n\n* swap order of typing for better parsing of normalization type\n\n* cleanup\n\n* Dynamically create DataFormat enum\n\n* Add examples to docstring\n\n* address pr comments and rename dataset.py to data.py\n\n* Fix pre-commit issues\n\n---------\n\nCo-authored-by: Dick Ameln <dick.ameln@intel.com>\n\n* Remove configurable parameters (#1453)\n\n* Refactor export\n\n* Fix entrypoint tests\n\n* remove match statements\n\n* Fix tests + remove get_configurable_params + fix hpo,benchmarking\n\n* Path->str\n\n* Update src/anomalib/models/__init__.py\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Update src/anomalib/utils/sweep/config.py\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Update tools/inference/README.md\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Update tools/inference/README.md\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* str->Path\n\n* str->Path\n\n* Fix model checkpoint path\n\n* typing + path + test order\n\n* Update src/anomalib/utils/sweep/config.py\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Mark tests as xfail\n\n* Fix notebook\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\nCo-authored-by: Dick Ameln <amelndjd@gmail.com>\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* [v1] - Tests: Add datamodule tests. (#1456)\n\n* Created tests/v1 directory\n\n* update license year\n\n* Add beantech generator\n\n* Refactor mvtec-ad and beantech\n\n* Add visa dataset generator\n\n* Add DummyImageGenerator\n\n* Use DummyImageGenerator in dummy mvtec dataset generator\n\n* Use DummyImageGenerator in dummy mvtec dataset generator\n\n* Use DummyImageGenerator in dummy mvtec3d dataset generator\n\n* Restructured the test directories\n\n* Fixed typo\n\n* Fix imports\n\n* Fix config path in export tests\n\n* Add kolektor dataset\n\n* add ucsdped generator\n\n* Fix tests\n\n* Revert conftest.py\n\n* add method for generating avenue dataset\n\n* Revert conftest.py\n\n* add method for generating shanghaitech dataset\n\n* swap order of typing for better parsing of normalization type\n\n* cleanup\n\n* create the data tests files\n\n* Dynamically create DataFormat enum\n\n* Add examples to docstring\n\n* add conftest.py\n\n* address pr comments and rename dataset.py to data.py\n\n* add some changes\n\n* Add test_datasets to the integration tests\n\n* Change order\n\n* Added datamodule tests\n\n* Format ruff\n\n* Address pre-commit issues\n\n* Fix video tests\n\n* Add the rest of the datamodule tests\n\n---------\n\nCo-authored-by: Dick Ameln <dick.ameln@intel.com>\n\n* [Custom Trainer] Add train subcommand (#1465)\n\nAdd train subcommand\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\n\n* Refactor `Tensor` annotation to `torch.Tensor` (#1477)\n\n* Modify Tensor to torch.Tensor\n\n* list[Tensor] to list[torch.Tensor]\n\n* TODO: Fix formatting issues\n\n* torch_all to torch.all\n\n* Remove redundant import\n\n* Apply ruff format\n\n* Fix the tests\n\n* Refactor tests Part 1 (#1473)\n\n* Refactor CLI tests\n\n* Select a random model\n\n* Fix test for all the models\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\n\n* Add API tests Part 2 (#1474)\n\n* Partially migrate unit tests Part 3 (#1480)\n\n* Refactor CLI tests\n\n* Add api tests\n\n* Select a random model\n\n* Fix test for all the models\n\n* Fix API tests\n\n* refactor pre-post processing + get model ckpt from fixture\n\n* Add tests for custom transforms\n\n* Address PR comments\n\n* Refactor ckpt_path fixture\n\n* Update conftest.py\n\n* Update __init__.py\n\n* Update __init__.py\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Reorg Part I: Data (#1483)\n\n* Update Anomalib data with the new structure\n\n* Update dataset imports and remove unused imports\n\n* ruff format in engine.py\n\n* Move TaskType to utils/types\n\n* Revert TaskType import from anomalib\n\n* Revert tox.ini\n\n* Refactor imports and fix import errors\n\n* Fix import errors and update type annotations\n\n* Fix imports in jupyter notebooks\n\n* Refactor import statements in test_visualizer.py\n\n* Reorg Part II: Remove `pre_processor` and `post_processor` subpackages (#1485)\n\n* Update Anomalib data with the new structure\n\n* Update dataset imports and remove unused imports\n\n* ruff format in engine.py\n\n* Move TaskType to utils/types\n\n* Revert TaskType import from anomalib\n\n* Revert tox.ini\n\n* Refactor imports and fix import errors\n\n* Fix import errors and update type annotations\n\n* Fix imports in jupyter notebooks\n\n* Remove pre-processor subpackage from anomalib\n\n* Remove unused imports and update import paths\n\n* Refactor import statements in test_visualizer.py\n\n* Remove unused code and deprecate Denormalize and\nToNumpy classes\n\n* Remove empty code cell\n\n* Add a description why input image is read from path\n\n* Fix bug in superimpose\n\n* Migrate deploy tests Part 4 (#1481)\n\n* Refactor CLI tests\n\n* Add api tests\n\n* Select a random model\n\n* Fix test for all the models\n\n* Fix API tests\n\n* refactor pre-post processing + get model ckpt from fixture\n\n* Add tests for custom transforms\n\n* Migrate deploy tests\n\n* trained_padim_path->ckpt_path\n\n* Split normalization line\n\n* Fix normalization class path\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\n\n* Migrate model components unit tests Part 5 (#1482)\n\n* Refactor CLI tests\n\n* Add api tests\n\n* Select a random model\n\n* Fix test for all the models\n\n* Fix API tests\n\n* refactor pre-post processing + get model ckpt from fixture\n\n* Add tests for custom transforms\n\n* Migrate deploy tests\n\n* Migrate model component tests\n\n* Migrate visualizer callback + cli tests\n\n* Fix lightning entrypoint test\n\n* trained_padim_path->ckpt_path\n\n* Add todo\n\n* Fix TaskType import\n\n* Apply suggestions from code review\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Reorg Part III: Move the main anomalib components from `anomalib.utils` to `anomalib` (#1487)\n\n* Update Anomalib data with the new structure\n\n* Update dataset imports and remove unused imports\n\n* ruff format in engine.py\n\n* Move TaskType to utils/types\n\n* Revert TaskType import from anomalib\n\n* Revert tox.ini\n\n* Refactor imports and fix import errors\n\n* Fix import errors and update type annotations\n\n* Fix imports in jupyter notebooks\n\n* Remove pre-processor subpackage from anomalib\n\n* Remove unused imports and update import paths\n\n* Refactor import statements in test_visualizer.py\n\n* Move callbacks from utils under anomalib\n\n* Fix import statements in benchmarking and CLI\nmodules\n\n* Move CLI under anomalib\n\n* Add benchmark to pipelines\n\n* Move hpo to pipelines\n\n* Move sweep to pipelines\n\n* Move loggers to anomalib\n\n* Move metrics to anomalib\n\n* Move callbacks from utils to test/utils\n\n* Move config to anomalib.utils\n\n* Fix the metric imports\n\n* Remove unused code and deprecate Denormalize and\nToNumpy classes\n\n* Remove empty code cell\n\n* Add a description why input image is read from path\n\n* Fix bug in superimpose\n\n* Move anomalib.utils.config.config to anomalib.utils.config\n\n* Fix config import\n\n* Merge feature/custom_trainer\n\n* Migrate tools test Part 6 (#1488)\n\n* Refactor CLI tests\n\n* Add api tests\n\n* Select a random model\n\n* Fix test for all the models\n\n* Fix API tests\n\n* refactor pre-post processing + get model ckpt from fixture\n\n* Add tests for custom transforms\n\n* Migrate deploy tests\n\n* Migrate model component tests\n\n* Migrate visualizer callback + cli tests\n\n* Fix lightning entrypoint test\n\n* trained_padim_path->ckpt_path\n\n* Migrate metrics tests\n\n* Migrate tools + remove nightly\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\n\n* \ud83d\ude9c Refactor padim and patchcore models (#1300)\n\n* Fix metadata path\n\n* Ignore hidden directories in folder dataset\n\n* Add check for mask_dir for segmentation tasks in Folder dataset\n\n* add is_fitted\n\n* self.model._is_fitted to self.model.is_fitted\n\n* Format anomaly module\n\n* Remove on_save_checkpoint\n\n* Refactor padim\n\n* Add __repr__ to anomaly score threshold\n\n* Revert patchcore config\n\n* Add memory bank modules for anomaly detection\n\n* Add explanation to MemoryBankTorchModule docstring.\n\n* Update memory bank module imports and fix typo in\nPadim model\n\n* Rename Dynamic Buffer Module to Memory Bank\nModule in docstring.\n\n* Revert \"Add explanation to MemoryBankTorchModule docstring.\"\n\nThis reverts commit 44c991450f7c78eee2b0ceb0e7c855c3893a0801.\n\n* Refactor memory bank modules based on Dick's suggestion\n\n* Fix model attribute assignment in lightning models\n\n* Add MemoryBankMixin to anomaly detection models\n\n* revert padim and patchcore\n\n* Reorder inheritance in anomaly detection models\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwin.vaidya@intel.com>\n\n* Migrate unit tests Part 7 (#1490)\n\n* Refactor CLI tests\n\n* Add api tests\n\n* Select a random model\n\n* Fix test for all the models\n\n* Fix API tests\n\n* refactor pre-post processing + get model ckpt from fixture\n\n* Add tests for custom transforms\n\n* Migrate deploy tests\n\n* Migrate model component tests\n\n* Migrate visualizer callback + cli tests\n\n* Fix lightning entrypoint test\n\n* trained_padim_path->ckpt_path\n\n* Migrate metrics tests\n\n* Migrate tools + remove nightly\n\n* Increase coverage\n\n* Migrate remaining tests\n\n* Fix imports\n\n* Fix import\n\n* Update tests/unit/deploy/test_inferencer.py\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Update test_get_logger.py\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Fix circular import in cdf normalizer (#1494)\n\n* fix circular import in cdf normalizer\n\n* fix pre-commit\n\n---------\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* \ud83d\udee0\ufe0f Refactor: Split models to image and video (#1493)\n\n* Move AiVad to anomalib.video\n\n* Move cfa to models.image\n\n* Add Ganomaly model to image models\n\n* Add Fastflow model to image models\n\n* Add EfficientAd anomaly detection model to image\nmodels\n\n* Move dfm, dfkde and draem\n\n* Add CS-Flow model implementation for image-based\ndefect detection\n\n* Add cflow to image models\n\n* Add padim to image models\n\n* Add patchcore to image models\n\n* Add Reverse distillation to image models.\n\n* Add rkde to image models\n\n* Add stfpm to image models.\n\n* Add image models for handling image datasets in\nanomalib\n\n* Update copyright year in model files\n\n* Update import statement for Fastflow model\n\n* Update image and video model documentation\n\n* Update src/anomalib/models/video/README.md\n\nCo-authored-by: Dick Ameln <amelndjd@gmail.com>\n\n* Update clip_length_in_frames parameter in\nAvenueDataset and Avenue classes\n\n* Remove folder references\n\n* Fix a typo in readme\n\n* Fix shape of image in batch\n\n* Update clip_length_in_frames parameter\n\n---------\n\nCo-authored-by: Dick Ameln <amelndjd@gmail.com>\n\n* v1: Update the readme file (#1503)\n\n* Fix metadata path\n\n* Ignore hidden directories in folder dataset\n\n* Add check for mask_dir for segmentation tasks in Folder dataset\n\n* Replace docs\n\n* Add each inferencing scripts as a details section\n\n* update readme\n\n* Add training\n\n* modify getting started\n\n* Make getting started a subsection\n\n* tmp\n\n* Added inference section\n\n* Refactor Lightning inference code\n\n* Update entry point in setup.py\n\n* Add training api example to readme\n\n* Update training command in README\n\n* Fix bug in login functionality\n\n* Update HPO and Logging Documentation\n\n* refactor getting started section\n\n* Update HPO and benchmarking commands\n\n* Update the image\n\n* Update README.md\n\nCo-authored-by: Dick Ameln <amelndjd@gmail.com>\n\n* Update README.md\n\nCo-authored-by: Dick Ameln <amelndjd@gmail.com>\n\n* Remove getting started section\n\n* Update README.md\n\nCo-authored-by: Dick Ameln <amelndjd@gmail.com>\n\n* Update README.md\n\nCo-authored-by: Dick Ameln <amelndjd@gmail.com>\n\n* Address the reviewer comments\n\n---------\n\nCo-authored-by: Dick Ameln <amelndjd@gmail.com>\n\n* Extend Engine Tests (#1509)\n\n* Add validate + predict\n\n* Add train\n\n* Add export tests + refactor export cli command\n\n* Fix tests\n\n* Fix jupyternotebook\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\n\n* v1: Create the new documentation via `sphinx-design` and `myst` (#1518)\n\n* removed docs\n\n* Created the new docs\n\n* Finished get started\n\n* Remove jupyter notebook from docs\n\n* Add mvtec to data\n\n* Add reference guide\n\n* Add section for each image datasets\n\n* Add base data modules and datasets\n\n* v1 - \ud83d\udcdd Update and Enhance Docstrings (#1532)\n\n* removed docs\n\n* Created the new docs\n\n* Finished get started\n\n* Remove jupyter notebook from docs\n\n* Add mvtec to data\n\n* Add reference guide\n\n* initial commit\n\n* Remove subrocess from btech\n\n* Remove unused import and commented out code\n\n* Add section for each image datasets\n\n* Add base data modules and datasets\n\n* Refactor dataset classes and add docstrings\n\n* Add initial draft for backbone docs\n\n* Update the docstring in folder_3d\n\n* Update mvtec 3d docstring\n\n* Added feature extractor tutorial\n\n* Add a readme file to the docs\n\n* Update folder data docstring\n\n* Update kolektor docstrings\n\n* Update kolektor docstring\n\n* Update mvtec docstring\n\n* Update visa docstrings\n\n* Update avenue docstring\n\n* Update cfa docstring\n\n* Update cflow docstring\n\n* Update csflow docstring\n\n* Update csflow docstring\n\n* Update dfkde docstrings\n\n* Update dfm docstring\n\n* Update draem docstring\n\n* Update efficient ad docstring\n\n* Update ganomaly docstring\n\n* Update padim and patchcore docstrings\n\n* Update reverse distillation docstring\n\n* update rkde docstring\n\n* update stfpm docstring\n\n* Update ai-vad docstring\n\n* Update feature extractors\n\n* add docstring to sparse random projection\n\n* Update dimensionality reduction components\n\n* Exclude prompt from copying\n\n* Normalizing flow update\n\n* Add pro\n\n* Add feature extractor docs\n\n* update aupr\n\n* Update aupr\n\n* Update aupro\n\n* Update auroc\n\n* Update docs/source/markdown/guides/how_to/models/feature_extractors.md\n\n* Update f1 and manual thresholds\n\n* add minmax\n\n* add optimal f1\n\n* update utils\n\n* update comet\n\n* add tensorboard\n\n* update wandb\n\n* Add callbacks\n\n* Update deploy docstrings\n\n* Fix pre-commit on Blasz changes\n\n* Change the requirement file in readthedocs config file\n\n* Partially address pr comments\n\n* Fix the model to padim for the cli integration tests\n\n---------\n\nCo-authored-by: Bla\u017e Rolih <61357777+blaz-r@users.noreply.github.com>\nCo-authored-by: Blaz Rolih <blaz.rolih@gmail.com>\n\n* Fix AI-VAD issues (#1524)\n\n* partially fix empty bbox issue\n\n* allow empty region detections\n\n* add torch implementation of gmm (WIP)\n\n* make knn mem bank persistent\n\n* set val_split_mode to same_as_test as default to enforce deterministicness\n\n* add unit tests and docstrings to gmm\n\n* improve typing of knn estimator\n\n* remove todo\n\n* update buffer name\n\n* fix minor mistakes in gmm implementation\n\n* remove unnecessary tensor conversion\n\n* fix visualization when predicting with video model\n\n* add __init__.py to components.cluster\n\n* check for empty bboxes in feature extractor\n\n* reduce default batch size\n\n* cast deep features to float\n\n* fix device issue\n\n* add unit tests for feature extractors\n\n* add license header\n\n* disable random model selection in integration tests\n\n* typing and docstrings\n\n* add test case for non-convergence warning\n\n* \ud83d\udcdd  v1 - Docs: Create a dedicated section for each model. (#1540)\n\n* Initial commit for model components\n\n* FIx the grid in model components\n\n* Add image models\n\n* Add video models\n\n* Fix titles and do some cleanup\n\n* reduce the sphinx version as it fails the readthedocs builds\n\n* Fix examples for reading transforms from albumentations Compose object and deserializing from a yaml file\n\n* Update __init__.py\n\n* OpenVINO NNCF updates (#1534)\n\n* Update versions of openvino and nncf\n\n* All export functions return the model path\n\n* Change default OV device to AUTO\n\n* Minor changes on openvino API\n\n* Fix pre-commit issues\n\n* Restored onnx dependency\n\n* Added OV export tests\n\n* Drop export tests\n\n* Rename path var\n\n* Renamed test paths\n\n* [Docs] Add average score to the FastFlow's performance results tables (#1587)\n\nAdd average score in the tables of performance results\n\n* Update the paper title in CS-FLOW and CFLOW readme (#1579)\n\n* Fix csflow name in readme\n\n* Update cflow name in readme\n\n* v1 - [Refactor] Reflect the changes in #1562 into v1 (#1595)\n\nReflect the changes in #1562 into v1\n\n* \u270f\ufe0f Refactor `ExportMode` to `ExportType` (#1594)\n\n* Update export_mode to export_type\n\n* Fix typo typel -> model\n\n* Revert the python version in the notebook\n\n* \ud83d\udcda v1 - Modify the PR template (#1596)\n\n* Modify the PR template\n\n* Update pull_request_template.md\n\n* added emojis to the checklist\n\n* [Bug] v1: Fix default input normalization method (#1583)\n\nFix default input normalization method\n\n* Modify `Engine.predict` (#1514)\n\n* Add validate + predict\n\n* Add train\n\n* Add export tests + refactor export cli command\n\n* Fix tests\n\n* Fix jupyternotebook\n\n* Update engine.predict + expand tests\n\n* Fix lightning entrypoint test\n\n* Address PR comments\n\n* Use only Padim\n\n* Fix commands\n\n* Move padim to common args\n\n* Address 1st PR comment\n\n* Address PR comments\n\n* Fix aivad tests\n\n* Fix missing docstring\n\n* Rename config to args\n\n* Add missing ckpt_path warning in predict\n\n* Remove ckpt_path as required parameter\n\n* Add tests for image path in predict\n\n* Fix image path in predict\n\n* Address PR comments\n\n* Fix missing checkpoint path\n\n* Fix fastflow precommit issue\n\n* Fix tests\n\n* Fix test\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\n\n* Fix issue with incorrect image save location (#1515)\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\n\n* Upgrade gradio version to 4.x (#1608)\n\n* upgrade gradio version to 4.x\n\n* refactor variable names\n\n* refactor\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\n\n* \u270d InferenceDataset->PredictDataset (#1544)\n\n* \u270d InferenceDataset->PredictDataset\n\n* update predict dataset references\n\n* update predict dataset references\n\n* update predict dataset references\n\n* update predict dataset references\n\n* update predict dataset references\n\n* update notebook\n\n---------\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\n\n* `FeatureExtractor` -> `TimmFeatureExtractor` (#1543)\n\nDeprecate FeatureExtractor\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\n\n* Add `LearningType` and refactor enums (#1598)\n\n* add LearningType and move enums to separate module\n\n* add enum definitions\n\n* move shared enums to root init\n\n* place version above enums\n\n* \ud83d\udcd8 Add custom data tutorial (#1571)\n\n* Add custom data tutorial\n\n* Add the custom data training instructions\n\n* Address PR comments\n\n* Start with a classification data\n\n* Release hazelnut toy dataset and refer to the link here.\n\n* Update docs/source/markdown/guides/how_to/data/custom_data.md\n\nCo-authored-by: Dick Ameln <amelndjd@gmail.com>\n\n* Update docs/source/markdown/guides/how_to/data/custom_data.md\n\nCo-authored-by: Dick Ameln <amelndjd@gmail.com>\n\n* Update docs/source/markdown/guides/how_to/data/custom_data.md\n\nCo-authored-by: Dick Ameln <amelndjd@gmail.com>\n\n* Remove no-val-test section from tutorials\n\n* Address PR comments\n\n---------\n\nCo-authored-by: Dick Ameln <amelndjd@gmail.com>\nCo-authored-by: Samet Akcay <sakcay@Samets-MacBook-Pro.local>\n\n* Add URL verification for downloading dataset (#1620)\n\n* add url path verification for dataset downloading module\n\n* specify node version to pre-commit-config\n\n* fix import errors on the notebooks\n\n* \ud83d\udc1e v1 - Fix training with mps accelerator (#1618)\n\n* Convert mask to float32 in AnomalibDataset\n\n* Convert the tensor to cpu before convrting to numpy\n\n* replace np.float32 with np.single\n\n* Update Engine docstrings (#1549)\n\nUpdate docstrings\n\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\n\n* Fixed shape error, allowing arbitary image sizes for EfficientAD\n\n* Revert the previous commit\n\n* Extend supported models in TimmFeatureExtractor (#1443)\n\nExtend Timm feature extractor for v1.0\n\n* \ud83d\udd12 v1 - Address security issues (#1637)\n\n* Address path traversal issues 1-3\n\n* address traversal path 6\n\n* Address traverse path 8\n\n* modify the comment to make it more descriptive\n\n* \ud83d\udc1e Fix mps float64 tensor conversion issue (#1644)\n\nFix leftover\n\n* \ud83d\udc1e Fix metadata_path arg to metadata in OpenVINO inferencer (#1648)\n\nFix metadata_path arg to metadata in OpenVINO inferencer\n\n* \ud83d\udd12 Address path traversal issues (#1643)\n\n* Address path traversal issues 1-3\n\n* address traversal path 6\n\n* Address traverse path 8\n\n* modify the comment to make it more descriptive\n\n* Update get_image_filename function to enhance the input security\n\n* fix example\n\n* Fix incorrect default value assignment\n\n* Refactor project_path fixture to create temporary directory in the root directory of the project\n\n* Update .gitignore file to include test-related files and directories\n\n* Refactor get_image_filenames_from_dir to filter out non-image files\n\n* Add test case for path outside base directory\n\n* Add examples to get_filenames\n\n* Address PR comments.\n\n* Renamed the tmp dir\n\n* \ud83d\udd12 Add `SECURITY.md` file (#1655)\n\n* Add SECURITY.md file\n\n* Add security item to the type of changes in the pull request template.\n\n* Update pr template\n\n* replace the security emoji\n\n* \ud83d\ude80 Add zero-/few-shot model support and WinCLIP model implementation (#1575)\n\n* add clip normalization\n\n* initial commit for winclip\n\n* add cosine similarity computation\n\n* add multiscale score computation\n\n* simplify mask generation\n\n* add few-shot extension (unvalidated)\n\n* refactor\n\n* cleanup\n\n* add todo\n\n* formatting\n\n* minor refactor\n\n* add comment\n\n* expose optimal F1 metric\n\n* some cleanup\n\n* add ln_after_pool logic\n\n* remove final_ln_after_pool\n\n* update module docstring and remove comments\n\n* add typing and docstrings to torch model\n\n* cleanup lightning model\n\n* hardcode backbone\n\n* n_shot -> k_shot\n\n* add temperature as constant\n\n* minor bugfix\n\n* add typing and docstrings to utils\n\n* set class name dynamically\n\n* replace inf values in harmonic aggregation\n\n* run validate before test\n\n* set default class name to None\n\n* formatting\n\n* remove config\n\n* comments\n\n* minor bugfix\n\n* Revert \"expose optimal F1 metric\"\n\nThis reverts commit e8e1ead9601d76c743af3678f26b1eb0e06d38fb.\n\n* more descriptive assert message\n\n* expose scales as configurable parameter and hardcode pretrained as constant\n\n* add readme\n\n* add images for readme\n\n* update docstrings\n\n* update license headers\n\n* ruff\n\n* add openclip as requirement\n\n* skip model tests for winclip\n\n* fix visualizer test\n\n* add example in docstring\n\n* fix typo in function name\n\n* typing\n\n* imports\n\n* docstrings\n\n* check if model has trainer attribute\n\n* remove pylint ignore statement\n\n* typing\n\n* docstring\n\n* improve tensor shape handling\n\n* refactor and rename class_scores function\n\n* add docstring example\n\n* commenting\n\n* Update src/anomalib/models/image/winclip/torch_model.py\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Update src/anomalib/models/image/winclip/torch_model.py\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Update src/anomalib/models/image/winclip/torch_model.py\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* formatting\n\n* docstrings\n\n* docstrings\n\n* comment\n\n* typing\n\n* multiscale -> multi_scale\n\n* add winclip to docs\n\n* add few_shot_source parameter\n\n* use PredictDataset in WinCLIP implementation\n\n* add learning type to winclip lightning model\n\n* add custom model checkpoint callback to save at validation end\n\n* remove trainer arguments from winclip model\n\n* prune state dict for smaller model file\n\n* add learning type logic to engine.train\n\n* pass full path to model checkpoint\n\n* remove training step\n\n* enable integration tests for winclip\n\n* fix typo\n\n* index masks at 0\n\n* formatting\n\n* simplify make_masks\n\n* validate inputs in make_masks\n\n* add unit tests for winclip utils\n\n* add default class_name to prompt ensemble\n\n* add unit tests for winclip prompt ensemble\n\n* add base class for normalization callback\n\n* add _should_run_validation check\n\n* add engine.model property\n\n* use custom modelcheckpoint in tests\n\n* update name in todos\n\n* fix predict tests\n\n* skip export tests for winclip\n\n* fix mistake in model retrieval from trainer\n\n* add model checks\n\n* fix checks\n\n* simplify model check\n\n* add todo for winclip export\n\n* add todo\n\n* add bufferlist mixin\n\n* update bufferlist docstring\n\n* add setup method and register buffers\n\n* import torch model in root of winclip module\n\n* add unit tests for bufferlist mixin\n\n* add unit tests for torch model\n\n* fix transform and update docstring\n\n* disable strict loading in export\n\n* initialize embeddings as tensors\n\n* add test to check if erors are raised\n\n* add todo\n\n* enable winclip export test\n\n* remove device references in torch model\n\n* restore frozen weights in load_state_dict\n\n* make embedding collection methods private\n\n* move state dict handling to winclip from base\n\n* fix typo\n\n* make generate_masks private\n\n* increase onnx opset version\n\n* remove future import\n\n* update docstring\n\n* Update src/anomalib/callbacks/normalization/__init__.py\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Update src/anomalib/callbacks/normalization/cdf_normalization.py\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Update src/anomalib/callbacks/normalization/min_max_normalization.py\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Update src/anomalib/engine/engine.py\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* typing in docstrings\n\n* Update src/anomalib/engine/engine.py\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Update src/anomalib/engine/engine.py\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Update src/anomalib/engine/engine.py\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Update src/anomalib/engine/engine.py\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* use exception instead of assert\n\n* update license header\n\n* docstrings\n\n* bufferlist -> buffer_list\n\n---------\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* WinCLIP attribution (#1662)\n\ngive credit to related works\n\n* \ud83d\udd00 Merge main to v1 (#1652)\n\n* Address tiler issues (#1411)\n\n* fix tiler\n\n* deprecate random tile locations\n\n* restore random tiling in tile method\n\n* check tiling section in config\n\n* disable tiling for ganomalu\n\n* pad -> padding\n\n* Refactor Reverse Distillation to match official code (#1389)\n\n* Non-mandatory early stopping\n\n* Added conv4 and bn4 to OCBE\n\n* Loss as in the official code (flattened arrays)\n\n* Added comment on how to use torchvision model as an encoder to reproduce results in the paper\n\n* Remove early stop from config, change default anomaly_map_mode to add\n\n* pre-commit fix\n\n* Updated results\n\n* Update src/anomalib/models/reverse_distillation/README.md\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Update src/anomalib/models/reverse_distillation/README.md\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Update src/anomalib/models/reverse_distillation/README.md\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Remove early_stopping\n\n* Update src/anomalib/models/reverse_distillation/lightning_model.py\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Easier to read code\n\n---------\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Patch for the WinError183 on the OpenVino export mode (#1386)\n\n* Fix WinError183 (Windows Error)\n\n* Add commentary of the change\n\n---------\n\nCo-authored-by: Youho99 <gaylord.giret@viacesi.fr>\n\n* Add DSR model (#1142)\n\n* added license, init.py and draft readme\n\n* added draft DSR files\n\n* minor comment update\n\n* Implemented dsr model + comments\n\n* added dsr discrete model\n\n* added defect generation in torch model + dsr to list of existing methods in init.py\n\n* fixed torch model, started implementing lightning model, implemented anomaly generator\n\n* added loss file for DSR\n\n* Added loss, improved lightning module\n\n* Finished up global implementation of DSR second phase\n\n* minor fixes\n\n* Bugfixes\n\n* Fixed DSR loss calculation\n\n* on_training_start -> on_train_start\n\n* pre-commit run\n\n* updated DSR documentation\n\n* reset config file\n\n* added automatic pretraining weight download\n\n* testing pretrained weights. fixed embedding size in upsampling module and image recon module, to be fixed in original branch\n\n* successful testing on pretrained dsr weights\n\n* checked test quality with pretrained weights, fixed anomaly score calculation\n\n* training is functional\n\n* Fixed training procedure\n\n* test still working\n\n* working upsampling module training and testing\n\n* fixed minor bugs\n\n* updated documentation\n\n* added tests and doc\n\n* adapted learning schedule to steps\n\n* Update src/anomalib/models/dsr/anomaly_generator.py\n\nCo-authored-by: Ashwin Vaidya <ashwinnitinvaidya@gmail.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\nCo-authored-by: Ashwin Vaidya <ashwinnitinvaidya@gmail.com>\n\n* refactored outputs into dicts\n\n* remove super() args\n\n* changed downloading weights from anomalib releases + minor fixes\n\n* pre commit hooks + minor fixes\n\n* removed configurable ckpt path refs + default iteration nb from paper\n\n* cleaned up dsr.rst and turned exceptions into RuntimeErrors\n\n* Added upsampling ratio parameter to set third training phase epochs\n\n* Added batched evalaution + minor code simplification\n\n* pre commit hooks\n\n* squeeze output image score tensor\n\n* readded new path check in efficient ad\n\n* fixed double step count with manual optimization\n\n* fixed trailing whitespace\n\n* Fix black issues\n\n* Apply suggestions from code review\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* review suggestions\n\n* updated architecture image links\n\n* Address mypy\n\n* changed output types for dsr model\n\n* readded dict outputs, adapted to TorchInferencer\n\n* fixed error in output dict\n\n* removed default imagenet norm\n\n---------\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\nCo-authored-by: Ashwin Vaidya <ashwinnitinvaidya@gmail.com>\n\n* Fix unexpected key pixel_metrics.AUPRO.fpr_limit (#1055)\n\n* fix unexpected key pixel_metrics.AUPRO.fpr_limit\n\nSigned-off-by: FanJiangIntel <fan.jiang@intel.com>\n\n* load AUPRO before create_metric_collection\n\nSigned-off-by: FanJiangIntel <fan.jiang@intel.com>\n\n* code refine\n\nSigned-off-by: FanJiangIntel <fan.jiang@intel.com>\n\n* fix comment\n\nSigned-off-by: FanJiangIntel <fan.jiang@intel.com>\n\n* fix\n\nSigned-off-by: FanJiangIntel <fan.jiang@intel.com>\n\n* Support test\n\nSigned-off-by: Kang Wenjing <wenjing.kang@intel.com>\n\n* Update test\n\nSigned-off-by: Kang Wenjing <wenjing.kang@intel.com>\n\n* Update test\n\nSigned-off-by: Kang Wenjing <wenjing.kang@intel.com>\n\n---------\n\nSigned-off-by: FanJiangIntel <fan.jiang@intel.com>\nSigned-off-by: Kang Wenjing <wenjing.kang@intel.com>\nCo-authored-by: FanJiangIntel <fan.jiang@intel.com>\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Improved speed and memory usage of mean+std calculation (#1457)\n\n* preexisting OpenCV version check added to `setup.py`, ran formatting pre-commit hooks on previous contribution. (#1424)\n\n* testing upstream switch\n\n* picked up on stale OpenCV `setup.py` issue #1041\n\n* \ud83d\udc1e Hotfix: Limit Gradio Version (#1458)\n\n* Fix metadata path\n\n* Ignore hidden directories in folder dataset\n\n* Add check for mask_dir for segmentation tasks in Folder dataset\n\n* Limit the gradio version to <4\n\n* Fix/efficient ad normalize before every validation (#1441)\n\n* Normalize anomaly maps before every validation\n\n* Remove print statement\n\n---------\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Fix DRAEM (#1431)\n\n* Fix beta in augmenter\n\n* Add scheduler\n\n* Change normalization to none\n\n* Replace two lr schedulers with MultiStepLR\n\n* Revert change to beta\n\n* Disable early stopping default\n\n* Format config\n\n* Add opacity parameter beta to config\n\n* Adding U-Flow method (#1415)\n\n* Added uflow model\n\n* Added documentation (README) for uflow model\n\n* Added uflow to the list of available models, and main README updated\n\n* Added missing images for the documentation\n\n* Update src/anomalib/models/uflow/anomaly_map.py\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Update src/anomalib/models/uflow/anomaly_map.py\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Update src/anomalib/models/uflow/feature_extraction.py\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Update src/anomalib/models/uflow/torch_model.py\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Added uflow to the reference guide in docs\n\n* Added uflow to the pre-merge tests\n\n* removed the _step function, and merged the code with training_step\n\n* added as a comment the values used in the paper\n\n* re-factorized feature extractors to use the TimmFeatureExtractor class\n\n* added annotations for some functions, where the flow graph is created\n\n* updated readme to fix images loading\n\n* Added link in the README to the original code for reproducing the results\n\n* Removed unused kwargs\n\n* Added docstrigs with args explanations to UFlow classes\n\n* Added models in a github release, and linked here\n\n* Passing all pre-commit checks\n\n* Changed freia's AllInOneBlock by Anomalib's version, and converted the subnet contructor to a Class, in order to be pickable, that is needed to export the model to torch\n\n---------\n\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\n\n* Update README.md\n\n* \ud83d\udcd8 Announce anomalib v1 on the main `README.md` (#1542)\n\n* Fix metadata path\n\n* Ignore hidden directories in folder dataset\n\n* Add check for mask_dir for segmentation tasks in Folder dataset\n\n* Limit the gradio version to <4\n\n* Announce anomalib v1 on readme\n\n* Add the installation instructions and update the documentation link\n\n* Fixed DSR (#1486)\n\n* fixed DSR squeeze bug\n\n* added comment\n\n* Refactor/extensions custom dataset (#1562)\n\n* Explanation how to use extension names in the config file\n\n* Added information about extensions to the error message and control of the user input\n\n* Easier to read code\n\n* Replacing assert with raise\n\n* \ud83d\udcda Modify the PR template (#1611)\n\nUpdate pull_request_template.md\n\n* Fix result image URLs (#1510)\n\n* Fix tests\n\n* refactor path + fix issues + fix linting issues\n\n* Migrate docs\n\n* fix typing\n\n* fix failing model tests\n\n* Fix tests\n\n* Address PR comments\n\n* Fixed shape error, allowing arbitary image sizes for EfficientAD (#1537)\n\n* Fixed shape error, allowing arbitrary image sizes. Replaced integer parsing by floor operation\n\n* Replaced calculation by ceil operation. Solution of shape error is to round up and not down for the last upsample layer\n\n* Add comment for ceil oepration\n\n* Formatting with pre-commit hook\n\n* Clean up badge\n\n---------\n\nSigned-off-by: FanJiangIntel <fan.jiang@intel.com>\nSigned-off-by: Kang Wenjing <wenjing.kang@intel.com>\nCo-authored-by: Dick Ameln <dick.ameln@intel.com>\nCo-authored-by: abc-125 <63813435+abc-125@users.noreply.github.com>\nCo-authored-by: Samet Akcay <samet.akcay@intel.com>\nCo-authored-by: ggiret-thinkdeep <146845847+ggiret-thinkdeep@users.noreply.github.com>\nCo-authored-by: Youho99 <gaylord.giret@viacesi.fr>\nCo-authored-by: Philippe Carvalho <31983398+phcarval@users.noreply.github.com>\nCo-authored-by: Wenjing Kang <wenjing.kang@intel.com>\nCo-authored-by: FanJiangIntel <fan.jiang@intel.com>\nCo-authored-by: belfner <belfner@belfner.com>\nCo-authored-by: Abdulla Al Blooshi <76493346+abdullamatar@users.noreply.github.com>\nCo-authored-by: Bla\u017e Rolih <61357777+blaz-r@users.noreply.github.com>\nCo-authored-by: Mat\u00edas Tailanian <895687+mtailanian@users.noreply.github.com>\nCo-authored-by: Jan Schl\u00fcter <github@jan-schlueter.de>\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\nCo-authored-by: Christopher <48522299+holzweber@users.noreply.github.com>\n\n* Update license headers\n\n---------\n\nSigned-off-by: FanJiangIntel <fan.jiang@intel.com>\nSigned-off-by: Kang Wenjing <wenjing.kang@intel.com>\nCo-authored-by: Weilin Xu <mzweilin@gmail.com>\nCo-authored-by: Samet <samet.akcay@intel.com>\nCo-authored-by: Ashwin Vaidya <ashwinitinvaidya@gmail.com>\nCo-authored-by: Dick Ameln <dick.ameln@intel.com>\nCo-authored-by: abc-125 <63813435+abc-125@users.noreply.github.com>\nCo-authored-by: Harim Kang <harim.kang@intel.com>\nCo-authored-by: Dick Ameln <amelndjd@gmail.com>\nCo-authored-by: Bla\u017e Rolih <61357777+blaz-r@users.noreply.github.com>\nCo-authored-by: Blaz Rolih <blaz.rolih@gmail.com>\nCo-authored-by: Adrian Boguszewski <adekboguszewski@gmail.com>\nCo-authored-by: Willy Fitra Hendria <willyfitrahendria@gmail.com>\nCo-authored-by: Samet Akcay <sakcay@Samets-MacBook-Pro.local>\nCo-authored-by: Yunchu Lee <yunchu.lee@intel.com>\nCo-authored-by: ggiret-thinkdeep <146845847+ggiret-thinkdeep@users.noreply.github.com>\nCo-authored-by: Youho99 <gaylord.giret@viacesi.fr>\nCo-authored-by: Philippe Carvalho <31983398+phcarval@users.noreply.github.com>\nCo-authored-by: Wenjing Kang <wenjing.kang@intel.com>\nCo-authored-by: FanJiangIntel <fan.jiang@intel.com>\nCo-authored-by: belfner <belfner@belfner.com>\nCo-authored-by: Abdulla Al Blooshi <76493346+abdullamatar@users.noreply.github.com>\nCo-authored-by: Mat\u00edas Tailanian <895687+mtailanian@users.noreply.github.com>\nCo-authored-by: Jan Schl\u00fcter <github@jan-schlueter.de>\nCo-authored-by: Christopher <48522299+holzweber@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_ens10.py",
        "commit_date": "2023-11-28T08:12:22Z",
        "message": "Fix SPHINX inference memory with image input (#116)\n\n* fix embedding precision w/ image to save memory\n\n* pin gradio version"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_ens10.py",
        "commit_date": "2023-11-23T17:14:43Z",
        "message": "fix sphinx sphinx-1k sphinx-2k quant (#113)"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_ens10.py",
        "commit_date": "2023-11-18T16:56:22Z",
        "message": "Move base python path from LLaMA2-Accessory/accessory to LLaMA2-Accessory"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_ens10.py",
        "commit_date": "2023-11-07T15:48:47Z",
        "message": "llama ens 10 ens5p2 pose demo"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_ens5p2.py",
        "commit_date": "2023-11-18T16:56:22Z",
        "message": "Move base python path from LLaMA2-Accessory/accessory to LLaMA2-Accessory"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_ens5p2.py",
        "commit_date": "2023-11-07T15:48:47Z",
        "message": "llama ens 10 ens5p2 pose demo"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_adapter.py",
        "commit_date": "2023-11-18T16:56:22Z",
        "message": "Move base python path from LLaMA2-Accessory/accessory to LLaMA2-Accessory"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_adapter.py",
        "commit_date": "2023-10-17T17:12:30Z",
        "message": "support simplified env, fix demo bug"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_adapter.py",
        "commit_date": "2023-10-12T13:36:30Z",
        "message": "align llama_adapter to original repo, support padded resize"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_adapter.py",
        "commit_date": "2023-10-07T05:11:35Z",
        "message": "llama adpater v2 align with original"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_adapter.py",
        "commit_date": "2023-08-04T11:05:54Z",
        "message": "Support loading models trained with different model_parallel_world_size. (#16)\n\n* temp save\n\n* quick fix of demo memory issue\n\n* Refactor tensor creation dtype / device control.\n\nThis commit makes two changes during model creation:\n1. Decouples promote_trainable_params_to_fp32 from model __init__. This\n   is to avoid casting to fp32 to save memory in inference-only mode\n   (#4).\n2. Use a context manager to manage default tensor type change. In the\n   previous version, the default tensor type is reset to\n   torch.FloatTensor after creating the vision model, which is\n   technically incorrect and should be the previous default tensor type\n   instead. We implement our own context manager because the official\n   context managers seem to be incomplete at this time (PyTorch 2.0.1):\n   No dtype manager is provided and set_default_device is ineffective to\n   the torch.Tensor calls which are used in fairscale.\n\n* Change CLIP dtype management in llama.py\n\nIt is probably safer to keep CLIP at its original precision (e.g., fp16)\nregardless of the autocast setting: Some casting (e.g., from fp16 to\nbf16) may be lossy and can potentially harm the pre-trained model.\n\nKeep the changes to llama.py only at this moment since a lot of copy-\npasted codes may be refactored in the future (#3).\n\n* Respect args.precision when saving checkpoints.\n\n* Support checkpoint merge\n\nCheckpoint merge is suported in misc/tensor_parallel.py. Merge requires\nthat the checkpoint_mp_world_size % mp_world_size == 0. Support for\nsplit (i.e., when mp_world_size % checkpoint_mp_world_size == 0) and\nredistribute (for general mp_world_size and checkpoint_mp_world_size\nvalues) will be added in the future.\n\nAlso changing multi_turn demo to use the new loading function with merge\nsupport.\n\n* move printing trainable params\n\n* move training model creation back to cpu\n\nCloses #15, #13"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_adapter.py",
        "commit_date": "2023-08-02T06:24:04Z",
        "message": "update forward definition syntax to match llama_peft.py\n\nthis makes it easier to diff the two files to understand the differences in the implementation\n\nfor example:\n\nvim -d model/LLM/llama_adapter.py model/LLM/llama_peft.py\ndiff -uNdr model/LLM/llama_adapter.py model/LLM/llama_peft.py"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_adapter.py",
        "commit_date": "2023-07-30T06:54:48Z",
        "message": "llama_config nargs bug fix, llama_adapter refactor"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_adapter.py",
        "commit_date": "2023-07-25T06:23:44Z",
        "message": "refactor data configs to configs/data; add llama adapter"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_ens_peft.py",
        "commit_date": "2023-11-18T16:56:22Z",
        "message": "Move base python path from LLaMA2-Accessory/accessory to LLaMA2-Accessory"
    },
    {
        "repo_url": "github.com/Alpha-VLLM/LLaMA2-Accessory",
        "filepath": "accessory/model/LLM/llama_ens_peft.py",
        "commit_date": "2023-10-17T17:12:30Z",
        "message": "support simplified env, fix demo bug"
    },
    {
        "repo_url": "github.com/YuxinWenRick/hard-prompts-made-easy",
        "filepath": "run.py",
        "commit_date": "2023-02-10T01:54:36Z",
        "message": "add CLI script, clean up readme, clean up printing"
    },
    {
        "repo_url": "github.com/ali-vilab/i2vgen-xl",
        "filepath": "tools/modules/clip_embedder.py",
        "commit_date": "2023-12-14T14:55:47Z",
        "message": "add i2vgen and t2v code and model"
    },
    {
        "repo_url": "github.com/alaamaalouf/FollowAnything",
        "filepath": "follow_anything.py",
        "commit_date": "2023-08-13T15:15:37Z",
        "message": "follow_anything stop once it finishes reading offline video"
    },
    {
        "repo_url": "github.com/alaamaalouf/FollowAnything",
        "filepath": "follow_anything.py",
        "commit_date": "2023-08-01T14:18:07Z",
        "message": "code for detect, track, and follow + code for creating query features using DINO"
    },
    {
        "repo_url": "github.com/VoltaML/voltaML-fast-stable-diffusion",
        "filepath": "core/interrogation/clip.py",
        "commit_date": "2023-11-07T21:11:27Z",
        "message": "Redo imports from diffusers to satisfy Pylance"
    },
    {
        "repo_url": "github.com/VoltaML/voltaML-fast-stable-diffusion",
        "filepath": "core/interrogation/clip.py",
        "commit_date": "2023-08-02T17:36:09Z",
        "message": "Purge rich.progress and replace it with tqdm, queue progress added"
    },
    {
        "repo_url": "github.com/VoltaML/voltaML-fast-stable-diffusion",
        "filepath": "core/interrogation/clip.py",
        "commit_date": "2023-08-01T18:35:14Z",
        "message": "Arbitrary size + Inference cleanup (#128)\n\nCleaned the inference backend code a little bit, now it should have less\nredundant/copy-pasted code.\n\nAlso made odd/non-8-aligned sizes possible to do with PyTorch.\n*AIT - arbitrary sizes at the very least - has some peculiar issues with\nControlNet so I decided to drop it from this PR*\n\n---------\n\nCo-authored-by: Stax124 <tamoncz@gmail.com>\nCo-authored-by: Stax124 <60222162+Stax124@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/VoltaML/voltaML-fast-stable-diffusion",
        "filepath": "core/interrogation/clip.py",
        "commit_date": "2023-07-29T09:20:23Z",
        "message": "Quantized CLIP (#123)\n\n# TL;DR;\n- `/api/hardware/capabilities`\n- drops support for IREE (may come back later, just don't have the time\nto support it)\n- CLIP quantization\n- frontend: settings ui revamp\n  - more *optional, **disappearing*** settings based on conditions\n  - dynamic options based on capabilities of host machine\n# \nIn an attempt to further lower latency, I've tried to quantize the text\nencoder to 8bit/4bit.\nThis PR also includes some revamps to the settings menu and a new API\nendpoint `/hardware/capabilities`.\nThe endpoint returns a short run-down of what the hardware can do, an\nexample response is the following:\n```json5\n{\n\t\"supported_backends\": [\"cpu\", \"cuda\"],\n\t\"supported_precisions_cpu\": [\"float32\", \"bfloat16\"],\n\t\"supported_precisions_gpu\": [\"float32\", \"bfloat16\", \"float16\"],\n\t\"supported_torch_compile_backends\": [\"aot_ts_nvfuser\", \"cudagraphs\", \"inductor\", \"nvprims_nvfuser\", \"onnxrt\"],\n\t\"has_tensorfloat\": true, // true if user has ampere+\n\t\"has_tensor_cores\": true, // true if user has volta+\n\t\"supports_xformers\": false, // true if current installation supports xformers, false for torch nightlies\n\t\"supports_int8\": true // true if a basic quantized matmul is successful\n}\n```\nQuantization requires the installation of\n[bitsandbytes](https://github.com/TimDettmers/bitsandbytes/), however it\nis completely optional. Attached is the same generation (40 steps,\nDPMSolverSinglestep, seed: 123123, prompt: \"1girl\") at different\nquantization levels.\n\nTODO:\n\n- [ ] Update capabilities when something changes\n- [ ] Investigate print-out when loading models with quantization\n\nFull precision:\n\n![f711ee6d-c547-4880-886a-7b3c0b054f03-0](https://github.com/VoltaML/voltaML-fast-stable-diffusion/assets/22561485/0f5b2c96-cc95-4b73-b798-604b18dcb84a)\n\n8-bit quantization:\n\n![58a32c8f-735e-4640-aec9-2239bab7bb6d-0](https://github.com/VoltaML/voltaML-fast-stable-diffusion/assets/22561485/218b3697-408c-4df6-985c-17685ea28fff)\n\n4-bit quantization:\n\n![8fe123f0-8246-42c7-b14e-42f6538db7f3-0](https://github.com/VoltaML/voltaML-fast-stable-diffusion/assets/22561485/afb06e0b-b467-47cc-adba-d4c41766327b)\n\n---------\n\nCo-authored-by: Stax124 <tamoncz@gmail.com>"
    },
    {
        "repo_url": "github.com/VoltaML/voltaML-fast-stable-diffusion",
        "filepath": "core/interrogation/clip.py",
        "commit_date": "2023-06-18T11:41:08Z",
        "message": "Affix deps, fix AITemplate, set SDPA as default attention"
    },
    {
        "repo_url": "github.com/VoltaML/voltaML-fast-stable-diffusion",
        "filepath": "core/interrogation/clip.py",
        "commit_date": "2023-05-18T17:14:47Z",
        "message": "Autocast, VAE tiling & fix offload (#82)\n\n* autocast stuff\n\n* autocast pt. 2\n\n* enable offload\n\n* autocast\n\n* frontend\n\n* Fix model offload... again...\n\n* Precision changes & autocast fixes\n\n* fix model offload in img2img\n\n* fix no-offload\n\n* only initialize autocast if directml autocast is needed\n\n* fix disable=True on torch.dml.autocast\n\n* Build frontend with locked deps\n\n* onnx fix\n\n* fix ait\n\n* reorder sorts\n\n---------\n\nCo-authored-by: Stax124 <tamoncz@gmail.com>"
    },
    {
        "repo_url": "github.com/VoltaML/voltaML-fast-stable-diffusion",
        "filepath": "core/interrogation/clip.py",
        "commit_date": "2023-05-06T17:10:05Z",
        "message": "Multiple device support & PyTorch optimizations (#62)\n\n* Start work on further jit trace options\n\n* format\n\n* AMD gpu info support & memory clear option\n\n* clear_memory_policy\n\n* Fix manager\n\n* Cuda flags\n\n* Pyamdgpuinfo\n\n* format\n\n* Cleanup, refactor\n\n* format\n\n* oops\n\n* fix perf_loop\n\n* format\n\n* fix everything\n\n* format\n\n* Formatting, support for extra requirement check for Linux\n\n* trace\n\n* format\n\n* UI, renames, cleanup\n\n* PyLint cleanups\n\n* Rebuild frontend\n\n* no more one-way\n\n* only enable reduced prec on ampere+\n\n* Cleaning stuff up & HW Scheduling check on Windows\n\n* format\n\n* Add DirectML support\n\n* fix stuffix stuff\n\n* a\n\n* ipex stuff pt. 1\n\n* Revamp #install_pytorch() and further IPEX optimizations\n\n* try & fix xpu\n\n* fix pt2\n\n* fix requirements\n\n* ipex fix\n\n* XPU changes\n\n* Revamp install_requirements.py & fix config order\n\n* fix interrogators on directml\n\n* start work on iree\n\n* fix things\n\n* iree progress\n\nI GOT PAST TORCH-MLIR :tada:\nnow I just need to get iree to work as well :)\n\n* cleanup\n\n* basic profiling\n\n* Black, fix some merge issues\n\n* Get rid of the profiler\n\n* Reformat\n\n* Fix .vscode/settings.json\n\n* Cleanup, purge unnecessary stuff\n\n* Fix TextualInversions on Windows\n\n* Get rid of duped file, add spinner to optimization\n\n* Frontend add more backend settings\n\n---------\n\nCo-authored-by: Stax124 <tamoncz@gmail.com>"
    },
    {
        "repo_url": "github.com/VoltaML/voltaML-fast-stable-diffusion",
        "filepath": "core/interrogation/clip.py",
        "commit_date": "2023-04-23T10:48:33Z",
        "message": "Interrogators (#56)\n\n* try to quantize things\n\n* Start work on interrogators\n\n(Deepdanbooru is the only one tested, and it works)\n\n* finish interrogation work\n\n* Revert lwp_sd.py\n\n* Refactor\n\n* Partial Frontend for tagger\n\n* Fix requirement installer\n\n* Frontend functional\n\n---------\n\nCo-authored-by: Stax124 <tamoncz@gmail.com>"
    },
    {
        "repo_url": "github.com/lancedb/vectordb-recipes",
        "filepath": "examples/arxiv-recommender/main.py",
        "commit_date": "2024-02-01T16:01:42Z",
        "message": "linting  (#130)\n\n* talk with podcast\n\n* added talk-with-podcast\n\n* imgs\n\n* updated readme\n\n* added requirements file\n\n* added support for Langroid\n\n* typo\n\n* requirements update\n\n* path and colab fix\n\n* sentiment link fix\n\n* comments in colab\n\n* fix\n\n* description change\n\n* doc link fix\n\n* talk-with-wikipedia\n\n* added pylinting\n\n* added pylinting\n\n* removed pylint\n\n* lint test\n\n* lint path\n\n* added polar\n\n---------\n\nCo-authored-by: Ayush Chaurasia <ayush.chaurarsia@gmail.com>"
    },
    {
        "repo_url": "github.com/kabachuha/sd-webui-text2video",
        "filepath": "scripts/modelscope/clip_hardcode.py",
        "commit_date": "2023-05-30T14:23:45Z",
        "message": "relicense sd-webui-text2video under AGPL v3.0\n\nIncorporates the original ModelScope code into the extension and licenses them both under the Apache 2.0-compatible AGPL v3.0"
    },
    {
        "repo_url": "github.com/kabachuha/sd-webui-text2video",
        "filepath": "scripts/modelscope/clip_hardcode.py",
        "commit_date": "2023-04-20T19:49:26Z",
        "message": "Fix[122b] fixes 3rd reported exception found in #122 - clip_hardcode :: missing opts.use_old_emphasis_implementation"
    },
    {
        "repo_url": "github.com/OpenGVLab/Instruct2Act",
        "filepath": "engine_robotic.py",
        "commit_date": "2023-05-18T12:21:58Z",
        "message": "update the readme && remove some useless lines"
    },
    {
        "repo_url": "github.com/Nerogar/OneTrainer",
        "filepath": "modules/module/HPSv2ScoreModel.py",
        "commit_date": "2023-10-25T16:38:57Z",
        "message": "load HPSv2 with the correct precision"
    },
    {
        "repo_url": "github.com/mlcommons/inference",
        "filepath": "text_to_image/tools/clip/clip_encoder.py",
        "commit_date": "2023-12-19T17:25:38Z",
        "message": "Match clip and fid score calculation (#1536)\n\n* Add SD model links in README\n\n* Match clip and fid score calculation"
    },
    {
        "repo_url": "github.com/deforum-art/deforum-stable-diffusion",
        "filepath": "src/ldm/modules/encoders/modules.py",
        "commit_date": "2022-12-31T22:26:12Z",
        "message": "fix defaults, errors, print statements, etc"
    },
    {
        "repo_url": "github.com/yangxy/PASD",
        "filepath": "test_pasd.py",
        "commit_date": "2023-10-18T02:03:58Z",
        "message": "fix bug & add offset noise"
    },
    {
        "repo_url": "github.com/ali-vilab/videocomposer",
        "filepath": "tools/videocomposer/inference_multi.py",
        "commit_date": "2023-06-15T13:08:41Z",
        "message": "The first version of the code and model"
    },
    {
        "repo_url": "github.com/ali-vilab/videocomposer",
        "filepath": "tools/videocomposer/inference_single.py",
        "commit_date": "2023-06-15T13:08:41Z",
        "message": "The first version of the code and model"
    },
    {
        "repo_url": "github.com/mertyg/vision-language-models-are-bows",
        "filepath": "model_zoo/__init__.py",
        "commit_date": "2023-02-20T09:29:10Z",
        "message": "initial release. Sorry for the delay and potential missing bits, please see the important note in readme."
    },
    {
        "repo_url": "github.com/lucidrains/gigagan-pytorch",
        "filepath": "gigagan_pytorch/open_clip.py",
        "commit_date": "2023-07-21T00:26:47Z",
        "message": "for conditional training, complete the auxiliary clip contrastive loss"
    },
    {
        "repo_url": "github.com/lucidrains/gigagan-pytorch",
        "filepath": "gigagan_pytorch/open_clip.py",
        "commit_date": "2023-04-06T15:36:56Z",
        "message": "auxiliary losses almost all complete, save for text conditioning within vision aided discriminator"
    },
    {
        "repo_url": "github.com/lucidrains/gigagan-pytorch",
        "filepath": "gigagan_pytorch/open_clip.py",
        "commit_date": "2023-04-04T20:03:50Z",
        "message": "clean solution to reconstruction loss from any fmap resolution in the discriminator"
    },
    {
        "repo_url": "github.com/lucidrains/gigagan-pytorch",
        "filepath": "gigagan_pytorch/open_clip.py",
        "commit_date": "2023-04-03T18:07:28Z",
        "message": "make some headway into vision-aided gan loss"
    },
    {
        "repo_url": "github.com/lucidrains/gigagan-pytorch",
        "filepath": "gigagan_pytorch/open_clip.py",
        "commit_date": "2023-03-29T20:53:19Z",
        "message": "knock out two of the aux losses"
    },
    {
        "repo_url": "github.com/mkshing/e4t-diffusion",
        "filepath": "e4t/encoder.py",
        "commit_date": "2023-03-16T23:08:18Z",
        "message": "fix clip pooling part to align with the paper"
    },
    {
        "repo_url": "github.com/Nota-NetsPresso/BK-SDM",
        "filepath": "src/eval_clip_score.py",
        "commit_date": "2024-02-27T14:26:01Z",
        "message": "#51 update package and copyright info"
    },
    {
        "repo_url": "github.com/LAION-AI/CLIP_benchmark",
        "filepath": "clip_benchmark/models/open_clip.py",
        "commit_date": "2023-01-05T22:50:20Z",
        "message": "Support Japanese CLIP by rinna (#50)\n\n* support japanese clip\n\n* Add comments\n\n* add `ja_clip` flag to base_args for passing tests\n\n* add japanese clip to features in readme\n\n* support japanese-clip for retrieval\n\n* load proper model by model_type\n\n* fix test\n\n* undo changes in metrics\n\n* add wrapper for ja_clip\n\n* Rename models.py to model_collection.py\n\n* use model_collection in cli.py\n\n* import os in cli.py\n\n* Update cli.py\n\n* delete duplicate of loading `model_collection`\n\n* rename a var `model` to `model_name` in models\n\n* small fixes for better name in japanese_clip.py\n\n* add How to add other CLIP models in readme\n\nCo-authored-by: Romain Beaumont <romain.rom1@gmail.com>"
    },
    {
        "repo_url": "github.com/pharmapsychotic/clip-interrogator",
        "filepath": "clip_interrogator/clip_interrogator.py",
        "commit_date": "2023-03-20T03:43:46Z",
        "message": "Support for many different caption models:\nblip-base, blip-large, blip2-2.7b, blip2-flan-t5-xl, git-large-coco"
    },
    {
        "repo_url": "github.com/pharmapsychotic/clip-interrogator",
        "filepath": "clip_interrogator/clip_interrogator.py",
        "commit_date": "2023-03-20T01:02:23Z",
        "message": "Expose LabelTable and load_list and give example in README how they can be used to rank your own list of terms."
    },
    {
        "repo_url": "github.com/pharmapsychotic/clip-interrogator",
        "filepath": "clip_interrogator/clip_interrogator.py",
        "commit_date": "2023-02-20T22:11:33Z",
        "message": "More safetensor, download, and VRAM improvements"
    },
    {
        "repo_url": "github.com/pharmapsychotic/clip-interrogator",
        "filepath": "clip_interrogator/clip_interrogator.py",
        "commit_date": "2023-02-19T04:29:36Z",
        "message": "When blip_offload enabled keep BLIP model on CPU to start."
    },
    {
        "repo_url": "github.com/pharmapsychotic/clip-interrogator",
        "filepath": "clip_interrogator/clip_interrogator.py",
        "commit_date": "2023-02-18T20:53:02Z",
        "message": "safetensors!\n- store cached embeddings in safetensor format\n- updated huggingface ci-preprocess repo\n- bumped version to 0.5.0"
    },
    {
        "repo_url": "github.com/pharmapsychotic/clip-interrogator",
        "filepath": "clip_interrogator/clip_interrogator.py",
        "commit_date": "2023-02-05T18:18:12Z",
        "message": "0.4.2:\n- upgrade chain to take a min_count parameter so it won't early out until it has considered at least min_count flavors\n- interrogate method (\"best\" mode) also checks against classic and fast to use their output if it's better\n- fix bug of config.download_cache option not being used!\n- add notes on Config object to readme"
    },
    {
        "repo_url": "github.com/pharmapsychotic/clip-interrogator",
        "filepath": "clip_interrogator/clip_interrogator.py",
        "commit_date": "2023-02-05T00:32:49Z",
        "message": "Bunch of updates! (#40)\n\n- auto download the cache files from huggingface\n- experimental negative prompt mode\n- slight quality and performance improvement to best mode\n- analyze tab in Colab and run_gradio to get table of ranked terms"
    },
    {
        "repo_url": "github.com/pharmapsychotic/clip-interrogator",
        "filepath": "clip_interrogator/clip_interrogator.py",
        "commit_date": "2023-01-06T01:44:38Z",
        "message": "Make the BLIP model configurable, can set config.blip_model_type now to 'base' or 'large'"
    },
    {
        "repo_url": "github.com/pharmapsychotic/clip-interrogator",
        "filepath": "clip_interrogator/clip_interrogator.py",
        "commit_date": "2022-11-29T22:38:48Z",
        "message": "0.3.1 fix for running on cpu, update readme usage instructions"
    },
    {
        "repo_url": "github.com/pharmapsychotic/clip-interrogator",
        "filepath": "clip_interrogator/clip_interrogator.py",
        "commit_date": "2022-11-28T18:36:24Z",
        "message": "Handle exception trying to load cached table"
    },
    {
        "repo_url": "github.com/pharmapsychotic/clip-interrogator",
        "filepath": "clip_interrogator/clip_interrogator.py",
        "commit_date": "2022-11-28T18:30:39Z",
        "message": "Default to ViT-L, lower intermediate count for Colab with ViT-H"
    },
    {
        "repo_url": "github.com/pharmapsychotic/clip-interrogator",
        "filepath": "clip_interrogator/clip_interrogator.py",
        "commit_date": "2022-11-27T23:54:13Z",
        "message": "Ability to swap CLIP models (takes about 5s for ViTL and 10s for ViTH), update Replicate cog"
    },
    {
        "repo_url": "github.com/pharmapsychotic/clip-interrogator",
        "filepath": "clip_interrogator/clip_interrogator.py",
        "commit_date": "2022-11-25T16:33:13Z",
        "message": "Handle differences in how open_clip does prompt truncation, run_gradio support for all the open_clip models and --share option."
    },
    {
        "repo_url": "github.com/pharmapsychotic/clip-interrogator",
        "filepath": "clip_interrogator/clip_interrogator.py",
        "commit_date": "2022-11-25T15:26:00Z",
        "message": "Shuffle BLIP back to system RAM to help with 16GB Colab"
    },
    {
        "repo_url": "github.com/pharmapsychotic/clip-interrogator",
        "filepath": "clip_interrogator/clip_interrogator.py",
        "commit_date": "2022-11-24T21:27:28Z",
        "message": "First test version with OpenCLIP and ViTH!"
    },
    {
        "repo_url": "github.com/pharmapsychotic/clip-interrogator",
        "filepath": "clip_interrogator/clip_interrogator.py",
        "commit_date": "2022-11-21T20:17:36Z",
        "message": "Update notebook batch processing with option to rename files so can be used with [filewords] in Dreambooth!\n- new `quiet` config option so CLIP Interrogator doesn't print and tqdm\n- `max_flavors` option to each interrogate method"
    },
    {
        "repo_url": "github.com/pharmapsychotic/clip-interrogator",
        "filepath": "clip_interrogator/clip_interrogator.py",
        "commit_date": "2022-11-06T04:21:13Z",
        "message": "Gradio version plus classic and fast modes"
    },
    {
        "repo_url": "github.com/data2ml/all-clip",
        "filepath": "all_clip/open_clip.py",
        "commit_date": "2024-01-21T20:47:24Z",
        "message": "Split in one file per model."
    },
    {
        "repo_url": "github.com/tgxs002/align_sd",
        "filepath": "process_diffusiondb.py",
        "commit_date": "2023-05-10T12:34:17Z",
        "message": "highlight updates, fix typo, add regularization images"
    },
    {
        "repo_url": "github.com/Anything-of-anything/Anything-3D",
        "filepath": "AnyObject3D/src/3DFuse/ldm/modules/encoders/modules.py",
        "commit_date": "2023-06-09T10:09:44Z",
        "message": "[feats] release basic code for AnyObject3D"
    },
    {
        "repo_url": "github.com/mlfoundations/datacomp",
        "filepath": "eval_utils/wds_eval.py",
        "commit_date": "2023-07-22T20:33:48Z",
        "message": "Style fix (#35)\n\n* precommit changes\n\n* add precommit hook"
    },
    {
        "repo_url": "github.com/kerrj/lerf",
        "filepath": "lerf/encoders/openclip_encoder.py",
        "commit_date": "2024-01-18T00:12:46Z",
        "message": "Update paths+configs to nerfstudio 1.0 version"
    },
    {
        "repo_url": "github.com/kerrj/lerf",
        "filepath": "lerf/encoders/openclip_encoder.py",
        "commit_date": "2023-04-19T21:06:17Z",
        "message": "refactor to fix handle in callback"
    },
    {
        "repo_url": "github.com/kerrj/lerf",
        "filepath": "lerf/encoders/openclip_encoder.py",
        "commit_date": "2023-04-14T22:03:56Z",
        "message": "code for using new viewer with custom ViewerText"
    },
    {
        "repo_url": "github.com/concept-graphs/concept-graphs",
        "filepath": "conceptgraph/scripts/eval_replica_semseg.py",
        "commit_date": "2024-01-16T06:40:59Z",
        "message": "add sem seg evaluation script on replica"
    },
    {
        "repo_url": "github.com/facebookresearch/MetaCLIP",
        "filepath": "tests/simple_test.py",
        "commit_date": "2023-12-21T22:37:02Z",
        "message": "replace customized config class w/ dataclass"
    },
    {
        "repo_url": "github.com/concept-graphs/concept-graphs",
        "filepath": "conceptgraph/scripts/generate_gsa_results.py",
        "commit_date": "2024-02-28T04:49:23Z",
        "message": "refactor get clip feature function to be batched"
    },
    {
        "repo_url": "github.com/concept-graphs/concept-graphs",
        "filepath": "conceptgraph/scripts/generate_gsa_results.py",
        "commit_date": "2024-02-23T04:39:42Z",
        "message": "change outdated tag2text stuff to ram stuff"
    },
    {
        "repo_url": "github.com/facebookresearch/MetaCLIP",
        "filepath": "tests/pretrained_test.py",
        "commit_date": "2023-12-21T22:37:02Z",
        "message": "replace customized config class w/ dataclass"
    },
    {
        "repo_url": "github.com/lucidrains/perfusion-pytorch",
        "filepath": "perfusion_pytorch/open_clip.py",
        "commit_date": "2023-08-14T16:07:14Z",
        "message": "add a function that can accept open clip, a bunch of prompts as List[str], and return the C covariance matrix needed"
    },
    {
        "repo_url": "github.com/painebenjamin/app.enfugue.ai",
        "filepath": "src/python/enfugue/diffusion/animate/dragnuwa/svd/modules/encoders/modules.py",
        "commit_date": "2024-01-13T02:28:55Z",
        "message": "pass typecheck/importcheck hundreds more changes to nuwa"
    },
    {
        "repo_url": "github.com/painebenjamin/app.enfugue.ai",
        "filepath": "src/python/enfugue/diffusion/animate/dragnuwa/svd/modules/encoders/modules.py",
        "commit_date": "2024-01-11T04:17:15Z",
        "message": "add dragnuwa, add motion vector editor to front end and back end, numerous changes"
    },
    {
        "repo_url": "github.com/painebenjamin/app.enfugue.ai",
        "filepath": "src/python/enfugue/diffusion/support/upscale/ccsr/ldm/modules/encoders/modules.py",
        "commit_date": "2024-01-27T07:29:15Z",
        "message": "add ccsr, fix hotshot motion attn"
    },
    {
        "repo_url": "github.com/concept-graphs/concept-graphs",
        "filepath": "conceptgraph/scripts/streamlined_detections.py",
        "commit_date": "2024-02-28T04:49:58Z",
        "message": "add clip fts, add profiling code, other small updates"
    },
    {
        "repo_url": "github.com/NVIDIA/NeMo-Megatron-Launcher",
        "filepath": "launcher_scripts/nemo_launcher/collections/eval_diffusion_fid_clip/compute_clip_score.py",
        "commit_date": "2024-02-06T22:48:23Z",
        "message": "Multimodal Launcher Merge (#186)\n\n* Add initial support for multimodal data preparation\n\n* Add multimodal training\n\n* Add multimodal conversion and finetuning\n\n* Add multimodal in fw inference\n\n* Fix code style, add comments, refactor yaml config to make more readable\n\n* Update multimodal configs in fw inference\n\n* Add SD conf\n\n* Add final two sub stages: precache encodings and output wdinfo;\nUnify argument names;\nChange encoder class paths to new repo (multimodal_merge)\n\n* pycharm reformat\n\n* minor changes\n\n* Fix bug where precache encodings couldn't use multiple GPUs on the same node\n\n* Add multimodal readme\n\n* Add model configurations\n\n* Update configs\n\n* Update configs for CLIP\n\n* Add support for downloading dataset for instruct_pix2pix\n\n* Add Dreambooth and SD fix\n\n* Add vision and multimodal to internal/main\n\n* Fix sd default config\n\n* Update dreambooth conf\n\n* Remove glue download args\n\n* dreambooth inference script path changes\n\n* fwk inference path change\n\n* fwk inference path change\n\n* Add seed\n\n* Add seed to stable diffusion inference\n\n* Fix SD confs\n\n* Add evaluation for ViT and CLIP\n\n* Add nemo checkpoint saving in ViT fine-tuning\n\n* Fix ins-p2p conversion path\n\n* Add Imagen Launcher Support\n\n* Imagen: disable inductor by default\n\n* change micro bs => 8 for 2b model\n\n* Update dreambooth config\n\n* Add download and preprocess scripts for fid evaluation (mscoco 2014)\n\n* Add a few advanced features for precaching\n\n* Fix conversion for SD legacy checkpoints\n\n* Fix version base\n\n* Iputterman/deployment\n\n(cherry picked from commit 3a097c50e0b89642d048d7588fb0c6f9858b387f)\n\n* Fix SD configuration\n\n* Disable save nemo on train end\n\n* Remove coco download scripts\n\n* headers for deployment\n\n* change data paths in training configs to be consistent with dataprep scripts\n\n* Add Diffusion Modal Eval\n\n* Clean configs and update readme\n\n* Remove auto configurator as it doesn't support multimodal ATM\n\n* Update default configs\n\n* Update feature matrix\n\n* Update feature matrix\n\n* Update dreambooth related configs.\n\n* Update dreambooth readme\n\n* Give a default value to vae.bin since it's required\n\n* Update readme\n\n* readme revised\n\n* small updates\n\n* uppercase for SD\n\n* Fix diffusion eval\n\n* Update SD training readme\n\n* Fix override tweaks\n\n* Update Ins P2P example\n\n* Fix readme format\n\n* Update SD training readme about dataset size\n\n* Add number of images\n\n* Update readme and add more training configs.\n\n* FID eval now only accepts predownloaded tf_inception weights.\n\n* Remove version, add local_clip_files arg to training scripts\n\n* Fix format\n\n* Update LICENSE\n\n* Safe remove Dockerfile in multimodal\n\n* Update LICENSE format\n\n* Fix bug\n\n* Add change log and reformat\n\n* Fix README.md\n\n* Fix image path\n\n* readme revised\n\n* Make eval scripts work\n\n* Revert \"Remove version, add local_clip_files arg to training scripts\"\n\nThis reverts commit ee03ea834390db29041391438737c1001ace9416.\n\n* Revert \"FID eval now only accepts predownloaded tf_inception weights.\"\n\nThis reverts commit a66069d659525dc9cc769bfe3117804df3e8dd5b.\n\n* Add clip version\n\n* Filtering only works with infinite sampler.\n\n* Update readme\n\n* Minor updates and adding more remarks\n\n* readme revised\n\n* readme revised\n\n* update container in config\n\n* readme revised\n\n* changelog->release notes\n\n* exp manager name changed\n\n* Update example image for instruct p2p\n\n* Delete imagen related folders\n\n* Remove imagen\n\n* Update default precision\n\n* license updated\n\n* name needs to be fixed\n\n* Update readme for Dreambooth source images.\n\n* Update save dir\n\n* Inference now looks for ckpt in conversion results\n\n* dreambooth inference name changed\n\n* README edits\n\n* add nemo sha to support matrix\n\n* Update ToC\n\n* Fix typo\n\n* update support matrix\n\n* Fix README.md format\n\n* Precommit checks CI job\n\n* run style-check only on mrs\n\n* Multiple precaching improvements:\n- file objects in tar now have the same name as source tar file\n- fix bug where encoder for each task was moved to GPU0, leading to OOM for large encoders\n- add option to save original video in tarfile\n\n* add merge_source_tar as a substage in multimodal dataprep\n\n* sort content of tarfiles after appending, otherwise webdataset cannot read it\n\n* more error handling in merge_source_tar\n\n* sort input for better determinism\n\n* remove personal info in path\n\n* update default hydra config\n\n* update readme to mention new substage and update yaml for example usage\n\n* maybe fix the CI for multimodal export conf\n\n* Updates related to SD 1.5\n\n* Update Distributed Optimizer Readme and Configs\n\n* Imagen Launcher Configs\n\n* Update NeMo clip configs on Launcher\n\n* Fix local_root_path in imagen\n\n* Add external conversion stage; add hf clip and openclip conversion\n\n* Add conversion from external checkpoint for CLIP in README.md\n\n* Fix readme language\n\n* Change dependency `aftercorr` to `afterok`\n\n* Fix typo\n\n* Controlnet and SD nhwc support\n\n* Imagen Launcher Completeness\n\n* ControlNet Triton+TRT\n\n* Add SD2 support\n\n* Add the pictures\n\n* Update SD2 readme\n\n* Imagen Readme\n\n* Sd readme update\n\n* Imagen: CI Fix exporting script issue.\n\n* Add clip_version in fid_clip yaml\n\n* space fixed\n\n* update imagen A100 vs. H100 batch size\n\n* Add comment on using larger bs for Imagen training\n\n* Sd2 fix\n\n* rephrase batch size issue\n\n* Clean known issues\n\n* Rename `replace_sampler_ddp`\n\n* Add neva support\n\n* Imagen 23.08 Launcher Support\n\n* Update `save_nemo_on_train_end` default to True in mm\n\n* Update neva\n\n* Update config for nemo nightly\n\n* Fix duplicated yaml file\n\n* Fix wandb default names\n\n* Fix template path\n\n* Fix default wandb logger name\n\n* Fix llama2 config\n\n* enable EMA for imagen base64-500m\n\n* Content filtering launcher\n\n* Update NeVa Configurations and README\n\n* Add neva conversion yaml\n\n* Fix neva conversion yaml\n\n* Fix launcher\n\n* Update Dreambooth config\n\n* Add neva results\n\n* Fix README.md\n\n* Fix README.md\n\n* Update sd config for o2\n\n* Revert \"Update sd config for o2\"\n\n* Fix path\n\n* Update file nsfw_L_14.yaml\n\n* DreamFusion\n\n* Fix ToC\n\n* Fix Readme\n\n* Add NeVa TRT support\n\n* Update readme of dreambooth.\n\n* Fix TRT polography issues\n\n* update README.md\n\n* Update SD perf\n\n* config default precision switch to bf16\n\n* update known issues\n\n* Update Imagen README\n\n* typo on Imagen README\n\n* Fix readme\n\n* Fix readme\n\n* Fix readme\n\n* Fix default container path\n\n* Add neva peft config\n\n* Update HP\n\n* Update module freeze\n\n* Fix README.md\n\n* Change quick-gelu to approx-gelu\n\n* Add 70B config\n\n* Update 70B config\n\n* Update llava 1.5 in configs\n\n* Update llava 1.5 in configs\n\n* Update llava 1.5 finetune configs\n\n* Update llava 1.5 finetune configs\n\n* Fix one epoch step\n\n* Update for batched inference pipeline\n\n* Update code paths in stages\n\n* Update few files\n\n* add back few files\n\n* add back few mm files\n\n* path fix\n\n* Update default filename\n\n* Neva default config update\n\n* SD path fix\n\n* fix target\n\n* Remove not required stuff\n\n* Few fixes\n\n* reformat\n\n* fix unit test\n\n---------\n\nCo-authored-by: Chen Cui <chcui@nvidia.com>\nCo-authored-by: Yu Yao <yuya@nvidia.com>\nCo-authored-by: Mingyuan Ma <mingyuanm@nvidia.com>\nCo-authored-by: Ao Tang <aot@nvidia.com>\nCo-authored-by: Izzy Putterman <iputterman@nvidia.com>\nCo-authored-by: ntajbakhsh <ntajbakhsh@nvidia.com>\nCo-authored-by: Maanu Grover <maanug@nvidia.com>\nCo-authored-by: Aastha <aasthaj@nvidia.com>\nCo-authored-by: Alexandre Milesi <alexandrem@nvidia.com>\nCo-authored-by: Bobby Chen <bobchen@nvidia.com>\nCo-authored-by: Lukasz Pierscieniewski <lukaszp@nvidia.com>\nCo-authored-by: Ahmad Kiswani <kiswani.ahmad@gmail.com>"
    },
    {
        "repo_url": "github.com/d8ahazard/sd_smartprocess",
        "filepath": "clipinterrogator.py",
        "commit_date": "2023-01-03T02:59:28Z",
        "message": "Add min/max CLIP length, don't adjust image size unless we tell it to"
    },
    {
        "repo_url": "github.com/d8ahazard/sd_smartprocess",
        "filepath": "clipinterrogator.py",
        "commit_date": "2022-12-10T23:33:34Z",
        "message": "Super Update\n\nAdd WD14 tagger.\nAdd CLIP v2.1 interrogator.\nAllow filtering wd14, booru tags by score.\nAdd don't rename option.\nUpdate ReallySafe\nBump BLIP version?\nRemove split image options.\nCompletely overhaul smartprocess..."
    },
    {
        "repo_url": "github.com/wolverinn/stable-diffusion-multi-user",
        "filepath": "modules/sd_disable_initialization.py",
        "commit_date": "2023-09-04T15:19:23Z",
        "message": "update sd-webui version to torch2.0 and add extension support"
    },
    {
        "repo_url": "github.com/wolverinn/stable-diffusion-multi-user",
        "filepath": "repositories/stable-diffusion-stability-ai/ldm/modules/encoders/modules.py",
        "commit_date": "2023-09-04T15:19:23Z",
        "message": "update sd-webui version to torch2.0 and add extension support"
    },
    {
        "repo_url": "github.com/ChenDelong1999/RemoteCLIP",
        "filepath": "retrieval.py",
        "commit_date": "2023-11-07T02:54:39Z",
        "message": "Modify retrieval evaluation information to improve readibility"
    },
    {
        "repo_url": "github.com/neggles/sd-webui-arc",
        "filepath": "modules/sd_disable_initialization.py",
        "commit_date": "2023-02-05T08:21:00Z",
        "message": "make it possible to load SD1 checkpoints without CLIP"
    },
    {
        "repo_url": "github.com/neggles/sd-webui-arc",
        "filepath": "modules/sd_disable_initialization.py",
        "commit_date": "2023-01-22T05:20:48Z",
        "message": "fix missing field for aesthetic embedding extension"
    },
    {
        "repo_url": "github.com/neggles/sd-webui-arc",
        "filepath": "modules/sd_disable_initialization.py",
        "commit_date": "2023-01-11T15:54:13Z",
        "message": "fix for an error caused by skipping initialization, for realsies this time: TypeError: expected str, bytes or os.PathLike object, not NoneType"
    },
    {
        "repo_url": "github.com/neggles/sd-webui-arc",
        "filepath": "modules/sd_disable_initialization.py",
        "commit_date": "2023-01-10T14:46:59Z",
        "message": "add support for transformers==4.25.1\nadd fallback for when quick model creation fails"
    },
    {
        "repo_url": "github.com/neggles/sd-webui-arc",
        "filepath": "modules/sd_disable_initialization.py",
        "commit_date": "2023-01-10T13:51:04Z",
        "message": "add more stuff to ignore when creating model from config\nprevent .vae.safetensors files from being listed as stable diffusion models"
    },
    {
        "repo_url": "github.com/neggles/sd-webui-arc",
        "filepath": "modules/sd_disable_initialization.py",
        "commit_date": "2023-01-10T11:08:29Z",
        "message": "disable torch weight initialization and CLIP downloading/reading checkpoint to speedup creating sd model from config"
    },
    {
        "repo_url": "github.com/neggles/neurosis",
        "filepath": "src/neurosis/models/text_encoder/clip.py",
        "commit_date": "2023-12-12T04:04:28Z",
        "message": "make HF shut up when loading CLIP subsections"
    },
    {
        "repo_url": "github.com/neggles/neurosis",
        "filepath": "src/neurosis/models/text_encoder/clip.py",
        "commit_date": "2023-12-10T05:32:29Z",
        "message": "get uwu about `NotImplementedError`s and pedantic about `ValueError`s\n\nalso further battles won in the fight against assertions"
    },
    {
        "repo_url": "github.com/neggles/neurosis",
        "filepath": "src/neurosis/models/text_encoder/clip.py",
        "commit_date": "2023-12-06T05:24:25Z",
        "message": "clean up some clip init logic"
    },
    {
        "repo_url": "github.com/neggles/neurosis",
        "filepath": "src/neurosis/models/text_encoder/clip.py",
        "commit_date": "2023-12-06T03:57:52Z",
        "message": "ok i think this works now. SD1.5 sampling is still broke"
    },
    {
        "repo_url": "github.com/neggles/neurosis",
        "filepath": "src/neurosis/models/text_encoder/clip.py",
        "commit_date": "2023-12-06T03:57:52Z",
        "message": "committing as-is, throws error, good luck salt"
    },
    {
        "repo_url": "github.com/neggles/neurosis",
        "filepath": "src/neurosis/models/text_encoder/clip.py",
        "commit_date": "2023-11-12T04:31:19Z",
        "message": "Add new functions and fix bugs in various modules. Thing works now maybe."
    },
    {
        "repo_url": "github.com/neggles/neurosis",
        "filepath": "src/neurosis/models/text_encoder/clip.py",
        "commit_date": "2023-08-04T16:01:23Z",
        "message": "more refactoring, loads now but HF dataset eats itself"
    },
    {
        "repo_url": "github.com/neggles/neurosis",
        "filepath": "src/neurosis/models/text_encoder/clip.py",
        "commit_date": "2023-08-02T02:52:25Z",
        "message": "pull in a bunch of code from generative-models"
    },
    {
        "repo_url": "github.com/leptonai/examples",
        "filepath": "advanced/open-clip/open-clip.py",
        "commit_date": "2023-08-15T21:42:04Z",
        "message": "chore(examples): merge the clip model code form leptonai/lepton to here"
    },
    {
        "repo_url": "github.com/leptonai/examples",
        "filepath": "advanced/open-clip/open-clip.py",
        "commit_date": "2023-08-14T23:48:44Z",
        "message": "feat(clip) : add clip as an example"
    },
    {
        "repo_url": "github.com/lucidrains/classifier-free-guidance-pytorch",
        "filepath": "classifier_free_guidance_pytorch/open_clip.py",
        "commit_date": "2023-03-08T16:07:13Z",
        "message": "fix open clip model name, address https://github.com/lucidrains/classifier-free-guidance-pytorch/issues/1"
    },
    {
        "repo_url": "github.com/lucidrains/classifier-free-guidance-pytorch",
        "filepath": "classifier_free_guidance_pytorch/open_clip.py",
        "commit_date": "2022-12-16T19:36:44Z",
        "message": "demonstrate ability to condition on both CLIP and T5, as in most recent nvidia text-to-image paper"
    },
    {
        "repo_url": "github.com/lucidrains/classifier-free-guidance-pytorch",
        "filepath": "classifier_free_guidance_pytorch/open_clip.py",
        "commit_date": "2022-12-15T19:47:47Z",
        "message": "get ready to condition robotic transformer"
    },
    {
        "repo_url": "github.com/lucidrains/classifier-free-guidance-pytorch",
        "filepath": "classifier_free_guidance_pytorch/open_clip.py",
        "commit_date": "2022-12-15T17:53:39Z",
        "message": "make open clip work, use own tokenizer as it seems to be broken from main package"
    },
    {
        "repo_url": "github.com/MedARC-AI/fMRI-Algonauts-Challenge-2023",
        "filepath": "src/models.py",
        "commit_date": "2023-06-03T21:45:14Z",
        "message": "better starting point with updated wds"
    },
    {
        "repo_url": "github.com/kabachuha/InfiNet",
        "filepath": "t2v_modules/clip_wrap.py",
        "commit_date": "2023-03-28T14:31:20Z",
        "message": "add the not touched yet ModelScope modules"
    },
    {
        "repo_url": "github.com/adodge/ComfyLib",
        "filepath": "comfy/hazard/ldm/modules/encoders/modules.py",
        "commit_date": "2023-02-26T03:09:37Z",
        "message": "creating a library interface for the SD implementation in ComfyUI"
    },
    {
        "repo_url": "github.com/altndrr/vic",
        "filepath": "src/models/clip.py",
        "commit_date": "2024-02-02T14:14:47Z",
        "message": "Improve metrics compute (#17)\n\n* Compute metrics once\n\n* Compute semantic iou and similarity on step\n\n* Ensure batch dim in `SentenceScore`"
    },
    {
        "repo_url": "github.com/altndrr/vic",
        "filepath": "src/models/clip.py",
        "commit_date": "2023-12-14T18:38:49Z",
        "message": "Major code overhaul (#16)\n\n* Remove `.yaml` extension from config files\n\n* Remove `.compile` in `train.py`\n\n* Print config tree as last of extras\n\n* Remove colorlogger\n\n* Add missing types and docstrings\n\n* Update data\n\n* Suppress kaggle `OSError`\n\n* Add requests header to avoid HTTP 406 errors in download\n\n* Update metrics\n\n* Support masking in `NearestNeighboursClassifier`\n\n* Refactor retrieval system\n\n* Refactor models\n\n* Support disabling loggers\n\n* Fix wandb hparams logging format\n\n* Remove optim from CLIP config\n\n* Add interrogate pre-commit\n\n* Fix pytype issues\n\n* Fix default models filepath\n\n* Set default CaSED alpha to `0.7`\n\n* Add new dependencies\n\n* Rename `databases.json` file\n\n* Update citation\n\n* Bump repo version\n\n* Add reference in method\n\n* Update HuggingFace inference code\n\n* Add logo"
    },
    {
        "repo_url": "github.com/bentoml/CLIP-API-service",
        "filepath": "src/clip_api_service/models/openclip.py",
        "commit_date": "2023-10-11T18:33:36Z",
        "message": "fix: serve not using defined model"
    },
    {
        "repo_url": "github.com/bentoml/CLIP-API-service",
        "filepath": "src/clip_api_service/models/openclip.py",
        "commit_date": "2023-05-23T05:16:39Z",
        "message": "PDM, Ruff, Black - build WIP"
    },
    {
        "repo_url": "github.com/Understanding-Visual-Datasets/VisDiff",
        "filepath": "serve/clip_server.py",
        "commit_date": "2023-11-29T20:48:05Z",
        "message": "initial release\n\nCo-Authored-By: Lisa Dunlap <25967790+lisadunlap@users.noreply.github.com>\nCo-Authored-By: Yuhui Zhang <yuhuiz@cs.stanford.edu>"
    },
    {
        "repo_url": "github.com/hammoudhasan/SynthCLIP",
        "filepath": "ImageGen/ldm/modules/encoders/modules.py",
        "commit_date": "2024-02-09T00:58:26Z",
        "message": "Added ImageGen code and formatted files"
    },
    {
        "repo_url": "github.com/pravdomil/Rerender-A-Video",
        "filepath": "ControlNet/ldm/modules/encoders/modules.py",
        "commit_date": "2023-06-16T08:15:01Z",
        "message": "merge (#1)\n\n\n- first commit (eab2e368258e9e9b159c3ad6abbc89227cdf3976)"
    },
    {
        "repo_url": "github.com/yandex-research/adaptive-diffusion",
        "filepath": "consistency_models_sd/evaluations/clip_score.py",
        "commit_date": "2023-12-19T05:34:44Z",
        "message": "Initial update\n* two examples of t2i\n* arxiv link"
    },
    {
        "repo_url": "github.com/distable/core",
        "filepath": "src_plugins/paella/colab.py",
        "commit_date": "2023-04-25T18:23:58Z",
        "message": "flower model and other many hundreds of improvements"
    },
    {
        "repo_url": "github.com/salesforce/BannerGen",
        "filepath": "InstructPix2Pix/stable_diffusion/ldm/modules/encoders/modules.py",
        "commit_date": "2023-11-09T05:06:04Z",
        "message": "Initial commit of 3 BannerGen models"
    },
    {
        "repo_url": "github.com/NeuralRealm/StableFusion",
        "filepath": "stablefusion/scripts/clip_interrogator.py",
        "commit_date": "2023-03-03T12:10:15Z",
        "message": "rearranged the file and add upscaler feature"
    },
    {
        "repo_url": "github.com/r0mar0ma/sd-webui-pez-dispenser",
        "filepath": "scripts/pez-dispenser.py",
        "commit_date": "2023-05-09T13:58:27Z",
        "message": "Do not save unnecessary values into ui-config.json"
    },
    {
        "repo_url": "github.com/r0mar0ma/sd-webui-pez-dispenser",
        "filepath": "scripts/pez-dispenser.py",
        "commit_date": "2023-04-29T13:10:05Z",
        "message": "Error message if not all packages installed"
    },
    {
        "repo_url": "github.com/gortizji/tangent_task_arithmetic",
        "filepath": "src/modeling.py",
        "commit_date": "2023-06-08T09:13:17Z",
        "message": "Upload clean code and update README.md."
    },
    {
        "repo_url": "github.com/Deep-Spark/deepsparkhub",
        "filepath": "multimodal/diffusion/ControlNet/ldm/modules/encoders/modules.py",
        "commit_date": "2023-12-14T09:17:34Z",
        "message": "move controlnet location\n\nSigned-off-by: majorli <mingjiang.li@iluvatar.com>"
    },
    {
        "repo_url": "github.com/Ascend/ModelZoo-PyTorch",
        "filepath": "AscendIE/AscendIE/StableDiffusion/clip_score.py",
        "commit_date": "2023-10-09T12:26:20Z",
        "message": "!5610 [\u81ea\u7814][PyTorch\u79bb\u7ebf\u63a8\u7406][Text-to-Image][StableDiffusion] \u6dfb\u52a0AIE-SD\u7684unetparser\u529f\u80fd\u3001\u7cbe\u5ea6\u8ba1\u7b97\u529f\u80fd\u53caREADME\u63a8\u7406\u6307\u5bfc\n* fix bugs\n* Merge https://gitee.com/ascend/ModelZoo-PyTorch\n* fix codecheck issues\n* Merge https://gitee.com/ascend/ModelZoo-PyTorch\n* add unet_onnx_parser, clip score and readme"
    },
    {
        "repo_url": "github.com/Ascend/ModelZoo-PyTorch",
        "filepath": "ACL_PyTorch/built-in/foundation_models/stable_diffusion/clip_score.py",
        "commit_date": "2023-08-29T11:58:57Z",
        "message": "!5419 [\u81ea\u7814][PyTorch\u79bb\u7ebf\u63a8\u7406][Text-to-Image] StableDiffusion \u6dfb\u52a0\u7cbe\u5ea6\u9a8c\u8bc1\u3001\u4fee\u6539\u6539\u56fe\u811a\u672c\n* \u4fee\u590datc\u53c2\u6570\n* \u4fee\u6539main\u4f20\u53c2\u903b\u8f91\n* \u66ff\u6362\u4f7f\u7528fdopen\u8bfb\u53d6\u6587\u4ef6\n* \u4fee\u590d\u6539\u56fe\u811a\u672c\n* \u6dfb\u52a0\u7cbe\u5ea6\u8ba1\u7b97\u76f8\u5173\u5185\u5bb9\n* \u6dfb\u52a0\u7cbe\u5ea6\u8ba1\u7b97\u811a\u672c\n* \u6dfb\u52a0\u7cbe\u5ea6\u9a8c\u8bc1\u4f9d\u8d56\n* \u6dfb\u52a0\u652f\u6301\u8bfb\u53d6Parti\u6570\u636e\u96c6"
    },
    {
        "repo_url": "github.com/Ascend/ModelZoo-PyTorch",
        "filepath": "PyTorch/built-in/others/OpenCLIP_for_PyTorch/tests/util_test.py",
        "commit_date": "2023-05-17T12:47:36Z",
        "message": "!4706 [\u81ea\u7814][PyTorch] [OpenClip For Pytorch] \u521d\u6b21\u63d0\u4ea4\n* \u63d0\u4ea4open clip\u539f\u59cb\u4ee3\u7801"
    },
    {
        "repo_url": "github.com/Ascend/ModelZoo-PyTorch",
        "filepath": "PyTorch/built-in/others/OpenCLIP_for_PyTorch/tests/test_inference.py",
        "commit_date": "2023-05-17T12:47:36Z",
        "message": "!4706 [\u81ea\u7814][PyTorch] [OpenClip For Pytorch] \u521d\u6b21\u63d0\u4ea4\n* \u63d0\u4ea4open clip\u539f\u59cb\u4ee3\u7801"
    },
    {
        "repo_url": "github.com/Ascend/ModelZoo-PyTorch",
        "filepath": "PyTorch/built-in/others/OpenCLIP_for_PyTorch/tests/test_inference_simple.py",
        "commit_date": "2023-05-17T12:47:36Z",
        "message": "!4706 [\u81ea\u7814][PyTorch] [OpenClip For Pytorch] \u521d\u6b21\u63d0\u4ea4\n* \u63d0\u4ea4open clip\u539f\u59cb\u4ee3\u7801"
    },
    {
        "repo_url": "github.com/Birch-san/diffusers-play",
        "filepath": "src/helpers/embed_text.py",
        "commit_date": "2023-01-03T00:52:05Z",
        "message": "support splicing together CLIP multiple embeddings to utilise the triple context length of waifu-diffusion"
    },
    {
        "repo_url": "github.com/Birch-san/diffusers-play",
        "filepath": "src/helpers/embed_text.py",
        "commit_date": "2022-12-24T17:22:08Z",
        "message": "factor out construction of embedder from tokenizer & text encoder, because ti_train has a need to share the embedder-maker code"
    },
    {
        "repo_url": "github.com/Birch-san/diffusers-play",
        "filepath": "src/helpers/embed_text.py",
        "commit_date": "2022-11-24T23:19:51Z",
        "message": "stable diffusion v2 working on-CPU via k-diffusion. just noise on MPS."
    },
    {
        "repo_url": "github.com/camenduru/Rerender-hf",
        "filepath": "ControlNet/ldm/modules/encoders/modules.py",
        "commit_date": "2023-06-16T08:15:01Z",
        "message": "merge (#1)\n\n\n- first commit (eab2e368258e9e9b159c3ad6abbc89227cdf3976)"
    },
    {
        "repo_url": "github.com/johnowhitaker/tglcourse",
        "filepath": "tglcourse/generation_utils.py",
        "commit_date": "2022-11-17T12:31:25Z",
        "message": "First pass at l4 and updates to generators and losses"
    },
    {
        "repo_url": "github.com/johnowhitaker/tglcourse",
        "filepath": "tglcourse/generation_utils.py",
        "commit_date": "2022-10-07T13:36:32Z",
        "message": "Getting generators/losses working in the optimization lesson as a quick demo for what that bonus notebook holds"
    },
    {
        "repo_url": "github.com/johnowhitaker/tglcourse",
        "filepath": "tglcourse/generation_utils.py",
        "commit_date": "2022-10-07T12:31:18Z",
        "message": "Adding generators and losses draft, plus new requirements"
    },
    {
        "repo_url": "github.com/iejMac/clip-video-encode",
        "filepath": "clip_video_encode/simplemapper.py",
        "commit_date": "2023-09-13T02:54:38Z",
        "message": "clip-video-encode: add VQ-GAN frame tokenization (#77)\n\n* clip-video-encode: add VQ-GAN frame tokenization\n\n* it works\n\n* automatic is_gumbel\n\n* update\n\n* bigger bs\n\n* attempt at resuming\n\n* update tests\n\n* fix black\n\n* fix black\n\n* fix\n\n---------\n\nCo-authored-by: iejmac <iejmac@ip-26-0-151-20.us-west-2.compute.internal>\nCo-authored-by: iejmac <iejmac@ip-172-64-56-42.us-west-2.compute.internal>\nCo-authored-by: iejmac <iejmac@ip-26-0-146-117.us-west-2.compute.internal>\nCo-authored-by: iejmac <iejmac@ip-26-0-155-198.us-west-2.compute.internal>"
    },
    {
        "repo_url": "github.com/iejMac/clip-video-encode",
        "filepath": "clip_video_encode/simplemapper.py",
        "commit_date": "2023-04-26T07:35:37Z",
        "message": "captioning: allow setting params (#73)\n\n* captioning: allow setting params\n\n* fix black\n\n* docstring\n\n---------\n\nCo-authored-by: iejmac <iejmac@ip-26-0-151-112.us-west-2.compute.internal>"
    },
    {
        "repo_url": "github.com/iejMac/clip-video-encode",
        "filepath": "clip_video_encode/simplemapper.py",
        "commit_date": "2023-04-06T10:55:57Z",
        "message": "Add CLIP similarity if caption exists + add autocast in FrameMapper (#69)\n\n* Add CLIP similarity if caption exists\n\n* progress\n\n* fix lint\n\n* batch normalization of caption embs"
    },
    {
        "repo_url": "github.com/iejMac/clip-video-encode",
        "filepath": "clip_video_encode/simplemapper.py",
        "commit_date": "2023-04-02T10:43:30Z",
        "message": "Add CoCa captioning (#67)\n\n* Add CoCa captioning\n\n* works\n\n* fix black\n\n* req\n\n* fix lint\n\n* fix black\n\n* reset"
    },
    {
        "repo_url": "github.com/iejMac/clip-video-encode",
        "filepath": "clip_video_encode/simplemapper.py",
        "commit_date": "2022-07-31T20:45:12Z",
        "message": "LiveNumpyEncoder: waits for incoming frame arrays and encodes them with CLIP (1700 samples/s) (#31)\n\n* NumpyEncoder: waits for incoming frame arrays and encodes them with CLIP\n\n* works 1700 FPS\n\n* pass in dev\n\n* rename\n\n* add example\n\n* fix lint"
    },
    {
        "repo_url": "github.com/iejMac/clip-video-encode",
        "filepath": "clip_video_encode/simplemapper.py",
        "commit_date": "2022-06-15T06:52:42Z",
        "message": "Asynchronously Read and Encode frames for efficiency. (~4x speedup depending on target video FPS) (#7)\n\n* Asynchronously Load, Embed, and Save videos for efficiency\n\n* saver: initial version\n\n* that was supposed to be the loader\n\n* small update\n\n* small testing script\n\n* new idea\n\n* adding batcher\n\n* taking step back and reimplementing main in modules\n\n* Simple version of all 4 components\n\n* Move preprocessing to batcher + log perf in samples/s + batcher 2.7x performance improvement\n\n* Use multiprocessing to make Reader faster\n\n* simplereader -> reader\n\n* Batcher improvements + perf metrics update\n\n* add unit tests for all modules\n\n* add modules to package\n\n* Reading + Mapping parallelism\n\n* final version for update\n\n* Revert to regular preprocessing, channel dim might've been mixed up\n\n* tests updated\n\n* new clip_video_encode script\n\n* fix ci\n\n* ci fix\n\n* shared_memory not in python < 3.8\n\n* some linting + remove time prints\n\n* linting\n\n* lint fix\n\n* better default value for dest"
    },
    {
        "repo_url": "github.com/iejMac/clip-video-encode",
        "filepath": "tests/test_similarity.py",
        "commit_date": "2022-10-14T16:28:22Z",
        "message": "Fix: Take preprocessing from open_clip + Add sim test (#52)\n\n* Fix: Take preprocessing from open_clip + Add sim test\n\n* fix\n\n* similarity test\n\n* fix lint\n\n* no need to repeat stuff\n\n* force cpu\n\n* fix syntax\n\n* fix lint\n\n* works"
    },
    {
        "repo_url": "github.com/eslambakr/HRS_benchmark",
        "filepath": "codes/t2i_models/Paella/paella.py",
        "commit_date": "2023-02-23T09:26:03Z",
        "message": "add support to new txt2img models"
    },
    {
        "repo_url": "github.com/eslambakr/HRS_benchmark",
        "filepath": "codes/t2i_models/Paella/paella_minimal.py",
        "commit_date": "2023-02-23T09:26:03Z",
        "message": "add support to new txt2img models"
    },
    {
        "repo_url": "github.com/eslambakr/HRS_benchmark",
        "filepath": "codes/t2i_models/Paella/paella_inference.py",
        "commit_date": "2023-02-23T09:26:03Z",
        "message": "add support to new txt2img models"
    },
    {
        "repo_url": "github.com/eslambakr/HRS_benchmark",
        "filepath": "codes/t2i_models/sd_v2/ldm/modules/encoders/modules.py",
        "commit_date": "2023-02-04T22:25:33Z",
        "message": "add detection metric, meta-prompt generation codes, chatGPT codes, SD_v1, and SD_v2 codes"
    },
    {
        "repo_url": "github.com/eslambakr/HRS_benchmark",
        "filepath": "codes/t2i_models/Paella/ongoing_research/scaling/paella_h.py",
        "commit_date": "2023-02-23T09:26:03Z",
        "message": "add support to new txt2img models"
    },
    {
        "repo_url": "github.com/eslambakr/HRS_benchmark",
        "filepath": "codes/t2i_models/Paella/ongoing_research/text-to-video/train_maskgit.py",
        "commit_date": "2023-02-23T09:26:03Z",
        "message": "add support to new txt2img models"
    },
    {
        "repo_url": "github.com/kakaobrain/nvs-adapter",
        "filepath": "sgm/modules/encoders/modules.py",
        "commit_date": "2024-01-16T05:21:47Z",
        "message": "initial commit\n\nCo-authored-by: Yoonwoo Jeong <jeongyw12382@postech.ac.kr>\nCo-authored-by: Jinwoo Lee <chopper.lee@kakaobrain.com>\nCo-authored-by: Chiheon Kim <chiheon.kim@kakaobrain.com>\nCo-authored-by: Minsu Cho <mscho@postech.ac.kr>\nCo-authored-by: Doyup Lee <doyup@runwayml.com>"
    },
    {
        "repo_url": "github.com/mlcommons/training_results_v3.1",
        "filepath": "NVIDIA/benchmarks/stable_diffusion/implementations/pytorch/evaluation/compute_clip_score.py",
        "commit_date": "2023-11-07T21:26:23Z",
        "message": "Upload results\n\nCo-authored-by: Achorn, Keith <keith.achorn@intel.com>\nCo-authored-by: Arjun Suresh <arjunsuresh1987@gmail.com>\nCo-authored-by: BladeRD-SIT <78253812+BladeRD-SIT@users.noreply.github.com>\nCo-authored-by: Bruce Lin <bruce5_lin@asus.com>\nCo-authored-by: Bruno Ferreira <bruno.mgf@gmail.com>\nCo-authored-by: Burhan Ul Tayyab <burhanultayyab@ailiverse.com>\nCo-authored-by: BurhanUlTayyab <burhanultayyab@ailiverse.com>\nCo-authored-by: ch155260 <kein.kung@gigacomputing.com>\nCo-authored-by: ch155260 <kevin.kung@gigacomputing.com>\nCo-authored-by: ctlee <ching-tao.lee@qct.io>\nCo-authored-by: Diane Feddema <dianefeddema@gmail.com>\nCo-authored-by: elim <elim@krai.ai>\nCo-authored-by: erichan1 <30481032+erichan1@users.noreply.github.com>\nCo-authored-by: Frank Han <frank.han@dell.com>\nCo-authored-by: Guenther Schmuelling <guschmue@microsoft.com>\nCo-authored-by: guschmue <guschmue@microsoft.com>\nCo-authored-by: hanyunfan <frank.han@dell.com>\nCo-authored-by: Hiwot Tadese Kassa <36334927+hiwotadese@users.noreply.github.com>\nCo-authored-by: Hugo Affaticati <haffaticati@microsoft.com>\nCo-authored-by: itayhubara <itayh@campus.technion.ac.il>\nCo-authored-by: Jason Zhu <ccbt87@gmail.com>\nCo-authored-by: Jason Zhu <jasonzhu@supermicro.com>\nCo-authored-by: l00010728 <luxu@xfusion.com>\nCo-authored-by: lbhli001 <libaihong@ieisystem.com>\nCo-authored-by: lizraymond <liz.raymond@dell.com>\nCo-authored-by: Matt Frank <mfrank@nvidia.com>\nCo-authored-by: Nathan Wasson <nathanw@mlcommons.org>\nCo-authored-by: Nils Smeds <nsmeds@lenovo.com>\nCo-authored-by: Notsu, Takahiro <notsu.takahiro@fujitsu.com>\nCo-authored-by: Pablo Gonzalez <pablo.gonzalez@factored.ai>\nCo-authored-by: Peter Mattson <34691613+petermattson@users.noreply.github.com>\nCo-authored-by: pgmpablo157321 <pablo.gonzalez@factored.ai>\nCo-authored-by: Qinwen Xu <qinwen@google.com>\nCo-authored-by: rakshithvasudev <rakshithvbharani@gmail.com>\nCo-authored-by: Ritika Borkar <rborkar@nvidia.com>\nCo-authored-by: Shriya Balaji Palsamudram <spalsamudram@nvidia.com>\nCo-authored-by: SupermicroML <78233566+SupermicroML@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/mlcommons/training_results_v3.1",
        "filepath": "Dell/benchmarks/stable_diffusion/implementations/pytorch/evaluation/compute_clip_score.py",
        "commit_date": "2023-11-07T21:26:23Z",
        "message": "Upload results\n\nCo-authored-by: Achorn, Keith <keith.achorn@intel.com>\nCo-authored-by: Arjun Suresh <arjunsuresh1987@gmail.com>\nCo-authored-by: BladeRD-SIT <78253812+BladeRD-SIT@users.noreply.github.com>\nCo-authored-by: Bruce Lin <bruce5_lin@asus.com>\nCo-authored-by: Bruno Ferreira <bruno.mgf@gmail.com>\nCo-authored-by: Burhan Ul Tayyab <burhanultayyab@ailiverse.com>\nCo-authored-by: BurhanUlTayyab <burhanultayyab@ailiverse.com>\nCo-authored-by: ch155260 <kein.kung@gigacomputing.com>\nCo-authored-by: ch155260 <kevin.kung@gigacomputing.com>\nCo-authored-by: ctlee <ching-tao.lee@qct.io>\nCo-authored-by: Diane Feddema <dianefeddema@gmail.com>\nCo-authored-by: elim <elim@krai.ai>\nCo-authored-by: erichan1 <30481032+erichan1@users.noreply.github.com>\nCo-authored-by: Frank Han <frank.han@dell.com>\nCo-authored-by: Guenther Schmuelling <guschmue@microsoft.com>\nCo-authored-by: guschmue <guschmue@microsoft.com>\nCo-authored-by: hanyunfan <frank.han@dell.com>\nCo-authored-by: Hiwot Tadese Kassa <36334927+hiwotadese@users.noreply.github.com>\nCo-authored-by: Hugo Affaticati <haffaticati@microsoft.com>\nCo-authored-by: itayhubara <itayh@campus.technion.ac.il>\nCo-authored-by: Jason Zhu <ccbt87@gmail.com>\nCo-authored-by: Jason Zhu <jasonzhu@supermicro.com>\nCo-authored-by: l00010728 <luxu@xfusion.com>\nCo-authored-by: lbhli001 <libaihong@ieisystem.com>\nCo-authored-by: lizraymond <liz.raymond@dell.com>\nCo-authored-by: Matt Frank <mfrank@nvidia.com>\nCo-authored-by: Nathan Wasson <nathanw@mlcommons.org>\nCo-authored-by: Nils Smeds <nsmeds@lenovo.com>\nCo-authored-by: Notsu, Takahiro <notsu.takahiro@fujitsu.com>\nCo-authored-by: Pablo Gonzalez <pablo.gonzalez@factored.ai>\nCo-authored-by: Peter Mattson <34691613+petermattson@users.noreply.github.com>\nCo-authored-by: pgmpablo157321 <pablo.gonzalez@factored.ai>\nCo-authored-by: Qinwen Xu <qinwen@google.com>\nCo-authored-by: rakshithvasudev <rakshithvbharani@gmail.com>\nCo-authored-by: Ritika Borkar <rborkar@nvidia.com>\nCo-authored-by: Shriya Balaji Palsamudram <spalsamudram@nvidia.com>\nCo-authored-by: SupermicroML <78233566+SupermicroML@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/mlcommons/training_results_v3.1",
        "filepath": "Intel-HabanaLabs/benchmarks/stable_diffusion/ldm/models/clip_encoder.py",
        "commit_date": "2023-11-07T21:26:23Z",
        "message": "Upload results\n\nCo-authored-by: Achorn, Keith <keith.achorn@intel.com>\nCo-authored-by: Arjun Suresh <arjunsuresh1987@gmail.com>\nCo-authored-by: BladeRD-SIT <78253812+BladeRD-SIT@users.noreply.github.com>\nCo-authored-by: Bruce Lin <bruce5_lin@asus.com>\nCo-authored-by: Bruno Ferreira <bruno.mgf@gmail.com>\nCo-authored-by: Burhan Ul Tayyab <burhanultayyab@ailiverse.com>\nCo-authored-by: BurhanUlTayyab <burhanultayyab@ailiverse.com>\nCo-authored-by: ch155260 <kein.kung@gigacomputing.com>\nCo-authored-by: ch155260 <kevin.kung@gigacomputing.com>\nCo-authored-by: ctlee <ching-tao.lee@qct.io>\nCo-authored-by: Diane Feddema <dianefeddema@gmail.com>\nCo-authored-by: elim <elim@krai.ai>\nCo-authored-by: erichan1 <30481032+erichan1@users.noreply.github.com>\nCo-authored-by: Frank Han <frank.han@dell.com>\nCo-authored-by: Guenther Schmuelling <guschmue@microsoft.com>\nCo-authored-by: guschmue <guschmue@microsoft.com>\nCo-authored-by: hanyunfan <frank.han@dell.com>\nCo-authored-by: Hiwot Tadese Kassa <36334927+hiwotadese@users.noreply.github.com>\nCo-authored-by: Hugo Affaticati <haffaticati@microsoft.com>\nCo-authored-by: itayhubara <itayh@campus.technion.ac.il>\nCo-authored-by: Jason Zhu <ccbt87@gmail.com>\nCo-authored-by: Jason Zhu <jasonzhu@supermicro.com>\nCo-authored-by: l00010728 <luxu@xfusion.com>\nCo-authored-by: lbhli001 <libaihong@ieisystem.com>\nCo-authored-by: lizraymond <liz.raymond@dell.com>\nCo-authored-by: Matt Frank <mfrank@nvidia.com>\nCo-authored-by: Nathan Wasson <nathanw@mlcommons.org>\nCo-authored-by: Nils Smeds <nsmeds@lenovo.com>\nCo-authored-by: Notsu, Takahiro <notsu.takahiro@fujitsu.com>\nCo-authored-by: Pablo Gonzalez <pablo.gonzalez@factored.ai>\nCo-authored-by: Peter Mattson <34691613+petermattson@users.noreply.github.com>\nCo-authored-by: pgmpablo157321 <pablo.gonzalez@factored.ai>\nCo-authored-by: Qinwen Xu <qinwen@google.com>\nCo-authored-by: rakshithvasudev <rakshithvbharani@gmail.com>\nCo-authored-by: Ritika Borkar <rborkar@nvidia.com>\nCo-authored-by: Shriya Balaji Palsamudram <spalsamudram@nvidia.com>\nCo-authored-by: SupermicroML <78233566+SupermicroML@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/mlcommons/training_results_v3.1",
        "filepath": "Intel-HabanaLabs/benchmarks/stable_diffusion/ldm/modules/encoders/modules.py",
        "commit_date": "2023-11-07T21:26:23Z",
        "message": "Upload results\n\nCo-authored-by: Achorn, Keith <keith.achorn@intel.com>\nCo-authored-by: Arjun Suresh <arjunsuresh1987@gmail.com>\nCo-authored-by: BladeRD-SIT <78253812+BladeRD-SIT@users.noreply.github.com>\nCo-authored-by: Bruce Lin <bruce5_lin@asus.com>\nCo-authored-by: Bruno Ferreira <bruno.mgf@gmail.com>\nCo-authored-by: Burhan Ul Tayyab <burhanultayyab@ailiverse.com>\nCo-authored-by: BurhanUlTayyab <burhanultayyab@ailiverse.com>\nCo-authored-by: ch155260 <kein.kung@gigacomputing.com>\nCo-authored-by: ch155260 <kevin.kung@gigacomputing.com>\nCo-authored-by: ctlee <ching-tao.lee@qct.io>\nCo-authored-by: Diane Feddema <dianefeddema@gmail.com>\nCo-authored-by: elim <elim@krai.ai>\nCo-authored-by: erichan1 <30481032+erichan1@users.noreply.github.com>\nCo-authored-by: Frank Han <frank.han@dell.com>\nCo-authored-by: Guenther Schmuelling <guschmue@microsoft.com>\nCo-authored-by: guschmue <guschmue@microsoft.com>\nCo-authored-by: hanyunfan <frank.han@dell.com>\nCo-authored-by: Hiwot Tadese Kassa <36334927+hiwotadese@users.noreply.github.com>\nCo-authored-by: Hugo Affaticati <haffaticati@microsoft.com>\nCo-authored-by: itayhubara <itayh@campus.technion.ac.il>\nCo-authored-by: Jason Zhu <ccbt87@gmail.com>\nCo-authored-by: Jason Zhu <jasonzhu@supermicro.com>\nCo-authored-by: l00010728 <luxu@xfusion.com>\nCo-authored-by: lbhli001 <libaihong@ieisystem.com>\nCo-authored-by: lizraymond <liz.raymond@dell.com>\nCo-authored-by: Matt Frank <mfrank@nvidia.com>\nCo-authored-by: Nathan Wasson <nathanw@mlcommons.org>\nCo-authored-by: Nils Smeds <nsmeds@lenovo.com>\nCo-authored-by: Notsu, Takahiro <notsu.takahiro@fujitsu.com>\nCo-authored-by: Pablo Gonzalez <pablo.gonzalez@factored.ai>\nCo-authored-by: Peter Mattson <34691613+petermattson@users.noreply.github.com>\nCo-authored-by: pgmpablo157321 <pablo.gonzalez@factored.ai>\nCo-authored-by: Qinwen Xu <qinwen@google.com>\nCo-authored-by: rakshithvasudev <rakshithvbharani@gmail.com>\nCo-authored-by: Ritika Borkar <rborkar@nvidia.com>\nCo-authored-by: Shriya Balaji Palsamudram <spalsamudram@nvidia.com>\nCo-authored-by: SupermicroML <78233566+SupermicroML@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/xmed-lab/CLIPN",
        "filepath": "hand-crafted/src/classification.py",
        "commit_date": "2023-12-02T14:47:55Z",
        "message": "Upload\n\nUpload codes for hand-crafted version."
    },
    {
        "repo_url": "github.com/gregor-ge/mBLIP",
        "filepath": "data/pretrain/hard_examples.py",
        "commit_date": "2023-09-22T09:15:00Z",
        "message": "Updated for version 2 on arxiv"
    },
    {
        "repo_url": "github.com/gregor-ge/mBLIP",
        "filepath": "data/pretrain/generate_match_train.py",
        "commit_date": "2023-09-22T09:15:00Z",
        "message": "Updated for version 2 on arxiv"
    },
    {
        "repo_url": "github.com/emu1729/GIST",
        "filepath": "caption_matching.py",
        "commit_date": "2023-07-19T19:06:41Z",
        "message": "Added caption matching script and example"
    },
    {
        "repo_url": "github.com/emu1729/GIST",
        "filepath": "caption_matching.py",
        "commit_date": "2023-07-19T18:38:25Z",
        "message": "Added datasets and sample metadata files"
    },
    {
        "repo_url": "github.com/autodistill/autodistill-metaclip",
        "filepath": "autodistill_metaclip/metaclip_model.py",
        "commit_date": "2023-10-26T11:18:33Z",
        "message": "update package config, run black and isort"
    },
    {
        "repo_url": "github.com/ml-research/i2p",
        "filepath": "models/vision/paella.py",
        "commit_date": "2023-04-25T12:05:18Z",
        "message": "new structure and added altdiffusion and paella"
    },
    {
        "repo_url": "github.com/nostalgebraist/nostalgebraist-autoresponder",
        "filepath": "src/ml/captioning.py",
        "commit_date": "2023-02-17T18:39:29Z",
        "message": "log exceptions on ml machines in a way we can print in main machine logs"
    },
    {
        "repo_url": "github.com/nostalgebraist/nostalgebraist-autoresponder",
        "filepath": "src/ml/captioning.py",
        "commit_date": "2023-02-12T20:38:02Z",
        "message": "save memory by moving clip encoder to cpu after use + turn guidance back on"
    },
    {
        "repo_url": "github.com/nostalgebraist/nostalgebraist-autoresponder",
        "filepath": "src/ml/captioning.py",
        "commit_date": "2023-01-31T18:20:06Z",
        "message": "align inference prompt for captioning with training data"
    },
    {
        "repo_url": "github.com/nostalgebraist/nostalgebraist-autoresponder",
        "filepath": "src/ml/captioning.py",
        "commit_date": "2022-11-26T19:38:41Z",
        "message": "fix interaction between kv_cache_scope and activate+deactivate helpers"
    },
    {
        "repo_url": "github.com/nostalgebraist/nostalgebraist-autoresponder",
        "filepath": "src/ml/captioning.py",
        "commit_date": "2022-11-26T18:33:42Z",
        "message": "avoid repeatedly moving adapters from cpu to gpu during long strings of captioning requests"
    },
    {
        "repo_url": "github.com/nostalgebraist/nostalgebraist-autoresponder",
        "filepath": "src/ml/captioning.py",
        "commit_date": "2022-11-01T14:38:02Z",
        "message": "avoid wasting gpu memory on magma's clip encoder when not in use"
    },
    {
        "repo_url": "github.com/nostalgebraist/nostalgebraist-autoresponder",
        "filepath": "src/ml/captioning.py",
        "commit_date": "2022-10-10T15:56:09Z",
        "message": "use context manager + disable buffer in head predict"
    },
    {
        "repo_url": "github.com/nostalgebraist/nostalgebraist-autoresponder",
        "filepath": "src/ml/captioning.py",
        "commit_date": "2022-10-05T18:34:22Z",
        "message": "turn kv buffer off before captioning bc it interacts badly with guidance"
    },
    {
        "repo_url": "github.com/nostalgebraist/nostalgebraist-autoresponder",
        "filepath": "src/ml/captioning.py",
        "commit_date": "2022-09-15T20:27:59Z",
        "message": "keep captioning adapters on gpu when enough vram available"
    },
    {
        "repo_url": "github.com/nostalgebraist/nostalgebraist-autoresponder",
        "filepath": "src/ml/captioning.py",
        "commit_date": "2022-07-06T19:08:14Z",
        "message": "generate several captions and use longest one"
    },
    {
        "repo_url": "github.com/nostalgebraist/nostalgebraist-autoresponder",
        "filepath": "src/ml/captioning.py",
        "commit_date": "2022-07-06T16:31:23Z",
        "message": "make guidance work in magma captioning"
    },
    {
        "repo_url": "github.com/nostalgebraist/nostalgebraist-autoresponder",
        "filepath": "src/ml/captioning.py",
        "commit_date": "2022-06-24T22:32:52Z",
        "message": "move adapters to cpu when not in use"
    }
]