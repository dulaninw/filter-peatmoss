{
    "0": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2023-09-15T23:15:19Z",
                "message_summary": "Keep task level checkpoint key name generic (#5330)"
            }
        ]
    },
    "1": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2022-12-13T07:36:17Z",
                "message_summary": "Remove missing config entries when loading task from checkpoint"
            }
        ]
    },
    "2": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2022-12-12T16:53:56Z",
                "message_summary": "data2vec v2.0 (#4903)",
                "ptm_addition_details": {
                    "name": "data2v2c",
                    "version": "2.0"
                }
            }
        ]
    },
    "3": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2022-06-22T21:03:17Z",
                "message_summary": "add check for OC version in fairseq"
            }
        ]
    },
    "4": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2022-04-14T22:39:36Z",
                "message_summary": "Fix typo in exception value",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "5": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2022-01-21T17:27:49Z",
                "message_summary": "Fix breakage from D33649708"
            }
        ]
    },
    "6": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2022-01-20T08:02:16Z",
                "message_summary": "Data2vec prelim (#2929)",
                "update_details": "Default behavior changed to raise an exception when fields in config do not match model dataclass fields."
            }
        ]
    },
    "7": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2022-01-19T03:29:52Z",
                "message_summary": "Decode using EMA model in IPL recipe",
                "ptm_addition_details": {
                    "name": "EMA model",
                    "version": "N/A",
                    "problem_addressed": "Decoding in transducer IPL recipe using EMA model"
                },
                "update_details": "Add option to use the EMA model for decoding in transducer IPL recipe by passing --ipl-decode-ema. Note EMA should be enabled as in the diff D24238379 (https://github.com/pytorch/fairseq/commit/8feccf94412424a4683b01090de36fa77cb4951d) using options --store-ema --ema-start-update and --ema-decay."
            }
        ]
    },
    "8": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2022-01-07T08:39:11Z",
                "message_summary": "Formatting fix: get CI green (#2860)"
            }
        ]
    },
    "9": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-12-31T02:38:12Z",
                "message_summary": "Add strict option to checkpoint_utils.load_pretrained_component_from_model()"
            }
        ]
    },
    "10": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-12-17T00:11:19Z",
                "message_summary": "formatting fix (#2816)",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "11": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-12-11T00:55:12Z",
                "message_summary": "Add loading from HuggingFace Hub"
            }
        ]
    },
    "12": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-11-29T20:32:59Z",
                "message_summary": "Add linting with black (#2678)"
            }
        ]
    },
    "13": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-11-19T00:38:31Z",
                "message_summary": "Merge --use-ontology-for-* into --use-ontology",
                "update_details": "Merged three options related to ontology into one (--use-ontology). Logic for avoiding loading teacher models was moved out of `checkpoint_utils.py`. A new way to load a student model without loading its teachers was introduced by specifying `arg_overrides={\"ignore_teachers\": True}` when calling `load_model_ensemble`."
            }
        ]
    },
    "14": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-10-23T02:51:09Z",
                "message_summary": "set num_update before loading state dict (#2491)",
                "ptm_addition_details": null,
                "update_details": "Set `model.num_updates` in `load_model_ensemble_and_task()` before loading `state_dict`, like what's done in `fairseq/trainer.py`, because a model's `state_dict` may depend on `num_update`."
            }
        ]
    },
    "15": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-09-13T20:20:35Z",
                "message_summary": "Back out \"Fairseq needs to store and load metadata from model state_dict\" (#3861)",
                "update_details": "backout fairseq changes. fix with a suggested, more optimal changes in checkopint utils."
            }
        ]
    },
    "16": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-09-09T01:18:30Z",
                "message_summary": "Fairseq needs to store and load metadata from model state_dict",
                "update_details": "Fairseq checkpoint saving and loading should mirror torch's checkpoint by saving and loading \"state_dict()._metadata\"."
            }
        ]
    },
    "17": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-09-01T19:29:51Z",
                "message_summary": "Adds Exponential moving average (EMA) model for Kaizen semi-supervised training",
                "ptm_addition_details": {
                    "name": "Exponential moving average (EMA)",
                    "problem_addressed": "Kaizen semi-supervised training https://arxiv.org/abs/2106.07759"
                },
                "update_details": "1. Add `ema.store_ema` to enable storing EMA. EMA will be written to extra_state in the state dict while saving checkpoint.\n2. `ema.ema_start_update` to control when the EMA starts accumulating\n3. Tasks can use `uses_ema` property to decide if the EMA should be passed to the task. (Default is False)\n4. `load_ema_from_checkpoint` can be used to load EMA model in place of the model to be used for evaluation. Pyspeech has eval-ema option for this."
            }
        ]
    },
    "18": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-08-02T21:36:32Z",
                "message_summary": "fixing checkpoint config upgrade for generation print_alignment (#2125)",
                "ptm_addition_details": {},
                "update_details": "Fixes config upgrade conditions for upgrading generation. print_alignment",
                "replacement_info": "",
                "removal_details": ""
            }
        ]
    },
    "19": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-29T23:30:41Z",
                "message_summary": "seed random suffix in checkpoint to be consistent across shards",
                "ptm_addition_details": null,
                "update_details": "Seeded the random suffix to ensure consistency across shards when saving sharded checkpoints, especially when training with FSDP and use_sharded_state=True. This change aims to facilitate proper loading of checkpoints in downstream applications.",
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "20": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-29T23:30:41Z",
                "message_summary": "use pathmanager to delete old checkpoints"
            }
        ]
    },
    "21": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-29T23:30:40Z",
                "message_summary": "use suffix when saving best checkpoints with metric"
            }
        ]
    },
    "22": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-09T23:25:02Z",
                "message_summary": "Roll back os.path.abspath change"
            }
        ]
    },
    "23": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-08T22:27:57Z",
                "message_summary": "Fix static container (#2036)",
                "update_details": "Fixes StatefulContainer being static which is a problem when loading a checkpoint with a task that already has the same keys in the container. Also prints the full path to checkpoints when saving (useful with hydra) and crashes if repeatedly failing to save a checkpoint."
            }
        ]
    },
    "24": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-06T22:07:31Z",
                "message_summary": "Fixes tests/test_train.py to mock checkpoint.save_dir config node",
                "update_details": "Changed the assumption about the object passed to load_checkpoint() to ensure the mocked config has checkpoint.save_dir for test execution."
            }
        ]
    },
    "25": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-01T13:37:47Z",
                "message_summary": "Hydra 1.1 compatibility: Use an explicit schema for the primary config",
                "ptm_addition_details": null,
                "update_details": "Fixes compatibility with Hydra 1.1. The result is compatible with both Hydra 1.0 and Hydra 1.1, allowing a smoother migration to Hydra 1.1.",
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "26": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-06-15T21:09:52Z",
                "message_summary": "Check attributes in trainer and checkpoint loading before using them",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "27": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-06-04T23:19:56Z",
                "message_summary": "Fix loading some TALNet models",
                "update_details": "Replaced the \"kd_binary_cross_entropy\" criterion with the \"wav2vec\" criterion when loading models to fix loading issues of old models trained with the previous criterion. Removed the \"log_keys\" argument if it's `None` to prevent errors with criteria like wav2vec that require a list argument.",
                "tasks": [
                    "Fix loading some TALNet models"
                ]
            }
        ]
    },
    "28": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-06-04T00:49:12Z",
                "message_summary": "Teacher-student learning for TALNet",
                "update_details": "Implemented teacher-student learning for TALNet. Three classes participate in the teacher-student learning process: task loads teacher models, model generates predictions using teacher models and mixes them with original targets, Wav2VecCriterion computes loss using mixed targets but still uses original targets for MAP and MAUC metrics. Two types of teachers: static teachers (file storing predictions on training data) and dynamic teachers (model files loaded at the beginning of training). Code related to static teachers copied from KnowledgeDistillationBinaryCrossEntropyCriterion class. Teacher models stored in the task object and not saved into checkpoints.",
                "tasks": [
                    "Teacher-student learning for TALNet"
                ]
            }
        ]
    },
    "29": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-05-26T00:45:51Z",
                "message_summary": "support FSDP sharded_state checkpoint loading during inference",
                "update_details": "Using the feature added by QuentinDuval, sharded states can be consolidated into full regular states, enabling inferences on sharded states almost transparently. The commit introduces logic to determine the type of checkpoint to load based on the presence of specific files and user-defined parameters.",
                "tasks": [
                    "FSDP sharded_state checkpoint loading during inference"
                ]
            }
        ]
    },
    "30": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-05-22T07:22:05Z",
                "message_summary": "Fixing s2t transformer and N-best checkpoint saving",
                "update_details": "Fixing the default value for `encoder_freezing_updates` in s2t transformer. Fixing N-best checkpoint saving: the previous implementation compares the new checkpoint with only the previous best one but not the previous N best ones. This leads to suboptimal results on N-best checkpoint averaging."
            }
        ]
    },
    "31": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-05-21T23:18:21Z",
                "message_summary": "Attempt to make non-sharded FSDP checkpoint behave like regular checkpoint"
            }
        ]
    },
    "32": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-04-14T08:50:07Z",
                "message_summary": "Add keep_interval_updates_pattern"
            }
        ]
    },
    "33": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-04-14T08:50:06Z",
                "message_summary": "enable manifold checkpoints with --keep-interval-updates"
            }
        ]
    },
    "34": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-03-30T01:02:50Z",
                "message_summary": "BASE layers (#1654)"
            }
        ]
    },
    "35": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-03-26T14:18:59Z",
                "message_summary": "FSDP uses new optimizer gathering to save optimizer state (#1744)",
                "update_details": "Full unflattened optimizer state dict is in `checkpoints/shard_0.pt`, other checkpoint files do not have the `last_optimizer_state` key. Requires master version of fairscale (eventually fairscale>=0.3.3)"
            }
        ]
    },
    "36": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-03-04T21:32:44Z",
                "message_summary": "Move checkpoint state_dict creation into Trainer (#1666)",
                "update_details": "Exposing a `state_dict` method inside the Trainer to simplify the checkpoint saving logic and enable saving multiple checkpoints for different optimizer state shards.",
                "tasks": [
                    "Move checkpoint state_dict creation into Trainer"
                ]
            }
        ]
    },
    "37": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-03-04T05:17:29Z",
                "message_summary": "minor fixes and improvements",
                "update_details": "- Convert config persisted in checkpoints into a plain dict when saving and back to omegaconf config when loading to avoid compatibility issues between different versions of python, omegaconf, etc.\n- Update checkpoints that have old print_alignment saved.\n- Add lr_float to composite optimizer to enable sweeping on lr with auto sweepers like ax.\n- Fixing some edge cases for config loading."
            }
        ]
    },
    "38": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-03-02T17:26:03Z",
                "message_summary": "Integrate ioPath's async writes feature into Fairseq checkpoint writing.",
                "update_details": "Created new checkpoint config param `--write-checkpoints-asynchronously` with default value `False`. Aliased to `--save-async`. Added new methods to `PathManager` class in `file_io.py` for async writes using ioPath's async `PathManager`.",
                "tasks": [
                    "Integrate ioPath's async writes feature into Fairseq checkpoint writing"
                ]
            }
        ]
    },
    "39": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-02-11T22:00:25Z",
                "message_summary": "save task state in the checkpoint (#1562)"
            }
        ]
    },
    "40": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-02-02T23:51:11Z",
                "message_summary": "Fix the task data arg conversion to string.",
                "update_details": "Task data argument type was changed from a list of strings to a string to address test failures related to task data argument compatibility.",
                "tasks": [
                    "T83395097",
                    "T83395052"
                ]
            }
        ]
    },
    "41": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-31T04:16:56Z",
                "message_summary": "Fix incorrect local cache for checkpoint_last.pt when training is restarted on the same host"
            }
        ]
    },
    "42": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-23T19:16:56Z",
                "message_summary": "fairseq checkpoint improvements"
            }
        ]
    },
    "43": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-18T19:45:08Z",
                "message_summary": "Refactor eval_lm to support library usage",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "44": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-18T15:40:49Z",
                "message_summary": "Support atomic saves for checkpoints (#1520)"
            }
        ]
    },
    "45": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-16T01:47:42Z",
                "message_summary": "Fix loading of very old checkpoints (#1512)"
            }
        ]
    },
    "46": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-08T23:49:12Z",
                "message_summary": "fix wav2vec scripts",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "47": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-05T15:37:51Z",
                "message_summary": "Rename optimization.min_lr -> optimization.stop_min_lr",
                "update_details": "The commit involves renaming 'optimization.min_lr' to 'optimization.stop_min_lr' in the file 'checkpoint_utils.py'."
            }
        ]
    },
    "48": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-11-30T22:20:36Z",
                "message_summary": "Add/fix tests",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "49": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-11-18T01:08:13Z",
                "message_summary": "fix loading ensembles",
                "update_details": "Fixes loading ensembles. Previous change used the state of the first model for all models in the ensemble"
            }
        ]
    },
    "50": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-11-11T18:15:10Z",
                "message_summary": "load dataset with saved task config (optionally) (#1423)",
                "update_details": "Added an argument to load_dataset that provides task configuration from the checkpoint. Different tasks can decide what to do with it afterwards."
            }
        ]
    },
    "51": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-11-09T23:46:00Z",
                "message_summary": "migrate wav2vec2 model (#1409)",
                "ptm_addition_details": {
                    "name": "wav2vec2",
                    "version": "not specified",
                    "problem_addressed": "not specified"
                },
                "update_details": "Minor bug fixes included in the migration process."
            }
        ]
    },
    "52": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-11-05T23:29:33Z",
                "message_summary": "Move TPU grad reductions out of Trainer into TPUDistributedDataParallel (#1397)",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "53": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-10-23T07:07:33Z",
                "message_summary": "refactor dataclass related files, add proper types for static checkin (#1371)",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "54": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-10-22T23:31:49Z",
                "message_summary": "rename remove_bpe to post_process; add aliasing",
                "update_details": "Rename remove_bpe to post_process and add aliasing for compatibility with existing command lines and checkpoint upgrades.",
                "tasks": [
                    "rename remove_bpe to post_process",
                    "add aliasing"
                ]
            }
        ]
    },
    "55": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-10-20T07:32:26Z",
                "message_summary": "Enable Hydra configs in fairseq",
                "ptm_addition_details": null,
                "update_details": "Migrated \"args\" object into omegaconf \"DictConfig\" at all legacy entry points. Components like bpe encoders and tokenizers were migrated from secondary registries to facilitate smoother migration. Code referencing migrated fairseq components is being changed to inherit from \"Legacy*\" components. Tests are expected to capture most of the changes.",
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "56": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-10-19T01:14:51Z",
                "message_summary": "Apply black+isort"
            }
        ]
    },
    "57": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-10-12T17:55:40Z",
                "message_summary": "Support generation with huge pipeline parallel Transformer models (#1297)",
                "ptm_addition_details": {
                    "name": "PipelineParallelTransformer",
                    "version": "N/A",
                    "problem_addressed": "Support generation with PipelineParallelTransformer models"
                },
                "update_details": "Support loading sharded checkpoints, ability to provide a fixed dictionary for multilingual machine translation to match the dictionary used for training the checkpoint"
            }
        ]
    },
    "58": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-10-09T20:34:59Z",
                "message_summary": "Improve dictionary & checkpoint reading w/ local caching"
            }
        ]
    },
    "59": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-09-02T01:17:33Z",
                "message_summary": "Initial support for ZeRO optimizer state sharding (#1259)",
                "ptm_addition_details": null,
                "update_details": "FairseqOSS will work with any optimizer and dtype. Future plans include support for reduce instead of all_reduce, gradient sharding, and parameter sharding.",
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "60": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-08-14T17:24:51Z",
                "message_summary": "Misc fixes (#2448)"
            }
        ]
    },
    "61": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-08-06T17:20:39Z",
                "message_summary": "Multilingual v1: Multilingual Training with multiple bitext and monolingual datasets: add finetuning options",
                "update_details": "Minor changes to fairseq/checkpoint_utils.py to add finetuning option instead of using restore_file which will restore from the original model when being requeued."
            }
        ]
    },
    "62": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-08-04T15:25:50Z",
                "message_summary": "Fixes checkpoint_path while loading a model-parallel checkpoint (#2365)"
            }
        ]
    },
    "63": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-05-26T22:59:59Z",
                "message_summary": "Several small fixes (incl. set default --data-buffer-size=10)",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "64": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-05-11T17:34:42Z",
                "message_summary": "Generalize moving of tensors to CPU in checkpoints",
                "update_details": "This is needed for TPUs"
            }
        ]
    },
    "65": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-04-14T17:58:38Z",
                "message_summary": "update checkpoint mkdir behavior (issue #1986) (#2011)",
                "update_details": "Create checkpoint directory in ```save_checkpoint()``` instead of ```load_checkpoint()```."
            }
        ]
    },
    "66": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-03-28T03:22:36Z",
                "message_summary": "adding code to load and save model parallel checkpoint",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "67": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-03-11T07:36:45Z",
                "message_summary": "PySpeech TALNet: Convert to JIT and quantize",
                "update_details": "Update the TALNet model to be converted to JIT format and quantized by the `jit_pyspeech_nn_model` tool. The model structure has extra parameters `fc_att.weight` and `fc_att.bias`, requiring old model files to be loaded with `strict=False`. Changes also allow `strict=False` where necessary. Renames options of `jit_pyspeech_nn_model` for better convention conformity."
            }
        ]
    },
    "68": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-03-07T17:12:14Z",
                "message_summary": "Fix epoch reporting when restoring checkpoint",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "69": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-03-05T00:37:24Z",
                "message_summary": "Use 1-based indexing for epochs everywhere",
                "update_details": "Fixed inconsistency in the usage of 0-based or 1-based indexing for epochs. Internal indexing is now 0-based, while logging and checkpoint naming still use 1-based indexing."
            }
        ]
    },
    "70": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-02-19T04:07:05Z",
                "message_summary": "tts synthesis script"
            }
        ]
    },
    "71": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-01-22T23:36:28Z",
                "message_summary": "fblearner pyspeech manifold migration",
                "update_details": "Migrate save_dir to manifold in fblearner, and copy from manifold to gluster after training to keep downstream workflows happy."
            }
        ]
    },
    "72": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-01-20T20:15:27Z",
                "message_summary": "Fix the problem of passing None to format() when val_loss is None",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "73": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-01-17T00:14:45Z",
                "message_summary": "Switch to Python logging (+ lint)",
                "update_details": "Python logging offers a number of benefits, such as logging timestamps, better cross-library compatibility, ability to add multiple output handlers, etc."
            }
        ]
    },
    "74": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-01-16T09:59:15Z",
                "message_summary": "Split from PR#968. add --keep-best-checkpoints (#990)",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "75": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-12-17T01:22:11Z",
                "message_summary": "More fully deprecate --raw-text and --lazy-load (fixes #1488)"
            }
        ]
    },
    "76": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-12-06T02:24:56Z",
                "message_summary": "Add wrapper abstraction on top of fvcore PathManager to insulate OSS compatibility"
            }
        ]
    },
    "77": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-12-02T21:26:48Z",
                "message_summary": "Apply Black auto-formatting"
            }
        ]
    },
    "78": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-11-22T01:50:42Z",
                "message_summary": "Quick fix for Structured Dropout checkpointing (#1406)",
                "update_details": "Fix allows the user to checkpoint a translation model properly after applying layer pruning on a restored transformer file"
            }
        ]
    },
    "79": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-11-21T00:52:59Z",
                "message_summary": "Refactor data sharding to be specified via caller of task rather than task itself"
            }
        ]
    },
    "80": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-10-30T19:55:54Z",
                "message_summary": "layer drop",
                "update_details": "Enables layer drop in transformer decoder in production training pipeline (ptt_transformer). Additional logic added to handle corresponding dropping layers at test time in exported model."
            }
        ]
    },
    "81": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-10-27T19:10:53Z",
                "message_summary": "adding layerdrop code for training, pruning, and readme (#890)",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "82": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-10-12T04:53:11Z",
                "message_summary": "Added option to save checkpoints using Path Manager."
            }
        ]
    },
    "83": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-10-09T21:53:27Z",
                "message_summary": "Fix data loading memory issue in pyspeech",
                "update_details": "1. Modified data loading to shard data while reading the handles file to address memory issue. 2. Added method for synchronizing shards with the same number of batches. 3. Removed redundant loading of training dataset and batch iterator in train.py and when loading the checkpoint to reduce loading time for large files."
            }
        ]
    },
    "84": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-09-20T16:34:58Z",
                "message_summary": "added multilingual masked LM training",
                "ptm_addition_details": {
                    "name": "multilingual-RoBERTa",
                    "version": "unknown",
                    "problem_addressed": "Training with aconneau XLM data"
                },
                "update_details": "Two pieces remaining: 1) `XLM` limits batch to be from the same language, 2) `sample_ratio` in `ConcatDataset` handling by sounding off the ratio to `first decimal` and then multiplying by `10`. Simple heuristics are considered for handling the ratio.",
                "tasks": [
                    "multilingual masked LM training"
                ]
            }
        ]
    },
    "85": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-08-21T20:43:01Z",
                "message_summary": "Parameterized criterions (#808)",
                "update_details": "Support criterion with parameters, such as AutoSegmentationCriterion (ASG) used in wav2letter which has a transition matrix parameter. This is needed to integrate wav2letter's ASG into PySpeech. Parameters in criterions will be updated by optimizers with a configurable learning rate, saved and loaded from checkpoints, preserving backward compatibility for criterions without parameters, and synchronized across nodes in distributed training."
            }
        ]
    },
    "86": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-08-12T16:08:16Z",
                "message_summary": "Lint"
            }
        ]
    },
    "87": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-08-12T14:17:21Z",
                "message_summary": "Update --restore-file logic (partially fixes #999)"
            }
        ]
    },
    "88": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-08-10T15:16:50Z",
                "message_summary": "Add WSC task and criterion"
            }
        ]
    },
    "89": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-08-01T12:55:57Z",
                "message_summary": "Update PyTorch Hub interface"
            }
        ]
    },
    "90": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-07-30T14:48:23Z",
                "message_summary": "Relicense fairseq under MIT license",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "91": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-07-29T23:06:26Z",
                "message_summary": "adding glue data preprocessing scripts",
                "update_details": "Added glue data pre-processing script. Updated README with usage."
            }
        ]
    },
    "92": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-07-24T23:59:07Z",
                "message_summary": "check save_dir before beginning training",
                "update_details": "Adding a check at the beginning of the train loop to ensure the checkpoint directory is globally readable to prevent issues encountered after 8 hours of training."
            }
        ]
    },
    "93": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-07-19T20:13:43Z",
                "message_summary": "Rename _load_model_ensemble -> load_model_ensemble_and_task"
            }
        ]
    },
    "94": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-07-01T21:10:49Z",
                "message_summary": "Fixes checkpointing bug introduced in 89e077c"
            }
        ]
    },
    "95": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-06-30T18:35:33Z",
                "message_summary": "Add additional options for configuring writing of checkpoints"
            }
        ]
    },
    "96": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-06-11T18:19:58Z",
                "message_summary": "Automatically fill in default values from add_args"
            }
        ]
    },
    "97": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-05-30T18:41:40Z",
                "message_summary": "Add --reset-dataloader"
            }
        ]
    },
    "98": {
        "repository": "github.com/facebookresearch/fairseq",
        "text": [
            {
                "repo_url": "github.com/facebookresearch/fairseq",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-05-17T21:25:56Z",
                "message_summary": "Small features + lint"
            }
        ]
    },
    "99": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2022-06-22T21:03:17Z",
                "message_summary": "add check for OC version in fairseq"
            }
        ]
    },
    "100": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2022-04-14T22:39:36Z",
                "message_summary": "Fix typo in exception value",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "101": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2022-01-21T17:27:49Z",
                "message_summary": "Fix breakage from D33649708"
            }
        ]
    },
    "102": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2022-01-20T08:02:16Z",
                "message_summary": "Data2vec prelim (#2929)",
                "update_details": "Default behavior changed to raise an exception when fields in config do not have a corresponding field in the model dataclass"
            }
        ]
    },
    "103": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2022-01-19T03:29:52Z",
                "message_summary": "Decode using EMA model in IPL recipe",
                "ptm_addition_details": {
                    "name": "EMA model",
                    "version": "N/A",
                    "problem_addressed": "Decoding in transducer IPL recipe"
                },
                "update_details": "Add option to use the EMA model for decoding in transducer IPL recipe by passing --ipl-decode-ema. Note EMA should be enabled as in the diff D24238379 (https://github.com/pytorch/fairseq/commit/8feccf94412424a4683b01090de36fa77cb4951d) using options --store-ema --ema-start-update and --ema-decay."
            }
        ]
    },
    "104": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2022-01-07T08:39:11Z",
                "message_summary": "Formatting fix: get CI green (#2860)"
            }
        ]
    },
    "105": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-12-31T02:38:12Z",
                "message_summary": "Add strict option to checkpoint_utils. load_pretrained_component_from_model()"
            }
        ]
    },
    "106": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-12-17T00:11:19Z",
                "message_summary": "formatting fix",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "107": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-12-11T00:55:12Z",
                "message_summary": "Add loading from HuggingFace Hub"
            }
        ]
    },
    "108": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-11-29T20:32:59Z",
                "message_summary": "Add linting with black (#2678)"
            }
        ]
    },
    "109": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-11-19T00:38:31Z",
                "message_summary": "Merge --use-ontology-for-* into --use-ontology",
                "update_details": "Merged three ontology options (`--use-ontology-for-training`, `--use-ontology-for-validation`, `--use-ontology-for-balancing`) into one (`--use-ontology`). Logic for avoiding loading teacher models moved out of `checkpoint_utils.py`. New method specified for loading a student model without loading its teachers.",
                "tasks": [
                    "Merge ontology options",
                    "Move logic for avoiding loading teacher models"
                ]
            }
        ]
    },
    "110": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-10-23T02:51:09Z",
                "message_summary": "set num_update before loading state dict (#2491)",
                "ptm_addition_details": null,
                "update_details": "Set `model.num_updates` in `load_model_ensemble_and_task()` before loading `state_dict`, like what's done in `fairseq/trainer.py`, because a model's `state_dict` may depend on `num_update`.",
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "111": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-09-13T20:20:35Z",
                "message_summary": "Back out \"Fairseq needs to store and load metadata from model state_dict\" (#3861)",
                "update_details": "Backout fairseq changes and fix with suggested, more optimal changes in checkpoint utils."
            }
        ]
    },
    "112": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-09-09T01:18:30Z",
                "message_summary": "Fairseq needs to store and load metadata from model state_dict",
                "update_details": "Fairseq checkpoint saving and loading should mirror torch's checkpoint by saving and loading \"state_dict()._metadata\"."
            }
        ]
    },
    "113": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-09-01T19:29:51Z",
                "message_summary": "Adds Exponential moving average (EMA) model for Kaizen semi-supervised training",
                "ptm_addition_details": {
                    "name": "Exponential moving average (EMA)",
                    "problem_addressed": "Kaizen semi-supervised training"
                }
            },
            {
                "update_details": "Add `ema.store_ema` to enable storing EMA. EMA will be written to extra_state in the state dict while saving checkpoint. `ema.ema_start_update` to control when the EMA starts accumulating. Tasks can use `uses_ema` property to decide if the EMA should be passed to the task. `load_ema_from_checkpoint` can be used to load EMA model in place of the model to be used for evaluation. Pyspeech has eval-ema option for this."
            }
        ]
    },
    "114": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-08-02T21:36:32Z",
                "message_summary": "Fixing checkpoint config upgrade for generation print_alignment",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "115": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-29T23:30:41Z",
                "message_summary": "seed random suffix in checkpoint to be consistent across shards",
                "ptm_addition_details": null,
                "update_details": "Seeds the random suffix to be consistent across shards for saving sharded checkpoints, addressing the issue of different suffixes for each shard when training with FSDP and use_sharded_state=True.",
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "116": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-29T23:30:41Z",
                "message_summary": "use pathmanager to delete old checkpoints"
            }
        ]
    },
    "117": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-29T23:30:40Z",
                "message_summary": "use suffix when saving best checkpoints with metric"
            }
        ]
    },
    "118": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-09T23:25:02Z",
                "message_summary": "Roll back os.path.abspath change"
            }
        ]
    },
    "119": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-08T22:27:57Z",
                "message_summary": "Fix static container (#2036)"
            }
        ]
    },
    "120": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-06T22:07:31Z",
                "message_summary": "No significant Pre-Trained Models activities mentioned."
            }
        ]
    },
    "121": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-01T13:37:47Z",
                "message_summary": "Hydra 1.1 compatibility: Use an explicit schema for the primary config",
                "ptm_addition_details": null,
                "update_details": "Fixes compatibility with Hydra 1.1. The result is compatible with both Hydra 1.0 and Hydra 1.1, and will allow a smoother migration to Hydra 1.1. There will be some follow-up to make the code fully compatible with 1.1 once Hydra 1.1 is the default version in fbcode."
            }
        ]
    },
    "122": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-06-15T21:09:52Z",
                "message_summary": "Check attributes in trainer and checkpoint loading before using them",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "123": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-06-04T23:19:56Z",
                "message_summary": "Fix loading some TALNet models",
                "update_details": "Replaced the \"kd_binary_cross_entropy\" criterion with the \"wav2vec\" criterion when loading models to address loading failures of old models trained with the previous criterion. Removed the \"log_keys\" argument if it's `None` to prevent errors with criteria like wav2vec that require a list argument.",
                "tasks": [
                    "Fix loading some TALNet models"
                ]
            }
        ]
    },
    "124": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-06-04T00:49:12Z",
                "message_summary": "Teacher-student learning for TALNet",
                "update_details": "Teacher-student learning for TALNet implemented. Two types of teachers introduced: static teachers and dynamic teachers. Code related to static teachers copied from `KnowledgeDistillationBinaryCrossEntropyCriterion` class for cleanup in D28728718. Teacher models stored in the task object and not saved into checkpoints."
            }
        ]
    },
    "125": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-05-26T00:45:51Z",
                "message_summary": "support FSDP sharded_state checkpoint loading during inference",
                "update_details": "Using the feature added by QuentinDuval, sharded states can be consolidated into full regular states, enabling inferences on sharded states almost transparently. The update includes smart loading logic based on the existence of specific checkpoint files and user-defined parameters.",
                "tasks": [
                    "support FSDP sharded_state checkpoint loading during inference"
                ]
            }
        ]
    },
    "126": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-05-22T07:22:05Z",
                "message_summary": "fixing s2t transformer and N-best checkpoint saving",
                "update_details": "Fixing the default value for `encoder_freezing_updates` in s2t transformer. Fixing N-best checkpoint saving to compare the new checkpoint with the previous N best ones for better results on N-best checkpoint averaging."
            }
        ]
    },
    "127": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-05-21T23:18:21Z",
                "message_summary": "Attempt to make non-sharded FSDP checkpoint behave like regular checkpoint"
            }
        ]
    },
    "128": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-04-14T08:50:07Z",
                "message_summary": "add keep_interval_updates_pattern"
            }
        ]
    },
    "129": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-04-14T08:50:06Z",
                "message_summary": "enable manifold checkpoints with --keep-interval-updates"
            }
        ]
    },
    "130": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-03-30T01:02:50Z",
                "message_summary": "BASE layers (#1654)"
            }
        ]
    },
    "131": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-03-26T14:18:59Z",
                "message_summary": "FSDP uses new optimizer gathering to save optimizer state",
                "update_details": "Full unflattened optimizer state dict is in `checkpoints/shard_0.pt`, other checkpoint files do not have the `last_optimizer_state` key. Requires master version of fairscale (eventually fairscale>=0.3.3)"
            }
        ]
    },
    "132": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-03-04T21:32:44Z",
                "message_summary": "Move checkpoint state_dict creation into Trainer (#1666)",
                "update_details": "Exposing a `state_dict` method inside the Trainer to simplify the checkpoint saving logic and enable saving multiple checkpoints for different optimizer state shards.",
                "tasks": [
                    "Move checkpoint state_dict creation into Trainer"
                ]
            }
        ]
    },
    "133": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-03-04T05:17:29Z",
                "message_summary": "minor fixes and improvements",
                "update_details": "- Convert config persisted in checkpoints into a plain dict when saving and back to omegaconf config when loading to avoid compatibility issues between different versions of python, omegaconf, etc.\n- Update checkpoints that have old print_alignment saved.\n- Add lr_float to composite optimizer to enable sweeping on lr with auto sweepers like ax.\n- Fixing some edge cases for config loading."
            }
        ]
    },
    "134": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-03-02T17:26:03Z",
                "message_summary": "Integrate ioPath's async writes feature into Fairseq checkpoint writing.",
                "update_details": "Created new checkpoint config param `--write-checkpoints-asynchronously` with default value `False`. Aliased to `--save-async`. Added new methods to `PathManager` class in `file_io.py` for async writes using ioPath's async `PathManager`.",
                "tasks": [
                    "Integrate ioPath's async writes feature into Fairseq checkpoint writing"
                ]
            }
        ]
    },
    "135": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-02-11T22:00:25Z",
                "message_summary": "save task state in the checkpoint (#1562)"
            }
        ]
    },
    "136": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-02-02T23:51:11Z",
                "message_summary": "Fix the task data arg conversion to string.",
                "update_details": "Task data argument type was changed from a list of strings to a string to address test failures related to task data argument compatibility.",
                "tasks": [
                    "T83395097",
                    "T83395052"
                ]
            }
        ]
    },
    "137": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-31T04:16:56Z",
                "message_summary": "Fix incorrect local cache for checkpoint_last.pt when training is restarted on the same host"
            }
        ]
    },
    "138": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-23T19:16:56Z",
                "message_summary": "fairseq checkpoint improvements"
            }
        ]
    },
    "139": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-18T19:45:08Z",
                "message_summary": "Refactor eval_lm to support library usage",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "140": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-18T15:40:49Z",
                "message_summary": "Support atomic saves for checkpoints",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "141": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-16T01:47:42Z",
                "message_summary": "Fix loading of very old checkpoints",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "142": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-08T23:49:12Z",
                "message_summary": "fix wav2vec scripts",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "143": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-05T15:37:51Z",
                "message_summary": "Rename optimization.min_lr -> optimization.stop_min_lr",
                "update_details": "The commit involves renaming 'optimization.min_lr' to 'optimization.stop_min_lr' in the file 'checkpoint_utils.py'",
                "tasks": [
                    "Rename optimization.min_lr -> optimization.stop_min_lr"
                ]
            }
        ]
    },
    "144": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-11-30T22:20:36Z",
                "message_summary": "Add/fix tests",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "145": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-11-18T01:08:13Z",
                "message_summary": "fix loading ensembles",
                "update_details": "Fixes loading ensembles. Previous change used the state of the first model for all models in the ensemble"
            }
        ]
    },
    "146": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-11-11T18:15:10Z",
                "message_summary": "load dataset with saved task config (optionally)",
                "update_details": "Added an argument to load_dataset that provides task configuration from the checkpoint. Different tasks can decide what to do with it afterwards."
            }
        ]
    },
    "147": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-11-09T23:46:00Z",
                "message_summary": "migrate wav2vec2 model (#1409)",
                "ptm_addition_details": {
                    "name": "wav2vec2",
                    "version": "unknown",
                    "problem_addressed": "model migration"
                }
            },
            {
                "tasks": [
                    "model migration",
                    "minor bug fixes"
                ]
            }
        ]
    },
    "148": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-11-05T23:29:33Z",
                "message_summary": "Move TPU grad reductions out of Trainer into TPUDistributedDataParallel (#1397)",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "149": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-10-23T07:07:33Z",
                "message_summary": "Refactor dataclass related files and add proper types for static checking.",
                "update_details": "Refactored the dataclass hierarchy to improve organization and avoid circular references. Added a top-level FairseqConfig and updated type hints to reflect the correct config type if known."
            }
        ]
    },
    "150": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-10-22T23:31:49Z",
                "message_summary": "rename remove_bpe to post_process; add aliasing",
                "update_details": "Rename remove_bpe to post_process and add aliasing for compatibility with existing command lines. Checkpoint upgrades were also added to ensure continued functionality.",
                "tasks": [
                    "rename remove_bpe to post_process",
                    "add aliasing"
                ]
            }
        ]
    },
    "151": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-10-20T07:32:26Z",
                "message_summary": "Enable Hydra configs in fairseq",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "152": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-10-19T01:14:51Z",
                "message_summary": "Apply black+isort"
            }
        ]
    },
    "153": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-10-12T17:55:40Z",
                "message_summary": "Support generation with huge pipeline parallel Transformer models (#1297)",
                "ptm_addition_details": {
                    "name": "PipelineParallelTransformer",
                    "version": "N/A",
                    "problem_addressed": "Support generation with PipelineParallelTransformer models"
                },
                "update_details": "Support loading sharded checkpoints, ability to provide a fixed dictionary for multilingual machine translation to match the dictionary used for training the checkpoint"
            }
        ]
    },
    "154": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-10-09T20:34:59Z",
                "message_summary": "Improve dictionary & checkpoint reading w/ local caching"
            }
        ]
    },
    "155": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-09-02T01:17:33Z",
                "message_summary": "Initial support for ZeRO optimizer state sharding (#1259)",
                "ptm_addition_details": null,
                "update_details": "FairseqOSS will work with any optimizer and dtype. Added support for reduce instead of all_reduce, gradient sharding, and parameter sharding.",
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "156": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-08-14T17:24:51Z",
                "message_summary": "Misc fixes (#2448)"
            }
        ]
    },
    "157": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-08-06T17:20:39Z",
                "message_summary": "Multilingual v1: Multilingual Training with multiple bitext and monolingual datasets: add finetuning options",
                "update_details": "Minor changes to fairseq/checkpoint_utils.py to add finetuning option instead of using restore_file which will restore from the original model when being requeued."
            }
        ]
    },
    "158": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-08-04T15:25:50Z",
                "message_summary": "Fixes checkpoint_path while loading a model-parallel checkpoint",
                "update_details": "Fixes an issue related to loading a model-parallel checkpoint",
                "tasks": [
                    "Fixing checkpoint_path"
                ]
            }
        ]
    },
    "159": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-05-26T22:59:59Z",
                "message_summary": "Several small fixes (incl. set default --data-buffer-size=10)",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "160": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-05-11T17:34:42Z",
                "message_summary": "Generalize moving of tensors to CPU in checkpoints",
                "ptm_addition_details": null,
                "update_details": "This update is necessary for TPUs.",
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "161": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-04-14T17:58:38Z",
                "message_summary": "update checkpoint mkdir behavior (issue #1986) (#2011)",
                "update_details": "Create checkpoint directory in ```save_checkpoint()``` instead of ```load_checkpoint()```."
            }
        ]
    },
    "162": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-03-28T03:22:36Z",
                "message_summary": "adding code to load and save model parallel checkpoint",
                "ptm_addition_details": {}
            }
        ]
    },
    "163": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-03-11T07:36:45Z",
                "message_summary": "PySpeech TALNet: Convert to JIT and quantize",
                "ptm_addition_details": {
                    "name": "TALNet",
                    "version": "not specified",
                    "problem_addressed": "Update the TALNet model to be converted to JIT format and quantized by the `jit_pyspeech_nn_model` tool."
                },
                "update_details": "The updated model structure is slightly incompatible with the model files saved before, requiring old model files to be loaded with `strict=False`. Changes were made to allow `strict=False` where necessary. Options of `jit_pyspeech_nn_model` were renamed to conform better to convention.",
                "tasks": [
                    "Convert TALNet model to JIT format",
                    "Quantize TALNet model using `jit_pyspeech_nn_model` tool",
                    "Update model structure to handle compatibility issues with old model files",
                    "Rename options of `jit_pyspeech_nn_model` for better convention compliance"
                ]
            }
        ]
    },
    "164": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-03-07T17:12:14Z",
                "message_summary": "Fix epoch reporting when restoring checkpoint",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "165": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-03-05T00:37:24Z",
                "message_summary": "Use 1-based indexing for epochs everywhere",
                "update_details": "Fixed inconsistency in the usage of 0-based and 1-based indexing for epochs. The update ensures internal consistency with 0-based indexing while maintaining 1-based indexing for logging and checkpoint naming."
            }
        ]
    },
    "166": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-02-19T04:07:05Z",
                "message_summary": "tts synthesis script",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "167": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-01-22T23:36:28Z",
                "message_summary": "fblearner pyspeech manifold migration"
            }
        ]
    },
    "168": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-01-20T20:15:27Z",
                "message_summary": "Fix the problem of passing None to format() when val_loss is None",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "169": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-01-17T00:14:45Z",
                "message_summary": "Switch to Python logging (+ lint)",
                "update_details": "Python logging offers a number of benefits, such as logging timestamps, better cross-library compatibility, ability to add multiple output handlers, etc."
            }
        ]
    },
    "170": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-01-16T09:59:15Z",
                "message_summary": "Split from PR#968. add --keep-best-checkpoints (#990)",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "171": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-12-17T01:22:11Z",
                "message_summary": "More fully deprecate --raw-text and --lazy-load (fixes #1488)"
            }
        ]
    },
    "172": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-12-06T02:24:56Z",
                "message_summary": "Add wrapper abstraction on top of fvcore PathManager to insulate OSS compatibility"
            }
        ]
    },
    "173": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-12-02T21:26:48Z",
                "message_summary": "Apply Black auto-formatting"
            }
        ]
    },
    "174": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-11-22T01:50:42Z",
                "message_summary": "Quick fix for Structured Dropout checkpointing (#1406)",
                "update_details": "This fix allows the user to checkpoint a translation model properly after applying layer pruning on a restored transformer file."
            }
        ]
    },
    "175": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-11-21T00:52:59Z",
                "message_summary": "Refactor data sharding to be specified via caller of task rather than task itself"
            }
        ]
    },
    "176": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-10-30T19:55:54Z",
                "message_summary": "layer drop",
                "update_details": "Enables layer drop in transformer decoder in production training pipeline (ptt_transformer). Additional logic added to handle corresponding dropping layers at test time in exported model."
            }
        ]
    },
    "177": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-10-27T19:10:53Z",
                "message_summary": "adding layerdrop code for training, pruning, and readme (#890)",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "178": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-10-12T04:53:11Z",
                "message_summary": "Added option to save checkpoints using Path Manager."
            }
        ]
    },
    "179": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-10-09T21:53:27Z",
                "message_summary": "Fix data loading memory issue in pyspeech"
            }
        ]
    },
    "180": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-09-20T16:34:58Z",
                "message_summary": "added multilingual masked LM training",
                "ptm_addition_details": {
                    "name": "multilingual-RoBERTa",
                    "version": "unknown",
                    "problem_addressed": "Training with aconneau XLM data"
                },
                "update_details": "Two pieces remaining: 1) `XLM` limits batch to be from the same language, potential implementation of `batch_by_size_and_language` function. 2) Handling `sample_ratio` in `ConcatDataset` by adjusting the ratio for better performance.",
                "tasks": [
                    "multilingual masked LM training"
                ]
            }
        ]
    },
    "181": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-08-21T20:43:01Z",
                "message_summary": "Parameterized criterions (#808)",
                "update_details": "Support criterion with parameters, such as AutoSegmentationCriterion (ASG) used in wav2letter which has a transition matrix parameter. This is needed to integrate wav2letter's ASG into PySpeech. Parameters in criterions will be updated by optimizers with a configurable learning rate, saved and loaded from checkpoints, preserving backward compatibility for criterions without parameters, and synchronized across nodes in distributed training."
            }
        ]
    },
    "182": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-08-12T16:08:16Z",
                "message_summary": "Lint"
            }
        ]
    },
    "183": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-08-12T14:17:21Z",
                "message_summary": "Update --restore-file logic (partially fixes #999)"
            }
        ]
    },
    "184": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-08-10T15:16:50Z",
                "message_summary": "Add WSC task and criterion"
            }
        ]
    },
    "185": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-08-01T12:55:57Z",
                "message_summary": "Update PyTorch Hub interface"
            }
        ]
    },
    "186": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-07-30T14:48:23Z",
                "message_summary": "Relicense fairseq under MIT license",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "187": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-07-29T23:06:26Z",
                "message_summary": "adding glue data preprocessing scripts",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "188": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-07-24T23:59:07Z",
                "message_summary": "check save_dir before beginning training",
                "update_details": "Adding a check at the beginning of the train loop to ensure the checkpoint directory is globally readable to prevent issues encountered after 8 hours of training."
            }
        ]
    },
    "189": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-07-19T20:13:43Z",
                "message_summary": "Rename _load_model_ensemble -> load_model_ensemble_and_task"
            }
        ]
    },
    "190": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-07-01T21:10:49Z",
                "message_summary": "Fixes checkpointing bug introduced in 89e077c"
            }
        ]
    },
    "191": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-06-30T18:35:33Z",
                "message_summary": "Add additional options for configuring writing of checkpoints"
            }
        ]
    },
    "192": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-06-11T18:19:58Z",
                "message_summary": "Automatically fill in default values from add_args"
            }
        ]
    },
    "193": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-05-30T18:41:40Z",
                "message_summary": "Add --reset-dataloader"
            }
        ]
    },
    "194": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-05-17T21:25:56Z",
                "message_summary": "Small features + lint"
            }
        ]
    },
    "195": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-05-17T04:03:08Z",
                "message_summary": "Clean up sharded train iterator"
            }
        ]
    },
    "196": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-05-14T19:57:12Z",
                "message_summary": "Move save/load checkpoint functions to utils",
                "update_details": "Moved `load_checkpoint`, `save_checkpoint`, and `reload_train` from train.py to checkpoint_utils.py. Moved `get_perplexity` from train.py to utils.py. This change aims to make train.py lighter and enable the reuse of utils functionality when fairseq is used as an external library."
            }
        ]
    },
    "197": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-05-11T14:56:45Z",
                "message_summary": "Add missing options to TransformerDecoderLayer"
            }
        ]
    },
    "198": {
        "repository": "github.com/freewym/espresso",
        "text": [
            {
                "repo_url": "github.com/freewym/espresso",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-05-06T07:17:45Z",
                "message_summary": "Load pretrained encoder or decoder (#705)",
                "ptm_addition_details": null,
                "update_details": "Functionality added to load a pretrained encoder or decoder from another pretrained model into the current model."
            }
        ]
    },
    "199": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2022-07-06T12:58:32Z",
                "message_summary": "[nllb] No Language Left Behind @ 200"
            }
        ]
    },
    "200": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2022-01-21T17:27:49Z",
                "message_summary": "Fix breakage from D33649708"
            }
        ]
    },
    "201": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2022-01-20T08:02:16Z",
                "message_summary": "Data2vec prelim (#2929)",
                "update_details": "Minor improvements and bug fixes. Default behavior changed to raise an exception when fields in config do not have a corresponding field in the model dataclass."
            }
        ]
    },
    "202": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2022-01-19T03:29:52Z",
                "message_summary": "Decode using EMA model in IPL recipe",
                "ptm_addition_details": {
                    "name": "EMA model",
                    "version": "N/A",
                    "problem_addressed": "Decoding in transducer IPL recipe using EMA model"
                },
                "update_details": "Add option to use the EMA model for decoding in transducer IPL recipe by passing --ipl-decode-ema. Note EMA should be enabled as in the diff D24238379 (https://github.com/pytorch/fairseq/commit/8feccf94412424a4683b01090de36fa77cb4951d) using options --store-ema --ema-start-update and --ema-decay."
            }
        ]
    },
    "203": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2022-01-07T08:39:11Z",
                "message_summary": "Formatting fix: get CI green (#2860)"
            }
        ]
    },
    "204": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-12-31T02:38:12Z",
                "message_summary": "Add strict option to checkpoint_utils.load_pretrained_component_from_model()"
            }
        ]
    },
    "205": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-12-17T00:11:19Z",
                "message_summary": "formatting fix (#2816)",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "206": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-12-11T00:55:12Z",
                "message_summary": "Add loading from HuggingFace Hub"
            }
        ]
    },
    "207": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-11-29T20:32:59Z",
                "message_summary": "Add linting with black (#2678)"
            }
        ]
    },
    "208": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-11-19T00:38:31Z",
                "message_summary": "Merge --use-ontology-for-* into --use-ontology",
                "update_details": "Merged three options related to ontology into one (--use-ontology). Logic for avoiding loading teacher models moved out of checkpoint_utils.py to allow loading a student model without its teachers.",
                "tasks": [
                    "Merge options related to ontology",
                    "Move logic for avoiding loading teacher models"
                ]
            }
        ]
    },
    "209": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-10-23T02:51:09Z",
                "message_summary": "set num_update before loading state dict (#2491)",
                "ptm_addition_details": null,
                "update_details": "Set `model.num_updates` in `load_model_ensemble_and_task()` before loading `state_dict`, like what's done in `fairseq/trainer.py`, because a model's `state_dict` may depend on `num_update`.",
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "210": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-09-13T20:20:35Z",
                "message_summary": "Back out \"Fairseq needs to store and load metadata from model state_dict\" (#3861)",
                "update_details": "Fix with suggested, more optimal changes in checkpoint utils"
            }
        ]
    },
    "211": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-09-09T01:18:30Z",
                "message_summary": "Fairseq needs to store and load metadata from model state_dict",
                "update_details": "Fairseq checkpoint saving and loading should mirror torch's checkpoint by saving and loading \"state_dict()._metadata\". During model loading and saving, Quantization-aware-training models in Pytorch encounters a bug related to mismatched state_dict values. The issue was identified in torch's use of state_dict()._metadata to store module._version, which was not stored in fairseq checkpoints or loaded during checkpoint loading.",
                "tasks": [
                    "Quantization-aware-training models",
                    "Pytorch",
                    "Fairseq"
                ]
            }
        ]
    },
    "212": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-09-01T19:29:51Z",
                "message_summary": "Adds Exponential moving average (EMA) model for Kaizen semi-supervised training",
                "ptm_addition_details": {
                    "name": "Exponential moving average (EMA)",
                    "version": "N/A",
                    "problem_addressed": "Kaizen semi-supervised training"
                }
            },
            {
                "update_details": "Adds functionality for EMA model: `ema.store_ema` to enable storing EMA, `ema.ema_start_update` to control when EMA starts accumulating, `uses_ema` property for tasks to decide EMA usage, and `load_ema_from_checkpoint` to load EMA model for evaluation."
            }
        ]
    },
    "213": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-08-02T21:36:32Z",
                "message_summary": "fixing checkpoint config upgrade for generation print_alignment",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "214": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-29T23:30:41Z",
                "message_summary": "seed random suffix in checkpoint to be consistent across shards",
                "update_details": "Seeds the random suffix to ensure consistency across shards when saving sharded checkpoints, particularly when training with FSDP and use_sharded_state=True. This change aims to address the issue where different suffixes for each shard made it challenging for downstream applications to load the checkpoints properly.",
                "tasks": [
                    "seed random suffix",
                    "consistent suffix across shards"
                ]
            }
        ]
    },
    "215": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-29T23:30:41Z",
                "message_summary": "use pathmanager to delete old checkpoints"
            }
        ]
    },
    "216": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-29T23:30:40Z",
                "message_summary": "use suffix when saving best checkpoints with metric"
            }
        ]
    },
    "217": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-09T23:25:02Z",
                "message_summary": "Roll back os.path.abspath change"
            }
        ]
    },
    "218": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-08T22:27:57Z",
                "message_summary": "Fix static container (#2036)",
                "update_details": "Fixes StatefulContainer being static which is a problem when you load a checkpoint with a task that already has the same keys in the container. Also prints the full path to checkpoints when saving (useful with hydra) and crashes if repeatedly failing to save a checkpoint."
            }
        ]
    },
    "219": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-06T22:07:31Z",
                "message_summary": "No significant Pre-Trained Models activities mentioned."
            }
        ]
    },
    "220": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-01T13:37:47Z",
                "message_summary": "Hydra 1.1 compatibility: Use an explicit schema for the primary config",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "221": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-06-15T21:09:52Z",
                "message_summary": "Check attributes in trainer and checkpoint loading before using them",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "222": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-06-04T23:19:56Z",
                "message_summary": "Fix loading some TALNet models",
                "update_details": "Replaced the \"kd_binary_cross_entropy\" criterion with the \"wav2vec\" criterion when loading models to fix loading errors. Removed the \"log_keys\" argument if it's `None` to prevent errors with criteria like wav2vec that require a list argument.",
                "tasks": [
                    "Fix loading some TALNet models"
                ]
            }
        ]
    },
    "223": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-06-04T00:49:12Z",
                "message_summary": "Teacher-student learning for TALNet",
                "ptm_addition_details": {
                    "name": "TALNet"
                },
                "update_details": "Teacher-student learning implemented for TALNet. Two types of teachers introduced: static teachers and dynamic teachers. Code related to static teachers copied over from `KnowledgeDistillationBinaryCrossEntropyCriterion` class. Teacher models stored in the task object and not saved into checkpoints."
            }
        ]
    },
    "224": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-05-26T00:45:51Z",
                "message_summary": "support FSDP sharded_state checkpoint loading during inference",
                "update_details": "Added support for loading sharded states into full regular states during inference. The update allows inferences on sharded states almost transparently. The logic for determining the type of checkpoint to load based on the existence of specific files and parameters was implemented.",
                "tasks": [
                    "FSDP sharded_state checkpoint loading during inference"
                ]
            }
        ]
    },
    "225": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-05-22T07:22:05Z",
                "message_summary": "fixing s2t transformer and N-best checkpoint saving",
                "update_details": "Fixing the default value for `encoder_freezing_updates` in s2t transformer. Fixing N-best checkpoint saving to compare the new checkpoint with the previous N best ones for optimal results on N-best checkpoint averaging."
            }
        ]
    },
    "226": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-05-21T23:18:21Z",
                "message_summary": "Attempt to make non-sharded FSDP checkpoint behave like regular checkpoint",
                "update_details": "Overall just wondering if the feature is desirable. If it is, the next diff which supports loading sharded checkpoint into a consolidated state dict cleaner. A couple of advantages: 1. Allows resuming from other DDP trainers. 2. Allows resuming into other DDP trainers or FSDP of a different configuration. 3. Non-sharded FSDP checkpoint can be loaded with regular load_model_ensemble_and_task(). For old training workflow that's not using `--use-sharded-state`, please rename the checkpoint to remove the \"-shard0\" for resuming training.",
                "tasks": [
                    "attempt to make non-sharded FSDP checkpoint behave like regular checkpoint"
                ]
            }
        ]
    },
    "227": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-04-14T08:50:07Z",
                "message_summary": "add keep_interval_updates_pattern"
            }
        ]
    },
    "228": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-04-14T08:50:06Z",
                "message_summary": "enable manifold checkpoints with --keep-interval-updates"
            }
        ]
    },
    "229": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-03-30T01:02:50Z",
                "message_summary": "BASE layers (#1654)"
            }
        ]
    },
    "230": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-03-26T14:18:59Z",
                "message_summary": "FSDP uses new optimizer gathering to save optimizer state (#1744)",
                "update_details": "Full unflattened optimizer state dict is in `checkpoints/shard_0.pt`, other checkpoint files do not have the `last_optimizer_state` key. Requires master version of fairscale (eventually fairscale>=0.3.3)"
            }
        ]
    },
    "231": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-03-04T21:32:44Z",
                "message_summary": "Move checkpoint state_dict creation into Trainer (#1666)"
            }
        ]
    },
    "232": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-03-04T05:17:29Z",
                "message_summary": "minor fixes and improvements",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "233": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-03-02T17:26:03Z",
                "message_summary": "Integrate ioPath's async writes feature into Fairseq checkpoint writing.",
                "update_details": "Created new checkpoint config param `--write-checkpoints-asynchronously` with default value `False`. Aliased to `--save-async`. Added new methods to `PathManager` class in `file_io.py` for async writes using ioPath's async `PathManager`.",
                "tasks": [
                    "Integrate ioPath's async writes feature into Fairseq checkpoint writing"
                ]
            }
        ]
    },
    "234": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-02-11T22:00:25Z",
                "message_summary": "save task state in the checkpoint (#1562)"
            }
        ]
    },
    "235": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-02-02T23:51:11Z",
                "message_summary": "Fix the task data arg conversion to string.",
                "update_details": "Task data argument type was changed from a list of strings to a string to address test failures related to task data argument compatibility.",
                "tasks": [
                    "T83395097",
                    "T83395052"
                ]
            }
        ]
    },
    "236": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-31T04:16:56Z",
                "message_summary": "Fix incorrect local cache for checkpoint_last.pt when training is restarted on the same host"
            }
        ]
    },
    "237": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-23T19:16:56Z",
                "message_summary": "fairseq checkpoint improvements"
            }
        ]
    },
    "238": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-18T19:45:08Z",
                "message_summary": "Refactor eval_lm to support library usage"
            }
        ]
    },
    "239": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-18T15:40:49Z",
                "message_summary": "Support atomic saves for checkpoints",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "240": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-16T01:47:42Z",
                "message_summary": "Fix loading of very old checkpoints (#1512)"
            }
        ]
    },
    "241": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-08T23:49:12Z",
                "message_summary": "fix wav2vec scripts (#1494)",
                "update_details": "docs + migration of old models"
            }
        ]
    },
    "242": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-05T15:37:51Z",
                "message_summary": "Rename optimization.min_lr -> optimization.stop_min_lr",
                "update_details": "The commit involves renaming optimization.min_lr to optimization.stop_min_lr.",
                "tasks": [
                    "Rename optimization.min_lr to optimization.stop_min_lr"
                ]
            }
        ]
    },
    "243": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-11-30T22:20:36Z",
                "message_summary": "Add/fix tests",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "244": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-11-18T01:08:13Z",
                "message_summary": "fix loading ensembles",
                "update_details": "Fixes loading ensembles. Previous change used the state of the first model for all models in the ensemble"
            }
        ]
    },
    "245": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-11-11T18:15:10Z",
                "message_summary": "load dataset with saved task config (optionally) (#1423)",
                "update_details": "Added an argument to load_dataset that provides task configuration from the checkpoint. Different tasks can decide what to do with it afterwards."
            }
        ]
    },
    "246": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-11-09T23:46:00Z",
                "message_summary": "migrate wav2vec2 model (#1409)",
                "ptm_addition_details": {
                    "name": "wav2vec2",
                    "version": "unknown",
                    "problem_addressed": "model migration"
                }
            },
            {
                "tasks": [
                    "model migration",
                    "minor bug fixes"
                ]
            }
        ]
    },
    "247": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-11-05T23:29:33Z",
                "message_summary": "Move TPU grad reductions out of Trainer into TPUDistributedDataParallel (#1397)",
                "update_details": "TPU grad reductions were moved out of Trainer into TPUDistributedDataParallel.",
                "tasks": [
                    "Move TPU grad reductions out of Trainer into TPUDistributedDataParallel"
                ]
            }
        ]
    },
    "248": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-10-23T07:07:33Z",
                "message_summary": "Refactor dataclass related files, add proper types for static checking",
                "update_details": "Refactored the dataclass hierarchy to improve organization and avoid circular references. Added a top-level FairseqConfig and updated type hints to reflect correct config types when known."
            }
        ]
    },
    "249": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-10-22T23:31:49Z",
                "message_summary": "rename remove_bpe to post_process; add aliasing",
                "update_details": "Rename remove_bpe to post_process and add aliasing for compatibility with existing command lines. Checkpoint upgrades were added to ensure continued functionality.",
                "tasks": [
                    "rename remove_bpe to post_process",
                    "add aliasing"
                ]
            }
        ]
    },
    "250": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-10-20T07:32:26Z",
                "message_summary": "Enable Hydra configs in fairseq",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "251": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-10-19T01:14:51Z",
                "message_summary": "Apply black+isort"
            }
        ]
    },
    "252": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-10-12T17:55:40Z",
                "message_summary": "Support generation with huge pipeline parallel Transformer models (#1297)",
                "ptm_addition_details": {
                    "name": "PipelineParallelTransformer",
                    "version": "N/A",
                    "problem_addressed": "Support generation with PipelineParallelTransformer models"
                },
                "update_details": "Support loading sharded checkpoints, ability to provide a fixed dictionary for multilingual machine translation, and support generation with PipelineParallelTransformer models"
            }
        ]
    },
    "253": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-10-09T20:34:59Z",
                "message_summary": "Improve dictionary & checkpoint reading w/ local caching"
            }
        ]
    },
    "254": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-09-02T01:17:33Z",
                "message_summary": "Initial support for ZeRO optimizer state sharding (#1259)",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "255": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-08-14T17:24:51Z",
                "message_summary": "Misc fixes (#2448)"
            }
        ]
    },
    "256": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-08-06T17:20:39Z",
                "message_summary": "Multilingual v1: Multilingual Training with multiple bitext and monolingual datasets: add finetuning options",
                "update_details": "Minor changes to fairseq/checkpoint_utils.py to add finetuning option instead of using restore_file which will restore from the original model when being requeued."
            }
        ]
    },
    "257": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-08-04T15:25:50Z",
                "message_summary": "Fixes checkpoint_path while loading a model-parallel checkpoint",
                "update_details": "Fixes issue #2351 in Fairseq related to checkpoint_path when loading a model-parallel checkpoint"
            }
        ]
    },
    "258": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-05-26T22:59:59Z",
                "message_summary": "Several small fixes (incl. set default --data-buffer-size=10)",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "259": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-05-11T17:34:42Z",
                "message_summary": "Generalize moving of tensors to CPU in checkpoints",
                "update_details": "This is needed for TPUs"
            }
        ]
    },
    "260": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-04-14T17:58:38Z",
                "message_summary": "update checkpoint mkdir behavior (issue #1986) (#2011)",
                "update_details": "Create checkpoint directory in ```save_checkpoint()``` instead of ```load_checkpoint()```."
            }
        ]
    },
    "261": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-03-28T03:22:36Z",
                "message_summary": "adding code to load and save model parallel checkpoint",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "262": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-03-11T07:36:45Z",
                "message_summary": "PySpeech TALNet: Convert to JIT and quantize",
                "ptm_addition_details": {
                    "name": "TALNet"
                },
                "update_details": "Update the TALNet model to be converted to JIT format and quantized by the `jit_pyspeech_nn_model` tool. The model structure has extra parameters `fc_att.weight` and `fc_att.bias`, requiring old model files to be loaded with `strict=False`. Changes also allow `strict=False` where necessary. Renames options of `jit_pyspeech_nn_model` to conform better to convention."
            }
        ]
    },
    "263": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-03-07T17:12:14Z",
                "message_summary": "Fix epoch reporting when restoring checkpoint",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "264": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-03-05T00:37:24Z",
                "message_summary": "Use 1-based indexing for epochs everywhere",
                "update_details": "Fixed inconsistency in the usage of 0-based and 1-based indexing for epochs. Internally switched to 0-based indexing while maintaining 1-based indexing for logging and checkpoint naming."
            }
        ]
    },
    "265": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-02-19T04:07:05Z",
                "message_summary": "tts synthesis script"
            }
        ]
    },
    "266": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-01-22T23:36:28Z",
                "message_summary": "fblearner pyspeech manifold migration"
            }
        ]
    },
    "267": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-01-20T20:15:27Z",
                "message_summary": "Fix the problem of passing None to format() when val_loss is None",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "268": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-01-17T00:14:45Z",
                "message_summary": "Switch to Python logging (+ lint) (#1627)",
                "update_details": "Switch to Python logging for benefits like logging timestamps, better cross-library compatibility, and multiple output handlers."
            }
        ]
    },
    "269": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-01-16T09:59:15Z",
                "message_summary": "Split from PR#968. add --keep-best-checkpoints (#990)",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "270": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-12-17T01:22:11Z",
                "message_summary": "More fully deprecate --raw-text and --lazy-load (fixes #1488)"
            }
        ]
    },
    "271": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-12-06T02:24:56Z",
                "message_summary": "Add wrapper abstraction on top of fvcore PathManager to insulate OSS compatibility"
            }
        ]
    },
    "272": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-12-02T21:26:48Z",
                "message_summary": "Apply Black auto-formatting"
            }
        ]
    },
    "273": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-11-22T01:50:42Z",
                "message_summary": "Quick fix for Structured Dropout checkpointing (#1406)",
                "update_details": "This fix allows the user to checkpoint a translation model properly after applying layer pruning on a restored transformer file."
            }
        ]
    },
    "274": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-11-21T00:52:59Z",
                "message_summary": "Refactor data sharding to be specified via caller of task rather than task itself"
            }
        ]
    },
    "275": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-10-30T19:55:54Z",
                "message_summary": "layer drop",
                "update_details": "Enables layer drop in transformer decoder in production training pipeline (ptt_transformer). Additional logic added to handle corresponding dropping layers at test time in exported model."
            }
        ]
    },
    "276": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-10-27T19:10:53Z",
                "message_summary": "adding layerdrop code for training, pruning, and readme (#890)",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "277": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-10-12T04:53:11Z",
                "message_summary": "Added option to save checkpoints using Path Manager."
            }
        ]
    },
    "278": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-10-09T21:53:27Z",
                "message_summary": "Fix data loading memory issue in pyspeech",
                "update_details": "Data loading modified to shard data during handle loading, added synchronization for shard batches, and removed redundant dataset loading in train.py"
            }
        ]
    },
    "279": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-09-20T16:34:58Z",
                "message_summary": "added multilingual masked LM training",
                "update_details": "The multilingual-RoBERTa training is working with aconneau XLM data. Two pieces remaining: 1) `XLM` limits batch to be from the same language, which can be implemented by adding `batch_by_size_and_language` function. 2) Handling `sample_ratio` in `ConcatDataset` by adjusting the ratio to the first decimal and multiplying by 10.",
                "tasks": [
                    "multilingual masked LM training"
                ]
            }
        ]
    },
    "280": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-08-21T20:43:01Z",
                "message_summary": "Parameterized criterions (#808)",
                "ptm_addition_details": {
                    "name": "AutoSegmentationCriterion (ASG)"
                },
                "update_details": "Support criterion with parameters, such as AutoSegmentationCriterion (ASG) used in wav2letter which has a transition matrix parameter. This is needed to integrate wav2letter's ASG into PySpeech. Parameters in criterions will be updated by optimizers with a configurable learning rate, saved and loaded from checkpoints, and synchronized across nodes in distributed training.",
                "tasks": [
                    "Integrate wav2letter's ASG into PySpeech"
                ]
            }
        ]
    },
    "281": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-08-12T16:08:16Z",
                "message_summary": "Lint"
            }
        ]
    },
    "282": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-08-12T14:17:21Z",
                "message_summary": "Update --restore-file logic (partially fixes #999)"
            }
        ]
    },
    "283": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-08-10T15:16:50Z",
                "message_summary": "Add WSC task and criterion"
            }
        ]
    },
    "284": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-08-01T12:55:57Z",
                "message_summary": "Update PyTorch Hub interface"
            }
        ]
    },
    "285": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-07-30T14:48:23Z",
                "message_summary": "Relicense fairseq under MIT license",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "286": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-07-29T23:06:26Z",
                "message_summary": "adding glue data preprocessing scripts",
                "update_details": "Added glue data pre-processing script. Updated README with usage."
            }
        ]
    },
    "287": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-07-24T23:59:07Z",
                "message_summary": "check save_dir before beginning training"
            }
        ]
    },
    "288": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-07-19T20:13:43Z",
                "message_summary": "Rename _load_model_ensemble -> load_model_ensemble_and_task"
            }
        ]
    },
    "289": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-07-01T21:10:49Z",
                "message_summary": "Fixes checkpointing bug introduced in 89e077c"
            }
        ]
    },
    "290": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-06-30T18:35:33Z",
                "message_summary": "Add additional options for configuring writing of checkpoints"
            }
        ]
    },
    "291": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-06-11T18:19:58Z",
                "message_summary": "Automatically fill in default values from add_args"
            }
        ]
    },
    "292": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-05-30T18:41:40Z",
                "message_summary": "Add --reset-dataloader"
            }
        ]
    },
    "293": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-05-17T21:25:56Z",
                "message_summary": "Small features + lint"
            }
        ]
    },
    "294": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-05-17T04:03:08Z",
                "message_summary": "Clean up sharded train iterator"
            }
        ]
    },
    "295": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-05-14T19:57:12Z",
                "message_summary": "Move save/load checkpoint functions to utils",
                "update_details": "Move load_checkpoint, save_checkpoint, and reload_train from train.py to checkpoint_utils.py. Move get_perplexity from train.py to utils.py. This will make train.py lighter and allow us to reuse all this utils functionality when fairseq is used as an external library."
            }
        ]
    },
    "296": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-05-11T14:56:45Z",
                "message_summary": "Add missing options to TransformerDecoderLayer"
            }
        ]
    },
    "297": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-05-06T07:17:45Z",
                "message_summary": "Load pretrained encoder or decoder (#705)",
                "ptm_addition_details": null,
                "update_details": "Functionality added to load a pretrained encoder or decoder from another pretrained model into the current model."
            }
        ]
    },
    "298": {
        "repository": "github.com/gordicaleksa/Open-NLLB",
        "text": [
            {
                "repo_url": "github.com/gordicaleksa/Open-NLLB",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-04-30T02:50:58Z",
                "message_summary": "Merge internal changes (#654)",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "299": {
        "repository": "github.com/princeton-nlp/DinkyTrain",
        "text": [
            {
                "repo_url": "github.com/princeton-nlp/DinkyTrain",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2022-04-26T01:00:09Z",
                "message_summary": "Initial fairseq commit using 7e758841da9e05cb21826a60d30a563a9e189d1d"
            }
        ]
    },
    "300": {
        "repository": "github.com/gonglinyuan/metro_t0",
        "text": [
            {
                "repo_url": "github.com/gonglinyuan/metro_t0",
                "filepath": "training/fairseq/checkpoint_utils.py",
                "commit_date": "2023-05-28T18:06:22Z",
                "message_summary": "Add code for pretraining and finetuning"
            }
        ]
    },
    "301": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2022-01-07T08:39:11Z",
                "message_summary": "Formatting fix: get CI green (#2860)"
            }
        ]
    },
    "302": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-12-31T02:38:12Z",
                "message_summary": "Add strict option to checkpoint_utils.load_pretrained_component_from_model()"
            }
        ]
    },
    "303": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-12-17T00:11:19Z",
                "message_summary": "formatting fix (#2816)",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "304": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-12-11T00:55:12Z",
                "message_summary": "Add loading from HuggingFace Hub"
            }
        ]
    },
    "305": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-11-29T20:32:59Z",
                "message_summary": "Add linting with black (#2678)"
            }
        ]
    },
    "306": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-11-19T00:38:31Z",
                "message_summary": "Merge --use-ontology-for-* into --use-ontology",
                "update_details": "Merged three options related to ontology into one (--use-ontology). Logic for avoiding loading teacher models was moved out of `checkpoint_utils.py`. New method specified for loading a student model without loading its teachers.",
                "tasks": [
                    "Merge options related to ontology",
                    "Move logic for avoiding loading teacher models",
                    "Specify new method for loading student model without loading teachers"
                ]
            }
        ]
    },
    "307": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-10-23T02:51:09Z",
                "message_summary": "Set num_update before loading state dict",
                "update_details": "Set `model.num_updates` in `load_model_ensemble_and_task()` before loading `state_dict`, like what's done in `fairseq/trainer.py`, because a model's `state_dict` may depend on `num_update`."
            }
        ]
    },
    "308": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-09-13T20:20:35Z",
                "message_summary": "Back out \"Fairseq needs to store and load metadata from model state_dict\" (#3861)",
                "update_details": "Fix with suggested, more optimal changes in checkpoint utils."
            }
        ]
    },
    "309": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-09-09T01:18:30Z",
                "message_summary": "Fairseq needs to store and load metadata from model state_dict",
                "update_details": "Fairseq checkpoint saving and loading should mirror torch's checkpoint by saving and loading \"state_dict()._metadata\"."
            }
        ]
    },
    "310": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-09-01T19:29:51Z",
                "message_summary": "Adds Exponential moving average (EMA) model for Kaizen semi-supervised training",
                "ptm_addition_details": {
                    "name": "Exponential moving average (EMA)",
                    "problem_addressed": "Kaizen semi-supervised training"
                }
            },
            {
                "update_details": "Adds EMA functionalities such as storing EMA, controlling EMA accumulation start, loading EMA from checkpoint, and passing EMA to tasks for decision-making. EMA class introduced to store exponentially decayed model parameters for better performance in inference or fine-tuning."
            }
        ]
    },
    "311": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-08-02T21:36:32Z",
                "message_summary": "Fixing checkpoint config upgrade for generation print_alignment",
                "ptm_addition_details": null,
                "update_details": "Fixes config upgrade conditions for upgrading generation print_alignment",
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "312": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-29T23:30:41Z",
                "message_summary": "seed random suffix in checkpoint to be consistent across shards"
            }
        ]
    },
    "313": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-29T23:30:41Z",
                "message_summary": "use pathmanager to delete old checkpoints"
            }
        ]
    },
    "314": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-29T23:30:40Z",
                "message_summary": "use suffix when saving best checkpoints with metric"
            }
        ]
    },
    "315": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-09T23:25:02Z",
                "message_summary": "Roll back os.path.abspath change"
            }
        ]
    },
    "316": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-08T22:27:57Z",
                "message_summary": "Fix static container (#2036)"
            }
        ]
    },
    "317": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-06T22:07:31Z",
                "message_summary": "No significant Pre-Trained Models activities mentioned."
            }
        ]
    },
    "318": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-07-01T13:37:47Z",
                "message_summary": "Hydra 1.1 compatibility: Use an explicit schema for the primary config",
                "ptm_addition_details": null,
                "update_details": "Fixes compatibility with Hydra 1.1. The result is compatible with both Hydra 1.0 and Hydra 1.1, and will allow a smoother migration to Hydra 1.1. There will be some follow-up to make the code fully compatible with 1.1 once Hydra 1.1 is the default version in fbcode.",
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "319": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-06-15T21:09:52Z",
                "message_summary": "Check attributes in trainer and checkpoint loading before using them",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "320": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-06-04T23:19:56Z",
                "message_summary": "Fix loading some TALNet models",
                "update_details": "Replaced the \"kd_binary_cross_entropy\" criterion with the \"wav2vec\" criterion when loading models to fix loading issues with old models trained with the previous criterion. Removed the \"log_keys\" argument if it's `None` to prevent errors with criteria like wav2vec that require a list argument.",
                "tasks": [
                    "Fix loading some TALNet models"
                ]
            }
        ]
    },
    "321": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-06-04T00:49:12Z",
                "message_summary": "Teacher-student learning for TALNet",
                "update_details": "Teacher-student learning for TALNet implemented. Three classes participate in the teacher-student learning process: task loads teacher models, model generates predictions using teacher models and mixes them with original targets, Wav2VecCriterion computes loss using mixed targets but still uses original targets for MAP and MAUC metrics. Two types of teachers: static teachers (file storing predictions on training data) and dynamic teachers (model files loaded at the beginning of training). Static teachers are no longer used. Code related to static teachers copied from KnowledgeDistillationBinaryCrossEntropyCriterion class. Teacher models stored in task object and not saved into checkpoints.",
                "tasks": [
                    "Teacher-student learning for TALNet"
                ]
            }
        ]
    },
    "322": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-05-26T00:45:51Z",
                "message_summary": "support FSDP sharded_state checkpoint loading during inference",
                "update_details": "Using the feature added by QuentinDuval, sharded states can be consolidated into full regular states, enabling inferences on sharded states almost transparently. The logic for determining the type of checkpoint to load based on the existence of specific files and parameters is explained. There is a suggestion for a potential enhancement to automatically determine the number of checkpoints needed.",
                "tasks": [
                    "support FSDP sharded_state checkpoint loading during inference"
                ]
            }
        ]
    },
    "323": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-05-22T07:22:05Z",
                "message_summary": "fixing s2t transformer and N-best checkpoint saving",
                "update_details": "Fixing the default value for `encoder_freezing_updates` in s2t transformer. Fixing N-best checkpoint saving to compare the new checkpoint with the previous N best ones for improved results on N-best checkpoint averaging."
            }
        ]
    },
    "324": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-05-21T23:18:21Z",
                "message_summary": "attempt to make non-sharded FSDP checkpoint behave like regular checkpoint",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "325": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-04-14T08:50:07Z",
                "message_summary": "add keep_interval_updates_pattern"
            }
        ]
    },
    "326": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-04-14T08:50:06Z",
                "message_summary": "enable manifold checkpoints with --keep-interval-updates"
            }
        ]
    },
    "327": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-03-30T01:02:50Z",
                "message_summary": "BASE layers (#1654)"
            }
        ]
    },
    "328": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-03-26T14:18:59Z",
                "message_summary": "FSDP uses new optimizer gathering to save optimizer state (#1744)",
                "update_details": "Full unflattened optimizer state dict is in `checkpoints/shard_0.pt`, other checkpoint files do not have the `last_optimizer_state` key. Requires master version of fairscale (eventually fairscale>=0.3.3)"
            }
        ]
    },
    "329": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-03-04T21:32:44Z",
                "message_summary": "Move checkpoint state_dict creation into Trainer (#1666)",
                "update_details": "Exposing a `state_dict` method inside the Trainer to simplify the checkpoint saving logic and enable saving multiple checkpoints for different optimizer state shards.",
                "tasks": [
                    "Move checkpoint state_dict creation into Trainer"
                ]
            }
        ]
    },
    "330": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-03-04T05:17:29Z",
                "message_summary": "minor fixes and improvements",
                "update_details": "- Convert config persisted in checkpoints into a plain dict when saving and back to omegaconf config when loading to avoid compatibility issues between different versions of python, omegaconf, etc.\n- Update checkpoints that have old print_alignment saved.\n- Add lr_float to composite optimizer to enable sweeping on lr with auto sweepers like ax.\n- Fixing some edge cases for config loading."
            }
        ]
    },
    "331": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-03-02T17:26:03Z",
                "message_summary": "Integrate ioPath's async writes feature into Fairseq checkpoint writing.",
                "update_details": "Created new checkpoint config param `--write-checkpoints-asynchronously` with default value `False`. Aliased to `--save-async`. Added new methods to `PathManager` class in `file_io.py` for async writes using ioPath's async `PathManager`.",
                "tasks": [
                    "Integrate ioPath's async writes feature into Fairseq checkpoint writing"
                ]
            }
        ]
    },
    "332": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-02-11T22:00:25Z",
                "message_summary": "save task state in the checkpoint (#1562)",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "333": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2021-02-02T23:51:11Z",
                "message_summary": "Fix the task data arg conversion to string.",
                "update_details": "Fixing the task data arg to be a string instead of list of strings."
            }
        ]
    },
    "334": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-31T04:16:56Z",
                "message_summary": "Fix incorrect local cache for checkpoint_last.pt when training is restarted on the same host"
            }
        ]
    },
    "335": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-23T19:16:56Z",
                "message_summary": "fairseq checkpoint improvements"
            }
        ]
    },
    "336": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-18T19:45:08Z",
                "message_summary": "Refactor eval_lm to support library usage",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "337": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-18T15:40:49Z",
                "message_summary": "Support atomic saves for checkpoints (#1520)"
            }
        ]
    },
    "338": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-16T01:47:42Z",
                "message_summary": "Fix loading of very old checkpoints",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "339": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-08T23:49:12Z",
                "message_summary": "fix wav2vec scripts (#1494)",
                "update_details": "+ docs + migration of old models"
            }
        ]
    },
    "340": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-12-05T15:37:51Z",
                "message_summary": "Rename optimization.min_lr -> optimization.stop_min_lr",
                "update_details": "The commit involves renaming 'optimization.min_lr' to 'optimization.stop_min_lr' in the file 'checkpoint_utils.py'."
            }
        ]
    },
    "341": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-11-30T22:20:36Z",
                "message_summary": "Add/fix tests",
                "update_details": "Added a test for loading ensemble checkpoints and a test for LayerDrop, along with fixing the latter.",
                "tasks": [
                    "Add test for loading ensemble checkpoints",
                    "Add test for LayerDrop",
                    "Fix LayerDrop"
                ]
            }
        ]
    },
    "342": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-11-18T01:08:13Z",
                "message_summary": "fix loading ensembles",
                "update_details": "Fixes loading ensembles. Previous change used the state of the first model for all models in the ensemble"
            }
        ]
    },
    "343": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-11-11T18:15:10Z",
                "message_summary": "load dataset with saved task config (optionally) (#1423)",
                "update_details": "Added an argument to load_dataset that provides task configuration from the checkpoint. Different tasks can decide what to do with it afterwards."
            }
        ]
    },
    "344": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-11-09T23:46:00Z",
                "message_summary": "migrate wav2vec2 model (#1409)",
                "ptm_addition_details": {
                    "name": "wav2vec2",
                    "version": "unknown",
                    "problem_addressed": "model migration"
                }
            },
            {
                "update_details": "Minor bug fixes"
            }
        ]
    },
    "345": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-11-05T23:29:33Z",
                "message_summary": "Move TPU grad reductions out of Trainer into TPUDistributedDataParallel (#1397)",
                "update_details": "TPU grad reductions were moved out of Trainer into TPUDistributedDataParallel.",
                "tasks": [
                    "Move TPU grad reductions out of Trainer into TPUDistributedDataParallel"
                ]
            }
        ]
    },
    "346": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-10-23T07:07:33Z",
                "message_summary": "Refactor dataclass related files, add proper types for static checking",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "347": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-10-22T23:31:49Z",
                "message_summary": "rename remove_bpe to post_process; add aliasing",
                "update_details": "Rename remove_bpe to post_process and add aliasing for compatibility with existing command lines. Checkpoint upgrades were added to ensure continued functionality.",
                "tasks": [
                    "rename remove_bpe to post_process",
                    "add aliasing"
                ]
            }
        ]
    },
    "348": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-10-20T07:32:26Z",
                "message_summary": "Enable Hydra configs in fairseq",
                "ptm_addition_details": null,
                "update_details": "Migrated various components from secondary registries to make the migration smoother. Changed code references to migrated fairseq components to inherit from 'Legacy*' components.",
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "349": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-10-19T01:14:51Z",
                "message_summary": "Apply black+isort (#1357)"
            }
        ]
    },
    "350": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-10-12T17:55:40Z",
                "message_summary": "Support generation with huge pipeline parallel Transformer models (#1297)",
                "ptm_addition_details": {
                    "name": "PipelineParallelTransformer",
                    "version": "N/A",
                    "problem_addressed": "Support generation with PipelineParallelTransformer models"
                },
                "update_details": "Support loading sharded checkpoints, ability to provide a fixed dictionary for multilingual machine translation, and support generation with PipelineParallelTransformer models"
            }
        ]
    },
    "351": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-10-09T20:34:59Z",
                "message_summary": "Improve dictionary & checkpoint reading w/ local caching"
            }
        ]
    },
    "352": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-09-02T01:17:33Z",
                "message_summary": "Initial support for ZeRO optimizer state sharding (#1259)",
                "update_details": "FairseqOSS will work with any optimizer and dtype. TODO: support reduce instead of all_reduce, support gradient sharding, support parameter sharding",
                "tasks": [
                    "support reduce instead of all_reduce",
                    "support gradient sharding",
                    "support parameter sharding"
                ]
            }
        ]
    },
    "353": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-08-14T17:24:51Z",
                "message_summary": "Misc fixes",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "354": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-08-06T17:20:39Z",
                "message_summary": "Multilingual v1: Multilingual Training with multiple bitext and monolingual datasets: add finetuning options",
                "update_details": "Minor changes to fairseq/checkpoint_utils.py to add finetuning option instead of using restore_file which will restore from the original model when being requeued."
            }
        ]
    },
    "355": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-08-04T15:25:50Z",
                "message_summary": "Fixes checkpoint_path while loading a model-parallel checkpoint",
                "update_details": "Fixes issue #2351 related to loading a model-parallel checkpoint"
            }
        ]
    },
    "356": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-05-26T22:59:59Z",
                "message_summary": "Several small fixes (incl. set default --data-buffer-size=10)",
                "update_details": "Several small fixes were made, including setting the default --data-buffer-size to 10."
            }
        ]
    },
    "357": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-05-11T17:34:42Z",
                "message_summary": "Generalize moving of tensors to CPU in checkpoints",
                "update_details": "This is needed for TPUs"
            }
        ]
    },
    "358": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-04-14T17:58:38Z",
                "message_summary": "update checkpoint mkdir behavior (issue #1986) (#2011)",
                "update_details": "Create checkpoint directory in ```save_checkpoint()``` instead of ```load_checkpoint()```."
            }
        ]
    },
    "359": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-03-28T03:22:36Z",
                "message_summary": "adding code to load and save model parallel checkpoint",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "360": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-03-11T07:36:45Z",
                "message_summary": "PySpeech TALNet: Convert to JIT and quantize",
                "ptm_addition_details": {
                    "name": "TALNet",
                    "version": "not specified",
                    "problem_addressed": "Update the TALNet model to be converted to JIT format and quantized by the `jit_pyspeech_nn_model` tool."
                },
                "update_details": "The updated model structure is slightly incompatible with the model files saved before, requiring old model files to be loaded with `strict=False`. Changes were made to allow `strict=False` where necessary. Options of `jit_pyspeech_nn_model` were renamed to conform better to convention.",
                "tasks": [
                    "Convert TALNet model to JIT format",
                    "Quantize TALNet model using `jit_pyspeech_nn_model` tool",
                    "Update model structure to accommodate changes"
                ]
            }
        ]
    },
    "361": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-03-07T17:12:14Z",
                "message_summary": "Fix epoch reporting when restoring checkpoint",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "362": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-03-05T00:37:24Z",
                "message_summary": "Use 1-based indexing for epochs everywhere",
                "update_details": "Fixing inconsistency in the usage of 0-based or 1-based indexing for epochs. Internally, the system will use 0-based indexing while logging and checkpoint naming will continue to use 1-based indexing."
            }
        ]
    },
    "363": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-02-19T04:07:05Z",
                "message_summary": "tts synthesis script"
            }
        ]
    },
    "364": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-01-22T23:36:28Z",
                "message_summary": "fblearner pyspeech manifold migration"
            }
        ]
    },
    "365": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-01-20T20:15:27Z",
                "message_summary": "Fix the problem of passing None to format() when val_loss is None",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "366": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-01-17T00:14:45Z",
                "message_summary": "Switch to Python logging (+ lint)",
                "update_details": "Python logging offers benefits such as logging timestamps, better cross-library compatibility, and the ability to add multiple output handlers.",
                "tasks": [
                    "Switch to Python logging"
                ]
            }
        ]
    },
    "367": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2020-01-16T09:59:15Z",
                "message_summary": "Split from PR#968. add --keep-best-checkpoints (#990)",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "368": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-12-17T01:22:11Z",
                "message_summary": "More fully deprecate --raw-text and --lazy-load (fixes #1488)"
            }
        ]
    },
    "369": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-12-06T02:24:56Z",
                "message_summary": "Add wrapper abstraction on top of fvcore PathManager to insulate OSS compatibility"
            }
        ]
    },
    "370": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-12-02T21:26:48Z",
                "message_summary": "Apply Black auto-formatting"
            }
        ]
    },
    "371": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-11-22T01:50:42Z",
                "message_summary": "Quick fix for Structured Dropout checkpointing (#1406)",
                "update_details": "This fix allows the user to checkpoint a translation model properly after applying layer pruning on a restored transformer file."
            }
        ]
    },
    "372": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-11-21T00:52:59Z",
                "message_summary": "Refactor data sharding to be specified via caller of task rather than task itself"
            }
        ]
    },
    "373": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-10-30T19:55:54Z",
                "message_summary": "layer drop",
                "update_details": "Enables layer drop in transformer decoder in production training pipeline (ptt_transformer). Additional logic added to handle corresponding dropping layers at test time in exported model."
            }
        ]
    },
    "374": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-10-27T19:10:53Z",
                "message_summary": "adding layerdrop code for training, pruning, and readme (#890)",
                "ptm_addition_details": null,
                "update_details": null,
                "replacement_info": null,
                "removal_details": null
            }
        ]
    },
    "375": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-10-12T04:53:11Z",
                "message_summary": "Added option to save checkpoints using Path Manager."
            }
        ]
    },
    "376": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-10-09T21:53:27Z",
                "message_summary": "Fix data loading memory issue in pyspeech",
                "update_details": "1. Modified data loading to shard data while reading the handles file to address memory issue. 2. Added synchronization method for data sharding at loading time. 3. Removed redundant loading of training dataset and batch iterator in fairspeq/train.py.",
                "tasks": [
                    "Fix data loading memory issue",
                    "Modify data loading for sharding",
                    "Add synchronization method for data sharding",
                    "Remove redundant loading in fairspeq/train.py"
                ]
            }
        ]
    },
    "377": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-09-20T16:34:58Z",
                "message_summary": "added multilingual masked LM training",
                "update_details": "The multilingual-RoBERTa training is working with aconneau XLM data. Two pieces remaining: 1) `XLM` limits batch to be from the same language, which can be implemented by adding `batch_by_size_and_language` function. 2) Handling `sample_ratio` in `ConcatDataset` by adjusting the ratio to the first decimal and multiplying by 10. Other options for handling this were discussed offline.",
                "tasks": [
                    "multilingual masked LM training"
                ]
            }
        ]
    },
    "378": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-08-21T20:43:01Z",
                "message_summary": "Parameterized criterions (#808)",
                "update_details": "Support criterion with parameters, such as AutoSegmentationCriterion (ASG) used in wav2letter which has a transition matrix parameter. This is needed to integrate wav2letter's ASG into PySpeech. Parameters in criterions will be updated by optimizers with a configurable learning rate, saved and loaded from checkpoints, preserving backward compatibility for criterions without parameters, and synchronized across nodes in distributed training."
            }
        ]
    },
    "379": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-08-12T16:08:16Z",
                "message_summary": "Lint"
            }
        ]
    },
    "380": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-08-12T14:17:21Z",
                "message_summary": "Update --restore-file logic (partially fixes #999)"
            }
        ]
    },
    "381": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-08-10T15:16:50Z",
                "message_summary": "Add WSC task and criterion"
            }
        ]
    },
    "382": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-08-01T12:55:57Z",
                "message_summary": "Update PyTorch Hub interface"
            }
        ]
    },
    "383": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-07-30T14:48:23Z",
                "message_summary": "Relicense fairseq under MIT license",
                "update_details": "Relicensing fairseq under the MIT license from the previous BSD+PATENTS license due to controversy."
            }
        ]
    },
    "384": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-07-29T23:06:26Z",
                "message_summary": "adding glue data preprocessing scripts",
                "update_details": "Added glue data pre-processing script. Updated README with usage.",
                "tasks": [
                    "releasing fairseq dictionary and remove hardcoded path",
                    "remove hard-coded path for bpe-encoding"
                ]
            }
        ]
    },
    "385": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-07-24T23:59:07Z",
                "message_summary": "check save_dir before beginning training",
                "update_details": "Adding a check at the beginning of the training loop to ensure the checkpoint directory is globally readable and prevent issues encountered after 8 hours of training."
            }
        ]
    },
    "386": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-07-19T20:13:43Z",
                "message_summary": "Rename _load_model_ensemble -> load_model_ensemble_and_task"
            }
        ]
    },
    "387": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-07-01T21:10:49Z",
                "message_summary": "Fixes checkpointing bug introduced in 89e077c"
            }
        ]
    },
    "388": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-06-30T18:35:33Z",
                "message_summary": "Add additional options for configuring writing of checkpoints"
            }
        ]
    },
    "389": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-06-11T18:19:58Z",
                "message_summary": "Automatically fill in default values from add_args"
            }
        ]
    },
    "390": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-05-30T18:41:40Z",
                "message_summary": "Add --reset-dataloader"
            }
        ]
    },
    "391": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-05-17T21:25:56Z",
                "message_summary": "Small features + lint"
            }
        ]
    },
    "392": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-05-17T04:03:08Z",
                "message_summary": "Clean up sharded train iterator"
            }
        ]
    },
    "393": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-05-14T19:57:12Z",
                "message_summary": "Move save/load checkpoint functions to utils",
                "update_details": "Move `load_checkpoint`, `save_checkpoint` and `reload_train` from train.py to checkpoint_utils.py\nMove `get_perplexity` from train.py to utils.py.\nThis will make train.py lighter and allow us to reuse all this utils functionality when fairseq is used as external library."
            }
        ]
    },
    "394": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-05-11T14:56:45Z",
                "message_summary": "Add missing options to TransformerDecoderLayer"
            }
        ]
    },
    "395": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-05-06T07:17:45Z",
                "message_summary": "Load pretrained encoder or decoder (#705)",
                "ptm_addition_details": null,
                "update_details": "Functionality added to load a pretrained encoder or decoder from another pretrained model into the current model."
            }
        ]
    },
    "396": {
        "repository": "github.com/imatge-upc/sign-topic",
        "text": [
            {
                "repo_url": "github.com/imatge-upc/sign-topic",
                "filepath": "fairseq/checkpoint_utils.py",
                "commit_date": "2019-04-30T02:50:58Z",
                "message_summary": "Merge internal changes (#654)",
                "update_details": "Add --add-bos-token option to LM task. Cleanup utils.py and options.py"
            }
        ]
    },
    "397": {
        "repository": "github.com/SillyTavern/SillyTavern-extras",
        "text": [
            {
                "repo_url": "github.com/SillyTavern/SillyTavern-extras",
                "filepath": "modules/voice_conversion/fairseq/checkpoint_utils.py",
                "commit_date": "2023-08-10T00:58:52Z",
                "message_summary": "Add monkey patched fairseq package to run on python 3.11 (what is needed for our use of RVC at least)"
            }
        ]
    },
    "398": {
        "repository": "github.com/UncleTensor/AudioSubnet",
        "text": [
            {
                "repo_url": "github.com/UncleTensor/AudioSubnet",
                "filepath": "fseq/fairseq/checkpoint_utils.py",
                "commit_date": "2024-02-12T17:35:20Z",
                "message_summary": "Added Text to Music models, custom models option and auto_update (yes/no)"
            }
        ]
    },
    "399": {
        "repository": "github.com/NLP2CT/kNN-TL",
        "text": [
            {
                "repo_url": "github.com/NLP2CT/kNN-TL",
                "filepath": "kNN-TL/fairseq/checkpoint_utils.py",
                "commit_date": "2023-07-04T08:51:56Z",
                "message_summary": "Added new files and modified existing files for project setup"
            }
        ]
    }
}