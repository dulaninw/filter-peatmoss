[
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2023-09-15T23:15:19Z",
        "message": "Keep task level checkpoint key name generic (#5330)"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2023-09-15T19:01:49Z",
        "message": "initial revision (#5328)"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2022-12-13T07:36:17Z",
        "message": "remove missing config entries when loading task from checkpoint (#4905)"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2022-12-12T16:53:56Z",
        "message": "data2vec v2.0 (#4903)\n\ndata2v2c 2.0\nCo-authored-by: Arun Babu <arbabu@fb.com>\nCo-authored-by: Wei-Ning Hsu <wnhsu@csail.mit.edu>"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2022-06-22T21:03:17Z",
        "message": "add check for OC version in fairseq\n\nSummary: fairseq patches a omegaconf internal util function that no longer exists in OmegaConf 2.2. This is a fix to make it compatible with both versions.\n\nReviewed By: dianaml0\n\nDifferential Revision: D37323720\n\nfbshipit-source-id: 1b15b86decc70776303afe4a9a4c63acfef27ffc"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2022-04-14T22:39:36Z",
        "message": "Fix typo in exception value (#4334)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes typo\n\n## PR review\n\n## Did you have fun?\n\nPull Request resolved: https://github.com/pytorch/fairseq/pull/4334\n\nReviewed By: Mortimerp9\n\nDifferential Revision: D35503972\n\nPulled By: dianaml0\n\nfbshipit-source-id: 09893de009d398e7a048ec89f757634ddc10139d"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2022-01-21T17:27:49Z",
        "message": "Fix breakage from D33649708\n\nSummary: https://www.internalfb.com/diff/D33649708 (https://github.com/pytorch/fairseq/commit/995c204337d16a6146a433cee360e5a5bfbc9a6f)?src_version_fbid=1030479880843010&dst_version_fbid=247617347518523&transaction_fbid=1601081576900014\n\nReviewed By: alexeib\n\nDifferential Revision: D33696937\n\nfbshipit-source-id: 9a17610e3f4eb3dd2b2131a3f9fb42732a31b47f"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2022-01-20T08:02:16Z",
        "message": "Data2vec prelim (#2929)\n\nSummary:\nPreliminaries for data2vec release, include some minor improvements and bug fixes\n\nMost important change is that we now default to raising an exception when fields in config do not have a corresponding field in the model dataclass\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2929\n\nReviewed By: wnhsu\n\nDifferential Revision: D33649708\n\nPulled By: alexeib\n\nfbshipit-source-id: 629bdb4c361550740b451c570c2005bb956c6fcb"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2022-01-19T03:29:52Z",
        "message": "Decode using EMA model in IPL recipe\n\nSummary: Add option to use the EMA model for decoding in transducer IPL recipe by passing --ipl-decode-ema. Note EMA should be enabled as in the diff D24238379 (https://github.com/pytorch/fairseq/commit/8feccf94412424a4683b01090de36fa77cb4951d) using options --store-ema --ema-start-update and --ema-decay.\n\nReviewed By: cruvadom\n\nDifferential Revision: D31983366\n\nfbshipit-source-id: 2bf63b3f7d1b5fa8804b3a7e9bfab71a463ca957"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2022-01-07T08:39:11Z",
        "message": "Formatting fix: get CI green (#2860)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nApplies `black` and `isort` to files\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2860\n\nReviewed By: Mortimerp9\n\nDifferential Revision: D33456637\n\nPulled By: dianaml0\n\nfbshipit-source-id: 560b8d3a8f589cbecc92d0d21163596b5d47d609"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-12-31T02:38:12Z",
        "message": "add strict option to checkpoint_utils. load_pretrained_component_from_model()\n\nSummary: Add strict option to checkpoint_utils. load_pretrained_component_from_model()\n\nReviewed By: sravyapopuri388\n\nDifferential Revision: D33304224\n\nfbshipit-source-id: 2284a21dfea7810ec212f15daadeeeb45c6dca1b"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-12-17T00:11:19Z",
        "message": "formatting fix (#2816)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nfix `black` failures\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2816\n\nReviewed By: alexeib\n\nDifferential Revision: D33172615\n\nPulled By: dianaml0\n\nfbshipit-source-id: 36b141f42941670f1bfa981041d878042feb0428"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-12-11T00:55:12Z",
        "message": "Add loading from HuggingFace Hub\n\nSummary: Add loading from HuggingFace Hub. Revised from and to replace D32697723 (accepted).\n\nReviewed By: pipibjc, dianaml0\n\nDifferential Revision: D32964041\n\nfbshipit-source-id: 39676aa0ecb10454ae76b70968d5abe96ab6da54"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-11-29T20:32:59Z",
        "message": "Add linting with black (#2678)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2678\n\nReviewed By: Mortimerp9\n\nDifferential Revision: D32653381\n\nPulled By: dianaml0\n\nfbshipit-source-id: 2810d14867cd7d64f4d340740e2b590b82de47fe"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-11-19T00:38:31Z",
        "message": "Merge --use-ontology-for-* into --use-ontology\n\nSummary:\nThere are three options for the ontology:\n* `--use-ontology-for-training`\n* `--use-ontology-for-validation`\n* `--use-ontology-for-balancing`\n\nThe first two must always be set together.\n\nIn the past, I observed that it's best not to use ontology for data balancing even if we use ontology for training and validation. But now I no longer observe this.\n\nTherefore, I'm merging all these three options into one (`--use-ontology`).\n\nIn addition, I'm also moving the logic of avoiding loading teacher models out of `checkpoint_utils.py`. If you want to load a student model without loading its teachers (e.g. for prediction only), specify `arg_overrides={\"ignore_teachers\": True}` when calling `load_model_ensemble`.\n\nReviewed By: xiaoxiao26\n\nDifferential Revision: D32518830\n\nfbshipit-source-id: 103c6458f7927ec5ca7470109c8f956c00f514a2"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-10-23T02:51:09Z",
        "message": "set num_update before loading state dict (#2491)\n\nSummary:\n## What does this PR do?\nSet `model.num_updates` in `load_model_ensemble_and_task()` before loading `state_dict`, like what's done in `fairseq/trainer.py`, because a model's `state_dict` may depend on `num_update`.\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2491\n\nReviewed By: xuqiantong\n\nDifferential Revision: D31863368\n\nPulled By: wnhsu\n\nfbshipit-source-id: c70051f898819cc43b02c9f5765429e9f194aed5"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-09-13T20:20:35Z",
        "message": "Back out \"Fairseq needs to store and load metadata from model state_dict\" (#3861)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/fairseq/pull/3861\n\nbackout fairseq changes. fix with a suggested, more optimal changes in checkopint utils.\n\nReviewed By: zhengwy888\n\nDifferential Revision: D30886481\n\nfbshipit-source-id: 12b6dd4d5107ab4371b73a58d9a044a17c733260"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-09-09T01:18:30Z",
        "message": "Fairseq needs to store and load metadata from model state_dict\n\nSummary:\n## TL;DR\nFairseq checkpoint saving and loading should mirror torch's checkpoint by saving and loading \"state_dict()._metadata\".\n\n## Long Story:\n\n#### What happened:\nDuring model loading and saving, Quantization-aware-training models in Pytorch encounters a weird bug that says state_dict \"fake_weight_quant.weight.min_val\" is mismatched to \"min_vals\".\n\n#### What was the reason:\n- We found the issue in that torch uses state_dict()._metadata to store module._version, but the metadata was never store in checkpoint, nor are they loaded during checkpoint loading in fairseq.\n\nReviewed By: frankseide\n\nDifferential Revision: D30649933\n\nfbshipit-source-id: ce262486b9b95fbcece463fa05c4e1903d4232d7"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-09-01T19:29:51Z",
        "message": "EMA\n\nSummary:\nAdds Exponential moving average (EMA) model for Kaizen semi-supervised training https://arxiv.org/abs/2106.07759\n\n1. Add `ema.store_ema` to enable storing EMA. EMA will be written to extra_state in the state dict while saving checkpoint.\n2. `ema.ema_start_update` to control when the EMA starts accumulating\n3. Tasks can use `uses_ema` property to decide if the EMA should be passed to the task. (Default is False)\n4. `load_ema_from_checkpoint` can be used to load EMA model in place of the model to be used for evalutation. Pyspeech has eval-ema option for this.\n\n```\nThis module has the EMA class used to store a copy of the exponentially decayed\nmodel params.\n\nTypical usage of EMA class involves initializing an object using an existing\nmodel (random or from a seed model) and setting the config like ema_decay,\nema_start_update which determine how the EMA model is updated. After every\nupdate of the model i.e. at the end of the train_step, the EMA should be updated\nby passing the new model to the EMA.step function. The EMA model state dict\ncan be stored in the extra state under the key of \"ema\" and dumped\ninto a checkpoint and loaded. The EMA object can be passed to tasks\nby setting task.uses_ema property.\nEMA is a smoothed/ensemble model which might have better performance\nwhen used for inference or further fine-tuning. EMA class has a\nreverse function to load the EMA params into a model and use it\nlike a regular model.\n```\n\nReviewed By: cruvadom\n\nDifferential Revision: D24238379\n\nfbshipit-source-id: 879d3ba5070a614b7d365f9503af357001e875b2"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-08-02T21:36:32Z",
        "message": "fixing checkpoint config upgrade for generation print_alignment (#2125)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes config upgrade conditions for upgrading generation. print_alignment\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2125\n\nReviewed By: myleott\n\nDifferential Revision: D30049140\n\nPulled By: jingfeidu\n\nfbshipit-source-id: e613821e94d0cdb876c35bc6e3fede7affbf4628"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-29T23:30:41Z",
        "message": "seed random suffix in checkpoint to be consistent across shards\n\nSummary:\nCurrently the random suffix for saving sharded checkpoints can be different for each shard when training with FSDP and use_sharded_state=True. This makes it difficult for downstream applications to load the checkpoints properly, since each shard may have different suffixes.\n\nThis diff seeds the random suffix to be consistent across shards\n\nReviewed By: zhengwy888\n\nDifferential Revision: D29951167\n\nfbshipit-source-id: 65749357e62a28978f3b46b71767204a508e1f61"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-29T23:30:41Z",
        "message": "use pathmanager to delete old checkpoints\n\nSummary: --keep-best-checkpoints doesn't work for Manifold paths because the current code assumes normal paths. Lets use  PathManager to properly remove them.\n\nReviewed By: myleott, sshleifer\n\nDifferential Revision: D29947965\n\nfbshipit-source-id: 237a7b5aaa8293bb203ad05937decdf3b3ae2fc0"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-29T23:30:40Z",
        "message": "use suffix when saving best checkpoints with metric\n\nSummary: This is needed when training with FSDP + sharded state, as the checkpoints should have `-shard0.pt`\n\nReviewed By: sshleifer\n\nDifferential Revision: D29947728\n\nfbshipit-source-id: d2cb7c23e1e6d027d115cb734e2f2f1c34239eba"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-09T23:25:02Z",
        "message": "Roll back os.path.abspath change\n\nReviewed By: donhusa\n\nDifferential Revision: D29641968\n\nfbshipit-source-id: eca379158055f3e38e9c053b06db56842265e53a"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-08T22:27:57Z",
        "message": "Fix static container (#2036)\n\nSummary:\nfixes StatefulContainer being static which is a problem when you load a checkpoint with task that already has the same keys in the container\n\nalso print full path to checkpoints when saving (useful with hydra) and crash if repeatedly failing to save a checkpoint\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2036\n\nReviewed By: arbabu123\n\nDifferential Revision: D29608430\n\nPulled By: alexeib\n\nfbshipit-source-id: 1b65c8f839e02de9110af3ec53f1e7d48a4908f7"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-06T22:07:31Z",
        "message": "fixes tests/test_train.py to mock checkpoint.save_dir config node (#3675)\n\nSummary:\n## What does this PR do?\nSome downstream users reported that errors when passing Namespace to load_checkpoint().\n\nA recent change made the assumption that the passed object is dict like (dict or DictConfig) that have a get function.\nThis changes that and make sure the mocked config have checkpoint.save_dir to allow the test to run.\n\nPull Request resolved: https://github.com/pytorch/fairseq/pull/3675\n\nReviewed By: omry\n\nDifferential Revision: D29564805\n\nPulled By: lematt1991\n\nfbshipit-source-id: 89308811da382667f6c5d3152ee2d6480416ee62"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-01T13:37:47Z",
        "message": "Hydra 1.1 compatibility: Use an explicit schema for the primary config (#3659)\n\nSummary:\n## What does this PR do?\nFixes compatibility with Hydra 1.1.\nThe result is compatible with both Hydra 1.0 and Hydra 1.1, and will allow a smoother migration to Hydra 1.1.\n\nAt this point I am not yet removing the restriction on the Hydra version from setup.py:\n1. It depends on some Hydra 1.1 changes that are not yet released (It will be compatible with 1.1.1).\n2. Upgrading will result in deprecation warnings, and fixing them will break compatibility with Hydra 1.0.\n\nThere will be some followup to make the code fully compatible with 1.1 once Hydra 1.1 is the default version in fbcode.\n\nPull Request resolved: https://github.com/pytorch/fairseq/pull/3659\n\nReviewed By: omry\n\nDifferential Revision: D29498036\n\nPulled By: lematt1991\n\nfbshipit-source-id: 96999cde5daad6749ef4d3ddf6a36a1e984ff201"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-06-15T21:09:52Z",
        "message": "Check attributes in trainer and checkpoint loading before using them (#1970)\n\nSummary:\n## What does this PR do?\nFixes None exception when some attributes in  don't exist in cfg.\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1970\n\nReviewed By: alexeib\n\nDifferential Revision: D29140036\n\nPulled By: hikushalhere\n\nfbshipit-source-id: 7d941bcae6bb000c281a43ca2cd0876a49912ab9"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-06-04T23:19:56Z",
        "message": "Fix loading some TALNet models\n\nSummary:\nD28728718 cleaned up the \"kd_binary_cross_entropy\" criterion, but this caused loading old models trained with this criterion to fail.\nThis diff replaces the \"kd_binary_cross_entropy\" criterion with the \"wav2vec\" criterion when loading models, and fixes this error.\n\nIt also removes the \"log_keys\" argument if it's `None`. Some criteria (e.g. wav2vec) require this argument to be a list, and will supply a default value of `[]` when it's absent. The presence of the `None` value prevents the use of this default value and causes an error.\n\nDifferential Revision: D28901263\n\nfbshipit-source-id: 9b33aed35e76d2c734d1d4e2cbca1ff193a8c920"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-06-04T00:49:12Z",
        "message": "Teacher-student learning for TALNet\n\nSummary:\nThis diff implements teacher-student learning for TALNet.\n\nThree classes take part in the teacher-student learning:\n* The task loads the teacher models;\n* The model generates predictions using the teacher models, and mixes them with the original targets;\n* The `Wav2VecCriterion` reads the mixed targets to compute the loss. However, it still uses the original targets to compute the MAP and MAUC metrics.\n\nThere are two types of teachers:\n* Static teachers: a file that stores predictions on training data which have been produced by running a model offline;\n* Dynamic teachers: model files that are loaded at the beginning of training and executed on the fly to produce predictions.\n\nWe actually no longer use static teachers. The code about static teachers are copied over from the `KnowledgeDistillationBinaryCrossEntropyCriterion` class. This class will be cleaned up in D28728718.\n\nThe teacher models are stored in the task object, and will not be saved into checkpoints.\n\nReviewed By: alexeib\n\nDifferential Revision: D28728707\n\nfbshipit-source-id: 0fcfc00db2e7194a6f7ee687cad9fa72e82a028b"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-05-26T00:45:51Z",
        "message": "support FSDP sharded_state checkpoint loading during inference\n\nSummary:\nusing the very useful feature added by QuentinDuval https://github.com/facebookresearch/fairscale/pull/683/files , we can consolidate sharded states into a full regular states. this allows inferences on sharded state almost transparently.\n\nThe main complexity comes from trying to be smart about what kind of checkpoint the user wants to load. not sure if this is over-engineering\n1. if the file checkpoint-shard0.pt exists, and `--checkpoint-shard-count` is > 1, then we load sharded FSDP checkpoint\n2. if checkpoint-shard0.pt exists but --checkpoint-shard-count=1, we load consolidated FSDP checkpoint\n3. if checkpoint-shard0.pt does not exist, but --checkpoint-shard-count > 1, we load model parallel checkpoint\n4. otherwise we are loading a single, plain checkpoint.\n\nIn theory we could be even smarter and load shard0.pt to check how many more checkpoints are needed. this is not implemented, though it will save the user having to specify --checkpoint-shard-count.\n\nReviewed By: sshleifer\n\nDifferential Revision: D28563441\n\nfbshipit-source-id: dcafcaa7c9eaf5c9ff94f55c16bb3424c98dfa59"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-05-22T07:22:05Z",
        "message": "fixing s2t transformer and N-best checkpoint saving\n\nSummary:\n- fixing the default value for `encoder_freezing_updates` in s2t transformer\n- fixing N-best checkpoint saving: the previous implementation compares the new checkpoint with only the previous best one but not the previous N best ones. This leads to suboptimal results on N-best checkpoint averaging.\n\nReviewed By: jmp84\n\nDifferential Revision: D28546493\n\nfbshipit-source-id: 44ec6d5ab49347f392d71269c5dcfd154b00c11e"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-05-21T23:18:21Z",
        "message": "attempt to make non-sharded FSDP checkpoint behave like regular checkpoint\n\nSummary:\noverall just wondering if feature is desirable. if it is, the next diff which supports loading sharded checkpoint into a consolidated state dict cleaner.\n\na couple advantages\n1. allows resuming from other DDP trainers.\n2. allows resuming into other DDP trainers. or FSDP of a different configuration.\n3. none-sharded FSDP checkpoint can be loaded with regular load_model_ensemble_and_task()\n\nFor old training workflow that's not using `--use-sharded-state`, please rename the checkpoint to remove the \"-shard0\" for resuming training.\n\nReviewed By: sshleifer\n\nDifferential Revision: D28563032\n\nfbshipit-source-id: ced72bed969319ab6306059721f56e29b2c3d892"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-04-14T08:50:07Z",
        "message": "add keep_interval_updates_pattern\n\nSummary:\nMotivation:\n\nI want to save checkpoints frequently, due to unreliable jobs in FB cluster that restart frequently. I want to do this without spamming Manifold storage, but still save some historical checkpoints (i.e. every 10k updates), so I can track how WER evolves over time.\n\nTo save frequently, I can use a small --save-interval-updates.\n\nTo delete old checkpoints to save storage, I can use --keep-interval-updates.\n\nHowever, this deletes all old checkpoints. This is where --keep-interval-updates-pattern comes in. If I now do:\n\n```\n--save-interval-updates 1000\n--keep-interval-updates 1\n--keep-interval-updates-pattern 10000\n```\n\nThis will:\n1. checkpoint every 1000 updates so that job restarts don't impact us significantly\n2. keep only the latest checkpoint to avoid saving a bunch of huge models in manifold\n3. make an exception for #2 for every 10k updates so we can track WER over time\n\nReviewed By: myleott\n\nDifferential Revision: D27578403\n\nfbshipit-source-id: 5aec2dc9a22778015f7a3daa017210190af81240"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-04-14T08:50:06Z",
        "message": "enable manifold checkpoints with --keep-interval-updates\n\nSummary: Useful to enable --keep-interval-updates with Manifold checkpoints\n\nReviewed By: myleott\n\nDifferential Revision: D27577116\n\nfbshipit-source-id: 6d4ae5aaccc07ecaed8ba6a333b6ab78b148187a"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-03-30T01:02:50Z",
        "message": "BASE layers (#1654)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1654\n\nReviewed By: myleott\n\nDifferential Revision: D27128074\n\nPulled By: shruti-bh\n\nfbshipit-source-id: ac86d383cd53c9c9bdd946fea839a37b719d95e3"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-03-26T14:18:59Z",
        "message": "FSDP uses new optimizer gathering to save optimizer state (#1744)\n\nSummary:\n- Full unflattened optimizer state dict is in `checkpoints/shard_0.pt`, other checkpoint files do not have the `last_optimizer_state` key.\n- requires master version of fairscale (eventually fairscale>=0.3.3)\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1744\n\nReviewed By: myleott\n\nDifferential Revision: D27342305\n\nPulled By: sshleifer\n\nfbshipit-source-id: 7442b8c6ed01599d8ab0050213e84051f4e98acd"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-03-04T21:32:44Z",
        "message": "Move checkpoint state_dict creation into Trainer (#1666)\n\nSummary:\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1666\n\nContext: the checkpoint saving call stack has become a bit convoluted:\n```\ntrain.py\n+ checkpoint_utils.save_checkpoint\n + trainer.save_checkpoint\n  + checkpoint_utils.save_state\n   + checkpoint_utils.torch_persistent_save\n```\n\nThis diff slightly simplifies the checkpoint saving logic by exposing a `state_dict` method inside the Trainer. This simplifies the call stack to:\n```\ntrain.py\n+ checkpoint_utils.save_checkpoint\n + trainer.save_checkpoint\n  + checkpoint_utils.torch_persistent_save\n```\n\nThis new structure is important for the FullyShardedDataParallel diff (next diff in the stack), since it enables the Trainer to save multiple checkpoints for the different optimizer state shards.\n\nTest Plan:\n- unit tests\n- trained WMT En-De models; confirmed checkpoints save/load properly, resuming from a checkpoint gives identical results\n- `buck test fblearner/flow/projects/langtech/translation:tests` (2 failures are in trunk too): https://www.internalfb.com/intern/testinfra/testconsole/testrun/2533274840914654/\n\nReviewed By: zhengwy888\n\nDifferential Revision: D26771146\n\nPulled By: myleott\n\nfbshipit-source-id: 10f91979cd42205c1d8abcaa9ab56f63eba31e93"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-03-04T05:17:29Z",
        "message": "minor fixes and improvements (#1671)\n\nSummary:\nthere are a few changes here:\n- convert config persisted in checkpoints into a plain dict when saving and back to omegaconf config when loading: this helps avoid compatibility issues between different versions of python, omegaconf, etc\n- update checkpoints that have old print_alignment saved\n- add lr_float to composite optimizer to enable sweeping on lr with auto sweepers like ax\n- fixing some edge cases for config loading\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1671\n\nReviewed By: myleott\n\nDifferential Revision: D26791583\n\nPulled By: alexeib\n\nfbshipit-source-id: 124dec74932052925c43b6a93130f4428803cb46"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-03-02T17:26:03Z",
        "message": "ioPath async - opt-in Fairseq integration (#1635)\n\nSummary:\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1635\n\n**Summary:** Integrate ioPath's async writes feature into Fairseq checkpoint writing.\n\n**Details:**\n- Created new checkpoint config param `--write-checkpoints-asynchronously` with default value `False`. Aliased to `--save-async`.\n- Added to `PathManager` class in `file_io.py` to include `PathManager.opena(...)` and `PathManager.async_close()`. These new methods use ioPath's async `PathManager`.\n\n**Usage:**\n```\npython train.py --save-async\n```\n---------\nNOTE: **QUESTIONS**\n1. In the current implementation, we don't save `checkpoint_best` and `checkpoint_latest` since ioPath doesn't yet have a \"wait until a file is written and then copy/move it to another path\" feature. Is this okay for now?\n2. Should I mimic the atomic vs non-atomic save structure that synchronous Fairseq checkpoint writes have?\n\n**Note to Eric:** Keep this integration in check with D26375501.\n\nReviewed By: myleott\n\nDifferential Revision: D26467815\n\nfbshipit-source-id: 50068ef7bf9a6d5cea4d5e0d13d672604dc4a6b0"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-02-11T22:00:25Z",
        "message": "save task state in the checkpoint (#1562)\n\nSummary:\nthis allows tasks to declare some properties they'd like to save in the checkpoint (such as a dictionary), which are loaded when checkpoint is restored.\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1562\n\nTest Plan: tested by training a new wav2vec model, then finetuning it, then decoding it and making sure the dict only loaded once, during fine tuning process (and was obtained from checkpoint for decoding)\n\nReviewed By: myleott, gwenzek\n\nDifferential Revision: D25937974\n\nPulled By: alexeib\n\nfbshipit-source-id: b9908042f76ec8cda943f33885eb9b1f121662ae"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-02-02T23:51:11Z",
        "message": "Fix the task data arg conversion to string.\n\nSummary: We were getting some test failures on our end due to incompatibility of task data argument type. The actual exception is defined in this task: T83395097 and T83395052. Fixing the task data arg to be a string instead of list of strings.\n\nReviewed By: myleott\n\nDifferential Revision: D26205482\n\nfbshipit-source-id: d29d1ee7c469177e8bdad7ca603938f8450bd81c"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-31T04:16:56Z",
        "message": "Fix incorrect local cache for checkpoint_last.pt when training is restarted on the same host\n\nReviewed By: myleott\n\nDifferential Revision: D25719057\n\nfbshipit-source-id: 2bf7dd93b6d223804da0326a0ed347e5e353f1f0"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-23T19:16:56Z",
        "message": "fairseq checkpoint improvements\n\nReviewed By: myleott\n\nDifferential Revision: D25677238\n\nfbshipit-source-id: b43075034c953491211f19a5464148de4758df83"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-18T19:45:08Z",
        "message": "Refactor eval_lm to support library usage (#1513)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1513\n\nTest Plan: Imported from OSS\n\nReviewed By: alexeib\n\nDifferential Revision: D25570467\n\nPulled By: myleott\n\nfbshipit-source-id: 062f748e287797f4f01c605e0b544ef3e698851f"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-18T15:40:49Z",
        "message": "Support atomic saves for checkpoints (#1520)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1520\n\nTest Plan: Imported from OSS\n\nReviewed By: stephenroller\n\nDifferential Revision: D25632782\n\nPulled By: myleott\n\nfbshipit-source-id: bdbe2aed6254d0b023b33f8027dfbd939f1fd271"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-16T01:47:42Z",
        "message": "Fix loading of very old checkpoints (#1512)\n\nSummary:\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1512\n\nSee https://github.com/pytorch/fairseq/issues/3032 for context\n\nTest Plan: Imported from OSS\n\nReviewed By: alexeib\n\nDifferential Revision: D25570470\n\nPulled By: myleott\n\nfbshipit-source-id: 9227b1ca36cd81ff72acdb5e03fd574e3e8769be"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-08T23:49:12Z",
        "message": "fix wav2vec scripts (#1494)\n\nSummary:\nfixes #2942\n+ docs + migration of old models\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1494\n\nReviewed By: myleott\n\nDifferential Revision: D25404601\n\nPulled By: alexeib\n\nfbshipit-source-id: 092f145602522f8e7ea3eaa709bfe602a4d29d8b"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-05T15:37:51Z",
        "message": "Rename optimization.min_lr -> optimization.stop_min_lr (#1486)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1486\n\nTest Plan: Imported from OSS\n\nReviewed By: alexeib\n\nDifferential Revision: D25342181\n\nPulled By: myleott\n\nfbshipit-source-id: 7d1cfb26334fff26d688648724ab073e5fb956f5"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-11-30T22:20:36Z",
        "message": "Add/fix tests (#1468)\n\nSummary:\n- add test for loading ensemble checkpoints (and confirmed it fails if I revert: https://github.com/pytorch/fairseq/commit/265791b727b664d4d7da3abd918a3f6fb70d7337)\n- add test for LayerDrop (and fix it)\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1468\n\nReviewed By: alexeib\n\nDifferential Revision: D25223272\n\nPulled By: myleott\n\nfbshipit-source-id: 3f06f753605af251567c70d2961f5506ea423499"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-11-18T01:08:13Z",
        "message": "fix loading ensembles (#1442)\n\nSummary:\nfixes loading ensembles. previous change used the state of the first model for all models in the ensemble\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1442\n\nReviewed By: chtran\n\nDifferential Revision: D25035706\n\nPulled By: alexeib\n\nfbshipit-source-id: 9029999be0f1703efb1df20bec2890de59449f1f"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-11-11T18:15:10Z",
        "message": "load dataset with saved task config (optionally) (#1423)\n\nSummary:\nthis adds an argument to load_dataset that provides task configuration from the checkpoint. different tasks can decide what to do with it afterwards.\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1423\n\nReviewed By: myleott\n\nDifferential Revision: D24875706\n\nPulled By: alexeib\n\nfbshipit-source-id: 5bb1e2b7495520c456024dc7b0751b65cb05b473"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-11-09T23:46:00Z",
        "message": "migrate wav2vec2 model (#1409)\n\nSummary:\nsee title\nalso includes some minor bug fixes\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1409\n\nReviewed By: myleott\n\nDifferential Revision: D24822219\n\nPulled By: alexeib\n\nfbshipit-source-id: b18f9a8af42ced37880c23dd6ad1ec4df3dfc040"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-11-05T23:29:33Z",
        "message": "Move TPU grad reductions out of Trainer into TPUDistributedDataParallel (#1397)\n\nSummary:\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1397\n\nData parallel command: `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --share-decoder-input-output-embed --lr 0.0001`\n\nData parallel before:\n```\n2020-11-04 08:20:13 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)\n2020-11-04 08:20:13 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8\n2020-11-04 08:20:13 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt\n2020-11-04 08:20:13 | INFO | fairseq.trainer | loading train data for epoch 1\n2020-11-04 08:20:14 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train\n2020-11-04 08:20:14 | INFO | fairseq.optim.adam | using FusedAdam\n2020-11-04 08:20:14 | INFO | fairseq.trainer | begin training epoch 1\n2020-11-04 08:20:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=5\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108002, wps=160870, ups=4.91, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.8, wps=517232, ups=15.77, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.4, wps=537322, ups=16.38, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492.1, wps=540488, ups=16.48, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603.2, wps=543411, ups=16.57, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=540359, ups=16.47, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:20 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=533446, ups=16.26, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:20 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.4, wps=539734, ups=16.46, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=6\n```\n\nData parallel after:\n```\n2020-11-04 08:14:02 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)\n2020-11-04 08:14:02 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8\n2020-11-04 08:14:02 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt\n2020-11-04 08:14:02 | INFO | fairseq.trainer | loading train data for epoch 1\n2020-11-04 08:14:03 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train\n2020-11-04 08:14:03 | INFO | fairseq.optim.adam | using FusedAdam\n2020-11-04 08:14:03 | INFO | fairseq.trainer | begin training epoch 1\n2020-11-04 08:14:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108002, wps=157099, ups=4.79, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.8, wps=560049, ups=17.08, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.4, wps=558507, ups=17.03, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492.1, wps=514194, ups=15.68, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603.2, wps=552676, ups=16.85, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:09 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=546402, ups=16.66, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:09 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=508472, ups=15.5, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:09 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.4, wps=552493, ups=16.84, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=6\n```\n\nData parallel command (no_c10d): `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --share-decoder-input-output-embed --lr 0.0001 --dp-backend no_c10d`\n\nData parallel before:\n```\n2020-11-04 08:19:25 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)\n2020-11-04 08:19:25 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8\n2020-11-04 08:19:25 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt\n2020-11-04 08:19:25 | INFO | fairseq.trainer | loading train data for epoch 1\n2020-11-04 08:19:25 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train\n2020-11-04 08:19:26 | INFO | fairseq.optim.adam | using FusedAdam\n2020-11-04 08:19:26 | INFO | fairseq.trainer | begin training epoch 1\n2020-11-04 08:19:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n2020-11-04 08:19:31 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108001, wps=141659, ups=4.32, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.9, wps=503762, ups=15.36, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.5, wps=488599, ups=14.9, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492, wps=507855, ups=15.48, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603, wps=503270, ups=15.34, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=7\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=467778, ups=14.26, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=7\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=503800, ups=15.36, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=7\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.3, wps=468486, ups=14.28, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=7\n```\n\nData parallel after:\n```\n2020-11-04 08:14:50 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)\n2020-11-04 08:14:50 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8\n2020-11-04 08:14:50 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt\n2020-11-04 08:14:50 | INFO | fairseq.trainer | loading train data for epoch 1\n2020-11-04 08:14:50 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train\n2020-11-04 08:14:51 | INFO | fairseq.optim.adam | using FusedAdam\n2020-11-04 08:14:51 | INFO | fairseq.trainer | begin training epoch 1\n2020-11-04 08:14:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108001, wps=137677, ups=4.2, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.9, wps=519541, ups=15.84, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.5, wps=517063, ups=15.76, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492, wps=490728, ups=14.95, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603, wps=505262, ups=15.41, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=508874, ups=15.52, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:57 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=518028, ups=15.79, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:57 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.3, wps=515996, ups=15.73, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=7\n```\n\nModel parallel command: `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm_megatron --decoder-layers 4 --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --model-parallel-size 2 --share-decoder-input-output-embed --lr 0.0001`\n\nModel parallel before:\n```\n2020-11-04 08:18:38 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)\n2020-11-04 08:18:38 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8\n2020-11-04 08:18:38 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last-model_part-0.pt\n2020-11-04 08:18:38 | INFO | fairseq.trainer | loading train data for epoch 1\n2020-11-04 08:18:38 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train\n2020-11-04 08:18:39 | INFO | fairseq.optim.adam | using FusedAdam\n2020-11-04 08:18:39 | INFO | fairseq.trainer | begin training epoch 1\n2020-11-04 08:18:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n2020-11-04 08:18:45 | INFO | train_inner | epoch 001:      2 / 7173 loss=55.997, ppl=7.19017e+16, wps=0, ups=0, wpb=16384, bsz=32, num_updates=1, lr=0.0001, gnorm=14.03, loss_scale=64, train_wall=1, wall=7\n2020-11-04 08:18:45 | INFO | train_inner | epoch 001:      3 / 7173 loss=28.372, ppl=3.47501e+08, wps=48371.7, ups=2.95, wpb=16384, bsz=32, num_updates=2, lr=0.0001, gnorm=15.339, loss_scale=64, train_wall=0, wall=8\n2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      4 / 7173 loss=15.855, ppl=59276.8, wps=72422.5, ups=4.42, wpb=16384, bsz=32, num_updates=3, lr=0.0001, gnorm=4.189, loss_scale=64, train_wall=0, wall=8\n2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      5 / 7173 loss=14.713, ppl=26858.7, wps=72933.5, ups=4.45, wpb=16384, bsz=32, num_updates=4, lr=0.0001, gnorm=4.751, loss_scale=64, train_wall=0, wall=8\n2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      6 / 7173 loss=13.901, ppl=15299.7, wps=71974.8, ups=4.39, wpb=16384, bsz=32, num_updates=5, lr=0.0001, gnorm=4.361, loss_scale=64, train_wall=0, wall=8\n2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      7 / 7173 loss=13.312, ppl=10169.5, wps=72897.8, ups=4.45, wpb=16384, bsz=32, num_updates=6, lr=0.0001, gnorm=3.307, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:18:47 | INFO | train_inner | epoch 001:      8 / 7173 loss=12.914, ppl=7720.21, wps=73044.6, ups=4.46, wpb=16384, bsz=32, num_updates=7, lr=0.0001, gnorm=5.473, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:18:47 | INFO | train_inner | epoch 001:      9 / 7173 loss=12.56, ppl=6036.72, wps=73453.1, ups=4.48, wpb=16384, bsz=32, num_updates=8, lr=0.0001, gnorm=6.112, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:18:47 | INFO | train_inner | epoch 001:     10 / 7173 loss=12.116, ppl=4437.77, wps=73442.6, ups=4.48, wpb=16384, bsz=32, num_updates=9, lr=0.0001, gnorm=4.415, loss_scale=64, train_wall=0, wall=9\n```\n\nModel parallel after:\n```\n2020-11-04 08:12:09 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)\n2020-11-04 08:12:09 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8\n2020-11-04 08:12:09 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last-model_part-0.pt\n2020-11-04 08:12:09 | INFO | fairseq.trainer | loading train data for epoch 1\n2020-11-04 08:12:09 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train\n2020-11-04 08:12:10 | INFO | fairseq.optim.adam | using FusedAdam\n2020-11-04 08:12:10 | INFO | fairseq.trainer | begin training epoch 1\n2020-11-04 08:12:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      2 / 7173 loss=55.997, ppl=7.19017e+16, wps=0, ups=0, wpb=16384, bsz=32, num_updates=1, lr=0.0001, gnorm=14.03, loss_scale=64, train_wall=1, wall=8\n2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      3 / 7173 loss=28.372, ppl=3.47501e+08, wps=53097, ups=3.24, wpb=16384, bsz=32, num_updates=2, lr=0.0001, gnorm=15.339, loss_scale=64, train_wall=0, wall=8\n2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      4 / 7173 loss=15.855, ppl=59276.8, wps=72355.5, ups=4.42, wpb=16384, bsz=32, num_updates=3, lr=0.0001, gnorm=4.189, loss_scale=64, train_wall=0, wall=8\n2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      5 / 7173 loss=14.713, ppl=26858.7, wps=70526.4, ups=4.3, wpb=16384, bsz=32, num_updates=4, lr=0.0001, gnorm=4.751, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      6 / 7173 loss=13.901, ppl=15299.7, wps=73063.5, ups=4.46, wpb=16384, bsz=32, num_updates=5, lr=0.0001, gnorm=4.361, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      7 / 7173 loss=13.312, ppl=10169.5, wps=73559.4, ups=4.49, wpb=16384, bsz=32, num_updates=6, lr=0.0001, gnorm=3.307, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      8 / 7173 loss=12.914, ppl=7720.21, wps=72693.2, ups=4.44, wpb=16384, bsz=32, num_updates=7, lr=0.0001, gnorm=5.473, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      9 / 7173 loss=12.56, ppl=6036.72, wps=73531.2, ups=4.49, wpb=16384, bsz=32, num_updates=8, lr=0.0001, gnorm=6.112, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:12:19 | INFO | train_inner | epoch 001:     10 / 7173 loss=12.116, ppl=4437.77, wps=73187.6, ups=4.47, wpb=16384, bsz=32, num_updates=9, lr=0.0001, gnorm=4.415, loss_scale=64, train_wall=0, wall=10\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D24729295\n\nPulled By: myleott\n\nfbshipit-source-id: beee8bdece3eaa0419a2e813990420411e507c75"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-10-23T07:07:33Z",
        "message": "refactor dataclass related files, add proper types for static checkin\u2026 (#1371)\n\nSummary:\n- refactor dataclass/ hierarchy to make it a bit more sane (while avoiding circular references)\n- add top level FairseqConfig\n- change typehints to reflect the correct config type if it is known\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1371\n\nReviewed By: myleott\n\nDifferential Revision: D24469026\n\nPulled By: alexeib\n\nfbshipit-source-id: 01f68918f761d51ec5216286b8959ad35f41a7b2"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-10-22T23:31:49Z",
        "message": "rename remove_bpe to post_process; add aliasing (#1369)\n\nSummary:\nsome binaries (e.g. speech based ones) used --post-process, some used --remove-bpe. --post-process seems more appropriate as it does more than just remove bpe at the moment. this renames remove_bpe to post_process, adds alias so existing command lines would work and adds checkpoint upgrades so they continue to work also.\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1369\n\nReviewed By: myleott\n\nDifferential Revision: D24465040\n\nPulled By: alexeib\n\nfbshipit-source-id: 1b3e388291ccc403e76e069ef6606b80ead863a7"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-10-20T07:32:26Z",
        "message": "Enable Hydra configs in fairseq (#1343) (#1510)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/1510\n\nthis is the main pr that switches on hydra functionality in fairseq\n\nwe migrate \"args\" object into omegaconf \"DictConfig\" at all legacy entry points\n\nin addition this migrates various components from secondary registries (like bpe encoders and tokenizers) to make the migration smoother\n\ni am going through code that references migrated fairseq components and changing it to inherit from \"Legacy*\" components instead. hopefully tests will catch most of this\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1343\n\nReviewed By: myleott\n\nDifferential Revision: D23973928\n\nPulled By: alexeib\n\nfbshipit-source-id: dd9554981fff51ea75c1ff343874d1d6e61793c9"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-10-19T01:14:51Z",
        "message": "Apply black+isort (#1357)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1357\n\nReviewed By: alexeib\n\nDifferential Revision: D24377772\n\nfbshipit-source-id: 51581af041d42d62166b33a35a1a4228b1a76f0c"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-10-12T17:55:40Z",
        "message": "Support generation with huge pipeline parallel Transformer models (#1297)\n\nSummary:\n## What is this PR about?\n* Support loading sharded checkpoints (loading and saving checkpoints without sharding for 30B models runs OOM)\n* Ability to provide a fixed dictionary for multilingual machine translation to match the dictionary used for training the checkpoint\n* Support generation with PipelineParallelTransformer models\n\n## Testing\n\n```\npython fairseq_cli/generate.py \\\n    /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted  \\\n    --batch-size 1 \\\n    --path /large_exp/angelafan/checkpoints/shru/mmmt_5b_checkpoints_for_master/checkpoint4.pt \\\n    -s en -t es --remove-bpe 'sentencepiece' --beam 5 \\\n    --task translation_multi_simple_epoch \\\n    --lang-pairs /private/home/angelafan/mmt/lang100_bt_pairs.txt \\\n    --decoder-langtok --encoder-langtok src --gen-subset valid --fp16 \\\n    --dataset-impl mmap \\\n    --distributed-world-size 1 --distributed-no-spawn \\\n    --pipeline-model-parallel \\\n    --pipeline-chunks 1 \\\n    --pipeline-encoder-balance '[26]' \\\n    --pipeline-encoder-devices '[0]' \\\n    --pipeline-decoder-balance '[26]' \\\n    --pipeline-decoder-devices '[0]' \\\n    --fixed-dictionary /private/home/shru/projects/fairseq-py-kakaoval-mmt-save-redist-gen/dict.100langs.txt\n2020-09-23 23:18:27 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might ca\nuse misalignment in pretraining and finetuning.\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['af', 'am', 'ar', 'arbt', 'ast', 'az', 'azbt', 'ba', 'be', 'bebt', 'bg', 'bgbt', 'bn', 'bnbt', 'br', 'bs', 'ca', 'ceb', 'cs', 'csb\nt', 'cy', 'da', 'de', 'debt', 'el', 'en', 'es', 'esbt', 'et', 'etbt', 'fa', 'fabt', 'ff', 'fi', 'fr', 'frbt', 'fy', 'ga', 'gd', 'gl', 'gu', 'ha', 'he', 'hebt', 'hi', 'hibt', 'hr', 'ht', 'hu', 'hubt', 'hy', 'hybt', 'id', 'ig', 'ilo', 'is',\n 'it', 'ja', 'jabt', 'jv', 'ka', 'kabt', 'kk', 'km', 'kn', 'ko', 'kobt', 'lb', 'lg', 'ln', 'lo', 'lt', 'lv', 'mg', 'mk', 'mkbt', 'ml', 'mn', 'mr', 'mrbt', 'ms', 'msbt', 'my', 'ne', 'nebt', 'nl', 'no', 'ns', 'oc', 'or', 'pa', 'pl', 'ps', '\npt', 'ro', 'robt', 'ru', 'sd', 'si', 'sk', 'sl', 'so', 'sq', 'sr', 'srbt', 'ss', 'su', 'sv', 'sw', 'ta', 'th', 'tl', 'tn', 'tr', 'trbt', 'uk', 'ur', 'uz', 'vi', 'vibt', 'wo', 'xh', 'yi', 'yo', 'zh', 'zhbt', 'zu']\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [en] dictionary: 128112 types\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [es] dictionary: 128112 types\n2020-09-23 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None\n2020-09-23 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:en-es': 1}\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:en-es src_langtok: 128022; tgt_langtok: 128023\n2020-09-23 23:18:27 | INFO | fairseq.data.data_utils | loaded 4231 examples from: /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted/valid.en-es.en\n2020-09-23 23:18:27 | INFO | fairseq.data.data_utils | loaded 4231 examples from: /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted/valid.en-es.es\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted valid en-es 4231 examples\n2020-09-23 23:18:27 | INFO | fairseq_cli.generate | loading model(s) from /large_exp/angelafan/checkpoints/mmmt_100_langs_new_dict_5b_model/checkpoint4.pt\nbalance=[29,22,1], devices=[0,1,0], chunks=8, checkpoint=always\n2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler order indices time: 0:00:00.004368\n2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler filter_by_size time: 0:00:00.057953\n2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler batch_by_size time: 0:00:01.158644\n2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:01.223290\n2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\nS-2521  __en__ No.\nT-2521  No.\nH-2521  -2.4406352043151855     y no.\nD-2521  -2.4406352043151855     y no.\nP-2521  -5.2677 -4.4289 -0.1922 -2.1578 -0.1565\nS-2261  __en__ Why?\nT-2261  \u00bfPor qu\u00e9?\nH-2261  -1.7077901363372803     \u00bfY por qu\u00e9?\nD-2261  -1.7077901363372803     \u00bfY por qu\u00e9?\nP-2261  -5.2733 -0.7970 -3.7643 -0.5474 -0.3339 -1.0629 -0.1757\n```\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1297\n\nReviewed By: myleott, msbaines\n\nDifferential Revision: D23991647\n\nPulled By: shruti-bh\n\nfbshipit-source-id: 81ea1af64cbe75c8b53e050d2c2339dcb70fe1eb"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-10-09T20:34:59Z",
        "message": "Improve dictionary & checkpoint reading w/ local caching\n\nReviewed By: myleott\n\nDifferential Revision: D24148700\n\nfbshipit-source-id: 666300639243688939e137be748f7b76fc3c21a6"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-09-02T01:17:33Z",
        "message": "Initial support for ZeRO optimizer state sharding (#1259)\n\nSummary:\nFairseqOSS will work with any optimizer and dtype.\n\nTODO(future PR):\n* support reduce instead of all_reduce\n* support gradient sharding\n* support parameter sharding\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1259\n\nTest Plan:\nVerified that checkpoint save and restore work.\n\nVerified that grad_norm, loss, and ppl are identical with and without\nsharding enable.\n\nBefore:\n\n$ fairseq-train --task language_modeling   data-bin/wikitext-103   --save-dir checkpoints/transformer_wikitext-103   --arch transformer_lm --share-decoder-input-output-embed   --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 512 --sample-break-mode none   --max-tokens 2048 --update-freq 16   --max-update 50000  --memory-efficient-fp16 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50\n...\n2020-08-27 22:24:51 | INFO | train_inner | epoch 001:     49 / 394 loss=18.84, ppl=469411, wps=269226, ups=1.03, wpb=262144, bsz=512, num_updates=45, lr=5.72388e-06, gnorm=5.769, loss_scale=8, train_wall=1, wall=68\n2020-08-27 22:24:52 | INFO | train_inner | epoch 001:     50 / 394 loss=18.787, ppl=452312, wps=256992, ups=0.98, wpb=262144, bsz=512, num_updates=46, lr=5.84885e-06, gnorm=5.512, loss_scale=8, train_wall=1, wall=69\n2020-08-27 22:24:53 | INFO | train_inner | epoch 001:     51 / 394 loss=18.74, ppl=437735, wps=259178, ups=0.99, wpb=262144, bsz=512, num_updates=47, lr=5.97383e-06, gnorm=5.298, loss_scale=8, train_wall=1, wall=70\n2020-08-27 22:24:54 | INFO | train_inner | epoch 001:     52 / 394 loss=18.683, ppl=420727, wps=257710, ups=0.98, wpb=262144, bsz=512, num_updates=48, lr=6.0988e-06, gnorm=5.094, loss_scale=8, train_wall=1, wall=71\n2020-08-27 22:24:55 | INFO | train_inner | epoch 001:     53 / 394 loss=18.623, ppl=403794, wps=269279, ups=1.03, wpb=262144, bsz=512, num_updates=49, lr=6.22378e-06, gnorm=4.893, loss_scale=8, train_wall=1, wall=72\n2020-08-27 22:24:56 | INFO | train_inner | epoch 001:     54 / 394 loss=18.574, ppl=390255, wps=264616, ups=1.01, wpb=262144, bsz=512, num_updates=50, lr=6.34875e-06, gnorm=4.684, loss_scale=8, train_wall=1, wall=73\n2020-08-27 22:24:56 | INFO | fairseq_cli.train | begin save checkpoint\n2020-08-27 22:24:56 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n2020-08-27 22:24:56 | INFO | train | epoch 001 | loss 19.736 | ppl 873122 | wps 264825 | ups 1.01 | wpb 262144 | bsz 512 | num_updates 50 | lr 6.34875e-06 | gnorm 8.898 | loss_scale 8 | train_wall 66 | wall 73\n2020-08-27 22:24:56 | INFO | fairseq_cli.train | done training in 72.2 seconds\n\nAfter:\n\n$ fairseq-train --task language_modeling   data-bin/wikitext-103   --save-dir checkpoints/transformer_wikitext-103   --arch transformer_lm --share-decoder-input-output-embed   --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 512 --sample-break-mode none   --max-tokens 2048 --update-freq 16   --max-update 50000  --memory-efficient-fp16 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --zero-sharding os\n...\n2020-08-27 22:22:55 | INFO | train_inner | epoch 001:     49 / 394 loss=18.84, ppl=469411, wps=267663, ups=1.02, wpb=262144, bsz=512, num_updates=45, lr=5.72388e-06, gnorm=5.769, loss_scale=8, train_wall=1, wall=68\n2020-08-27 22:22:56 | INFO | train_inner | epoch 001:     50 / 394 loss=18.787, ppl=452312, wps=252797, ups=0.96, wpb=262144, bsz=512, num_updates=46, lr=5.84885e-06, gnorm=5.512, loss_scale=8, train_wall=1, wall=69\n2020-08-27 22:22:57 | INFO | train_inner | epoch 001:     51 / 394 loss=18.74, ppl=437735, wps=267692, ups=1.02, wpb=262144, bsz=512, num_updates=47, lr=5.97383e-06, gnorm=5.298, loss_scale=8, train_wall=1, wall=70\n2020-08-27 22:22:58 | INFO | train_inner | epoch 001:     52 / 394 loss=18.683, ppl=420727, wps=267507, ups=1.02, wpb=262144, bsz=512, num_updates=48, lr=6.0988e-06, gnorm=5.094, loss_scale=8, train_wall=1, wall=71\n2020-08-27 22:22:59 | INFO | train_inner | epoch 001:     53 / 394 loss=18.623, ppl=403794, wps=254410, ups=0.97, wpb=262144, bsz=512, num_updates=49, lr=6.22378e-06, gnorm=4.893, loss_scale=8, train_wall=1, wall=72\n2020-08-27 22:23:00 | INFO | train_inner | epoch 001:     54 / 394 loss=18.574, ppl=390255, wps=268234, ups=1.02, wpb=262144, bsz=512, num_updates=50, lr=6.34875e-06, gnorm=4.684, loss_scale=8, train_wall=1, wall=73\n2020-08-27 22:23:00 | INFO | fairseq_cli.train | begin save checkpoint\n2020-08-27 22:23:00 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n2020-08-27 22:23:00 | INFO | train | epoch 001 | loss 19.736 | ppl 873122 | wps 263570 | ups 1.01 | wpb 262144 | bsz 512 | num_updates 50 | lr 6.34875e-06 | gnorm 8.898 | loss_scale 8 | train_wall 66 | wall 73\n2020-08-27 22:23:00 | INFO | fairseq_cli.train | done training in 72.3 seconds\n\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nReviewed By: myleott\n\nDifferential Revision: D23432082\n\nPulled By: msbaines\n\nfbshipit-source-id: 6a020b25e36a3d9283582b7d89a6a53038e5b181"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-08-14T17:24:51Z",
        "message": "Misc fixes (#2448)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2448\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D23011193\n\nPulled By: myleott\n\nfbshipit-source-id: 1a29481707108e4465aca78ec1581fb79f05efba"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-08-06T17:20:39Z",
        "message": "Multilingual v1: Multilingual Training with multiple bitext and monolingual datasets: add finetuning options\n\nSummary:\nA first version of XLNMT multilingual project code release: Multilingual Training with multiple bitext\n\n- Minor changes to\n    - fairseq/checkpoint_utils.py to add finetuning option instead of using restore_file which will restore from original model when being requeued.\n\nReviewed By: myleott\n\nDifferential Revision: D22483494\n\nfbshipit-source-id: 733300fd6a4d185e561c793ea668047c96f616c6"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-08-04T15:25:50Z",
        "message": "Fixes checkpoint_path while loading a model-parallel checkpoint (#2365)\n\nSummary:\nFixes https://github.com/pytorch/fairseq/issues/2351\n\nPull Request resolved: https://github.com/pytorch/fairseq/pull/2365\n\nReviewed By: pipibjc\n\nDifferential Revision: D22727384\n\nPulled By: myleott\n\nfbshipit-source-id: e2ff703181a6b8f10df9b4ee7aa3f9e128c04b4e"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-05-26T22:59:59Z",
        "message": "Several small fixes (incl. set default --data-buffer-size=10) (#2163)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2163\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D21665601\n\nPulled By: myleott\n\nfbshipit-source-id: 47673ff7f07acf0002c4e28380aa08ff917618ee"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-05-11T17:34:42Z",
        "message": "Generalize moving of tensors to CPU in checkpoints (#2098)\n\nSummary:\nThis is needed for TPUs\nPull Request resolved: https://github.com/pytorch/fairseq/pull/2098\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D21455095\n\nPulled By: myleott\n\nfbshipit-source-id: f0f88e715884f44bc254b71f7694ac90200d92fc"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-04-14T17:58:38Z",
        "message": "update checkpoint mkdir behavior (issue #1986) (#2011)\n\nSummary:\nCreate checkpoint directory in ```save_checkpoint()```instead of ```load_checkpoint()```.\n\nhttps://github.com/pytorch/fairseq/issues/1986\nPull Request resolved: https://github.com/pytorch/fairseq/pull/2011\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D21017208\n\nPulled By: myleott\n\nfbshipit-source-id: 4f76afc1751f987b8ab2c170ca306d07bd5ffe83"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-03-28T03:22:36Z",
        "message": "adding code to load and save model parallel checkpoint (#1119)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1119\n\nReviewed By: myleott\n\nDifferential Revision: D20712488\n\nfbshipit-source-id: 941ef251c9e2deb8933d88188fac56ee8c5be9b7"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-03-11T07:36:45Z",
        "message": "PySpeech TALNet: Convert to JIT and quantize\n\nSummary:\nUpdate the TALNet model so it can be converted to JIT format and quantized by the `jit_pyspeech_nn_model` tool.\n\nThe updated model structure is slight incompatible with the model files saved before (the model structure has the extra parameters `fc_att.weight` and `fc_att.bias`), so old model files must be loaded with `strict=False`. This diff also makes changes to allow `strict=False` where necessary.\n\nAlso renames the options of `jit_pyspeech_nn_model` to make them conform better to convention.\n\nReviewed By: jay-mahadeokar\n\nDifferential Revision: D20369643\n\nfbshipit-source-id: 0950a26abc20d0ae5929c8d0a03f83cab4a59200"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-03-07T17:12:14Z",
        "message": "Fix epoch reporting when restoring checkpoint (#1075)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1075\n\nDifferential Revision: D20322672\n\nPulled By: myleott\n\nfbshipit-source-id: 8845a10b10442bed77670b7740afa271123a3b5d"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-03-05T00:37:24Z",
        "message": "Use 1-based indexing for epochs everywhere (#1053)\n\nSummary:\nWe are somewhat inconsistent in whether we're using 0-based or 1-based indexing for epochs. This should fix things to be 0-based internally, with logging and checkpoint naming still using 1-based indexing.\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1053\n\nReviewed By: spencerp\n\nDifferential Revision: D20160715\n\nPulled By: myleott\n\nfbshipit-source-id: 4ed94f9c371e1bfe29bcfa087fa6756507d6e627"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-02-19T04:07:05Z",
        "message": "tts synthesis script\n\nSummary: add a synth.py to pyspeech to run tts synthesis for a particular text.\n\nDifferential Revision: D19786089\n\nfbshipit-source-id: 2aadd004609dfbcf477e58bc01e2b05df19c0e26"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-01-22T23:36:28Z",
        "message": "fblearner pyspeech manifold migration\n\nSummary:\nwas planning to migrate everything at once. but because pyspeech and tuna diverged from D19060980, it's too complicated to do the migration.\n\nmigrate save_dir to manifold in fblearner, and copy from manifold to gluster after training to keep downstream workflows happy.\n\nReviewed By: zdavid1995\n\nDifferential Revision: D19433205\n\nfbshipit-source-id: bd8d969c001952d85167a63f4d55fe97b93aceb5"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-01-20T20:15:27Z",
        "message": "fix the problem of passing None to format() when val_loss is None (e.\u2026 (#1633)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [x ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ x] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\nPull Request resolved: https://github.com/pytorch/fairseq/pull/1633\n\nDifferential Revision: D19470727\n\nPulled By: myleott\n\nfbshipit-source-id: a0bbefe20b4692306c57104f3e545912e20055e6"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-01-17T00:14:45Z",
        "message": "Switch to Python logging (+ lint) (#1627)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/fairseq/pull/1627\n\nPython logging offers a number of benefits, such as logging timestamps, better\ncross-library compatibility, ability to add multiple output handlers, etc.\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/646\n\nReviewed By: spencerp\n\nDifferential Revision: D15815620\n\nPulled By: myleott\n\nfbshipit-source-id: 5e64e9929b5e4b9dd5bb49bcdf7c510631907134"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-01-16T09:59:15Z",
        "message": "Split from PR#968. add --keep-best-checkpoints (#990)\n\nSummary:\nFixes https://github.com/fairinternal/fairseq-py/issues/968. Split --keep-best-checkpoints from the original request.\nUse scores as the names to save the checkpoints\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/990\n\nDifferential Revision: D19411250\n\nPulled By: MultiPath\n\nfbshipit-source-id: 82b0db614208eee54c9c0e470ad7faa6481747d5"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-12-17T01:22:11Z",
        "message": "More fully deprecate --raw-text and --lazy-load (fixes #1488)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/947\n\nDifferential Revision: D19084273\n\nPulled By: myleott\n\nfbshipit-source-id: de80d9abfac8e3d813a9c9b343b41327c500344e"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-12-06T02:24:56Z",
        "message": "Add wrapper abstraction on top of fvcore PathManager to insulate OSS compatibility\n\nReviewed By: myleott\n\nDifferential Revision: D18736914\n\nfbshipit-source-id: 897ba7463f7a8ebc0969c01c57bfe04fbf9e71c8"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-12-02T21:26:48Z",
        "message": "Apply Black auto-formatting\n\nReviewed By: sujitoc\n\nDifferential Revision: D18738392\n\nfbshipit-source-id: b7b7b75ef97946786c463c1887ef9a8676f030e6"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-11-22T01:50:42Z",
        "message": "Quick fix for Structured Dropout checkpointing (#1406)\n\nSummary:\nHere's a quick fix for https://github.com/pytorch/fairseq/issues/1403.\n\nTo keep it short, this fix allows the user to checkpoint a translation model properly after applying layer pruning on a restored transformer file.\nPull Request resolved: https://github.com/pytorch/fairseq/pull/1406\n\nDifferential Revision: D18637540\n\nPulled By: myleott\n\nfbshipit-source-id: 0f5e91e05e6579f0f459bc5293e9b14cb267322d"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-11-21T00:52:59Z",
        "message": "Refactor data sharding to be specified via caller of task rather than task itself\n\nSummary: Modifying number of shards internally to disable data sharding for batch iteration is dangerous because the caller of these tasks is not limited to fairspeq/train. So therefore we should put the onus of data sharding properly on the caller rather than the task itself.\n\nReviewed By: myleott\n\nDifferential Revision: D18456424\n\nfbshipit-source-id: d46be16c441c50082f9a768d0b259e6c28a4b67b"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-10-30T19:55:54Z",
        "message": "layer drop\n\nSummary: This diff enables layer drop in transformer decoder in production training pipeline (ptt_transformer). It builds on top of the fairseq implementation D18094657 added by Angela Fan, and added additional logic to handle corresponding dropping layers at test time in exported model.\n\nReviewed By: jhcross\n\nDifferential Revision: D18165586\n\nfbshipit-source-id: 373ac00268a25fa9e412edcb483becdfe792d992"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-10-27T19:10:53Z",
        "message": "adding layerdrop code for training, pruning, and readme (#890)\n\nSummary:\nTEST 1: EVALUATION TIME WORKS\nchecked\nachieves correct model perplexity: 18.68\n\nTEST 2: TRAINING NEW MODEL WORKS\nchecked\n\nwithout layerdrop:\n--decoder-layerdrop 0 OR no flag at all\n| epoch 001:     10 / 11201 loss=27.469, nll_loss=27.469, ppl=185799477.36, wps=1764, ups=0, wpb=9216.000, bsz=3.000, num_updates=7, lr=0.0004376, gnorm=25.471, clip=1.000, oom=0.000, loss_scale=8.000, wall=37, train_wall=30\n| epoch 001:     20 / 11201 loss=27.443, nll_loss=27.443, ppl=182500427.22, wps=2449, ups=0, wpb=9216.000, bsz=3.000, num_updates=17, lr=0.0010626, gnorm=25.273, clip=1.000, oom=0.000, loss_scale=8.000, wall=64, train_wall=57\n| epoch 001:     30 / 11201 loss=27.404, nll_loss=27.404, ppl=177612215.78, wps=2720, ups=0, wpb=9216.000, bsz=3.000, num_updates=27, lr=0.0016876, gnorm=25.136, clip=1.000, oom=0.000, loss_scale=8.000, wall=91, train_wall=84\n| epoch 001:     40 / 11201 loss=27.009, nll_loss=27.009, ppl=135079983.00, wps=2865, ups=0, wpb=9216.000, bsz=3.000, num_updates=37, lr=0.0023126, gnorm=24.311, clip=1.000, oom=0.000, loss_scale=8.000, wall=119, train_wall=112\n| epoch 001:     50 / 11201 loss=26.418, nll_loss=26.418, ppl=89680259.41, wps=2952, ups=0, wpb=9216.000, bsz=3.000, num_updates=47, lr=0.0029376, gnorm=22.775, clip=1.000, oom=0.000, loss_scale=8.000, wall=147, train_wall=140\n\nwith layerdrop (regularization effect should be seen in PPL):\n--decoder-layerdrop 0.2\n\n| epoch 001:     10 / 11201 loss=25.186, nll_loss=25.186, ppl=38182937.27, wps=2428, ups=0, wpb=9216.000, bsz=3.000, num_updates=8, lr=0.0005001, gnorm=17.082, clip=1.000, oom=0.000, loss_scale=16.000, wall=30, train_wall=24\n| epoch 001:     20 / 11201 loss=25.270, nll_loss=25.270, ppl=40451933.50, wps=3173, ups=0, wpb=9216.000, bsz=3.000, num_updates=18, lr=0.0011251, gnorm=17.162, clip=1.000, oom=0.000, loss_scale=16.000, wall=52, train_wall=45\n| epoch 001:     30 / 11201 loss=25.349, nll_loss=25.349, ppl=42752256.68, wps=3454, ups=0, wpb=9216.000, bsz=3.000, num_updates=28, lr=0.0017501, gnorm=17.370, clip=1.000, oom=0.000, loss_scale=16.000, wall=75, train_wall=68\n| epoch 001:     40 / 11201 loss=25.115, nll_loss=25.115, ppl=36343806.30, wps=3619, ups=0, wpb=9216.000, bsz=3.000, num_updates=38, lr=0.0023751, gnorm=16.945, clip=1.000, oom=0.000, loss_scale=16.000, wall=97, train_wall=90\n| epoch 001:     50 / 11201 loss=24.804, nll_loss=24.804, ppl=29284345.78, wps=3716, ups=0, wpb=9216.000, bsz=3.000, num_updates=48, lr=0.0030001, gnorm=16.406, clip=1.000, oom=0.000, loss_scale=16.000, wall=119, train_wall=112\n\nTEST 3: PICKING UP TRAINING FROM EXISTING MODEL\nchecked\n\n| loaded checkpoint /checkpoint/angelafan/structured_0.1_block_8_sd02/checkpoint_last.pt (epoch 272 @ 381066 updates)\n| loading train data for epoch 272\n| loaded 1801350 examples from: /private/home/angelafan/lm_work/fairseq-py/data-bin/wikitext-103/train\n\nTEST 4: EVALUATING EXISTING BERT MODEL REPROS RESULTS\n| [input] dictionary: 50265 types\n| [label] dictionary: 9 types\n| Accuracy:  0.9231651376146789\nachieves correct accuracy on SST2 for this model\n\nTEST 5: TRAINING NEW BERT MODEL WORKS\nchecked and works\n\nTEST 6: NMT\n\nwithout layerdrop\n--encoder-layerdrop 0 --decoder-layerdrop 0 OR combinations of flag specified and not specified\n\n| epoch 001:     10 / 92203 loss=15.820, nll_loss=15.830, ppl=58267.93, wps=4902, ups=0, wpb=1477.818, bsz=51.636, num_updates=11, lr=1.47473e-06, gnorm=7.207, clip=0.000, oom=0.000, loss_scale=128.000, wall=60, train_wall=3\n| epoch 001:     20 / 92203 loss=15.523, nll_loss=15.501, ppl=46359.29, wps=5037, ups=0, wpb=1496.476, bsz=45.333, num_updates=21, lr=2.72448e-06, gnorm=6.869, clip=0.000, oom=0.000, loss_scale=128.000, wall=63, train_wall=6\n| epoch 001:     30 / 92203 loss=15.185, nll_loss=15.123, ppl=35695.79, wps=5085, ups=0, wpb=1519.355, bsz=44.645, num_updates=31, lr=3.97423e-06, gnorm=6.186, clip=0.000, oom=0.000, loss_scale=128.000, wall=66, train_wall=9\n| epoch 001:     40 / 92203 loss=14.940, nll_loss=14.849, ppl=29505.60, wps=5116, ups=1, wpb=1521.244, bsz=42.927, num_updates=41, lr=5.22398e-06, gnorm=5.610, clip=0.000, oom=0.000, loss_scale=128.000, wall=69, train_wall=12\n| epoch 001:     50 / 92203 loss=14.745, nll_loss=14.630, ppl=25346.87, wps=5070, ups=1, wpb=1507.961, bsz=41.725, num_updates=51, lr=6.47373e-06, gnorm=5.104, clip=0.000, oom=0.000, loss_scale=128.000, wall=71, train_wall=15\n\nwith layerdrop (regularization effect should be seen in PPL)\n\nA) works with --encoder-layerdrop 0.2 --decoder-layerdrop 0.2\nB) works with different settings --encoder-layerdrop 0.3 --decoder-layerdrop 0.5\nC) works with one on and one off --encoder-layerdrop 0.2 --decoder-layerdrop 0\n\n| epoch 001:     10 / 92203 loss=15.817, nll_loss=15.828, ppl=58158.54, wps=5355, ups=0, wpb=1477.818, bsz=51.636, num_updates=11, lr=1.47473e-06, gnorm=6.959, clip=0.000, oom=0.000, loss_scale=128.000, wall=59, train_wall=3\n| epoch 001:     20 / 92203 loss=15.650, nll_loss=15.641, ppl=51111.63, wps=5515, ups=0, wpb=1496.476, bsz=45.333, num_updates=21, lr=2.72448e-06, gnorm=6.825, clip=0.000, oom=0.000, loss_scale=128.000, wall=61, train_wall=6\n| epoch 001:     30 / 92203 loss=15.440, nll_loss=15.408, ppl=43491.58, wps=5602, ups=0, wpb=1519.355, bsz=44.645, num_updates=31, lr=3.97423e-06, gnorm=6.576, clip=0.000, oom=0.000, loss_scale=128.000, wall=64, train_wall=8\n| epoch 001:     40 / 92203 loss=15.247, nll_loss=15.193, ppl=37457.14, wps=5676, ups=1, wpb=1521.244, bsz=42.927, num_updates=41, lr=5.22398e-06, gnorm=6.124, clip=0.000, oom=0.000, loss_scale=128.000, wall=67, train_wall=11\n| epoch 001:     50 / 92203 loss=15.055, nll_loss=14.977, ppl=32259.92, wps=5598, ups=1, wpb=1507.961, bsz=41.725, num_updates=51, lr=6.47373e-06, gnorm=5.661, clip=0.000, oom=0.000, loss_scale=128.000, wall=69, train_wall=14\n\nTEST 7: PRUNING TESTCASES\n\nA) after adding the pruning flags, model can evaluate as a full model\nchecked, reaches correct PPL\nnum. model params: 246933504\n| Evaluated 217646 tokens in 196.3s (1108.99 tokens/s)\n| Loss: 2.9275, Perplexity: 18.68\n\nB) after adding pruning flags, model can be pruned. this works with multiple flag settings\nchecked three cases:\nnum. model params: 146163712\n| Evaluated 217646 tokens in 106.0s (2054.07 tokens/s)\n| Loss: 3.0932, Perplexity: 22.05\n\nnum. model params: 209144832\n| Evaluated 217646 tokens in 162.8s (1336.99 tokens/s)\n| Loss: 2.9526, Perplexity: 19.16\n\nC) model can pick up training if you want to finetune the pruned model\nchecked:\n| loading train data for epoch 272\n| loaded 1801350 examples from: /private/home/angelafan/lm_work/fairseq-py/data-bin/wikitext-103/train\n| WARNING: overflow detected, setting loss scale to: 64.0\n| WARNING: overflow detected, setting loss scale to: 32.0\n| epoch 272:   1500 / 5601 loss=5.015, nll_loss=5.015, ppl=32.33, wps=11598, ups=1, wpb=18432.000, bsz=6.000, num_updates=98, lr=0.0061251, gnorm=0.613, clip=1.000, oom=0.000, loss_scale=32.000, wall=156, train_wall=252396\n\nD) works with BERT\nchecked:\nwithout specifying any flags, reproduces the correct standard accuracy\nwith flags, produces the correct pruned accuracy\n\n| [input] dictionary: 50265 types\n| [label] dictionary: 9 types\n| Accuracy:  0.9231651376146789\n\n| [input] dictionary: 50265 types\n| [label] dictionary: 9 types\n| Pruning model to specified layer configuration - this works best if the model was trained with LayerDrop\n| Accuracy:  0.9220183486238532\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/890\n\nReviewed By: edunov\n\nDifferential Revision: D18094657\n\nPulled By: huihuifan\n\nfbshipit-source-id: 2bbaa2ff0039e906782694fc2038b8c17a8693e7"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-10-12T04:53:11Z",
        "message": "Added option to save checkpoints using Path Manager.\n\nSummary: Added option to save checkpoints using Path Manager.\n\nReviewed By: hudeven\n\nDifferential Revision: D17392754\n\nfbshipit-source-id: 4b8e556ef8455a1548e5a083d779ed809cd785be"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-10-09T21:53:27Z",
        "message": "Fix data loading memory issue in pyspeech\n\nSummary:\nWe currently shard data when creating the batch iterator. This means we first load all indicese/frame lengths/handles into memory, and then do the sharding. This makes it impossible to train on large datasets with a high amount of workers  because each worker will need to load the entire dataset into memory. For training on a million hours of data (i.e. semi-supervised or unsupervised approaches) this data loading just makes it flat out impossible to use 8 GPU's.\n\n3 changes:\n\n1. This diff modifies the data loading such that we do the sharding while we read the handles file, rather than later. This modification is done on a task-by-task basis, since the task specifies how the data is loaded. I've tried to make the code compatible with both sharding during handle loading and sharding during batch iteration. I've currently only done the sharding during handle loading for the aligned_training task.\n\n2. To support data sharding at data loading time and the requirement that all shards must have exactly the same # of batches, I've added a method to do this synchronization where all shards with too many batches would just truncate the extra ones, similar to what we already do.\n\n2. In fairspeq/train.py, we are actually loading the training dataset and batch iterator twice, once in train.py and once when loading the checkpoint (which we always do regardless if there is a checkpoint). This means double the loading time which can be painful for very large files. I've removed the extraneous loading in this diff as well.\n\nReviewed By: yqwangustc\n\nDifferential Revision: D17750715\n\nfbshipit-source-id: 0e6e3d363525fa5661f1c784303390ea13f46377"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-09-20T16:34:58Z",
        "message": "added multilingual masked LM training (#849)\n\nSummary:\nThe multilingual-RoBERTa training is working with aconneau XLM data.\n\nTwo pieces remaining:\n\n1) `XLM` limits batch to be from same language, I am not 100% sure about the reason for that, but should be easy to implement, basically we can add `batch_by_size_and_language` instead of default `batch_by_size` function. If it's not critical, I would want to leave it out as it keeps the code very clean and simple.\n\n2) `sample_ratio` in `ConcatDataset` works with `int` by tiling the datasets based on ratio. Currently I am handling it by sounding off the ratio to `first decimal` and then multiplying by `10`. We can see if some such simple heuristics are good enough, there are other options (we can talk about them offline).\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/849\n\nDifferential Revision: D17162460\n\nfbshipit-source-id: d967f3d872f7a1f0aa4ea418bd362b68af9e432f"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-08-21T20:43:01Z",
        "message": "Parameterized criterions (#808)\n\nSummary:\nSupport criterion with parameters, such as AutoSegmentationCriterion (ASG) used in wav2letter which has a transition matrix parameter. This is needed to integrate wav2letter's ASG into PySpeech.\n\nWith this diff, parameters in criterions will be:\n(1) updated by optimizers, with a configurable learning rate\n(2) saved and loaded from checkpoints, preserving backward compatibility for criterions without parameters\n(3) synchronized across nodes in distributed training.\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/808\n\nReviewed By: jcai1\n\nDifferential Revision: D16934097\n\nPulled By: okhonko\n\nfbshipit-source-id: 121ec9382459385c6f9cbef3a8274bec1a434038"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-08-12T16:08:16Z",
        "message": "Lint\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/817\n\nDifferential Revision: D16762905\n\nPulled By: myleott\n\nfbshipit-source-id: d920595bec44ed26b72dfc6fbc15c0aa107b4e56"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-08-12T14:17:21Z",
        "message": "Update --restore-file logic (partially fixes #999)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1007\n\nDifferential Revision: D16762490\n\nPulled By: myleott\n\nfbshipit-source-id: d67137bcf581887850323d188bb4ea643a35ac9e"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-08-10T15:16:50Z",
        "message": "Add WSC task and criterion\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1004\n\nDifferential Revision: D16751443\n\nPulled By: myleott\n\nfbshipit-source-id: f70acd6c7be6d69da45b5b32fe4c4eff021539ab"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-08-01T12:55:57Z",
        "message": "Update PyTorch Hub interface\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/782\n\nDifferential Revision: D16542256\n\nPulled By: myleott\n\nfbshipit-source-id: ea3279e7a1ce4687a5914f32b76787c419be1ffa"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-07-30T14:48:23Z",
        "message": "Relicense fairseq under MIT license (#786)\n\nSummary:\nThe previous BSD+PATENTS license was controversial. We have been\napproved to relicense fairseq under the MIT license.\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/786\n\nDifferential Revision: D16560654\n\nPulled By: myleott\n\nfbshipit-source-id: f78b1beb4f2895dd7b9bfc79f5f952a2bfb94034"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-07-29T23:06:26Z",
        "message": "adding glue data preprocessing scripts (#771)\n\nSummary:\n1) Added glue data pre-processing script.\n2) updated README with usage.\n\nTODO:\n1) releasing fairseq dictionary and remove hardcoded path.\n2) remove hard-coded path for bpe-encoding,\n\nmyleott what do you recommend for above TODOs?\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/771\n\nReviewed By: myleott\n\nDifferential Revision: D16547679\n\nPulled By: myleott\n\nfbshipit-source-id: 6a6562d9b6215523d048fdf3daee63ffac21e231"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-07-24T23:59:07Z",
        "message": "check save_dir before beginning training\n\nSummary: I sadly discovery that my checkpoint directory wasn't globally readable after 8 hours of training. Adding this check at the beginning of train loop to keep that from happening again!\n\nReviewed By: myleott\n\nDifferential Revision: D16455394\n\nfbshipit-source-id: 35959aa058150b2afb63710c468d01ebc8a12b0c"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-07-19T20:13:43Z",
        "message": "Rename _load_model_ensemble -> load_model_ensemble_and_task\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/738\n\nDifferential Revision: D16377803\n\nPulled By: myleott\n\nfbshipit-source-id: 6beb2f78e7464b70ff65a965d2b747cdca0ca951"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-07-01T21:10:49Z",
        "message": "Fixes checkpointing bug introduced in 89e077c\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/847\n\nDifferential Revision: D16075498\n\nPulled By: myleott\n\nfbshipit-source-id: 62e27a8c4764f53f181c502674dfab1e6b0537e2"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-06-30T18:35:33Z",
        "message": "Add additional options for configuring writing of checkpoints\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/697\n\nDifferential Revision: D16068465\n\nPulled By: myleott\n\nfbshipit-source-id: c2563c3c682e7e8406e6d7c8e895d8afbec551eb"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-06-11T18:19:58Z",
        "message": "Automatically fill in default values from add_args\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/797\n\nDifferential Revision: D15761071\n\nPulled By: myleott\n\nfbshipit-source-id: 257d4a2297e83da7e59baed154dbafd6bfe614bf"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-05-30T18:41:40Z",
        "message": "Add --reset-dataloader\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/613\n\nDifferential Revision: D15541384\n\nPulled By: myleott\n\nfbshipit-source-id: ef2c0b0a51cdf37af2ccff0546f524d49f87e65d"
    },
    {
        "repo_url": "github.com/facebookresearch/fairseq",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-05-17T21:25:56Z",
        "message": "Small features + lint\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/588\n\nDifferential Revision: D15389638\n\nPulled By: myleott\n\nfbshipit-source-id: 4632ce22d51dc2c74d250bae999630095d849701"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2022-06-22T21:03:17Z",
        "message": "add check for OC version in fairseq\n\nSummary: fairseq patches a omegaconf internal util function that no longer exists in OmegaConf 2.2. This is a fix to make it compatible with both versions.\n\nReviewed By: dianaml0\n\nDifferential Revision: D37323720\n\nfbshipit-source-id: 1b15b86decc70776303afe4a9a4c63acfef27ffc"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2022-04-14T22:39:36Z",
        "message": "Fix typo in exception value (#4334)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes typo\n\n## PR review\n\n## Did you have fun?\n\nPull Request resolved: https://github.com/pytorch/fairseq/pull/4334\n\nReviewed By: Mortimerp9\n\nDifferential Revision: D35503972\n\nPulled By: dianaml0\n\nfbshipit-source-id: 09893de009d398e7a048ec89f757634ddc10139d"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2022-01-21T17:27:49Z",
        "message": "Fix breakage from D33649708\n\nSummary: https://www.internalfb.com/diff/D33649708 (https://github.com/pytorch/fairseq/commit/995c204337d16a6146a433cee360e5a5bfbc9a6f)?src_version_fbid=1030479880843010&dst_version_fbid=247617347518523&transaction_fbid=1601081576900014\n\nReviewed By: alexeib\n\nDifferential Revision: D33696937\n\nfbshipit-source-id: 9a17610e3f4eb3dd2b2131a3f9fb42732a31b47f"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2022-01-20T08:02:16Z",
        "message": "Data2vec prelim (#2929)\n\nSummary:\nPreliminaries for data2vec release, include some minor improvements and bug fixes\n\nMost important change is that we now default to raising an exception when fields in config do not have a corresponding field in the model dataclass\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2929\n\nReviewed By: wnhsu\n\nDifferential Revision: D33649708\n\nPulled By: alexeib\n\nfbshipit-source-id: 629bdb4c361550740b451c570c2005bb956c6fcb"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2022-01-19T03:29:52Z",
        "message": "Decode using EMA model in IPL recipe\n\nSummary: Add option to use the EMA model for decoding in transducer IPL recipe by passing --ipl-decode-ema. Note EMA should be enabled as in the diff D24238379 (https://github.com/pytorch/fairseq/commit/8feccf94412424a4683b01090de36fa77cb4951d) using options --store-ema --ema-start-update and --ema-decay.\n\nReviewed By: cruvadom\n\nDifferential Revision: D31983366\n\nfbshipit-source-id: 2bf63b3f7d1b5fa8804b3a7e9bfab71a463ca957"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2022-01-07T08:39:11Z",
        "message": "Formatting fix: get CI green (#2860)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nApplies `black` and `isort` to files\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2860\n\nReviewed By: Mortimerp9\n\nDifferential Revision: D33456637\n\nPulled By: dianaml0\n\nfbshipit-source-id: 560b8d3a8f589cbecc92d0d21163596b5d47d609"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-12-31T02:38:12Z",
        "message": "add strict option to checkpoint_utils. load_pretrained_component_from_model()\n\nSummary: Add strict option to checkpoint_utils. load_pretrained_component_from_model()\n\nReviewed By: sravyapopuri388\n\nDifferential Revision: D33304224\n\nfbshipit-source-id: 2284a21dfea7810ec212f15daadeeeb45c6dca1b"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-12-17T00:11:19Z",
        "message": "formatting fix (#2816)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nfix `black` failures\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2816\n\nReviewed By: alexeib\n\nDifferential Revision: D33172615\n\nPulled By: dianaml0\n\nfbshipit-source-id: 36b141f42941670f1bfa981041d878042feb0428"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-12-11T00:55:12Z",
        "message": "Add loading from HuggingFace Hub\n\nSummary: Add loading from HuggingFace Hub. Revised from and to replace D32697723 (accepted).\n\nReviewed By: pipibjc, dianaml0\n\nDifferential Revision: D32964041\n\nfbshipit-source-id: 39676aa0ecb10454ae76b70968d5abe96ab6da54"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-11-29T20:32:59Z",
        "message": "Add linting with black (#2678)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2678\n\nReviewed By: Mortimerp9\n\nDifferential Revision: D32653381\n\nPulled By: dianaml0\n\nfbshipit-source-id: 2810d14867cd7d64f4d340740e2b590b82de47fe"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-11-19T00:38:31Z",
        "message": "Merge --use-ontology-for-* into --use-ontology\n\nSummary:\nThere are three options for the ontology:\n* `--use-ontology-for-training`\n* `--use-ontology-for-validation`\n* `--use-ontology-for-balancing`\n\nThe first two must always be set together.\n\nIn the past, I observed that it's best not to use ontology for data balancing even if we use ontology for training and validation. But now I no longer observe this.\n\nTherefore, I'm merging all these three options into one (`--use-ontology`).\n\nIn addition, I'm also moving the logic of avoiding loading teacher models out of `checkpoint_utils.py`. If you want to load a student model without loading its teachers (e.g. for prediction only), specify `arg_overrides={\"ignore_teachers\": True}` when calling `load_model_ensemble`.\n\nReviewed By: xiaoxiao26\n\nDifferential Revision: D32518830\n\nfbshipit-source-id: 103c6458f7927ec5ca7470109c8f956c00f514a2"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-10-23T02:51:09Z",
        "message": "set num_update before loading state dict (#2491)\n\nSummary:\n## What does this PR do?\nSet `model.num_updates` in `load_model_ensemble_and_task()` before loading `state_dict`, like what's done in `fairseq/trainer.py`, because a model's `state_dict` may depend on `num_update`.\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2491\n\nReviewed By: xuqiantong\n\nDifferential Revision: D31863368\n\nPulled By: wnhsu\n\nfbshipit-source-id: c70051f898819cc43b02c9f5765429e9f194aed5"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-09-13T20:20:35Z",
        "message": "Back out \"Fairseq needs to store and load metadata from model state_dict\" (#3861)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/fairseq/pull/3861\n\nbackout fairseq changes. fix with a suggested, more optimal changes in checkopint utils.\n\nReviewed By: zhengwy888\n\nDifferential Revision: D30886481\n\nfbshipit-source-id: 12b6dd4d5107ab4371b73a58d9a044a17c733260"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-09-09T01:18:30Z",
        "message": "Fairseq needs to store and load metadata from model state_dict\n\nSummary:\n## TL;DR\nFairseq checkpoint saving and loading should mirror torch's checkpoint by saving and loading \"state_dict()._metadata\".\n\n## Long Story:\n\n#### What happened:\nDuring model loading and saving, Quantization-aware-training models in Pytorch encounters a weird bug that says state_dict \"fake_weight_quant.weight.min_val\" is mismatched to \"min_vals\".\n\n#### What was the reason:\n- We found the issue in that torch uses state_dict()._metadata to store module._version, but the metadata was never store in checkpoint, nor are they loaded during checkpoint loading in fairseq.\n\nReviewed By: frankseide\n\nDifferential Revision: D30649933\n\nfbshipit-source-id: ce262486b9b95fbcece463fa05c4e1903d4232d7"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-09-01T19:29:51Z",
        "message": "EMA\n\nSummary:\nAdds Exponential moving average (EMA) model for Kaizen semi-supervised training https://arxiv.org/abs/2106.07759\n\n1. Add `ema.store_ema` to enable storing EMA. EMA will be written to extra_state in the state dict while saving checkpoint.\n2. `ema.ema_start_update` to control when the EMA starts accumulating\n3. Tasks can use `uses_ema` property to decide if the EMA should be passed to the task. (Default is False)\n4. `load_ema_from_checkpoint` can be used to load EMA model in place of the model to be used for evalutation. Pyspeech has eval-ema option for this.\n\n```\nThis module has the EMA class used to store a copy of the exponentially decayed\nmodel params.\n\nTypical usage of EMA class involves initializing an object using an existing\nmodel (random or from a seed model) and setting the config like ema_decay,\nema_start_update which determine how the EMA model is updated. After every\nupdate of the model i.e. at the end of the train_step, the EMA should be updated\nby passing the new model to the EMA.step function. The EMA model state dict\ncan be stored in the extra state under the key of \"ema\" and dumped\ninto a checkpoint and loaded. The EMA object can be passed to tasks\nby setting task.uses_ema property.\nEMA is a smoothed/ensemble model which might have better performance\nwhen used for inference or further fine-tuning. EMA class has a\nreverse function to load the EMA params into a model and use it\nlike a regular model.\n```\n\nReviewed By: cruvadom\n\nDifferential Revision: D24238379\n\nfbshipit-source-id: 879d3ba5070a614b7d365f9503af357001e875b2"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-08-02T21:36:32Z",
        "message": "fixing checkpoint config upgrade for generation print_alignment (#2125)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes config upgrade conditions for upgrading generation. print_alignment\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2125\n\nReviewed By: myleott\n\nDifferential Revision: D30049140\n\nPulled By: jingfeidu\n\nfbshipit-source-id: e613821e94d0cdb876c35bc6e3fede7affbf4628"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-29T23:30:41Z",
        "message": "seed random suffix in checkpoint to be consistent across shards\n\nSummary:\nCurrently the random suffix for saving sharded checkpoints can be different for each shard when training with FSDP and use_sharded_state=True. This makes it difficult for downstream applications to load the checkpoints properly, since each shard may have different suffixes.\n\nThis diff seeds the random suffix to be consistent across shards\n\nReviewed By: zhengwy888\n\nDifferential Revision: D29951167\n\nfbshipit-source-id: 65749357e62a28978f3b46b71767204a508e1f61"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-29T23:30:41Z",
        "message": "use pathmanager to delete old checkpoints\n\nSummary: --keep-best-checkpoints doesn't work for Manifold paths because the current code assumes normal paths. Lets use  PathManager to properly remove them.\n\nReviewed By: myleott, sshleifer\n\nDifferential Revision: D29947965\n\nfbshipit-source-id: 237a7b5aaa8293bb203ad05937decdf3b3ae2fc0"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-29T23:30:40Z",
        "message": "use suffix when saving best checkpoints with metric\n\nSummary: This is needed when training with FSDP + sharded state, as the checkpoints should have `-shard0.pt`\n\nReviewed By: sshleifer\n\nDifferential Revision: D29947728\n\nfbshipit-source-id: d2cb7c23e1e6d027d115cb734e2f2f1c34239eba"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-09T23:25:02Z",
        "message": "Roll back os.path.abspath change\n\nReviewed By: donhusa\n\nDifferential Revision: D29641968\n\nfbshipit-source-id: eca379158055f3e38e9c053b06db56842265e53a"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-08T22:27:57Z",
        "message": "Fix static container (#2036)\n\nSummary:\nfixes StatefulContainer being static which is a problem when you load a checkpoint with task that already has the same keys in the container\n\nalso print full path to checkpoints when saving (useful with hydra) and crash if repeatedly failing to save a checkpoint\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2036\n\nReviewed By: arbabu123\n\nDifferential Revision: D29608430\n\nPulled By: alexeib\n\nfbshipit-source-id: 1b65c8f839e02de9110af3ec53f1e7d48a4908f7"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-06T22:07:31Z",
        "message": "fixes tests/test_train.py to mock checkpoint.save_dir config node (#3675)\n\nSummary:\n## What does this PR do?\nSome downstream users reported that errors when passing Namespace to load_checkpoint().\n\nA recent change made the assumption that the passed object is dict like (dict or DictConfig) that have a get function.\nThis changes that and make sure the mocked config have checkpoint.save_dir to allow the test to run.\n\nPull Request resolved: https://github.com/pytorch/fairseq/pull/3675\n\nReviewed By: omry\n\nDifferential Revision: D29564805\n\nPulled By: lematt1991\n\nfbshipit-source-id: 89308811da382667f6c5d3152ee2d6480416ee62"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-01T13:37:47Z",
        "message": "Hydra 1.1 compatibility: Use an explicit schema for the primary config (#3659)\n\nSummary:\n## What does this PR do?\nFixes compatibility with Hydra 1.1.\nThe result is compatible with both Hydra 1.0 and Hydra 1.1, and will allow a smoother migration to Hydra 1.1.\n\nAt this point I am not yet removing the restriction on the Hydra version from setup.py:\n1. It depends on some Hydra 1.1 changes that are not yet released (It will be compatible with 1.1.1).\n2. Upgrading will result in deprecation warnings, and fixing them will break compatibility with Hydra 1.0.\n\nThere will be some followup to make the code fully compatible with 1.1 once Hydra 1.1 is the default version in fbcode.\n\nPull Request resolved: https://github.com/pytorch/fairseq/pull/3659\n\nReviewed By: omry\n\nDifferential Revision: D29498036\n\nPulled By: lematt1991\n\nfbshipit-source-id: 96999cde5daad6749ef4d3ddf6a36a1e984ff201"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-06-15T21:09:52Z",
        "message": "Check attributes in trainer and checkpoint loading before using them (#1970)\n\nSummary:\n## What does this PR do?\nFixes None exception when some attributes in  don't exist in cfg.\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1970\n\nReviewed By: alexeib\n\nDifferential Revision: D29140036\n\nPulled By: hikushalhere\n\nfbshipit-source-id: 7d941bcae6bb000c281a43ca2cd0876a49912ab9"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-06-04T23:19:56Z",
        "message": "Fix loading some TALNet models\n\nSummary:\nD28728718 cleaned up the \"kd_binary_cross_entropy\" criterion, but this caused loading old models trained with this criterion to fail.\nThis diff replaces the \"kd_binary_cross_entropy\" criterion with the \"wav2vec\" criterion when loading models, and fixes this error.\n\nIt also removes the \"log_keys\" argument if it's `None`. Some criteria (e.g. wav2vec) require this argument to be a list, and will supply a default value of `[]` when it's absent. The presence of the `None` value prevents the use of this default value and causes an error.\n\nDifferential Revision: D28901263\n\nfbshipit-source-id: 9b33aed35e76d2c734d1d4e2cbca1ff193a8c920"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-06-04T00:49:12Z",
        "message": "Teacher-student learning for TALNet\n\nSummary:\nThis diff implements teacher-student learning for TALNet.\n\nThree classes take part in the teacher-student learning:\n* The task loads the teacher models;\n* The model generates predictions using the teacher models, and mixes them with the original targets;\n* The `Wav2VecCriterion` reads the mixed targets to compute the loss. However, it still uses the original targets to compute the MAP and MAUC metrics.\n\nThere are two types of teachers:\n* Static teachers: a file that stores predictions on training data which have been produced by running a model offline;\n* Dynamic teachers: model files that are loaded at the beginning of training and executed on the fly to produce predictions.\n\nWe actually no longer use static teachers. The code about static teachers are copied over from the `KnowledgeDistillationBinaryCrossEntropyCriterion` class. This class will be cleaned up in D28728718.\n\nThe teacher models are stored in the task object, and will not be saved into checkpoints.\n\nReviewed By: alexeib\n\nDifferential Revision: D28728707\n\nfbshipit-source-id: 0fcfc00db2e7194a6f7ee687cad9fa72e82a028b"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-05-26T00:45:51Z",
        "message": "support FSDP sharded_state checkpoint loading during inference\n\nSummary:\nusing the very useful feature added by QuentinDuval https://github.com/facebookresearch/fairscale/pull/683/files , we can consolidate sharded states into a full regular states. this allows inferences on sharded state almost transparently.\n\nThe main complexity comes from trying to be smart about what kind of checkpoint the user wants to load. not sure if this is over-engineering\n1. if the file checkpoint-shard0.pt exists, and `--checkpoint-shard-count` is > 1, then we load sharded FSDP checkpoint\n2. if checkpoint-shard0.pt exists but --checkpoint-shard-count=1, we load consolidated FSDP checkpoint\n3. if checkpoint-shard0.pt does not exist, but --checkpoint-shard-count > 1, we load model parallel checkpoint\n4. otherwise we are loading a single, plain checkpoint.\n\nIn theory we could be even smarter and load shard0.pt to check how many more checkpoints are needed. this is not implemented, though it will save the user having to specify --checkpoint-shard-count.\n\nReviewed By: sshleifer\n\nDifferential Revision: D28563441\n\nfbshipit-source-id: dcafcaa7c9eaf5c9ff94f55c16bb3424c98dfa59"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-05-22T07:22:05Z",
        "message": "fixing s2t transformer and N-best checkpoint saving\n\nSummary:\n- fixing the default value for `encoder_freezing_updates` in s2t transformer\n- fixing N-best checkpoint saving: the previous implementation compares the new checkpoint with only the previous best one but not the previous N best ones. This leads to suboptimal results on N-best checkpoint averaging.\n\nReviewed By: jmp84\n\nDifferential Revision: D28546493\n\nfbshipit-source-id: 44ec6d5ab49347f392d71269c5dcfd154b00c11e"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-05-21T23:18:21Z",
        "message": "attempt to make non-sharded FSDP checkpoint behave like regular checkpoint\n\nSummary:\noverall just wondering if feature is desirable. if it is, the next diff which supports loading sharded checkpoint into a consolidated state dict cleaner.\n\na couple advantages\n1. allows resuming from other DDP trainers.\n2. allows resuming into other DDP trainers. or FSDP of a different configuration.\n3. none-sharded FSDP checkpoint can be loaded with regular load_model_ensemble_and_task()\n\nFor old training workflow that's not using `--use-sharded-state`, please rename the checkpoint to remove the \"-shard0\" for resuming training.\n\nReviewed By: sshleifer\n\nDifferential Revision: D28563032\n\nfbshipit-source-id: ced72bed969319ab6306059721f56e29b2c3d892"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-04-14T08:50:07Z",
        "message": "add keep_interval_updates_pattern\n\nSummary:\nMotivation:\n\nI want to save checkpoints frequently, due to unreliable jobs in FB cluster that restart frequently. I want to do this without spamming Manifold storage, but still save some historical checkpoints (i.e. every 10k updates), so I can track how WER evolves over time.\n\nTo save frequently, I can use a small --save-interval-updates.\n\nTo delete old checkpoints to save storage, I can use --keep-interval-updates.\n\nHowever, this deletes all old checkpoints. This is where --keep-interval-updates-pattern comes in. If I now do:\n\n```\n--save-interval-updates 1000\n--keep-interval-updates 1\n--keep-interval-updates-pattern 10000\n```\n\nThis will:\n1. checkpoint every 1000 updates so that job restarts don't impact us significantly\n2. keep only the latest checkpoint to avoid saving a bunch of huge models in manifold\n3. make an exception for #2 for every 10k updates so we can track WER over time\n\nReviewed By: myleott\n\nDifferential Revision: D27578403\n\nfbshipit-source-id: 5aec2dc9a22778015f7a3daa017210190af81240"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-04-14T08:50:06Z",
        "message": "enable manifold checkpoints with --keep-interval-updates\n\nSummary: Useful to enable --keep-interval-updates with Manifold checkpoints\n\nReviewed By: myleott\n\nDifferential Revision: D27577116\n\nfbshipit-source-id: 6d4ae5aaccc07ecaed8ba6a333b6ab78b148187a"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-03-30T01:02:50Z",
        "message": "BASE layers (#1654)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1654\n\nReviewed By: myleott\n\nDifferential Revision: D27128074\n\nPulled By: shruti-bh\n\nfbshipit-source-id: ac86d383cd53c9c9bdd946fea839a37b719d95e3"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-03-26T14:18:59Z",
        "message": "FSDP uses new optimizer gathering to save optimizer state (#1744)\n\nSummary:\n- Full unflattened optimizer state dict is in `checkpoints/shard_0.pt`, other checkpoint files do not have the `last_optimizer_state` key.\n- requires master version of fairscale (eventually fairscale>=0.3.3)\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1744\n\nReviewed By: myleott\n\nDifferential Revision: D27342305\n\nPulled By: sshleifer\n\nfbshipit-source-id: 7442b8c6ed01599d8ab0050213e84051f4e98acd"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-03-04T21:32:44Z",
        "message": "Move checkpoint state_dict creation into Trainer (#1666)\n\nSummary:\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1666\n\nContext: the checkpoint saving call stack has become a bit convoluted:\n```\ntrain.py\n+ checkpoint_utils.save_checkpoint\n + trainer.save_checkpoint\n  + checkpoint_utils.save_state\n   + checkpoint_utils.torch_persistent_save\n```\n\nThis diff slightly simplifies the checkpoint saving logic by exposing a `state_dict` method inside the Trainer. This simplifies the call stack to:\n```\ntrain.py\n+ checkpoint_utils.save_checkpoint\n + trainer.save_checkpoint\n  + checkpoint_utils.torch_persistent_save\n```\n\nThis new structure is important for the FullyShardedDataParallel diff (next diff in the stack), since it enables the Trainer to save multiple checkpoints for the different optimizer state shards.\n\nTest Plan:\n- unit tests\n- trained WMT En-De models; confirmed checkpoints save/load properly, resuming from a checkpoint gives identical results\n- `buck test fblearner/flow/projects/langtech/translation:tests` (2 failures are in trunk too): https://www.internalfb.com/intern/testinfra/testconsole/testrun/2533274840914654/\n\nReviewed By: zhengwy888\n\nDifferential Revision: D26771146\n\nPulled By: myleott\n\nfbshipit-source-id: 10f91979cd42205c1d8abcaa9ab56f63eba31e93"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-03-04T05:17:29Z",
        "message": "minor fixes and improvements (#1671)\n\nSummary:\nthere are a few changes here:\n- convert config persisted in checkpoints into a plain dict when saving and back to omegaconf config when loading: this helps avoid compatibility issues between different versions of python, omegaconf, etc\n- update checkpoints that have old print_alignment saved\n- add lr_float to composite optimizer to enable sweeping on lr with auto sweepers like ax\n- fixing some edge cases for config loading\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1671\n\nReviewed By: myleott\n\nDifferential Revision: D26791583\n\nPulled By: alexeib\n\nfbshipit-source-id: 124dec74932052925c43b6a93130f4428803cb46"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-03-02T17:26:03Z",
        "message": "ioPath async - opt-in Fairseq integration (#1635)\n\nSummary:\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1635\n\n**Summary:** Integrate ioPath's async writes feature into Fairseq checkpoint writing.\n\n**Details:**\n- Created new checkpoint config param `--write-checkpoints-asynchronously` with default value `False`. Aliased to `--save-async`.\n- Added to `PathManager` class in `file_io.py` to include `PathManager.opena(...)` and `PathManager.async_close()`. These new methods use ioPath's async `PathManager`.\n\n**Usage:**\n```\npython train.py --save-async\n```\n---------\nNOTE: **QUESTIONS**\n1. In the current implementation, we don't save `checkpoint_best` and `checkpoint_latest` since ioPath doesn't yet have a \"wait until a file is written and then copy/move it to another path\" feature. Is this okay for now?\n2. Should I mimic the atomic vs non-atomic save structure that synchronous Fairseq checkpoint writes have?\n\n**Note to Eric:** Keep this integration in check with D26375501.\n\nReviewed By: myleott\n\nDifferential Revision: D26467815\n\nfbshipit-source-id: 50068ef7bf9a6d5cea4d5e0d13d672604dc4a6b0"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-02-11T22:00:25Z",
        "message": "save task state in the checkpoint (#1562)\n\nSummary:\nthis allows tasks to declare some properties they'd like to save in the checkpoint (such as a dictionary), which are loaded when checkpoint is restored.\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1562\n\nTest Plan: tested by training a new wav2vec model, then finetuning it, then decoding it and making sure the dict only loaded once, during fine tuning process (and was obtained from checkpoint for decoding)\n\nReviewed By: myleott, gwenzek\n\nDifferential Revision: D25937974\n\nPulled By: alexeib\n\nfbshipit-source-id: b9908042f76ec8cda943f33885eb9b1f121662ae"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-02-02T23:51:11Z",
        "message": "Fix the task data arg conversion to string.\n\nSummary: We were getting some test failures on our end due to incompatibility of task data argument type. The actual exception is defined in this task: T83395097 and T83395052. Fixing the task data arg to be a string instead of list of strings.\n\nReviewed By: myleott\n\nDifferential Revision: D26205482\n\nfbshipit-source-id: d29d1ee7c469177e8bdad7ca603938f8450bd81c"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-31T04:16:56Z",
        "message": "Fix incorrect local cache for checkpoint_last.pt when training is restarted on the same host\n\nReviewed By: myleott\n\nDifferential Revision: D25719057\n\nfbshipit-source-id: 2bf7dd93b6d223804da0326a0ed347e5e353f1f0"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-23T19:16:56Z",
        "message": "fairseq checkpoint improvements\n\nReviewed By: myleott\n\nDifferential Revision: D25677238\n\nfbshipit-source-id: b43075034c953491211f19a5464148de4758df83"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-18T19:45:08Z",
        "message": "Refactor eval_lm to support library usage (#1513)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1513\n\nTest Plan: Imported from OSS\n\nReviewed By: alexeib\n\nDifferential Revision: D25570467\n\nPulled By: myleott\n\nfbshipit-source-id: 062f748e287797f4f01c605e0b544ef3e698851f"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-18T15:40:49Z",
        "message": "Support atomic saves for checkpoints (#1520)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1520\n\nTest Plan: Imported from OSS\n\nReviewed By: stephenroller\n\nDifferential Revision: D25632782\n\nPulled By: myleott\n\nfbshipit-source-id: bdbe2aed6254d0b023b33f8027dfbd939f1fd271"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-16T01:47:42Z",
        "message": "Fix loading of very old checkpoints (#1512)\n\nSummary:\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1512\n\nSee https://github.com/pytorch/fairseq/issues/3032 for context\n\nTest Plan: Imported from OSS\n\nReviewed By: alexeib\n\nDifferential Revision: D25570470\n\nPulled By: myleott\n\nfbshipit-source-id: 9227b1ca36cd81ff72acdb5e03fd574e3e8769be"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-08T23:49:12Z",
        "message": "fix wav2vec scripts (#1494)\n\nSummary:\nfixes #2942\n+ docs + migration of old models\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1494\n\nReviewed By: myleott\n\nDifferential Revision: D25404601\n\nPulled By: alexeib\n\nfbshipit-source-id: 092f145602522f8e7ea3eaa709bfe602a4d29d8b"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-05T15:37:51Z",
        "message": "Rename optimization.min_lr -> optimization.stop_min_lr (#1486)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1486\n\nTest Plan: Imported from OSS\n\nReviewed By: alexeib\n\nDifferential Revision: D25342181\n\nPulled By: myleott\n\nfbshipit-source-id: 7d1cfb26334fff26d688648724ab073e5fb956f5"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-11-30T22:20:36Z",
        "message": "Add/fix tests (#1468)\n\nSummary:\n- add test for loading ensemble checkpoints (and confirmed it fails if I revert: https://github.com/pytorch/fairseq/commit/265791b727b664d4d7da3abd918a3f6fb70d7337)\n- add test for LayerDrop (and fix it)\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1468\n\nReviewed By: alexeib\n\nDifferential Revision: D25223272\n\nPulled By: myleott\n\nfbshipit-source-id: 3f06f753605af251567c70d2961f5506ea423499"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-11-18T01:08:13Z",
        "message": "fix loading ensembles (#1442)\n\nSummary:\nfixes loading ensembles. previous change used the state of the first model for all models in the ensemble\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1442\n\nReviewed By: chtran\n\nDifferential Revision: D25035706\n\nPulled By: alexeib\n\nfbshipit-source-id: 9029999be0f1703efb1df20bec2890de59449f1f"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-11-11T18:15:10Z",
        "message": "load dataset with saved task config (optionally) (#1423)\n\nSummary:\nthis adds an argument to load_dataset that provides task configuration from the checkpoint. different tasks can decide what to do with it afterwards.\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1423\n\nReviewed By: myleott\n\nDifferential Revision: D24875706\n\nPulled By: alexeib\n\nfbshipit-source-id: 5bb1e2b7495520c456024dc7b0751b65cb05b473"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-11-09T23:46:00Z",
        "message": "migrate wav2vec2 model (#1409)\n\nSummary:\nsee title\nalso includes some minor bug fixes\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1409\n\nReviewed By: myleott\n\nDifferential Revision: D24822219\n\nPulled By: alexeib\n\nfbshipit-source-id: b18f9a8af42ced37880c23dd6ad1ec4df3dfc040"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-11-05T23:29:33Z",
        "message": "Move TPU grad reductions out of Trainer into TPUDistributedDataParallel (#1397)\n\nSummary:\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1397\n\nData parallel command: `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --share-decoder-input-output-embed --lr 0.0001`\n\nData parallel before:\n```\n2020-11-04 08:20:13 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)\n2020-11-04 08:20:13 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8\n2020-11-04 08:20:13 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt\n2020-11-04 08:20:13 | INFO | fairseq.trainer | loading train data for epoch 1\n2020-11-04 08:20:14 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train\n2020-11-04 08:20:14 | INFO | fairseq.optim.adam | using FusedAdam\n2020-11-04 08:20:14 | INFO | fairseq.trainer | begin training epoch 1\n2020-11-04 08:20:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=5\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108002, wps=160870, ups=4.91, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.8, wps=517232, ups=15.77, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.4, wps=537322, ups=16.38, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492.1, wps=540488, ups=16.48, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603.2, wps=543411, ups=16.57, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=540359, ups=16.47, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:20 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=533446, ups=16.26, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:20 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.4, wps=539734, ups=16.46, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=6\n```\n\nData parallel after:\n```\n2020-11-04 08:14:02 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)\n2020-11-04 08:14:02 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8\n2020-11-04 08:14:02 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt\n2020-11-04 08:14:02 | INFO | fairseq.trainer | loading train data for epoch 1\n2020-11-04 08:14:03 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train\n2020-11-04 08:14:03 | INFO | fairseq.optim.adam | using FusedAdam\n2020-11-04 08:14:03 | INFO | fairseq.trainer | begin training epoch 1\n2020-11-04 08:14:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108002, wps=157099, ups=4.79, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.8, wps=560049, ups=17.08, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.4, wps=558507, ups=17.03, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492.1, wps=514194, ups=15.68, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603.2, wps=552676, ups=16.85, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:09 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=546402, ups=16.66, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:09 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=508472, ups=15.5, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:09 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.4, wps=552493, ups=16.84, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=6\n```\n\nData parallel command (no_c10d): `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --share-decoder-input-output-embed --lr 0.0001 --dp-backend no_c10d`\n\nData parallel before:\n```\n2020-11-04 08:19:25 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)\n2020-11-04 08:19:25 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8\n2020-11-04 08:19:25 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt\n2020-11-04 08:19:25 | INFO | fairseq.trainer | loading train data for epoch 1\n2020-11-04 08:19:25 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train\n2020-11-04 08:19:26 | INFO | fairseq.optim.adam | using FusedAdam\n2020-11-04 08:19:26 | INFO | fairseq.trainer | begin training epoch 1\n2020-11-04 08:19:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n2020-11-04 08:19:31 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108001, wps=141659, ups=4.32, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.9, wps=503762, ups=15.36, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.5, wps=488599, ups=14.9, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492, wps=507855, ups=15.48, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603, wps=503270, ups=15.34, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=7\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=467778, ups=14.26, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=7\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=503800, ups=15.36, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=7\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.3, wps=468486, ups=14.28, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=7\n```\n\nData parallel after:\n```\n2020-11-04 08:14:50 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)\n2020-11-04 08:14:50 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8\n2020-11-04 08:14:50 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt\n2020-11-04 08:14:50 | INFO | fairseq.trainer | loading train data for epoch 1\n2020-11-04 08:14:50 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train\n2020-11-04 08:14:51 | INFO | fairseq.optim.adam | using FusedAdam\n2020-11-04 08:14:51 | INFO | fairseq.trainer | begin training epoch 1\n2020-11-04 08:14:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108001, wps=137677, ups=4.2, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.9, wps=519541, ups=15.84, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.5, wps=517063, ups=15.76, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492, wps=490728, ups=14.95, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603, wps=505262, ups=15.41, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=508874, ups=15.52, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:57 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=518028, ups=15.79, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:57 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.3, wps=515996, ups=15.73, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=7\n```\n\nModel parallel command: `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm_megatron --decoder-layers 4 --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --model-parallel-size 2 --share-decoder-input-output-embed --lr 0.0001`\n\nModel parallel before:\n```\n2020-11-04 08:18:38 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)\n2020-11-04 08:18:38 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8\n2020-11-04 08:18:38 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last-model_part-0.pt\n2020-11-04 08:18:38 | INFO | fairseq.trainer | loading train data for epoch 1\n2020-11-04 08:18:38 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train\n2020-11-04 08:18:39 | INFO | fairseq.optim.adam | using FusedAdam\n2020-11-04 08:18:39 | INFO | fairseq.trainer | begin training epoch 1\n2020-11-04 08:18:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n2020-11-04 08:18:45 | INFO | train_inner | epoch 001:      2 / 7173 loss=55.997, ppl=7.19017e+16, wps=0, ups=0, wpb=16384, bsz=32, num_updates=1, lr=0.0001, gnorm=14.03, loss_scale=64, train_wall=1, wall=7\n2020-11-04 08:18:45 | INFO | train_inner | epoch 001:      3 / 7173 loss=28.372, ppl=3.47501e+08, wps=48371.7, ups=2.95, wpb=16384, bsz=32, num_updates=2, lr=0.0001, gnorm=15.339, loss_scale=64, train_wall=0, wall=8\n2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      4 / 7173 loss=15.855, ppl=59276.8, wps=72422.5, ups=4.42, wpb=16384, bsz=32, num_updates=3, lr=0.0001, gnorm=4.189, loss_scale=64, train_wall=0, wall=8\n2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      5 / 7173 loss=14.713, ppl=26858.7, wps=72933.5, ups=4.45, wpb=16384, bsz=32, num_updates=4, lr=0.0001, gnorm=4.751, loss_scale=64, train_wall=0, wall=8\n2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      6 / 7173 loss=13.901, ppl=15299.7, wps=71974.8, ups=4.39, wpb=16384, bsz=32, num_updates=5, lr=0.0001, gnorm=4.361, loss_scale=64, train_wall=0, wall=8\n2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      7 / 7173 loss=13.312, ppl=10169.5, wps=72897.8, ups=4.45, wpb=16384, bsz=32, num_updates=6, lr=0.0001, gnorm=3.307, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:18:47 | INFO | train_inner | epoch 001:      8 / 7173 loss=12.914, ppl=7720.21, wps=73044.6, ups=4.46, wpb=16384, bsz=32, num_updates=7, lr=0.0001, gnorm=5.473, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:18:47 | INFO | train_inner | epoch 001:      9 / 7173 loss=12.56, ppl=6036.72, wps=73453.1, ups=4.48, wpb=16384, bsz=32, num_updates=8, lr=0.0001, gnorm=6.112, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:18:47 | INFO | train_inner | epoch 001:     10 / 7173 loss=12.116, ppl=4437.77, wps=73442.6, ups=4.48, wpb=16384, bsz=32, num_updates=9, lr=0.0001, gnorm=4.415, loss_scale=64, train_wall=0, wall=9\n```\n\nModel parallel after:\n```\n2020-11-04 08:12:09 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)\n2020-11-04 08:12:09 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8\n2020-11-04 08:12:09 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last-model_part-0.pt\n2020-11-04 08:12:09 | INFO | fairseq.trainer | loading train data for epoch 1\n2020-11-04 08:12:09 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train\n2020-11-04 08:12:10 | INFO | fairseq.optim.adam | using FusedAdam\n2020-11-04 08:12:10 | INFO | fairseq.trainer | begin training epoch 1\n2020-11-04 08:12:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      2 / 7173 loss=55.997, ppl=7.19017e+16, wps=0, ups=0, wpb=16384, bsz=32, num_updates=1, lr=0.0001, gnorm=14.03, loss_scale=64, train_wall=1, wall=8\n2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      3 / 7173 loss=28.372, ppl=3.47501e+08, wps=53097, ups=3.24, wpb=16384, bsz=32, num_updates=2, lr=0.0001, gnorm=15.339, loss_scale=64, train_wall=0, wall=8\n2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      4 / 7173 loss=15.855, ppl=59276.8, wps=72355.5, ups=4.42, wpb=16384, bsz=32, num_updates=3, lr=0.0001, gnorm=4.189, loss_scale=64, train_wall=0, wall=8\n2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      5 / 7173 loss=14.713, ppl=26858.7, wps=70526.4, ups=4.3, wpb=16384, bsz=32, num_updates=4, lr=0.0001, gnorm=4.751, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      6 / 7173 loss=13.901, ppl=15299.7, wps=73063.5, ups=4.46, wpb=16384, bsz=32, num_updates=5, lr=0.0001, gnorm=4.361, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      7 / 7173 loss=13.312, ppl=10169.5, wps=73559.4, ups=4.49, wpb=16384, bsz=32, num_updates=6, lr=0.0001, gnorm=3.307, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      8 / 7173 loss=12.914, ppl=7720.21, wps=72693.2, ups=4.44, wpb=16384, bsz=32, num_updates=7, lr=0.0001, gnorm=5.473, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      9 / 7173 loss=12.56, ppl=6036.72, wps=73531.2, ups=4.49, wpb=16384, bsz=32, num_updates=8, lr=0.0001, gnorm=6.112, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:12:19 | INFO | train_inner | epoch 001:     10 / 7173 loss=12.116, ppl=4437.77, wps=73187.6, ups=4.47, wpb=16384, bsz=32, num_updates=9, lr=0.0001, gnorm=4.415, loss_scale=64, train_wall=0, wall=10\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D24729295\n\nPulled By: myleott\n\nfbshipit-source-id: beee8bdece3eaa0419a2e813990420411e507c75"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-10-23T07:07:33Z",
        "message": "refactor dataclass related files, add proper types for static checkin\u2026 (#1371)\n\nSummary:\n- refactor dataclass/ hierarchy to make it a bit more sane (while avoiding circular references)\n- add top level FairseqConfig\n- change typehints to reflect the correct config type if it is known\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1371\n\nReviewed By: myleott\n\nDifferential Revision: D24469026\n\nPulled By: alexeib\n\nfbshipit-source-id: 01f68918f761d51ec5216286b8959ad35f41a7b2"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-10-22T23:31:49Z",
        "message": "rename remove_bpe to post_process; add aliasing (#1369)\n\nSummary:\nsome binaries (e.g. speech based ones) used --post-process, some used --remove-bpe. --post-process seems more appropriate as it does more than just remove bpe at the moment. this renames remove_bpe to post_process, adds alias so existing command lines would work and adds checkpoint upgrades so they continue to work also.\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1369\n\nReviewed By: myleott\n\nDifferential Revision: D24465040\n\nPulled By: alexeib\n\nfbshipit-source-id: 1b3e388291ccc403e76e069ef6606b80ead863a7"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-10-20T07:32:26Z",
        "message": "Enable Hydra configs in fairseq (#1343) (#1510)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/1510\n\nthis is the main pr that switches on hydra functionality in fairseq\n\nwe migrate \"args\" object into omegaconf \"DictConfig\" at all legacy entry points\n\nin addition this migrates various components from secondary registries (like bpe encoders and tokenizers) to make the migration smoother\n\ni am going through code that references migrated fairseq components and changing it to inherit from \"Legacy*\" components instead. hopefully tests will catch most of this\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1343\n\nReviewed By: myleott\n\nDifferential Revision: D23973928\n\nPulled By: alexeib\n\nfbshipit-source-id: dd9554981fff51ea75c1ff343874d1d6e61793c9"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-10-19T01:14:51Z",
        "message": "Apply black+isort (#1357)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1357\n\nReviewed By: alexeib\n\nDifferential Revision: D24377772\n\nfbshipit-source-id: 51581af041d42d62166b33a35a1a4228b1a76f0c"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-10-12T17:55:40Z",
        "message": "Support generation with huge pipeline parallel Transformer models (#1297)\n\nSummary:\n## What is this PR about?\n* Support loading sharded checkpoints (loading and saving checkpoints without sharding for 30B models runs OOM)\n* Ability to provide a fixed dictionary for multilingual machine translation to match the dictionary used for training the checkpoint\n* Support generation with PipelineParallelTransformer models\n\n## Testing\n\n```\npython fairseq_cli/generate.py \\\n    /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted  \\\n    --batch-size 1 \\\n    --path /large_exp/angelafan/checkpoints/shru/mmmt_5b_checkpoints_for_master/checkpoint4.pt \\\n    -s en -t es --remove-bpe 'sentencepiece' --beam 5 \\\n    --task translation_multi_simple_epoch \\\n    --lang-pairs /private/home/angelafan/mmt/lang100_bt_pairs.txt \\\n    --decoder-langtok --encoder-langtok src --gen-subset valid --fp16 \\\n    --dataset-impl mmap \\\n    --distributed-world-size 1 --distributed-no-spawn \\\n    --pipeline-model-parallel \\\n    --pipeline-chunks 1 \\\n    --pipeline-encoder-balance '[26]' \\\n    --pipeline-encoder-devices '[0]' \\\n    --pipeline-decoder-balance '[26]' \\\n    --pipeline-decoder-devices '[0]' \\\n    --fixed-dictionary /private/home/shru/projects/fairseq-py-kakaoval-mmt-save-redist-gen/dict.100langs.txt\n2020-09-23 23:18:27 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might ca\nuse misalignment in pretraining and finetuning.\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['af', 'am', 'ar', 'arbt', 'ast', 'az', 'azbt', 'ba', 'be', 'bebt', 'bg', 'bgbt', 'bn', 'bnbt', 'br', 'bs', 'ca', 'ceb', 'cs', 'csb\nt', 'cy', 'da', 'de', 'debt', 'el', 'en', 'es', 'esbt', 'et', 'etbt', 'fa', 'fabt', 'ff', 'fi', 'fr', 'frbt', 'fy', 'ga', 'gd', 'gl', 'gu', 'ha', 'he', 'hebt', 'hi', 'hibt', 'hr', 'ht', 'hu', 'hubt', 'hy', 'hybt', 'id', 'ig', 'ilo', 'is',\n 'it', 'ja', 'jabt', 'jv', 'ka', 'kabt', 'kk', 'km', 'kn', 'ko', 'kobt', 'lb', 'lg', 'ln', 'lo', 'lt', 'lv', 'mg', 'mk', 'mkbt', 'ml', 'mn', 'mr', 'mrbt', 'ms', 'msbt', 'my', 'ne', 'nebt', 'nl', 'no', 'ns', 'oc', 'or', 'pa', 'pl', 'ps', '\npt', 'ro', 'robt', 'ru', 'sd', 'si', 'sk', 'sl', 'so', 'sq', 'sr', 'srbt', 'ss', 'su', 'sv', 'sw', 'ta', 'th', 'tl', 'tn', 'tr', 'trbt', 'uk', 'ur', 'uz', 'vi', 'vibt', 'wo', 'xh', 'yi', 'yo', 'zh', 'zhbt', 'zu']\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [en] dictionary: 128112 types\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [es] dictionary: 128112 types\n2020-09-23 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None\n2020-09-23 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:en-es': 1}\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:en-es src_langtok: 128022; tgt_langtok: 128023\n2020-09-23 23:18:27 | INFO | fairseq.data.data_utils | loaded 4231 examples from: /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted/valid.en-es.en\n2020-09-23 23:18:27 | INFO | fairseq.data.data_utils | loaded 4231 examples from: /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted/valid.en-es.es\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted valid en-es 4231 examples\n2020-09-23 23:18:27 | INFO | fairseq_cli.generate | loading model(s) from /large_exp/angelafan/checkpoints/mmmt_100_langs_new_dict_5b_model/checkpoint4.pt\nbalance=[29,22,1], devices=[0,1,0], chunks=8, checkpoint=always\n2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler order indices time: 0:00:00.004368\n2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler filter_by_size time: 0:00:00.057953\n2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler batch_by_size time: 0:00:01.158644\n2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:01.223290\n2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\nS-2521  __en__ No.\nT-2521  No.\nH-2521  -2.4406352043151855     y no.\nD-2521  -2.4406352043151855     y no.\nP-2521  -5.2677 -4.4289 -0.1922 -2.1578 -0.1565\nS-2261  __en__ Why?\nT-2261  \u00bfPor qu\u00e9?\nH-2261  -1.7077901363372803     \u00bfY por qu\u00e9?\nD-2261  -1.7077901363372803     \u00bfY por qu\u00e9?\nP-2261  -5.2733 -0.7970 -3.7643 -0.5474 -0.3339 -1.0629 -0.1757\n```\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1297\n\nReviewed By: myleott, msbaines\n\nDifferential Revision: D23991647\n\nPulled By: shruti-bh\n\nfbshipit-source-id: 81ea1af64cbe75c8b53e050d2c2339dcb70fe1eb"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-10-09T20:34:59Z",
        "message": "Improve dictionary & checkpoint reading w/ local caching\n\nReviewed By: myleott\n\nDifferential Revision: D24148700\n\nfbshipit-source-id: 666300639243688939e137be748f7b76fc3c21a6"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-09-02T01:17:33Z",
        "message": "Initial support for ZeRO optimizer state sharding (#1259)\n\nSummary:\nFairseqOSS will work with any optimizer and dtype.\n\nTODO(future PR):\n* support reduce instead of all_reduce\n* support gradient sharding\n* support parameter sharding\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1259\n\nTest Plan:\nVerified that checkpoint save and restore work.\n\nVerified that grad_norm, loss, and ppl are identical with and without\nsharding enable.\n\nBefore:\n\n$ fairseq-train --task language_modeling   data-bin/wikitext-103   --save-dir checkpoints/transformer_wikitext-103   --arch transformer_lm --share-decoder-input-output-embed   --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 512 --sample-break-mode none   --max-tokens 2048 --update-freq 16   --max-update 50000  --memory-efficient-fp16 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50\n...\n2020-08-27 22:24:51 | INFO | train_inner | epoch 001:     49 / 394 loss=18.84, ppl=469411, wps=269226, ups=1.03, wpb=262144, bsz=512, num_updates=45, lr=5.72388e-06, gnorm=5.769, loss_scale=8, train_wall=1, wall=68\n2020-08-27 22:24:52 | INFO | train_inner | epoch 001:     50 / 394 loss=18.787, ppl=452312, wps=256992, ups=0.98, wpb=262144, bsz=512, num_updates=46, lr=5.84885e-06, gnorm=5.512, loss_scale=8, train_wall=1, wall=69\n2020-08-27 22:24:53 | INFO | train_inner | epoch 001:     51 / 394 loss=18.74, ppl=437735, wps=259178, ups=0.99, wpb=262144, bsz=512, num_updates=47, lr=5.97383e-06, gnorm=5.298, loss_scale=8, train_wall=1, wall=70\n2020-08-27 22:24:54 | INFO | train_inner | epoch 001:     52 / 394 loss=18.683, ppl=420727, wps=257710, ups=0.98, wpb=262144, bsz=512, num_updates=48, lr=6.0988e-06, gnorm=5.094, loss_scale=8, train_wall=1, wall=71\n2020-08-27 22:24:55 | INFO | train_inner | epoch 001:     53 / 394 loss=18.623, ppl=403794, wps=269279, ups=1.03, wpb=262144, bsz=512, num_updates=49, lr=6.22378e-06, gnorm=4.893, loss_scale=8, train_wall=1, wall=72\n2020-08-27 22:24:56 | INFO | train_inner | epoch 001:     54 / 394 loss=18.574, ppl=390255, wps=264616, ups=1.01, wpb=262144, bsz=512, num_updates=50, lr=6.34875e-06, gnorm=4.684, loss_scale=8, train_wall=1, wall=73\n2020-08-27 22:24:56 | INFO | fairseq_cli.train | begin save checkpoint\n2020-08-27 22:24:56 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n2020-08-27 22:24:56 | INFO | train | epoch 001 | loss 19.736 | ppl 873122 | wps 264825 | ups 1.01 | wpb 262144 | bsz 512 | num_updates 50 | lr 6.34875e-06 | gnorm 8.898 | loss_scale 8 | train_wall 66 | wall 73\n2020-08-27 22:24:56 | INFO | fairseq_cli.train | done training in 72.2 seconds\n\nAfter:\n\n$ fairseq-train --task language_modeling   data-bin/wikitext-103   --save-dir checkpoints/transformer_wikitext-103   --arch transformer_lm --share-decoder-input-output-embed   --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 512 --sample-break-mode none   --max-tokens 2048 --update-freq 16   --max-update 50000  --memory-efficient-fp16 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --zero-sharding os\n...\n2020-08-27 22:22:55 | INFO | train_inner | epoch 001:     49 / 394 loss=18.84, ppl=469411, wps=267663, ups=1.02, wpb=262144, bsz=512, num_updates=45, lr=5.72388e-06, gnorm=5.769, loss_scale=8, train_wall=1, wall=68\n2020-08-27 22:22:56 | INFO | train_inner | epoch 001:     50 / 394 loss=18.787, ppl=452312, wps=252797, ups=0.96, wpb=262144, bsz=512, num_updates=46, lr=5.84885e-06, gnorm=5.512, loss_scale=8, train_wall=1, wall=69\n2020-08-27 22:22:57 | INFO | train_inner | epoch 001:     51 / 394 loss=18.74, ppl=437735, wps=267692, ups=1.02, wpb=262144, bsz=512, num_updates=47, lr=5.97383e-06, gnorm=5.298, loss_scale=8, train_wall=1, wall=70\n2020-08-27 22:22:58 | INFO | train_inner | epoch 001:     52 / 394 loss=18.683, ppl=420727, wps=267507, ups=1.02, wpb=262144, bsz=512, num_updates=48, lr=6.0988e-06, gnorm=5.094, loss_scale=8, train_wall=1, wall=71\n2020-08-27 22:22:59 | INFO | train_inner | epoch 001:     53 / 394 loss=18.623, ppl=403794, wps=254410, ups=0.97, wpb=262144, bsz=512, num_updates=49, lr=6.22378e-06, gnorm=4.893, loss_scale=8, train_wall=1, wall=72\n2020-08-27 22:23:00 | INFO | train_inner | epoch 001:     54 / 394 loss=18.574, ppl=390255, wps=268234, ups=1.02, wpb=262144, bsz=512, num_updates=50, lr=6.34875e-06, gnorm=4.684, loss_scale=8, train_wall=1, wall=73\n2020-08-27 22:23:00 | INFO | fairseq_cli.train | begin save checkpoint\n2020-08-27 22:23:00 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n2020-08-27 22:23:00 | INFO | train | epoch 001 | loss 19.736 | ppl 873122 | wps 263570 | ups 1.01 | wpb 262144 | bsz 512 | num_updates 50 | lr 6.34875e-06 | gnorm 8.898 | loss_scale 8 | train_wall 66 | wall 73\n2020-08-27 22:23:00 | INFO | fairseq_cli.train | done training in 72.3 seconds\n\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nReviewed By: myleott\n\nDifferential Revision: D23432082\n\nPulled By: msbaines\n\nfbshipit-source-id: 6a020b25e36a3d9283582b7d89a6a53038e5b181"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-08-14T17:24:51Z",
        "message": "Misc fixes (#2448)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2448\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D23011193\n\nPulled By: myleott\n\nfbshipit-source-id: 1a29481707108e4465aca78ec1581fb79f05efba"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-08-06T17:20:39Z",
        "message": "Multilingual v1: Multilingual Training with multiple bitext and monolingual datasets: add finetuning options\n\nSummary:\nA first version of XLNMT multilingual project code release: Multilingual Training with multiple bitext\n\n- Minor changes to\n    - fairseq/checkpoint_utils.py to add finetuning option instead of using restore_file which will restore from original model when being requeued.\n\nReviewed By: myleott\n\nDifferential Revision: D22483494\n\nfbshipit-source-id: 733300fd6a4d185e561c793ea668047c96f616c6"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-08-04T15:25:50Z",
        "message": "Fixes checkpoint_path while loading a model-parallel checkpoint (#2365)\n\nSummary:\nFixes https://github.com/pytorch/fairseq/issues/2351\n\nPull Request resolved: https://github.com/pytorch/fairseq/pull/2365\n\nReviewed By: pipibjc\n\nDifferential Revision: D22727384\n\nPulled By: myleott\n\nfbshipit-source-id: e2ff703181a6b8f10df9b4ee7aa3f9e128c04b4e"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-05-26T22:59:59Z",
        "message": "Several small fixes (incl. set default --data-buffer-size=10) (#2163)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2163\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D21665601\n\nPulled By: myleott\n\nfbshipit-source-id: 47673ff7f07acf0002c4e28380aa08ff917618ee"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-05-11T17:34:42Z",
        "message": "Generalize moving of tensors to CPU in checkpoints (#2098)\n\nSummary:\nThis is needed for TPUs\nPull Request resolved: https://github.com/pytorch/fairseq/pull/2098\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D21455095\n\nPulled By: myleott\n\nfbshipit-source-id: f0f88e715884f44bc254b71f7694ac90200d92fc"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-04-14T17:58:38Z",
        "message": "update checkpoint mkdir behavior (issue #1986) (#2011)\n\nSummary:\nCreate checkpoint directory in ```save_checkpoint()```instead of ```load_checkpoint()```.\n\nhttps://github.com/pytorch/fairseq/issues/1986\nPull Request resolved: https://github.com/pytorch/fairseq/pull/2011\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D21017208\n\nPulled By: myleott\n\nfbshipit-source-id: 4f76afc1751f987b8ab2c170ca306d07bd5ffe83"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-03-28T03:22:36Z",
        "message": "adding code to load and save model parallel checkpoint (#1119)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1119\n\nReviewed By: myleott\n\nDifferential Revision: D20712488\n\nfbshipit-source-id: 941ef251c9e2deb8933d88188fac56ee8c5be9b7"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-03-11T07:36:45Z",
        "message": "PySpeech TALNet: Convert to JIT and quantize\n\nSummary:\nUpdate the TALNet model so it can be converted to JIT format and quantized by the `jit_pyspeech_nn_model` tool.\n\nThe updated model structure is slight incompatible with the model files saved before (the model structure has the extra parameters `fc_att.weight` and `fc_att.bias`), so old model files must be loaded with `strict=False`. This diff also makes changes to allow `strict=False` where necessary.\n\nAlso renames the options of `jit_pyspeech_nn_model` to make them conform better to convention.\n\nReviewed By: jay-mahadeokar\n\nDifferential Revision: D20369643\n\nfbshipit-source-id: 0950a26abc20d0ae5929c8d0a03f83cab4a59200"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-03-07T17:12:14Z",
        "message": "Fix epoch reporting when restoring checkpoint (#1075)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1075\n\nDifferential Revision: D20322672\n\nPulled By: myleott\n\nfbshipit-source-id: 8845a10b10442bed77670b7740afa271123a3b5d"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-03-05T00:37:24Z",
        "message": "Use 1-based indexing for epochs everywhere (#1053)\n\nSummary:\nWe are somewhat inconsistent in whether we're using 0-based or 1-based indexing for epochs. This should fix things to be 0-based internally, with logging and checkpoint naming still using 1-based indexing.\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1053\n\nReviewed By: spencerp\n\nDifferential Revision: D20160715\n\nPulled By: myleott\n\nfbshipit-source-id: 4ed94f9c371e1bfe29bcfa087fa6756507d6e627"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-02-19T04:07:05Z",
        "message": "tts synthesis script\n\nSummary: add a synth.py to pyspeech to run tts synthesis for a particular text.\n\nDifferential Revision: D19786089\n\nfbshipit-source-id: 2aadd004609dfbcf477e58bc01e2b05df19c0e26"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-01-22T23:36:28Z",
        "message": "fblearner pyspeech manifold migration\n\nSummary:\nwas planning to migrate everything at once. but because pyspeech and tuna diverged from D19060980, it's too complicated to do the migration.\n\nmigrate save_dir to manifold in fblearner, and copy from manifold to gluster after training to keep downstream workflows happy.\n\nReviewed By: zdavid1995\n\nDifferential Revision: D19433205\n\nfbshipit-source-id: bd8d969c001952d85167a63f4d55fe97b93aceb5"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-01-20T20:15:27Z",
        "message": "fix the problem of passing None to format() when val_loss is None (e.\u2026 (#1633)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [x ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ x] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\nPull Request resolved: https://github.com/pytorch/fairseq/pull/1633\n\nDifferential Revision: D19470727\n\nPulled By: myleott\n\nfbshipit-source-id: a0bbefe20b4692306c57104f3e545912e20055e6"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-01-17T00:14:45Z",
        "message": "Switch to Python logging (+ lint) (#1627)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/fairseq/pull/1627\n\nPython logging offers a number of benefits, such as logging timestamps, better\ncross-library compatibility, ability to add multiple output handlers, etc.\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/646\n\nReviewed By: spencerp\n\nDifferential Revision: D15815620\n\nPulled By: myleott\n\nfbshipit-source-id: 5e64e9929b5e4b9dd5bb49bcdf7c510631907134"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-01-16T09:59:15Z",
        "message": "Split from PR#968. add --keep-best-checkpoints (#990)\n\nSummary:\nFixes https://github.com/fairinternal/fairseq-py/issues/968. Split --keep-best-checkpoints from the original request.\nUse scores as the names to save the checkpoints\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/990\n\nDifferential Revision: D19411250\n\nPulled By: MultiPath\n\nfbshipit-source-id: 82b0db614208eee54c9c0e470ad7faa6481747d5"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-12-17T01:22:11Z",
        "message": "More fully deprecate --raw-text and --lazy-load (fixes #1488)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/947\n\nDifferential Revision: D19084273\n\nPulled By: myleott\n\nfbshipit-source-id: de80d9abfac8e3d813a9c9b343b41327c500344e"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-12-06T02:24:56Z",
        "message": "Add wrapper abstraction on top of fvcore PathManager to insulate OSS compatibility\n\nReviewed By: myleott\n\nDifferential Revision: D18736914\n\nfbshipit-source-id: 897ba7463f7a8ebc0969c01c57bfe04fbf9e71c8"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-12-02T21:26:48Z",
        "message": "Apply Black auto-formatting\n\nReviewed By: sujitoc\n\nDifferential Revision: D18738392\n\nfbshipit-source-id: b7b7b75ef97946786c463c1887ef9a8676f030e6"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-11-22T01:50:42Z",
        "message": "Quick fix for Structured Dropout checkpointing (#1406)\n\nSummary:\nHere's a quick fix for https://github.com/pytorch/fairseq/issues/1403.\n\nTo keep it short, this fix allows the user to checkpoint a translation model properly after applying layer pruning on a restored transformer file.\nPull Request resolved: https://github.com/pytorch/fairseq/pull/1406\n\nDifferential Revision: D18637540\n\nPulled By: myleott\n\nfbshipit-source-id: 0f5e91e05e6579f0f459bc5293e9b14cb267322d"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-11-21T00:52:59Z",
        "message": "Refactor data sharding to be specified via caller of task rather than task itself\n\nSummary: Modifying number of shards internally to disable data sharding for batch iteration is dangerous because the caller of these tasks is not limited to fairspeq/train. So therefore we should put the onus of data sharding properly on the caller rather than the task itself.\n\nReviewed By: myleott\n\nDifferential Revision: D18456424\n\nfbshipit-source-id: d46be16c441c50082f9a768d0b259e6c28a4b67b"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-10-30T19:55:54Z",
        "message": "layer drop\n\nSummary: This diff enables layer drop in transformer decoder in production training pipeline (ptt_transformer). It builds on top of the fairseq implementation D18094657 added by Angela Fan, and added additional logic to handle corresponding dropping layers at test time in exported model.\n\nReviewed By: jhcross\n\nDifferential Revision: D18165586\n\nfbshipit-source-id: 373ac00268a25fa9e412edcb483becdfe792d992"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-10-27T19:10:53Z",
        "message": "adding layerdrop code for training, pruning, and readme (#890)\n\nSummary:\nTEST 1: EVALUATION TIME WORKS\nchecked\nachieves correct model perplexity: 18.68\n\nTEST 2: TRAINING NEW MODEL WORKS\nchecked\n\nwithout layerdrop:\n--decoder-layerdrop 0 OR no flag at all\n| epoch 001:     10 / 11201 loss=27.469, nll_loss=27.469, ppl=185799477.36, wps=1764, ups=0, wpb=9216.000, bsz=3.000, num_updates=7, lr=0.0004376, gnorm=25.471, clip=1.000, oom=0.000, loss_scale=8.000, wall=37, train_wall=30\n| epoch 001:     20 / 11201 loss=27.443, nll_loss=27.443, ppl=182500427.22, wps=2449, ups=0, wpb=9216.000, bsz=3.000, num_updates=17, lr=0.0010626, gnorm=25.273, clip=1.000, oom=0.000, loss_scale=8.000, wall=64, train_wall=57\n| epoch 001:     30 / 11201 loss=27.404, nll_loss=27.404, ppl=177612215.78, wps=2720, ups=0, wpb=9216.000, bsz=3.000, num_updates=27, lr=0.0016876, gnorm=25.136, clip=1.000, oom=0.000, loss_scale=8.000, wall=91, train_wall=84\n| epoch 001:     40 / 11201 loss=27.009, nll_loss=27.009, ppl=135079983.00, wps=2865, ups=0, wpb=9216.000, bsz=3.000, num_updates=37, lr=0.0023126, gnorm=24.311, clip=1.000, oom=0.000, loss_scale=8.000, wall=119, train_wall=112\n| epoch 001:     50 / 11201 loss=26.418, nll_loss=26.418, ppl=89680259.41, wps=2952, ups=0, wpb=9216.000, bsz=3.000, num_updates=47, lr=0.0029376, gnorm=22.775, clip=1.000, oom=0.000, loss_scale=8.000, wall=147, train_wall=140\n\nwith layerdrop (regularization effect should be seen in PPL):\n--decoder-layerdrop 0.2\n\n| epoch 001:     10 / 11201 loss=25.186, nll_loss=25.186, ppl=38182937.27, wps=2428, ups=0, wpb=9216.000, bsz=3.000, num_updates=8, lr=0.0005001, gnorm=17.082, clip=1.000, oom=0.000, loss_scale=16.000, wall=30, train_wall=24\n| epoch 001:     20 / 11201 loss=25.270, nll_loss=25.270, ppl=40451933.50, wps=3173, ups=0, wpb=9216.000, bsz=3.000, num_updates=18, lr=0.0011251, gnorm=17.162, clip=1.000, oom=0.000, loss_scale=16.000, wall=52, train_wall=45\n| epoch 001:     30 / 11201 loss=25.349, nll_loss=25.349, ppl=42752256.68, wps=3454, ups=0, wpb=9216.000, bsz=3.000, num_updates=28, lr=0.0017501, gnorm=17.370, clip=1.000, oom=0.000, loss_scale=16.000, wall=75, train_wall=68\n| epoch 001:     40 / 11201 loss=25.115, nll_loss=25.115, ppl=36343806.30, wps=3619, ups=0, wpb=9216.000, bsz=3.000, num_updates=38, lr=0.0023751, gnorm=16.945, clip=1.000, oom=0.000, loss_scale=16.000, wall=97, train_wall=90\n| epoch 001:     50 / 11201 loss=24.804, nll_loss=24.804, ppl=29284345.78, wps=3716, ups=0, wpb=9216.000, bsz=3.000, num_updates=48, lr=0.0030001, gnorm=16.406, clip=1.000, oom=0.000, loss_scale=16.000, wall=119, train_wall=112\n\nTEST 3: PICKING UP TRAINING FROM EXISTING MODEL\nchecked\n\n| loaded checkpoint /checkpoint/angelafan/structured_0.1_block_8_sd02/checkpoint_last.pt (epoch 272 @ 381066 updates)\n| loading train data for epoch 272\n| loaded 1801350 examples from: /private/home/angelafan/lm_work/fairseq-py/data-bin/wikitext-103/train\n\nTEST 4: EVALUATING EXISTING BERT MODEL REPROS RESULTS\n| [input] dictionary: 50265 types\n| [label] dictionary: 9 types\n| Accuracy:  0.9231651376146789\nachieves correct accuracy on SST2 for this model\n\nTEST 5: TRAINING NEW BERT MODEL WORKS\nchecked and works\n\nTEST 6: NMT\n\nwithout layerdrop\n--encoder-layerdrop 0 --decoder-layerdrop 0 OR combinations of flag specified and not specified\n\n| epoch 001:     10 / 92203 loss=15.820, nll_loss=15.830, ppl=58267.93, wps=4902, ups=0, wpb=1477.818, bsz=51.636, num_updates=11, lr=1.47473e-06, gnorm=7.207, clip=0.000, oom=0.000, loss_scale=128.000, wall=60, train_wall=3\n| epoch 001:     20 / 92203 loss=15.523, nll_loss=15.501, ppl=46359.29, wps=5037, ups=0, wpb=1496.476, bsz=45.333, num_updates=21, lr=2.72448e-06, gnorm=6.869, clip=0.000, oom=0.000, loss_scale=128.000, wall=63, train_wall=6\n| epoch 001:     30 / 92203 loss=15.185, nll_loss=15.123, ppl=35695.79, wps=5085, ups=0, wpb=1519.355, bsz=44.645, num_updates=31, lr=3.97423e-06, gnorm=6.186, clip=0.000, oom=0.000, loss_scale=128.000, wall=66, train_wall=9\n| epoch 001:     40 / 92203 loss=14.940, nll_loss=14.849, ppl=29505.60, wps=5116, ups=1, wpb=1521.244, bsz=42.927, num_updates=41, lr=5.22398e-06, gnorm=5.610, clip=0.000, oom=0.000, loss_scale=128.000, wall=69, train_wall=12\n| epoch 001:     50 / 92203 loss=14.745, nll_loss=14.630, ppl=25346.87, wps=5070, ups=1, wpb=1507.961, bsz=41.725, num_updates=51, lr=6.47373e-06, gnorm=5.104, clip=0.000, oom=0.000, loss_scale=128.000, wall=71, train_wall=15\n\nwith layerdrop (regularization effect should be seen in PPL)\n\nA) works with --encoder-layerdrop 0.2 --decoder-layerdrop 0.2\nB) works with different settings --encoder-layerdrop 0.3 --decoder-layerdrop 0.5\nC) works with one on and one off --encoder-layerdrop 0.2 --decoder-layerdrop 0\n\n| epoch 001:     10 / 92203 loss=15.817, nll_loss=15.828, ppl=58158.54, wps=5355, ups=0, wpb=1477.818, bsz=51.636, num_updates=11, lr=1.47473e-06, gnorm=6.959, clip=0.000, oom=0.000, loss_scale=128.000, wall=59, train_wall=3\n| epoch 001:     20 / 92203 loss=15.650, nll_loss=15.641, ppl=51111.63, wps=5515, ups=0, wpb=1496.476, bsz=45.333, num_updates=21, lr=2.72448e-06, gnorm=6.825, clip=0.000, oom=0.000, loss_scale=128.000, wall=61, train_wall=6\n| epoch 001:     30 / 92203 loss=15.440, nll_loss=15.408, ppl=43491.58, wps=5602, ups=0, wpb=1519.355, bsz=44.645, num_updates=31, lr=3.97423e-06, gnorm=6.576, clip=0.000, oom=0.000, loss_scale=128.000, wall=64, train_wall=8\n| epoch 001:     40 / 92203 loss=15.247, nll_loss=15.193, ppl=37457.14, wps=5676, ups=1, wpb=1521.244, bsz=42.927, num_updates=41, lr=5.22398e-06, gnorm=6.124, clip=0.000, oom=0.000, loss_scale=128.000, wall=67, train_wall=11\n| epoch 001:     50 / 92203 loss=15.055, nll_loss=14.977, ppl=32259.92, wps=5598, ups=1, wpb=1507.961, bsz=41.725, num_updates=51, lr=6.47373e-06, gnorm=5.661, clip=0.000, oom=0.000, loss_scale=128.000, wall=69, train_wall=14\n\nTEST 7: PRUNING TESTCASES\n\nA) after adding the pruning flags, model can evaluate as a full model\nchecked, reaches correct PPL\nnum. model params: 246933504\n| Evaluated 217646 tokens in 196.3s (1108.99 tokens/s)\n| Loss: 2.9275, Perplexity: 18.68\n\nB) after adding pruning flags, model can be pruned. this works with multiple flag settings\nchecked three cases:\nnum. model params: 146163712\n| Evaluated 217646 tokens in 106.0s (2054.07 tokens/s)\n| Loss: 3.0932, Perplexity: 22.05\n\nnum. model params: 209144832\n| Evaluated 217646 tokens in 162.8s (1336.99 tokens/s)\n| Loss: 2.9526, Perplexity: 19.16\n\nC) model can pick up training if you want to finetune the pruned model\nchecked:\n| loading train data for epoch 272\n| loaded 1801350 examples from: /private/home/angelafan/lm_work/fairseq-py/data-bin/wikitext-103/train\n| WARNING: overflow detected, setting loss scale to: 64.0\n| WARNING: overflow detected, setting loss scale to: 32.0\n| epoch 272:   1500 / 5601 loss=5.015, nll_loss=5.015, ppl=32.33, wps=11598, ups=1, wpb=18432.000, bsz=6.000, num_updates=98, lr=0.0061251, gnorm=0.613, clip=1.000, oom=0.000, loss_scale=32.000, wall=156, train_wall=252396\n\nD) works with BERT\nchecked:\nwithout specifying any flags, reproduces the correct standard accuracy\nwith flags, produces the correct pruned accuracy\n\n| [input] dictionary: 50265 types\n| [label] dictionary: 9 types\n| Accuracy:  0.9231651376146789\n\n| [input] dictionary: 50265 types\n| [label] dictionary: 9 types\n| Pruning model to specified layer configuration - this works best if the model was trained with LayerDrop\n| Accuracy:  0.9220183486238532\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/890\n\nReviewed By: edunov\n\nDifferential Revision: D18094657\n\nPulled By: huihuifan\n\nfbshipit-source-id: 2bbaa2ff0039e906782694fc2038b8c17a8693e7"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-10-12T04:53:11Z",
        "message": "Added option to save checkpoints using Path Manager.\n\nSummary: Added option to save checkpoints using Path Manager.\n\nReviewed By: hudeven\n\nDifferential Revision: D17392754\n\nfbshipit-source-id: 4b8e556ef8455a1548e5a083d779ed809cd785be"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-10-09T21:53:27Z",
        "message": "Fix data loading memory issue in pyspeech\n\nSummary:\nWe currently shard data when creating the batch iterator. This means we first load all indicese/frame lengths/handles into memory, and then do the sharding. This makes it impossible to train on large datasets with a high amount of workers  because each worker will need to load the entire dataset into memory. For training on a million hours of data (i.e. semi-supervised or unsupervised approaches) this data loading just makes it flat out impossible to use 8 GPU's.\n\n3 changes:\n\n1. This diff modifies the data loading such that we do the sharding while we read the handles file, rather than later. This modification is done on a task-by-task basis, since the task specifies how the data is loaded. I've tried to make the code compatible with both sharding during handle loading and sharding during batch iteration. I've currently only done the sharding during handle loading for the aligned_training task.\n\n2. To support data sharding at data loading time and the requirement that all shards must have exactly the same # of batches, I've added a method to do this synchronization where all shards with too many batches would just truncate the extra ones, similar to what we already do.\n\n2. In fairspeq/train.py, we are actually loading the training dataset and batch iterator twice, once in train.py and once when loading the checkpoint (which we always do regardless if there is a checkpoint). This means double the loading time which can be painful for very large files. I've removed the extraneous loading in this diff as well.\n\nReviewed By: yqwangustc\n\nDifferential Revision: D17750715\n\nfbshipit-source-id: 0e6e3d363525fa5661f1c784303390ea13f46377"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-09-20T16:34:58Z",
        "message": "added multilingual masked LM training (#849)\n\nSummary:\nThe multilingual-RoBERTa training is working with aconneau XLM data.\n\nTwo pieces remaining:\n\n1) `XLM` limits batch to be from same language, I am not 100% sure about the reason for that, but should be easy to implement, basically we can add `batch_by_size_and_language` instead of default `batch_by_size` function. If it's not critical, I would want to leave it out as it keeps the code very clean and simple.\n\n2) `sample_ratio` in `ConcatDataset` works with `int` by tiling the datasets based on ratio. Currently I am handling it by sounding off the ratio to `first decimal` and then multiplying by `10`. We can see if some such simple heuristics are good enough, there are other options (we can talk about them offline).\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/849\n\nDifferential Revision: D17162460\n\nfbshipit-source-id: d967f3d872f7a1f0aa4ea418bd362b68af9e432f"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-08-21T20:43:01Z",
        "message": "Parameterized criterions (#808)\n\nSummary:\nSupport criterion with parameters, such as AutoSegmentationCriterion (ASG) used in wav2letter which has a transition matrix parameter. This is needed to integrate wav2letter's ASG into PySpeech.\n\nWith this diff, parameters in criterions will be:\n(1) updated by optimizers, with a configurable learning rate\n(2) saved and loaded from checkpoints, preserving backward compatibility for criterions without parameters\n(3) synchronized across nodes in distributed training.\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/808\n\nReviewed By: jcai1\n\nDifferential Revision: D16934097\n\nPulled By: okhonko\n\nfbshipit-source-id: 121ec9382459385c6f9cbef3a8274bec1a434038"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-08-12T16:08:16Z",
        "message": "Lint\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/817\n\nDifferential Revision: D16762905\n\nPulled By: myleott\n\nfbshipit-source-id: d920595bec44ed26b72dfc6fbc15c0aa107b4e56"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-08-12T14:17:21Z",
        "message": "Update --restore-file logic (partially fixes #999)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1007\n\nDifferential Revision: D16762490\n\nPulled By: myleott\n\nfbshipit-source-id: d67137bcf581887850323d188bb4ea643a35ac9e"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-08-10T15:16:50Z",
        "message": "Add WSC task and criterion\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1004\n\nDifferential Revision: D16751443\n\nPulled By: myleott\n\nfbshipit-source-id: f70acd6c7be6d69da45b5b32fe4c4eff021539ab"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-08-01T12:55:57Z",
        "message": "Update PyTorch Hub interface\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/782\n\nDifferential Revision: D16542256\n\nPulled By: myleott\n\nfbshipit-source-id: ea3279e7a1ce4687a5914f32b76787c419be1ffa"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-07-30T14:48:23Z",
        "message": "Relicense fairseq under MIT license (#786)\n\nSummary:\nThe previous BSD+PATENTS license was controversial. We have been\napproved to relicense fairseq under the MIT license.\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/786\n\nDifferential Revision: D16560654\n\nPulled By: myleott\n\nfbshipit-source-id: f78b1beb4f2895dd7b9bfc79f5f952a2bfb94034"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-07-29T23:06:26Z",
        "message": "adding glue data preprocessing scripts (#771)\n\nSummary:\n1) Added glue data pre-processing script.\n2) updated README with usage.\n\nTODO:\n1) releasing fairseq dictionary and remove hardcoded path.\n2) remove hard-coded path for bpe-encoding,\n\nmyleott what do you recommend for above TODOs?\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/771\n\nReviewed By: myleott\n\nDifferential Revision: D16547679\n\nPulled By: myleott\n\nfbshipit-source-id: 6a6562d9b6215523d048fdf3daee63ffac21e231"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-07-24T23:59:07Z",
        "message": "check save_dir before beginning training\n\nSummary: I sadly discovery that my checkpoint directory wasn't globally readable after 8 hours of training. Adding this check at the beginning of train loop to keep that from happening again!\n\nReviewed By: myleott\n\nDifferential Revision: D16455394\n\nfbshipit-source-id: 35959aa058150b2afb63710c468d01ebc8a12b0c"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-07-19T20:13:43Z",
        "message": "Rename _load_model_ensemble -> load_model_ensemble_and_task\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/738\n\nDifferential Revision: D16377803\n\nPulled By: myleott\n\nfbshipit-source-id: 6beb2f78e7464b70ff65a965d2b747cdca0ca951"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-07-01T21:10:49Z",
        "message": "Fixes checkpointing bug introduced in 89e077c\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/847\n\nDifferential Revision: D16075498\n\nPulled By: myleott\n\nfbshipit-source-id: 62e27a8c4764f53f181c502674dfab1e6b0537e2"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-06-30T18:35:33Z",
        "message": "Add additional options for configuring writing of checkpoints\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/697\n\nDifferential Revision: D16068465\n\nPulled By: myleott\n\nfbshipit-source-id: c2563c3c682e7e8406e6d7c8e895d8afbec551eb"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-06-11T18:19:58Z",
        "message": "Automatically fill in default values from add_args\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/797\n\nDifferential Revision: D15761071\n\nPulled By: myleott\n\nfbshipit-source-id: 257d4a2297e83da7e59baed154dbafd6bfe614bf"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-05-30T18:41:40Z",
        "message": "Add --reset-dataloader\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/613\n\nDifferential Revision: D15541384\n\nPulled By: myleott\n\nfbshipit-source-id: ef2c0b0a51cdf37af2ccff0546f524d49f87e65d"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-05-17T21:25:56Z",
        "message": "Small features + lint\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/588\n\nDifferential Revision: D15389638\n\nPulled By: myleott\n\nfbshipit-source-id: 4632ce22d51dc2c74d250bae999630095d849701"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-05-17T04:03:08Z",
        "message": "Clean up sharded train iterator\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/586\n\nDifferential Revision: D15372949\n\nPulled By: myleott\n\nfbshipit-source-id: c1cf1c645e8d55fc8568f23a47c45677ac9ab1da"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-05-14T19:57:12Z",
        "message": "Move save/load checkpoint functions to utils\n\nSummary:\nMove `load_checkpoint`, `save_checkpoint` and `reload_train` from train.py to checkpoint_utils.py\nMove `get_perplexity` from train.py to utils.py.\nThis will make train.py lighter and allow us to reuse all this utils functionality when fairseq is used as external library.\n\nReviewed By: myleott\n\nDifferential Revision: D15289607\n\nfbshipit-source-id: 4b7c95225ac22e402bcda3497811361809110df1"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-05-11T14:56:45Z",
        "message": "Add missing options to TransformerDecoderLayer\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/560\n\nDifferential Revision: D15260838\n\nPulled By: myleott\n\nfbshipit-source-id: 5f80dd82775c10ce46a3e1c451ccaf0ef55bfa31"
    },
    {
        "repo_url": "github.com/freewym/espresso",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-05-06T07:17:45Z",
        "message": "Load pretrained encoder or decoder (#705)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/fairseq/pull/705\n\nThis adds functionality in fairseq to load a pretrained encoder or decoder from another pretrained model into the current model.\n\nReviewed By: jmp84\n\nDifferential Revision: D15207084\n\nfbshipit-source-id: 32a710ff77389928e20793c71d312863df9dd8ae"
    },
    {
        "repo_url": "github.com/microsoft/NeuralSpeech",
        "filepath": "VideoDubber/fairseq/checkpoint_utils.py",
        "commit_date": "2023-07-30T12:50:32Z",
        "message": "add"
    },
    {
        "repo_url": "github.com/OFA-Sys/ONE-PEACE",
        "filepath": "fairseq/fairseq/checkpoint_utils.py",
        "commit_date": "2023-05-23T05:53:57Z",
        "message": "code cleanup"
    },
    {
        "repo_url": "github.com/OFA-Sys/ONE-PEACE",
        "filepath": "fairseq/fairseq/checkpoint_utils.py",
        "commit_date": "2023-05-19T06:52:54Z",
        "message": "code init"
    },
    {
        "repo_url": "github.com/Victorwz/LongMem",
        "filepath": "fairseq/fairseq/checkpoint_utils.py",
        "commit_date": "2023-06-13T12:02:52Z",
        "message": "initial"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2022-07-06T12:58:32Z",
        "message": "[nllb] No Language Left Behind @ 200\n\nCo-authored-by: Shruti Bhosale <7892195+shruti-bh@users.noreply.github.com>\nCo-authored-by: Anna Sun <13106449+annasun28@users.noreply.github.com>\nCo-authored-by: Maha Elbayad <elbayadm@users.noreply.github.com>\nCo-authored-by: Jean Maillard <107696+jeanm@users.noreply.github.com>\nCo-authored-by: James Cross <10384938+jhcross@users.noreply.github.com>\nCo-authored-by: Onur \u00c7elebi <celebio@users.noreply.github.com>\nCo-authored-by: Kevin Heffernan <73017975+heffernankevin@users.noreply.github.com>\nCo-authored-by: Kaushik Ram Sadagopan <29103305+kauterry@users.noreply.github.com>\nCo-authored-by: Angela Fan <3150717+huihuifan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2022-01-21T17:27:49Z",
        "message": "Fix breakage from D33649708\n\nSummary: https://www.internalfb.com/diff/D33649708 (https://github.com/pytorch/fairseq/commit/995c204337d16a6146a433cee360e5a5bfbc9a6f)?src_version_fbid=1030479880843010&dst_version_fbid=247617347518523&transaction_fbid=1601081576900014\n\nReviewed By: alexeib\n\nDifferential Revision: D33696937\n\nfbshipit-source-id: 9a17610e3f4eb3dd2b2131a3f9fb42732a31b47f"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2022-01-20T08:02:16Z",
        "message": "Data2vec prelim (#2929)\n\nSummary:\nPreliminaries for data2vec release, include some minor improvements and bug fixes\n\nMost important change is that we now default to raising an exception when fields in config do not have a corresponding field in the model dataclass\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2929\n\nReviewed By: wnhsu\n\nDifferential Revision: D33649708\n\nPulled By: alexeib\n\nfbshipit-source-id: 629bdb4c361550740b451c570c2005bb956c6fcb"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2022-01-19T03:29:52Z",
        "message": "Decode using EMA model in IPL recipe\n\nSummary: Add option to use the EMA model for decoding in transducer IPL recipe by passing --ipl-decode-ema. Note EMA should be enabled as in the diff D24238379 (https://github.com/pytorch/fairseq/commit/8feccf94412424a4683b01090de36fa77cb4951d) using options --store-ema --ema-start-update and --ema-decay.\n\nReviewed By: cruvadom\n\nDifferential Revision: D31983366\n\nfbshipit-source-id: 2bf63b3f7d1b5fa8804b3a7e9bfab71a463ca957"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2022-01-07T08:39:11Z",
        "message": "Formatting fix: get CI green (#2860)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nApplies `black` and `isort` to files\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2860\n\nReviewed By: Mortimerp9\n\nDifferential Revision: D33456637\n\nPulled By: dianaml0\n\nfbshipit-source-id: 560b8d3a8f589cbecc92d0d21163596b5d47d609"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-12-31T02:38:12Z",
        "message": "add strict option to checkpoint_utils. load_pretrained_component_from_model()\n\nSummary: Add strict option to checkpoint_utils. load_pretrained_component_from_model()\n\nReviewed By: sravyapopuri388\n\nDifferential Revision: D33304224\n\nfbshipit-source-id: 2284a21dfea7810ec212f15daadeeeb45c6dca1b"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-12-17T00:11:19Z",
        "message": "formatting fix (#2816)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nfix `black` failures\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2816\n\nReviewed By: alexeib\n\nDifferential Revision: D33172615\n\nPulled By: dianaml0\n\nfbshipit-source-id: 36b141f42941670f1bfa981041d878042feb0428"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-12-11T00:55:12Z",
        "message": "Add loading from HuggingFace Hub\n\nSummary: Add loading from HuggingFace Hub. Revised from and to replace D32697723 (accepted).\n\nReviewed By: pipibjc, dianaml0\n\nDifferential Revision: D32964041\n\nfbshipit-source-id: 39676aa0ecb10454ae76b70968d5abe96ab6da54"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-11-29T20:32:59Z",
        "message": "Add linting with black (#2678)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2678\n\nReviewed By: Mortimerp9\n\nDifferential Revision: D32653381\n\nPulled By: dianaml0\n\nfbshipit-source-id: 2810d14867cd7d64f4d340740e2b590b82de47fe"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-11-19T00:38:31Z",
        "message": "Merge --use-ontology-for-* into --use-ontology\n\nSummary:\nThere are three options for the ontology:\n* `--use-ontology-for-training`\n* `--use-ontology-for-validation`\n* `--use-ontology-for-balancing`\n\nThe first two must always be set together.\n\nIn the past, I observed that it's best not to use ontology for data balancing even if we use ontology for training and validation. But now I no longer observe this.\n\nTherefore, I'm merging all these three options into one (`--use-ontology`).\n\nIn addition, I'm also moving the logic of avoiding loading teacher models out of `checkpoint_utils.py`. If you want to load a student model without loading its teachers (e.g. for prediction only), specify `arg_overrides={\"ignore_teachers\": True}` when calling `load_model_ensemble`.\n\nReviewed By: xiaoxiao26\n\nDifferential Revision: D32518830\n\nfbshipit-source-id: 103c6458f7927ec5ca7470109c8f956c00f514a2"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-10-23T02:51:09Z",
        "message": "set num_update before loading state dict (#2491)\n\nSummary:\n## What does this PR do?\nSet `model.num_updates` in `load_model_ensemble_and_task()` before loading `state_dict`, like what's done in `fairseq/trainer.py`, because a model's `state_dict` may depend on `num_update`.\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2491\n\nReviewed By: xuqiantong\n\nDifferential Revision: D31863368\n\nPulled By: wnhsu\n\nfbshipit-source-id: c70051f898819cc43b02c9f5765429e9f194aed5"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-09-13T20:20:35Z",
        "message": "Back out \"Fairseq needs to store and load metadata from model state_dict\" (#3861)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/fairseq/pull/3861\n\nbackout fairseq changes. fix with a suggested, more optimal changes in checkopint utils.\n\nReviewed By: zhengwy888\n\nDifferential Revision: D30886481\n\nfbshipit-source-id: 12b6dd4d5107ab4371b73a58d9a044a17c733260"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-09-09T01:18:30Z",
        "message": "Fairseq needs to store and load metadata from model state_dict\n\nSummary:\n## TL;DR\nFairseq checkpoint saving and loading should mirror torch's checkpoint by saving and loading \"state_dict()._metadata\".\n\n## Long Story:\n\n#### What happened:\nDuring model loading and saving, Quantization-aware-training models in Pytorch encounters a weird bug that says state_dict \"fake_weight_quant.weight.min_val\" is mismatched to \"min_vals\".\n\n#### What was the reason:\n- We found the issue in that torch uses state_dict()._metadata to store module._version, but the metadata was never store in checkpoint, nor are they loaded during checkpoint loading in fairseq.\n\nReviewed By: frankseide\n\nDifferential Revision: D30649933\n\nfbshipit-source-id: ce262486b9b95fbcece463fa05c4e1903d4232d7"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-09-01T19:29:51Z",
        "message": "EMA\n\nSummary:\nAdds Exponential moving average (EMA) model for Kaizen semi-supervised training https://arxiv.org/abs/2106.07759\n\n1. Add `ema.store_ema` to enable storing EMA. EMA will be written to extra_state in the state dict while saving checkpoint.\n2. `ema.ema_start_update` to control when the EMA starts accumulating\n3. Tasks can use `uses_ema` property to decide if the EMA should be passed to the task. (Default is False)\n4. `load_ema_from_checkpoint` can be used to load EMA model in place of the model to be used for evalutation. Pyspeech has eval-ema option for this.\n\n```\nThis module has the EMA class used to store a copy of the exponentially decayed\nmodel params.\n\nTypical usage of EMA class involves initializing an object using an existing\nmodel (random or from a seed model) and setting the config like ema_decay,\nema_start_update which determine how the EMA model is updated. After every\nupdate of the model i.e. at the end of the train_step, the EMA should be updated\nby passing the new model to the EMA.step function. The EMA model state dict\ncan be stored in the extra state under the key of \"ema\" and dumped\ninto a checkpoint and loaded. The EMA object can be passed to tasks\nby setting task.uses_ema property.\nEMA is a smoothed/ensemble model which might have better performance\nwhen used for inference or further fine-tuning. EMA class has a\nreverse function to load the EMA params into a model and use it\nlike a regular model.\n```\n\nReviewed By: cruvadom\n\nDifferential Revision: D24238379\n\nfbshipit-source-id: 879d3ba5070a614b7d365f9503af357001e875b2"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-08-02T21:36:32Z",
        "message": "fixing checkpoint config upgrade for generation print_alignment (#2125)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes config upgrade conditions for upgrading generation. print_alignment\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2125\n\nReviewed By: myleott\n\nDifferential Revision: D30049140\n\nPulled By: jingfeidu\n\nfbshipit-source-id: e613821e94d0cdb876c35bc6e3fede7affbf4628"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-29T23:30:41Z",
        "message": "seed random suffix in checkpoint to be consistent across shards\n\nSummary:\nCurrently the random suffix for saving sharded checkpoints can be different for each shard when training with FSDP and use_sharded_state=True. This makes it difficult for downstream applications to load the checkpoints properly, since each shard may have different suffixes.\n\nThis diff seeds the random suffix to be consistent across shards\n\nReviewed By: zhengwy888\n\nDifferential Revision: D29951167\n\nfbshipit-source-id: 65749357e62a28978f3b46b71767204a508e1f61"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-29T23:30:41Z",
        "message": "use pathmanager to delete old checkpoints\n\nSummary: --keep-best-checkpoints doesn't work for Manifold paths because the current code assumes normal paths. Lets use  PathManager to properly remove them.\n\nReviewed By: myleott, sshleifer\n\nDifferential Revision: D29947965\n\nfbshipit-source-id: 237a7b5aaa8293bb203ad05937decdf3b3ae2fc0"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-29T23:30:40Z",
        "message": "use suffix when saving best checkpoints with metric\n\nSummary: This is needed when training with FSDP + sharded state, as the checkpoints should have `-shard0.pt`\n\nReviewed By: sshleifer\n\nDifferential Revision: D29947728\n\nfbshipit-source-id: d2cb7c23e1e6d027d115cb734e2f2f1c34239eba"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-09T23:25:02Z",
        "message": "Roll back os.path.abspath change\n\nReviewed By: donhusa\n\nDifferential Revision: D29641968\n\nfbshipit-source-id: eca379158055f3e38e9c053b06db56842265e53a"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-08T22:27:57Z",
        "message": "Fix static container (#2036)\n\nSummary:\nfixes StatefulContainer being static which is a problem when you load a checkpoint with task that already has the same keys in the container\n\nalso print full path to checkpoints when saving (useful with hydra) and crash if repeatedly failing to save a checkpoint\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2036\n\nReviewed By: arbabu123\n\nDifferential Revision: D29608430\n\nPulled By: alexeib\n\nfbshipit-source-id: 1b65c8f839e02de9110af3ec53f1e7d48a4908f7"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-06T22:07:31Z",
        "message": "fixes tests/test_train.py to mock checkpoint.save_dir config node (#3675)\n\nSummary:\n## What does this PR do?\nSome downstream users reported that errors when passing Namespace to load_checkpoint().\n\nA recent change made the assumption that the passed object is dict like (dict or DictConfig) that have a get function.\nThis changes that and make sure the mocked config have checkpoint.save_dir to allow the test to run.\n\nPull Request resolved: https://github.com/pytorch/fairseq/pull/3675\n\nReviewed By: omry\n\nDifferential Revision: D29564805\n\nPulled By: lematt1991\n\nfbshipit-source-id: 89308811da382667f6c5d3152ee2d6480416ee62"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-01T13:37:47Z",
        "message": "Hydra 1.1 compatibility: Use an explicit schema for the primary config (#3659)\n\nSummary:\n## What does this PR do?\nFixes compatibility with Hydra 1.1.\nThe result is compatible with both Hydra 1.0 and Hydra 1.1, and will allow a smoother migration to Hydra 1.1.\n\nAt this point I am not yet removing the restriction on the Hydra version from setup.py:\n1. It depends on some Hydra 1.1 changes that are not yet released (It will be compatible with 1.1.1).\n2. Upgrading will result in deprecation warnings, and fixing them will break compatibility with Hydra 1.0.\n\nThere will be some followup to make the code fully compatible with 1.1 once Hydra 1.1 is the default version in fbcode.\n\nPull Request resolved: https://github.com/pytorch/fairseq/pull/3659\n\nReviewed By: omry\n\nDifferential Revision: D29498036\n\nPulled By: lematt1991\n\nfbshipit-source-id: 96999cde5daad6749ef4d3ddf6a36a1e984ff201"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-06-15T21:09:52Z",
        "message": "Check attributes in trainer and checkpoint loading before using them (#1970)\n\nSummary:\n## What does this PR do?\nFixes None exception when some attributes in  don't exist in cfg.\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1970\n\nReviewed By: alexeib\n\nDifferential Revision: D29140036\n\nPulled By: hikushalhere\n\nfbshipit-source-id: 7d941bcae6bb000c281a43ca2cd0876a49912ab9"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-06-04T23:19:56Z",
        "message": "Fix loading some TALNet models\n\nSummary:\nD28728718 cleaned up the \"kd_binary_cross_entropy\" criterion, but this caused loading old models trained with this criterion to fail.\nThis diff replaces the \"kd_binary_cross_entropy\" criterion with the \"wav2vec\" criterion when loading models, and fixes this error.\n\nIt also removes the \"log_keys\" argument if it's `None`. Some criteria (e.g. wav2vec) require this argument to be a list, and will supply a default value of `[]` when it's absent. The presence of the `None` value prevents the use of this default value and causes an error.\n\nDifferential Revision: D28901263\n\nfbshipit-source-id: 9b33aed35e76d2c734d1d4e2cbca1ff193a8c920"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-06-04T00:49:12Z",
        "message": "Teacher-student learning for TALNet\n\nSummary:\nThis diff implements teacher-student learning for TALNet.\n\nThree classes take part in the teacher-student learning:\n* The task loads the teacher models;\n* The model generates predictions using the teacher models, and mixes them with the original targets;\n* The `Wav2VecCriterion` reads the mixed targets to compute the loss. However, it still uses the original targets to compute the MAP and MAUC metrics.\n\nThere are two types of teachers:\n* Static teachers: a file that stores predictions on training data which have been produced by running a model offline;\n* Dynamic teachers: model files that are loaded at the beginning of training and executed on the fly to produce predictions.\n\nWe actually no longer use static teachers. The code about static teachers are copied over from the `KnowledgeDistillationBinaryCrossEntropyCriterion` class. This class will be cleaned up in D28728718.\n\nThe teacher models are stored in the task object, and will not be saved into checkpoints.\n\nReviewed By: alexeib\n\nDifferential Revision: D28728707\n\nfbshipit-source-id: 0fcfc00db2e7194a6f7ee687cad9fa72e82a028b"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-05-26T00:45:51Z",
        "message": "support FSDP sharded_state checkpoint loading during inference\n\nSummary:\nusing the very useful feature added by QuentinDuval https://github.com/facebookresearch/fairscale/pull/683/files , we can consolidate sharded states into a full regular states. this allows inferences on sharded state almost transparently.\n\nThe main complexity comes from trying to be smart about what kind of checkpoint the user wants to load. not sure if this is over-engineering\n1. if the file checkpoint-shard0.pt exists, and `--checkpoint-shard-count` is > 1, then we load sharded FSDP checkpoint\n2. if checkpoint-shard0.pt exists but --checkpoint-shard-count=1, we load consolidated FSDP checkpoint\n3. if checkpoint-shard0.pt does not exist, but --checkpoint-shard-count > 1, we load model parallel checkpoint\n4. otherwise we are loading a single, plain checkpoint.\n\nIn theory we could be even smarter and load shard0.pt to check how many more checkpoints are needed. this is not implemented, though it will save the user having to specify --checkpoint-shard-count.\n\nReviewed By: sshleifer\n\nDifferential Revision: D28563441\n\nfbshipit-source-id: dcafcaa7c9eaf5c9ff94f55c16bb3424c98dfa59"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-05-22T07:22:05Z",
        "message": "fixing s2t transformer and N-best checkpoint saving\n\nSummary:\n- fixing the default value for `encoder_freezing_updates` in s2t transformer\n- fixing N-best checkpoint saving: the previous implementation compares the new checkpoint with only the previous best one but not the previous N best ones. This leads to suboptimal results on N-best checkpoint averaging.\n\nReviewed By: jmp84\n\nDifferential Revision: D28546493\n\nfbshipit-source-id: 44ec6d5ab49347f392d71269c5dcfd154b00c11e"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-05-21T23:18:21Z",
        "message": "attempt to make non-sharded FSDP checkpoint behave like regular checkpoint\n\nSummary:\noverall just wondering if feature is desirable. if it is, the next diff which supports loading sharded checkpoint into a consolidated state dict cleaner.\n\na couple advantages\n1. allows resuming from other DDP trainers.\n2. allows resuming into other DDP trainers. or FSDP of a different configuration.\n3. none-sharded FSDP checkpoint can be loaded with regular load_model_ensemble_and_task()\n\nFor old training workflow that's not using `--use-sharded-state`, please rename the checkpoint to remove the \"-shard0\" for resuming training.\n\nReviewed By: sshleifer\n\nDifferential Revision: D28563032\n\nfbshipit-source-id: ced72bed969319ab6306059721f56e29b2c3d892"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-04-14T08:50:07Z",
        "message": "add keep_interval_updates_pattern\n\nSummary:\nMotivation:\n\nI want to save checkpoints frequently, due to unreliable jobs in FB cluster that restart frequently. I want to do this without spamming Manifold storage, but still save some historical checkpoints (i.e. every 10k updates), so I can track how WER evolves over time.\n\nTo save frequently, I can use a small --save-interval-updates.\n\nTo delete old checkpoints to save storage, I can use --keep-interval-updates.\n\nHowever, this deletes all old checkpoints. This is where --keep-interval-updates-pattern comes in. If I now do:\n\n```\n--save-interval-updates 1000\n--keep-interval-updates 1\n--keep-interval-updates-pattern 10000\n```\n\nThis will:\n1. checkpoint every 1000 updates so that job restarts don't impact us significantly\n2. keep only the latest checkpoint to avoid saving a bunch of huge models in manifold\n3. make an exception for #2 for every 10k updates so we can track WER over time\n\nReviewed By: myleott\n\nDifferential Revision: D27578403\n\nfbshipit-source-id: 5aec2dc9a22778015f7a3daa017210190af81240"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-04-14T08:50:06Z",
        "message": "enable manifold checkpoints with --keep-interval-updates\n\nSummary: Useful to enable --keep-interval-updates with Manifold checkpoints\n\nReviewed By: myleott\n\nDifferential Revision: D27577116\n\nfbshipit-source-id: 6d4ae5aaccc07ecaed8ba6a333b6ab78b148187a"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-03-30T01:02:50Z",
        "message": "BASE layers (#1654)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1654\n\nReviewed By: myleott\n\nDifferential Revision: D27128074\n\nPulled By: shruti-bh\n\nfbshipit-source-id: ac86d383cd53c9c9bdd946fea839a37b719d95e3"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-03-26T14:18:59Z",
        "message": "FSDP uses new optimizer gathering to save optimizer state (#1744)\n\nSummary:\n- Full unflattened optimizer state dict is in `checkpoints/shard_0.pt`, other checkpoint files do not have the `last_optimizer_state` key.\n- requires master version of fairscale (eventually fairscale>=0.3.3)\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1744\n\nReviewed By: myleott\n\nDifferential Revision: D27342305\n\nPulled By: sshleifer\n\nfbshipit-source-id: 7442b8c6ed01599d8ab0050213e84051f4e98acd"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-03-04T21:32:44Z",
        "message": "Move checkpoint state_dict creation into Trainer (#1666)\n\nSummary:\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1666\n\nContext: the checkpoint saving call stack has become a bit convoluted:\n```\ntrain.py\n+ checkpoint_utils.save_checkpoint\n + trainer.save_checkpoint\n  + checkpoint_utils.save_state\n   + checkpoint_utils.torch_persistent_save\n```\n\nThis diff slightly simplifies the checkpoint saving logic by exposing a `state_dict` method inside the Trainer. This simplifies the call stack to:\n```\ntrain.py\n+ checkpoint_utils.save_checkpoint\n + trainer.save_checkpoint\n  + checkpoint_utils.torch_persistent_save\n```\n\nThis new structure is important for the FullyShardedDataParallel diff (next diff in the stack), since it enables the Trainer to save multiple checkpoints for the different optimizer state shards.\n\nTest Plan:\n- unit tests\n- trained WMT En-De models; confirmed checkpoints save/load properly, resuming from a checkpoint gives identical results\n- `buck test fblearner/flow/projects/langtech/translation:tests` (2 failures are in trunk too): https://www.internalfb.com/intern/testinfra/testconsole/testrun/2533274840914654/\n\nReviewed By: zhengwy888\n\nDifferential Revision: D26771146\n\nPulled By: myleott\n\nfbshipit-source-id: 10f91979cd42205c1d8abcaa9ab56f63eba31e93"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-03-04T05:17:29Z",
        "message": "minor fixes and improvements (#1671)\n\nSummary:\nthere are a few changes here:\n- convert config persisted in checkpoints into a plain dict when saving and back to omegaconf config when loading: this helps avoid compatibility issues between different versions of python, omegaconf, etc\n- update checkpoints that have old print_alignment saved\n- add lr_float to composite optimizer to enable sweeping on lr with auto sweepers like ax\n- fixing some edge cases for config loading\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1671\n\nReviewed By: myleott\n\nDifferential Revision: D26791583\n\nPulled By: alexeib\n\nfbshipit-source-id: 124dec74932052925c43b6a93130f4428803cb46"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-03-02T17:26:03Z",
        "message": "ioPath async - opt-in Fairseq integration (#1635)\n\nSummary:\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1635\n\n**Summary:** Integrate ioPath's async writes feature into Fairseq checkpoint writing.\n\n**Details:**\n- Created new checkpoint config param `--write-checkpoints-asynchronously` with default value `False`. Aliased to `--save-async`.\n- Added to `PathManager` class in `file_io.py` to include `PathManager.opena(...)` and `PathManager.async_close()`. These new methods use ioPath's async `PathManager`.\n\n**Usage:**\n```\npython train.py --save-async\n```\n---------\nNOTE: **QUESTIONS**\n1. In the current implementation, we don't save `checkpoint_best` and `checkpoint_latest` since ioPath doesn't yet have a \"wait until a file is written and then copy/move it to another path\" feature. Is this okay for now?\n2. Should I mimic the atomic vs non-atomic save structure that synchronous Fairseq checkpoint writes have?\n\n**Note to Eric:** Keep this integration in check with D26375501.\n\nReviewed By: myleott\n\nDifferential Revision: D26467815\n\nfbshipit-source-id: 50068ef7bf9a6d5cea4d5e0d13d672604dc4a6b0"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-02-11T22:00:25Z",
        "message": "save task state in the checkpoint (#1562)\n\nSummary:\nthis allows tasks to declare some properties they'd like to save in the checkpoint (such as a dictionary), which are loaded when checkpoint is restored.\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1562\n\nTest Plan: tested by training a new wav2vec model, then finetuning it, then decoding it and making sure the dict only loaded once, during fine tuning process (and was obtained from checkpoint for decoding)\n\nReviewed By: myleott, gwenzek\n\nDifferential Revision: D25937974\n\nPulled By: alexeib\n\nfbshipit-source-id: b9908042f76ec8cda943f33885eb9b1f121662ae"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-02-02T23:51:11Z",
        "message": "Fix the task data arg conversion to string.\n\nSummary: We were getting some test failures on our end due to incompatibility of task data argument type. The actual exception is defined in this task: T83395097 and T83395052. Fixing the task data arg to be a string instead of list of strings.\n\nReviewed By: myleott\n\nDifferential Revision: D26205482\n\nfbshipit-source-id: d29d1ee7c469177e8bdad7ca603938f8450bd81c"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-31T04:16:56Z",
        "message": "Fix incorrect local cache for checkpoint_last.pt when training is restarted on the same host\n\nReviewed By: myleott\n\nDifferential Revision: D25719057\n\nfbshipit-source-id: 2bf7dd93b6d223804da0326a0ed347e5e353f1f0"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-23T19:16:56Z",
        "message": "fairseq checkpoint improvements\n\nReviewed By: myleott\n\nDifferential Revision: D25677238\n\nfbshipit-source-id: b43075034c953491211f19a5464148de4758df83"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-18T19:45:08Z",
        "message": "Refactor eval_lm to support library usage (#1513)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1513\n\nTest Plan: Imported from OSS\n\nReviewed By: alexeib\n\nDifferential Revision: D25570467\n\nPulled By: myleott\n\nfbshipit-source-id: 062f748e287797f4f01c605e0b544ef3e698851f"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-18T15:40:49Z",
        "message": "Support atomic saves for checkpoints (#1520)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1520\n\nTest Plan: Imported from OSS\n\nReviewed By: stephenroller\n\nDifferential Revision: D25632782\n\nPulled By: myleott\n\nfbshipit-source-id: bdbe2aed6254d0b023b33f8027dfbd939f1fd271"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-16T01:47:42Z",
        "message": "Fix loading of very old checkpoints (#1512)\n\nSummary:\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1512\n\nSee https://github.com/pytorch/fairseq/issues/3032 for context\n\nTest Plan: Imported from OSS\n\nReviewed By: alexeib\n\nDifferential Revision: D25570470\n\nPulled By: myleott\n\nfbshipit-source-id: 9227b1ca36cd81ff72acdb5e03fd574e3e8769be"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-08T23:49:12Z",
        "message": "fix wav2vec scripts (#1494)\n\nSummary:\nfixes #2942\n+ docs + migration of old models\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1494\n\nReviewed By: myleott\n\nDifferential Revision: D25404601\n\nPulled By: alexeib\n\nfbshipit-source-id: 092f145602522f8e7ea3eaa709bfe602a4d29d8b"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-05T15:37:51Z",
        "message": "Rename optimization.min_lr -> optimization.stop_min_lr (#1486)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1486\n\nTest Plan: Imported from OSS\n\nReviewed By: alexeib\n\nDifferential Revision: D25342181\n\nPulled By: myleott\n\nfbshipit-source-id: 7d1cfb26334fff26d688648724ab073e5fb956f5"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-11-30T22:20:36Z",
        "message": "Add/fix tests (#1468)\n\nSummary:\n- add test for loading ensemble checkpoints (and confirmed it fails if I revert: https://github.com/pytorch/fairseq/commit/265791b727b664d4d7da3abd918a3f6fb70d7337)\n- add test for LayerDrop (and fix it)\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1468\n\nReviewed By: alexeib\n\nDifferential Revision: D25223272\n\nPulled By: myleott\n\nfbshipit-source-id: 3f06f753605af251567c70d2961f5506ea423499"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-11-18T01:08:13Z",
        "message": "fix loading ensembles (#1442)\n\nSummary:\nfixes loading ensembles. previous change used the state of the first model for all models in the ensemble\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1442\n\nReviewed By: chtran\n\nDifferential Revision: D25035706\n\nPulled By: alexeib\n\nfbshipit-source-id: 9029999be0f1703efb1df20bec2890de59449f1f"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-11-11T18:15:10Z",
        "message": "load dataset with saved task config (optionally) (#1423)\n\nSummary:\nthis adds an argument to load_dataset that provides task configuration from the checkpoint. different tasks can decide what to do with it afterwards.\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1423\n\nReviewed By: myleott\n\nDifferential Revision: D24875706\n\nPulled By: alexeib\n\nfbshipit-source-id: 5bb1e2b7495520c456024dc7b0751b65cb05b473"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-11-09T23:46:00Z",
        "message": "migrate wav2vec2 model (#1409)\n\nSummary:\nsee title\nalso includes some minor bug fixes\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1409\n\nReviewed By: myleott\n\nDifferential Revision: D24822219\n\nPulled By: alexeib\n\nfbshipit-source-id: b18f9a8af42ced37880c23dd6ad1ec4df3dfc040"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-11-05T23:29:33Z",
        "message": "Move TPU grad reductions out of Trainer into TPUDistributedDataParallel (#1397)\n\nSummary:\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1397\n\nData parallel command: `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --share-decoder-input-output-embed --lr 0.0001`\n\nData parallel before:\n```\n2020-11-04 08:20:13 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)\n2020-11-04 08:20:13 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8\n2020-11-04 08:20:13 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt\n2020-11-04 08:20:13 | INFO | fairseq.trainer | loading train data for epoch 1\n2020-11-04 08:20:14 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train\n2020-11-04 08:20:14 | INFO | fairseq.optim.adam | using FusedAdam\n2020-11-04 08:20:14 | INFO | fairseq.trainer | begin training epoch 1\n2020-11-04 08:20:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=5\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108002, wps=160870, ups=4.91, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.8, wps=517232, ups=15.77, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.4, wps=537322, ups=16.38, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492.1, wps=540488, ups=16.48, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603.2, wps=543411, ups=16.57, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=540359, ups=16.47, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:20 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=533446, ups=16.26, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:20 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.4, wps=539734, ups=16.46, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=6\n```\n\nData parallel after:\n```\n2020-11-04 08:14:02 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)\n2020-11-04 08:14:02 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8\n2020-11-04 08:14:02 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt\n2020-11-04 08:14:02 | INFO | fairseq.trainer | loading train data for epoch 1\n2020-11-04 08:14:03 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train\n2020-11-04 08:14:03 | INFO | fairseq.optim.adam | using FusedAdam\n2020-11-04 08:14:03 | INFO | fairseq.trainer | begin training epoch 1\n2020-11-04 08:14:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108002, wps=157099, ups=4.79, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.8, wps=560049, ups=17.08, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.4, wps=558507, ups=17.03, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492.1, wps=514194, ups=15.68, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603.2, wps=552676, ups=16.85, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:09 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=546402, ups=16.66, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:09 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=508472, ups=15.5, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:09 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.4, wps=552493, ups=16.84, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=6\n```\n\nData parallel command (no_c10d): `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --share-decoder-input-output-embed --lr 0.0001 --dp-backend no_c10d`\n\nData parallel before:\n```\n2020-11-04 08:19:25 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)\n2020-11-04 08:19:25 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8\n2020-11-04 08:19:25 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt\n2020-11-04 08:19:25 | INFO | fairseq.trainer | loading train data for epoch 1\n2020-11-04 08:19:25 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train\n2020-11-04 08:19:26 | INFO | fairseq.optim.adam | using FusedAdam\n2020-11-04 08:19:26 | INFO | fairseq.trainer | begin training epoch 1\n2020-11-04 08:19:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n2020-11-04 08:19:31 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108001, wps=141659, ups=4.32, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.9, wps=503762, ups=15.36, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.5, wps=488599, ups=14.9, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492, wps=507855, ups=15.48, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603, wps=503270, ups=15.34, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=7\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=467778, ups=14.26, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=7\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=503800, ups=15.36, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=7\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.3, wps=468486, ups=14.28, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=7\n```\n\nData parallel after:\n```\n2020-11-04 08:14:50 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)\n2020-11-04 08:14:50 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8\n2020-11-04 08:14:50 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt\n2020-11-04 08:14:50 | INFO | fairseq.trainer | loading train data for epoch 1\n2020-11-04 08:14:50 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train\n2020-11-04 08:14:51 | INFO | fairseq.optim.adam | using FusedAdam\n2020-11-04 08:14:51 | INFO | fairseq.trainer | begin training epoch 1\n2020-11-04 08:14:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108001, wps=137677, ups=4.2, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.9, wps=519541, ups=15.84, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.5, wps=517063, ups=15.76, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492, wps=490728, ups=14.95, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603, wps=505262, ups=15.41, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=508874, ups=15.52, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:57 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=518028, ups=15.79, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:57 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.3, wps=515996, ups=15.73, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=7\n```\n\nModel parallel command: `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm_megatron --decoder-layers 4 --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --model-parallel-size 2 --share-decoder-input-output-embed --lr 0.0001`\n\nModel parallel before:\n```\n2020-11-04 08:18:38 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)\n2020-11-04 08:18:38 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8\n2020-11-04 08:18:38 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last-model_part-0.pt\n2020-11-04 08:18:38 | INFO | fairseq.trainer | loading train data for epoch 1\n2020-11-04 08:18:38 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train\n2020-11-04 08:18:39 | INFO | fairseq.optim.adam | using FusedAdam\n2020-11-04 08:18:39 | INFO | fairseq.trainer | begin training epoch 1\n2020-11-04 08:18:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n2020-11-04 08:18:45 | INFO | train_inner | epoch 001:      2 / 7173 loss=55.997, ppl=7.19017e+16, wps=0, ups=0, wpb=16384, bsz=32, num_updates=1, lr=0.0001, gnorm=14.03, loss_scale=64, train_wall=1, wall=7\n2020-11-04 08:18:45 | INFO | train_inner | epoch 001:      3 / 7173 loss=28.372, ppl=3.47501e+08, wps=48371.7, ups=2.95, wpb=16384, bsz=32, num_updates=2, lr=0.0001, gnorm=15.339, loss_scale=64, train_wall=0, wall=8\n2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      4 / 7173 loss=15.855, ppl=59276.8, wps=72422.5, ups=4.42, wpb=16384, bsz=32, num_updates=3, lr=0.0001, gnorm=4.189, loss_scale=64, train_wall=0, wall=8\n2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      5 / 7173 loss=14.713, ppl=26858.7, wps=72933.5, ups=4.45, wpb=16384, bsz=32, num_updates=4, lr=0.0001, gnorm=4.751, loss_scale=64, train_wall=0, wall=8\n2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      6 / 7173 loss=13.901, ppl=15299.7, wps=71974.8, ups=4.39, wpb=16384, bsz=32, num_updates=5, lr=0.0001, gnorm=4.361, loss_scale=64, train_wall=0, wall=8\n2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      7 / 7173 loss=13.312, ppl=10169.5, wps=72897.8, ups=4.45, wpb=16384, bsz=32, num_updates=6, lr=0.0001, gnorm=3.307, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:18:47 | INFO | train_inner | epoch 001:      8 / 7173 loss=12.914, ppl=7720.21, wps=73044.6, ups=4.46, wpb=16384, bsz=32, num_updates=7, lr=0.0001, gnorm=5.473, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:18:47 | INFO | train_inner | epoch 001:      9 / 7173 loss=12.56, ppl=6036.72, wps=73453.1, ups=4.48, wpb=16384, bsz=32, num_updates=8, lr=0.0001, gnorm=6.112, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:18:47 | INFO | train_inner | epoch 001:     10 / 7173 loss=12.116, ppl=4437.77, wps=73442.6, ups=4.48, wpb=16384, bsz=32, num_updates=9, lr=0.0001, gnorm=4.415, loss_scale=64, train_wall=0, wall=9\n```\n\nModel parallel after:\n```\n2020-11-04 08:12:09 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)\n2020-11-04 08:12:09 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8\n2020-11-04 08:12:09 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last-model_part-0.pt\n2020-11-04 08:12:09 | INFO | fairseq.trainer | loading train data for epoch 1\n2020-11-04 08:12:09 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train\n2020-11-04 08:12:10 | INFO | fairseq.optim.adam | using FusedAdam\n2020-11-04 08:12:10 | INFO | fairseq.trainer | begin training epoch 1\n2020-11-04 08:12:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      2 / 7173 loss=55.997, ppl=7.19017e+16, wps=0, ups=0, wpb=16384, bsz=32, num_updates=1, lr=0.0001, gnorm=14.03, loss_scale=64, train_wall=1, wall=8\n2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      3 / 7173 loss=28.372, ppl=3.47501e+08, wps=53097, ups=3.24, wpb=16384, bsz=32, num_updates=2, lr=0.0001, gnorm=15.339, loss_scale=64, train_wall=0, wall=8\n2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      4 / 7173 loss=15.855, ppl=59276.8, wps=72355.5, ups=4.42, wpb=16384, bsz=32, num_updates=3, lr=0.0001, gnorm=4.189, loss_scale=64, train_wall=0, wall=8\n2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      5 / 7173 loss=14.713, ppl=26858.7, wps=70526.4, ups=4.3, wpb=16384, bsz=32, num_updates=4, lr=0.0001, gnorm=4.751, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      6 / 7173 loss=13.901, ppl=15299.7, wps=73063.5, ups=4.46, wpb=16384, bsz=32, num_updates=5, lr=0.0001, gnorm=4.361, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      7 / 7173 loss=13.312, ppl=10169.5, wps=73559.4, ups=4.49, wpb=16384, bsz=32, num_updates=6, lr=0.0001, gnorm=3.307, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      8 / 7173 loss=12.914, ppl=7720.21, wps=72693.2, ups=4.44, wpb=16384, bsz=32, num_updates=7, lr=0.0001, gnorm=5.473, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      9 / 7173 loss=12.56, ppl=6036.72, wps=73531.2, ups=4.49, wpb=16384, bsz=32, num_updates=8, lr=0.0001, gnorm=6.112, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:12:19 | INFO | train_inner | epoch 001:     10 / 7173 loss=12.116, ppl=4437.77, wps=73187.6, ups=4.47, wpb=16384, bsz=32, num_updates=9, lr=0.0001, gnorm=4.415, loss_scale=64, train_wall=0, wall=10\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D24729295\n\nPulled By: myleott\n\nfbshipit-source-id: beee8bdece3eaa0419a2e813990420411e507c75"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-10-23T07:07:33Z",
        "message": "refactor dataclass related files, add proper types for static checkin\u2026 (#1371)\n\nSummary:\n- refactor dataclass/ hierarchy to make it a bit more sane (while avoiding circular references)\n- add top level FairseqConfig\n- change typehints to reflect the correct config type if it is known\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1371\n\nReviewed By: myleott\n\nDifferential Revision: D24469026\n\nPulled By: alexeib\n\nfbshipit-source-id: 01f68918f761d51ec5216286b8959ad35f41a7b2"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-10-22T23:31:49Z",
        "message": "rename remove_bpe to post_process; add aliasing (#1369)\n\nSummary:\nsome binaries (e.g. speech based ones) used --post-process, some used --remove-bpe. --post-process seems more appropriate as it does more than just remove bpe at the moment. this renames remove_bpe to post_process, adds alias so existing command lines would work and adds checkpoint upgrades so they continue to work also.\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1369\n\nReviewed By: myleott\n\nDifferential Revision: D24465040\n\nPulled By: alexeib\n\nfbshipit-source-id: 1b3e388291ccc403e76e069ef6606b80ead863a7"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-10-20T07:32:26Z",
        "message": "Enable Hydra configs in fairseq (#1343) (#1510)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/1510\n\nthis is the main pr that switches on hydra functionality in fairseq\n\nwe migrate \"args\" object into omegaconf \"DictConfig\" at all legacy entry points\n\nin addition this migrates various components from secondary registries (like bpe encoders and tokenizers) to make the migration smoother\n\ni am going through code that references migrated fairseq components and changing it to inherit from \"Legacy*\" components instead. hopefully tests will catch most of this\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1343\n\nReviewed By: myleott\n\nDifferential Revision: D23973928\n\nPulled By: alexeib\n\nfbshipit-source-id: dd9554981fff51ea75c1ff343874d1d6e61793c9"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-10-19T01:14:51Z",
        "message": "Apply black+isort (#1357)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1357\n\nReviewed By: alexeib\n\nDifferential Revision: D24377772\n\nfbshipit-source-id: 51581af041d42d62166b33a35a1a4228b1a76f0c"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-10-12T17:55:40Z",
        "message": "Support generation with huge pipeline parallel Transformer models (#1297)\n\nSummary:\n## What is this PR about?\n* Support loading sharded checkpoints (loading and saving checkpoints without sharding for 30B models runs OOM)\n* Ability to provide a fixed dictionary for multilingual machine translation to match the dictionary used for training the checkpoint\n* Support generation with PipelineParallelTransformer models\n\n## Testing\n\n```\npython fairseq_cli/generate.py \\\n    /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted  \\\n    --batch-size 1 \\\n    --path /large_exp/angelafan/checkpoints/shru/mmmt_5b_checkpoints_for_master/checkpoint4.pt \\\n    -s en -t es --remove-bpe 'sentencepiece' --beam 5 \\\n    --task translation_multi_simple_epoch \\\n    --lang-pairs /private/home/angelafan/mmt/lang100_bt_pairs.txt \\\n    --decoder-langtok --encoder-langtok src --gen-subset valid --fp16 \\\n    --dataset-impl mmap \\\n    --distributed-world-size 1 --distributed-no-spawn \\\n    --pipeline-model-parallel \\\n    --pipeline-chunks 1 \\\n    --pipeline-encoder-balance '[26]' \\\n    --pipeline-encoder-devices '[0]' \\\n    --pipeline-decoder-balance '[26]' \\\n    --pipeline-decoder-devices '[0]' \\\n    --fixed-dictionary /private/home/shru/projects/fairseq-py-kakaoval-mmt-save-redist-gen/dict.100langs.txt\n2020-09-23 23:18:27 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might ca\nuse misalignment in pretraining and finetuning.\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['af', 'am', 'ar', 'arbt', 'ast', 'az', 'azbt', 'ba', 'be', 'bebt', 'bg', 'bgbt', 'bn', 'bnbt', 'br', 'bs', 'ca', 'ceb', 'cs', 'csb\nt', 'cy', 'da', 'de', 'debt', 'el', 'en', 'es', 'esbt', 'et', 'etbt', 'fa', 'fabt', 'ff', 'fi', 'fr', 'frbt', 'fy', 'ga', 'gd', 'gl', 'gu', 'ha', 'he', 'hebt', 'hi', 'hibt', 'hr', 'ht', 'hu', 'hubt', 'hy', 'hybt', 'id', 'ig', 'ilo', 'is',\n 'it', 'ja', 'jabt', 'jv', 'ka', 'kabt', 'kk', 'km', 'kn', 'ko', 'kobt', 'lb', 'lg', 'ln', 'lo', 'lt', 'lv', 'mg', 'mk', 'mkbt', 'ml', 'mn', 'mr', 'mrbt', 'ms', 'msbt', 'my', 'ne', 'nebt', 'nl', 'no', 'ns', 'oc', 'or', 'pa', 'pl', 'ps', '\npt', 'ro', 'robt', 'ru', 'sd', 'si', 'sk', 'sl', 'so', 'sq', 'sr', 'srbt', 'ss', 'su', 'sv', 'sw', 'ta', 'th', 'tl', 'tn', 'tr', 'trbt', 'uk', 'ur', 'uz', 'vi', 'vibt', 'wo', 'xh', 'yi', 'yo', 'zh', 'zhbt', 'zu']\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [en] dictionary: 128112 types\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [es] dictionary: 128112 types\n2020-09-23 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None\n2020-09-23 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:en-es': 1}\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:en-es src_langtok: 128022; tgt_langtok: 128023\n2020-09-23 23:18:27 | INFO | fairseq.data.data_utils | loaded 4231 examples from: /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted/valid.en-es.en\n2020-09-23 23:18:27 | INFO | fairseq.data.data_utils | loaded 4231 examples from: /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted/valid.en-es.es\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted valid en-es 4231 examples\n2020-09-23 23:18:27 | INFO | fairseq_cli.generate | loading model(s) from /large_exp/angelafan/checkpoints/mmmt_100_langs_new_dict_5b_model/checkpoint4.pt\nbalance=[29,22,1], devices=[0,1,0], chunks=8, checkpoint=always\n2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler order indices time: 0:00:00.004368\n2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler filter_by_size time: 0:00:00.057953\n2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler batch_by_size time: 0:00:01.158644\n2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:01.223290\n2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\nS-2521  __en__ No.\nT-2521  No.\nH-2521  -2.4406352043151855     y no.\nD-2521  -2.4406352043151855     y no.\nP-2521  -5.2677 -4.4289 -0.1922 -2.1578 -0.1565\nS-2261  __en__ Why?\nT-2261  \u00bfPor qu\u00e9?\nH-2261  -1.7077901363372803     \u00bfY por qu\u00e9?\nD-2261  -1.7077901363372803     \u00bfY por qu\u00e9?\nP-2261  -5.2733 -0.7970 -3.7643 -0.5474 -0.3339 -1.0629 -0.1757\n```\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1297\n\nReviewed By: myleott, msbaines\n\nDifferential Revision: D23991647\n\nPulled By: shruti-bh\n\nfbshipit-source-id: 81ea1af64cbe75c8b53e050d2c2339dcb70fe1eb"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-10-09T20:34:59Z",
        "message": "Improve dictionary & checkpoint reading w/ local caching\n\nReviewed By: myleott\n\nDifferential Revision: D24148700\n\nfbshipit-source-id: 666300639243688939e137be748f7b76fc3c21a6"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-09-02T01:17:33Z",
        "message": "Initial support for ZeRO optimizer state sharding (#1259)\n\nSummary:\nFairseqOSS will work with any optimizer and dtype.\n\nTODO(future PR):\n* support reduce instead of all_reduce\n* support gradient sharding\n* support parameter sharding\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1259\n\nTest Plan:\nVerified that checkpoint save and restore work.\n\nVerified that grad_norm, loss, and ppl are identical with and without\nsharding enable.\n\nBefore:\n\n$ fairseq-train --task language_modeling   data-bin/wikitext-103   --save-dir checkpoints/transformer_wikitext-103   --arch transformer_lm --share-decoder-input-output-embed   --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 512 --sample-break-mode none   --max-tokens 2048 --update-freq 16   --max-update 50000  --memory-efficient-fp16 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50\n...\n2020-08-27 22:24:51 | INFO | train_inner | epoch 001:     49 / 394 loss=18.84, ppl=469411, wps=269226, ups=1.03, wpb=262144, bsz=512, num_updates=45, lr=5.72388e-06, gnorm=5.769, loss_scale=8, train_wall=1, wall=68\n2020-08-27 22:24:52 | INFO | train_inner | epoch 001:     50 / 394 loss=18.787, ppl=452312, wps=256992, ups=0.98, wpb=262144, bsz=512, num_updates=46, lr=5.84885e-06, gnorm=5.512, loss_scale=8, train_wall=1, wall=69\n2020-08-27 22:24:53 | INFO | train_inner | epoch 001:     51 / 394 loss=18.74, ppl=437735, wps=259178, ups=0.99, wpb=262144, bsz=512, num_updates=47, lr=5.97383e-06, gnorm=5.298, loss_scale=8, train_wall=1, wall=70\n2020-08-27 22:24:54 | INFO | train_inner | epoch 001:     52 / 394 loss=18.683, ppl=420727, wps=257710, ups=0.98, wpb=262144, bsz=512, num_updates=48, lr=6.0988e-06, gnorm=5.094, loss_scale=8, train_wall=1, wall=71\n2020-08-27 22:24:55 | INFO | train_inner | epoch 001:     53 / 394 loss=18.623, ppl=403794, wps=269279, ups=1.03, wpb=262144, bsz=512, num_updates=49, lr=6.22378e-06, gnorm=4.893, loss_scale=8, train_wall=1, wall=72\n2020-08-27 22:24:56 | INFO | train_inner | epoch 001:     54 / 394 loss=18.574, ppl=390255, wps=264616, ups=1.01, wpb=262144, bsz=512, num_updates=50, lr=6.34875e-06, gnorm=4.684, loss_scale=8, train_wall=1, wall=73\n2020-08-27 22:24:56 | INFO | fairseq_cli.train | begin save checkpoint\n2020-08-27 22:24:56 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n2020-08-27 22:24:56 | INFO | train | epoch 001 | loss 19.736 | ppl 873122 | wps 264825 | ups 1.01 | wpb 262144 | bsz 512 | num_updates 50 | lr 6.34875e-06 | gnorm 8.898 | loss_scale 8 | train_wall 66 | wall 73\n2020-08-27 22:24:56 | INFO | fairseq_cli.train | done training in 72.2 seconds\n\nAfter:\n\n$ fairseq-train --task language_modeling   data-bin/wikitext-103   --save-dir checkpoints/transformer_wikitext-103   --arch transformer_lm --share-decoder-input-output-embed   --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 512 --sample-break-mode none   --max-tokens 2048 --update-freq 16   --max-update 50000  --memory-efficient-fp16 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --zero-sharding os\n...\n2020-08-27 22:22:55 | INFO | train_inner | epoch 001:     49 / 394 loss=18.84, ppl=469411, wps=267663, ups=1.02, wpb=262144, bsz=512, num_updates=45, lr=5.72388e-06, gnorm=5.769, loss_scale=8, train_wall=1, wall=68\n2020-08-27 22:22:56 | INFO | train_inner | epoch 001:     50 / 394 loss=18.787, ppl=452312, wps=252797, ups=0.96, wpb=262144, bsz=512, num_updates=46, lr=5.84885e-06, gnorm=5.512, loss_scale=8, train_wall=1, wall=69\n2020-08-27 22:22:57 | INFO | train_inner | epoch 001:     51 / 394 loss=18.74, ppl=437735, wps=267692, ups=1.02, wpb=262144, bsz=512, num_updates=47, lr=5.97383e-06, gnorm=5.298, loss_scale=8, train_wall=1, wall=70\n2020-08-27 22:22:58 | INFO | train_inner | epoch 001:     52 / 394 loss=18.683, ppl=420727, wps=267507, ups=1.02, wpb=262144, bsz=512, num_updates=48, lr=6.0988e-06, gnorm=5.094, loss_scale=8, train_wall=1, wall=71\n2020-08-27 22:22:59 | INFO | train_inner | epoch 001:     53 / 394 loss=18.623, ppl=403794, wps=254410, ups=0.97, wpb=262144, bsz=512, num_updates=49, lr=6.22378e-06, gnorm=4.893, loss_scale=8, train_wall=1, wall=72\n2020-08-27 22:23:00 | INFO | train_inner | epoch 001:     54 / 394 loss=18.574, ppl=390255, wps=268234, ups=1.02, wpb=262144, bsz=512, num_updates=50, lr=6.34875e-06, gnorm=4.684, loss_scale=8, train_wall=1, wall=73\n2020-08-27 22:23:00 | INFO | fairseq_cli.train | begin save checkpoint\n2020-08-27 22:23:00 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n2020-08-27 22:23:00 | INFO | train | epoch 001 | loss 19.736 | ppl 873122 | wps 263570 | ups 1.01 | wpb 262144 | bsz 512 | num_updates 50 | lr 6.34875e-06 | gnorm 8.898 | loss_scale 8 | train_wall 66 | wall 73\n2020-08-27 22:23:00 | INFO | fairseq_cli.train | done training in 72.3 seconds\n\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nReviewed By: myleott\n\nDifferential Revision: D23432082\n\nPulled By: msbaines\n\nfbshipit-source-id: 6a020b25e36a3d9283582b7d89a6a53038e5b181"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-08-14T17:24:51Z",
        "message": "Misc fixes (#2448)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2448\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D23011193\n\nPulled By: myleott\n\nfbshipit-source-id: 1a29481707108e4465aca78ec1581fb79f05efba"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-08-06T17:20:39Z",
        "message": "Multilingual v1: Multilingual Training with multiple bitext and monolingual datasets: add finetuning options\n\nSummary:\nA first version of XLNMT multilingual project code release: Multilingual Training with multiple bitext\n\n- Minor changes to\n    - fairseq/checkpoint_utils.py to add finetuning option instead of using restore_file which will restore from original model when being requeued.\n\nReviewed By: myleott\n\nDifferential Revision: D22483494\n\nfbshipit-source-id: 733300fd6a4d185e561c793ea668047c96f616c6"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-08-04T15:25:50Z",
        "message": "Fixes checkpoint_path while loading a model-parallel checkpoint (#2365)\n\nSummary:\nFixes https://github.com/pytorch/fairseq/issues/2351\n\nPull Request resolved: https://github.com/pytorch/fairseq/pull/2365\n\nReviewed By: pipibjc\n\nDifferential Revision: D22727384\n\nPulled By: myleott\n\nfbshipit-source-id: e2ff703181a6b8f10df9b4ee7aa3f9e128c04b4e"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-05-26T22:59:59Z",
        "message": "Several small fixes (incl. set default --data-buffer-size=10) (#2163)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2163\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D21665601\n\nPulled By: myleott\n\nfbshipit-source-id: 47673ff7f07acf0002c4e28380aa08ff917618ee"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-05-11T17:34:42Z",
        "message": "Generalize moving of tensors to CPU in checkpoints (#2098)\n\nSummary:\nThis is needed for TPUs\nPull Request resolved: https://github.com/pytorch/fairseq/pull/2098\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D21455095\n\nPulled By: myleott\n\nfbshipit-source-id: f0f88e715884f44bc254b71f7694ac90200d92fc"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-04-14T17:58:38Z",
        "message": "update checkpoint mkdir behavior (issue #1986) (#2011)\n\nSummary:\nCreate checkpoint directory in ```save_checkpoint()```instead of ```load_checkpoint()```.\n\nhttps://github.com/pytorch/fairseq/issues/1986\nPull Request resolved: https://github.com/pytorch/fairseq/pull/2011\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D21017208\n\nPulled By: myleott\n\nfbshipit-source-id: 4f76afc1751f987b8ab2c170ca306d07bd5ffe83"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-03-28T03:22:36Z",
        "message": "adding code to load and save model parallel checkpoint (#1119)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1119\n\nReviewed By: myleott\n\nDifferential Revision: D20712488\n\nfbshipit-source-id: 941ef251c9e2deb8933d88188fac56ee8c5be9b7"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-03-11T07:36:45Z",
        "message": "PySpeech TALNet: Convert to JIT and quantize\n\nSummary:\nUpdate the TALNet model so it can be converted to JIT format and quantized by the `jit_pyspeech_nn_model` tool.\n\nThe updated model structure is slight incompatible with the model files saved before (the model structure has the extra parameters `fc_att.weight` and `fc_att.bias`), so old model files must be loaded with `strict=False`. This diff also makes changes to allow `strict=False` where necessary.\n\nAlso renames the options of `jit_pyspeech_nn_model` to make them conform better to convention.\n\nReviewed By: jay-mahadeokar\n\nDifferential Revision: D20369643\n\nfbshipit-source-id: 0950a26abc20d0ae5929c8d0a03f83cab4a59200"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-03-07T17:12:14Z",
        "message": "Fix epoch reporting when restoring checkpoint (#1075)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1075\n\nDifferential Revision: D20322672\n\nPulled By: myleott\n\nfbshipit-source-id: 8845a10b10442bed77670b7740afa271123a3b5d"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-03-05T00:37:24Z",
        "message": "Use 1-based indexing for epochs everywhere (#1053)\n\nSummary:\nWe are somewhat inconsistent in whether we're using 0-based or 1-based indexing for epochs. This should fix things to be 0-based internally, with logging and checkpoint naming still using 1-based indexing.\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1053\n\nReviewed By: spencerp\n\nDifferential Revision: D20160715\n\nPulled By: myleott\n\nfbshipit-source-id: 4ed94f9c371e1bfe29bcfa087fa6756507d6e627"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-02-19T04:07:05Z",
        "message": "tts synthesis script\n\nSummary: add a synth.py to pyspeech to run tts synthesis for a particular text.\n\nDifferential Revision: D19786089\n\nfbshipit-source-id: 2aadd004609dfbcf477e58bc01e2b05df19c0e26"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-01-22T23:36:28Z",
        "message": "fblearner pyspeech manifold migration\n\nSummary:\nwas planning to migrate everything at once. but because pyspeech and tuna diverged from D19060980, it's too complicated to do the migration.\n\nmigrate save_dir to manifold in fblearner, and copy from manifold to gluster after training to keep downstream workflows happy.\n\nReviewed By: zdavid1995\n\nDifferential Revision: D19433205\n\nfbshipit-source-id: bd8d969c001952d85167a63f4d55fe97b93aceb5"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-01-20T20:15:27Z",
        "message": "fix the problem of passing None to format() when val_loss is None (e.\u2026 (#1633)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [x ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ x] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\nPull Request resolved: https://github.com/pytorch/fairseq/pull/1633\n\nDifferential Revision: D19470727\n\nPulled By: myleott\n\nfbshipit-source-id: a0bbefe20b4692306c57104f3e545912e20055e6"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-01-17T00:14:45Z",
        "message": "Switch to Python logging (+ lint) (#1627)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/fairseq/pull/1627\n\nPython logging offers a number of benefits, such as logging timestamps, better\ncross-library compatibility, ability to add multiple output handlers, etc.\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/646\n\nReviewed By: spencerp\n\nDifferential Revision: D15815620\n\nPulled By: myleott\n\nfbshipit-source-id: 5e64e9929b5e4b9dd5bb49bcdf7c510631907134"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-01-16T09:59:15Z",
        "message": "Split from PR#968. add --keep-best-checkpoints (#990)\n\nSummary:\nFixes https://github.com/fairinternal/fairseq-py/issues/968. Split --keep-best-checkpoints from the original request.\nUse scores as the names to save the checkpoints\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/990\n\nDifferential Revision: D19411250\n\nPulled By: MultiPath\n\nfbshipit-source-id: 82b0db614208eee54c9c0e470ad7faa6481747d5"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-12-17T01:22:11Z",
        "message": "More fully deprecate --raw-text and --lazy-load (fixes #1488)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/947\n\nDifferential Revision: D19084273\n\nPulled By: myleott\n\nfbshipit-source-id: de80d9abfac8e3d813a9c9b343b41327c500344e"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-12-06T02:24:56Z",
        "message": "Add wrapper abstraction on top of fvcore PathManager to insulate OSS compatibility\n\nReviewed By: myleott\n\nDifferential Revision: D18736914\n\nfbshipit-source-id: 897ba7463f7a8ebc0969c01c57bfe04fbf9e71c8"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-12-02T21:26:48Z",
        "message": "Apply Black auto-formatting\n\nReviewed By: sujitoc\n\nDifferential Revision: D18738392\n\nfbshipit-source-id: b7b7b75ef97946786c463c1887ef9a8676f030e6"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-11-22T01:50:42Z",
        "message": "Quick fix for Structured Dropout checkpointing (#1406)\n\nSummary:\nHere's a quick fix for https://github.com/pytorch/fairseq/issues/1403.\n\nTo keep it short, this fix allows the user to checkpoint a translation model properly after applying layer pruning on a restored transformer file.\nPull Request resolved: https://github.com/pytorch/fairseq/pull/1406\n\nDifferential Revision: D18637540\n\nPulled By: myleott\n\nfbshipit-source-id: 0f5e91e05e6579f0f459bc5293e9b14cb267322d"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-11-21T00:52:59Z",
        "message": "Refactor data sharding to be specified via caller of task rather than task itself\n\nSummary: Modifying number of shards internally to disable data sharding for batch iteration is dangerous because the caller of these tasks is not limited to fairspeq/train. So therefore we should put the onus of data sharding properly on the caller rather than the task itself.\n\nReviewed By: myleott\n\nDifferential Revision: D18456424\n\nfbshipit-source-id: d46be16c441c50082f9a768d0b259e6c28a4b67b"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-10-30T19:55:54Z",
        "message": "layer drop\n\nSummary: This diff enables layer drop in transformer decoder in production training pipeline (ptt_transformer). It builds on top of the fairseq implementation D18094657 added by Angela Fan, and added additional logic to handle corresponding dropping layers at test time in exported model.\n\nReviewed By: jhcross\n\nDifferential Revision: D18165586\n\nfbshipit-source-id: 373ac00268a25fa9e412edcb483becdfe792d992"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-10-27T19:10:53Z",
        "message": "adding layerdrop code for training, pruning, and readme (#890)\n\nSummary:\nTEST 1: EVALUATION TIME WORKS\nchecked\nachieves correct model perplexity: 18.68\n\nTEST 2: TRAINING NEW MODEL WORKS\nchecked\n\nwithout layerdrop:\n--decoder-layerdrop 0 OR no flag at all\n| epoch 001:     10 / 11201 loss=27.469, nll_loss=27.469, ppl=185799477.36, wps=1764, ups=0, wpb=9216.000, bsz=3.000, num_updates=7, lr=0.0004376, gnorm=25.471, clip=1.000, oom=0.000, loss_scale=8.000, wall=37, train_wall=30\n| epoch 001:     20 / 11201 loss=27.443, nll_loss=27.443, ppl=182500427.22, wps=2449, ups=0, wpb=9216.000, bsz=3.000, num_updates=17, lr=0.0010626, gnorm=25.273, clip=1.000, oom=0.000, loss_scale=8.000, wall=64, train_wall=57\n| epoch 001:     30 / 11201 loss=27.404, nll_loss=27.404, ppl=177612215.78, wps=2720, ups=0, wpb=9216.000, bsz=3.000, num_updates=27, lr=0.0016876, gnorm=25.136, clip=1.000, oom=0.000, loss_scale=8.000, wall=91, train_wall=84\n| epoch 001:     40 / 11201 loss=27.009, nll_loss=27.009, ppl=135079983.00, wps=2865, ups=0, wpb=9216.000, bsz=3.000, num_updates=37, lr=0.0023126, gnorm=24.311, clip=1.000, oom=0.000, loss_scale=8.000, wall=119, train_wall=112\n| epoch 001:     50 / 11201 loss=26.418, nll_loss=26.418, ppl=89680259.41, wps=2952, ups=0, wpb=9216.000, bsz=3.000, num_updates=47, lr=0.0029376, gnorm=22.775, clip=1.000, oom=0.000, loss_scale=8.000, wall=147, train_wall=140\n\nwith layerdrop (regularization effect should be seen in PPL):\n--decoder-layerdrop 0.2\n\n| epoch 001:     10 / 11201 loss=25.186, nll_loss=25.186, ppl=38182937.27, wps=2428, ups=0, wpb=9216.000, bsz=3.000, num_updates=8, lr=0.0005001, gnorm=17.082, clip=1.000, oom=0.000, loss_scale=16.000, wall=30, train_wall=24\n| epoch 001:     20 / 11201 loss=25.270, nll_loss=25.270, ppl=40451933.50, wps=3173, ups=0, wpb=9216.000, bsz=3.000, num_updates=18, lr=0.0011251, gnorm=17.162, clip=1.000, oom=0.000, loss_scale=16.000, wall=52, train_wall=45\n| epoch 001:     30 / 11201 loss=25.349, nll_loss=25.349, ppl=42752256.68, wps=3454, ups=0, wpb=9216.000, bsz=3.000, num_updates=28, lr=0.0017501, gnorm=17.370, clip=1.000, oom=0.000, loss_scale=16.000, wall=75, train_wall=68\n| epoch 001:     40 / 11201 loss=25.115, nll_loss=25.115, ppl=36343806.30, wps=3619, ups=0, wpb=9216.000, bsz=3.000, num_updates=38, lr=0.0023751, gnorm=16.945, clip=1.000, oom=0.000, loss_scale=16.000, wall=97, train_wall=90\n| epoch 001:     50 / 11201 loss=24.804, nll_loss=24.804, ppl=29284345.78, wps=3716, ups=0, wpb=9216.000, bsz=3.000, num_updates=48, lr=0.0030001, gnorm=16.406, clip=1.000, oom=0.000, loss_scale=16.000, wall=119, train_wall=112\n\nTEST 3: PICKING UP TRAINING FROM EXISTING MODEL\nchecked\n\n| loaded checkpoint /checkpoint/angelafan/structured_0.1_block_8_sd02/checkpoint_last.pt (epoch 272 @ 381066 updates)\n| loading train data for epoch 272\n| loaded 1801350 examples from: /private/home/angelafan/lm_work/fairseq-py/data-bin/wikitext-103/train\n\nTEST 4: EVALUATING EXISTING BERT MODEL REPROS RESULTS\n| [input] dictionary: 50265 types\n| [label] dictionary: 9 types\n| Accuracy:  0.9231651376146789\nachieves correct accuracy on SST2 for this model\n\nTEST 5: TRAINING NEW BERT MODEL WORKS\nchecked and works\n\nTEST 6: NMT\n\nwithout layerdrop\n--encoder-layerdrop 0 --decoder-layerdrop 0 OR combinations of flag specified and not specified\n\n| epoch 001:     10 / 92203 loss=15.820, nll_loss=15.830, ppl=58267.93, wps=4902, ups=0, wpb=1477.818, bsz=51.636, num_updates=11, lr=1.47473e-06, gnorm=7.207, clip=0.000, oom=0.000, loss_scale=128.000, wall=60, train_wall=3\n| epoch 001:     20 / 92203 loss=15.523, nll_loss=15.501, ppl=46359.29, wps=5037, ups=0, wpb=1496.476, bsz=45.333, num_updates=21, lr=2.72448e-06, gnorm=6.869, clip=0.000, oom=0.000, loss_scale=128.000, wall=63, train_wall=6\n| epoch 001:     30 / 92203 loss=15.185, nll_loss=15.123, ppl=35695.79, wps=5085, ups=0, wpb=1519.355, bsz=44.645, num_updates=31, lr=3.97423e-06, gnorm=6.186, clip=0.000, oom=0.000, loss_scale=128.000, wall=66, train_wall=9\n| epoch 001:     40 / 92203 loss=14.940, nll_loss=14.849, ppl=29505.60, wps=5116, ups=1, wpb=1521.244, bsz=42.927, num_updates=41, lr=5.22398e-06, gnorm=5.610, clip=0.000, oom=0.000, loss_scale=128.000, wall=69, train_wall=12\n| epoch 001:     50 / 92203 loss=14.745, nll_loss=14.630, ppl=25346.87, wps=5070, ups=1, wpb=1507.961, bsz=41.725, num_updates=51, lr=6.47373e-06, gnorm=5.104, clip=0.000, oom=0.000, loss_scale=128.000, wall=71, train_wall=15\n\nwith layerdrop (regularization effect should be seen in PPL)\n\nA) works with --encoder-layerdrop 0.2 --decoder-layerdrop 0.2\nB) works with different settings --encoder-layerdrop 0.3 --decoder-layerdrop 0.5\nC) works with one on and one off --encoder-layerdrop 0.2 --decoder-layerdrop 0\n\n| epoch 001:     10 / 92203 loss=15.817, nll_loss=15.828, ppl=58158.54, wps=5355, ups=0, wpb=1477.818, bsz=51.636, num_updates=11, lr=1.47473e-06, gnorm=6.959, clip=0.000, oom=0.000, loss_scale=128.000, wall=59, train_wall=3\n| epoch 001:     20 / 92203 loss=15.650, nll_loss=15.641, ppl=51111.63, wps=5515, ups=0, wpb=1496.476, bsz=45.333, num_updates=21, lr=2.72448e-06, gnorm=6.825, clip=0.000, oom=0.000, loss_scale=128.000, wall=61, train_wall=6\n| epoch 001:     30 / 92203 loss=15.440, nll_loss=15.408, ppl=43491.58, wps=5602, ups=0, wpb=1519.355, bsz=44.645, num_updates=31, lr=3.97423e-06, gnorm=6.576, clip=0.000, oom=0.000, loss_scale=128.000, wall=64, train_wall=8\n| epoch 001:     40 / 92203 loss=15.247, nll_loss=15.193, ppl=37457.14, wps=5676, ups=1, wpb=1521.244, bsz=42.927, num_updates=41, lr=5.22398e-06, gnorm=6.124, clip=0.000, oom=0.000, loss_scale=128.000, wall=67, train_wall=11\n| epoch 001:     50 / 92203 loss=15.055, nll_loss=14.977, ppl=32259.92, wps=5598, ups=1, wpb=1507.961, bsz=41.725, num_updates=51, lr=6.47373e-06, gnorm=5.661, clip=0.000, oom=0.000, loss_scale=128.000, wall=69, train_wall=14\n\nTEST 7: PRUNING TESTCASES\n\nA) after adding the pruning flags, model can evaluate as a full model\nchecked, reaches correct PPL\nnum. model params: 246933504\n| Evaluated 217646 tokens in 196.3s (1108.99 tokens/s)\n| Loss: 2.9275, Perplexity: 18.68\n\nB) after adding pruning flags, model can be pruned. this works with multiple flag settings\nchecked three cases:\nnum. model params: 146163712\n| Evaluated 217646 tokens in 106.0s (2054.07 tokens/s)\n| Loss: 3.0932, Perplexity: 22.05\n\nnum. model params: 209144832\n| Evaluated 217646 tokens in 162.8s (1336.99 tokens/s)\n| Loss: 2.9526, Perplexity: 19.16\n\nC) model can pick up training if you want to finetune the pruned model\nchecked:\n| loading train data for epoch 272\n| loaded 1801350 examples from: /private/home/angelafan/lm_work/fairseq-py/data-bin/wikitext-103/train\n| WARNING: overflow detected, setting loss scale to: 64.0\n| WARNING: overflow detected, setting loss scale to: 32.0\n| epoch 272:   1500 / 5601 loss=5.015, nll_loss=5.015, ppl=32.33, wps=11598, ups=1, wpb=18432.000, bsz=6.000, num_updates=98, lr=0.0061251, gnorm=0.613, clip=1.000, oom=0.000, loss_scale=32.000, wall=156, train_wall=252396\n\nD) works with BERT\nchecked:\nwithout specifying any flags, reproduces the correct standard accuracy\nwith flags, produces the correct pruned accuracy\n\n| [input] dictionary: 50265 types\n| [label] dictionary: 9 types\n| Accuracy:  0.9231651376146789\n\n| [input] dictionary: 50265 types\n| [label] dictionary: 9 types\n| Pruning model to specified layer configuration - this works best if the model was trained with LayerDrop\n| Accuracy:  0.9220183486238532\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/890\n\nReviewed By: edunov\n\nDifferential Revision: D18094657\n\nPulled By: huihuifan\n\nfbshipit-source-id: 2bbaa2ff0039e906782694fc2038b8c17a8693e7"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-10-12T04:53:11Z",
        "message": "Added option to save checkpoints using Path Manager.\n\nSummary: Added option to save checkpoints using Path Manager.\n\nReviewed By: hudeven\n\nDifferential Revision: D17392754\n\nfbshipit-source-id: 4b8e556ef8455a1548e5a083d779ed809cd785be"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-10-09T21:53:27Z",
        "message": "Fix data loading memory issue in pyspeech\n\nSummary:\nWe currently shard data when creating the batch iterator. This means we first load all indicese/frame lengths/handles into memory, and then do the sharding. This makes it impossible to train on large datasets with a high amount of workers  because each worker will need to load the entire dataset into memory. For training on a million hours of data (i.e. semi-supervised or unsupervised approaches) this data loading just makes it flat out impossible to use 8 GPU's.\n\n3 changes:\n\n1. This diff modifies the data loading such that we do the sharding while we read the handles file, rather than later. This modification is done on a task-by-task basis, since the task specifies how the data is loaded. I've tried to make the code compatible with both sharding during handle loading and sharding during batch iteration. I've currently only done the sharding during handle loading for the aligned_training task.\n\n2. To support data sharding at data loading time and the requirement that all shards must have exactly the same # of batches, I've added a method to do this synchronization where all shards with too many batches would just truncate the extra ones, similar to what we already do.\n\n2. In fairspeq/train.py, we are actually loading the training dataset and batch iterator twice, once in train.py and once when loading the checkpoint (which we always do regardless if there is a checkpoint). This means double the loading time which can be painful for very large files. I've removed the extraneous loading in this diff as well.\n\nReviewed By: yqwangustc\n\nDifferential Revision: D17750715\n\nfbshipit-source-id: 0e6e3d363525fa5661f1c784303390ea13f46377"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-09-20T16:34:58Z",
        "message": "added multilingual masked LM training (#849)\n\nSummary:\nThe multilingual-RoBERTa training is working with aconneau XLM data.\n\nTwo pieces remaining:\n\n1) `XLM` limits batch to be from same language, I am not 100% sure about the reason for that, but should be easy to implement, basically we can add `batch_by_size_and_language` instead of default `batch_by_size` function. If it's not critical, I would want to leave it out as it keeps the code very clean and simple.\n\n2) `sample_ratio` in `ConcatDataset` works with `int` by tiling the datasets based on ratio. Currently I am handling it by sounding off the ratio to `first decimal` and then multiplying by `10`. We can see if some such simple heuristics are good enough, there are other options (we can talk about them offline).\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/849\n\nDifferential Revision: D17162460\n\nfbshipit-source-id: d967f3d872f7a1f0aa4ea418bd362b68af9e432f"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-08-21T20:43:01Z",
        "message": "Parameterized criterions (#808)\n\nSummary:\nSupport criterion with parameters, such as AutoSegmentationCriterion (ASG) used in wav2letter which has a transition matrix parameter. This is needed to integrate wav2letter's ASG into PySpeech.\n\nWith this diff, parameters in criterions will be:\n(1) updated by optimizers, with a configurable learning rate\n(2) saved and loaded from checkpoints, preserving backward compatibility for criterions without parameters\n(3) synchronized across nodes in distributed training.\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/808\n\nReviewed By: jcai1\n\nDifferential Revision: D16934097\n\nPulled By: okhonko\n\nfbshipit-source-id: 121ec9382459385c6f9cbef3a8274bec1a434038"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-08-12T16:08:16Z",
        "message": "Lint\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/817\n\nDifferential Revision: D16762905\n\nPulled By: myleott\n\nfbshipit-source-id: d920595bec44ed26b72dfc6fbc15c0aa107b4e56"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-08-12T14:17:21Z",
        "message": "Update --restore-file logic (partially fixes #999)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1007\n\nDifferential Revision: D16762490\n\nPulled By: myleott\n\nfbshipit-source-id: d67137bcf581887850323d188bb4ea643a35ac9e"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-08-10T15:16:50Z",
        "message": "Add WSC task and criterion\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1004\n\nDifferential Revision: D16751443\n\nPulled By: myleott\n\nfbshipit-source-id: f70acd6c7be6d69da45b5b32fe4c4eff021539ab"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-08-01T12:55:57Z",
        "message": "Update PyTorch Hub interface\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/782\n\nDifferential Revision: D16542256\n\nPulled By: myleott\n\nfbshipit-source-id: ea3279e7a1ce4687a5914f32b76787c419be1ffa"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-07-30T14:48:23Z",
        "message": "Relicense fairseq under MIT license (#786)\n\nSummary:\nThe previous BSD+PATENTS license was controversial. We have been\napproved to relicense fairseq under the MIT license.\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/786\n\nDifferential Revision: D16560654\n\nPulled By: myleott\n\nfbshipit-source-id: f78b1beb4f2895dd7b9bfc79f5f952a2bfb94034"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-07-29T23:06:26Z",
        "message": "adding glue data preprocessing scripts (#771)\n\nSummary:\n1) Added glue data pre-processing script.\n2) updated README with usage.\n\nTODO:\n1) releasing fairseq dictionary and remove hardcoded path.\n2) remove hard-coded path for bpe-encoding,\n\nmyleott what do you recommend for above TODOs?\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/771\n\nReviewed By: myleott\n\nDifferential Revision: D16547679\n\nPulled By: myleott\n\nfbshipit-source-id: 6a6562d9b6215523d048fdf3daee63ffac21e231"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-07-24T23:59:07Z",
        "message": "check save_dir before beginning training\n\nSummary: I sadly discovery that my checkpoint directory wasn't globally readable after 8 hours of training. Adding this check at the beginning of train loop to keep that from happening again!\n\nReviewed By: myleott\n\nDifferential Revision: D16455394\n\nfbshipit-source-id: 35959aa058150b2afb63710c468d01ebc8a12b0c"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-07-19T20:13:43Z",
        "message": "Rename _load_model_ensemble -> load_model_ensemble_and_task\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/738\n\nDifferential Revision: D16377803\n\nPulled By: myleott\n\nfbshipit-source-id: 6beb2f78e7464b70ff65a965d2b747cdca0ca951"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-07-01T21:10:49Z",
        "message": "Fixes checkpointing bug introduced in 89e077c\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/847\n\nDifferential Revision: D16075498\n\nPulled By: myleott\n\nfbshipit-source-id: 62e27a8c4764f53f181c502674dfab1e6b0537e2"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-06-30T18:35:33Z",
        "message": "Add additional options for configuring writing of checkpoints\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/697\n\nDifferential Revision: D16068465\n\nPulled By: myleott\n\nfbshipit-source-id: c2563c3c682e7e8406e6d7c8e895d8afbec551eb"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-06-11T18:19:58Z",
        "message": "Automatically fill in default values from add_args\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/797\n\nDifferential Revision: D15761071\n\nPulled By: myleott\n\nfbshipit-source-id: 257d4a2297e83da7e59baed154dbafd6bfe614bf"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-05-30T18:41:40Z",
        "message": "Add --reset-dataloader\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/613\n\nDifferential Revision: D15541384\n\nPulled By: myleott\n\nfbshipit-source-id: ef2c0b0a51cdf37af2ccff0546f524d49f87e65d"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-05-17T21:25:56Z",
        "message": "Small features + lint\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/588\n\nDifferential Revision: D15389638\n\nPulled By: myleott\n\nfbshipit-source-id: 4632ce22d51dc2c74d250bae999630095d849701"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-05-17T04:03:08Z",
        "message": "Clean up sharded train iterator\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/586\n\nDifferential Revision: D15372949\n\nPulled By: myleott\n\nfbshipit-source-id: c1cf1c645e8d55fc8568f23a47c45677ac9ab1da"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-05-14T19:57:12Z",
        "message": "Move save/load checkpoint functions to utils\n\nSummary:\nMove `load_checkpoint`, `save_checkpoint` and `reload_train` from train.py to checkpoint_utils.py\nMove `get_perplexity` from train.py to utils.py.\nThis will make train.py lighter and allow us to reuse all this utils functionality when fairseq is used as external library.\n\nReviewed By: myleott\n\nDifferential Revision: D15289607\n\nfbshipit-source-id: 4b7c95225ac22e402bcda3497811361809110df1"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-05-11T14:56:45Z",
        "message": "Add missing options to TransformerDecoderLayer\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/560\n\nDifferential Revision: D15260838\n\nPulled By: myleott\n\nfbshipit-source-id: 5f80dd82775c10ce46a3e1c451ccaf0ef55bfa31"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-05-06T07:17:45Z",
        "message": "Load pretrained encoder or decoder (#705)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/fairseq/pull/705\n\nThis adds functionality in fairseq to load a pretrained encoder or decoder from another pretrained model into the current model.\n\nReviewed By: jmp84\n\nDifferential Revision: D15207084\n\nfbshipit-source-id: 32a710ff77389928e20793c71d312863df9dd8ae"
    },
    {
        "repo_url": "github.com/gordicaleksa/Open-NLLB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-04-30T02:50:58Z",
        "message": "Merge internal changes (#654)\n\nSummary:\n- Add --add-bos-token option to LM task\n- Cleanup utils.py and options.py\nPull Request resolved: https://github.com/pytorch/fairseq/pull/654\n\nDifferential Revision: D15041794\n\nPulled By: myleott\n\nfbshipit-source-id: 3ad00007769d5f48308052cfd40de39c5ffa1a6e"
    },
    {
        "repo_url": "github.com/princeton-nlp/DinkyTrain",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2022-04-26T01:00:09Z",
        "message": "Copy checkpoints with hard linking"
    },
    {
        "repo_url": "github.com/princeton-nlp/DinkyTrain",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2022-04-26T01:00:09Z",
        "message": "Initial fairseq commit using 7e758841da9e05cb21826a60d30a563a9e189d1d\n\nSee https://github.com/pytorch/fairseq/commit/7e758841da9e05cb21826a60d30a563a9e189d1d"
    },
    {
        "repo_url": "github.com/thu-coai/DA-Transformer",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2023-05-07T13:22:39Z",
        "message": "add demo & fix bugs"
    },
    {
        "repo_url": "github.com/thu-coai/DA-Transformer",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2022-07-15T03:00:28Z",
        "message": "first upload"
    },
    {
        "repo_url": "github.com/gonglinyuan/metro_t0",
        "filepath": "training/fairseq/checkpoint_utils.py",
        "commit_date": "2023-05-28T18:06:22Z",
        "message": "Add code for pretraining and finetuning\n\nNot ready yet, needs checking"
    },
    {
        "repo_url": "github.com/jungokasai/twist_decoding",
        "filepath": "fairseq/fairseq/checkpoint_utils.py",
        "commit_date": "2022-05-16T22:59:30Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/LUMIA-Group/FourierTransformer",
        "filepath": "Summarization/fairseq/checkpoint_utils.py",
        "commit_date": "2023-06-30T10:21:53Z",
        "message": "add codes"
    },
    {
        "repo_url": "github.com/ChenxinAn-fdu/CoNT",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2022-10-28T03:08:58Z",
        "message": "update fairseq code"
    },
    {
        "repo_url": "github.com/chenllliang/ParetoMNMT",
        "filepath": "fairseq/fairseq/checkpoint_utils.py",
        "commit_date": "2023-03-22T03:34:59Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2022-01-07T08:39:11Z",
        "message": "Formatting fix: get CI green (#2860)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nApplies `black` and `isort` to files\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2860\n\nReviewed By: Mortimerp9\n\nDifferential Revision: D33456637\n\nPulled By: dianaml0\n\nfbshipit-source-id: 560b8d3a8f589cbecc92d0d21163596b5d47d609"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-12-31T02:38:12Z",
        "message": "add strict option to checkpoint_utils. load_pretrained_component_from_model()\n\nSummary: Add strict option to checkpoint_utils. load_pretrained_component_from_model()\n\nReviewed By: sravyapopuri388\n\nDifferential Revision: D33304224\n\nfbshipit-source-id: 2284a21dfea7810ec212f15daadeeeb45c6dca1b"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-12-17T00:11:19Z",
        "message": "formatting fix (#2816)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nfix `black` failures\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2816\n\nReviewed By: alexeib\n\nDifferential Revision: D33172615\n\nPulled By: dianaml0\n\nfbshipit-source-id: 36b141f42941670f1bfa981041d878042feb0428"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-12-11T00:55:12Z",
        "message": "Add loading from HuggingFace Hub\n\nSummary: Add loading from HuggingFace Hub. Revised from and to replace D32697723 (accepted).\n\nReviewed By: pipibjc, dianaml0\n\nDifferential Revision: D32964041\n\nfbshipit-source-id: 39676aa0ecb10454ae76b70968d5abe96ab6da54"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-11-29T20:32:59Z",
        "message": "Add linting with black (#2678)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2678\n\nReviewed By: Mortimerp9\n\nDifferential Revision: D32653381\n\nPulled By: dianaml0\n\nfbshipit-source-id: 2810d14867cd7d64f4d340740e2b590b82de47fe"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-11-19T00:38:31Z",
        "message": "Merge --use-ontology-for-* into --use-ontology\n\nSummary:\nThere are three options for the ontology:\n* `--use-ontology-for-training`\n* `--use-ontology-for-validation`\n* `--use-ontology-for-balancing`\n\nThe first two must always be set together.\n\nIn the past, I observed that it's best not to use ontology for data balancing even if we use ontology for training and validation. But now I no longer observe this.\n\nTherefore, I'm merging all these three options into one (`--use-ontology`).\n\nIn addition, I'm also moving the logic of avoiding loading teacher models out of `checkpoint_utils.py`. If you want to load a student model without loading its teachers (e.g. for prediction only), specify `arg_overrides={\"ignore_teachers\": True}` when calling `load_model_ensemble`.\n\nReviewed By: xiaoxiao26\n\nDifferential Revision: D32518830\n\nfbshipit-source-id: 103c6458f7927ec5ca7470109c8f956c00f514a2"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-10-23T02:51:09Z",
        "message": "set num_update before loading state dict (#2491)\n\nSummary:\n## What does this PR do?\nSet `model.num_updates` in `load_model_ensemble_and_task()` before loading `state_dict`, like what's done in `fairseq/trainer.py`, because a model's `state_dict` may depend on `num_update`.\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2491\n\nReviewed By: xuqiantong\n\nDifferential Revision: D31863368\n\nPulled By: wnhsu\n\nfbshipit-source-id: c70051f898819cc43b02c9f5765429e9f194aed5"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-09-13T20:20:35Z",
        "message": "Back out \"Fairseq needs to store and load metadata from model state_dict\" (#3861)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/fairseq/pull/3861\n\nbackout fairseq changes. fix with a suggested, more optimal changes in checkopint utils.\n\nReviewed By: zhengwy888\n\nDifferential Revision: D30886481\n\nfbshipit-source-id: 12b6dd4d5107ab4371b73a58d9a044a17c733260"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-09-09T01:18:30Z",
        "message": "Fairseq needs to store and load metadata from model state_dict\n\nSummary:\n## TL;DR\nFairseq checkpoint saving and loading should mirror torch's checkpoint by saving and loading \"state_dict()._metadata\".\n\n## Long Story:\n\n#### What happened:\nDuring model loading and saving, Quantization-aware-training models in Pytorch encounters a weird bug that says state_dict \"fake_weight_quant.weight.min_val\" is mismatched to \"min_vals\".\n\n#### What was the reason:\n- We found the issue in that torch uses state_dict()._metadata to store module._version, but the metadata was never store in checkpoint, nor are they loaded during checkpoint loading in fairseq.\n\nReviewed By: frankseide\n\nDifferential Revision: D30649933\n\nfbshipit-source-id: ce262486b9b95fbcece463fa05c4e1903d4232d7"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-09-01T19:29:51Z",
        "message": "EMA\n\nSummary:\nAdds Exponential moving average (EMA) model for Kaizen semi-supervised training https://arxiv.org/abs/2106.07759\n\n1. Add `ema.store_ema` to enable storing EMA. EMA will be written to extra_state in the state dict while saving checkpoint.\n2. `ema.ema_start_update` to control when the EMA starts accumulating\n3. Tasks can use `uses_ema` property to decide if the EMA should be passed to the task. (Default is False)\n4. `load_ema_from_checkpoint` can be used to load EMA model in place of the model to be used for evalutation. Pyspeech has eval-ema option for this.\n\n```\nThis module has the EMA class used to store a copy of the exponentially decayed\nmodel params.\n\nTypical usage of EMA class involves initializing an object using an existing\nmodel (random or from a seed model) and setting the config like ema_decay,\nema_start_update which determine how the EMA model is updated. After every\nupdate of the model i.e. at the end of the train_step, the EMA should be updated\nby passing the new model to the EMA.step function. The EMA model state dict\ncan be stored in the extra state under the key of \"ema\" and dumped\ninto a checkpoint and loaded. The EMA object can be passed to tasks\nby setting task.uses_ema property.\nEMA is a smoothed/ensemble model which might have better performance\nwhen used for inference or further fine-tuning. EMA class has a\nreverse function to load the EMA params into a model and use it\nlike a regular model.\n```\n\nReviewed By: cruvadom\n\nDifferential Revision: D24238379\n\nfbshipit-source-id: 879d3ba5070a614b7d365f9503af357001e875b2"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-08-02T21:36:32Z",
        "message": "fixing checkpoint config upgrade for generation print_alignment (#2125)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes config upgrade conditions for upgrading generation. print_alignment\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2125\n\nReviewed By: myleott\n\nDifferential Revision: D30049140\n\nPulled By: jingfeidu\n\nfbshipit-source-id: e613821e94d0cdb876c35bc6e3fede7affbf4628"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-29T23:30:41Z",
        "message": "seed random suffix in checkpoint to be consistent across shards\n\nSummary:\nCurrently the random suffix for saving sharded checkpoints can be different for each shard when training with FSDP and use_sharded_state=True. This makes it difficult for downstream applications to load the checkpoints properly, since each shard may have different suffixes.\n\nThis diff seeds the random suffix to be consistent across shards\n\nReviewed By: zhengwy888\n\nDifferential Revision: D29951167\n\nfbshipit-source-id: 65749357e62a28978f3b46b71767204a508e1f61"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-29T23:30:41Z",
        "message": "use pathmanager to delete old checkpoints\n\nSummary: --keep-best-checkpoints doesn't work for Manifold paths because the current code assumes normal paths. Lets use  PathManager to properly remove them.\n\nReviewed By: myleott, sshleifer\n\nDifferential Revision: D29947965\n\nfbshipit-source-id: 237a7b5aaa8293bb203ad05937decdf3b3ae2fc0"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-29T23:30:40Z",
        "message": "use suffix when saving best checkpoints with metric\n\nSummary: This is needed when training with FSDP + sharded state, as the checkpoints should have `-shard0.pt`\n\nReviewed By: sshleifer\n\nDifferential Revision: D29947728\n\nfbshipit-source-id: d2cb7c23e1e6d027d115cb734e2f2f1c34239eba"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-09T23:25:02Z",
        "message": "Roll back os.path.abspath change\n\nReviewed By: donhusa\n\nDifferential Revision: D29641968\n\nfbshipit-source-id: eca379158055f3e38e9c053b06db56842265e53a"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-08T22:27:57Z",
        "message": "Fix static container (#2036)\n\nSummary:\nfixes StatefulContainer being static which is a problem when you load a checkpoint with task that already has the same keys in the container\n\nalso print full path to checkpoints when saving (useful with hydra) and crash if repeatedly failing to save a checkpoint\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2036\n\nReviewed By: arbabu123\n\nDifferential Revision: D29608430\n\nPulled By: alexeib\n\nfbshipit-source-id: 1b65c8f839e02de9110af3ec53f1e7d48a4908f7"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-06T22:07:31Z",
        "message": "fixes tests/test_train.py to mock checkpoint.save_dir config node (#3675)\n\nSummary:\n## What does this PR do?\nSome downstream users reported that errors when passing Namespace to load_checkpoint().\n\nA recent change made the assumption that the passed object is dict like (dict or DictConfig) that have a get function.\nThis changes that and make sure the mocked config have checkpoint.save_dir to allow the test to run.\n\nPull Request resolved: https://github.com/pytorch/fairseq/pull/3675\n\nReviewed By: omry\n\nDifferential Revision: D29564805\n\nPulled By: lematt1991\n\nfbshipit-source-id: 89308811da382667f6c5d3152ee2d6480416ee62"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-07-01T13:37:47Z",
        "message": "Hydra 1.1 compatibility: Use an explicit schema for the primary config (#3659)\n\nSummary:\n## What does this PR do?\nFixes compatibility with Hydra 1.1.\nThe result is compatible with both Hydra 1.0 and Hydra 1.1, and will allow a smoother migration to Hydra 1.1.\n\nAt this point I am not yet removing the restriction on the Hydra version from setup.py:\n1. It depends on some Hydra 1.1 changes that are not yet released (It will be compatible with 1.1.1).\n2. Upgrading will result in deprecation warnings, and fixing them will break compatibility with Hydra 1.0.\n\nThere will be some followup to make the code fully compatible with 1.1 once Hydra 1.1 is the default version in fbcode.\n\nPull Request resolved: https://github.com/pytorch/fairseq/pull/3659\n\nReviewed By: omry\n\nDifferential Revision: D29498036\n\nPulled By: lematt1991\n\nfbshipit-source-id: 96999cde5daad6749ef4d3ddf6a36a1e984ff201"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-06-15T21:09:52Z",
        "message": "Check attributes in trainer and checkpoint loading before using them (#1970)\n\nSummary:\n## What does this PR do?\nFixes None exception when some attributes in  don't exist in cfg.\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1970\n\nReviewed By: alexeib\n\nDifferential Revision: D29140036\n\nPulled By: hikushalhere\n\nfbshipit-source-id: 7d941bcae6bb000c281a43ca2cd0876a49912ab9"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-06-04T23:19:56Z",
        "message": "Fix loading some TALNet models\n\nSummary:\nD28728718 cleaned up the \"kd_binary_cross_entropy\" criterion, but this caused loading old models trained with this criterion to fail.\nThis diff replaces the \"kd_binary_cross_entropy\" criterion with the \"wav2vec\" criterion when loading models, and fixes this error.\n\nIt also removes the \"log_keys\" argument if it's `None`. Some criteria (e.g. wav2vec) require this argument to be a list, and will supply a default value of `[]` when it's absent. The presence of the `None` value prevents the use of this default value and causes an error.\n\nDifferential Revision: D28901263\n\nfbshipit-source-id: 9b33aed35e76d2c734d1d4e2cbca1ff193a8c920"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-06-04T00:49:12Z",
        "message": "Teacher-student learning for TALNet\n\nSummary:\nThis diff implements teacher-student learning for TALNet.\n\nThree classes take part in the teacher-student learning:\n* The task loads the teacher models;\n* The model generates predictions using the teacher models, and mixes them with the original targets;\n* The `Wav2VecCriterion` reads the mixed targets to compute the loss. However, it still uses the original targets to compute the MAP and MAUC metrics.\n\nThere are two types of teachers:\n* Static teachers: a file that stores predictions on training data which have been produced by running a model offline;\n* Dynamic teachers: model files that are loaded at the beginning of training and executed on the fly to produce predictions.\n\nWe actually no longer use static teachers. The code about static teachers are copied over from the `KnowledgeDistillationBinaryCrossEntropyCriterion` class. This class will be cleaned up in D28728718.\n\nThe teacher models are stored in the task object, and will not be saved into checkpoints.\n\nReviewed By: alexeib\n\nDifferential Revision: D28728707\n\nfbshipit-source-id: 0fcfc00db2e7194a6f7ee687cad9fa72e82a028b"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-05-26T00:45:51Z",
        "message": "support FSDP sharded_state checkpoint loading during inference\n\nSummary:\nusing the very useful feature added by QuentinDuval https://github.com/facebookresearch/fairscale/pull/683/files , we can consolidate sharded states into a full regular states. this allows inferences on sharded state almost transparently.\n\nThe main complexity comes from trying to be smart about what kind of checkpoint the user wants to load. not sure if this is over-engineering\n1. if the file checkpoint-shard0.pt exists, and `--checkpoint-shard-count` is > 1, then we load sharded FSDP checkpoint\n2. if checkpoint-shard0.pt exists but --checkpoint-shard-count=1, we load consolidated FSDP checkpoint\n3. if checkpoint-shard0.pt does not exist, but --checkpoint-shard-count > 1, we load model parallel checkpoint\n4. otherwise we are loading a single, plain checkpoint.\n\nIn theory we could be even smarter and load shard0.pt to check how many more checkpoints are needed. this is not implemented, though it will save the user having to specify --checkpoint-shard-count.\n\nReviewed By: sshleifer\n\nDifferential Revision: D28563441\n\nfbshipit-source-id: dcafcaa7c9eaf5c9ff94f55c16bb3424c98dfa59"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-05-22T07:22:05Z",
        "message": "fixing s2t transformer and N-best checkpoint saving\n\nSummary:\n- fixing the default value for `encoder_freezing_updates` in s2t transformer\n- fixing N-best checkpoint saving: the previous implementation compares the new checkpoint with only the previous best one but not the previous N best ones. This leads to suboptimal results on N-best checkpoint averaging.\n\nReviewed By: jmp84\n\nDifferential Revision: D28546493\n\nfbshipit-source-id: 44ec6d5ab49347f392d71269c5dcfd154b00c11e"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-05-21T23:18:21Z",
        "message": "attempt to make non-sharded FSDP checkpoint behave like regular checkpoint\n\nSummary:\noverall just wondering if feature is desirable. if it is, the next diff which supports loading sharded checkpoint into a consolidated state dict cleaner.\n\na couple advantages\n1. allows resuming from other DDP trainers.\n2. allows resuming into other DDP trainers. or FSDP of a different configuration.\n3. none-sharded FSDP checkpoint can be loaded with regular load_model_ensemble_and_task()\n\nFor old training workflow that's not using `--use-sharded-state`, please rename the checkpoint to remove the \"-shard0\" for resuming training.\n\nReviewed By: sshleifer\n\nDifferential Revision: D28563032\n\nfbshipit-source-id: ced72bed969319ab6306059721f56e29b2c3d892"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-04-14T08:50:07Z",
        "message": "add keep_interval_updates_pattern\n\nSummary:\nMotivation:\n\nI want to save checkpoints frequently, due to unreliable jobs in FB cluster that restart frequently. I want to do this without spamming Manifold storage, but still save some historical checkpoints (i.e. every 10k updates), so I can track how WER evolves over time.\n\nTo save frequently, I can use a small --save-interval-updates.\n\nTo delete old checkpoints to save storage, I can use --keep-interval-updates.\n\nHowever, this deletes all old checkpoints. This is where --keep-interval-updates-pattern comes in. If I now do:\n\n```\n--save-interval-updates 1000\n--keep-interval-updates 1\n--keep-interval-updates-pattern 10000\n```\n\nThis will:\n1. checkpoint every 1000 updates so that job restarts don't impact us significantly\n2. keep only the latest checkpoint to avoid saving a bunch of huge models in manifold\n3. make an exception for #2 for every 10k updates so we can track WER over time\n\nReviewed By: myleott\n\nDifferential Revision: D27578403\n\nfbshipit-source-id: 5aec2dc9a22778015f7a3daa017210190af81240"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-04-14T08:50:06Z",
        "message": "enable manifold checkpoints with --keep-interval-updates\n\nSummary: Useful to enable --keep-interval-updates with Manifold checkpoints\n\nReviewed By: myleott\n\nDifferential Revision: D27577116\n\nfbshipit-source-id: 6d4ae5aaccc07ecaed8ba6a333b6ab78b148187a"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-03-30T01:02:50Z",
        "message": "BASE layers (#1654)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1654\n\nReviewed By: myleott\n\nDifferential Revision: D27128074\n\nPulled By: shruti-bh\n\nfbshipit-source-id: ac86d383cd53c9c9bdd946fea839a37b719d95e3"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-03-26T14:18:59Z",
        "message": "FSDP uses new optimizer gathering to save optimizer state (#1744)\n\nSummary:\n- Full unflattened optimizer state dict is in `checkpoints/shard_0.pt`, other checkpoint files do not have the `last_optimizer_state` key.\n- requires master version of fairscale (eventually fairscale>=0.3.3)\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1744\n\nReviewed By: myleott\n\nDifferential Revision: D27342305\n\nPulled By: sshleifer\n\nfbshipit-source-id: 7442b8c6ed01599d8ab0050213e84051f4e98acd"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-03-04T21:32:44Z",
        "message": "Move checkpoint state_dict creation into Trainer (#1666)\n\nSummary:\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1666\n\nContext: the checkpoint saving call stack has become a bit convoluted:\n```\ntrain.py\n+ checkpoint_utils.save_checkpoint\n + trainer.save_checkpoint\n  + checkpoint_utils.save_state\n   + checkpoint_utils.torch_persistent_save\n```\n\nThis diff slightly simplifies the checkpoint saving logic by exposing a `state_dict` method inside the Trainer. This simplifies the call stack to:\n```\ntrain.py\n+ checkpoint_utils.save_checkpoint\n + trainer.save_checkpoint\n  + checkpoint_utils.torch_persistent_save\n```\n\nThis new structure is important for the FullyShardedDataParallel diff (next diff in the stack), since it enables the Trainer to save multiple checkpoints for the different optimizer state shards.\n\nTest Plan:\n- unit tests\n- trained WMT En-De models; confirmed checkpoints save/load properly, resuming from a checkpoint gives identical results\n- `buck test fblearner/flow/projects/langtech/translation:tests` (2 failures are in trunk too): https://www.internalfb.com/intern/testinfra/testconsole/testrun/2533274840914654/\n\nReviewed By: zhengwy888\n\nDifferential Revision: D26771146\n\nPulled By: myleott\n\nfbshipit-source-id: 10f91979cd42205c1d8abcaa9ab56f63eba31e93"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-03-04T05:17:29Z",
        "message": "minor fixes and improvements (#1671)\n\nSummary:\nthere are a few changes here:\n- convert config persisted in checkpoints into a plain dict when saving and back to omegaconf config when loading: this helps avoid compatibility issues between different versions of python, omegaconf, etc\n- update checkpoints that have old print_alignment saved\n- add lr_float to composite optimizer to enable sweeping on lr with auto sweepers like ax\n- fixing some edge cases for config loading\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1671\n\nReviewed By: myleott\n\nDifferential Revision: D26791583\n\nPulled By: alexeib\n\nfbshipit-source-id: 124dec74932052925c43b6a93130f4428803cb46"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-03-02T17:26:03Z",
        "message": "ioPath async - opt-in Fairseq integration (#1635)\n\nSummary:\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1635\n\n**Summary:** Integrate ioPath's async writes feature into Fairseq checkpoint writing.\n\n**Details:**\n- Created new checkpoint config param `--write-checkpoints-asynchronously` with default value `False`. Aliased to `--save-async`.\n- Added to `PathManager` class in `file_io.py` to include `PathManager.opena(...)` and `PathManager.async_close()`. These new methods use ioPath's async `PathManager`.\n\n**Usage:**\n```\npython train.py --save-async\n```\n---------\nNOTE: **QUESTIONS**\n1. In the current implementation, we don't save `checkpoint_best` and `checkpoint_latest` since ioPath doesn't yet have a \"wait until a file is written and then copy/move it to another path\" feature. Is this okay for now?\n2. Should I mimic the atomic vs non-atomic save structure that synchronous Fairseq checkpoint writes have?\n\n**Note to Eric:** Keep this integration in check with D26375501.\n\nReviewed By: myleott\n\nDifferential Revision: D26467815\n\nfbshipit-source-id: 50068ef7bf9a6d5cea4d5e0d13d672604dc4a6b0"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-02-11T22:00:25Z",
        "message": "save task state in the checkpoint (#1562)\n\nSummary:\nthis allows tasks to declare some properties they'd like to save in the checkpoint (such as a dictionary), which are loaded when checkpoint is restored.\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1562\n\nTest Plan: tested by training a new wav2vec model, then finetuning it, then decoding it and making sure the dict only loaded once, during fine tuning process (and was obtained from checkpoint for decoding)\n\nReviewed By: myleott, gwenzek\n\nDifferential Revision: D25937974\n\nPulled By: alexeib\n\nfbshipit-source-id: b9908042f76ec8cda943f33885eb9b1f121662ae"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2021-02-02T23:51:11Z",
        "message": "Fix the task data arg conversion to string.\n\nSummary: We were getting some test failures on our end due to incompatibility of task data argument type. The actual exception is defined in this task: T83395097 and T83395052. Fixing the task data arg to be a string instead of list of strings.\n\nReviewed By: myleott\n\nDifferential Revision: D26205482\n\nfbshipit-source-id: d29d1ee7c469177e8bdad7ca603938f8450bd81c"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-31T04:16:56Z",
        "message": "Fix incorrect local cache for checkpoint_last.pt when training is restarted on the same host\n\nReviewed By: myleott\n\nDifferential Revision: D25719057\n\nfbshipit-source-id: 2bf7dd93b6d223804da0326a0ed347e5e353f1f0"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-23T19:16:56Z",
        "message": "fairseq checkpoint improvements\n\nReviewed By: myleott\n\nDifferential Revision: D25677238\n\nfbshipit-source-id: b43075034c953491211f19a5464148de4758df83"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-18T19:45:08Z",
        "message": "Refactor eval_lm to support library usage (#1513)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1513\n\nTest Plan: Imported from OSS\n\nReviewed By: alexeib\n\nDifferential Revision: D25570467\n\nPulled By: myleott\n\nfbshipit-source-id: 062f748e287797f4f01c605e0b544ef3e698851f"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-18T15:40:49Z",
        "message": "Support atomic saves for checkpoints (#1520)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1520\n\nTest Plan: Imported from OSS\n\nReviewed By: stephenroller\n\nDifferential Revision: D25632782\n\nPulled By: myleott\n\nfbshipit-source-id: bdbe2aed6254d0b023b33f8027dfbd939f1fd271"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-16T01:47:42Z",
        "message": "Fix loading of very old checkpoints (#1512)\n\nSummary:\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1512\n\nSee https://github.com/pytorch/fairseq/issues/3032 for context\n\nTest Plan: Imported from OSS\n\nReviewed By: alexeib\n\nDifferential Revision: D25570470\n\nPulled By: myleott\n\nfbshipit-source-id: 9227b1ca36cd81ff72acdb5e03fd574e3e8769be"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-08T23:49:12Z",
        "message": "fix wav2vec scripts (#1494)\n\nSummary:\nfixes #2942\n+ docs + migration of old models\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1494\n\nReviewed By: myleott\n\nDifferential Revision: D25404601\n\nPulled By: alexeib\n\nfbshipit-source-id: 092f145602522f8e7ea3eaa709bfe602a4d29d8b"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-12-05T15:37:51Z",
        "message": "Rename optimization.min_lr -> optimization.stop_min_lr (#1486)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1486\n\nTest Plan: Imported from OSS\n\nReviewed By: alexeib\n\nDifferential Revision: D25342181\n\nPulled By: myleott\n\nfbshipit-source-id: 7d1cfb26334fff26d688648724ab073e5fb956f5"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-11-30T22:20:36Z",
        "message": "Add/fix tests (#1468)\n\nSummary:\n- add test for loading ensemble checkpoints (and confirmed it fails if I revert: https://github.com/pytorch/fairseq/commit/265791b727b664d4d7da3abd918a3f6fb70d7337)\n- add test for LayerDrop (and fix it)\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1468\n\nReviewed By: alexeib\n\nDifferential Revision: D25223272\n\nPulled By: myleott\n\nfbshipit-source-id: 3f06f753605af251567c70d2961f5506ea423499"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-11-18T01:08:13Z",
        "message": "fix loading ensembles (#1442)\n\nSummary:\nfixes loading ensembles. previous change used the state of the first model for all models in the ensemble\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1442\n\nReviewed By: chtran\n\nDifferential Revision: D25035706\n\nPulled By: alexeib\n\nfbshipit-source-id: 9029999be0f1703efb1df20bec2890de59449f1f"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-11-11T18:15:10Z",
        "message": "load dataset with saved task config (optionally) (#1423)\n\nSummary:\nthis adds an argument to load_dataset that provides task configuration from the checkpoint. different tasks can decide what to do with it afterwards.\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1423\n\nReviewed By: myleott\n\nDifferential Revision: D24875706\n\nPulled By: alexeib\n\nfbshipit-source-id: 5bb1e2b7495520c456024dc7b0751b65cb05b473"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-11-09T23:46:00Z",
        "message": "migrate wav2vec2 model (#1409)\n\nSummary:\nsee title\nalso includes some minor bug fixes\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1409\n\nReviewed By: myleott\n\nDifferential Revision: D24822219\n\nPulled By: alexeib\n\nfbshipit-source-id: b18f9a8af42ced37880c23dd6ad1ec4df3dfc040"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-11-05T23:29:33Z",
        "message": "Move TPU grad reductions out of Trainer into TPUDistributedDataParallel (#1397)\n\nSummary:\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1397\n\nData parallel command: `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --share-decoder-input-output-embed --lr 0.0001`\n\nData parallel before:\n```\n2020-11-04 08:20:13 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)\n2020-11-04 08:20:13 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8\n2020-11-04 08:20:13 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt\n2020-11-04 08:20:13 | INFO | fairseq.trainer | loading train data for epoch 1\n2020-11-04 08:20:14 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train\n2020-11-04 08:20:14 | INFO | fairseq.optim.adam | using FusedAdam\n2020-11-04 08:20:14 | INFO | fairseq.trainer | begin training epoch 1\n2020-11-04 08:20:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=5\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108002, wps=160870, ups=4.91, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.8, wps=517232, ups=15.77, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.4, wps=537322, ups=16.38, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492.1, wps=540488, ups=16.48, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603.2, wps=543411, ups=16.57, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:19 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=540359, ups=16.47, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:20 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=533446, ups=16.26, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:20:20 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.4, wps=539734, ups=16.46, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=6\n```\n\nData parallel after:\n```\n2020-11-04 08:14:02 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)\n2020-11-04 08:14:02 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8\n2020-11-04 08:14:02 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt\n2020-11-04 08:14:02 | INFO | fairseq.trainer | loading train data for epoch 1\n2020-11-04 08:14:03 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train\n2020-11-04 08:14:03 | INFO | fairseq.optim.adam | using FusedAdam\n2020-11-04 08:14:03 | INFO | fairseq.trainer | begin training epoch 1\n2020-11-04 08:14:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108002, wps=157099, ups=4.79, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.8, wps=560049, ups=17.08, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.4, wps=558507, ups=17.03, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492.1, wps=514194, ups=15.68, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:08 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603.2, wps=552676, ups=16.85, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:09 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=546402, ups=16.66, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:09 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=508472, ups=15.5, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:09 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.4, wps=552493, ups=16.84, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=6\n```\n\nData parallel command (no_c10d): `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --share-decoder-input-output-embed --lr 0.0001 --dp-backend no_c10d`\n\nData parallel before:\n```\n2020-11-04 08:19:25 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)\n2020-11-04 08:19:25 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8\n2020-11-04 08:19:25 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt\n2020-11-04 08:19:25 | INFO | fairseq.trainer | loading train data for epoch 1\n2020-11-04 08:19:25 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train\n2020-11-04 08:19:26 | INFO | fairseq.optim.adam | using FusedAdam\n2020-11-04 08:19:26 | INFO | fairseq.trainer | begin training epoch 1\n2020-11-04 08:19:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n2020-11-04 08:19:31 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108001, wps=141659, ups=4.32, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.9, wps=503762, ups=15.36, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.5, wps=488599, ups=14.9, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492, wps=507855, ups=15.48, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603, wps=503270, ups=15.34, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=7\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=467778, ups=14.26, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=7\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=503800, ups=15.36, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=7\n2020-11-04 08:19:32 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.3, wps=468486, ups=14.28, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=7\n```\n\nData parallel after:\n```\n2020-11-04 08:14:50 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)\n2020-11-04 08:14:50 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8\n2020-11-04 08:14:50 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt\n2020-11-04 08:14:50 | INFO | fairseq.trainer | loading train data for epoch 1\n2020-11-04 08:14:50 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train\n2020-11-04 08:14:51 | INFO | fairseq.optim.adam | using FusedAdam\n2020-11-04 08:14:51 | INFO | fairseq.trainer | begin training epoch 1\n2020-11-04 08:14:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      2 / 3587 loss=19.682, ppl=841142, wps=0, ups=0, wpb=32768, bsz=64, num_updates=1, lr=0.0001, gnorm=13.17, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      3 / 3587 loss=16.721, ppl=108001, wps=137677, ups=4.2, wpb=32768, bsz=64, num_updates=2, lr=0.0001, gnorm=4.507, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      4 / 3587 loss=16.07, ppl=68785.9, wps=519541, ups=15.84, wpb=32768, bsz=64, num_updates=3, lr=0.0001, gnorm=2.737, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      5 / 3587 loss=15.714, ppl=53741.5, wps=517063, ups=15.76, wpb=32768, bsz=64, num_updates=4, lr=0.0001, gnorm=2.542, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      6 / 3587 loss=15.441, ppl=44492, wps=490728, ups=14.95, wpb=32768, bsz=64, num_updates=5, lr=0.0001, gnorm=2.485, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      7 / 3587 loss=15.199, ppl=37603, wps=505262, ups=15.41, wpb=32768, bsz=64, num_updates=6, lr=0.0001, gnorm=2.382, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:56 | INFO | train_inner | epoch 001:      8 / 3587 loss=14.984, ppl=32414, wps=508874, ups=15.52, wpb=32768, bsz=64, num_updates=7, lr=0.0001, gnorm=2.274, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:57 | INFO | train_inner | epoch 001:      9 / 3587 loss=14.7, ppl=26622.2, wps=518028, ups=15.79, wpb=32768, bsz=64, num_updates=8, lr=0.0001, gnorm=2.16, loss_scale=64, train_wall=0, wall=6\n2020-11-04 08:14:57 | INFO | train_inner | epoch 001:     10 / 3587 loss=14.482, ppl=22875.3, wps=515996, ups=15.73, wpb=32768, bsz=64, num_updates=9, lr=0.0001, gnorm=2.055, loss_scale=64, train_wall=0, wall=7\n```\n\nModel parallel command: `python train.py ~/data/data-bin/wikitext-103-roberta-bpe-bin/ --task language_modeling --arch transformer_lm_megatron --decoder-layers 4 --batch-size 8 --tokens-per-sample 512 --log-format simple --log-interval 1 --fp16 --optimizer adam --model-parallel-size 2 --share-decoder-input-output-embed --lr 0.0001`\n\nModel parallel before:\n```\n2020-11-04 08:18:38 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)\n2020-11-04 08:18:38 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8\n2020-11-04 08:18:38 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last-model_part-0.pt\n2020-11-04 08:18:38 | INFO | fairseq.trainer | loading train data for epoch 1\n2020-11-04 08:18:38 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train\n2020-11-04 08:18:39 | INFO | fairseq.optim.adam | using FusedAdam\n2020-11-04 08:18:39 | INFO | fairseq.trainer | begin training epoch 1\n2020-11-04 08:18:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n2020-11-04 08:18:45 | INFO | train_inner | epoch 001:      2 / 7173 loss=55.997, ppl=7.19017e+16, wps=0, ups=0, wpb=16384, bsz=32, num_updates=1, lr=0.0001, gnorm=14.03, loss_scale=64, train_wall=1, wall=7\n2020-11-04 08:18:45 | INFO | train_inner | epoch 001:      3 / 7173 loss=28.372, ppl=3.47501e+08, wps=48371.7, ups=2.95, wpb=16384, bsz=32, num_updates=2, lr=0.0001, gnorm=15.339, loss_scale=64, train_wall=0, wall=8\n2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      4 / 7173 loss=15.855, ppl=59276.8, wps=72422.5, ups=4.42, wpb=16384, bsz=32, num_updates=3, lr=0.0001, gnorm=4.189, loss_scale=64, train_wall=0, wall=8\n2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      5 / 7173 loss=14.713, ppl=26858.7, wps=72933.5, ups=4.45, wpb=16384, bsz=32, num_updates=4, lr=0.0001, gnorm=4.751, loss_scale=64, train_wall=0, wall=8\n2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      6 / 7173 loss=13.901, ppl=15299.7, wps=71974.8, ups=4.39, wpb=16384, bsz=32, num_updates=5, lr=0.0001, gnorm=4.361, loss_scale=64, train_wall=0, wall=8\n2020-11-04 08:18:46 | INFO | train_inner | epoch 001:      7 / 7173 loss=13.312, ppl=10169.5, wps=72897.8, ups=4.45, wpb=16384, bsz=32, num_updates=6, lr=0.0001, gnorm=3.307, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:18:47 | INFO | train_inner | epoch 001:      8 / 7173 loss=12.914, ppl=7720.21, wps=73044.6, ups=4.46, wpb=16384, bsz=32, num_updates=7, lr=0.0001, gnorm=5.473, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:18:47 | INFO | train_inner | epoch 001:      9 / 7173 loss=12.56, ppl=6036.72, wps=73453.1, ups=4.48, wpb=16384, bsz=32, num_updates=8, lr=0.0001, gnorm=6.112, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:18:47 | INFO | train_inner | epoch 001:     10 / 7173 loss=12.116, ppl=4437.77, wps=73442.6, ups=4.48, wpb=16384, bsz=32, num_updates=9, lr=0.0001, gnorm=4.415, loss_scale=64, train_wall=0, wall=9\n```\n\nModel parallel after:\n```\n2020-11-04 08:12:09 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)\n2020-11-04 08:12:09 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 8\n2020-11-04 08:12:09 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last-model_part-0.pt\n2020-11-04 08:12:09 | INFO | fairseq.trainer | loading train data for epoch 1\n2020-11-04 08:12:09 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /private/home/myleott/data/data-bin/wikitext-103-roberta-bpe-bin/train\n2020-11-04 08:12:10 | INFO | fairseq.optim.adam | using FusedAdam\n2020-11-04 08:12:10 | INFO | fairseq.trainer | begin training epoch 1\n2020-11-04 08:12:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      2 / 7173 loss=55.997, ppl=7.19017e+16, wps=0, ups=0, wpb=16384, bsz=32, num_updates=1, lr=0.0001, gnorm=14.03, loss_scale=64, train_wall=1, wall=8\n2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      3 / 7173 loss=28.372, ppl=3.47501e+08, wps=53097, ups=3.24, wpb=16384, bsz=32, num_updates=2, lr=0.0001, gnorm=15.339, loss_scale=64, train_wall=0, wall=8\n2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      4 / 7173 loss=15.855, ppl=59276.8, wps=72355.5, ups=4.42, wpb=16384, bsz=32, num_updates=3, lr=0.0001, gnorm=4.189, loss_scale=64, train_wall=0, wall=8\n2020-11-04 08:12:17 | INFO | train_inner | epoch 001:      5 / 7173 loss=14.713, ppl=26858.7, wps=70526.4, ups=4.3, wpb=16384, bsz=32, num_updates=4, lr=0.0001, gnorm=4.751, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      6 / 7173 loss=13.901, ppl=15299.7, wps=73063.5, ups=4.46, wpb=16384, bsz=32, num_updates=5, lr=0.0001, gnorm=4.361, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      7 / 7173 loss=13.312, ppl=10169.5, wps=73559.4, ups=4.49, wpb=16384, bsz=32, num_updates=6, lr=0.0001, gnorm=3.307, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      8 / 7173 loss=12.914, ppl=7720.21, wps=72693.2, ups=4.44, wpb=16384, bsz=32, num_updates=7, lr=0.0001, gnorm=5.473, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:12:18 | INFO | train_inner | epoch 001:      9 / 7173 loss=12.56, ppl=6036.72, wps=73531.2, ups=4.49, wpb=16384, bsz=32, num_updates=8, lr=0.0001, gnorm=6.112, loss_scale=64, train_wall=0, wall=9\n2020-11-04 08:12:19 | INFO | train_inner | epoch 001:     10 / 7173 loss=12.116, ppl=4437.77, wps=73187.6, ups=4.47, wpb=16384, bsz=32, num_updates=9, lr=0.0001, gnorm=4.415, loss_scale=64, train_wall=0, wall=10\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D24729295\n\nPulled By: myleott\n\nfbshipit-source-id: beee8bdece3eaa0419a2e813990420411e507c75"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-10-23T07:07:33Z",
        "message": "refactor dataclass related files, add proper types for static checkin\u2026 (#1371)\n\nSummary:\n- refactor dataclass/ hierarchy to make it a bit more sane (while avoiding circular references)\n- add top level FairseqConfig\n- change typehints to reflect the correct config type if it is known\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1371\n\nReviewed By: myleott\n\nDifferential Revision: D24469026\n\nPulled By: alexeib\n\nfbshipit-source-id: 01f68918f761d51ec5216286b8959ad35f41a7b2"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-10-22T23:31:49Z",
        "message": "rename remove_bpe to post_process; add aliasing (#1369)\n\nSummary:\nsome binaries (e.g. speech based ones) used --post-process, some used --remove-bpe. --post-process seems more appropriate as it does more than just remove bpe at the moment. this renames remove_bpe to post_process, adds alias so existing command lines would work and adds checkpoint upgrades so they continue to work also.\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1369\n\nReviewed By: myleott\n\nDifferential Revision: D24465040\n\nPulled By: alexeib\n\nfbshipit-source-id: 1b3e388291ccc403e76e069ef6606b80ead863a7"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-10-20T07:32:26Z",
        "message": "Enable Hydra configs in fairseq (#1343) (#1510)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/1510\n\nthis is the main pr that switches on hydra functionality in fairseq\n\nwe migrate \"args\" object into omegaconf \"DictConfig\" at all legacy entry points\n\nin addition this migrates various components from secondary registries (like bpe encoders and tokenizers) to make the migration smoother\n\ni am going through code that references migrated fairseq components and changing it to inherit from \"Legacy*\" components instead. hopefully tests will catch most of this\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1343\n\nReviewed By: myleott\n\nDifferential Revision: D23973928\n\nPulled By: alexeib\n\nfbshipit-source-id: dd9554981fff51ea75c1ff343874d1d6e61793c9"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-10-19T01:14:51Z",
        "message": "Apply black+isort (#1357)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1357\n\nReviewed By: alexeib\n\nDifferential Revision: D24377772\n\nfbshipit-source-id: 51581af041d42d62166b33a35a1a4228b1a76f0c"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-10-12T17:55:40Z",
        "message": "Support generation with huge pipeline parallel Transformer models (#1297)\n\nSummary:\n## What is this PR about?\n* Support loading sharded checkpoints (loading and saving checkpoints without sharding for 30B models runs OOM)\n* Ability to provide a fixed dictionary for multilingual machine translation to match the dictionary used for training the checkpoint\n* Support generation with PipelineParallelTransformer models\n\n## Testing\n\n```\npython fairseq_cli/generate.py \\\n    /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted  \\\n    --batch-size 1 \\\n    --path /large_exp/angelafan/checkpoints/shru/mmmt_5b_checkpoints_for_master/checkpoint4.pt \\\n    -s en -t es --remove-bpe 'sentencepiece' --beam 5 \\\n    --task translation_multi_simple_epoch \\\n    --lang-pairs /private/home/angelafan/mmt/lang100_bt_pairs.txt \\\n    --decoder-langtok --encoder-langtok src --gen-subset valid --fp16 \\\n    --dataset-impl mmap \\\n    --distributed-world-size 1 --distributed-no-spawn \\\n    --pipeline-model-parallel \\\n    --pipeline-chunks 1 \\\n    --pipeline-encoder-balance '[26]' \\\n    --pipeline-encoder-devices '[0]' \\\n    --pipeline-decoder-balance '[26]' \\\n    --pipeline-decoder-devices '[0]' \\\n    --fixed-dictionary /private/home/shru/projects/fairseq-py-kakaoval-mmt-save-redist-gen/dict.100langs.txt\n2020-09-23 23:18:27 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might ca\nuse misalignment in pretraining and finetuning.\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['af', 'am', 'ar', 'arbt', 'ast', 'az', 'azbt', 'ba', 'be', 'bebt', 'bg', 'bgbt', 'bn', 'bnbt', 'br', 'bs', 'ca', 'ceb', 'cs', 'csb\nt', 'cy', 'da', 'de', 'debt', 'el', 'en', 'es', 'esbt', 'et', 'etbt', 'fa', 'fabt', 'ff', 'fi', 'fr', 'frbt', 'fy', 'ga', 'gd', 'gl', 'gu', 'ha', 'he', 'hebt', 'hi', 'hibt', 'hr', 'ht', 'hu', 'hubt', 'hy', 'hybt', 'id', 'ig', 'ilo', 'is',\n 'it', 'ja', 'jabt', 'jv', 'ka', 'kabt', 'kk', 'km', 'kn', 'ko', 'kobt', 'lb', 'lg', 'ln', 'lo', 'lt', 'lv', 'mg', 'mk', 'mkbt', 'ml', 'mn', 'mr', 'mrbt', 'ms', 'msbt', 'my', 'ne', 'nebt', 'nl', 'no', 'ns', 'oc', 'or', 'pa', 'pl', 'ps', '\npt', 'ro', 'robt', 'ru', 'sd', 'si', 'sk', 'sl', 'so', 'sq', 'sr', 'srbt', 'ss', 'su', 'sv', 'sw', 'ta', 'th', 'tl', 'tn', 'tr', 'trbt', 'uk', 'ur', 'uz', 'vi', 'vibt', 'wo', 'xh', 'yi', 'yo', 'zh', 'zhbt', 'zu']\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [en] dictionary: 128112 types\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [es] dictionary: 128112 types\n2020-09-23 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None\n2020-09-23 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:en-es': 1}\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:en-es src_langtok: 128022; tgt_langtok: 128023\n2020-09-23 23:18:27 | INFO | fairseq.data.data_utils | loaded 4231 examples from: /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted/valid.en-es.en\n2020-09-23 23:18:27 | INFO | fairseq.data.data_utils | loaded 4231 examples from: /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted/valid.en-es.es\n2020-09-23 23:18:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | /large_exp/angelafan/cc100_multilingual/eval/binarized_spm_128k_dict/ted valid en-es 4231 examples\n2020-09-23 23:18:27 | INFO | fairseq_cli.generate | loading model(s) from /large_exp/angelafan/checkpoints/mmmt_100_langs_new_dict_5b_model/checkpoint4.pt\nbalance=[29,22,1], devices=[0,1,0], chunks=8, checkpoint=always\n2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler order indices time: 0:00:00.004368\n2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler filter_by_size time: 0:00:00.057953\n2020-09-23 23:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] batch_sampler batch_by_size time: 0:00:01.158644\n2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:01.223290\n2020-09-23 23:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\nS-2521  __en__ No.\nT-2521  No.\nH-2521  -2.4406352043151855     y no.\nD-2521  -2.4406352043151855     y no.\nP-2521  -5.2677 -4.4289 -0.1922 -2.1578 -0.1565\nS-2261  __en__ Why?\nT-2261  \u00bfPor qu\u00e9?\nH-2261  -1.7077901363372803     \u00bfY por qu\u00e9?\nD-2261  -1.7077901363372803     \u00bfY por qu\u00e9?\nP-2261  -5.2733 -0.7970 -3.7643 -0.5474 -0.3339 -1.0629 -0.1757\n```\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1297\n\nReviewed By: myleott, msbaines\n\nDifferential Revision: D23991647\n\nPulled By: shruti-bh\n\nfbshipit-source-id: 81ea1af64cbe75c8b53e050d2c2339dcb70fe1eb"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-10-09T20:34:59Z",
        "message": "Improve dictionary & checkpoint reading w/ local caching\n\nReviewed By: myleott\n\nDifferential Revision: D24148700\n\nfbshipit-source-id: 666300639243688939e137be748f7b76fc3c21a6"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-09-02T01:17:33Z",
        "message": "Initial support for ZeRO optimizer state sharding (#1259)\n\nSummary:\nFairseqOSS will work with any optimizer and dtype.\n\nTODO(future PR):\n* support reduce instead of all_reduce\n* support gradient sharding\n* support parameter sharding\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1259\n\nTest Plan:\nVerified that checkpoint save and restore work.\n\nVerified that grad_norm, loss, and ppl are identical with and without\nsharding enable.\n\nBefore:\n\n$ fairseq-train --task language_modeling   data-bin/wikitext-103   --save-dir checkpoints/transformer_wikitext-103   --arch transformer_lm --share-decoder-input-output-embed   --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 512 --sample-break-mode none   --max-tokens 2048 --update-freq 16   --max-update 50000  --memory-efficient-fp16 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50\n...\n2020-08-27 22:24:51 | INFO | train_inner | epoch 001:     49 / 394 loss=18.84, ppl=469411, wps=269226, ups=1.03, wpb=262144, bsz=512, num_updates=45, lr=5.72388e-06, gnorm=5.769, loss_scale=8, train_wall=1, wall=68\n2020-08-27 22:24:52 | INFO | train_inner | epoch 001:     50 / 394 loss=18.787, ppl=452312, wps=256992, ups=0.98, wpb=262144, bsz=512, num_updates=46, lr=5.84885e-06, gnorm=5.512, loss_scale=8, train_wall=1, wall=69\n2020-08-27 22:24:53 | INFO | train_inner | epoch 001:     51 / 394 loss=18.74, ppl=437735, wps=259178, ups=0.99, wpb=262144, bsz=512, num_updates=47, lr=5.97383e-06, gnorm=5.298, loss_scale=8, train_wall=1, wall=70\n2020-08-27 22:24:54 | INFO | train_inner | epoch 001:     52 / 394 loss=18.683, ppl=420727, wps=257710, ups=0.98, wpb=262144, bsz=512, num_updates=48, lr=6.0988e-06, gnorm=5.094, loss_scale=8, train_wall=1, wall=71\n2020-08-27 22:24:55 | INFO | train_inner | epoch 001:     53 / 394 loss=18.623, ppl=403794, wps=269279, ups=1.03, wpb=262144, bsz=512, num_updates=49, lr=6.22378e-06, gnorm=4.893, loss_scale=8, train_wall=1, wall=72\n2020-08-27 22:24:56 | INFO | train_inner | epoch 001:     54 / 394 loss=18.574, ppl=390255, wps=264616, ups=1.01, wpb=262144, bsz=512, num_updates=50, lr=6.34875e-06, gnorm=4.684, loss_scale=8, train_wall=1, wall=73\n2020-08-27 22:24:56 | INFO | fairseq_cli.train | begin save checkpoint\n2020-08-27 22:24:56 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n2020-08-27 22:24:56 | INFO | train | epoch 001 | loss 19.736 | ppl 873122 | wps 264825 | ups 1.01 | wpb 262144 | bsz 512 | num_updates 50 | lr 6.34875e-06 | gnorm 8.898 | loss_scale 8 | train_wall 66 | wall 73\n2020-08-27 22:24:56 | INFO | fairseq_cli.train | done training in 72.2 seconds\n\nAfter:\n\n$ fairseq-train --task language_modeling   data-bin/wikitext-103   --save-dir checkpoints/transformer_wikitext-103   --arch transformer_lm --share-decoder-input-output-embed   --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 512 --sample-break-mode none   --max-tokens 2048 --update-freq 16   --max-update 50000  --memory-efficient-fp16 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --zero-sharding os\n...\n2020-08-27 22:22:55 | INFO | train_inner | epoch 001:     49 / 394 loss=18.84, ppl=469411, wps=267663, ups=1.02, wpb=262144, bsz=512, num_updates=45, lr=5.72388e-06, gnorm=5.769, loss_scale=8, train_wall=1, wall=68\n2020-08-27 22:22:56 | INFO | train_inner | epoch 001:     50 / 394 loss=18.787, ppl=452312, wps=252797, ups=0.96, wpb=262144, bsz=512, num_updates=46, lr=5.84885e-06, gnorm=5.512, loss_scale=8, train_wall=1, wall=69\n2020-08-27 22:22:57 | INFO | train_inner | epoch 001:     51 / 394 loss=18.74, ppl=437735, wps=267692, ups=1.02, wpb=262144, bsz=512, num_updates=47, lr=5.97383e-06, gnorm=5.298, loss_scale=8, train_wall=1, wall=70\n2020-08-27 22:22:58 | INFO | train_inner | epoch 001:     52 / 394 loss=18.683, ppl=420727, wps=267507, ups=1.02, wpb=262144, bsz=512, num_updates=48, lr=6.0988e-06, gnorm=5.094, loss_scale=8, train_wall=1, wall=71\n2020-08-27 22:22:59 | INFO | train_inner | epoch 001:     53 / 394 loss=18.623, ppl=403794, wps=254410, ups=0.97, wpb=262144, bsz=512, num_updates=49, lr=6.22378e-06, gnorm=4.893, loss_scale=8, train_wall=1, wall=72\n2020-08-27 22:23:00 | INFO | train_inner | epoch 001:     54 / 394 loss=18.574, ppl=390255, wps=268234, ups=1.02, wpb=262144, bsz=512, num_updates=50, lr=6.34875e-06, gnorm=4.684, loss_scale=8, train_wall=1, wall=73\n2020-08-27 22:23:00 | INFO | fairseq_cli.train | begin save checkpoint\n2020-08-27 22:23:00 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n2020-08-27 22:23:00 | INFO | train | epoch 001 | loss 19.736 | ppl 873122 | wps 263570 | ups 1.01 | wpb 262144 | bsz 512 | num_updates 50 | lr 6.34875e-06 | gnorm 8.898 | loss_scale 8 | train_wall 66 | wall 73\n2020-08-27 22:23:00 | INFO | fairseq_cli.train | done training in 72.3 seconds\n\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nReviewed By: myleott\n\nDifferential Revision: D23432082\n\nPulled By: msbaines\n\nfbshipit-source-id: 6a020b25e36a3d9283582b7d89a6a53038e5b181"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-08-14T17:24:51Z",
        "message": "Misc fixes (#2448)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2448\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D23011193\n\nPulled By: myleott\n\nfbshipit-source-id: 1a29481707108e4465aca78ec1581fb79f05efba"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-08-06T17:20:39Z",
        "message": "Multilingual v1: Multilingual Training with multiple bitext and monolingual datasets: add finetuning options\n\nSummary:\nA first version of XLNMT multilingual project code release: Multilingual Training with multiple bitext\n\n- Minor changes to\n    - fairseq/checkpoint_utils.py to add finetuning option instead of using restore_file which will restore from original model when being requeued.\n\nReviewed By: myleott\n\nDifferential Revision: D22483494\n\nfbshipit-source-id: 733300fd6a4d185e561c793ea668047c96f616c6"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-08-04T15:25:50Z",
        "message": "Fixes checkpoint_path while loading a model-parallel checkpoint (#2365)\n\nSummary:\nFixes https://github.com/pytorch/fairseq/issues/2351\n\nPull Request resolved: https://github.com/pytorch/fairseq/pull/2365\n\nReviewed By: pipibjc\n\nDifferential Revision: D22727384\n\nPulled By: myleott\n\nfbshipit-source-id: e2ff703181a6b8f10df9b4ee7aa3f9e128c04b4e"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-05-26T22:59:59Z",
        "message": "Several small fixes (incl. set default --data-buffer-size=10) (#2163)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2163\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D21665601\n\nPulled By: myleott\n\nfbshipit-source-id: 47673ff7f07acf0002c4e28380aa08ff917618ee"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-05-11T17:34:42Z",
        "message": "Generalize moving of tensors to CPU in checkpoints (#2098)\n\nSummary:\nThis is needed for TPUs\nPull Request resolved: https://github.com/pytorch/fairseq/pull/2098\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D21455095\n\nPulled By: myleott\n\nfbshipit-source-id: f0f88e715884f44bc254b71f7694ac90200d92fc"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-04-14T17:58:38Z",
        "message": "update checkpoint mkdir behavior (issue #1986) (#2011)\n\nSummary:\nCreate checkpoint directory in ```save_checkpoint()```instead of ```load_checkpoint()```.\n\nhttps://github.com/pytorch/fairseq/issues/1986\nPull Request resolved: https://github.com/pytorch/fairseq/pull/2011\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D21017208\n\nPulled By: myleott\n\nfbshipit-source-id: 4f76afc1751f987b8ab2c170ca306d07bd5ffe83"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-03-28T03:22:36Z",
        "message": "adding code to load and save model parallel checkpoint (#1119)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1119\n\nReviewed By: myleott\n\nDifferential Revision: D20712488\n\nfbshipit-source-id: 941ef251c9e2deb8933d88188fac56ee8c5be9b7"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-03-11T07:36:45Z",
        "message": "PySpeech TALNet: Convert to JIT and quantize\n\nSummary:\nUpdate the TALNet model so it can be converted to JIT format and quantized by the `jit_pyspeech_nn_model` tool.\n\nThe updated model structure is slight incompatible with the model files saved before (the model structure has the extra parameters `fc_att.weight` and `fc_att.bias`), so old model files must be loaded with `strict=False`. This diff also makes changes to allow `strict=False` where necessary.\n\nAlso renames the options of `jit_pyspeech_nn_model` to make them conform better to convention.\n\nReviewed By: jay-mahadeokar\n\nDifferential Revision: D20369643\n\nfbshipit-source-id: 0950a26abc20d0ae5929c8d0a03f83cab4a59200"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-03-07T17:12:14Z",
        "message": "Fix epoch reporting when restoring checkpoint (#1075)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1075\n\nDifferential Revision: D20322672\n\nPulled By: myleott\n\nfbshipit-source-id: 8845a10b10442bed77670b7740afa271123a3b5d"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-03-05T00:37:24Z",
        "message": "Use 1-based indexing for epochs everywhere (#1053)\n\nSummary:\nWe are somewhat inconsistent in whether we're using 0-based or 1-based indexing for epochs. This should fix things to be 0-based internally, with logging and checkpoint naming still using 1-based indexing.\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1053\n\nReviewed By: spencerp\n\nDifferential Revision: D20160715\n\nPulled By: myleott\n\nfbshipit-source-id: 4ed94f9c371e1bfe29bcfa087fa6756507d6e627"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-02-19T04:07:05Z",
        "message": "tts synthesis script\n\nSummary: add a synth.py to pyspeech to run tts synthesis for a particular text.\n\nDifferential Revision: D19786089\n\nfbshipit-source-id: 2aadd004609dfbcf477e58bc01e2b05df19c0e26"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-01-22T23:36:28Z",
        "message": "fblearner pyspeech manifold migration\n\nSummary:\nwas planning to migrate everything at once. but because pyspeech and tuna diverged from D19060980, it's too complicated to do the migration.\n\nmigrate save_dir to manifold in fblearner, and copy from manifold to gluster after training to keep downstream workflows happy.\n\nReviewed By: zdavid1995\n\nDifferential Revision: D19433205\n\nfbshipit-source-id: bd8d969c001952d85167a63f4d55fe97b93aceb5"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-01-20T20:15:27Z",
        "message": "fix the problem of passing None to format() when val_loss is None (e.\u2026 (#1633)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [x ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ x] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\nPull Request resolved: https://github.com/pytorch/fairseq/pull/1633\n\nDifferential Revision: D19470727\n\nPulled By: myleott\n\nfbshipit-source-id: a0bbefe20b4692306c57104f3e545912e20055e6"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-01-17T00:14:45Z",
        "message": "Switch to Python logging (+ lint) (#1627)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/fairseq/pull/1627\n\nPython logging offers a number of benefits, such as logging timestamps, better\ncross-library compatibility, ability to add multiple output handlers, etc.\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/646\n\nReviewed By: spencerp\n\nDifferential Revision: D15815620\n\nPulled By: myleott\n\nfbshipit-source-id: 5e64e9929b5e4b9dd5bb49bcdf7c510631907134"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2020-01-16T09:59:15Z",
        "message": "Split from PR#968. add --keep-best-checkpoints (#990)\n\nSummary:\nFixes https://github.com/fairinternal/fairseq-py/issues/968. Split --keep-best-checkpoints from the original request.\nUse scores as the names to save the checkpoints\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/990\n\nDifferential Revision: D19411250\n\nPulled By: MultiPath\n\nfbshipit-source-id: 82b0db614208eee54c9c0e470ad7faa6481747d5"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-12-17T01:22:11Z",
        "message": "More fully deprecate --raw-text and --lazy-load (fixes #1488)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/947\n\nDifferential Revision: D19084273\n\nPulled By: myleott\n\nfbshipit-source-id: de80d9abfac8e3d813a9c9b343b41327c500344e"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-12-06T02:24:56Z",
        "message": "Add wrapper abstraction on top of fvcore PathManager to insulate OSS compatibility\n\nReviewed By: myleott\n\nDifferential Revision: D18736914\n\nfbshipit-source-id: 897ba7463f7a8ebc0969c01c57bfe04fbf9e71c8"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-12-02T21:26:48Z",
        "message": "Apply Black auto-formatting\n\nReviewed By: sujitoc\n\nDifferential Revision: D18738392\n\nfbshipit-source-id: b7b7b75ef97946786c463c1887ef9a8676f030e6"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-11-22T01:50:42Z",
        "message": "Quick fix for Structured Dropout checkpointing (#1406)\n\nSummary:\nHere's a quick fix for https://github.com/pytorch/fairseq/issues/1403.\n\nTo keep it short, this fix allows the user to checkpoint a translation model properly after applying layer pruning on a restored transformer file.\nPull Request resolved: https://github.com/pytorch/fairseq/pull/1406\n\nDifferential Revision: D18637540\n\nPulled By: myleott\n\nfbshipit-source-id: 0f5e91e05e6579f0f459bc5293e9b14cb267322d"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-11-21T00:52:59Z",
        "message": "Refactor data sharding to be specified via caller of task rather than task itself\n\nSummary: Modifying number of shards internally to disable data sharding for batch iteration is dangerous because the caller of these tasks is not limited to fairspeq/train. So therefore we should put the onus of data sharding properly on the caller rather than the task itself.\n\nReviewed By: myleott\n\nDifferential Revision: D18456424\n\nfbshipit-source-id: d46be16c441c50082f9a768d0b259e6c28a4b67b"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-10-30T19:55:54Z",
        "message": "layer drop\n\nSummary: This diff enables layer drop in transformer decoder in production training pipeline (ptt_transformer). It builds on top of the fairseq implementation D18094657 added by Angela Fan, and added additional logic to handle corresponding dropping layers at test time in exported model.\n\nReviewed By: jhcross\n\nDifferential Revision: D18165586\n\nfbshipit-source-id: 373ac00268a25fa9e412edcb483becdfe792d992"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-10-27T19:10:53Z",
        "message": "adding layerdrop code for training, pruning, and readme (#890)\n\nSummary:\nTEST 1: EVALUATION TIME WORKS\nchecked\nachieves correct model perplexity: 18.68\n\nTEST 2: TRAINING NEW MODEL WORKS\nchecked\n\nwithout layerdrop:\n--decoder-layerdrop 0 OR no flag at all\n| epoch 001:     10 / 11201 loss=27.469, nll_loss=27.469, ppl=185799477.36, wps=1764, ups=0, wpb=9216.000, bsz=3.000, num_updates=7, lr=0.0004376, gnorm=25.471, clip=1.000, oom=0.000, loss_scale=8.000, wall=37, train_wall=30\n| epoch 001:     20 / 11201 loss=27.443, nll_loss=27.443, ppl=182500427.22, wps=2449, ups=0, wpb=9216.000, bsz=3.000, num_updates=17, lr=0.0010626, gnorm=25.273, clip=1.000, oom=0.000, loss_scale=8.000, wall=64, train_wall=57\n| epoch 001:     30 / 11201 loss=27.404, nll_loss=27.404, ppl=177612215.78, wps=2720, ups=0, wpb=9216.000, bsz=3.000, num_updates=27, lr=0.0016876, gnorm=25.136, clip=1.000, oom=0.000, loss_scale=8.000, wall=91, train_wall=84\n| epoch 001:     40 / 11201 loss=27.009, nll_loss=27.009, ppl=135079983.00, wps=2865, ups=0, wpb=9216.000, bsz=3.000, num_updates=37, lr=0.0023126, gnorm=24.311, clip=1.000, oom=0.000, loss_scale=8.000, wall=119, train_wall=112\n| epoch 001:     50 / 11201 loss=26.418, nll_loss=26.418, ppl=89680259.41, wps=2952, ups=0, wpb=9216.000, bsz=3.000, num_updates=47, lr=0.0029376, gnorm=22.775, clip=1.000, oom=0.000, loss_scale=8.000, wall=147, train_wall=140\n\nwith layerdrop (regularization effect should be seen in PPL):\n--decoder-layerdrop 0.2\n\n| epoch 001:     10 / 11201 loss=25.186, nll_loss=25.186, ppl=38182937.27, wps=2428, ups=0, wpb=9216.000, bsz=3.000, num_updates=8, lr=0.0005001, gnorm=17.082, clip=1.000, oom=0.000, loss_scale=16.000, wall=30, train_wall=24\n| epoch 001:     20 / 11201 loss=25.270, nll_loss=25.270, ppl=40451933.50, wps=3173, ups=0, wpb=9216.000, bsz=3.000, num_updates=18, lr=0.0011251, gnorm=17.162, clip=1.000, oom=0.000, loss_scale=16.000, wall=52, train_wall=45\n| epoch 001:     30 / 11201 loss=25.349, nll_loss=25.349, ppl=42752256.68, wps=3454, ups=0, wpb=9216.000, bsz=3.000, num_updates=28, lr=0.0017501, gnorm=17.370, clip=1.000, oom=0.000, loss_scale=16.000, wall=75, train_wall=68\n| epoch 001:     40 / 11201 loss=25.115, nll_loss=25.115, ppl=36343806.30, wps=3619, ups=0, wpb=9216.000, bsz=3.000, num_updates=38, lr=0.0023751, gnorm=16.945, clip=1.000, oom=0.000, loss_scale=16.000, wall=97, train_wall=90\n| epoch 001:     50 / 11201 loss=24.804, nll_loss=24.804, ppl=29284345.78, wps=3716, ups=0, wpb=9216.000, bsz=3.000, num_updates=48, lr=0.0030001, gnorm=16.406, clip=1.000, oom=0.000, loss_scale=16.000, wall=119, train_wall=112\n\nTEST 3: PICKING UP TRAINING FROM EXISTING MODEL\nchecked\n\n| loaded checkpoint /checkpoint/angelafan/structured_0.1_block_8_sd02/checkpoint_last.pt (epoch 272 @ 381066 updates)\n| loading train data for epoch 272\n| loaded 1801350 examples from: /private/home/angelafan/lm_work/fairseq-py/data-bin/wikitext-103/train\n\nTEST 4: EVALUATING EXISTING BERT MODEL REPROS RESULTS\n| [input] dictionary: 50265 types\n| [label] dictionary: 9 types\n| Accuracy:  0.9231651376146789\nachieves correct accuracy on SST2 for this model\n\nTEST 5: TRAINING NEW BERT MODEL WORKS\nchecked and works\n\nTEST 6: NMT\n\nwithout layerdrop\n--encoder-layerdrop 0 --decoder-layerdrop 0 OR combinations of flag specified and not specified\n\n| epoch 001:     10 / 92203 loss=15.820, nll_loss=15.830, ppl=58267.93, wps=4902, ups=0, wpb=1477.818, bsz=51.636, num_updates=11, lr=1.47473e-06, gnorm=7.207, clip=0.000, oom=0.000, loss_scale=128.000, wall=60, train_wall=3\n| epoch 001:     20 / 92203 loss=15.523, nll_loss=15.501, ppl=46359.29, wps=5037, ups=0, wpb=1496.476, bsz=45.333, num_updates=21, lr=2.72448e-06, gnorm=6.869, clip=0.000, oom=0.000, loss_scale=128.000, wall=63, train_wall=6\n| epoch 001:     30 / 92203 loss=15.185, nll_loss=15.123, ppl=35695.79, wps=5085, ups=0, wpb=1519.355, bsz=44.645, num_updates=31, lr=3.97423e-06, gnorm=6.186, clip=0.000, oom=0.000, loss_scale=128.000, wall=66, train_wall=9\n| epoch 001:     40 / 92203 loss=14.940, nll_loss=14.849, ppl=29505.60, wps=5116, ups=1, wpb=1521.244, bsz=42.927, num_updates=41, lr=5.22398e-06, gnorm=5.610, clip=0.000, oom=0.000, loss_scale=128.000, wall=69, train_wall=12\n| epoch 001:     50 / 92203 loss=14.745, nll_loss=14.630, ppl=25346.87, wps=5070, ups=1, wpb=1507.961, bsz=41.725, num_updates=51, lr=6.47373e-06, gnorm=5.104, clip=0.000, oom=0.000, loss_scale=128.000, wall=71, train_wall=15\n\nwith layerdrop (regularization effect should be seen in PPL)\n\nA) works with --encoder-layerdrop 0.2 --decoder-layerdrop 0.2\nB) works with different settings --encoder-layerdrop 0.3 --decoder-layerdrop 0.5\nC) works with one on and one off --encoder-layerdrop 0.2 --decoder-layerdrop 0\n\n| epoch 001:     10 / 92203 loss=15.817, nll_loss=15.828, ppl=58158.54, wps=5355, ups=0, wpb=1477.818, bsz=51.636, num_updates=11, lr=1.47473e-06, gnorm=6.959, clip=0.000, oom=0.000, loss_scale=128.000, wall=59, train_wall=3\n| epoch 001:     20 / 92203 loss=15.650, nll_loss=15.641, ppl=51111.63, wps=5515, ups=0, wpb=1496.476, bsz=45.333, num_updates=21, lr=2.72448e-06, gnorm=6.825, clip=0.000, oom=0.000, loss_scale=128.000, wall=61, train_wall=6\n| epoch 001:     30 / 92203 loss=15.440, nll_loss=15.408, ppl=43491.58, wps=5602, ups=0, wpb=1519.355, bsz=44.645, num_updates=31, lr=3.97423e-06, gnorm=6.576, clip=0.000, oom=0.000, loss_scale=128.000, wall=64, train_wall=8\n| epoch 001:     40 / 92203 loss=15.247, nll_loss=15.193, ppl=37457.14, wps=5676, ups=1, wpb=1521.244, bsz=42.927, num_updates=41, lr=5.22398e-06, gnorm=6.124, clip=0.000, oom=0.000, loss_scale=128.000, wall=67, train_wall=11\n| epoch 001:     50 / 92203 loss=15.055, nll_loss=14.977, ppl=32259.92, wps=5598, ups=1, wpb=1507.961, bsz=41.725, num_updates=51, lr=6.47373e-06, gnorm=5.661, clip=0.000, oom=0.000, loss_scale=128.000, wall=69, train_wall=14\n\nTEST 7: PRUNING TESTCASES\n\nA) after adding the pruning flags, model can evaluate as a full model\nchecked, reaches correct PPL\nnum. model params: 246933504\n| Evaluated 217646 tokens in 196.3s (1108.99 tokens/s)\n| Loss: 2.9275, Perplexity: 18.68\n\nB) after adding pruning flags, model can be pruned. this works with multiple flag settings\nchecked three cases:\nnum. model params: 146163712\n| Evaluated 217646 tokens in 106.0s (2054.07 tokens/s)\n| Loss: 3.0932, Perplexity: 22.05\n\nnum. model params: 209144832\n| Evaluated 217646 tokens in 162.8s (1336.99 tokens/s)\n| Loss: 2.9526, Perplexity: 19.16\n\nC) model can pick up training if you want to finetune the pruned model\nchecked:\n| loading train data for epoch 272\n| loaded 1801350 examples from: /private/home/angelafan/lm_work/fairseq-py/data-bin/wikitext-103/train\n| WARNING: overflow detected, setting loss scale to: 64.0\n| WARNING: overflow detected, setting loss scale to: 32.0\n| epoch 272:   1500 / 5601 loss=5.015, nll_loss=5.015, ppl=32.33, wps=11598, ups=1, wpb=18432.000, bsz=6.000, num_updates=98, lr=0.0061251, gnorm=0.613, clip=1.000, oom=0.000, loss_scale=32.000, wall=156, train_wall=252396\n\nD) works with BERT\nchecked:\nwithout specifying any flags, reproduces the correct standard accuracy\nwith flags, produces the correct pruned accuracy\n\n| [input] dictionary: 50265 types\n| [label] dictionary: 9 types\n| Accuracy:  0.9231651376146789\n\n| [input] dictionary: 50265 types\n| [label] dictionary: 9 types\n| Pruning model to specified layer configuration - this works best if the model was trained with LayerDrop\n| Accuracy:  0.9220183486238532\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/890\n\nReviewed By: edunov\n\nDifferential Revision: D18094657\n\nPulled By: huihuifan\n\nfbshipit-source-id: 2bbaa2ff0039e906782694fc2038b8c17a8693e7"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-10-12T04:53:11Z",
        "message": "Added option to save checkpoints using Path Manager.\n\nSummary: Added option to save checkpoints using Path Manager.\n\nReviewed By: hudeven\n\nDifferential Revision: D17392754\n\nfbshipit-source-id: 4b8e556ef8455a1548e5a083d779ed809cd785be"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-10-09T21:53:27Z",
        "message": "Fix data loading memory issue in pyspeech\n\nSummary:\nWe currently shard data when creating the batch iterator. This means we first load all indicese/frame lengths/handles into memory, and then do the sharding. This makes it impossible to train on large datasets with a high amount of workers  because each worker will need to load the entire dataset into memory. For training on a million hours of data (i.e. semi-supervised or unsupervised approaches) this data loading just makes it flat out impossible to use 8 GPU's.\n\n3 changes:\n\n1. This diff modifies the data loading such that we do the sharding while we read the handles file, rather than later. This modification is done on a task-by-task basis, since the task specifies how the data is loaded. I've tried to make the code compatible with both sharding during handle loading and sharding during batch iteration. I've currently only done the sharding during handle loading for the aligned_training task.\n\n2. To support data sharding at data loading time and the requirement that all shards must have exactly the same # of batches, I've added a method to do this synchronization where all shards with too many batches would just truncate the extra ones, similar to what we already do.\n\n2. In fairspeq/train.py, we are actually loading the training dataset and batch iterator twice, once in train.py and once when loading the checkpoint (which we always do regardless if there is a checkpoint). This means double the loading time which can be painful for very large files. I've removed the extraneous loading in this diff as well.\n\nReviewed By: yqwangustc\n\nDifferential Revision: D17750715\n\nfbshipit-source-id: 0e6e3d363525fa5661f1c784303390ea13f46377"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-09-20T16:34:58Z",
        "message": "added multilingual masked LM training (#849)\n\nSummary:\nThe multilingual-RoBERTa training is working with aconneau XLM data.\n\nTwo pieces remaining:\n\n1) `XLM` limits batch to be from same language, I am not 100% sure about the reason for that, but should be easy to implement, basically we can add `batch_by_size_and_language` instead of default `batch_by_size` function. If it's not critical, I would want to leave it out as it keeps the code very clean and simple.\n\n2) `sample_ratio` in `ConcatDataset` works with `int` by tiling the datasets based on ratio. Currently I am handling it by sounding off the ratio to `first decimal` and then multiplying by `10`. We can see if some such simple heuristics are good enough, there are other options (we can talk about them offline).\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/849\n\nDifferential Revision: D17162460\n\nfbshipit-source-id: d967f3d872f7a1f0aa4ea418bd362b68af9e432f"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-08-21T20:43:01Z",
        "message": "Parameterized criterions (#808)\n\nSummary:\nSupport criterion with parameters, such as AutoSegmentationCriterion (ASG) used in wav2letter which has a transition matrix parameter. This is needed to integrate wav2letter's ASG into PySpeech.\n\nWith this diff, parameters in criterions will be:\n(1) updated by optimizers, with a configurable learning rate\n(2) saved and loaded from checkpoints, preserving backward compatibility for criterions without parameters\n(3) synchronized across nodes in distributed training.\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/808\n\nReviewed By: jcai1\n\nDifferential Revision: D16934097\n\nPulled By: okhonko\n\nfbshipit-source-id: 121ec9382459385c6f9cbef3a8274bec1a434038"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-08-12T16:08:16Z",
        "message": "Lint\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/817\n\nDifferential Revision: D16762905\n\nPulled By: myleott\n\nfbshipit-source-id: d920595bec44ed26b72dfc6fbc15c0aa107b4e56"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-08-12T14:17:21Z",
        "message": "Update --restore-file logic (partially fixes #999)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1007\n\nDifferential Revision: D16762490\n\nPulled By: myleott\n\nfbshipit-source-id: d67137bcf581887850323d188bb4ea643a35ac9e"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-08-10T15:16:50Z",
        "message": "Add WSC task and criterion\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1004\n\nDifferential Revision: D16751443\n\nPulled By: myleott\n\nfbshipit-source-id: f70acd6c7be6d69da45b5b32fe4c4eff021539ab"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-08-01T12:55:57Z",
        "message": "Update PyTorch Hub interface\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/782\n\nDifferential Revision: D16542256\n\nPulled By: myleott\n\nfbshipit-source-id: ea3279e7a1ce4687a5914f32b76787c419be1ffa"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-07-30T14:48:23Z",
        "message": "Relicense fairseq under MIT license (#786)\n\nSummary:\nThe previous BSD+PATENTS license was controversial. We have been\napproved to relicense fairseq under the MIT license.\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/786\n\nDifferential Revision: D16560654\n\nPulled By: myleott\n\nfbshipit-source-id: f78b1beb4f2895dd7b9bfc79f5f952a2bfb94034"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-07-29T23:06:26Z",
        "message": "adding glue data preprocessing scripts (#771)\n\nSummary:\n1) Added glue data pre-processing script.\n2) updated README with usage.\n\nTODO:\n1) releasing fairseq dictionary and remove hardcoded path.\n2) remove hard-coded path for bpe-encoding,\n\nmyleott what do you recommend for above TODOs?\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/771\n\nReviewed By: myleott\n\nDifferential Revision: D16547679\n\nPulled By: myleott\n\nfbshipit-source-id: 6a6562d9b6215523d048fdf3daee63ffac21e231"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-07-24T23:59:07Z",
        "message": "check save_dir before beginning training\n\nSummary: I sadly discovery that my checkpoint directory wasn't globally readable after 8 hours of training. Adding this check at the beginning of train loop to keep that from happening again!\n\nReviewed By: myleott\n\nDifferential Revision: D16455394\n\nfbshipit-source-id: 35959aa058150b2afb63710c468d01ebc8a12b0c"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-07-19T20:13:43Z",
        "message": "Rename _load_model_ensemble -> load_model_ensemble_and_task\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/738\n\nDifferential Revision: D16377803\n\nPulled By: myleott\n\nfbshipit-source-id: 6beb2f78e7464b70ff65a965d2b747cdca0ca951"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-07-01T21:10:49Z",
        "message": "Fixes checkpointing bug introduced in 89e077c\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/847\n\nDifferential Revision: D16075498\n\nPulled By: myleott\n\nfbshipit-source-id: 62e27a8c4764f53f181c502674dfab1e6b0537e2"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-06-30T18:35:33Z",
        "message": "Add additional options for configuring writing of checkpoints\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/697\n\nDifferential Revision: D16068465\n\nPulled By: myleott\n\nfbshipit-source-id: c2563c3c682e7e8406e6d7c8e895d8afbec551eb"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-06-11T18:19:58Z",
        "message": "Automatically fill in default values from add_args\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/797\n\nDifferential Revision: D15761071\n\nPulled By: myleott\n\nfbshipit-source-id: 257d4a2297e83da7e59baed154dbafd6bfe614bf"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-05-30T18:41:40Z",
        "message": "Add --reset-dataloader\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/613\n\nDifferential Revision: D15541384\n\nPulled By: myleott\n\nfbshipit-source-id: ef2c0b0a51cdf37af2ccff0546f524d49f87e65d"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-05-17T21:25:56Z",
        "message": "Small features + lint\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/588\n\nDifferential Revision: D15389638\n\nPulled By: myleott\n\nfbshipit-source-id: 4632ce22d51dc2c74d250bae999630095d849701"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-05-17T04:03:08Z",
        "message": "Clean up sharded train iterator\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/586\n\nDifferential Revision: D15372949\n\nPulled By: myleott\n\nfbshipit-source-id: c1cf1c645e8d55fc8568f23a47c45677ac9ab1da"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-05-14T19:57:12Z",
        "message": "Move save/load checkpoint functions to utils\n\nSummary:\nMove `load_checkpoint`, `save_checkpoint` and `reload_train` from train.py to checkpoint_utils.py\nMove `get_perplexity` from train.py to utils.py.\nThis will make train.py lighter and allow us to reuse all this utils functionality when fairseq is used as external library.\n\nReviewed By: myleott\n\nDifferential Revision: D15289607\n\nfbshipit-source-id: 4b7c95225ac22e402bcda3497811361809110df1"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-05-11T14:56:45Z",
        "message": "Add missing options to TransformerDecoderLayer\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/560\n\nDifferential Revision: D15260838\n\nPulled By: myleott\n\nfbshipit-source-id: 5f80dd82775c10ce46a3e1c451ccaf0ef55bfa31"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-05-06T07:17:45Z",
        "message": "Load pretrained encoder or decoder (#705)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/fairseq/pull/705\n\nThis adds functionality in fairseq to load a pretrained encoder or decoder from another pretrained model into the current model.\n\nReviewed By: jmp84\n\nDifferential Revision: D15207084\n\nfbshipit-source-id: 32a710ff77389928e20793c71d312863df9dd8ae"
    },
    {
        "repo_url": "github.com/imatge-upc/sign-topic",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2019-04-30T02:50:58Z",
        "message": "Merge internal changes (#654)\n\nSummary:\n- Add --add-bos-token option to LM task\n- Cleanup utils.py and options.py\nPull Request resolved: https://github.com/pytorch/fairseq/pull/654\n\nDifferential Revision: D15041794\n\nPulled By: myleott\n\nfbshipit-source-id: 3ad00007769d5f48308052cfd40de39c5ffa1a6e"
    },
    {
        "repo_url": "github.com/SillyTavern/SillyTavern-extras",
        "filepath": "modules/voice_conversion/fairseq/checkpoint_utils.py",
        "commit_date": "2023-08-10T00:58:52Z",
        "message": "Add monkey patched fairseq package to run on python 3.11 (what is needed for our use of RVC at least)"
    },
    {
        "repo_url": "github.com/zjwang21/mix-phoneme-bert",
        "filepath": "fairseq/fairseq/checkpoint_utils.py",
        "commit_date": "2023-03-21T10:50:41Z",
        "message": "first"
    },
    {
        "repo_url": "github.com/StrongResearch/isc-demos",
        "filepath": "fairseq/fairseq/checkpoint_utils.py",
        "commit_date": "2023-12-03T22:15:32Z",
        "message": "atomic checkpointing"
    },
    {
        "repo_url": "github.com/StrongResearch/isc-demos",
        "filepath": "fairseq/fairseq/checkpoint_utils.py",
        "commit_date": "2023-12-01T04:49:47Z",
        "message": "add fairseq how to"
    },
    {
        "repo_url": "github.com/JeremyXSC/DMF",
        "filepath": "fairseq/fairseq/checkpoint_utils.py",
        "commit_date": "2022-10-26T11:38:01Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/Rongjiehuang/TranSpeech",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2023-03-29T15:24:17Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/epfml/pam",
        "filepath": "submodules/fairseq/fairseq/checkpoint_utils.py",
        "commit_date": "2023-05-31T09:00:54Z",
        "message": "Public release"
    },
    {
        "repo_url": "github.com/TakHemlata/SSL_Anti-spoofing",
        "filepath": "fairseq-a54021305d6b3c4c5959ac9395135f63202db8f1/fairseq/checkpoint_utils.py",
        "commit_date": "2022-05-02T17:59:04Z",
        "message": "SSL_w2v_spoofing_release"
    },
    {
        "repo_url": "github.com/chenllliang/Gradient-Vaccine",
        "filepath": "fairseq/fairseq/checkpoint_utils.py",
        "commit_date": "2022-07-27T17:43:39Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/fe1ixxu/Intra-Distillation",
        "filepath": "Machine_Translation/fairseq/checkpoint_utils.py",
        "commit_date": "2022-05-13T23:52:30Z",
        "message": "rename folder"
    },
    {
        "repo_url": "github.com/UncleTensor/AudioSubnet",
        "filepath": "fseq/fairseq/checkpoint_utils.py",
        "commit_date": "2024-02-12T17:35:20Z",
        "message": "Added Text to Music models, custom models option and auto_update (yes/no)"
    },
    {
        "repo_url": "github.com/alibaba/translation-suggestion-psgd",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2023-06-08T10:01:42Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/jungokasai/beam_with_patience",
        "filepath": "fairseq/fairseq/checkpoint_utils.py",
        "commit_date": "2022-03-24T07:18:58Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/hadasah/btm",
        "filepath": "fairseq/fairseq/checkpoint_utils.py",
        "commit_date": "2022-11-12T02:21:49Z",
        "message": "add fairseq fork"
    },
    {
        "repo_url": "github.com/demelin/transformer_lexical_shortcuts",
        "filepath": "fairseq/fairseq/checkpoint_utils.py",
        "commit_date": "2023-02-14T21:20:11Z",
        "message": "added fairseq implementation"
    },
    {
        "repo_url": "github.com/lsj2408/Graphormer-GD",
        "filepath": "engine/fairseq/checkpoint_utils.py",
        "commit_date": "2023-03-04T05:40:53Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/gonglinyuan/ast_t5",
        "filepath": "training/fairseq/checkpoint_utils.py",
        "commit_date": "2024-02-06T18:19:44Z",
        "message": "Reorganize"
    },
    {
        "repo_url": "github.com/JeanKaddour/lawa",
        "filepath": "bert/fairseq/fairseq/checkpoint_utils.py",
        "commit_date": "2022-10-05T16:22:43Z",
        "message": "Change fairseq to standalone repo"
    },
    {
        "repo_url": "github.com/mcao516/rej-summ",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2022-12-20T02:16:54Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/gpengzhi/Bi-SimCut",
        "filepath": "fairseq/fairseq/checkpoint_utils.py",
        "commit_date": "2022-07-25T06:05:38Z",
        "message": "Add fairseq"
    },
    {
        "repo_url": "github.com/0nutation/DUB",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2023-06-01T07:27:07Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/keyonvafa/career-code",
        "filepath": "fairseq/fairseq/checkpoint_utils.py",
        "commit_date": "2022-03-09T04:39:13Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/ictnlp/DASpeech",
        "filepath": "fairseq/fairseq/checkpoint_utils.py",
        "commit_date": "2023-10-10T13:47:12Z",
        "message": "Initial Release"
    },
    {
        "repo_url": "github.com/Ascend/ModelZoo-PyTorch",
        "filepath": "PyTorch/contrib/audio/wav2vec2.0/fairseq/checkpoint_utils.py",
        "commit_date": "2022-11-25T02:31:16Z",
        "message": "!1572 [\u91cd\u5e86\u5927\u5b66][\u9ad8\u6821\u8d21\u732e][Pytorch][Wav2vec2.0]--\u521d\u6b21\u63d0\u4ea4\n!1572 [\u91cd\u5e86\u5927\u5b66][\u9ad8\u6821\u8d21\u732e][Pytorch][Wav2vec2.0]--\u521d\u6b21\u63d0\u4ea4"
    },
    {
        "repo_url": "github.com/Ascend/ModelZoo-PyTorch",
        "filepath": "PyTorch/built-in/nlp/Data2vec_for_PyTorch/fairseq/checkpoint_utils.py",
        "commit_date": "2023-06-01T07:03:34Z",
        "message": "!4809 \u3010PyTorch\u3011\u3010built-in\u3011\u3010data2vec\u3011\u521d\u6b21\u63d0\u4ea4\n* data2vec \u9996\u6b21\u63d0\u4ea4"
    },
    {
        "repo_url": "github.com/Ascend/ModelZoo-PyTorch",
        "filepath": "PyTorch/built-in/nlp/Scaling-nmt_for_Pytorch/fairseq/checkpoint_utils.py",
        "commit_date": "2023-07-04T13:23:06Z",
        "message": "!5109 \u3010PyTorch\u3011\u3010built-in\u3011\u3010scaling-nmt\u3011\u521d\u6b21\u63d0\u4ea4\n* scaling-nmt \u521d\u6b21\u63d0\u4ea4"
    },
    {
        "repo_url": "github.com/Ascend/ModelZoo-PyTorch",
        "filepath": "PyTorch/built-in/nlp/Fairseq_Transformer_wmt18_for_PyTorch/fairseq/checkpoint_utils.py",
        "commit_date": "2023-05-26T08:35:33Z",
        "message": "[\u81ea\u7814][PyTorch][Fairseq_Transformer_wmt18 for PyTorch] init"
    },
    {
        "repo_url": "github.com/MANGA-UOFA/NAUS",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2022-03-16T17:57:43Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/tencent-ailab/TriNet",
        "filepath": "fairseq/checkpoint_utils.py",
        "commit_date": "2023-05-29T08:04:43Z",
        "message": "add files"
    },
    {
        "repo_url": "github.com/NLP2CT/kNN-TL",
        "filepath": "kNN-TL/fairseq/checkpoint_utils.py",
        "commit_date": "2023-07-04T08:51:56Z",
        "message": "\tmodified:   .DS_Store \n\tmodified:   README.md\n\tnew file:   inference-scripts/build_parent_datastore.sh\n\tnew file:   inference-scripts/inference_child.sh\n\tnew file:   inference-scripts/inference_pseudo_parent.sh\n\tnew file:   kNN-TL/.circleci/config.yml\n\tnew file:   kNN-TL/.github/CODEOWNERS\n\tnew file:   kNN-TL/.github/ISSUE_TEMPLATE.md\n\tnew file:   kNN-TL/.github/ISSUE_TEMPLATE/bug_report.md\n\tnew file:   kNN-TL/.github/ISSUE_TEMPLATE/documentation.md\n\tnew file:   kNN-TL/.github/ISSUE_TEMPLATE/feature_request.md\n\tnew file:   kNN-TL/.github/ISSUE_TEMPLATE/how-to-question.md\n\tnew file:   kNN-TL/.github/PULL_REQUEST_TEMPLATE.md\n\tnew file:   kNN-TL/.github/stale.yml\n\tnew file:   kNN-TL/.github/workflows/build.yml\n\tnew file:   kNN-TL/.github/workflows/release.yml\n\tnew file:   kNN-TL/.gitignore\n\tnew file:   kNN-TL/.gitmodules\n\tnew file:   kNN-TL/.isort.cfg\n\tnew file:   kNN-TL/.pre-commit-config.yaml\n\tnew file:   kNN-TL/CODE_OF_CONDUCT.md\n\tnew file:   kNN-TL/CONTRIBUTING.md\n\tnew file:   kNN-TL/LICENSE\n\tnew file:   kNN-TL/README.md\n\tnew file:   kNN-TL/RELEASE.md\n\tnew file:   kNN-TL/docs/Makefile\n\tnew file:   kNN-TL/docs/_static/theme_overrides.css\n\tnew file:   kNN-TL/docs/command_line_tools.rst\n\tnew file:   kNN-TL/docs/conf.py\n\tnew file:   kNN-TL/docs/criterions.rst\n\tnew file:   kNN-TL/docs/data.rst\n\tnew file:   kNN-TL/docs/docutils.conf\n\tnew file:   kNN-TL/docs/fairseq.gif\n\tnew file:   kNN-TL/docs/fairseq_logo.png\n\tnew file:   kNN-TL/docs/getting_started.rst\n\tnew file:   kNN-TL/docs/hydra_integration.md\n\tnew file:   kNN-TL/docs/index.rst\n\tnew file:   kNN-TL/docs/lr_scheduler.rst\n\tnew file:   kNN-TL/docs/make.bat\n\tnew file:   kNN-TL/docs/models.rst\n\tnew file:   kNN-TL/docs/modules.rst\n\tnew file:   kNN-TL/docs/optim.rst\n\tnew file:   kNN-TL/docs/overview.rst\n\tnew file:   kNN-TL/docs/requirements.txt\n\tnew file:   kNN-TL/docs/tasks.rst\n\tnew file:   kNN-TL/docs/tutorial_classifying_names.rst\n\tnew file:   kNN-TL/docs/tutorial_simple_lstm.rst\n\tnew file:   kNN-TL/examples/.gitignore\n\tnew file:   kNN-TL/examples/MMPT/.gitignore\n\tnew file:   kNN-TL/examples/MMPT/CONFIG.md\n\tnew file:   kNN-TL/examples/MMPT/DATASET.md\n\tnew file:   kNN-TL/examples/MMPT/README.md\n\tnew file:   kNN-TL/examples/MMPT/endtask.md\n\tnew file:   kNN-TL/examples/MMPT/locallaunch.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/__init__.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/datasets/__init__.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/datasets/fairseqmmdataset.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/datasets/mmdataset.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/evaluators/__init__.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/evaluators/evaluator.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/evaluators/metric.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/evaluators/predictor.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/losses/__init__.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/losses/fairseqmmloss.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/losses/loss.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/losses/nce.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/models/__init__.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/models/fairseqmmmodel.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/models/mmfusion.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/models/mmfusionnlg.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/models/transformermodel.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/modules/__init__.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/modules/mm.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/modules/retri.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/modules/vectorpool.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/processors/__init__.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/processors/dedupprocessor.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/processors/dsprocessor.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/processors/how2processor.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/processors/how2retriprocessor.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/processors/models/s3dg.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/processors/processor.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/tasks/__init__.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/tasks/fairseqmmtask.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/tasks/milncetask.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/tasks/retritask.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/tasks/task.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/tasks/vlmtask.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/utils/__init__.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/utils/load_config.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt/utils/shardedtensor.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt_cli/localjob.py\n\tnew file:   kNN-TL/examples/MMPT/mmpt_cli/predict.py\n\tnew file:   kNN-TL/examples/MMPT/pretraining.md\n\tnew file:   kNN-TL/examples/MMPT/projects/mfmmlm.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/mtm/mmfusionmtm.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/mtm/vlm.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/mtm/vlm/coin.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/mtm/vlm/crosstask.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/mtm/vlm/how2.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/mtm/vlm/test_coin.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/mtm/vlm/test_crosstask.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/mtm/vlm/test_crosstask_zs.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/mtm/vlm/test_vtt.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/mtm/vlm/test_vttqa.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/mtm/vlm/test_youcook.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/mtm/vlm/test_youcookcap.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/mtm/vlm/vtt.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/mtm/vlm/vttqa.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/mtm/vlm/youcook.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/mtm/vlm/youcookcap.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/retri/videoclip.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/retri/videoclip/coin_videoclip.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/retri/videoclip/crosstask_videoclip.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/retri/videoclip/how2.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/retri/videoclip/test_coin_videoclip.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/retri/videoclip/test_coin_zs.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/retri/videoclip/test_crosstask_videoclip.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/retri/videoclip/test_crosstask_zs_videoclip.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/retri/videoclip/test_didemo_zs.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/retri/videoclip/test_vtt_videoclip.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/retri/videoclip/test_vtt_zs.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/retri/videoclip/test_vttqa_videoclip.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/retri/videoclip/test_vttqa_zs.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/retri/videoclip/test_youcook_videoclip.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/retri/videoclip/test_youcook_zs.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/retri/videoclip/vtt_videoclip.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/retri/videoclip/vttqa_videoclip.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/retri/videoclip/youcook_videoclip.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/retri/videoretri.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/coin.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/coin_videoclip.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/crosstask.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/crosstask_videoclip.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/default.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/ft.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/how2.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/test.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/test_coin.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/test_coin_videoclip.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/test_coin_zs.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/test_crosstask.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/test_crosstask_videoclip.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/test_crosstask_zs.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/test_crosstask_zs_videoclip.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/test_didemo_zs.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/test_vtt.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/test_vtt_videoclip.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/test_vtt_zs.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/test_vttqa.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/test_vttqa_videoclip.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/test_vttqa_zs.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/test_youcook.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/test_youcook_videoclip.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/test_youcook_zs.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/test_youcookcap.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/vtt.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/vtt_videoclip.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/vttqa.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/vttqa_videoclip.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/youcook.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/youcook_videoclip.yaml\n\tnew file:   kNN-TL/examples/MMPT/projects/task/youcookcap.yaml\n\tnew file:   kNN-TL/examples/MMPT/scripts/text_token_extractor/configs/bert-base-uncased.yaml\n\tnew file:   kNN-TL/examples/MMPT/scripts/text_token_extractor/pretokenization.py\n\tnew file:   kNN-TL/examples/MMPT/scripts/video_feature_extractor/extract.py\n\tnew file:   kNN-TL/examples/MMPT/scripts/video_feature_extractor/how2/s3d.sh\n\tnew file:   kNN-TL/examples/MMPT/scripts/video_feature_extractor/model.py\n\tnew file:   kNN-TL/examples/MMPT/scripts/video_feature_extractor/pathbuilder.py\n\tnew file:   kNN-TL/examples/MMPT/scripts/video_feature_extractor/preprocessing.py\n\tnew file:   kNN-TL/examples/MMPT/scripts/video_feature_extractor/random_sequence_shuffler.py\n\tnew file:   kNN-TL/examples/MMPT/scripts/video_feature_extractor/shard_feature.py\n\tnew file:   kNN-TL/examples/MMPT/scripts/video_feature_extractor/videoreader.py\n\tnew file:   kNN-TL/examples/MMPT/setup.py\n\tnew file:   kNN-TL/examples/MMPT/videoclip.png\n\tnew file:   kNN-TL/examples/MMPT/vlm.png\n\tnew file:   kNN-TL/examples/__init__.py\n\tnew file:   kNN-TL/examples/adaptive_span/README.md\n\tnew file:   kNN-TL/examples/adaptive_span/__init__.py\n\tnew file:   kNN-TL/examples/adaptive_span/adagrad_with_grad_clip.py\n\tnew file:   kNN-TL/examples/adaptive_span/adaptive_span_attention.py\n\tnew file:   kNN-TL/examples/adaptive_span/adaptive_span_loss.py\n\tnew file:   kNN-TL/examples/adaptive_span/adaptive_span_model.py\n\tnew file:   kNN-TL/examples/adaptive_span/adaptive_span_model_wrapper.py\n\tnew file:   kNN-TL/examples/adaptive_span/truncated_bptt_lm_task.py\n\tnew file:   kNN-TL/examples/attention_head_selection/README.md\n\tnew file:   kNN-TL/examples/attention_head_selection/src/__init__.py\n\tnew file:   kNN-TL/examples/attention_head_selection/src/data/__init__.py\n\tnew file:   kNN-TL/examples/attention_head_selection/src/data/speech_to_text_dataset_with_domain.py\n\tnew file:   kNN-TL/examples/attention_head_selection/src/loss/__init__.py\n\tnew file:   kNN-TL/examples/attention_head_selection/src/loss/attention_head_selection.py\n\tnew file:   kNN-TL/examples/attention_head_selection/src/models/__init__.py\n\tnew file:   kNN-TL/examples/attention_head_selection/src/models/head_selection_s2t_transformer.py\n\tnew file:   kNN-TL/examples/attention_head_selection/src/models/head_selection_transformer.py\n\tnew file:   kNN-TL/examples/attention_head_selection/src/modules/__init__.py\n\tnew file:   kNN-TL/examples/attention_head_selection/src/modules/attn_head_selector.py\n\tnew file:   kNN-TL/examples/attention_head_selection/src/modules/head_selection_transformer_layer.py\n\tnew file:   kNN-TL/examples/attention_head_selection/src/modules/multihead_attention_selection.py\n\tnew file:   kNN-TL/examples/attention_head_selection/src/modules/multihead_functional.py\n\tnew file:   kNN-TL/examples/attention_head_selection/src/speech_to_text_head_selection.py\n\tnew file:   kNN-TL/examples/backtranslation/README.md\n\tnew file:   kNN-TL/examples/backtranslation/deduplicate_lines.py\n\tnew file:   kNN-TL/examples/backtranslation/extract_bt_data.py\n\tnew file:   kNN-TL/examples/backtranslation/prepare-de-monolingual.sh\n\tnew file:   kNN-TL/examples/backtranslation/prepare-wmt18en2de.sh\n\tnew file:   kNN-TL/examples/backtranslation/sacrebleu.sh\n\tnew file:   kNN-TL/examples/backtranslation/tokenized_bleu.sh\n\tnew file:   kNN-TL/examples/bart/README.glue.md\n\tnew file:   kNN-TL/examples/bart/README.md\n\tnew file:   kNN-TL/examples/bart/README.summarization.md\n\tnew file:   kNN-TL/examples/bart/summarize.py\n\tnew file:   kNN-TL/examples/byte_level_bpe/README.md\n\tnew file:   kNN-TL/examples/byte_level_bpe/get_bitext.py\n\tnew file:   kNN-TL/examples/byte_level_bpe/get_data.sh\n\tnew file:   kNN-TL/examples/byte_level_bpe/gru_transformer.py\n\tnew file:   kNN-TL/examples/camembert/README.md\n\tnew file:   kNN-TL/examples/constrained_decoding/README.md\n\tnew file:   kNN-TL/examples/constrained_decoding/normalize.py\n\tnew file:   kNN-TL/examples/constrained_decoding/tok.py\n\tnew file:   kNN-TL/examples/conv_seq2seq/README.md\n\tnew file:   kNN-TL/examples/criss/README.md\n\tnew file:   kNN-TL/examples/criss/download_and_preprocess_flores_test.sh\n\tnew file:   kNN-TL/examples/criss/download_and_preprocess_tatoeba.sh\n\tnew file:   kNN-TL/examples/criss/mining/mine.py\n\tnew file:   kNN-TL/examples/criss/mining/mine_example.sh\n\tnew file:   kNN-TL/examples/criss/save_encoder.py\n\tnew file:   kNN-TL/examples/criss/sentence_retrieval/encoder_analysis.py\n\tnew file:   kNN-TL/examples/criss/sentence_retrieval/sentence_retrieval_tatoeba.sh\n\tnew file:   kNN-TL/examples/criss/unsupervised_mt/eval.sh\n\tnew file:   kNN-TL/examples/cross_lingual_language_model/README.md\n\tnew file:   kNN-TL/examples/data2vec/README.md\n\tnew file:   kNN-TL/examples/data2vec/config/audio/pretraining/base_librispeech.yaml\n\tnew file:   kNN-TL/examples/data2vec/config/text/pretraining/base.yaml\n\tnew file:   kNN-TL/examples/data2vec/models/data2vec_audio.py\n\tnew file:   kNN-TL/examples/data2vec/models/data2vec_text.py\n\tnew file:   kNN-TL/examples/discriminative_reranking_nmt/README.md\n\tnew file:   kNN-TL/examples/discriminative_reranking_nmt/__init__.py\n\tnew file:   kNN-TL/examples/discriminative_reranking_nmt/config/deen.yaml\n\tnew file:   kNN-TL/examples/discriminative_reranking_nmt/criterions/__init__.py\n\tnew file:   kNN-TL/examples/discriminative_reranking_nmt/criterions/discriminative_reranking_criterion.py\n\tnew file:   kNN-TL/examples/discriminative_reranking_nmt/drnmt_rerank.py\n\tnew file:   kNN-TL/examples/discriminative_reranking_nmt/models/__init__.py\n\tnew file:   kNN-TL/examples/discriminative_reranking_nmt/models/discriminative_reranking_model.py\n\tnew file:   kNN-TL/examples/discriminative_reranking_nmt/scripts/prep_data.py\n\tnew file:   kNN-TL/examples/discriminative_reranking_nmt/tasks/__init__.py\n\tnew file:   kNN-TL/examples/discriminative_reranking_nmt/tasks/discriminative_reranking_task.py\n\tnew file:   kNN-TL/examples/fast_noisy_channel/README.md\n\tnew file:   kNN-TL/examples/fast_noisy_channel/__init__.py\n\tnew file:   kNN-TL/examples/fast_noisy_channel/noisy_channel_beam_search.py\n\tnew file:   kNN-TL/examples/fast_noisy_channel/noisy_channel_sequence_generator.py\n\tnew file:   kNN-TL/examples/fast_noisy_channel/noisy_channel_translation.py\n\tnew file:   kNN-TL/examples/flores101/README.md\n\tnew file:   kNN-TL/examples/flores101/flores_logo.png\n\tnew file:   kNN-TL/examples/fully_sharded_data_parallel/README.md\n\tnew file:   kNN-TL/examples/gottbert/README.md\n\tnew file:   kNN-TL/examples/hubert/README.md\n\tnew file:   kNN-TL/examples/hubert/config/decode/ax_sweep/ngram.yaml\n\tnew file:   kNN-TL/examples/hubert/config/decode/ax_sweep/transformer.yaml\n\tnew file:   kNN-TL/examples/hubert/config/decode/infer_fsqlm.yaml\n\tnew file:   kNN-TL/examples/hubert/config/decode/infer_kenlm.yaml\n\tnew file:   kNN-TL/examples/hubert/config/decode/infer_viterbi.yaml\n\tnew file:   kNN-TL/examples/hubert/config/decode/run/submitit_slurm.yaml\n\tnew file:   kNN-TL/examples/hubert/config/decode/run/submitit_slurm_8gpu.yaml\n\tnew file:   kNN-TL/examples/hubert/config/finetune/base_10h.yaml\n\tnew file:   kNN-TL/examples/hubert/config/finetune/ckpt/it1.yaml\n\tnew file:   kNN-TL/examples/hubert/config/finetune/lm/ls_4gram.yaml\n\tnew file:   kNN-TL/examples/hubert/config/finetune/run/submitit_reg.yaml\n\tnew file:   kNN-TL/examples/hubert/config/pretrain/data/iter1.yaml\n\tnew file:   kNN-TL/examples/hubert/config/pretrain/data/iter2.yaml\n\tnew file:   kNN-TL/examples/hubert/config/pretrain/hubert_base_librispeech.yaml\n\tnew file:   kNN-TL/examples/hubert/config/pretrain/hubert_large_librivox.yaml\n\tnew file:   kNN-TL/examples/hubert/config/pretrain/hubert_xlarge_librivox.yaml\n\tnew file:   kNN-TL/examples/hubert/config/pretrain/run/submitit_reg.yaml\n\tnew file:   kNN-TL/examples/hubert/measure_teacher_quality.py\n\tnew file:   kNN-TL/examples/hubert/simple_kmeans/README.md\n\tnew file:   kNN-TL/examples/hubert/simple_kmeans/dump_hubert_feature.py\n\tnew file:   kNN-TL/examples/hubert/simple_kmeans/dump_hubert_feature_s2t.py\n\tnew file:   kNN-TL/examples/hubert/simple_kmeans/dump_km_label.py\n\tnew file:   kNN-TL/examples/hubert/simple_kmeans/dump_mfcc_feature.py\n\tnew file:   kNN-TL/examples/hubert/simple_kmeans/dump_w2v2_feature.py\n\tnew file:   kNN-TL/examples/hubert/simple_kmeans/feature_utils.py\n\tnew file:   kNN-TL/examples/hubert/simple_kmeans/learn_kmeans.py\n\tnew file:   kNN-TL/examples/hubert/tests/6313-76958-0021.flac\n\tnew file:   kNN-TL/examples/hubert/tests/sample.base.L9.km500.km\n\tnew file:   kNN-TL/examples/hubert/tests/sample.base.L9.len\n\tnew file:   kNN-TL/examples/hubert/tests/sample.base.L9.npy\n\tnew file:   kNN-TL/examples/hubert/tests/sample.large.L20.len\n\tnew file:   kNN-TL/examples/hubert/tests/sample.large.L20.npy\n\tnew file:   kNN-TL/examples/hubert/tests/sample.large.hypo.word\n\tnew file:   kNN-TL/examples/hubert/tests/sample.xlarge.L30.len\n\tnew file:   kNN-TL/examples/hubert/tests/sample.xlarge.L30.npy\n\tnew file:   kNN-TL/examples/hubert/tests/sample.xlarge.hypo.word\n\tnew file:   kNN-TL/examples/hubert/tests/test_feature_and_unit.sh\n\tnew file:   kNN-TL/examples/hubert/tests/test_finetuned_asr.sh\n\tnew file:   kNN-TL/examples/hubert/update_ckpt.py\n\tnew file:   kNN-TL/examples/joint_alignment_translation/README.md\n\tnew file:   kNN-TL/examples/joint_alignment_translation/prepare-wmt18en2de_no_norm_no_escape_no_agressive.sh\n\tnew file:   kNN-TL/examples/language_model/README.adaptive_inputs.md\n\tnew file:   kNN-TL/examples/language_model/README.conv.md\n\tnew file:   kNN-TL/examples/language_model/README.md\n\tnew file:   kNN-TL/examples/language_model/prepare-wikitext-103.sh\n\tnew file:   kNN-TL/examples/laser/README.md\n\tnew file:   kNN-TL/examples/laser/laser_src/__init__.py\n\tnew file:   kNN-TL/examples/laser/laser_src/laser_lstm.py\n\tnew file:   kNN-TL/examples/laser/laser_src/laser_task.py\n\tnew file:   kNN-TL/examples/laser/laser_src/laser_transformer.py\n\tnew file:   kNN-TL/examples/laser/laser_src/multitask_data_utils.py\n\tnew file:   kNN-TL/examples/latent_depth/README.md\n\tnew file:   kNN-TL/examples/latent_depth/latent_depth_src/__init__.py\n\tnew file:   kNN-TL/examples/latent_depth/latent_depth_src/loss/__init__.py\n\tnew file:   kNN-TL/examples/latent_depth/latent_depth_src/loss/latent_depth.py\n\tnew file:   kNN-TL/examples/latent_depth/latent_depth_src/models/__init__.py\n\tnew file:   kNN-TL/examples/latent_depth/latent_depth_src/models/latent_multilingual_transformer.py\n\tnew file:   kNN-TL/examples/latent_depth/latent_depth_src/models/latent_transformer.py\n\tnew file:   kNN-TL/examples/latent_depth/latent_depth_src/modules/__init__.py\n\tnew file:   kNN-TL/examples/latent_depth/latent_depth_src/modules/latent_layers.py\n\tnew file:   kNN-TL/examples/latent_depth/latent_depth_src/multilingual_translation_latent_depth.py\n\tnew file:   kNN-TL/examples/layerdrop/README.md\n\tnew file:   kNN-TL/examples/linformer/README.md\n\tnew file:   kNN-TL/examples/linformer/linformer_src/__init__.py\n\tnew file:   kNN-TL/examples/linformer/linformer_src/models/__init__.py\n\tnew file:   kNN-TL/examples/linformer/linformer_src/models/linformer_roberta.py\n\tnew file:   kNN-TL/examples/linformer/linformer_src/modules/__init__.py\n\tnew file:   kNN-TL/examples/linformer/linformer_src/modules/linformer_sentence_encoder.py\n\tnew file:   kNN-TL/examples/linformer/linformer_src/modules/linformer_sentence_encoder_layer.py\n\tnew file:   kNN-TL/examples/linformer/linformer_src/modules/multihead_linear_attention.py\n\tnew file:   kNN-TL/examples/m2m_100/README.md\n\tnew file:   kNN-TL/examples/m2m_100/install_dependecies.sh\n\tnew file:   kNN-TL/examples/m2m_100/process_data/clean_histogram.py\n\tnew file:   kNN-TL/examples/m2m_100/process_data/dedup_data.py\n\tnew file:   kNN-TL/examples/m2m_100/process_data/remove_too_much_punc.py\n\tnew file:   kNN-TL/examples/m2m_100/tok.sh\n\tnew file:   kNN-TL/examples/m2m_100/tokenizers/README.md\n\tnew file:   kNN-TL/examples/m2m_100/tokenizers/seg_ja.sh\n\tnew file:   kNN-TL/examples/m2m_100/tokenizers/seg_ko.sh\n\tnew file:   kNN-TL/examples/m2m_100/tokenizers/thirdparty/.gitignore\n\tnew file:   kNN-TL/examples/m2m_100/tokenizers/tokenize_indic.py\n\tnew file:   kNN-TL/examples/m2m_100/tokenizers/tokenize_thai.py\n\tnew file:   kNN-TL/examples/m2m_100/tokenizers/tokenize_zh.py\n\tnew file:   kNN-TL/examples/m2m_100/tokenizers/tokenizer_ar.sh\n\tnew file:   kNN-TL/examples/mbart/README.md\n\tnew file:   kNN-TL/examples/megatron_11b/README.md\n\tnew file:   kNN-TL/examples/megatron_11b/detok.py\n\tnew file:   kNN-TL/examples/moe_lm/README.md\n\tnew file:   kNN-TL/examples/moe_lm/data_card.md\n\tnew file:   kNN-TL/examples/moe_lm/model_card.md\n\tnew file:   kNN-TL/examples/multilingual/ML50_langs.txt\n\tnew file:   kNN-TL/examples/multilingual/README.md\n\tnew file:   kNN-TL/examples/multilingual/data_scripts/README.md\n\tnew file:   kNN-TL/examples/multilingual/data_scripts/binarize.py\n\tnew file:   kNN-TL/examples/multilingual/data_scripts/check_iswlt_test_data.py\n\tnew file:   kNN-TL/examples/multilingual/data_scripts/check_self_overlaps.py\n\tnew file:   kNN-TL/examples/multilingual/data_scripts/check_valid_test_overlaps.py\n\tnew file:   kNN-TL/examples/multilingual/data_scripts/dedup_all.py\n\tnew file:   kNN-TL/examples/multilingual/data_scripts/download_ML50_v1.sh\n\tnew file:   kNN-TL/examples/multilingual/data_scripts/download_af_xh.sh\n\tnew file:   kNN-TL/examples/multilingual/data_scripts/download_flores_data.sh\n\tnew file:   kNN-TL/examples/multilingual/data_scripts/download_iitb.sh\n\tnew file:   kNN-TL/examples/multilingual/data_scripts/download_iwslt_and_extract.sh\n\tnew file:   kNN-TL/examples/multilingual/data_scripts/download_lotus.sh\n\tnew file:   kNN-TL/examples/multilingual/data_scripts/download_ted_and_extract.py\n\tnew file:   kNN-TL/examples/multilingual/data_scripts/download_wat19_my.sh\n\tnew file:   kNN-TL/examples/multilingual/data_scripts/download_wmt19_and_before.py\n\tnew file:   kNN-TL/examples/multilingual/data_scripts/download_wmt20.sh\n\tnew file:   kNN-TL/examples/multilingual/data_scripts/preprocess_ML50_v1.sh\n\tnew file:   kNN-TL/examples/multilingual/data_scripts/remove_valid_test_in_train.py\n\tnew file:   kNN-TL/examples/multilingual/data_scripts/requirement.txt\n\tnew file:   kNN-TL/examples/multilingual/data_scripts/utils/dedup.py\n\tnew file:   kNN-TL/examples/multilingual/data_scripts/utils/fasttext_multi_filter.py\n\tnew file:   kNN-TL/examples/multilingual/data_scripts/utils/strip_sgm.sh\n\tnew file:   kNN-TL/examples/multilingual/finetune_multilingual_model.sh\n\tnew file:   kNN-TL/examples/multilingual/multilingual_fairseq_gen.sh\n\tnew file:   kNN-TL/examples/multilingual/train_multilingual_model.sh\n\tnew file:   kNN-TL/examples/noisychannel/README.md\n\tnew file:   kNN-TL/examples/noisychannel/__init__.py\n\tnew file:   kNN-TL/examples/noisychannel/rerank.py\n\tnew file:   kNN-TL/examples/noisychannel/rerank_generate.py\n\tnew file:   kNN-TL/examples/noisychannel/rerank_options.py\n\tnew file:   kNN-TL/examples/noisychannel/rerank_score_bw.py\n\tnew file:   kNN-TL/examples/noisychannel/rerank_score_lm.py\n\tnew file:   kNN-TL/examples/noisychannel/rerank_tune.py\n\tnew file:   kNN-TL/examples/noisychannel/rerank_utils.py\n\tnew file:   kNN-TL/examples/nonautoregressive_translation/README.md\n\tnew file:   kNN-TL/examples/nonautoregressive_translation/scripts.md\n\tnew file:   kNN-TL/examples/normformer/README.md\n\tnew file:   kNN-TL/examples/normformer/train_lm.sh\n\tnew file:   kNN-TL/examples/operators/alignment_train_cpu.cpp\n\tnew file:   kNN-TL/examples/operators/alignment_train_cuda.cpp\n\tnew file:   kNN-TL/examples/operators/alignment_train_cuda.h\n\tnew file:   kNN-TL/examples/operators/alignment_train_kernel.cu\n\tnew file:   kNN-TL/examples/operators/utils.h\n\tnew file:   kNN-TL/examples/paraphraser/README.md\n\tnew file:   kNN-TL/examples/paraphraser/paraphrase.py\n\tnew file:   kNN-TL/examples/pay_less_attention_paper/README.md\n\tnew file:   kNN-TL/examples/pointer_generator/README.md\n\tnew file:   kNN-TL/examples/pointer_generator/README.xsum.md\n\tnew file:   kNN-TL/examples/pointer_generator/pointer_generator_src/__init__.py\n\tnew file:   kNN-TL/examples/pointer_generator/pointer_generator_src/transformer_pg.py\n\tnew file:   kNN-TL/examples/pointer_generator/postprocess.py\n\tnew file:   kNN-TL/examples/pointer_generator/preprocess.py\n\tnew file:   kNN-TL/examples/quant_noise/README.md\n\tnew file:   kNN-TL/examples/quant_noise/transformer_quantization_config.yaml\n\tnew file:   kNN-TL/examples/roberta/README.custom_classification.md\n\tnew file:   kNN-TL/examples/roberta/README.glue.md\n\tnew file:   kNN-TL/examples/roberta/README.md\n\tnew file:   kNN-TL/examples/roberta/README.pretraining.md\n\tnew file:   kNN-TL/examples/roberta/README.race.md\n\tnew file:   kNN-TL/examples/roberta/commonsense_qa/README.md\n\tnew file:   kNN-TL/examples/roberta/commonsense_qa/__init__.py\n\tnew file:   kNN-TL/examples/roberta/commonsense_qa/commonsense_qa_task.py\n\tnew file:   kNN-TL/examples/roberta/commonsense_qa/download_cqa_data.sh\n\tnew file:   kNN-TL/examples/roberta/config/finetuning/cola.yaml\n\tnew file:   kNN-TL/examples/roberta/config/finetuning/mnli.yaml\n\tnew file:   kNN-TL/examples/roberta/config/finetuning/mrpc.yaml\n\tnew file:   kNN-TL/examples/roberta/config/finetuning/qnli.yaml\n\tnew file:   kNN-TL/examples/roberta/config/finetuning/qqp.yaml\n\tnew file:   kNN-TL/examples/roberta/config/finetuning/rte.yaml\n\tnew file:   kNN-TL/examples/roberta/config/finetuning/sst_2.yaml\n\tnew file:   kNN-TL/examples/roberta/config/finetuning/sts_b.yaml\n\tnew file:   kNN-TL/examples/roberta/config/pretraining/base.yaml\n\tnew file:   kNN-TL/examples/roberta/multiprocessing_bpe_encoder.py\n\tnew file:   kNN-TL/examples/roberta/preprocess_GLUE_tasks.sh\n\tnew file:   kNN-TL/examples/roberta/preprocess_RACE.py\n\tnew file:   kNN-TL/examples/roberta/preprocess_RACE.sh\n\tnew file:   kNN-TL/examples/roberta/wsc/README.md\n\tnew file:   kNN-TL/examples/roberta/wsc/__init__.py\n\tnew file:   kNN-TL/examples/roberta/wsc/wsc_criterion.py\n\tnew file:   kNN-TL/examples/roberta/wsc/wsc_task.py\n\tnew file:   kNN-TL/examples/roberta/wsc/wsc_utils.py\n\tnew file:   kNN-TL/examples/rxf/README.md\n\tnew file:   kNN-TL/examples/rxf/__init__.py\n\tnew file:   kNN-TL/examples/rxf/rxf_src/__init__.py\n\tnew file:   kNN-TL/examples/rxf/rxf_src/label_smoothed_cross_entropy_r3f.py\n\tnew file:   kNN-TL/examples/rxf/rxf_src/sentence_prediction_r3f.py\n\tnew file:   kNN-TL/examples/scaling_nmt/README.md\n\tnew file:   kNN-TL/examples/shuffled_word_order/README.finetuning.md\n\tnew file:   kNN-TL/examples/shuffled_word_order/README.md\n\tnew file:   kNN-TL/examples/simultaneous_translation/README.md\n\tnew file:   kNN-TL/examples/simultaneous_translation/__init__.py\n\tnew file:   kNN-TL/examples/simultaneous_translation/docs/ende-mma.md\n\tnew file:   kNN-TL/examples/simultaneous_translation/docs/enja-waitk.md\n\tnew file:   kNN-TL/examples/simultaneous_translation/eval/agents/simul_t2t_enja.py\n\tnew file:   kNN-TL/examples/simultaneous_translation/models/__init__.py\n\tnew file:   kNN-TL/examples/simultaneous_translation/models/convtransformer_simul_trans.py\n\tnew file:   kNN-TL/examples/simultaneous_translation/models/transformer_monotonic_attention.py\n\tnew file:   kNN-TL/examples/simultaneous_translation/modules/__init__.py\n\tnew file:   kNN-TL/examples/simultaneous_translation/modules/fixed_pre_decision.py\n\tnew file:   kNN-TL/examples/simultaneous_translation/modules/monotonic_multihead_attention.py\n\tnew file:   kNN-TL/examples/simultaneous_translation/modules/monotonic_transformer_layer.py\n\tnew file:   kNN-TL/examples/simultaneous_translation/tests/test_alignment_train.py\n\tnew file:   kNN-TL/examples/simultaneous_translation/tests/test_text_models.py\n\tnew file:   kNN-TL/examples/simultaneous_translation/utils/__init__.py\n\tnew file:   kNN-TL/examples/simultaneous_translation/utils/functions.py\n\tnew file:   kNN-TL/examples/simultaneous_translation/utils/monotonic_attention.py\n\tnew file:   kNN-TL/examples/simultaneous_translation/utils/p_choose_strategy.py\n\tnew file:   kNN-TL/examples/speech_recognition/README.md\n\tnew file:   kNN-TL/examples/speech_recognition/__init__.py\n\tnew file:   kNN-TL/examples/speech_recognition/criterions/ASG_loss.py\n\tnew file:   kNN-TL/examples/speech_recognition/criterions/__init__.py\n\tnew file:   kNN-TL/examples/speech_recognition/criterions/cross_entropy_acc.py\n\tnew file:   kNN-TL/examples/speech_recognition/data/__init__.py\n\tnew file:   kNN-TL/examples/speech_recognition/data/asr_dataset.py\n\tnew file:   kNN-TL/examples/speech_recognition/data/collaters.py\n\tnew file:   kNN-TL/examples/speech_recognition/data/data_utils.py\n\tnew file:   kNN-TL/examples/speech_recognition/data/replabels.py\n\tnew file:   kNN-TL/examples/speech_recognition/datasets/asr_prep_json.py\n\tnew file:   kNN-TL/examples/speech_recognition/datasets/prepare-librispeech.sh\n\tnew file:   kNN-TL/examples/speech_recognition/infer.py\n\tnew file:   kNN-TL/examples/speech_recognition/kaldi/__init__.py\n\tnew file:   kNN-TL/examples/speech_recognition/kaldi/add-self-loop-simple.cc\n\tnew file:   kNN-TL/examples/speech_recognition/kaldi/config/kaldi_initializer.yaml\n\tnew file:   kNN-TL/examples/speech_recognition/kaldi/kaldi_decoder.py\n\tnew file:   kNN-TL/examples/speech_recognition/kaldi/kaldi_initializer.py\n\tnew file:   kNN-TL/examples/speech_recognition/models/__init__.py\n\tnew file:   kNN-TL/examples/speech_recognition/models/vggtransformer.py\n\tnew file:   kNN-TL/examples/speech_recognition/models/w2l_conv_glu_enc.py\n\tnew file:   kNN-TL/examples/speech_recognition/new/README.md\n\tnew file:   kNN-TL/examples/speech_recognition/new/__init__.py\n\tnew file:   kNN-TL/examples/speech_recognition/new/conf/hydra/sweeper/ax.yaml\n\tnew file:   kNN-TL/examples/speech_recognition/new/conf/infer.yaml\n\tnew file:   kNN-TL/examples/speech_recognition/new/decoders/__init__.py\n\tnew file:   kNN-TL/examples/speech_recognition/new/decoders/base_decoder.py\n\tnew file:   kNN-TL/examples/speech_recognition/new/decoders/decoder.py\n\tnew file:   kNN-TL/examples/speech_recognition/new/decoders/decoder_config.py\n\tnew file:   kNN-TL/examples/speech_recognition/new/decoders/flashlight_decoder.py\n\tnew file:   kNN-TL/examples/speech_recognition/new/decoders/viterbi_decoder.py\n\tnew file:   kNN-TL/examples/speech_recognition/new/infer.py\n\tnew file:   kNN-TL/examples/speech_recognition/tasks/__init__.py\n\tnew file:   kNN-TL/examples/speech_recognition/tasks/speech_recognition.py\n\tnew file:   kNN-TL/examples/speech_recognition/utils/wer_utils.py\n\tnew file:   kNN-TL/examples/speech_recognition/w2l_decoder.py\n\tnew file:   kNN-TL/examples/speech_synthesis/README.md\n\tnew file:   kNN-TL/examples/speech_synthesis/__init__.py\n\tnew file:   kNN-TL/examples/speech_synthesis/data_utils.py\n\tnew file:   kNN-TL/examples/speech_synthesis/docs/common_voice_example.md\n\tnew file:   kNN-TL/examples/speech_synthesis/docs/ljspeech_example.md\n\tnew file:   kNN-TL/examples/speech_synthesis/docs/vctk_example.md\n\tnew file:   kNN-TL/examples/speech_synthesis/evaluation/__init__.py\n\tnew file:   kNN-TL/examples/speech_synthesis/evaluation/eval_asr.py\n\tnew file:   kNN-TL/examples/speech_synthesis/evaluation/eval_f0.py\n\tnew file:   kNN-TL/examples/speech_synthesis/evaluation/eval_sp.py\n\tnew file:   kNN-TL/examples/speech_synthesis/evaluation/get_eval_manifest.py\n\tnew file:   kNN-TL/examples/speech_synthesis/generate_waveform.py\n\tnew file:   kNN-TL/examples/speech_synthesis/preprocessing/__init__.py\n\tnew file:  "
    },
    {
        "repo_url": "kNN-TL/examples/speech_synthesis/preprocessing/denoise_and_vad_audio.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_synthesis/preprocessing/denoiser/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_synthesis/preprocessing/denoiser/demucs.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_synthesis/preprocessing/denoiser/pretrained.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_synthesis/preprocessing/denoiser/resample.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_synthesis/preprocessing/denoiser/utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_synthesis/preprocessing/get_common_voice_audio_manifest.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_synthesis/preprocessing/get_feature_manifest.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_synthesis/preprocessing/get_ljspeech_audio_manifest.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_synthesis/preprocessing/get_speaker_embedding.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_synthesis/preprocessing/get_vctk_audio_manifest.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_synthesis/preprocessing/speaker_embedder/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_synthesis/preprocessing/vad/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_synthesis/utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_text_joint_to_text/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_text_joint_to_text/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_text_joint_to_text/configs/mustc_noise.list",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_text_joint_to_text/criterions/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_text_joint_to_text/criterions/multi_modality_compound.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_text_joint_to_text/criterions/multi_modality_cross_entropy.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_text_joint_to_text/criterions/text_guide_cross_entropy_acc.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_text_joint_to_text/data/pair_denoising_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_text_joint_to_text/docs/ende-mustc.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_text_joint_to_text/docs/iwslt2021.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_text_joint_to_text/docs/pre-training.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_text_joint_to_text/models/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_text_joint_to_text/models/joint_speech_text_pretrain_transformer.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_text_joint_to_text/models/s2t_dualinputtransformer.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_text_joint_to_text/models/s2t_dualinputwavtransformer.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_text_joint_to_text/models/s2t_dualinputxmtransformer.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_text_joint_to_text/scripts/convert_model.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_text_joint_to_text/scripts/g2p_encode.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_text_joint_to_text/tasks/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_text_joint_to_text/tasks/pair_denoising.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_text_joint_to_text/tasks/speech_text_denoise_pretrain.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_text_joint_to_text/tasks/speech_text_joint.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_speech/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_speech/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_speech/benchmarking/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_speech/benchmarking/configs/2StageS2ST.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_speech/benchmarking/configs/3StageS2ST.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_speech/benchmarking/configs/DirectS2U.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_speech/benchmarking/configs/S2T.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_speech/benchmarking/core.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_speech/benchmarking/data_utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_speech/benchmarking/get_metrics.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_speech/docs/direct_s2st_discrete_units.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_speech/docs/enhanced_direct_s2st_discrete_units.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_speech/docs/textless_s2st_real_data.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_speech/generate_waveform_from_code.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_speech/preprocessing/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_speech/preprocessing/data_utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_speech/preprocessing/prep_s2spect_data.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_speech/preprocessing/prep_s2ut_data.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_speech/preprocessing/prep_sn_data.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_speech/preprocessing/prep_sn_output_data.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_text/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_text/data_utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_text/docs/covost_example.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_text/docs/librispeech_example.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_text/docs/mtedx_example.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_text/docs/mustc_example.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_text/docs/simulst_mustc_example.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_text/prep_covost_data.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_text/prep_librispeech_data.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_text/prep_mtedx_data.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_text/prep_mustc_data.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_text/seg_mustc_data.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/speech_to_text/simultaneous_translation/agents/fairseq_simul_st_agent.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/stories/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/metrics/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/metrics/abx_metrics/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/metrics/abx_metrics/dump_abx_feats.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/metrics/asr_metrics/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/metrics/asr_metrics/continuation_eval.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/metrics/asr_metrics/misc/bleu_utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/metrics/asr_metrics/misc/cut_as.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/metrics/asr_metrics/misc/dict.ltr.txt",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/metrics/asr_metrics/ppx.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/metrics/asr_metrics/self_auto_bleu.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/speech2unit/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/speech2unit/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/speech2unit/clustering/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/speech2unit/clustering/cluster_kmeans.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/speech2unit/clustering/dump_feats.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/speech2unit/clustering/quantize_with_kmeans.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/speech2unit/clustering/utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/speech2unit/pretrained/cpc_feature_reader.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/speech2unit/pretrained/hubert_feature_reader.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/speech2unit/pretrained/logmel_feature_reader.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/speech2unit/pretrained/utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/speech2unit/pretrained/w2v2_feature_reader.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/tools/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/tools/resynthesize_speech.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/ulm/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/ulm/sample.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/convert_to_16k.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/glow.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/multiproc.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/synthesize_audio_from_units.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/tacotron2/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/tacotron2/audio_processing.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/tacotron2/cleaners.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/tacotron2/cmudict.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/tacotron2/layers.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/tacotron2/model.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/tacotron2/numbers.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/tacotron2/stft.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/tacotron2/symbols.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/tacotron2/text.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/tacotron2/utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/tacotron2/waveglow_denoiser.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/tts_data.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/gslm/unit2speech/utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/pgslm/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/pgslm/data_utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/pgslm/eval/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/pgslm/eval/cont_metrics.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/pgslm/generate_waveform.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/pgslm/inference_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/pgslm/naive_decoder.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/pgslm/prepare_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/pgslm/preprocess_f0.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/pgslm/quantize_f0.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/pgslm/sample/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/pgslm/sample/sample.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/pgslm/scripts/join_units_manifest.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/pgslm/scripts/prepare_data.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/pgslm/scripts/prepare_f0_quantization.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/pgslm/truncated_laplace.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/speech-resynth/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/textless_nlp/speech-resynth/img/fig.png",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/translation/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/translation/prepare-iwslt14.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/translation/prepare-iwslt17-multilingual.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/translation/prepare-wmt14en2de.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/translation/prepare-wmt14en2fr.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/translation_moe/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/translation_moe/score.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/translation_moe/translation_moe_src/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/translation_moe/translation_moe_src/logsumexp_moe.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/translation_moe/translation_moe_src/mean_pool_gating_network.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/translation_moe/translation_moe_src/translation_moe.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/truncated_bptt/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/truncated_bptt/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/truncated_bptt/transformer_xl_model.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/truncated_bptt/truncated_bptt_lm_task.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/unsupervised_quality_estimation/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/unsupervised_quality_estimation/aggregate_scores.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/unsupervised_quality_estimation/meteor.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/unsupervised_quality_estimation/repeat_lines.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/config/finetuning/base_100h.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/config/finetuning/base_10h.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/config/finetuning/base_10m.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/config/finetuning/base_1h.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/config/finetuning/base_960h.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/config/finetuning/vox_100h.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/config/finetuning/vox_10h.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/config/finetuning/vox_10m.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/config/finetuning/vox_1h.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/config/finetuning/vox_960h.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/config/pretraining/wav2vec2_base_librispeech.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/config/pretraining/wav2vec2_conformer_base_librispeech.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/config/pretraining/wav2vec2_conformer_large_librivox.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/config/pretraining/wav2vec2_large_librivox.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/config/pretraining/wav2vec2_large_librivox_tpu-pod.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/config/pretraining/wav2vec2_large_librivox_tpu.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/libri_labels.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/scripts/binarize_manifest.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/config/finetuning/w2v_finetune.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/config/gan/w2vu.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/config/generate/viterbi.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/config/timit_matched/test.uid",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/config/timit_matched/train.uid",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/config/timit_matched/train_text.uid",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/config/timit_matched/valid.uid",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/config/timit_unmatched/test.uid",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/config/timit_unmatched/train.uid",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/config/timit_unmatched/train_text.uid",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/config/timit_unmatched/valid.uid",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/data/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/data/extracted_features_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/data/random_input_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/cmd.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/decode_phone.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/decode_word_step1.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/decode_word_step2.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/local/copy_aligned_text.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/local/decode.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/local/prepare_data_from_w2v.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/local/prepare_lang.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/local/prepare_lang_word.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/local/prepare_lm.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/local/score.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/local/show_wer.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/local/train_subset_lgbeam.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/local/unsup_select.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/local/unsup_select_decode.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/local/unsup_select_decode_word.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/path.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/steps_gan/train_deltas.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/steps_gan/train_lda_mllt.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/steps_gan/train_sat.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/kaldi_self_train/st/train.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/models/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/models/wav2vec_u.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/scripts/apply_pca.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/scripts/copy_labels.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/scripts/filter_lexicon.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/scripts/filter_tsv.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/scripts/g2p_wrd_to_phn.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/scripts/ltr_to_wrd.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/scripts/mean_pool.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/scripts/merge_clusters.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/scripts/normalize_and_filter_text.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/scripts/normalize_text.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/scripts/pca.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/scripts/phonemize_with_sil.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/scripts/prepare_audio.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/scripts/prepare_text.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/scripts/prepare_timit.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/scripts/remove_silence.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/scripts/vads.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/scripts/wav2vec_apply_cluster_faiss.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/scripts/wav2vec_cluster_faiss.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/scripts/wav2vec_extract_features.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/scripts/wer.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/scripts/wrd_to_ltr.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/tasks/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/tasks/unpaired_audio_text.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/unsupervised/w2vu_generate.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/vq-wav2vec_featurize.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/wav2vec_featurize.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/wav2vec_manifest.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/xlsr/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wav2vec/xlsr/config/finetune.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wmt19/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wmt20/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wmt21/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wmt21/eval.sh",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wmt21/scripts/normalize-punctuation.perl",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/wmt21/scripts/replace-unicode-punctuation.perl",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/womens_bios/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/womens_bios/query_occupations_from_wikidata.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/xformers/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/xglm/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/xglm/model_card.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/xlmr/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/xmod/README.md",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/examples/xmod/preprocess_nli.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/benchmark/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/benchmark/benchmark_multihead_attention.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/benchmark/dummy_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/benchmark/dummy_lm.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/benchmark/dummy_masked_lm.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/benchmark/dummy_model.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/benchmark/dummy_mt.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/binarizer.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/checkpoint_utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/clib/cuda/ngram_repeat_block_cuda.cpp",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/clib/cuda/ngram_repeat_block_cuda_kernel.cu",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/clib/libbase/balanced_assignment.cpp",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/clib/libbleu/libbleu.cpp",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/clib/libbleu/module.cpp",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/clib/libnat/edit_dist.cpp",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/clib/libnat_cuda/binding.cpp",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/clib/libnat_cuda/edit_dist.cu",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/clib/libnat_cuda/edit_dist.h",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/config/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/config/config.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/config/model/transformer_lm/transformer_lm_baevski_gbw.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/config/model/transformer_lm/transformer_lm_baevski_wiki103.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/config/model/transformer_lm/transformer_lm_big.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/config/model/transformer_lm/transformer_lm_gbw.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/config/model/transformer_lm/transformer_lm_gpt.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/config/model/transformer_lm/transformer_lm_gpt2_big.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/config/model/transformer_lm/transformer_lm_gpt2_medium.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/config/model/transformer_lm/transformer_lm_gpt2_small.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/config/model/transformer_lm/transformer_lm_wiki103.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/config/model/wav2vec/vq_wav2vec_gumbel.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/config/model/wav2vec2/wav2vec2_base.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/config/model/wav2vec2/wav2vec2_large.yaml",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/criterions/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/criterions/adaptive_loss.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/criterions/composite_loss.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/criterions/cross_entropy.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/criterions/ctc.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/criterions/fairseq_criterion.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/criterions/fastspeech2_loss.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/criterions/hubert_criterion.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/criterions/label_smoothed_cross_entropy.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/criterions/label_smoothed_cross_entropy_latency_augmented.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/criterions/label_smoothed_cross_entropy_with_alignment.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/criterions/label_smoothed_cross_entropy_with_ctc.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/criterions/label_smoothed_cross_entropy_xkd.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/criterions/legacy_masked_lm.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/criterions/masked_lm.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/criterions/model_criterion.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/criterions/nat_loss.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/criterions/sentence_prediction.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/criterions/sentence_prediction_adapters.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/criterions/sentence_ranking.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/criterions/speech_to_speech_criterion.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/criterions/speech_ulm_criterion.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/criterions/tacotron2_loss.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/criterions/wav2vec_criterion.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/add_target_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/append_token_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/audio/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/audio/audio_utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/audio/data_cfg.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/audio/feature_transforms/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/audio/feature_transforms/delta_deltas.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/audio/feature_transforms/global_cmvn.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/audio/feature_transforms/specaugment.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/audio/feature_transforms/utterance_cmvn.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/audio/frm_text_to_speech_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/audio/hubert_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/audio/multi_modality_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/audio/raw_audio_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/audio/speech_to_speech_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/audio/speech_to_text_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/audio/speech_to_text_joint_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/audio/text_to_speech_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/backtranslation_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/base_wrapper_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/bucket_pad_length_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/codedataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/colorize_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/concat_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/concat_sentences_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/data_utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/data_utils_fast.pyx",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/denoising_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/dictionary.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/encoders/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/encoders/byte_bpe.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/encoders/byte_utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/encoders/bytes.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/encoders/characters.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/encoders/fastbpe.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/encoders/gpt2_bpe.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/encoders/gpt2_bpe_utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/encoders/hf_bert_bpe.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/encoders/hf_byte_bpe.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/encoders/moses_tokenizer.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/encoders/nltk_tokenizer.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/encoders/sentencepiece_bpe.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/encoders/space_tokenizer.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/encoders/subword_nmt_bpe.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/encoders/utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/fairseq_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/fasta_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/huffman/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/huffman/huffman_coder.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/huffman/huffman_mmap_indexed_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/id_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/indexed_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/iterators.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/language_pair_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/legacy/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/legacy/block_pair_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/legacy/masked_lm_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/legacy/masked_lm_dictionary.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/list_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/lm_context_window_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/lru_cache_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/mask_tokens_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/monolingual_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/multi_corpus_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/multi_corpus_sampled_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/multilingual/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/multilingual/multilingual_data_manager.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/multilingual/multilingual_utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/multilingual/sampled_multi_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/multilingual/sampled_multi_epoch_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/multilingual/sampling_method.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/nested_dictionary_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/noising.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/num_samples_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/numel_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/offset_tokens_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/pad_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/plasma_utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/prepend_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/prepend_token_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/raw_label_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/replace_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/resampling_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/roll_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/round_robin_zip_datasets.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/shorten_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/sort_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/strip_token_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/subsample_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/text_compressor.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/token_block_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/token_block_utils_fast.pyx",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/transform_eos_concat_langpair_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/transform_eos_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/data/transform_eos_lang_pair_dataset.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/dataclass/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/dataclass/configs.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/dataclass/constants.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/dataclass/initialize.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/dataclass/utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/distributed/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/distributed/distributed_timeout_wrapper.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/distributed/fully_sharded_data_parallel.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/distributed/legacy_distributed_data_parallel.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/distributed/module_proxy_wrapper.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/distributed/tpu_distributed_data_parallel.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/distributed/utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/file_chunker_utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/file_io.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/file_utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/hub_utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/incremental_decoding_utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/iterative_refinement_generator.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/logging/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/logging/meters.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/logging/metrics.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/logging/progress_bar.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/model_parallel/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/model_parallel/criterions/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/model_parallel/criterions/vocab_parallel_cross_entropy.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/model_parallel/megatron_trainer.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/model_parallel/models/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/model_parallel/models/pipeline_parallel_transformer/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/model_parallel/models/pipeline_parallel_transformer/layers.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/model_parallel/models/pipeline_parallel_transformer/model.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/model_parallel/models/roberta/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/model_parallel/models/roberta/model.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/model_parallel/models/transformer.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/model_parallel/models/transformer_lm.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/model_parallel/modules/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/model_parallel/modules/multihead_attention.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/model_parallel/modules/transformer_layer.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/bart/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/bart/hub_interface.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/bart/model.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/composite_encoder.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/distributed_fairseq_model.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/ema/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/ema/ema.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/fairseq_decoder.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/fairseq_encoder.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/fairseq_incremental_decoder.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/fairseq_model.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/fconv.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/fconv_lm.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/fconv_self_att.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/hubert/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/hubert/hubert.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/hubert/hubert_asr.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/huggingface/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/huggingface/hf_gpt2.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/lightconv.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/lightconv_lm.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/lstm.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/lstm_lm.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/masked_lm.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/model_utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/multilingual_transformer.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/nat/__init__.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/nat/cmlm_transformer.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/nat/fairseq_nat_model.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/nat/insertion_transformer.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/nat/iterative_nonautoregressive_transformer.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/nat/levenshtein_transformer.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/nat/levenshtein_utils.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/nat/nat_crf_transformer.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/nat/nonautoregressive_ensembles.py",
        "filepath": "",
        "commit_date": "",
        "message": ""
    },
    {
        "repo_url": "\tnew file:   kNN-TL/fairseq/models/na\u2026\"",
        "filepath": "",
        "commit_date": "",
        "message": ""
    }
]