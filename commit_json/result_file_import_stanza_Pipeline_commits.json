[
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/semgrex.py",
        "commit_date": "2022-08-03T08:43:40Z",
        "message": "Rough outline of a semgrex interface demo program"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2020-08-11T19:15:55Z",
        "message": "Merge pull request #422 from stanfordnlp/master\n\nBack-merge documentation changes from master to dev"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2020-06-22T23:32:20Z",
        "message": "Rename stanza.resource to stanza.resources; rename resources.py to common.py"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2020-06-22T22:49:38Z",
        "message": "Move all resources related scripts to stanza/resource"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2020-06-11T06:04:53Z",
        "message": "Fix demo script"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2020-03-06T07:45:58Z",
        "message": "renaming stanfordnlp to stanza!!"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-02-19T08:18:42Z",
        "message": "Use sys.exit in place of exit"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-31T02:24:01Z",
        "message": "Revert temporary fix for download logic"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-31T02:18:05Z",
        "message": "Fix download logic; Fix help message in demo script"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-31T01:53:22Z",
        "message": "Quick fix for demo script"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-26T05:33:04Z",
        "message": "Separate the classes for Tokens and syntactic Words"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-25T09:25:55Z",
        "message": "Undo changes to use_gpu argument"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-25T09:16:37Z",
        "message": "Modify the way devices are specified in pipeline"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-25T00:25:14Z",
        "message": "Swap French demo input with one that has MWT in the first sentence"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-24T22:22:28Z",
        "message": "Accommodate Vietnamese in the pipeline tokenizer processor; Add a Vietnamese example to demo script"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-23T08:51:06Z",
        "message": "Demo script goes multi-lingual; don't confirm overwrite by default (by do it in the demo script)"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-23T08:51:06Z",
        "message": "Change demo to match logic of new download function: existence of models is checked in download() now; Add easy toggle for other demo languages"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-22T23:48:13Z",
        "message": "Modernizing and reusing build_default_config; Path fix in demo pipeline construction"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-22T23:31:32Z",
        "message": "Modernizing the demo script; make the download interface more user-friendly"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-20T08:10:41Z",
        "message": "update and simplify basic demo"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-11T23:37:09Z",
        "message": "add demo"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/server/semgrex.py",
        "commit_date": "2023-10-17T01:22:41Z",
        "message": "Add an Enhanced graph option to the python semgrex interface\n\nAdds a method for adding a networkx graph as a DependencyGraph proto\nIncludes a test that the result on a simple graph is as expected\n\nWe then read in enhanced graphs, then send that graph to the CoreNLP semgrex rather than the basic dependencies\n\nA couple specific changes:\n\nWhen adding a token to a semgrex protobuf, add index for regular words, add index and emptyIndex for empty words\nAttachs the tokens from a sentence to the networkx graph when building the DependencyGraph proto from a Sentence\n\nCurrently hard to add a test for this, since it requires some extensive CoreNLP changes"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/server/semgrex.py",
        "commit_date": "2023-04-09T02:36:57Z",
        "message": "Add a flag to semgrex to read patterns from a file instead of command line"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/server/semgrex.py",
        "commit_date": "2023-04-07T01:14:49Z",
        "message": "Make echoing the input an option, not the default, in the Semgrex command line tool"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/server/semgrex.py",
        "commit_date": "2023-03-14T05:27:12Z",
        "message": "Instead of dumping out a completely inscrutable semgrex result, add comments to the doc to explain where the sentences matched"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/server/semgrex.py",
        "commit_date": "2023-03-14T04:38:31Z",
        "message": "Add flags to semgrex.py to load a conllu file and execute the semgrex on it, possibly changing the semgrex used.  Add a sample file for demoing the semgrex"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/server/semgrex.py",
        "commit_date": "2022-08-21T07:21:51Z",
        "message": "Add a bit of documentation to the process method of a semgrex context"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/server/semgrex.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Update an incorrect doc in the semgrex context manager"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/server/semgrex.py",
        "commit_date": "2021-04-18T23:31:20Z",
        "message": "Add a context which connects to the multi-request semgrex"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/server/semgrex.py",
        "commit_date": "2021-03-30T21:57:57Z",
        "message": "Separate out some common functions from the semgrex interface"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/server/semgrex.py",
        "commit_date": "2021-03-30T20:23:09Z",
        "message": "Update the sample to something which triggers obj in the current models"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/server/semgrex.py",
        "commit_date": "2020-12-01T08:03:46Z",
        "message": "Add doc to the semgrex module"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/server/semgrex.py",
        "commit_date": "2020-08-13T01:12:53Z",
        "message": "Use CORENLP_HOME or whatever when running the semgrex client"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/server/semgrex.py",
        "commit_date": "2020-08-13T01:12:53Z",
        "message": "Refactor the function that runs the actual java program"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/server/semgrex.py",
        "commit_date": "2020-08-04T07:14:45Z",
        "message": "Update the semgrex tool to send across lemma, ner, tag & cpos as well"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/server/semgrex.py",
        "commit_date": "2020-07-29T01:11:56Z",
        "message": "Python interface to the semgrex processor"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/mwt/utils.py",
        "commit_date": "2023-09-13T15:52:25Z",
        "message": "Add a tiny bit of doc to the resplit_mwt util"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/mwt/utils.py",
        "commit_date": "2022-10-02T17:13:03Z",
        "message": "Add a bit more doc"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/mwt/utils.py",
        "commit_date": "2022-09-30T20:33:25Z",
        "message": "Add a utility function to resplit raw tokens into MWT using a tokenize/mwt Pipeline.  As requested in #95\n\nIncludes an option to keep or ignore existing token boundaries"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/server/ud_enhancer.py",
        "commit_date": "2023-04-15T05:21:09Z",
        "message": "Use the default path rather than a specified CLASSPATH by default, including in a couple tests which work with the latest CoreNLP versions"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/server/ud_enhancer.py",
        "commit_date": "2021-07-13T07:26:08Z",
        "message": "This small hack is not needed now that a new version of CoreNLP has been released"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/server/ud_enhancer.py",
        "commit_date": "2021-04-18T23:31:20Z",
        "message": "Temporarily use $CLASSPATH, since this feature isn't in a released version yet"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/server/ud_enhancer.py",
        "commit_date": "2021-04-18T23:31:20Z",
        "message": "Tool that connects to the Java UD enhancer"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/server/tokensregex.py",
        "commit_date": "2021-04-07T06:21:30Z",
        "message": "Count all words, not just tokens"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/server/tokensregex.py",
        "commit_date": "2021-04-07T04:22:32Z",
        "message": "Refactor the add_sentence logic"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/server/tokensregex.py",
        "commit_date": "2021-03-31T05:48:52Z",
        "message": "Add a test for matching an NER trait and add a link to the tokensregex doc"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/server/tokensregex.py",
        "commit_date": "2021-03-30T22:35:35Z",
        "message": "Add an interface between stanza docs and corenlp tokensregex"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/pipeline/demo/demo_server.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Update demo code to include constituency parsing"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/pipeline/demo/demo_server.py",
        "commit_date": "2020-03-12T23:49:27Z",
        "message": "Update language list and misc updates"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/pipeline/demo/demo_server.py",
        "commit_date": "2020-03-06T07:46:58Z",
        "message": "move stanfordnlp dir to stanza!!"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2024-03-01T03:41:09Z",
        "message": "Add some debug logging when building a retag_pipeline - goal is to make sure it's calling the correct POS model"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2024-02-24T07:41:20Z",
        "message": "Add an option to turn off saving the optimizer when saving each constituency model"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2024-02-24T07:41:20Z",
        "message": "Add flags to control how often the save_each models get saved.  Intent is to make it so that we can skip 10 models at a time and use a series of these to measure the variance in a silver dataset's scores"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2024-02-24T07:41:20Z",
        "message": "Remove unnecessary repetition of existing save_each argument"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2024-02-22T01:31:45Z",
        "message": "Set the trainer logging level to DEBUG when starting a constituency training run.  Switch out a bunch of training log calls to use the training logger specifically to make it easier to log things like the optimizer creation"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2024-02-22T01:31:45Z",
        "message": "Integrate peft with the constituency parser.\n\nAdd a test that the two stage peft is correctly turning off finetuning for the second half of training"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2024-02-03T03:35:51Z",
        "message": "Add several resolutions of ambiguous transitions to the top down dynamic oracle.\nAlso, add a flag which turns on selective oracle transitions, and\nautomatically ignore transitions after UNKNOWN unless --oracle_level is set\n\nAdd an immediate close version of the close/open error.\n\nAdd an immediate close version of the close/shift error.\n\nAdd an alternate version of the close/shift and close/open resolutions - close at the end of the outer constituent\n\nProposed fix of Open/Open: turn the new Open into a Unary\n\nAdd a fix where open/open is closed at the end of the outer constituent\n\nAdd a shift/open ambiguous fix - turn them into Unary\n\nPut the Close for a shift/open at the end, rather than earlier\n\nIncludes stats for dev & test scores for various ambiguous transitions"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2024-01-22T23:47:58Z",
        "message": "Add oracle_level as a parameter to the save name"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-12-13T07:27:13Z",
        "message": "Add a flag to not check the transitions or constituents in the dev set"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-09-17T00:03:44Z",
        "message": "Add a bit to the name expansion for when to begin finetuning a transformer"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-09-02T17:52:46Z",
        "message": "Use the fancy new savename constructions in the run_constituency script"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-08-30T18:40:03Z",
        "message": "Comments on an experiment which didn't work out for mixing bert layers"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-08-25T05:45:01Z",
        "message": "Add a log of the tensor sizes"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-08-14T22:07:08Z",
        "message": "Refactor build_model_filename for run_constituency"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-08-14T22:00:12Z",
        "message": "Add the constituency parser arguments to the run_constituency script, similar to what is done for the lemmatizer & charlm"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-07-24T21:52:49Z",
        "message": "Make a bit of a generalized save name - will add more options as they come up"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-06-04T19:13:59Z",
        "message": "Model names were being created with blank spaces in them :/"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-05-24T07:30:26Z",
        "message": "Update con learning rates using a reduce-on-plateau scheme instead of the warmup\n\nThe warmup never did anything, so we leave it non-functional for now"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-05-03T21:42:11Z",
        "message": "Add a parameter which attempts to download a saved model for futher processing"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-04-04T16:44:09Z",
        "message": "Add a --tokenized_dir argument to the conparser and ensemble scripts which tokenizes all of the text files in a directory in one invocation.  The VLSP group wanted a text repo parsed"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-03-27T08:16:55Z",
        "message": "Add notes on an experiment that did nothing in the conparser"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-03-24T16:01:17Z",
        "message": "Add a long description of some experiments with low LR for bert and adamw as the second stage optimizer"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-03-22T22:04:48Z",
        "message": "backprop into the transformer...\n\nAdd parameters to decide whether or not finetune the transformer\nBy default, train Bert with a low LR and very low WD to avoid it overreacting to the training data\n\nCheck bert_finetune when loading models.  Set it to false if not in the model so that we can preserve old models\nAlso, check bert_finetune for whether or not to use the foundation_cache when building a new model\n\nAlso backprop phobert, xlnet, and bart\n\nSkip the first N steps for finetuning the transformer based on a parameter\nAdd an endpoint for the bert finetuning as well (another option would be separate initial phase and second phase values)\nAdd a separate argument for stage1_bert_finetuning\nIncludes functionality to finetune a limited number of layers\n\nInstead of using the FoundationCache, we need to Load a new copy of\nthe bert_model if the bert_model was saved in the constituency file\n(otherwise, the cached model would be clobered for any other users of\nthe model)\n\nAdd a single epoch test of the bert finetuning\n\nAdd a multiepoch test which checks that the weights are or aren't close to the final model in a manner expected by the bert training\n\nAdd a per-layer test of the bert backprop - check that if bert_finetune_layers is set, it only backprops the number of layers requested\n\nInclude xlnet versions of a couple of the bert tests.  These tests use a copied\ntiny-random-xlnet so that the numbers aren't expected to change between iterations\n\nAdd a tiny test of the Bart training in conparser\nphobert uses a different tokenizer, so we can't directly test it the same way we test the others\n\nA few notes are left behind on things TODO:\n\nTODO: Experiment with a low level of finetuning when using adamw\nIt would not work well with madgrad yet, since there is an issue where LR==0 still results in learning\nhttps://github.com/facebookresearch/madgrad/issues/16\n\nTODO: when saving a trainer which had its own transformer copy in it,\nwe need to resave the transformer - currently it gets put into\nunsaved_modules if the model wasn't training the transformer\nWe even leave behind a test which covers that situation, commented out\n\nTODO: when creating an LSTMModel in the Trainer for training a new model,\nwe need to use a non-cached version of it.  In particular, reusing the\nPOS tagger later on in training would be a problem\n(except we currently don't do that)"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-03-15T20:25:13Z",
        "message": "Fix a comment on the experimental differences"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-02-11T01:53:25Z",
        "message": "Add a flag for reversing the treebanks.  The model will reverse the sentences when making initial word queues, so that the bert & charlm models work as expected\n\nAdd a parameter to build_treebank to reverse the trees when building sequences.  Will facilitate parsing in reverse\n\nReverse sentences at the end of parsing in reverse mode so that we get the correct order back\n\nReverse sentences when verifying the transitions\n\nTest reversing a sentence when building the transition sequence"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-02-01T07:51:06Z",
        "message": "Clarify some comments on a conparser argument"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-12-07T07:06:06Z",
        "message": "Start passing around device instead of use_cuda\n\nAll models now work via the pipeline.  Need to double check that they work everywhere"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-12-05T20:41:18Z",
        "message": "Determine if cuda() is available and use that to set the random seed rather than making that an argument"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-12-01T19:20:16Z",
        "message": "Add a reference to more info on nonlinearity"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-12-01T19:20:13Z",
        "message": "Add a maxout linear such as in\n\nhttps://arxiv.org/pdf/1302.4389v4.pdf\n\n.get() the arg to keep old models alive\n\nIncludes some comments on accuracy with maxout"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-11-30T23:43:27Z",
        "message": "Add large_margin as an option for the loss.\n\nAdd a note on a large margin experiment (no improvement)"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-11-25T07:25:04Z",
        "message": "Add a focal loss option using an external library\n\nUnfortunately, it doesn't seem to improve results"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-11-22T22:07:41Z",
        "message": "Add a bunch of comments on the file layout for the model.  Add a missing citation"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-11-22T19:26:01Z",
        "message": "Default to multistage w/ madgrad if available, adamw if not\n\nDon't set training defaults if we're not training"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-11-17T15:40:32Z",
        "message": "Add an --oracle_level flag which optionally restricts which level to use for the oracle"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-11-15T00:12:17Z",
        "message": "Refactor adjusting the prediction string format"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-11-14T16:50:55Z",
        "message": "Add an option to not delete duplicate trees.  Relevant in the case of Vietnamese trees including multiple copies of quad, for example"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-11-14T05:57:59Z",
        "message": "Add nonlinearity experiments numbers\n\nUpdate numbers for the tests on nonlinearity. The numbers for the optimizers will follow later."
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-11-14T02:40:06Z",
        "message": "Add a format specifier which includes the index for the Vietnamese bakeoff"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-11-07T20:56:04Z",
        "message": "Add a variant of multihead attention where there's one key or one key per label\nSeems competitive with MAX but not a big improvement of any kind\n\nAdds an optional position encoding to the KEY / UNTIED_KEY constituency compositions\n(using reduce_position as a parameter for the size, 0 -> no position)\n\nreduce_position needs to be an unsaved module so that it doesn't barf if it gets reloaded later with a different size\n\nIncudes comments on a couple variants that didn't work - linear after the attention or a double position vector"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-11-01T16:03:20Z",
        "message": "Add an argument for partitioning / not partitioning lattn"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-11-01T01:14:11Z",
        "message": "Use some words from the silver dataset (currently |gold| words are added, even if that means some overlaps)"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-10-30T08:16:34Z",
        "message": "Add a separate argument for --silver_epoch_size, just in case people want that"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-10-30T06:20:37Z",
        "message": "Rough draft of using silver trees.\n\nMostly untested.  Includes an unfinished test of the silver data"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-10-29T02:20:00Z",
        "message": "refactor predict dir,file,format args so they can be used elsewhere if needed"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-10-29T00:33:40Z",
        "message": "Add functionality to turn a tokenized text file into a file of parse trees"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-10-28T01:49:18Z",
        "message": "Refactor the retagging args & pipeline creation into a separate modeule"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-10-25T15:45:13Z",
        "message": "Add a --predict_format option which will allow the user to specify how to write trees when outputting the model predictions"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-10-23T19:02:18Z",
        "message": "By default, turn off pattn & lattn (at least until we figure out how to extract value from them)"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-10-07T16:38:34Z",
        "message": "Add a flag to control the learning rate in the adadelta portion of --multistage"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-10-07T16:20:35Z",
        "message": "Add learning_ in front of momentum and weight_decay to make it clear those parameters are for the optimizer"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-09-26T17:11:39Z",
        "message": "Add a learned weighting between bert layers\n\nMake the number of hidden layers an option and start from zeros\nGeneralize the num_layers for Phobert and XLNet.  Keep old models alive\n\nIncludes an option to use the layers from the older versions of conparse"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-09-25T05:30:49Z",
        "message": "Add a TREE_LSTM node combination method.\n\nInstead of using a complicated bi-lstm or the much simpler max method,\ncombine children into a new item using a max-tree-lstm\n\nAttempts to integrate this method with the other combination methods\n\nNotes on variants\n\nAttempts to preserve old models & old model behavior - arguments are\nset to be like the original defaults\n\nAttempt to come up with an initial tree_cx for the TREE_LSTM method\nSupport both TREE_LSTM and the version with the CX, TREE_LSTM_CX"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-09-25T02:56:28Z",
        "message": "Add an option to choose an exact model path for retagging in the conparser"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-09-25T02:56:28Z",
        "message": "pos_package was not the correct flag for the retag_pipeline to choose a different package"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-09-22T02:33:21Z",
        "message": "Transformer stack using MHA instead of LSTM as an option for the transitions and constituents\n\nBegin integrating the TransformerTreeStack into the parser\nAdd arguments for converting transformer_stack and constituent_stack into ATTN\nUses a position encoding vector in the TransformerTreeStack\n\nKeep old models alive by setting StackHistory when loading a model\n\nAdd a length_limit option to the TransformerTreeStack (currently not used)\n\nUse the positions list to remember the length of a stack, since the stack might be shorter than expected when using length_limit\n\nMay want to save memory in some way for the stacked key and value fields.\n\nUnfortunately, in general this doesn't seem like an\nimprovement... maybe there's a bug in it, or maybe there's a set of\nhyperparameters which will work better, or maybe the LSTM is just\nbetter in general."
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-09-12T20:21:40Z",
        "message": "Don't double save_dir if the user gives save_dir as part of the model filename"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-09-12T06:26:56Z",
        "message": "Fix remove_optimizer mode"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-09-10T02:06:46Z",
        "message": "Always save checkpoints.  Always load from a checkpoint if one exists.\n\nBuild the constituency optimizer using knowledge of how far you are in the training process - multistage part 1 gets Adadelta, for example\nTest that a multistage training process builds the correct optimizers, including when reloading\n\nWhen continuing training from a checkpoint, use the existing epochs_trained\nRestart epochs count when doing a finetune"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-08-26T20:15:37Z",
        "message": "Grad clipping in the constituency parser.  Not a clear benefit yet, so just leaving this as an option."
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-08-23T03:57:52Z",
        "message": "Notes on a couple updates to defaults"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-08-22T23:15:24Z",
        "message": "Reorganize --optim flag to under the optimizer parameters"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-08-22T21:31:27Z",
        "message": "Make momentum an argument for the optimizer"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-08-17T21:08:37Z",
        "message": "Add a brief comment on beta2"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-08-17T20:47:27Z",
        "message": "Add a couple more nonlinearity options.  Simplify the creation of the nonlinearity layers"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-08-17T17:48:23Z",
        "message": "Fix some doc and some defaults"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-08-17T07:26:33Z",
        "message": "Add a projection to a smaller dimension to the lattn inputs"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-08-10T07:10:03Z",
        "message": "A couple notes on other options in the conparser training"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-07-31T22:27:29Z",
        "message": "Making combining the input the default for the lattn layer"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-06-24T06:19:05Z",
        "message": "Add an option to build the lattn out of the entire input, not just pattn\n\nFudging a flag when reading in the models makes existing models work with the new argument"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-06-24T06:17:44Z",
        "message": "AdaDelta warmup for the conparser.  Motivation: AdaDelta results in\nhigher scores overall, but learns 0s for the weights of the pattn and\nlattn layers.  AdamW learns weights for pattn, and the models are more\naccurate than models trained without pattn using AdamW, but the models\nare lower scores overall than the AdaDelta models.\n\nThis improves that by first running AdaDelta, then switching.\n\nNow, if --multistage is set, run AdaDelta for half the epochs with no\npattn or lattn.  Then start the specified optimizer for the rest of\nthe time with the full model.  If pattn and lattn are both present,\nthe model is 1/2 no attn, 1/4 pattn, 1/4 pattn and lattn\n\nIncluded is a test of both the two and three step versions\n\nAlso add a false version of the option - we may very well want to make\n--multistage the default"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-06-24T06:17:44Z",
        "message": "move some defaults from constituency_parser to utils (will be useful for using them elsewhere)"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-06-20T21:34:23Z",
        "message": "currently lattn positional dim is assumed to be 1/2 of pattn"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-06-20T18:57:07Z",
        "message": "Save all conparser models if a particular command line option is set"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-06-16T22:14:52Z",
        "message": "Adding an LR warmup to the optimizer\n\nsave & load scheduler as part of the trainer files"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-06-16T07:14:14Z",
        "message": "Finetune a model that was learned with no pattn to now use pattn\n\nFinally, a method that successfully trains pattn layers (seriously, the scores go up compared to adadelta or adamw by themselves)"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-06-14T08:17:21Z",
        "message": "Fix broken conparse finetune :/"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-06-08T08:03:33Z",
        "message": "Automagically fix xpos being None for the POS tagger used for a conparser retagging"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-06-05T00:18:21Z",
        "message": "Very simple wandb integration for NER, tokenizer, mwt, lemma, depparse, pos, charlm, classifier, conparse"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-05-26T20:03:05Z",
        "message": "Add arguments for epsilon and beta2 to initializing an AdamW optimizer"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-05-08T21:28:59Z",
        "message": "Another note on a failed experiment"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-05-08T02:46:23Z",
        "message": "Add some notes on an MLP experiment which didn't work"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-05-06T20:58:48Z",
        "message": "ATTN method to build larger constituents out of smaller constituents"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-05-02T13:51:13Z",
        "message": "This comment is wrong - it was fixed a long time ago!"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "A couple minor changes that came up when removing optimizers from conparser models"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Add a note on a failed experiment"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Remove the specialized unary transform\n\nAdd a note on the use of TOP_DOWN_UNARY"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "One last comment on a possible bilstm modification"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Remove an unused option which hurts scores.  (Not having this option makes other subsequent changes easier.)  Leave a bunch of comments explaining how it hurt the scores"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Init the embeddngs at a scale closer to the scale the Adadelta learns\n\nIn most cases the model mostly throws away the initial values and learns new ones, especially for the delta_embedding\nHowever, there is a slight but noticeable improvement in scores\nFor example, ja_alt at 200 iterations goes from 0.8980 to 0.8985"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "This WD seems to work better?  needs more testing"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Some notes on an adamw hill climb"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Add a regex to output the norms and grads of a subset of weights if requested"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Update notes on failed experiments with a couple results"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Log norms of matrices in the model when given a flag"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Always use the output dimension of the pattn layer for the input of the lattn layer, since nothing else currently works anyway"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Add a comment and a citation on what label_attention does"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "add an initial label attention to the lstm_model.py\n\nincorporates the labeled representation into the input\n\nuses command line options for label attention\n\neverything is in a single label_attention_module that can be reused, as long as the embeddings and the appropriate maskings are available"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "When combining children nodes, take the max value instead of applying a fancy LSTM\n\nThis seems to perform better in some test cases\n\nKeeps an old version of the constituent building around as an options\nAdds a test of different constituency composition functions"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "changes to the partitioned transformer so that it's more compact\n\npartitioned attention is separated as a module so that it can be easily reused\n\nalso, add an option for bias (defaults to false)"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/server/dependency_converter.py",
        "commit_date": "2023-01-11T16:01:20Z",
        "message": "Add an interface for the CoreNLP conversion from English constituencies to dependencies.  Only works for English.  Not currently unit tested (obviously tested during development) because it requires a new CoreNLP release first\n\nReturn the doc after processing - makes it more pipelineable"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/ner/convert_amt.py",
        "commit_date": "2022-08-19T21:32:12Z",
        "message": "Ignore punc as part of the labels"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/ner/convert_amt.py",
        "commit_date": "2022-08-19T21:30:14Z",
        "message": "Add flags to the convert_amt script to ignore or remap certain labels"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/ner/convert_amt.py",
        "commit_date": "2022-08-19T20:55:15Z",
        "message": "Add a little doc"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/ner/convert_amt.py",
        "commit_date": "2022-08-18T04:34:23Z",
        "message": "Add a tool to convert from an AMT annotation file to a bio/json NER file for Stanza"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/visualization/semgrex_app.py",
        "commit_date": "2023-03-10T22:53:19Z",
        "message": "Semgrex visualization features:\n\n- ipynb notebook\n- streamlit app!\n- utility methods shared between the two\n\nother notes:\n\n- Highlights colors use colorblind-friendly scheme\n- Added support for visualizing xpos when desired\n- Included addition to xpos toggle, clarifying use of upos in the case of unavailable xpos tags\n- Added ability to view windows of semgrex search hits instead of rendering all at once\n\nsquash by AngledLuffa, in case it is relevant:\n\n + 1100c93e...f0b2d8d1 semgrex_search_vis_2 -> semgrex_search_vis_2 (forced update)\n + f0b2d8d1...c77db680 semgrex_search_vis_2 -> semgrex_search_vis_2 (forced update)"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/ner/misc_to_date.py",
        "commit_date": "2023-06-14T07:41:19Z",
        "message": "For archival purposes, add a script which converted Misc to Date (mostly, some hand correcting needed)"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/ner/convert_bsnlp.py",
        "commit_date": "2021-05-31T18:52:41Z",
        "message": "Script to process bsnlp 2019 ner datasets.  Includes the ability to split the dataset into two pieces randomly\n\nOrders queries from longest to smallest when searching for the regex expressions in the Bulgarian data files.  This way, smaller NER don't clobber the bigger ones, plus missed matches are less common if we look for previously matched NER tags before rejecting a partial match\n\nLowercase entities when searching.  Finds a few hits without special casing\n\nAlso includes a map of special cases which seem to be typos in the dataset\n\nStubs are left for the other bsnlp languages.  Currently only BG is handled."
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/tokenization/tokenize_files.py",
        "commit_date": "2023-04-30T08:42:52Z",
        "message": "Split the input text into chunks to make it easier to process in memory.  This also makes it faster, actually, perhaps because the memory usage is less intensive"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/tokenization/tokenize_files.py",
        "commit_date": "2023-02-13T00:00:10Z",
        "message": "Move get_tqdm() to a separate utils file so that all the places which want tqdm() don't need to import all of torch.  Part of separating things to make it easier to install the CoreNLP by itself, such as in #1192"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/tokenization/tokenize_files.py",
        "commit_date": "2022-12-07T07:06:06Z",
        "message": "Pass around device instead of use_gpu when running a Pipeline or MultilingualPipeline.\n\nThere is one downside to this approach, which is that existing custom\nprocessors which were a subclass of an existing processor and used the\ninitializer with use_gpu will need to be updated.  Hopefully this is a\nvery small problem - most processor variants would be independent of\nthe previous use_gpu system\n\nTheoretically this would let us make Pipelines on an \"mps\" if we\nwanted, but the torch 1.13.0 MPS backend has a bug which specifically\nmakes bidirectional LSTMs with batch_first unusable, so .... yeah"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/tokenization/tokenize_files.py",
        "commit_date": "2022-10-04T01:13:34Z",
        "message": "Add a script to tokenize data files one sentence per line\n\nUses tokenizer processors directly instead of Trainers or Pipeline so that we can easily take advantage of the code to process bulk items.\n\nDigs into zip files when converting text data\n\nIncludes a small test"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/ner/convert_my_ucsy.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Myanmar NER processing of a dataset provided by UCSY"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/ner/convert_hy_armtdp.py",
        "commit_date": "2023-05-12T18:59:51Z",
        "message": "Fix error with default when building HY NER"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/ner/convert_hy_armtdp.py",
        "commit_date": "2023-04-18T19:19:00Z",
        "message": "Add a flag to not download the Armenian pipeline when producing an Armenian NER dataset.  Addresses https://github.com/stanfordnlp/stanza/commit/1a5cfd0b7b41521cf49dcb540023c9959c4b704b#r109444184"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/ner/convert_hy_armtdp.py",
        "commit_date": "2023-03-16T02:51:27Z",
        "message": "convert_hy_armtdp was creating an Armenian pipeline on import, which should be turned off"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/ner/convert_hy_armtdp.py",
        "commit_date": "2023-03-13T23:11:06Z",
        "message": "Added code for hy_armtdp - Armenian NER dataset"
    },
    {
        "repo_url": "github.com/AIGC-Audio/AudioGPT",
        "filepath": "NeuralSeq/modules/syntaspeech/syntactic_graph_buider.py",
        "commit_date": "2023-03-31T08:57:17Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/constituency/selftrain.py",
        "commit_date": "2023-06-18T07:37:06Z",
        "message": "len(docs) == 0 can happen if all the docs in a wikipedia file are those little links to some other doc"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/constituency/selftrain.py",
        "commit_date": "2023-02-13T00:00:10Z",
        "message": "Move get_tqdm() to a separate utils file so that all the places which want tqdm() don't need to import all of torch.  Part of separating things to make it easier to install the CoreNLP by itself, such as in #1192"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/constituency/selftrain.py",
        "commit_date": "2022-12-02T07:26:53Z",
        "message": "Move TextTooLongError to the bert_embedding module where it is used"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/constituency/selftrain.py",
        "commit_date": "2022-11-05T07:14:23Z",
        "message": "Discard Devanagari text from the VI wikipedia"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/constituency/selftrain.py",
        "commit_date": "2022-11-05T01:33:44Z",
        "message": "Also chuck some sentences with long words"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/constituency/selftrain.py",
        "commit_date": "2022-11-05T00:58:23Z",
        "message": "throw out long JA sentences as well when tokenizing Wikipedia"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/constituency/selftrain.py",
        "commit_date": "2022-11-04T22:08:14Z",
        "message": "Specifically exclude one sentence from VI Wikipedia which makes Bert sad"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/constituency/selftrain.py",
        "commit_date": "2022-11-04T19:18:54Z",
        "message": "Refactor the tokenization method from tokenize_wiki.py  Reuse it to add an option to selftrain_vi_quad which only tokenizes the data without parsing it"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/constituency/selftrain.py",
        "commit_date": "2022-10-20T17:57:04Z",
        "message": "A couple upgrades to the IT silver script and the silver scripts in general - allow printing trees in PTB format, allow skipping a large dataset for testing purposes, use FoundationCache to save on GPU memory"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/constituency/selftrain.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Initial version of a script to process some IT datasets"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/constituency/selftrain.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Remove empty documents and empty chunks when building selftrain datasets"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/constituency/selftrain.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Prototype process single file\n\nAttempt to filter long sentences directly in selftrain_single_file\n\nChunks the use of the sentence splitter\n\nSplit the text into chunks for tqdm and writing interim results"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/constituency/selftrain.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Refactor the wiki selftrain script so most of the pieces can be reused for vi quad\n\nAdjust maxlen to be smaller?  Lots of files are causing cuda errors\n\nTurn off shuffle of docs and files using a flag\n\nAlso, throw an exception for text too long in the parser, skipping those sentences when building a silver dataset\n\nVI quad dataset filter non-SQ sentences.\n\nAdds a simple test for parsing the vi_quad data"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/visualization/ner_visualization.py",
        "commit_date": "2022-07-16T18:10:17Z",
        "message": "Document visualization with Spacy.  Processes completed docs or raw strings\n\nIncludes right-to-left support (in the NER viz in particular, tags are flipped, for example)\n\nAdded more documentation for usage, including necessary spaCy installations\n\nIncludes Jupyter examples for visualization; spacy.render() functions well here\nAdding new Jupyter examples with support for new functions to visualize several strings with the same language pipeline"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/coref/convert_ontonotes.py",
        "commit_date": "2023-12-01T05:13:08Z",
        "message": "Add Karel's method to reattach heads using the conjunction - here we use CCONJ instead of CC (upos)"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/coref/convert_ontonotes.py",
        "commit_date": "2023-12-01T05:12:44Z",
        "message": "Initial version of script to convert OntoNotes from HF using Stanza's depparse"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/visualization/semgrex_visualizer.py",
        "commit_date": "2023-03-10T22:53:19Z",
        "message": "Semgrex visualization features:\n\n- ipynb notebook\n- streamlit app!\n- utility methods shared between the two\n\nother notes:\n\n- Highlights colors use colorblind-friendly scheme\n- Added support for visualizing xpos when desired\n- Included addition to xpos toggle, clarifying use of upos in the case of unavailable xpos tags\n- Added ability to view windows of semgrex search hits instead of rendering all at once\n\nsquash by AngledLuffa, in case it is relevant:\n\n + 1100c93e...f0b2d8d1 semgrex_search_vis_2 -> semgrex_search_vis_2 (forced update)\n + f0b2d8d1...c77db680 semgrex_search_vis_2 -> semgrex_search_vis_2 (forced update)"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_utils.py",
        "commit_date": "2023-03-23T03:29:53Z",
        "message": "Add a corona text classification dataset"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_utils.py",
        "commit_date": "2023-02-06T20:44:08Z",
        "message": "Add constituency items to SentimentDatums (convert from namedtuple to a simple class)\n\nUse the new _asdict method when writing a dataset\nRead trees back from the classifier data files.\nAdd a test of the tree reading by adding trees to one of the test datasets.\n\nMove the fake datasets to test_data.py for better organization - now they are all with the reading test code"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_utils.py",
        "commit_date": "2022-10-20T00:42:07Z",
        "message": "Add some more description in the event of an IndexError"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_utils.py",
        "commit_date": "2022-10-19T22:09:05Z",
        "message": "Extend read_snippets for sentiment datasets to read multiple columns and combine them into a single sentiment value"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_utils.py",
        "commit_date": "2022-10-04T17:49:54Z",
        "message": "Move SentimentDatum to classifiers/data.py and make the reading code read in SentimentDatum"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_utils.py",
        "commit_date": "2022-10-04T17:49:54Z",
        "message": "Add a couple tests of the building block methods from the Sentiment conversion tools"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_utils.py",
        "commit_date": "2022-07-30T18:02:33Z",
        "message": "Rename Fragment -> SentimentDatum to make a more understandable name for potential external users"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_utils.py",
        "commit_date": "2022-07-30T06:34:51Z",
        "message": "Further refactor - put the utility method in the utility methods file"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_utils.py",
        "commit_date": "2022-06-05T18:29:25Z",
        "message": "Add a processing for the MR l3cube sentiment"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_utils.py",
        "commit_date": "2022-06-05T18:29:25Z",
        "message": "Generalize the sentiment csv reading code and move it to process_utils"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_utils.py",
        "commit_date": "2022-06-05T18:29:25Z",
        "message": "Instead of blank items, remove http altogether in the sentiment preprocessing"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_utils.py",
        "commit_date": "2022-04-30T00:12:32Z",
        "message": "Read & write sentiment datasets as json"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_utils.py",
        "commit_date": "2022-04-29T06:09:12Z",
        "message": "Pass around lists of tokens instead of single strings.  Will make it easier to switch the input/output to a json format"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_utils.py",
        "commit_date": "2022-04-27T02:24:43Z",
        "message": "Refactor out the PTB retokenization"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_utils.py",
        "commit_date": "2022-04-26T06:28:38Z",
        "message": "move the scripts/sentiment python scripts to stanza/utils/datasets/sentiment"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_scare.py",
        "commit_date": "2022-10-04T17:49:54Z",
        "message": "Move SentimentDatum to classifiers/data.py and make the reading code read in SentimentDatum"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_scare.py",
        "commit_date": "2022-07-30T18:02:33Z",
        "message": "Rename Fragment -> SentimentDatum to make a more understandable name for potential external users"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_scare.py",
        "commit_date": "2022-05-02T13:51:13Z",
        "message": "Switch all sentiment data scripts to use .json as the output name, not .txt"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_scare.py",
        "commit_date": "2022-04-29T06:09:12Z",
        "message": "Pass around lists of tokens instead of single strings.  Will make it easier to switch the input/output to a json format"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_scare.py",
        "commit_date": "2022-04-29T05:46:35Z",
        "message": "Convert the .sh prep_sentiment script to .py"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_scare.py",
        "commit_date": "2022-04-26T06:28:38Z",
        "message": "move the scripts/sentiment python scripts to stanza/utils/datasets/sentiment"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_corona.py",
        "commit_date": "2023-03-23T03:29:53Z",
        "message": "Add a corona text classification dataset"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/constituency/tokenize_wiki.py",
        "commit_date": "2024-02-24T07:41:20Z",
        "message": "Refactor a couple parsing functions in trainer.py  Use this to build a new silver dataset producing script which uses two ensembles at once to directly find the matching trees along with counting the number of parsers which agree on the best tree\n\nAdd a script which extracts the trees we want of a certain match level\n\nSample command line for the wiki tokenization script"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/constituency/tokenize_wiki.py",
        "commit_date": "2024-02-11T08:41:42Z",
        "message": "Add arguments for choosing the tokenizer model and download method to the wikipedia tokenization script"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/constituency/tokenize_wiki.py",
        "commit_date": "2023-02-13T00:00:10Z",
        "message": "Move get_tqdm() to a separate utils file so that all the places which want tqdm() don't need to import all of torch.  Part of separating things to make it easier to install the CoreNLP by itself, such as in #1192"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/constituency/tokenize_wiki.py",
        "commit_date": "2022-11-06T04:32:35Z",
        "message": "Add a flag to remove all sentences which don't fit in a bert tokenizer when processing Wikipedia"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/constituency/tokenize_wiki.py",
        "commit_date": "2022-11-04T19:18:54Z",
        "message": "Refactor the tokenization method from tokenize_wiki.py  Reuse it to add an option to selftrain_vi_quad which only tokenizes the data without parsing it"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/constituency/tokenize_wiki.py",
        "commit_date": "2022-11-03T07:40:11Z",
        "message": "Add min_len and max_len args to tokenize_wiki.py.  Skip one line wiki docs, since those are likely to be useless"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/constituency/tokenize_wiki.py",
        "commit_date": "2022-10-28T23:53:21Z",
        "message": "ignore em dashes in Wikipedia, as that seems to be lists"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/constituency/tokenize_wiki.py",
        "commit_date": "2022-10-28T21:32:52Z",
        "message": "A script for tokenizing a Wikipedia file and writing it out"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/add_constituency.py",
        "commit_date": "2024-02-06T08:15:23Z",
        "message": "Temporary (?) patch for the batch_size, since some models were trained with the older dataloader code and a batch size of 5000"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/add_constituency.py",
        "commit_date": "2023-02-06T20:44:08Z",
        "message": "A script to add constituencies to a classifier dataset.\n\nResplits MWT (based on a command line flag) and then parses trees\nGuess the language based on either the dataset name or the filename (could be wrong, which will likely crash)\n\nAllows for different retag package & a specific constituency model save file\n\nWrite the output files elsewhere based on an option"
    },
    {
        "repo_url": "github.com/modelscope/modelscope",
        "filepath": "modelscope/preprocessors/nlp/space_T_en/fields/common_utils.py",
        "commit_date": "2022-10-25T04:26:25Z",
        "message": "[to #42322933] NLP 1030 Refactor \n\nFeatures:\n1. Refactor the directory structure of nlp models. All model files are placed into either the model folder or the task_model folder\n2. Refactor all the comments to google style\n3. Add detail comments to important tasks and nlp models, to list the description of the model, and its preprocessor&trainer\n4. Model Exporting now supports a direct all to TorchModelExporter(no need to derive from it)\n5. Refactor model save_pretrained method to support direct running(independent from trainer)\n6. Remove the judgement of Model in the pipeline base class, to support outer register models running in our pipelines\n7. Nlp trainer now has a NLPTrainingArguments class , user can pass arguments into the dataclass, and use it as a normal cfg_modify_fn, to simplify the operation of modify cfg.\n8. Merge the BACKBONES and the MODELS, so user can get a backbone with the Model.from_pretrained call\n9. Model.from_pretrained now support a task argument, so user can use a backbone and load it with a specific task class.\n10. Support Preprocessor.from_pretrained method\n11. Add standard return classes to important nlp tasks, so some of the pipelines and the models are independent now, the return values of the models will always be tensors, and the pipelines will take care of the conversion to numpy and the following stuffs.\n12. Split the file of the nlp preprocessors, to make the dir structure more clear.\n\nBugs Fixing:\n1. Fix a bug that lr_scheduler can be called earlier than the optimizer's step\n2. Fix a bug that the direct call of Pipelines (not from pipeline(xxx)) throws error\n3. Fix a bug that the trainer will not call the correct TaskDataset class\n4. Fix a bug that the internal loading of dataset will throws error in the trainer class\n        Link: https://code.alibaba-inc.com/Ali-MaaS/MaaS-lib/codereview/10490585"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/constituency/convert_it_turin.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Also, eliminate some single word trees from it_turin"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/constituency/convert_it_turin.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Start to integrate it_turin as a conversion script\n\nThis includes fixing a broken tag"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/constituency/convert_it_turin.py",
        "commit_date": "2021-09-24T23:40:05Z",
        "message": "Processing script for the IT Turin treebank\n\nRemaps and/or replaces some weird constituency labels\n\nRefactor some, get a bunch of the filtering applied to the test set.  Turns out there's at least one miswritten test tree as well\n\nPrune NONE ... turns out a lot of those are showing up.  Also double check that no train trees are in the test set, although that was not a problem\n\nSplits train into train & dev\n\nRemaps words as well so that [] is back the way it should be\n\nSkips a broken tree based on its preterminal\n\nResplits a bunch of the tokens when processing it_turin"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_es_tass2020.py",
        "commit_date": "2022-10-04T17:49:54Z",
        "message": "Move SentimentDatum to classifiers/data.py and make the reading code read in SentimentDatum"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_es_tass2020.py",
        "commit_date": "2022-08-20T17:21:06Z",
        "message": "Wrapper for converting Spanish TASS2020 for sentiment\nConversion script for the dataset - reads data directly from the .zip files"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_ren_chinese.py",
        "commit_date": "2022-10-04T17:49:54Z",
        "message": "Move SentimentDatum to classifiers/data.py and make the reading code read in SentimentDatum"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_ren_chinese.py",
        "commit_date": "2022-07-30T18:02:33Z",
        "message": "Rename Fragment -> SentimentDatum to make a more understandable name for potential external users"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_ren_chinese.py",
        "commit_date": "2022-05-02T13:51:13Z",
        "message": "Switch all sentiment data scripts to use .json as the output name, not .txt"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_ren_chinese.py",
        "commit_date": "2022-04-29T06:09:12Z",
        "message": "Pass around lists of tokens instead of single strings.  Will make it easier to switch the input/output to a json format"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_ren_chinese.py",
        "commit_date": "2022-04-29T05:46:35Z",
        "message": "Convert the .sh prep_sentiment script to .py"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_ren_chinese.py",
        "commit_date": "2022-04-26T06:28:38Z",
        "message": "move the scripts/sentiment python scripts to stanza/utils/datasets/sentiment"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/visualization/dependency_visualization.py",
        "commit_date": "2022-07-16T18:10:17Z",
        "message": "Document visualization with Spacy.  Processes completed docs or raw strings\n\nIncludes right-to-left support (in the NER viz in particular, tags are flipped, for example)\n\nAdded more documentation for usage, including necessary spaCy installations\n\nIncludes Jupyter examples for visualization; spacy.render() functions well here\nAdding new Jupyter examples with support for new functions to visualize several strings with the same language pipeline"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/constituency/selftrain_vi_quad.py",
        "commit_date": "2022-11-04T19:18:54Z",
        "message": "Refactor the tokenization method from tokenize_wiki.py  Reuse it to add an option to selftrain_vi_quad which only tokenizes the data without parsing it"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/constituency/selftrain_vi_quad.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Refactor the wiki selftrain script so most of the pieces can be reused for vi quad\n\nAdjust maxlen to be smaller?  Lots of files are causing cuda errors\n\nTurn off shuffle of docs and files using a flag\n\nAlso, throw an exception for text too long in the parser, skipping those sentences when building a silver dataset\n\nVI quad dataset filter non-SQ sentences.\n\nAdds a simple test for parsing the vi_quad data"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_usage_german.py",
        "commit_date": "2022-10-04T17:49:54Z",
        "message": "Move SentimentDatum to classifiers/data.py and make the reading code read in SentimentDatum"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_usage_german.py",
        "commit_date": "2022-07-30T18:02:33Z",
        "message": "Rename Fragment -> SentimentDatum to make a more understandable name for potential external users"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_usage_german.py",
        "commit_date": "2022-05-02T13:51:13Z",
        "message": "Switch all sentiment data scripts to use .json as the output name, not .txt"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_usage_german.py",
        "commit_date": "2022-04-29T06:09:12Z",
        "message": "Pass around lists of tokens instead of single strings.  Will make it easier to switch the input/output to a json format"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_usage_german.py",
        "commit_date": "2022-04-26T06:28:38Z",
        "message": "move the scripts/sentiment python scripts to stanza/utils/datasets/sentiment"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_it_sentipolc16.py",
        "commit_date": "2022-10-20T04:10:29Z",
        "message": "Also add an option to save the data to a different filename so that the script can conveniently be used to do pos & neg as separate test cases"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_it_sentipolc16.py",
        "commit_date": "2022-10-20T03:06:41Z",
        "message": "Add an argument to the IT sentiment preparation to split the mixed class in various ways"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_it_sentipolc16.py",
        "commit_date": "2022-10-20T00:46:36Z",
        "message": "Add an Italian dataset"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_vsfc_vietnamese.py",
        "commit_date": "2022-10-04T17:49:54Z",
        "message": "Move SentimentDatum to classifiers/data.py and make the reading code read in SentimentDatum"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_vsfc_vietnamese.py",
        "commit_date": "2022-07-30T18:02:33Z",
        "message": "Rename Fragment -> SentimentDatum to make a more understandable name for potential external users"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_vsfc_vietnamese.py",
        "commit_date": "2022-04-30T01:44:39Z",
        "message": "Switch the VI model to use words tokenized from the stanza tokenizer rather than white space split.  For some reason this causes my GPU to melt"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/process_vsfc_vietnamese.py",
        "commit_date": "2022-04-26T06:28:38Z",
        "message": "move the scripts/sentiment python scripts to stanza/utils/datasets/sentiment"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency/score_converted_dependencies.py",
        "commit_date": "2023-01-12T16:19:25Z",
        "message": "Initial version of a script to score the constituency parser as a dependency parser"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/convert_italian_poetry_classification.py",
        "commit_date": "2024-02-06T06:32:06Z",
        "message": "Update for new version of the poetry data"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/sentiment/convert_italian_poetry_classification.py",
        "commit_date": "2023-02-06T20:44:08Z",
        "message": "Script to turn Prof. Delmonte's poetry text into a sentiment dataset"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/mwt/test_utils.py",
        "commit_date": "2023-08-18T16:10:52Z",
        "message": "the updated GUM MWT model behaves a bit better than the previous one"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/mwt/test_utils.py",
        "commit_date": "2022-09-30T20:33:25Z",
        "message": "Add a utility function to resplit raw tokens into MWT using a tokenize/mwt Pipeline.  As requested in #95\n\nIncludes an option to keep or ignore existing token boundaries"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/common/test_doc.py",
        "commit_date": "2023-11-13T04:31:07Z",
        "message": "This is embarrassing... perhaps adding in the CoNLL data will fix it"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/common/test_doc.py",
        "commit_date": "2023-10-23T08:25:04Z",
        "message": "Add a doc_id field to the Sentence object, by request"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/common/test_doc.py",
        "commit_date": "2023-10-23T08:07:42Z",
        "message": "Update so that specifically setting sent_id as a comment sets the Sentence object's sent_id as well"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/common/test_doc.py",
        "commit_date": "2023-02-14T22:21:34Z",
        "message": "Add a method to reindex sentences.  Use it to keep the sent_id increasing even when using a 'stream' of documents"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/common/test_doc.py",
        "commit_date": "2023-01-12T04:59:07Z",
        "message": "Use comments to store the sentiment values as well"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/common/test_doc.py",
        "commit_date": "2023-01-12T02:51:30Z",
        "message": "When a comment is added to a sentence with a constituency tree, parse that comment into the tree object\n\nThis involves refactoring the StanzaObject, since it is used in both doc and parse_tree"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/common/test_doc.py",
        "commit_date": "2023-01-11T23:51:44Z",
        "message": "Add comments to the serialized format when saving a document.  This will keep any user added comments, but at the cost of making the files larger because they now have the text in a couple different places."
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/common/test_doc.py",
        "commit_date": "2023-01-02T02:50:45Z",
        "message": "Add a constituency comment to sentences in the doc\n\nThis adds the constituency tree to the CoNLL format as a comment\n\nIf there's somehow a newline in the parse tree, we convert \\n to *NL*\nto not make the CoNLL comments horribly broken - not sure this is\nideal, but hopefully \\n doesn't come up often"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/common/test_doc.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Rearrange most of the tests in stanza/tests"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_core.py",
        "commit_date": "2023-11-10T00:15:59Z",
        "message": "Add mwt to expected results for several more EN downloads"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_core.py",
        "commit_date": "2023-10-03T04:17:33Z",
        "message": "Update NER packages to reflect the new ontonotes_charlm default NER"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_core.py",
        "commit_date": "2023-02-04T07:43:37Z",
        "message": "Pick a language name we know won't exist for the test of an unknown language"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_core.py",
        "commit_date": "2023-02-04T07:30:22Z",
        "message": "Allow for loading a pipeline for a language that doesn't exist, as long as everything is specified"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_core.py",
        "commit_date": "2022-07-05T18:34:58Z",
        "message": "Add a test that download_method=None doesn't clobber the wrong model"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_core.py",
        "commit_date": "2022-07-05T18:34:55Z",
        "message": "Allow strings for the DownloadMethod.  Will hopefully make the interface simpler for people"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_core.py",
        "commit_date": "2022-05-30T07:32:52Z",
        "message": "Notes on which embeddings are used for which NER, in the form of a map of default pretrains\n\nUse the default pretrains when using run_ner.py\n\nUpdate prepare_resources to use the new ner embedding info"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_core.py",
        "commit_date": "2022-05-13T21:59:08Z",
        "message": "write temp directories to test working dir not present dir"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_core.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Allow for selectively using processors.  Answers #945"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_core.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Download missing models when creating a pipeline\n\nLess noisy logging when creating a pipeline from already downloaded items\nFilters variants before trying to download them\nDoesn't try to download models for annotators which don't exist\nAttempt to download a missing resources.json\nDownloads missing resources if REUSE_RESOURCES is set\n\nAdds tests of the reuse_resources and download_resources options\nincluding checks using mod times to verify that downloads occurred,\nand checks that various download methods will overwrite an incorrect model"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_core.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Move a bunch of tests to more appropriate homes"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/ner/test_ner_tagger.py",
        "commit_date": "2023-11-13T03:37:20Z",
        "message": "Use the correct pretrain path for the new NER default"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/ner/test_ner_tagger.py",
        "commit_date": "2023-10-18T15:10:19Z",
        "message": "Return entity F1 for each entity as part of the score_by_entity function in the scorer"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/ner/test_ner_tagger.py",
        "commit_date": "2023-10-03T04:17:33Z",
        "message": "Update NER packages to reflect the new ontonotes_charlm default NER"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/ner/test_ner_tagger.py",
        "commit_date": "2023-09-30T12:21:48Z",
        "message": "Unnecessary print at the end of a test"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/ner/test_ner_tagger.py",
        "commit_date": "2022-10-26T00:03:10Z",
        "message": "Mark this test with travis - not sure that is still relevant"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/ner/test_ner_tagger.py",
        "commit_date": "2022-08-25T01:07:44Z",
        "message": "Add a flag to output all the NER results to a file when evaluating"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/ner/test_ner_tagger.py",
        "commit_date": "2022-08-25T01:07:44Z",
        "message": "Add a simple test of ner_tagger.evaluate()"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/ner/test_ner_tagger.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Rearrange NER tests to their own directory"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/ner/test_convert_amt.py",
        "commit_date": "2022-08-19T23:30:41Z",
        "message": "Add a test for not capturing . at the end of a sentence if it was part of an AMT label"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/ner/test_convert_amt.py",
        "commit_date": "2022-08-19T20:40:31Z",
        "message": "Oops, finish an incomplete comment"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/ner/test_convert_amt.py",
        "commit_date": "2022-08-18T04:34:23Z",
        "message": "Add a tool to convert from an AMT annotation file to a bio/json NER file for Stanza"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_common.py",
        "commit_date": "2023-10-03T04:17:33Z",
        "message": "Update NER packages to reflect the new ontonotes_charlm default NER"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_common.py",
        "commit_date": "2023-03-22T03:12:50Z",
        "message": "Add a utility method for extracting the resources for a specific language, possibly following 'alias' links"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_common.py",
        "commit_date": "2022-08-02T06:30:14Z",
        "message": "Add a piece of doc"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_common.py",
        "commit_date": "2022-06-16T22:14:52Z",
        "message": "Change a bunch of tempdirs to be under TEST_WORKING_DIR"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_common.py",
        "commit_date": "2022-05-30T07:32:52Z",
        "message": "Notes on which embeddings are used for which NER, in the form of a map of default pretrains\n\nUse the default pretrains when using run_ner.py\n\nUpdate prepare_resources to use the new ner embedding info"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_common.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Add the ability to use an alternate_md5 to accept a file.  Will allow resource files to denote an older md5 which we allow when downloading a file.  In particular, that lets us get around the non-atomic nature of putting up a new resources.json file while the models are still uploading to huggingface"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_common.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Use ValueError and FileNotFoundError instead of assert for unexpected md5"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_common.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Allow specifying multiple packages of NER to download at the same time"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_common.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Add a test for the download of a single model and its dependencies"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_common.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Allow the package parameter to be a dict when specifying a list of processors.  Will make it easier to limit exactly which processors to choose, as package=None is no longer necessary"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_common.py",
        "commit_date": "2021-09-08T21:22:41Z",
        "message": "Separate tests of different components into different files"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_depparse.py",
        "commit_date": "2022-09-14T02:52:32Z",
        "message": "Squeeze a little bit more - only use depparse in the depparse pipeline"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_depparse.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "depparse requires lemmatization to get the right results"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_depparse.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Move a couple tests into their own pipeline directory"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_tokenizer.py",
        "commit_date": "2023-11-29T05:52:28Z",
        "message": "MWT expansion with MEXP attribute tags\n\nuses an enum to signify the manual MWT processing\n\nmanual MWT information is in a field instead of the misc field\n\ndefault of the manual expansion is `None`"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_tokenizer.py",
        "commit_date": "2023-11-10T01:30:23Z",
        "message": "spaCy tokenizer won't have MWT"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_tokenizer.py",
        "commit_date": "2023-11-10T00:21:17Z",
        "message": "Update the postprocessing test for the new English MWT default"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_tokenizer.py",
        "commit_date": "2023-11-09T23:51:44Z",
        "message": "Update a couple tests for the MWT change"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_tokenizer.py",
        "commit_date": "2023-10-02T20:59:27Z",
        "message": "Adds a tokenization postprocessor for manual token cleanup (#1290)\n\nIf a postprocessor is provided, the tokenizer passes the candidate tokenization before finalizing the document construction.  This allows for a postprocessor which fixes certain known errors or simply adjusts the tokenization to better match the user's downstream preferences"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_tokenizer.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Add unittest for char offset in skip_newline; fix over-scoped unittest; Fix further length calculation edge cases"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_tokenizer.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Move a bunch of tests to more appropriate homes"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/server/test_ud_enhancer.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Move a bunch of tests to a separate directory specifically for the corenlp client"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_decorators.py",
        "commit_date": "2023-09-14T05:13:00Z",
        "message": "I guess it's pretty close if the models learn another has a feature of Art or not"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_decorators.py",
        "commit_date": "2023-08-20T18:35:46Z",
        "message": "Update unit tests to match the newly trained POS models"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_decorators.py",
        "commit_date": "2022-12-07T07:06:06Z",
        "message": "Pass around device instead of use_gpu when running a Pipeline or MultilingualPipeline.\n\nThere is one downside to this approach, which is that existing custom\nprocessors which were a subclass of an existing processor and used the\ninitializer with use_gpu will need to be updated.  Hopefully this is a\nvery small problem - most processor variants would be independent of\nthe previous use_gpu system\n\nTheoretically this would let us make Pipelines on an \"mps\" if we\nwanted, but the torch 1.13.0 MPS backend has a bug which specifically\nmakes bidirectional LSTMs with batch_first unusable, so .... yeah"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_decorators.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Rearrange most of the tests in stanza/tests"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_lemmatizer.py",
        "commit_date": "2024-02-18T09:20:19Z",
        "message": "Update the caseless test to reflect the new models.  Example (or should we say Excerpt) chosen from EWT Dev"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_lemmatizer.py",
        "commit_date": "2024-02-03T03:34:40Z",
        "message": "Potentially lowercase the data in a lemmatizer if all of the training data (or a user flag) requested it\n\nTesting additions:\n\nAdd a basic unit test of the all_lowercase function\nAdd a test of the caseless lemmatizer in the Pipeline\nTest that the Latin ITTB lemmatizer is marked as caseless.  Check that the results for capitalized text is as expected\n\nAddresses https://github.com/stanfordnlp/stanza/issues/1330"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_lemmatizer.py",
        "commit_date": "2023-07-19T06:59:17Z",
        "message": "Add a flag to the pipeline lemmatizer which tells it to remember word,pos combinations it has seen before.  https://github.com/stanfordnlp/stanza/issues/1263"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_lemmatizer.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Move a bunch of tests to more appropriate homes"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/common/test_data_objects.py",
        "commit_date": "2023-11-13T04:33:25Z",
        "message": "Also embarrassing.  It should be able to recognize single word names"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/common/test_data_objects.py",
        "commit_date": "2022-09-14T02:20:10Z",
        "message": "Turn some pipelines getting built over and over into fixtures.  Will make them take up less GPU memory, even if the cleanup isn't reliable"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/common/test_data_objects.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Update a broken doc"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/common/test_data_objects.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Rearrange most of the tests in stanza/tests"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/tokenization/test_spaces.py",
        "commit_date": "2023-12-16T04:01:07Z",
        "message": "Refactor spaces_after and spaces_before so they are actual members of the Token, not just stuck on the MISC field"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/tokenization/test_spaces.py",
        "commit_date": "2023-12-16T04:01:07Z",
        "message": "Add a couple test cases of SpacesBefore and Space[s]After"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_requirements.py",
        "commit_date": "2023-11-09T23:51:44Z",
        "message": "Update a couple tests for the MWT change"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_requirements.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Move a bunch of tests to more appropriate homes"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/classifiers/test_classifier.py",
        "commit_date": "2024-01-27T03:06:01Z",
        "message": "Update test to reflect changed defaults"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/classifiers/test_classifier.py",
        "commit_date": "2024-01-27T03:06:01Z",
        "message": "Add a PEFT wrapper for the Sentiment training.\n\nWorks quite well on English, actually, even without splitting the optimizer or implementing any form of scheduling.\nWith no finetuning, adding electra-large to the 3 class English dataset (SST plus a few other pieces) gets 70 Macro F1.\nThe base finetuning gets between 74-75 macro F1 on sstplus, but frequently fails to successfully train, getting somewhere around 60 F1\nTraining with PEFT gets in the 74-75 F1 range each time, with no failures observed so far.\n\nAdds a chunk of test to the sentiment training which starts the Pipeline with a peft-trained model"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/classifiers/test_classifier.py",
        "commit_date": "2024-01-19T06:30:52Z",
        "message": "Add finetuning of the transformer to the CNN Classifier.  Include a couple tests using the tiny bert model from HF"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/classifiers/test_classifier.py",
        "commit_date": "2023-02-06T20:44:08Z",
        "message": "Add constituency items to SentimentDatums (convert from namedtuple to a simple class)\n\nUse the new _asdict method when writing a dataset\nRead trees back from the classifier data files.\nAdd a test of the tree reading by adding trees to one of the test datasets.\n\nMove the fake datasets to test_data.py for better organization - now they are all with the reading test code"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/classifiers/test_classifier.py",
        "commit_date": "2023-02-06T18:05:02Z",
        "message": "Move the fake_embeddings fixture to make it easier to reuse elsewhere, eg other tests which need the fake embeddings"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/classifiers/test_classifier.py",
        "commit_date": "2022-10-11T16:58:10Z",
        "message": "Remove unneeded import"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/classifiers/test_classifier.py",
        "commit_date": "2022-10-04T17:49:54Z",
        "message": "Refactor test_data.py - will be a good home for more classifier data tests"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/classifiers/test_classifier.py",
        "commit_date": "2022-10-04T15:32:24Z",
        "message": "Rename classifier_args -> utils.  Annoying to rebuild the models, but maybe worth the hassle to make things a bit cleaner"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/classifiers/test_classifier.py",
        "commit_date": "2022-09-13T23:26:06Z",
        "message": "Try to reduce the scope on various pipelines to make the test suite less likely to run out of GPU memory.  Not sure this is the correct approach"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/classifiers/test_classifier.py",
        "commit_date": "2022-08-31T20:38:43Z",
        "message": "Make saved models smaller in the classifier test.  Will hopefully save disk space and time"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/classifiers/test_classifier.py",
        "commit_date": "2022-08-31T19:46:20Z",
        "message": "Update a couple defaults based on recent experiments"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/classifiers/test_classifier.py",
        "commit_date": "2022-08-31T06:18:26Z",
        "message": "Add a checkpoint mechanism to sentiment\n\npass checkpoint_file to train_model in the unittest, but TODO: need to add tests for checkpointing"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/classifiers/test_classifier.py",
        "commit_date": "2022-08-31T05:43:23Z",
        "message": "Simplify the load mechanism in classifier Trainer so that the load() call loads the pretrain, charlm, etc"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/classifiers/test_classifier.py",
        "commit_date": "2022-08-31T01:59:57Z",
        "message": "Refactor a Trainer object out of the classifier.py main program.  In addition to the model, this saves and loads the optimizer and the number of epochs trained.  Purpose: to make it so that it is easy to checkpoint model training the same way the charlm is checkpointed"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/classifiers/test_classifier.py",
        "commit_date": "2022-08-31T00:44:10Z",
        "message": "Refactor a bunch of data manipulation methods to data.py"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/classifiers/test_classifier.py",
        "commit_date": "2022-08-22T16:49:47Z",
        "message": "Add the ability to add a list of filter output sizes instead of just one-size-fits-all"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/classifiers/test_classifier.py",
        "commit_date": "2022-08-15T00:11:01Z",
        "message": "2d conv.  Uses the width of a conv feature to rescale the output\nchannels, making it possible to combine the original full-width\nfeatures with the smaller conv features\n\nAdds some logging for how big each filter is"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/classifiers/test_classifier.py",
        "commit_date": "2022-08-14T05:44:11Z",
        "message": "Begin writing a test for the classifier\n\nTests building a model, saving & loading, iterating a couple times on\na couple varieties of models, and some of the utility methods\n\nTests bilstm & maxpool widths"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_installation.py",
        "commit_date": "2022-06-16T22:14:52Z",
        "message": "Change a bunch of tempdirs to be under TEST_WORKING_DIR"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_installation.py",
        "commit_date": "2021-10-04T05:11:13Z",
        "message": "Merge branch 'main' into dev"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_installation.py",
        "commit_date": "2021-09-08T21:22:41Z",
        "message": "Separate tests of different components into different files"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_installation.py",
        "commit_date": "2021-09-08T21:22:41Z",
        "message": "Move the installation test to its own directory"
    },
    {
        "repo_url": "github.com/XiangLi1999/Diffusion-LM",
        "filepath": "improved-diffusion/scripts/infill.py",
        "commit_date": "2022-06-06T22:38:39Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/microsoft/presidio",
        "filepath": "presidio-analyzer/presidio_analyzer/nlp_engine/stanza_nlp_engine.py",
        "commit_date": "2023-10-19T10:58:04Z",
        "message": "Integrating spacy-huggingface-pipelines and refactoring NlpEngine logic (#1159)"
    },
    {
        "repo_url": "github.com/microsoft/presidio",
        "filepath": "presidio-analyzer/presidio_analyzer/nlp_engine/stanza_nlp_engine.py",
        "commit_date": "2021-05-23T10:03:50Z",
        "message": "updated call to stanza via spacy-stanza (#711)"
    },
    {
        "repo_url": "github.com/microsoft/presidio",
        "filepath": "presidio-analyzer/presidio_analyzer/nlp_engine/stanza_nlp_engine.py",
        "commit_date": "2021-02-08T12:34:25Z",
        "message": "Configurable logging (#469)"
    },
    {
        "repo_url": "github.com/microsoft/presidio",
        "filepath": "presidio-analyzer/presidio_analyzer/nlp_engine/stanza_nlp_engine.py",
        "commit_date": "2021-02-03T23:00:55Z",
        "message": "revert"
    },
    {
        "repo_url": "github.com/microsoft/presidio",
        "filepath": "presidio-analyzer/presidio_analyzer/nlp_engine/stanza_nlp_engine.py",
        "commit_date": "2021-02-03T22:55:12Z",
        "message": "new logging"
    },
    {
        "repo_url": "github.com/microsoft/presidio",
        "filepath": "presidio-analyzer/presidio_analyzer/nlp_engine/stanza_nlp_engine.py",
        "commit_date": "2021-01-21T13:05:11Z",
        "message": "Linting updates for the analyzer given the new flake8 rules (#412)"
    },
    {
        "repo_url": "github.com/microsoft/presidio",
        "filepath": "presidio-analyzer/presidio_analyzer/nlp_engine/stanza_nlp_engine.py",
        "commit_date": "2021-01-13T13:40:20Z",
        "message": "Remove pylint"
    },
    {
        "repo_url": "github.com/microsoft/presidio",
        "filepath": "presidio-analyzer/presidio_analyzer/nlp_engine/stanza_nlp_engine.py",
        "commit_date": "2021-01-13T10:38:30Z",
        "message": "Omri374/analyzer v2 (#392)\n\n* Removed all previous dependencies from NOTICE\n\n* 1. Removed pyre2\n\n2. Removed the GRPC app.py and pb2 files\n3. Added type hints\n4. Removed recognizer store API calls\n5. Added batch calls to the analyzer engine (simple implementation)\n6. Used black to format all files\n7. SpacyNlpEngine now returns spacy tokens and not just the text tokens\n8. Fixed bug in us SSN recognizer\n9. Remove duplicates is now per recognizer (if the recognizer identifies the same text as entity by multiple different logics, e.g. regex patterns then only the one with the highest confidence is kept.\n9. Fixed tests to support 1-9\n10. Demo test now compares actual anonymized text with expected anonymized text (more representative of the real demo website)\n\n* updated setup.py\n\n* fixed logging across, new README\n\n* minor fixes\n\n* Set up CI with Azure Pipelines\n\n* removed analyzer_batch and added analyzer.get_supported_entities\n\n* Update azure-pipelines.yml for Azure Pipelines\n\n* Removed all previous dependencies from NOTICE\n\n* 1. Removed pyre2\n\n2. Removed the GRPC app.py and pb2 files\n3. Added type hints\n4. Removed recognizer store API calls\n5. Added batch calls to the analyzer engine (simple implementation)\n6. Used black to format all files\n7. SpacyNlpEngine now returns spacy tokens and not just the text tokens\n8. Fixed bug in us SSN recognizer\n9. Remove duplicates is now per recognizer (if the recognizer identifies the same text as entity by multiple different logics, e.g. regex patterns then only the one with the highest confidence is kept.\n9. Fixed tests to support 1-9\n10. Demo test now compares actual anonymized text with expected anonymized text (more representative of the real demo website)\n\n* updated setup.py\n\n* fixed logging across, new README\n\n* minor fixes\n\n* removed analyzer_batch and added analyzer.get_supported_entities\n\n* removed setup.cfg for now\n\n* readding setup.cfg with flake8 max-line-width\n\n* removed duplicate word\n\n* Added internal link to customization"
    },
    {
        "repo_url": "github.com/microsoft/presidio",
        "filepath": "presidio-analyzer/presidio_analyzer/nlp_engine/stanza_nlp_engine.py",
        "commit_date": "2020-07-22T14:58:44Z",
        "message": "[WIP] analyzer - multiple languages and nlp engines (#312)\n\n* analyzer - multiple languages and nlp engines\n\nInitially this was my attempt to use stanza, which is an nlp engine by\nStanford.  But generally, it's an update to allow for one to add NLP\nengines and custom recognizers more easily.  Specifically, I\nstandardized the format of the recognizers, removed use of global\nvariables when possible, and removed a lot of hard-coding of defaults.\n\nI am thinking of using presidio for several non-english projects at work\nand these are several of the changes that I made.\n\nBelow is a list of the changes in list form:\n\n* make spacy and/or stanza optional\n  * remove requirement of en_core_web_lg from install\n* allow predefined recognizers to take parameters\n  * this allows for easily using these as non-english recognizers\n* create config files for different NLP engines\n* create tests for stanza\n* make all spacy and stanza tests optional\n* create a Dockerfile for an anaconda-based image\n  * pytorch is built with MKL and is much faster on cpu from conda\n* completely rewrote the IBAN recognizer\n  * the current version only recognizes IBANs if they are the entirety\n    of the string.  This version will find IBANs in sentences.\n* fixed some tests\n* created a `run.sh` file, so just run dockers without rebuilding them\n\n\"Breaking\" Changes:\n\n* I would like to use [black](https://github.com/psf/black), but it's\n  not super friendly with pylint.  My suggestion is to drop pylint and\n  use black instead.\n* Default spacy model is `en` rather than `en_core_web_lg` and no spacy\n  models are downloaded by default.  The idea is to let the user choose\n  which models they want.  For non-english users, it saves a lot of time\n  at installation because you don't need to install the large spacy\n  model that you aren't using.\n\nSigned-off-by: David Pollack <d.pollack@solvemate.com>\n\n* spacy required, spacy-stanza, update tests\n\n* made spacy required\n* using spacy-stanza for stanza models\n* refactor tests to use pytest\n* make one test reliant on big model optional\n\n* refactor tests to pytest\n\nAll tests have been refactored to use pytest.  Previously, there was a\nmix of unittest, pytest and miscellaneous global initializations.  This\ncommit moves everything to pytest.  There is now extensive use of\nfixtures instead of global variables and parametrized tests instead of\nduplicated code for each test.  The major difference is that\nparametrized tests are not individually named.\n\n* changes based on PR comments\n\n* fixes to Dockerfiles\n\n* remove sys.path.append\n\n* fix pipeline errors (i.e. install spacy model)\n\nthis installs the big spacy model by default in the Docker and the Azure\npipeline.\n\n* fix rebase errors\n\n* use Pattern class\n\n* update docs\n\n* use PresidioLogger\n\n* linting fixes\n\n* move imports to top level\n\n* edits based on PR-review\n\n* add documentation and doc strings\n* change yaml field names to be more logical\n\n* fix pipelines based on PR comments"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_french_pipeline.py",
        "commit_date": "2023-12-16T04:01:07Z",
        "message": "Fix up the unit tests now that the output formats expect to have the Spaces annotations on the MISC columns"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_french_pipeline.py",
        "commit_date": "2023-09-21T07:06:38Z",
        "message": "Some changes in the unit test from using the combined model instead of the gsd model for French.  The obl:mod looks a little sus given how many obl:arg there are for public in the training data, but overall the combined model is still highly accurate on GSD and is much more accurate on the other three datasets in its collection"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_french_pipeline.py",
        "commit_date": "2023-08-25T00:00:43Z",
        "message": "Latest model updates this to a fixed instead of advmod.  It's not clear (to me) which is correct.  Alors almost always has fixed for incoming dependencies, but encore almost always has advmod for outgoing dependencies"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_french_pipeline.py",
        "commit_date": "2022-12-07T07:06:06Z",
        "message": "Add a test which checks that all models from a pipeline are on the expected device"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_french_pipeline.py",
        "commit_date": "2022-10-11T04:09:44Z",
        "message": "Update FR depparse test.  Not sure which label is correct, but overall the new model is more accurate"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_french_pipeline.py",
        "commit_date": "2022-07-27T03:05:56Z",
        "message": "Latest POS models change this word to NOUN"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_french_pipeline.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Move a couple tests into their own pipeline directory"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_arabic_pipeline.py",
        "commit_date": "2023-10-06T04:33:17Z",
        "message": "Add a small test of Arabic MWT and POS"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_english_pipeline.py",
        "commit_date": "2023-12-16T04:01:07Z",
        "message": "Fix up the unit tests now that the output formats expect to have the Spaces annotations on the MISC columns"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_english_pipeline.py",
        "commit_date": "2023-08-29T04:58:37Z",
        "message": "Fix an empty bulk process not processing correctly (#1278)"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_english_pipeline.py",
        "commit_date": "2023-02-14T22:21:34Z",
        "message": "Add a method to reindex sentences.  Use it to keep the sent_id increasing even when using a 'stream' of documents"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_english_pipeline.py",
        "commit_date": "2023-02-14T17:44:16Z",
        "message": "Add a streaming interface, as requested in #550"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_english_pipeline.py",
        "commit_date": "2023-02-14T17:15:35Z",
        "message": "Add a bulk_process method which does the Document wrapping for the user - try to make the interface a bit simpler for the user"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_english_pipeline.py",
        "commit_date": "2023-01-12T04:59:07Z",
        "message": "Use comments to store the sentiment values as well"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_english_pipeline.py",
        "commit_date": "2023-01-02T02:50:45Z",
        "message": "Add a constituency comment to sentences in the doc\n\nThis adds the constituency tree to the CoNLL format as a comment\n\nIf there's somehow a newline in the parse tree, we convert \\n to *NL*\nto not make the CoNLL comments horribly broken - not sure this is\nideal, but hopefully \\n doesn't come up often"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_english_pipeline.py",
        "commit_date": "2022-12-23T00:01:22Z",
        "message": "Move the token to conll text functionality to doc.py.  Use this to create C (with comments) and c (no comments) formats for Document and Sentence."
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_english_pipeline.py",
        "commit_date": "2022-12-07T07:06:06Z",
        "message": "Add a test which checks that all models from a pipeline are on the expected device"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_english_pipeline.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "New models get the features correct"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_english_pipeline.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Write NER tags to conll docs\n\ndon't put NER on the word misc field"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_english_pipeline.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Process sentence ids from the corpus, if available.\n\nThere is now a sentence.sent_id field on the Sentences which takes the\nvalue of the sent_id field from a document comments, if available, or\nreuses the sentence id set during document creation\n\nAlso, there is now a warning that .id will be deprecated (it's a\nkeyword, after all) in favor of .index.  Documentation is updated to\nno longer suggest that .index is 1 indexed, since it wasn't"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_english_pipeline.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Add a #text comment to each sentence in a doc if it doesn't already exist\n\nAdd #text markers to the unit tests where expected"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_english_pipeline.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Move a couple tests into their own pipeline directory"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/classifiers/test_process_utils.py",
        "commit_date": "2023-02-06T20:44:08Z",
        "message": "Add constituency items to SentimentDatums (convert from namedtuple to a simple class)\n\nUse the new _asdict method when writing a dataset\nRead trees back from the classifier data files.\nAdd a test of the tree reading by adding trees to one of the test datasets.\n\nMove the fake datasets to test_data.py for better organization - now they are all with the reading test code"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/classifiers/test_process_utils.py",
        "commit_date": "2022-10-19T22:09:05Z",
        "message": "Extend read_snippets for sentiment datasets to read multiple columns and combine them into a single sentiment value"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/classifiers/test_process_utils.py",
        "commit_date": "2022-10-04T17:49:54Z",
        "message": "Move SentimentDatum to classifiers/data.py and make the reading code read in SentimentDatum"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/classifiers/test_process_utils.py",
        "commit_date": "2022-10-04T17:49:54Z",
        "message": "Add a couple tests of the building block methods from the Sentiment conversion tools"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/tokenization/test_tokenize_data.py",
        "commit_date": "2022-06-30T01:29:33Z",
        "message": "Fix that it takes forever to tokenize a really long non-numeric token\n\nBug very helpfully reported by\nSk Adnan Hassan (VT) and Zainab Aamir (Stony Brook)"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/tokenization/test_tokenize_data.py",
        "commit_date": "2022-06-24T04:01:42Z",
        "message": "Add a few tests of the NUMERIC_RE"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/tokenization/test_tokenize_data.py",
        "commit_date": "2022-05-16T20:00:40Z",
        "message": "Add the skip_newlines test to the file reading version of the tokenizer data as well"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/tokenization/test_tokenize_data.py",
        "commit_date": "2022-05-16T19:48:53Z",
        "message": "Abstract away labels() rather than having the eval code know the format of the data object"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/tokenization/test_tokenize_data.py",
        "commit_date": "2022-05-16T06:53:53Z",
        "message": "For the MWT test, use the fake tokenizer files rather than putting in the fake data format"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/tokenization/test_tokenize_data.py",
        "commit_date": "2022-05-16T06:51:18Z",
        "message": "Factor out a method to write the input to temp files in a tokenizer test"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/tokenization/test_tokenize_data.py",
        "commit_date": "2022-05-16T06:33:51Z",
        "message": "Add a tiny bit of doc"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/tokenization/test_tokenize_data.py",
        "commit_date": "2022-05-16T04:07:02Z",
        "message": "Run some basic tests on the dictionary in the ZH tokenizer"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/tokenization/test_tokenize_data.py",
        "commit_date": "2022-05-15T07:06:47Z",
        "message": "Add a test which pokes the DataLoader object to make sure it is processing data as expected\n\nAdd a small test case for reading data from a file\n\nAdd a test of the skip_newline ability used in the Chinese tokenizer"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/tokenization/test_tokenize_data.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Move a bunch of tests to more appropriate homes"
    },
    {
        "repo_url": "github.com/explosion/projects",
        "filepath": "benchmarks/speed/scripts/run_nlp.py",
        "commit_date": "2022-08-05T12:14:31Z",
        "message": "Update speed benchmark (#120)\n\n* Update speed benchmark\n\n* Restore install command without torch URL"
    },
    {
        "repo_url": "github.com/explosion/projects",
        "filepath": "benchmarks/speed/scripts/run_nlp.py",
        "commit_date": "2021-02-01T13:34:47Z",
        "message": "Update speed benchmarking project (#29)\n\n* add token_splitter to spacy's trf model for speed benchmarking\n\n* update readme, clarify private files\n\n* limit Stanza's GPU memory usage\n\n* add depparse_batch_size\n\n* fix comment\n\n* add textcat_multilabel fix"
    },
    {
        "repo_url": "github.com/explosion/projects",
        "filepath": "benchmarks/speed/scripts/run_nlp.py",
        "commit_date": "2020-10-15T14:33:09Z",
        "message": "Fix transformer CPU"
    },
    {
        "repo_url": "github.com/explosion/projects",
        "filepath": "benchmarks/speed/scripts/run_nlp.py",
        "commit_date": "2020-10-15T14:13:27Z",
        "message": "Use 10k texts, not 1k"
    },
    {
        "repo_url": "github.com/explosion/projects",
        "filepath": "benchmarks/speed/scripts/run_nlp.py",
        "commit_date": "2020-10-15T12:42:17Z",
        "message": "Fix spacy batch size"
    },
    {
        "repo_url": "github.com/explosion/projects",
        "filepath": "benchmarks/speed/scripts/run_nlp.py",
        "commit_date": "2020-10-15T12:06:31Z",
        "message": "Upd benchmark"
    },
    {
        "repo_url": "github.com/explosion/projects",
        "filepath": "benchmarks/speed/scripts/run_nlp.py",
        "commit_date": "2020-10-15T11:09:29Z",
        "message": "Format"
    },
    {
        "repo_url": "github.com/explosion/projects",
        "filepath": "benchmarks/speed/scripts/run_nlp.py",
        "commit_date": "2020-10-15T11:08:33Z",
        "message": "Fix speed"
    },
    {
        "repo_url": "github.com/explosion/projects",
        "filepath": "benchmarks/speed/scripts/run_nlp.py",
        "commit_date": "2020-10-14T19:49:32Z",
        "message": "Fix import"
    },
    {
        "repo_url": "github.com/explosion/projects",
        "filepath": "benchmarks/speed/scripts/run_nlp.py",
        "commit_date": "2020-10-14T19:41:41Z",
        "message": "Fix batching"
    },
    {
        "repo_url": "github.com/explosion/projects",
        "filepath": "benchmarks/speed/scripts/run_nlp.py",
        "commit_date": "2020-10-14T19:36:37Z",
        "message": "Fix batching"
    },
    {
        "repo_url": "github.com/explosion/projects",
        "filepath": "benchmarks/speed/scripts/run_nlp.py",
        "commit_date": "2020-10-14T19:25:01Z",
        "message": "Update scripts"
    },
    {
        "repo_url": "github.com/explosion/projects",
        "filepath": "benchmarks/speed/scripts/run_nlp.py",
        "commit_date": "2020-09-29T08:10:23Z",
        "message": "add spacy transformers"
    },
    {
        "repo_url": "github.com/explosion/projects",
        "filepath": "benchmarks/speed/scripts/run_nlp.py",
        "commit_date": "2020-09-29T07:23:07Z",
        "message": "add udpipe"
    },
    {
        "repo_url": "github.com/explosion/projects",
        "filepath": "benchmarks/speed/scripts/run_nlp.py",
        "commit_date": "2020-09-28T20:14:09Z",
        "message": "refactor"
    },
    {
        "repo_url": "github.com/explosion/projects",
        "filepath": "benchmarks/speed/scripts/run_nlp.py",
        "commit_date": "2020-09-28T19:24:51Z",
        "message": "add flair models"
    },
    {
        "repo_url": "github.com/explosion/projects",
        "filepath": "benchmarks/speed/scripts/run_nlp.py",
        "commit_date": "2020-09-28T15:36:03Z",
        "message": "run each lib in separate python call to avoid mem accumulation"
    },
    {
        "repo_url": "github.com/explosion/projects",
        "filepath": "benchmarks/speed/scripts/run_nlp.py",
        "commit_date": "2020-09-28T13:03:46Z",
        "message": "run stanza"
    },
    {
        "repo_url": "github.com/explosion/projects",
        "filepath": "benchmarks/speed/scripts/run_nlp.py",
        "commit_date": "2020-09-28T09:41:50Z",
        "message": "fix HF model"
    },
    {
        "repo_url": "github.com/explosion/projects",
        "filepath": "benchmarks/speed/scripts/run_nlp.py",
        "commit_date": "2020-09-24T23:48:08Z",
        "message": "add requirements"
    },
    {
        "repo_url": "github.com/explosion/projects",
        "filepath": "benchmarks/speed/scripts/run_nlp.py",
        "commit_date": "2020-09-24T23:21:04Z",
        "message": "setup benchmarking project"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/tokenization/test_tokenize_utils.py",
        "commit_date": "2023-12-16T04:01:07Z",
        "message": "Fix up the unit tests now that the output formats expect to have the Spaces annotations on the MISC columns"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/tokenization/test_tokenize_utils.py",
        "commit_date": "2023-11-29T05:52:28Z",
        "message": "MWT expansion with MEXP attribute tags\n\nuses an enum to signify the manual MWT processing\n\nmanual MWT information is in a field instead of the misc field\n\ndefault of the manual expansion is `None`"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/tokenization/test_tokenize_utils.py",
        "commit_date": "2023-10-14T21:44:37Z",
        "message": "Add a (brief) test of turning a tokenizer training file into a lexicon"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/tokenization/test_tokenize_utils.py",
        "commit_date": "2023-10-02T20:59:27Z",
        "message": "Adds a tokenization postprocessor for manual token cleanup (#1290)\n\nIf a postprocessor is provided, the tokenizer passes the candidate tokenization before finalizing the document construction.  This allows for a postprocessor which fixes certain known errors or simply adjusts the tokenization to better match the user's downstream preferences"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/tokenization/test_tokenize_utils.py",
        "commit_date": "2022-05-15T07:06:44Z",
        "message": "Separate the next() functionality which advances an unfinished batch into a separate function.  Add a test that the separate function is being called"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/tokenization/test_tokenize_utils.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Add a utility method for constructing a Document from pretokenized sentences and an original text\n\nConsistently throws ValueError for problems with the pretokenization\nIncludes a test of a couple basic examples\n\nAddresses #967"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/tokenization/test_tokenize_utils.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Move a bunch of tests to more appropriate homes"
    },
    {
        "repo_url": "github.com/AlibabaResearch/DAMO-ConvAI",
        "filepath": "proton/preprocess/cu.py",
        "commit_date": "2022-08-06T11:43:17Z",
        "message": "add: proton"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_pipeline_mwt_expander.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Rearrange most of the tests in stanza/tests"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_pipeline_ner_processor.py",
        "commit_date": "2023-10-03T04:17:33Z",
        "message": "Update NER packages to reflect the new ontonotes_charlm default NER"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_pipeline_ner_processor.py",
        "commit_date": "2022-09-13T23:26:06Z",
        "message": "Try to reduce the scope on various pipelines to make the test suite less likely to run out of GPU memory.  Not sure this is the correct approach"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_pipeline_ner_processor.py",
        "commit_date": "2022-09-08T18:30:47Z",
        "message": "NER get_known_tags possibly applies to multiple models"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_pipeline_ner_processor.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Add a field for multiple NER annotations in a tuple\n\nAdd a test for the new multi_ner field in the pipeline"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_pipeline_ner_processor.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "At long last, add a mechanism for using multiple NER models at once\n\nDownload ncbi_disease as part of the testing infrastructure.  Use it to run two NER models at once"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_pipeline_ner_processor.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Move a couple tests into their own pipeline directory"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_pipeline_pos_processor.py",
        "commit_date": "2023-10-06T04:32:08Z",
        "message": "No need to download the resources every time this test is run - it should already be downloaded from setup.py"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_pipeline_pos_processor.py",
        "commit_date": "2022-07-06T23:59:48Z",
        "message": "Add getting all possible values for each feat"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_pipeline_pos_processor.py",
        "commit_date": "2022-07-02T00:20:50Z",
        "message": "Add methods for getting the xpos, upos, and feats from a pos model in the pos_processor.  If needed, we can always add it to the trainer or model as well."
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_pipeline_pos_processor.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Rearrange most of the tests in stanza/tests"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2023-10-17T23:51:16Z",
        "message": "Cleanup"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2023-10-17T23:51:16Z",
        "message": "Add BPE encoding, prefix/suffix tokens, target suffix support"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2023-10-17T23:51:16Z",
        "message": "Add tokenizer class"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2023-09-03T16:59:02Z",
        "message": "Formatting"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2023-06-23T00:20:23Z",
        "message": "store cached translations"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2023-03-25T19:09:49Z",
        "message": "Fix sort order in CompositeTranslation\n\nAll of the hypotheses scores from CTranslate2 are negative with\nhigher scores being better. This sorts in descending order to select\nthe translations with the best scores.\n\nhttps://github.com/argosopentech/argos-translate/issues/328"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2022-11-29T23:31:03Z",
        "message": "Fix Improve imports #297"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2022-11-26T20:38:44Z",
        "message": "Bug fix\n\n- https://github.com/argosopentech/argos-translate/issues/298"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2022-10-14T23:08:28Z",
        "message": "Error handling translate.get_language_from_code"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2022-10-14T23:07:58Z",
        "message": "Formatting"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2022-10-14T22:11:52Z",
        "message": "remove types in comments"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2022-10-14T22:11:52Z",
        "message": "remove comments & add missing types"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2022-10-14T22:11:52Z",
        "message": "add types & fix"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2022-10-14T22:11:52Z",
        "message": "add more types & fix a typo"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2022-10-14T22:11:52Z",
        "message": "add types for translate"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2022-09-16T22:39:02Z",
        "message": "Bug fix"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2022-09-11T19:01:15Z",
        "message": "Formatting"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2022-05-26T13:30:10Z",
        "message": "Add translate function"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2022-05-26T13:14:45Z",
        "message": "Add translate documentation"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2022-05-26T13:09:56Z",
        "message": "Document translation by code helper functions"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2022-05-26T12:59:35Z",
        "message": "doesnt return false, let the errors to be throw"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2022-05-26T12:59:35Z",
        "message": "rename function"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2022-05-26T12:59:35Z",
        "message": "add two helpers functions"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2022-02-09T23:03:41Z",
        "message": "remove repetition_penalty (#235)"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2022-02-08T11:37:13Z",
        "message": "add repetition_penalty (#234)"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-08-06T12:06:23Z",
        "message": "Tag translation improvements\n\n- Fix spaces formatting\n- Enable tag blacklist"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-08-05T00:06:40Z",
        "message": "Simplfied tag logic"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-08-04T12:38:33Z",
        "message": "Fixed tag translation formatting"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-07-13T22:09:55Z",
        "message": "Formatting"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-07-13T12:42:49Z",
        "message": "Fewshot sbd"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-06-21T12:34:21Z",
        "message": "Implemented fewshot translation"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-06-20T14:01:43Z",
        "message": "Support for LibreTranslate model"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-06-20T13:26:49Z",
        "message": "Reorganize sbd code"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-06-17T12:31:42Z",
        "message": "Reorganize argostranslate.models"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-06-17T12:22:30Z",
        "message": "Formatting"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-06-15T23:16:29Z",
        "message": "Upgraded to CTranslate2 2.0\n\n- https://github.com/argosopentech/argos-translate/issues/133\n- https://github.com/OpenNMT/CTranslate2/releases/tag/v2.0.0"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-06-13T13:14:38Z",
        "message": "Added LibreTranslate translation"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-06-06T19:02:04Z",
        "message": "Fixed unit tests"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-06-02T02:39:59Z",
        "message": "Bug fix"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-06-02T02:24:56Z",
        "message": "Improved logging"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-06-02T01:58:35Z",
        "message": "Set min beam size of 4\n\n- https://github.com/argosopentech/argos-translate/issues/121"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-06-02T01:53:47Z",
        "message": "Formatting"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-06-01T11:20:25Z",
        "message": "Update stanza check"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-06-01T11:20:25Z",
        "message": "Fix None check"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-06-01T11:20:25Z",
        "message": "Fix logging statements"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-06-01T11:20:25Z",
        "message": "Minor improvements"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-05-19T12:21:22Z",
        "message": "Filter available packages by sbd available"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-05-18T11:58:20Z",
        "message": "Filter installed languages for sbd"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-05-17T22:10:23Z",
        "message": "Added code formatting with black\n\n- https://github.com/argosopentech/argos-translate/issues/6"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-05-16T12:23:27Z",
        "message": "Auto download sbd package"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-05-15T01:01:38Z",
        "message": "Improved sentence boundary detection"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-05-04T00:30:03Z",
        "message": "Renamed warning log to match Python std lib"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-04-27T12:44:29Z",
        "message": "Use CTranslate2 to split batches\n\n- https://github.com/argosopentech/argos-translate/issues/97\n- https://github.com/argosopentech/argos-translate/commit/0827bebb28c10915814ced9b3de9c56b1c25fe5d#commitcomment-50047147"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-04-27T00:36:11Z",
        "message": "Break long translations over multiple batches\n\n- https://github.com/argosopentech/argos-translate/issues/97"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-04-24T20:29:35Z",
        "message": "Sentence boundary detection"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-04-07T11:12:28Z",
        "message": "Added argos_experimental setting"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-03-27T12:11:06Z",
        "message": "Deprecated translate.load_installed_languages"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-03-21T14:51:37Z",
        "message": "Use list-comprehension instead of `[]*` (multiple reference)\n\nThis avoids potential confusions, and also avoids shallow-copying\nthe class-object."
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-03-21T14:51:37Z",
        "message": "Add `@staticmethod` to static methods of `ITranslation`"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-03-21T12:48:02Z",
        "message": "Port the new combine-results loop from CachedTranslation to PackageTranslation\n\nThis prevents potential reference/mem unwanted overwrites, as explained\nin 55ee8cd13d"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-03-21T12:48:02Z",
        "message": "Fix wrong output issue for CachedTranslation.hypotheses()\n\nThere were two problems observed on the output when using this function\nwith multi-line input:\n\n* One was wrong indices (`hypotheses_to_return[j]`) being assigned\nFor multi-line inputs the result always was [..., Hypothesis('', 0)] except\nfor the first member of the list.\n* Second, the += operation was causing single-line results, ignoring all\n'\\n's that it should add.\n* While here take care of extra '\\n'-prefix (produced when using ('', 0) objs\nto prevent shallow-copy problem)"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-03-21T12:48:02Z",
        "message": "Bugfix: Invalidate cache after `num_hypotheses` changes\n\nAfter the `num_hypotheses` changes, we possibly have cached translations that\nare searched with different beam_search parameters, and also stored in\nthe lists with different lengths.\nThe latter becomes problem when combining paragraphs.\nSo in this case we re-do the search with new parameter(s), and cache the result."
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-03-21T12:48:02Z",
        "message": "Use a more precise and easier-to-read way for Hypothesis.__str__"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-03-21T12:48:02Z",
        "message": "Fix typo in method signature\n\nJust like `split_into_paragraphs()` the `combine_paragraphs()` also\nappears being used statically, so the syntax is fixed here."
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-03-20T18:48:02Z",
        "message": "Fixed rebasing bug"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-03-20T18:39:05Z",
        "message": "Implement __str__ and __repr__ for Hypothesis\n\nThis makes it easier to debug/represent the objects of this type"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-03-20T18:38:03Z",
        "message": "Renamed Hypothesis.output Hypothesis.value"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-03-20T18:09:29Z",
        "message": "Supported Hypothesis's for CachedTranslation"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-03-20T17:00:08Z",
        "message": "Implemented Hypothesis's for CompositeTranslations"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-03-20T15:48:11Z",
        "message": "Documentation for translation module"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-03-20T14:19:57Z",
        "message": "Added score to hypotheses"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-03-18T23:23:03Z",
        "message": "Added assertion that sentences within batch size"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-03-18T23:22:14Z",
        "message": "Renamed multi_translation\n\n- https://github.com/argosopentech/argos-translate/pull/52\n- Formatting"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-03-18T22:39:16Z",
        "message": "Keep the API backwards compatible\n\nHave `translate()` in which we call `multi_translate()` and return the first result."
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-03-18T22:39:16Z",
        "message": "Make it possible to return more than one 'hypothetical' translations\n\nThis change makes it possible to pass `num_hypotheses` and `beam_size`\nto CTranslate and have multiple results returned\nResults will be combined and compiled into a list and returned"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-03-16T01:23:16Z",
        "message": "Improved logging"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-03-07T21:39:41Z",
        "message": "Added more debug statements\n\n- https://github.com/argosopentech/argos-translate/issues/42"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-02-19T02:53:04Z",
        "message": "Added DEBUG flag and debugging print statements"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-02-02T23:58:51Z",
        "message": "Replaced unknowns with source tokens"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-02-02T23:55:21Z",
        "message": "Factored out tokenization logic"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-01-23T23:34:37Z",
        "message": "Fixed typo in docs"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2021-01-02T16:32:39Z",
        "message": "Changed CTranslate length_penalty to 0.2\n\n- https://forum.opennmt.net/t/suggested-value-for-length-penalty/4134\n- https://github.com/argosopentech/argos-translate/issues/4"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2020-11-01T21:39:26Z",
        "message": "Minor bug fixes"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2020-11-01T18:01:10Z",
        "message": "Defaulted to English as the from language"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2020-10-31T14:46:08Z",
        "message": "Cached CTranslate Translators"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2020-10-22T02:13:08Z",
        "message": "Switched to properly using CTranslate batches.\n\nAs described: https://forum.opennmt.net/t/gui-and-package-system-built-on-opennmt-pretrained-models/3992/6"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2020-09-29T00:30:28Z",
        "message": "Fixed bug in pivoting"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2020-09-24T02:39:47Z",
        "message": "Added caching layer"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2020-09-24T02:03:20Z",
        "message": "Cleaned up the logic for splitting paragraphs"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2020-09-24T01:23:03Z",
        "message": "Swithed translation.Translation to a better interface system"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2020-09-20T16:47:33Z",
        "message": "Fixed bug with space at the start of every paragraph"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2020-09-20T16:10:47Z",
        "message": "Fixed bug in composite and identity translation"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2020-09-17T23:22:14Z",
        "message": "Added support for maintaining newlines into translated text"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2020-09-13T21:11:21Z",
        "message": "Improved documentation\n\n- Fixed this https://github.com/readthedocs/readthedocs.org/issues/2569\nerror in ReadTheDocs"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2020-09-12T18:10:03Z",
        "message": "Removed excess logging"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2020-09-12T17:59:12Z",
        "message": "Refactored translate.py"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2020-09-12T15:39:26Z",
        "message": "Fixed bug in translating with multiple packages installed"
    },
    {
        "repo_url": "github.com/argosopentech/argos-translate",
        "filepath": "argostranslate/translate.py",
        "commit_date": "2020-09-10T01:48:40Z",
        "message": "Improved documentation and some refactoring."
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/tokenization/test_replace_long_tokens.py",
        "commit_date": "2023-11-12T23:29:49Z",
        "message": "Update the base test to be a bit quicker.  Also, test setting the max length with a flag"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/tokenization/test_replace_long_tokens.py",
        "commit_date": "2022-10-11T04:16:18Z",
        "message": "Check for and replace excessively long tokens with \"UNK\"; addresses issue 1137 (#1140)\n\nCheck for and replace excessively long tokens with \"UNK\"; addresses issue 1137.\n\nAdded simple test for tokenizer maxlen check\nmove length check to post tokenization stage"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/classifiers/test_constituency_classifier.py",
        "commit_date": "2023-02-06T20:44:08Z",
        "message": "Add a test that a fake constituency classifier can be loaded into the pipeline and run on a sentence"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/classifiers/test_constituency_classifier.py",
        "commit_date": "2023-02-06T20:44:08Z",
        "message": "Add an option to the constituency classifier which uses all words instead of just first/last\n\nThe classifier itself uses a max() to pick just one dim to pay attention to"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/classifiers/test_constituency_classifier.py",
        "commit_date": "2023-02-06T20:44:08Z",
        "message": "Learn classifier layers on top of a constituency parser as a new classifier model.\n\nIncludes a flag to backprop into the constituency parser's weights.\nMakes the classifier much more accurate (and slower to train)\n\nAlso test saving & loading a model\nAdd a basic test of the forward pass for the constituency classifier.\n\nIn the constituency classifier, extract one layer down instead of ROOT as an option\nAlso, run 1 head attn over all the nodes as an option\n\nTreeEmbedding is separate from ConstituencyClassifier\n\nThis allows reusing the tree embedding as an input to a constituency\nclassifier (so that a secondary model can give embeddings to the\nprimary model) or elsewhere as an embedding over words / sentences"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_pipeline_depparse_processor.py",
        "commit_date": "2022-09-13T23:26:06Z",
        "message": "Try to reduce the scope on various pipelines to make the test suite less likely to run out of GPU memory.  Not sure this is the correct approach"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_pipeline_depparse_processor.py",
        "commit_date": "2022-06-28T16:59:08Z",
        "message": "Add a method to extract the known relations from a dependency parser in a pipeline.  This was already possible using the vocab, but this is significantly easier for a user to find.  #1066"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_pipeline_sentiment_processor.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Move a couple tests into their own pipeline directory"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_pipeline_constituency_processor.py",
        "commit_date": "2023-01-02T02:50:45Z",
        "message": "Add a constituency comment to sentences in the doc\n\nThis adds the constituency tree to the CoNLL format as a comment\n\nIf there's somehow a newline in the parse tree, we convert \\n to *NL*\nto not make the CoNLL comments horribly broken - not sure this is\nideal, but hopefully \\n doesn't come up often"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_pipeline_constituency_processor.py",
        "commit_date": "2022-09-17T19:33:13Z",
        "message": "Maybe save a little GPU memory (fragmentation, at least) by reusing some large data structures"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_pipeline_constituency_processor.py",
        "commit_date": "2022-09-08T20:06:30Z",
        "message": "Add a method to get the constituents known by a conparser, as requested in #1066"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_pipeline_constituency_processor.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Order the items in a constituency query correctly.  Fixes #882"
    },
    {
        "repo_url": "github.com/XiangLi1999/Diffusion-LM",
        "filepath": "improved-diffusion/control_gen/eval_control.py",
        "commit_date": "2022-06-06T22:38:39Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/facebookresearch/minihack",
        "filepath": "minihack/wiki.py",
        "commit_date": "2021-09-27T23:37:57Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/datasets.py",
        "commit_date": "2021-01-12T00:08:22Z",
        "message": "Add method for generating words only"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/datasets.py",
        "commit_date": "2020-05-18T00:00:17Z",
        "message": "More UD"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/datasets.py",
        "commit_date": "2020-05-16T19:09:16Z",
        "message": "Urban dictionary prep"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/datasets.py",
        "commit_date": "2020-05-16T03:26:23Z",
        "message": "Clean notebooks, prepare for urban dictionary run"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/datasets.py",
        "commit_date": "2020-05-15T22:13:13Z",
        "message": "Switch cache to 100K words from 20K"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/datasets.py",
        "commit_date": "2020-05-13T04:46:11Z",
        "message": "Fix full-width CSS, twitter card, filter definitions < 3 words, add favicons"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/datasets.py",
        "commit_date": "2020-05-13T00:55:36Z",
        "message": "Fix twitter bot"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/datasets.py",
        "commit_date": "2020-05-09T07:25:23Z",
        "message": "Word generation: add a hail mary when failing, only do one pass"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/datasets.py",
        "commit_date": "2020-05-07T20:48:32Z",
        "message": "Wire up basic word service, move word generator interface to own file"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/datasets.py",
        "commit_date": "2020-05-05T18:10:49Z",
        "message": "Use custom inference in twitter bot"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/datasets.py",
        "commit_date": "2020-05-05T17:52:32Z",
        "message": "Get rid of example expansion"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/datasets.py",
        "commit_date": "2020-05-05T07:52:29Z",
        "message": "Add custom modeling utils with early exit for blacklist / end of sentence"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/datasets.py",
        "commit_date": "2020-05-05T05:28:41Z",
        "message": "Upgrade CSS"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/datasets.py",
        "commit_date": "2020-05-04T05:42:41Z",
        "message": "Add twitter card to website, move to training script with creativity evaluation"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/datasets.py",
        "commit_date": "2020-05-03T07:09:26Z",
        "message": "Move directories, gitignore, dockerize twitter bot"
    },
    {
        "repo_url": "github.com/AlibabaResearch/DAMO-ConvAI",
        "filepath": "s2sql/preprocess/common_utils.py",
        "commit_date": "2022-10-18T07:39:05Z",
        "message": "add: s2sql"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2023-01-17T10:00:30Z",
        "message": "fix pretokenized input format\n\nNow, a list of tokens is not valid input anymore"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2022-04-04T12:14:58Z",
        "message": "make style"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2022-04-04T12:11:49Z",
        "message": "add create_spacy_disable_sentence_segmentation as entrypoint"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2022-04-04T12:03:37Z",
        "message": "move merge_dicts_strict to utils"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2022-03-05T19:57:36Z",
        "message": "Allow disable_sbd for stanza"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2022-03-05T16:26:10Z",
        "message": "Allow users to disable spacy components and fix disable_sbd issue\n\nFixes #17"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2021-09-05T08:52:39Z",
        "message": "ignore import quality checks"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2021-06-29T13:48:38Z",
        "message": "avoid duplicate arguments: pop"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2021-06-29T13:11:02Z",
        "message": "make style"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2021-06-29T11:23:52Z",
        "message": "Update utils.py"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2021-06-29T10:04:58Z",
        "message": "remove default spacy parser"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2021-06-25T13:58:50Z",
        "message": "move parsing code to dataclass"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2021-06-23T16:04:23Z",
        "message": "auto-download udpipe models"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2021-06-23T15:41:12Z",
        "message": "update to spacy v3 pipeline"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2021-06-23T09:29:25Z",
        "message": "make style"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2020-05-11T16:47:25Z",
        "message": "make style && make quality"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2020-05-11T16:44:28Z",
        "message": "add ability to use is_tokenized for udpipe"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2020-05-05T16:48:44Z",
        "message": "docstring & typing"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2020-05-05T16:36:09Z",
        "message": "replace tokens_from_list by custom tokenizer\n\nSince tokens_from_list is deprecated, switch to a custom tokenizer when `is_tokenized` is requested. Input can be a string to be split on whitespace or a list of tokens. Works only for spaCy. Inspired by adrianeboyd https://github.com/explosion/spaCy/issues/5399#issuecomment-623593208"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2020-05-03T15:39:12Z",
        "message": "style and quality"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2020-05-03T15:15:14Z",
        "message": "rename pipeline_opts to parser_opts"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2020-05-01T16:51:59Z",
        "message": "allow pipeline_opts and kwargs in init_parser"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2020-05-01T13:22:57Z",
        "message": "require keywords for less important kwargs"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2020-04-30T12:26:27Z",
        "message": "add kwargs"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2020-04-30T11:12:16Z",
        "message": "re-order arguments in init_nlp"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2020-04-29T13:13:32Z",
        "message": "restructure and improve entrypoint\n\n- move ConllFormatter to separate file\n- add utils and expose init_parser (might be useful for users)\n- improve entrypoint usage"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2024-01-13T02:59:55Z",
        "message": "Settings: Settings - Part-of-speeach Tagging - Tagsets - Mapping Settings - Allow editing of tagset mapping of Stanza's Armenian (Eastern), Armenian (Western), Basque, Buryat (Russia), Danish, French, Greek (Modern), Hebrew (Modern), Hungarian, Ligurian, Manx, Marathi, Nigerian Pidgin, Pomak, Portuguese, Russian, Sanskrit, Sindhi, Sorbian (Upper), and Telugu part-of-speech taggers"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2024-01-05T07:57:03Z",
        "message": "Misc: Update copyright year"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2024-01-02T12:47:42Z",
        "message": "Dependencies: Add VADER; Utils: Add VADER's sentiment analyzers"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-12-30T10:49:39Z",
        "message": "Dependencies: Upgrade Stanza to 1.7.0; Utils: Add Stanza's Sindhi part-of-speech tagger"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-10-02T14:14:35Z",
        "message": "Dependencies: Upgrade spaCy to 3.7.2; Utils: Fix downloading of Stanza models"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-09-27T10:45:09Z",
        "message": "Dependencies: Upgrade PyInstaller to 6.0"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-09-23T11:43:01Z",
        "message": "Utils: Update spaCy's sentencizer"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-09-10T09:30:26Z",
        "message": "Utils: Add Stanza's French (Old) lemmatizer"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-09-10T08:49:05Z",
        "message": "Utils: Remove Stanza's Swedish Sign Language sentence tokenizer, word tokenizer, part-of-speech tagger, and dependency parser"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-09-10T08:43:01Z",
        "message": "Dependencies: Upgrade Stanza to 1.5.1; Utils: Add Stanza's Hebrew (Ancient), Kyrgyz, Manx, and Pomak sentence tokenizers / word tokenizers / part-of-speech taggers / lemmatizers / dependency parsers"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-09-04T13:45:52Z",
        "message": "Utils: Add Stanza's sentiment analyzers"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-09-03T18:08:00Z",
        "message": "Dependencies: Add Stanza; Utils: Add Stanza's sentence tokenizers, word tokenizers, part-of-speech taggers, lemmatizers, and dependency parsers"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-08-17T17:46:36Z",
        "message": "Misc: Update translations"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-08-11T16:36:09Z",
        "message": "Dependencies: Add Dostoevsky; Settings: Add Settings - Sentiment Analysis; Utils: Add Dostoevsky's Russian sentiment analyzer"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-08-08T13:51:22Z",
        "message": "Utils: Update spaCy's sentence recognizers, word tokenizers, part-of-speech taggers, lemmatizers, and dependency parsers"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-07-23T06:44:18Z",
        "message": "Utils: Add spaCy's Slovenian sentence recognizer, part-of-speech tagger, lemmatizer, and dependency parser"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-07-22T17:31:56Z",
        "message": "Utils: Add spaCy's Korean sentence recognizer, word tokenizer, part-of-speech tagger, lemmatizer, and dependency parser"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-03-13T19:09:27Z",
        "message": "Dependencies: Add python-mecab-ko; Utils: Add python-mecab-ko's MeCab"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-02-10T13:24:56Z",
        "message": "Releases: Update packaging script"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-02-08T15:13:01Z",
        "message": "Dependencies: 1. Add pymorphy3 2. Remove pymorphy2"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-02-02T06:42:01Z",
        "message": "Settings: Update global settings - measures"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-01-30T16:05:14Z",
        "message": "Utils: Speed up n-gram/skip-gram generation"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-01-19T16:15:38Z",
        "message": "Misc: Update copyright year"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-10-30T15:50:33Z",
        "message": "Work Area: Add Dependency Parser"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-10-09T20:25:53Z",
        "message": "Utils: Update spaCy's sentence tokenizers, word tokenizers, part-of-speech taggers, and lemmatizers"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-10-06T19:35:05Z",
        "message": "Utils: Update spaCy's sentence recognizers"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-09-23T19:51:17Z",
        "message": "Releases: Update packaging script"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-09-20T19:50:34Z",
        "message": "Utils: Add spaCy's Ukrainian part-of-speech tagger and lemmatizer"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-09-04T13:36:51Z",
        "message": "Utils: Add spaCy's Finnish part-of-speech tagger and lemmatizer"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-09-03T18:31:20Z",
        "message": "Utils: Add spaCy's Croatian and Swedish part-of-speech taggers"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-09-03T14:08:35Z",
        "message": "Utils: 1. Add spaCy's Sorbian (Lower) word tokenizer and stop word list 2. Add spaCy's Sorbian (Upper) word tokenizer and stop word list"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-09-02T18:45:32Z",
        "message": "Utils: Add Pyphen's Catalan syllable tokenizer"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-08-25T16:55:28Z",
        "message": "Utils: Add NLTK's legality syllable tokenizer and sonority sequencing syllable tokenizer"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-08-25T09:29:24Z",
        "message": "Tests: Add CI - SonarCloud"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-08-23T08:58:22Z",
        "message": "Utils: Add NLTK's regular-expression tokenizer"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-08-22T10:35:55Z",
        "message": "Dependencies: 1. Add spacy-pkuseg 2. Remove pkuseg"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-08-21T14:09:14Z",
        "message": "Work Area: Add Profiler - Count of Sentence Segments / Paragraph Length in Sentence Segments / Sentence Segment Length in Tokens / Count of n-length Sentence Segments"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-08-11T20:10:54Z",
        "message": "Doc: Add README - Pylint badge"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-08-10T19:50:25Z",
        "message": "Tests: Relocate test files"
    },
    {
        "repo_url": "github.com/stanford-oval/WikiChat",
        "filepath": "wikiextractor/split_passages.py",
        "commit_date": "2023-12-06T03:19:45Z",
        "message": "Add code and instructions"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2022-04-30T07:16:44Z",
        "message": "WIP"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2022-01-22T21:11:59Z",
        "message": "Various improvements, clients and clients docs."
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2021-11-19T17:33:01Z",
        "message": "Updated pythondocs, clean up code"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2021-09-12T18:30:33Z",
        "message": "Fix #130"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2021-06-26T08:44:58Z",
        "message": "Better stanza token conversion"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2021-06-26T08:37:19Z",
        "message": "Working AnnStanza.pipe"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2021-06-26T07:31:08Z",
        "message": "Require newer Stanza, test for Stanza pipe"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2021-03-19T07:52:03Z",
        "message": "Fix some code style issues"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2021-03-17T08:04:55Z",
        "message": "Address #69"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2021-02-19T17:58:08Z",
        "message": "Add support for space tokens."
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2021-02-19T17:28:43Z",
        "message": "Proper MWTs."
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2021-02-19T16:55:34Z",
        "message": "Better squeezing of multiword tokens"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2021-02-16T16:28:25Z",
        "message": "Better error handling in the executor"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2020-11-22T08:33:27Z",
        "message": "black reformatting"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2020-11-18T19:19:53Z",
        "message": "WIP"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2020-11-13T21:06:43Z",
        "message": "Require min version for bs4.\nOtherwise the suppression of the parser warning does not work."
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2020-10-17T11:21:32Z",
        "message": "Use pdoc3 for documentation."
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2020-10-14T20:16:50Z",
        "message": "WIP"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2020-10-14T19:01:58Z",
        "message": "WIP"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2020-08-20T12:07:12Z",
        "message": "Fix stanza lib problem.\nOddly something seems to have changed in how stanza represents ids.\nApparently this was string before and is int now. Just to be safe,\nwe always convert to string for now."
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2020-08-19T13:26:37Z",
        "message": "A few more for the changed annset.add method."
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2020-08-19T09:35:27Z",
        "message": "More API changes."
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2020-08-18T17:50:26Z",
        "message": "Re-implementation of Features, updated documentation."
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2020-04-29T17:16:07Z",
        "message": "Add support for Stanford Stanza"
    },
    {
        "repo_url": "github.com/AlibabaResearch/DAMO-ConvAI",
        "filepath": "proton/preprocess/common_utils.py",
        "commit_date": "2022-08-06T11:43:17Z",
        "message": "add: proton"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2022-02-06T11:04:17Z",
        "message": "Fixes #158"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2021-06-26T08:37:19Z",
        "message": "Working AnnStanza.pipe"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2021-06-26T07:31:08Z",
        "message": "Require newer Stanza, test for Stanza pipe"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2021-04-14T11:33:20Z",
        "message": "Stanza unit test: do not use GPU"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2021-04-14T11:26:38Z",
        "message": "Improve unit tests.\nAdd basic test for GateWorker"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2021-04-10T07:21:43Z",
        "message": "Cleanup code."
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2021-03-19T07:52:03Z",
        "message": "Fix some code style issues"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2021-02-06T17:27:45Z",
        "message": "Add unit tests, fix bugs, regenerate PythonDoc."
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2021-01-30T13:14:19Z",
        "message": "And yet another!"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2021-01-30T13:11:41Z",
        "message": "And yet another"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2020-11-22T08:33:27Z",
        "message": "black reformatting"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2020-10-27T22:22:25Z",
        "message": "Avoid downloading the model if it is already there."
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2020-10-27T15:58:10Z",
        "message": "Try to make the test work on travis"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2020-10-27T14:01:19Z",
        "message": "Fix bug in setup, remove stanfordnlp.\nAdd instead a test for stanza"
    },
    {
        "repo_url": "github.com/AlibabaResearch/DAMO-ConvAI",
        "filepath": "sunsql/preprocess/common_utils.py",
        "commit_date": "2022-09-19T01:42:32Z",
        "message": "add: sunsql"
    },
    {
        "repo_url": "github.com/XiangLi1999/Diffusion-LM",
        "filepath": "transformers/examples/pytorch/language-modeling/run_clm.py",
        "commit_date": "2022-08-25T22:09:55Z",
        "message": "Update run_clm.py"
    },
    {
        "repo_url": "github.com/XiangLi1999/Diffusion-LM",
        "filepath": "transformers/examples/pytorch/language-modeling/run_clm.py",
        "commit_date": "2022-06-06T22:38:39Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/marian-nmt/marian-examples",
        "filepath": "forced-translation/scripts/lemmatize.py",
        "commit_date": "2022-02-23T19:00:33Z",
        "message": "Forced translation\n\n* add forced-translation examples\n* Update .gitignore\n* Update README.md\n* remove glossary tokenization\n* add eval scripts\n* Update EXPERIMENTS.md\n\n    Added information regarding the two new testsets used for en-ro and\n    en-nb, that contained a specific domained, annotated with a glossary\n    with terms only specific to that domain to mimic better the Tilde's\n    ATS testset and glossary, used for en-lv and en-de. Also added the\n    human evaluation results for this two LPs.\n\n* Add time estimations to run end-2-end pipeline\n\nCo-authored-by: Pedro Coelho <pedro.coelho@unbabel.com>\nCo-authored-by: Toms Bergmanis <tomsbergmanis@gmail.com>"
    },
    {
        "repo_url": "github.com/marian-nmt/marian-examples",
        "filepath": "forced-translation/scripts/eval_lemmatized_glossary.py",
        "commit_date": "2022-02-23T19:00:33Z",
        "message": "Forced translation\n\n* add forced-translation examples\n* Update .gitignore\n* Update README.md\n* remove glossary tokenization\n* add eval scripts\n* Update EXPERIMENTS.md\n\n    Added information regarding the two new testsets used for en-ro and\n    en-nb, that contained a specific domained, annotated with a glossary\n    with terms only specific to that domain to mimic better the Tilde's\n    ATS testset and glossary, used for en-lv and en-de. Also added the\n    human evaluation results for this two LPs.\n\n* Add time estimations to run end-2-end pipeline\n\nCo-authored-by: Pedro Coelho <pedro.coelho@unbabel.com>\nCo-authored-by: Toms Bergmanis <tomsbergmanis@gmail.com>"
    },
    {
        "repo_url": "github.com/marian-nmt/marian-examples",
        "filepath": "forced-translation/scripts/add_target_lemma_annotations.py",
        "commit_date": "2022-02-23T19:00:33Z",
        "message": "Forced translation\n\n* add forced-translation examples\n* Update .gitignore\n* Update README.md\n* remove glossary tokenization\n* add eval scripts\n* Update EXPERIMENTS.md\n\n    Added information regarding the two new testsets used for en-ro and\n    en-nb, that contained a specific domained, annotated with a glossary\n    with terms only specific to that domain to mimic better the Tilde's\n    ATS testset and glossary, used for en-lv and en-de. Also added the\n    human evaluation results for this two LPs.\n\n* Add time estimations to run end-2-end pipeline\n\nCo-authored-by: Pedro Coelho <pedro.coelho@unbabel.com>\nCo-authored-by: Toms Bergmanis <tomsbergmanis@gmail.com>"
    },
    {
        "repo_url": "github.com/dh-miami/narratives_covid19",
        "filepath": "scripts/topic_modelling/topic_modelling.py",
        "commit_date": "2020-08-31T22:08:37Z",
        "message": "add NER and parallelization"
    },
    {
        "repo_url": "github.com/dh-miami/narratives_covid19",
        "filepath": "scripts/topic_modelling/topic_modelling.py",
        "commit_date": "2020-08-04T02:21:44Z",
        "message": "topic modelling update"
    },
    {
        "repo_url": "github.com/rali-udem/jsRealB",
        "filepath": "demos/UDregenerator/text2ud.py",
        "commit_date": "2022-10-29T20:17:57Z",
        "message": "text2ud: show progress during Stanza parsing"
    },
    {
        "repo_url": "github.com/rali-udem/jsRealB",
        "filepath": "demos/UDregenerator/text2ud.py",
        "commit_date": "2022-09-12T18:11:25Z",
        "message": "Version 4.5 - Code reorganization\n\nNo new features but the implementation now use \"real\" JavaScript classes and modules.\n- building relies on webpack which integrates JSON lexicons in the source. This simplifies greatly the integration process that previously used a series of scripts with a Makefile\n- demos have been ported to this organization\n- documentation has been updated\n- source files now have jsdoc comments\n- src/jsdoc is now available"
    },
    {
        "repo_url": "github.com/rali-udem/jsRealB",
        "filepath": "demos/UDregenerator/text2ud.py",
        "commit_date": "2021-05-19T01:02:10Z",
        "message": "Add variationsFromText to UDgenerator"
    },
    {
        "repo_url": "github.com/weixi-feng/Structured-Diffusion-Guidance",
        "filepath": "scripts/txt2img_demo.py",
        "commit_date": "2022-12-09T19:40:58Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/rali-udem/jsRealB",
        "filepath": "demos/Bilinguo/makeBilinguoData.py",
        "commit_date": "2023-06-03T19:29:14Z",
        "message": "Many improvements to the Bilinguo demo\n\n- improve generation from Python\n- fixes to conjugation table number for some verbs in lexicon-en.json"
    },
    {
        "repo_url": "github.com/rali-udem/jsRealB",
        "filepath": "demos/Bilinguo/makeBilinguoData.py",
        "commit_date": "2023-05-29T21:09:17Z",
        "message": "Bilinguo: add Python generator + README.md"
    },
    {
        "repo_url": "github.com/rali-udem/jsRealB",
        "filepath": "demos/UDregenerator/text2ud-new.py",
        "commit_date": "2023-05-17T20:58:21Z",
        "message": "add showTree for variationsFromText"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2023-05-10T10:53:55Z",
        "message": "Use the dict merge operation supported by 3.8"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2023-05-10T10:53:55Z",
        "message": "Fix potential bugs of `add_tokens`"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2023-05-10T10:53:55Z",
        "message": "Support vocab extension"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2023-05-10T10:53:55Z",
        "message": "Strip tokenized tokens in extended vocab"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2023-05-10T10:53:55Z",
        "message": "Consider added tokens as well"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-10-20T13:19:18Z",
        "message": "Add `tokens` property"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-10-05T12:56:26Z",
        "message": "Syntax sugars for dist training"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-09-27T12:22:06Z",
        "message": "Handle tok with `ByteLevel` pre_tokenizer properly"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-07-30T13:47:07Z",
        "message": "Try loading local transformers files first"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-07-30T13:47:07Z",
        "message": "Handle unk case for `TransformerTokenizer`"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-07-30T13:47:07Z",
        "message": "Skip special tokens and keep spaces while decoding"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-07-30T13:47:07Z",
        "message": "Fix potential bugs"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-07-30T13:47:07Z",
        "message": "Fix vocab bug within BPE decoding"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-07-30T13:47:07Z",
        "message": "Support subword-nmt backend"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-07-30T13:47:07Z",
        "message": "Provide `min_freq` for BPE Tokenizer"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-07-30T13:46:55Z",
        "message": "More usable BPE tokenizers"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-07-01T07:09:51Z",
        "message": "Specify word suffix for BPE Trainer"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-06-25T06:29:43Z",
        "message": "Wrapper for BPE Tokenizer"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-06-24T09:52:29Z",
        "message": "Syntactic sugar for Transformer tokenizers"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-04-01T05:39:07Z",
        "message": "Minor revisions"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-04-01T05:39:07Z",
        "message": "Integrated tokenizer (#47)"
    },
    {
        "repo_url": "github.com/jim-schwoebel/allie",
        "filepath": "features/text_features/helpers/blabla/blabla/document_processor.py",
        "commit_date": "2020-12-03T03:04:48Z",
        "message": "added allosaurus, surfboard, and blabla featurizers"
    },
    {
        "repo_url": "github.com/zhijing-jin/ARTS_TestSet",
        "filepath": "code/utils.py",
        "commit_date": "2020-06-14T20:06:18Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/ServiceNow/duorat",
        "filepath": "duorat/utils/tokenization.py",
        "commit_date": "2020-10-23T01:48:17Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/zzsfornlp/zmsp",
        "filepath": "msp2/scripts/wiki/raw2tok.py",
        "commit_date": "2022-10-12T18:51:22Z",
        "message": "Updates"
    },
    {
        "repo_url": "github.com/zzsfornlp/zmsp",
        "filepath": "mspx/tools/annotate/ann_stanza.py",
        "commit_date": "2023-06-20T01:13:08Z",
        "message": "Add some more."
    },
    {
        "repo_url": "github.com/zzsfornlp/zmsp",
        "filepath": "msp2/tools/annotate/ann_stanza.py",
        "commit_date": "2022-10-12T18:51:22Z",
        "message": "Updates"
    },
    {
        "repo_url": "github.com/zzsfornlp/zmsp",
        "filepath": "msp2/tools/annotate/ann_stanza.py",
        "commit_date": "2021-06-06T20:35:18Z",
        "message": "Update for srl_span with msp2."
    },
    {
        "repo_url": "github.com/zzsfornlp/zmsp",
        "filepath": "msp2/scripts/event/prep/s2_tokenize.py",
        "commit_date": "2022-10-12T18:51:22Z",
        "message": "Updates"
    },
    {
        "repo_url": "github.com/zzsfornlp/zmsp",
        "filepath": "msp2/tasks/zmtl3/mat/prep/brat2json.py",
        "commit_date": "2022-10-12T18:51:22Z",
        "message": "Updates"
    },
    {
        "repo_url": "github.com/zzsfornlp/zmsp",
        "filepath": "msp2/tasks/zmtl3/scripts/misc/query_gpt3.py",
        "commit_date": "2022-10-12T18:51:22Z",
        "message": "Updates"
    },
    {
        "repo_url": "github.com/zzsfornlp/zmsp",
        "filepath": "msp2/tasks/zmtl3/scripts/srl/sz_read_c09zhf.py",
        "commit_date": "2022-10-12T18:51:22Z",
        "message": "Updates"
    },
    {
        "repo_url": "github.com/zzsfornlp/zmsp",
        "filepath": "msp2/tasks/zmtl3/scripts/srl/sz_collect_onto.py",
        "commit_date": "2022-10-12T18:51:22Z",
        "message": "Updates"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/word_generator.py",
        "commit_date": "2020-05-17T04:13:56Z",
        "message": "UD in word service, bug fixes"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/word_generator.py",
        "commit_date": "2020-05-12T09:00:07Z",
        "message": "Add word already exists warning"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/word_generator.py",
        "commit_date": "2020-05-09T07:52:13Z",
        "message": "even better hail maries"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/word_generator.py",
        "commit_date": "2020-05-09T07:36:44Z",
        "message": "Bug fixzzz"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/word_generator.py",
        "commit_date": "2020-05-09T07:25:23Z",
        "message": "Word generation: add a hail mary when failing, only do one pass"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/word_generator.py",
        "commit_date": "2020-05-08T23:29:03Z",
        "message": "Webpack, website that queries backend"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/word_generator.py",
        "commit_date": "2020-05-07T20:48:32Z",
        "message": "Wire up basic word service, move word generator interface to own file"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2023-09-25T04:56:14Z",
        "message": "for backslash-escape"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2023-09-24T13:36:07Z",
        "message": "revert"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2023-09-24T12:06:15Z",
        "message": "https://github.com/KoichiYasuoka/UniDic2UD/issues/2"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2023-04-17T15:55:15Z",
        "message": "\u8fd1\u73fe\u4ee3\u53e3\u8a9e\u5c0f\u8aacUniDic"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2023-04-17T14:54:13Z",
        "message": "cwj-3.1.1 and csj-3.1.1"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2022-04-24T13:52:06Z",
        "message": "use UniDic-202203"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2022-04-24T10:33:27Z",
        "message": "ChamameWebAPI changed"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2022-04-15T14:55:30Z",
        "message": "URL update"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2021-11-07T04:59:15Z",
        "message": "URL changed"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2021-04-04T00:59:04Z",
        "message": "force download UniDic"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2021-04-03T06:37:59Z",
        "message": "gendai and spoken URLs changed"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2021-03-06T14:27:15Z",
        "message": "bug fix"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2021-02-21T08:58:06Z",
        "message": "Chamame moved to chamame.ninjal.ac.jp"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2021-02-20T14:05:29Z",
        "message": "Stanza==1.2 support"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2021-02-04T03:48:40Z",
        "message": "unidic-lite support"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2021-02-01T15:12:07Z",
        "message": "progressbar outputs sys.stderr"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2021-01-25T05:03:26Z",
        "message": "flush=True"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2021-01-23T13:54:49Z",
        "message": "AUX improved"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-12-28T03:38:53Z",
        "message": "ipadic support"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-11-23T10:20:15Z",
        "message": "bug fix"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-09-21T06:35:26Z",
        "message": "bug fix"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-09-19T05:06:35Z",
        "message": "use deplacy.PACKAGE_DIR"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-08-12T12:49:54Z",
        "message": "\u5f62\u5bb9\u8a5e-\u975e\u81ea\u7acb\u53ef\u80fd improved"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-08-12T08:46:24Z",
        "message": "\u52d5\u8a5e-\u975e\u81ea\u7acb\u53ef\u80fd improved"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-07-29T05:32:35Z",
        "message": "stanza_ja support"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-07-25T15:59:49Z",
        "message": "\u540d\u8a5e-\u52a9\u52d5\u8a5e\u8a9e\u5e79 support"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-07-08T05:39:49Z",
        "message": "unidic2ud.deprelja -> deplacy.deprelja"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-07-07T12:58:46Z",
        "message": "mecab-python3 1.0 support"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-07-02T01:46:35Z",
        "message": "to_tree(CatenaAnalysis) support"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-06-25T03:09:00Z",
        "message": "to_tree() uses deplacy"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-04-12T06:47:16Z",
        "message": "progress bar improved"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-03-31T04:30:21Z",
        "message": "to_tree() changed into BOX DRAWINGS DOUBLE"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-03-26T01:18:01Z",
        "message": "version 2.0.0"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-03-26T01:09:31Z",
        "message": "version 1.9.9"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-03-21T01:14:07Z",
        "message": "version 1.9.8"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-03-20T16:18:43Z",
        "message": "version 1.9.7"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-02-10T06:13:01Z",
        "message": "version 1.9.3"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-02-09T07:13:24Z",
        "message": "version 1.9.2"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-02-08T13:12:18Z",
        "message": "version 1.9.0"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-02-08T05:50:13Z",
        "message": "version 1.8.9"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-01-17T05:20:21Z",
        "message": "version 1.8.5"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-01-14T16:43:47Z",
        "message": "ipadic deleted from distribution"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-01-14T15:15:03Z",
        "message": "japanese-modern supported"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-01-13T06:39:34Z",
        "message": "unidic-csj-3.0.1 supported"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-01-03T00:42:30Z",
        "message": "version 1.7.6"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-12-29T01:48:04Z",
        "message": "version 1.7.3"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-12-27T08:44:07Z",
        "message": "version 1.6.8"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-12-27T06:35:22Z",
        "message": "fugashi.GenericTagger supported"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-12-26T14:15:50Z",
        "message": "version 1.6.6"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-12-20T10:32:53Z",
        "message": "version 1.6.2"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-12-18T07:40:06Z",
        "message": "version 1.5.9"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-12-14T23:51:39Z",
        "message": "version 1.5.8"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-12-10T06:46:38Z",
        "message": "version 1.5.3"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-11-22T05:42:52Z",
        "message": "to_tree(Japanese) supported"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-11-21T14:31:08Z",
        "message": "version 1.4.7"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-11-14T02:32:09Z",
        "message": "to_tree() released"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-11-13T03:00:10Z",
        "message": "\u6fc1\u70b9/\u534a\u6fc1\u70b9 process"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-11-10T14:47:36Z",
        "message": "PUNCT for ipadic"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-10-20T03:42:30Z",
        "message": "dictlist supported"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-09-15T07:00:41Z",
        "message": "version 1.1.4"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-09-15T06:38:16Z",
        "message": "to_svg(item) enhanced"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-09-14T23:20:56Z",
        "message": "to_svg() supported"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-31T23:12:40Z",
        "message": "bug fix for \u30d5\u30a3\u30e9\u30fc of ipadic"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-31T23:00:16Z",
        "message": "bug fix for Translit"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-31T22:38:28Z",
        "message": "bug fix for U+3000"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-29T04:18:55Z",
        "message": "unidic2ud.spacy supported"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-28T08:39:07Z",
        "message": "version 0.9.8 raw option supported"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-27T09:39:00Z",
        "message": "index for UDPipeEntry supported"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-27T05:49:26Z",
        "message": "bug fix for Translit of \"spoken\""
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-27T05:06:43Z",
        "message": "bug fix for Translit of \"gendai\""
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-27T04:50:26Z",
        "message": "bug fix for \"gendai\""
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-27T02:20:01Z",
        "message": "UDPipe models renamed"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-27T01:56:01Z",
        "message": "bug fix"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-26T22:22:32Z",
        "message": "UniDic=None as default"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-26T16:10:59Z",
        "message": "bug fix"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-26T15:45:01Z",
        "message": "initial release"
    },
    {
        "repo_url": "github.com/zhjohnchan/PTUnifier",
        "filepath": "ptunifier/metrics/jb_scorers/RadEntityMatchExact/RadEntityMatchExact.py",
        "commit_date": "2023-08-10T06:54:38Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/models/hybrid_qg/path_sampler.py",
        "commit_date": "2023-01-17T09:26:50Z",
        "message": "adding more comments and documentation"
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/models/hybrid_qg/path_sampler.py",
        "commit_date": "2022-09-26T08:40:56Z",
        "message": "adding a new dir for path sampling scripts"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2022-10-18T14:04:19Z",
        "message": "remove flair modules"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2022-10-17T20:05:39Z",
        "message": "README"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2022-10-06T20:35:23Z",
        "message": "fixes for keeping \\n in text"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2022-09-30T17:12:48Z",
        "message": "add binyan lookup for any VERB without binyan prediction"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2022-09-10T02:39:05Z",
        "message": "Merge branch 'dev'\n\nsync"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2022-09-08T05:11:05Z",
        "message": "bugfix"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2022-09-08T05:02:50Z",
        "message": "fix formatting indentation"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2022-09-08T04:45:36Z",
        "message": "integrated with the pipeline"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2022-08-24T17:43:04Z",
        "message": "update models URL"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2022-08-19T05:32:46Z",
        "message": "mtl with feats"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2022-08-16T18:02:56Z",
        "message": "integrated feats"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2022-08-15T18:24:17Z",
        "message": "feats"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2022-08-08T15:42:15Z",
        "message": "sbd plus pos. branching"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2022-08-03T19:04:06Z",
        "message": "bugfix"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2022-08-03T18:28:13Z",
        "message": "final results; integration pending"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2022-07-28T20:58:53Z",
        "message": "v1 pipeline"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2021-09-03T18:44:24Z",
        "message": "support Windows cp1255 Hebrew encoding"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2021-09-03T18:43:31Z",
        "message": "binyan postprocessing"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2021-07-29T20:02:06Z",
        "message": "add neural morphology+lemmatization"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2021-07-12T13:51:03Z",
        "message": "add neural POS tagger"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2021-06-14T14:46:53Z",
        "message": "hardwire unicode punctuation\n  * some punctuation glyphs are not found in training data\n  * hardwire tagging as punctuation"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2021-06-14T14:41:18Z",
        "message": "manage GPU memory for large single files"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2021-06-14T14:37:15Z",
        "message": "add `--from_pipes` option\n  * Allows input from text file with already segmented subwords\n  * Useful for segmenting data first, then manually correcting segmentation, then continuing with rest of NLP"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2021-05-26T17:43:33Z",
        "message": "add neural sentence splitter"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2021-05-26T17:41:17Z",
        "message": "add python neural parser"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2021-05-26T17:39:46Z",
        "message": "add plain text preservation"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2020-05-26T16:26:21Z",
        "message": "Allow relative import"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2020-05-26T16:18:31Z",
        "message": "Make module"
    },
    {
        "repo_url": "github.com/amir-zeldes/HebPipe",
        "filepath": "hebpipe/heb_pipe.py",
        "commit_date": "2020-05-26T15:55:08Z",
        "message": "restructure for pypi"
    },
    {
        "repo_url": "github.com/hoochanlon/scripts",
        "filepath": "d-python/get_resou_today_s.py",
        "commit_date": "2023-06-22T09:26:12Z",
        "message": "yes"
    },
    {
        "repo_url": "github.com/hoochanlon/scripts",
        "filepath": "d-python/get_resou_today_s.py",
        "commit_date": "2023-05-14T14:49:00Z",
        "message": "Update get_resou_today_s.py"
    },
    {
        "repo_url": "github.com/hoochanlon/scripts",
        "filepath": "d-python/get_resou_today_s.py",
        "commit_date": "2023-05-14T11:45:45Z",
        "message": "Update get_resou_today_s.py"
    },
    {
        "repo_url": "github.com/hoochanlon/scripts",
        "filepath": "d-python/get_resou_today_s.py",
        "commit_date": "2023-05-14T07:53:57Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/models/passage_qg/answer_sampler.py",
        "commit_date": "2022-10-06T12:41:49Z",
        "message": "Rework data loading and processing design\n\nThis change separates data loading from processing so that any dataset\nin the appropriate format can be used for generating questions and to\nallow for simple extension to other modalities.\n\n - There are now separate processors depending on the modality.\n\n - Also, the tests have been updated accordingly."
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/models/passage_qg/answer_sampler.py",
        "commit_date": "2022-07-29T17:56:41Z",
        "message": "include id for passage qg"
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/models/passage_qg/answer_sampler.py",
        "commit_date": "2022-07-06T15:07:45Z",
        "message": "Updated inference notebook for PassageQG"
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/models/passage_qg/answer_sampler.py",
        "commit_date": "2022-07-05T07:09:29Z",
        "message": "PassageQG can  take answers as input"
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/models/passage_qg/answer_sampler.py",
        "commit_date": "2022-07-01T11:40:25Z",
        "message": "Merge branch 'passageqg' into tableqg"
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/models/passage_qg/answer_sampler.py",
        "commit_date": "2022-07-01T08:52:39Z",
        "message": "Stanza added to setup.py"
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/models/passage_qg/answer_sampler.py",
        "commit_date": "2022-06-30T18:47:05Z",
        "message": "minor changes from code review suggestions-2"
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/models/passage_qg/answer_sampler.py",
        "commit_date": "2022-06-30T14:10:22Z",
        "message": "Answer sampler uses NERs  from Stanza: en, ru, ar, fi"
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/models/passage_qg/answer_sampler.py",
        "commit_date": "2022-06-29T10:44:31Z",
        "message": "Inference notebooks for TableQG and TyDi"
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/models/passage_qg/answer_sampler.py",
        "commit_date": "2022-06-26T21:41:12Z",
        "message": "PassageQG inference works. en_core_web_sm downloades with setup"
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/models/passage_qg/answer_sampler.py",
        "commit_date": "2022-06-26T20:51:20Z",
        "message": "inference for passage QG"
    },
    {
        "repo_url": "github.com/nipunsadvilkar/pySBD",
        "filepath": "benchmarks/genia_benchmark.py",
        "commit_date": "2020-07-19T17:25:34Z",
        "message": "\u26a1\ufe0f  Benchmarks scripts - genia & bigtext file"
    },
    {
        "repo_url": "github.com/nipunsadvilkar/pySBD",
        "filepath": "benchmarks/genia_benchmark.py",
        "commit_date": "2020-07-12T17:58:12Z",
        "message": "use format, delete other scripts"
    },
    {
        "repo_url": "github.com/nipunsadvilkar/pySBD",
        "filepath": "benchmarks/genia_benchmark.py",
        "commit_date": "2020-07-11T21:59:57Z",
        "message": "my benchmarks"
    },
    {
        "repo_url": "github.com/openredact/nerwhal",
        "filepath": "nerwhal/nlp_utils.py",
        "commit_date": "2020-08-18T15:39:02Z",
        "message": "Add and improve PyDocs"
    },
    {
        "repo_url": "github.com/openredact/nerwhal",
        "filepath": "nerwhal/nlp_utils.py",
        "commit_date": "2020-08-17T14:22:47Z",
        "message": "Fix caught error"
    },
    {
        "repo_url": "github.com/openredact/nerwhal",
        "filepath": "nerwhal/nlp_utils.py",
        "commit_date": "2020-07-31T06:37:48Z",
        "message": "Catch Exception instead of FileNotFoundError"
    },
    {
        "repo_url": "github.com/openredact/nerwhal",
        "filepath": "nerwhal/nlp_utils.py",
        "commit_date": "2020-07-31T05:30:00Z",
        "message": "Clean tokenizer and nlp interfaces"
    },
    {
        "repo_url": "github.com/openredact/nerwhal",
        "filepath": "nerwhal/nlp_utils.py",
        "commit_date": "2020-07-30T17:05:26Z",
        "message": "Use context words to boost score"
    },
    {
        "repo_url": "github.com/openredact/nerwhal",
        "filepath": "nerwhal/nlp_utils.py",
        "commit_date": "2020-07-29T13:58:20Z",
        "message": "Use spaCy tokenizer because stanza's mwts alter the original text"
    },
    {
        "repo_url": "github.com/openredact/nerwhal",
        "filepath": "nerwhal/nlp_utils.py",
        "commit_date": "2020-07-28T15:37:51Z",
        "message": "Refactor and fix bugs WIP"
    },
    {
        "repo_url": "github.com/consuldemocracy/consuldemocracy",
        "filepath": "public/machine_learning/scripts/budgets_related_content_and_tags_nmf.py",
        "commit_date": "2022-11-11T00:40:04Z",
        "message": "Use a different machine learning folder per tenant\n\nWe're using the \"tenants\" subfolder for consistency with the folder\nstructure we use in ActiveStorage and because some CONSUL installations\nmight have folders inside the `data` folder which might conflict with\nthe folders created by tenants.\n\nNote that the Python scripts have a lot of duplication, meaning we need\nto change all of them. I'm not refactoring them because I'm not familiar\nenough with these scripts (or with Python, for that matter).\n\nAlso note that the scripts folder is still shared by all tenants,\nmeaning it isn't possible to have different scripts for different\ntenants. I'm not sure how this situation should be handled; again, I'm\nnot familiar enough with this feature."
    },
    {
        "repo_url": "github.com/consuldemocracy/consuldemocracy",
        "filepath": "public/machine_learning/scripts/budgets_related_content_and_tags_nmf.py",
        "commit_date": "2021-09-07T16:18:47Z",
        "message": "Update machine learning scripts with NNMF and TextRank-GloVe techniques"
    },
    {
        "repo_url": "github.com/consuldemocracy/consuldemocracy",
        "filepath": "public/machine_learning/scripts/budgets_related_content_and_tags_nmf.py",
        "commit_date": "2021-08-16T14:31:04Z",
        "message": "Add experimental machine learning"
    },
    {
        "repo_url": "github.com/consuldemocracy/consuldemocracy",
        "filepath": "public/machine_learning/scripts/proposals_related_content_and_tags_nmf.py",
        "commit_date": "2022-11-11T00:40:04Z",
        "message": "Use a different machine learning folder per tenant\n\nWe're using the \"tenants\" subfolder for consistency with the folder\nstructure we use in ActiveStorage and because some CONSUL installations\nmight have folders inside the `data` folder which might conflict with\nthe folders created by tenants.\n\nNote that the Python scripts have a lot of duplication, meaning we need\nto change all of them. I'm not refactoring them because I'm not familiar\nenough with these scripts (or with Python, for that matter).\n\nAlso note that the scripts folder is still shared by all tenants,\nmeaning it isn't possible to have different scripts for different\ntenants. I'm not sure how this situation should be handled; again, I'm\nnot familiar enough with this feature."
    },
    {
        "repo_url": "github.com/consuldemocracy/consuldemocracy",
        "filepath": "public/machine_learning/scripts/proposals_related_content_and_tags_nmf.py",
        "commit_date": "2021-09-07T16:18:47Z",
        "message": "Update machine learning scripts with NNMF and TextRank-GloVe techniques"
    },
    {
        "repo_url": "github.com/consuldemocracy/consuldemocracy",
        "filepath": "public/machine_learning/scripts/proposals_related_content_and_tags_nmf.py",
        "commit_date": "2021-08-16T14:31:04Z",
        "message": "Add experimental machine learning"
    },
    {
        "repo_url": "github.com/deeppavlov/dream",
        "filepath": "annotators/bot_emotion_classifier/server.py",
        "commit_date": "2023-11-16T08:59:51Z",
        "message": "Dream emotion dist (#568)\n\n* add dream_emotion dist with annotators\n\n* add component cards\n\n* fix codestyle flake8\n\n* fix codestyle\n\n* another codestyle fix\n\n* PR fixes\n\n* fixed batches\n\n* move preprocessing to annotator\n\n* fix codestyle black\n\n* minor fixes\n\n* removed unnecessary api_configs\n\n* changed type of annotator to bot utts annotator\n\n* fixed component file\n\n* fixed service.yml files\n\n* fixed formatters\n\n* fixed emotional response server and dist files\n\n* moved prompt into separate file\n\n* fixed annotators\n\n* fixed annotators again\n\n* fixed emotion response selector\n\n* fixed emotion dist and components\n\n* fixed codestyle\n\n* fixed codestyle again\n\n* fixed codestyle black\n\n* fixed naming of emotion response selector\n\n* fixed emotional_bot_response component\n\n* fixed emotion response selector server\n\n* added prev_services in annotator in pipeline_conf\n\n* fixed result formatting of bot_emo_cls\n\n* fixed codestyle\n\n* fixed codestyle again\n\n* changed description of emo_bot_response\n\n* added necessary component cards\n\n* fix: timeout\n\n* fix: timeout\n\n---------\n\nCo-authored-by: Dilyara Zharikova (Baymurzina) <dilyara.rimovna@gmail.com>"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2023-08-11T11:51:46Z",
        "message": "pre-aligned pair upload: upload one pair per annotator"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2023-08-11T11:07:43Z",
        "message": "fix bug uploading pre-aligned & pre-split n:m pairs"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2023-08-11T10:57:08Z",
        "message": "fix bug error annotation"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2022-11-17T14:39:11Z",
        "message": "align documents with different simplified versions (references) and export them"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2022-10-06T12:30:04Z",
        "message": "bug fix: counting sentence ids of pre-split data\n\naddressing #1"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2022-05-16T10:01:42Z",
        "message": "bugfixes"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2022-04-14T12:02:50Z",
        "message": "update webcrawling"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2022-04-05T14:13:21Z",
        "message": "bug fixing"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2022-03-07T16:57:02Z",
        "message": "downgrade to spacy 2.3.7 (from 3.2.0) II"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2022-03-07T16:40:43Z",
        "message": "downgrade to spacy 2.3.7 (from 3.2.0)"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2022-03-05T13:16:11Z",
        "message": "\"show most similar sentence\"-button for alignment support"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2022-03-03T17:55:29Z",
        "message": "remove add_par_nr"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2022-03-03T17:53:33Z",
        "message": "uncomment add_par_nr"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2022-01-18T14:05:40Z",
        "message": "export document level (aligned and not-aligned)"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2022-01-14T15:58:19Z",
        "message": "add right-to-left reading support"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-12-30T17:38:13Z",
        "message": "improvement of export functions"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-12-30T12:45:28Z",
        "message": "huge update including user fields, local and web import, IAA alignment, config files annotation, web scraping"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-04-16T13:04:41Z",
        "message": "manual simplification module"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-04-07T17:59:33Z",
        "message": "add data with copyright threshold"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-04-07T12:35:50Z",
        "message": "add overview of all corpora and overview per corpus"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-04-06T18:12:10Z",
        "message": "remove django-language"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-04-06T10:39:54Z",
        "message": "add pre aligned data"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-03-01T14:34:40Z",
        "message": "import title in file header"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-03-01T11:02:55Z",
        "message": "fix reading file ending"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-02-25T17:34:33Z",
        "message": "visibiliy changes"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-02-04T14:07:53Z",
        "message": "add possible to align button"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-02-04T13:33:02Z",
        "message": "report malformed sentence"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-01-22T14:18:42Z",
        "message": "disable stanza download"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-01-22T13:36:26Z",
        "message": "new url structure, prev and next button, better HTML structure, change_log"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-01-15T13:49:38Z",
        "message": "add downloading stanza model"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-01-15T13:39:41Z",
        "message": "change spacy to stanza"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-01-15T12:15:51Z",
        "message": "automatic transformations in evaluation added"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-01-13T14:20:25Z",
        "message": "code clean up (new model structure) Part II"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-01-12T19:53:51Z",
        "message": "code clean up (new model structure) Part I"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-01-12T11:25:01Z",
        "message": "document table overview"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-01-08T15:57:05Z",
        "message": "upload parallel documents without sentence alignment"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-01-07T15:19:02Z",
        "message": "save and edit alignments"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-01-05T20:19:30Z",
        "message": "separate transformations and rating"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2020-10-09T08:37:39Z",
        "message": "overview of alignment and rating"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2020-10-07T15:42:36Z",
        "message": "alignment changing and rating"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2020-10-02T17:30:49Z",
        "message": "option to upload records"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2020-09-30T15:49:20Z",
        "message": "added app per task"
    },
    {
        "repo_url": "github.com/nipunsadvilkar/pySBD",
        "filepath": "benchmarks/benchmark_sbd_tools.py",
        "commit_date": "2020-07-19T17:25:34Z",
        "message": "\u26a1\ufe0f  Benchmarks scripts - genia & bigtext file"
    },
    {
        "repo_url": "github.com/AlibabaResearch/DAMO-ConvAI",
        "filepath": "star/LGESQL/cosql/preprocess/common_utils.py",
        "commit_date": "2022-11-14T06:09:25Z",
        "message": "add: star"
    },
    {
        "repo_url": "github.com/AlibabaResearch/DAMO-ConvAI",
        "filepath": "star/LGESQL/sparc/preprocess/common_utils.py",
        "commit_date": "2022-11-14T06:09:25Z",
        "message": "add: star"
    },
    {
        "repo_url": "github.com/nipunsadvilkar/pySBD",
        "filepath": "benchmarks/bigtext_speed_benchmark.py",
        "commit_date": "2020-07-19T17:25:34Z",
        "message": "\u26a1\ufe0f  Benchmarks scripts - genia & bigtext file"
    },
    {
        "repo_url": "github.com/AlibabaResearch/DAMO-ConvAI",
        "filepath": "graphix/data_all_in/preprocess/common_utils.py",
        "commit_date": "2023-06-05T01:35:55Z",
        "message": "update: graphix"
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/processors/hybrid_qg/hybridqa_processor.py",
        "commit_date": "2023-01-20T09:06:48Z",
        "message": "fixed wrong indendation"
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/processors/hybrid_qg/hybridqa_processor.py",
        "commit_date": "2023-01-17T09:26:50Z",
        "message": "adding more comments and documentation"
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/processors/hybrid_qg/hybridqa_processor.py",
        "commit_date": "2022-10-20T06:01:12Z",
        "message": "Resolved conflicts"
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/processors/hybrid_qg/hybridqa_processor.py",
        "commit_date": "2022-09-26T08:33:14Z",
        "message": "Adding data processing script for hybridqg training."
    },
    {
        "repo_url": "github.com/honghanhh/ner-combining-contextual-and-global-features",
        "filepath": "enconll03_baselines/conll_stanza.py",
        "commit_date": "2020-05-06T15:31:08Z",
        "message": "double check stanza, flair and evaluate results"
    },
    {
        "repo_url": "github.com/honghanhh/ner-combining-contextual-and-global-features",
        "filepath": "enconll03_baselines/conll_stanza.py",
        "commit_date": "2020-04-25T17:43:47Z",
        "message": "Evaluate Flair/Stanza on eng.test.2.examples.txt"
    },
    {
        "repo_url": "github.com/honghanhh/ner-combining-contextual-and-global-features",
        "filepath": "enconll03_baselines/conll_stanza.py",
        "commit_date": "2020-04-24T16:04:19Z",
        "message": "Update working status and stanza"
    },
    {
        "repo_url": "github.com/honghanhh/ner-combining-contextual-and-global-features",
        "filepath": "enconll03_baselines/conll_stanza.py",
        "commit_date": "2020-04-24T16:02:29Z",
        "message": "Test with stanza, reconfig downloaded model to fix error"
    },
    {
        "repo_url": "github.com/honghanhh/ner-combining-contextual-and-global-features",
        "filepath": "enconll03_baselines/conll_stanza.py",
        "commit_date": "2020-04-24T14:38:06Z",
        "message": "Test with flair embeddings, error pipeline in stanza"
    },
    {
        "repo_url": "github.com/honghanhh/ner-combining-contextual-and-global-features",
        "filepath": "enconll03_baselines/conll_stanza.py",
        "commit_date": "2020-04-20T08:08:20Z",
        "message": "Update task 3 and baseline code using stanza and flair"
    },
    {
        "repo_url": "github.com/mudabek/encoding-cxr-report-gen",
        "filepath": "ner_reports.py",
        "commit_date": "2022-11-24T13:17:45Z",
        "message": "full"
    },
    {
        "repo_url": "github.com/ncbi/GNorm2",
        "filepath": "GeneNER_SpeAss_run.py",
        "commit_date": "2023-01-31T01:02:11Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/ncbi/GNorm2",
        "filepath": "src_python/GeneNER/BIO_format.py",
        "commit_date": "2023-09-05T22:16:40Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/ncbi/GNorm2",
        "filepath": "src_python/SpeAss/SA_Pubtator_Conll.py",
        "commit_date": "2023-09-05T22:16:40Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/gregorycrane/Homerica",
        "filepath": "stanza-greek.py",
        "commit_date": "2021-09-28T15:16:18Z",
        "message": "base workflow"
    },
    {
        "repo_url": "github.com/AlibabaResearch/DAMO-ConvAI",
        "filepath": "graphix/data_all_in/preprocess/common_utils_proton.py",
        "commit_date": "2023-06-05T01:35:55Z",
        "message": "update: graphix"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2022-12-19T21:04:18Z",
        "message": "fixes issues caused by pymorphy and unnamed unicode characters"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2022-05-25T18:12:12Z",
        "message": "fixed pep8 issues"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-11-03T22:52:24Z",
        "message": "changed stanza chinese processing to use neural model"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-11-03T22:38:06Z",
        "message": "finished jieba integration"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-11-03T12:33:51Z",
        "message": "standalone jieba tokenizer"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-10-19T20:55:32Z",
        "message": "temporarily adds jieba back through stanza - stanza model loading and downloading needs work"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-10-18T18:14:14Z",
        "message": "Fixes #2 removes hard coded locations for models"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-10-18T18:04:54Z",
        "message": "update text processing to remove dependencies on install locations"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-07-20T13:18:26Z",
        "message": "using named arguments"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-07-11T12:59:14Z",
        "message": "adds updated parsivar dependency for Farsi stemming"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-07-09T00:00:02Z",
        "message": "adds farsi stemmer"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-06-29T17:50:09Z",
        "message": "catches recursion error in porter stemmer and passes the token through unchanged"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-06-23T18:17:25Z",
        "message": "cleaned up lowercase normalization"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-06-09T21:59:24Z",
        "message": "limit stanza to a single thread for now"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-06-09T21:09:27Z",
        "message": "stanza also needs to the package turned off when using jieba to prevent it from loading the kitchen sink"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-06-08T19:24:48Z",
        "message": "only download resources when not already there for stanza"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-28T14:26:40Z",
        "message": "Merge branch 'load_psq_table' into 'master'\n\nHandles loading PSQ translation tables\n\nSee merge request research/scale2021!124"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-28T12:44:30Z",
        "message": "can use ~ in model path"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-28T11:54:24Z",
        "message": "fixed error message that had old config path"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-27T23:30:26Z",
        "message": "put most of the infrastructure in place to generate PSQs"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-27T19:48:38Z",
        "message": "adds pre and post functions for normalization"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-25T20:20:04Z",
        "message": "delay loading nlp models until the pipeline begins"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-25T20:16:05Z",
        "message": "cleaned up inheritance for text processing"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-24T22:16:42Z",
        "message": "only load some nlp libraries when needed"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-22T19:31:53Z",
        "message": "Fixes #101 stop words works with stemming now"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-22T18:07:08Z",
        "message": "removed multiplex support from patapsco"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-22T15:49:29Z",
        "message": "switch to ISO 639-3"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-22T14:40:10Z",
        "message": "better construction of tokenizers and stemmers"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-22T13:08:46Z",
        "message": "porter stemming shouldn't also lowercase the stems"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-21T20:21:12Z",
        "message": "first part of porter stemmer implementation"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-21T19:33:09Z",
        "message": "improve configuration of text processing including model path"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-21T18:56:50Z",
        "message": "removed a debugging statement"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-20T12:29:47Z",
        "message": "adds stemming for spaCy"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-20T11:26:31Z",
        "message": "adds tests for stanza lemmatization and fixes issue with Arabic"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-19T22:52:47Z",
        "message": "restructure how tokenizers are built"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-19T20:25:38Z",
        "message": "Merge branch 'master' into 'normalize_text'\n\n# Conflicts:\n#   infrastructure/pipeline/environment.yml\n#   infrastructure/pipeline/patapsco/text.py\n#   infrastructure/pipeline/setup.py"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-19T14:58:49Z",
        "message": "adds character ngrams"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-19T13:13:25Z",
        "message": "adds moses tokenizer"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-18T13:10:57Z",
        "message": "correctly using model name or path to model directory in loader"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-18T12:36:57Z",
        "message": "fixed style issue and set spacy version"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-18T12:35:02Z",
        "message": "update model locations"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-18T12:16:03Z",
        "message": "updated spacy code and tests"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-18T11:35:06Z",
        "message": "adds spacy tokenization"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-15T23:23:46Z",
        "message": "switch to our own normalization code"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-11T00:21:16Z",
        "message": "fixed typo on stanza location on grid"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-10T22:59:41Z",
        "message": "the model path for stanza can be set now"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-04T13:00:11Z",
        "message": "Merge branch 'master' into 'add_stanza'\n\n# Conflicts:\n#   infrastructure/pipeline/patapsco/text.py"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-04T12:53:19Z",
        "message": "adds chinese tokenization using jieba"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-04T12:00:10Z",
        "message": "better logging for stanza"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-04T11:34:16Z",
        "message": "fixes spelling mistake"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-04T11:29:53Z",
        "message": "added StanzaTokenizer for english"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-04T11:08:15Z",
        "message": "Refs #9 integrates scriptnorm"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-04-24T16:05:51Z",
        "message": "better name for mock stemmer"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-04-24T15:45:54Z",
        "message": "moved config schema into a single file"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-04-23T18:03:56Z",
        "message": "better validation of config with unit tests for building the pipelines"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-04-23T13:55:30Z",
        "message": "standardizes text processing config"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-04-20T12:50:19Z",
        "message": "fixed bug with a single split"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-04-19T20:35:27Z",
        "message": "adds splitter for text processing"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-04-19T19:26:27Z",
        "message": "adds stop words"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-04-16T22:52:22Z",
        "message": "renamed the pipeline package to patapsco"
    },
    {
        "repo_url": "github.com/smartcontractsyn/iSyn",
        "filepath": "ContractToSmartContract-python/rule_based_extraction.py",
        "commit_date": "2022-01-20T13:34:12Z",
        "message": "Init artifact repo"
    },
    {
        "repo_url": "github.com/huggingface/api-inference-community",
        "filepath": "docker_images/stanza/app/pipelines/token_classification.py",
        "commit_date": "2023-04-19T12:57:59Z",
        "message": "[Diffusers] Add text-guided image to image (#223)\n\n* [Diffusers] Add image to image\n\n* fix some stuff\n\n* make style\n\n* make style\n\n* Update docker_images/diffusers/app/pipelines/image_to_image.py\n\nCo-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>\n\n* Fixing diffusers.\n\n* Apply suggestions from code review\n\nCo-authored-by: Omar Sanseviero <osanseviero@gmail.com>\n\n* Apply suggestions from code review\n\n* up\n\n* make style\n\n* Apply suggestions from code review\n\n* finalize\n\n---------\n\nCo-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>\nCo-authored-by: Omar Sanseviero <osanseviero@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/api-inference-community",
        "filepath": "docker_images/stanza/app/pipelines/token_classification.py",
        "commit_date": "2022-03-16T10:53:41Z",
        "message": "MNT move api-inference-community to root of the repo"
    },
    {
        "repo_url": "github.com/quadrismegistus/cadence",
        "filepath": "cadence/parsers/stanzanlp.py",
        "commit_date": "2022-01-20T22:29:11Z",
        "message": "fixes and readme updated"
    },
    {
        "repo_url": "github.com/quadrismegistus/cadence",
        "filepath": "cadence/parsers/stanzanlp.py",
        "commit_date": "2022-01-15T18:05:11Z",
        "message": "working on web app"
    },
    {
        "repo_url": "github.com/quadrismegistus/cadence",
        "filepath": "cadence/parsers/stanzanlp.py",
        "commit_date": "2022-01-13T16:15:06Z",
        "message": "mtree fixes"
    },
    {
        "repo_url": "github.com/quadrismegistus/cadence",
        "filepath": "cadence/parsers/stanzanlp.py",
        "commit_date": "2022-01-13T14:52:21Z",
        "message": "fix"
    },
    {
        "repo_url": "github.com/quadrismegistus/cadence",
        "filepath": "cadence/parsers/stanzanlp.py",
        "commit_date": "2022-01-13T14:43:32Z",
        "message": "fix stanza bug"
    },
    {
        "repo_url": "github.com/quadrismegistus/cadence",
        "filepath": "cadence/parsers/stanzanlp.py",
        "commit_date": "2022-01-13T12:48:50Z",
        "message": "stanza edits"
    },
    {
        "repo_url": "github.com/quadrismegistus/cadence",
        "filepath": "cadence/parsers/stanzanlp.py",
        "commit_date": "2022-01-13T12:48:21Z",
        "message": "fix"
    },
    {
        "repo_url": "github.com/quadrismegistus/cadence",
        "filepath": "cadence/parsers/stanzanlp.py",
        "commit_date": "2022-01-11T03:48:52Z",
        "message": "major text obj improvements"
    },
    {
        "repo_url": "github.com/quadrismegistus/cadence",
        "filepath": "cadence/parsers/stanzanlp.py",
        "commit_date": "2022-01-10T21:08:22Z",
        "message": "updates"
    },
    {
        "repo_url": "github.com/quadrismegistus/cadence",
        "filepath": "cadence/parsers/stanzanlp.py",
        "commit_date": "2022-01-09T17:13:55Z",
        "message": "updates"
    },
    {
        "repo_url": "github.com/quadrismegistus/cadence",
        "filepath": "cadence/parsers/stanzanlp.py",
        "commit_date": "2022-01-08T22:42:37Z",
        "message": "updates"
    },
    {
        "repo_url": "github.com/alexandrainst/danlp",
        "filepath": "examples/benchmarks/pos_benchmarks.py",
        "commit_date": "2021-09-29T11:39:16Z",
        "message": "Fixing errors for the documentation page (Tasks, Datasets, Frameworks) (#139)\n\n* Fix Windows compatibility error\n\n* Fix Windows compatibility error 2\n\n* added test script to gitignore\n\n* Accidentally pushed local test artifacts.\n\n* Fix Windows compatibility error 3\n\n* Removed a parentheses\n\n* Fix windows Compatibility error 4\n\n* Updated comments and changed input string\n\n* Updated another comment\n\n* Removed unexpected key word arguments\n\n* example sentences for Frameworks flair 'Your dataset'\n\n* Fixed typo and updated documentation to fit with flair 0.8\n\n* Now includes line to download the required stanza model\n\n* Avoiding division by zero\n\n* added comment that describes how to run the dacy benchmark functions\n\n* removed relative import, which doesn't work for Windows and Mac\n\n* Updated version of pyicu\n\n* Updated print on how to download and where to place model\n\n* added line that downloads needed resources\n\n* compatibility with flair 0.8\n\n* Windows compatibility\n\n* reverting gitignore\n\n* fix pycache ignore\n\n* compatibility with flair 0.8\n\n* coherence between model_path and output\n\n* coherence between model_path and output\n\n* removed relative import\n\n* Revert changed version of PyICU\n\n* Fixed comments\n\nCo-authored-by: Oliver Kinch <oliver@alex5925.local>"
    },
    {
        "repo_url": "github.com/alexandrainst/danlp",
        "filepath": "examples/benchmarks/pos_benchmarks.py",
        "commit_date": "2021-04-20T09:31:13Z",
        "message": "Add Stanza benchmark (#128)\n\n* Add stanza benchmarking\n\n* Update benchmark requirements\n\n* Add documentation"
    },
    {
        "repo_url": "github.com/alexandrainst/danlp",
        "filepath": "examples/benchmarks/pos_benchmarks.py",
        "commit_date": "2021-04-07T14:23:11Z",
        "message": "adding dacy to danlp (#118)\n\n* adding dacy to danlp\n\n* rem test and fixed req\n\n* updated ner\n\n* updated pos\n\n* added dep\n\n* added dacy benchmark to main as a comment\n\n* removed dacy small from NER\n\n* added version to DaCy models\n\nCo-authored-by: KennethEnevoldsen <kennethcenevolsen@gmail.com>"
    },
    {
        "repo_url": "github.com/alexandrainst/danlp",
        "filepath": "examples/benchmarks/pos_benchmarks.py",
        "commit_date": "2021-02-05T14:08:41Z",
        "message": "Add NER benchmark evaluation with NERDA"
    },
    {
        "repo_url": "github.com/alexandrainst/danlp",
        "filepath": "examples/benchmarks/pos_benchmarks.py",
        "commit_date": "2020-10-18T15:28:25Z",
        "message": "Fix import error in benchmarks"
    },
    {
        "repo_url": "github.com/alexandrainst/danlp",
        "filepath": "examples/benchmarks/pos_benchmarks.py",
        "commit_date": "2020-10-15T13:58:54Z",
        "message": "Move metrics to benchmark utils"
    },
    {
        "repo_url": "github.com/alexandrainst/danlp",
        "filepath": "examples/benchmarks/pos_benchmarks.py",
        "commit_date": "2020-10-14T15:30:44Z",
        "message": "Add printing of speed (sentences per second) to benchmarks (utils)"
    },
    {
        "repo_url": "github.com/alexandrainst/danlp",
        "filepath": "examples/benchmarks/pos_benchmarks.py",
        "commit_date": "2020-10-13T12:25:39Z",
        "message": "update use of metrics in benchmarks"
    },
    {
        "repo_url": "github.com/alexandrainst/danlp",
        "filepath": "examples/benchmarks/pos_benchmarks.py",
        "commit_date": "2020-09-09T14:11:02Z",
        "message": "fixing the calculation of scores for POS-tagging\n* accuracy instead of F1\n* corrected output for the polyglot model"
    },
    {
        "repo_url": "github.com/alexandrainst/danlp",
        "filepath": "examples/benchmarks/pos_benchmarks.py",
        "commit_date": "2020-03-31T09:25:58Z",
        "message": "Update pos with benchmark and docs\n\n Fix #26 and update function name in loading flair pos tagger"
    },
    {
        "repo_url": "github.com/gazelle93/Transformer-Various-Positional-Encoding",
        "filepath": "text_processing.py",
        "commit_date": "2022-05-27T19:39:47Z",
        "message": "Create text_processing.py"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/coreference/v1/utilities.py",
        "commit_date": "2023-04-03T09:36:57Z",
        "message": "Fixed initialize_coreference_components: added possibility to specify resources_root_dir"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/coreference/v1/utilities.py",
        "commit_date": "2023-03-30T15:12:28Z",
        "message": "Added EstonianCoreferenceSystem (v1.0.0) to estnltk_neural"
    },
    {
        "repo_url": "github.com/explosion/spacy-stanza",
        "filepath": "tests/test_language.py",
        "commit_date": "2022-05-27T14:12:39Z",
        "message": "Extend to stanza v1.4"
    },
    {
        "repo_url": "github.com/explosion/spacy-stanza",
        "filepath": "tests/test_language.py",
        "commit_date": "2021-11-09T09:18:13Z",
        "message": "Fallback to upos instead of to feats (#77)\n\n* fallback to upos instead of to feats\n\n* Use test data from UD train set\n\n* more future-proof test for Spanish\n\n* delete comment\n\n* make black"
    },
    {
        "repo_url": "github.com/explosion/spacy-stanza",
        "filepath": "tests/test_language.py",
        "commit_date": "2021-11-09T08:50:11Z",
        "message": "Updates for stanza v1.3.0, set version to v1.0.1 (#79)\n\n* Extend to stanza v1.3.x\n\n* Set version to v1.0.1\n\n* Update tests for more recent models"
    },
    {
        "repo_url": "github.com/explosion/spacy-stanza",
        "filepath": "tests/test_language.py",
        "commit_date": "2021-03-02T13:10:15Z",
        "message": "Fix trailing whitespace token handling (#64)"
    },
    {
        "repo_url": "github.com/explosion/spacy-stanza",
        "filepath": "tests/test_language.py",
        "commit_date": "2021-03-02T09:01:16Z",
        "message": "Refactor for spaCy v3.0 (#58)\n\n* Refactor for spaCy v3.0\n\n* Implement the stanza processing as a custom registered tokenizer\n`spacy_stanza.PipelineAsTokenizer.v1`\n  * Include all `stanza.Pipeline` settings in the tokenizer config block\n  * Move the custom user hooks into the tokenizer processing\n\n* Add the helper method `spacy_stanza.blank()` for pipeline\ninitialization, which sets up the custom tokenizer in the config and\npasses the config on to `spacy.blank()`\n\n* Update README\n\n* Update README [ci skip]\n\n* Simplify config setup\n\n* Clean up and reformat\n\n* Use plain pip install in README\n\n* Fix dir Pipeline arg\n\n* Add test that loads directly from a minimal config\n\n* Support kwargs directly in pipeline init method\n\nSupport passing kwargs directly from pipeline init method rather than\nrequiring them to be set in `[nlp.tokenizer.kwargs]`.\n\n* Follow Pipeline API instead of spacy.blank API\n\nRename `spacy_stanza.blank` to `spacy_stanza.load_pipeline`.\n\n* Set version to 1.0.0 and update contact in setup"
    },
    {
        "repo_url": "github.com/explosion/spacy-stanza",
        "filepath": "tests/test_language.py",
        "commit_date": "2021-02-03T15:50:16Z",
        "message": "Update for spacy v3 (#55)\n\n* Update for spacy v3.0.0\n\n* Update requirements in requirements.txt\n\n* Sync requirements.txt and setup.py\n\n* Update for spacy v3.0 and stanza v1.2"
    },
    {
        "repo_url": "github.com/explosion/spacy-stanza",
        "filepath": "tests/test_language.py",
        "commit_date": "2020-09-02T11:10:11Z",
        "message": "Fix alignment for spacy tokenizer (#44)\n\nFix the alignment when spacy is used as the tokenizer and may return\nwhitespace tokens."
    },
    {
        "repo_url": "github.com/explosion/spacy-stanza",
        "filepath": "tests/test_language.py",
        "commit_date": "2020-06-26T08:38:04Z",
        "message": "Rewrite alignment to preserve whitespace tokens (#41)\n\nRewrite the alignment algorithm to create the words and spaces using a\ncopy of `spacy.util.get_words_and_spaces` and align the stanza\nannotation to the `Doc`, adjusting the positions and offsets around the\nadditional whitespace tokens."
    },
    {
        "repo_url": "github.com/explosion/spacy-stanza",
        "filepath": "tests/test_language.py",
        "commit_date": "2020-03-19T09:27:43Z",
        "message": "Show warning if entities don't map to tokens"
    },
    {
        "repo_url": "github.com/explosion/spacy-stanza",
        "filepath": "tests/test_language.py",
        "commit_date": "2020-03-17T18:15:17Z",
        "message": "StanfordNLP -> Stanza (#26)\n\n* stanfordnlp -> stanza\n\n* Create azure-pipelines.yml\n\n* Update README.md\n\n* Try to fix PyTorch installation for windows\n\n* Don't test on Windows for now\n\n* Update README.md\n\n* Fix typo [ci skip]"
    },
    {
        "repo_url": "github.com/explosion/spacy-stanza",
        "filepath": "tests/test_language.py",
        "commit_date": "2019-05-31T09:56:50Z",
        "message": "Relax version requirement for stanfordnlp to include v0.2.0. (#15)\n\n* For compatibility with native Spacy language classes allow passing of empty text strings. This will produce 0-length docs, rather than raising an exception.\n\n* Increment minor version.\n\n* Relax version requirement for stanfordnlp to include new 0.2.0.\n\n* Make tests pass with stanfordnlp 0.2.0. Some POS tag predictions have flipped in ambiguous cases (e.g. DET/PRON, VERB/AUX)."
    },
    {
        "repo_url": "github.com/explosion/spacy-stanza",
        "filepath": "tests/test_language.py",
        "commit_date": "2019-01-31T14:24:22Z",
        "message": "Update test_language.py"
    },
    {
        "repo_url": "github.com/explosion/spacy-stanza",
        "filepath": "tests/test_language.py",
        "commit_date": "2019-01-31T14:19:04Z",
        "message": "Move tests"
    },
    {
        "repo_url": "github.com/language-ml/parsi.io",
        "filepath": "parsi_io/modules/stockmarket_event_extractor/stockmarket_event_extractor.py",
        "commit_date": "2022-10-27T20:42:58Z",
        "message": "Update stockmarket_event_extractor.py"
    },
    {
        "repo_url": "github.com/language-ml/parsi.io",
        "filepath": "parsi_io/modules/stockmarket_event_extractor/stockmarket_event_extractor.py",
        "commit_date": "2022-10-14T11:00:46Z",
        "message": "add stockmarket_event_extractor"
    },
    {
        "repo_url": "github.com/cltk/cltk",
        "filepath": "src/cltk/dependency/stanza_wrapper.py",
        "commit_date": "2024-02-06T01:48:11Z",
        "message": "Add typing (#1242)\n\n* misc typing fixes\n\n* more cleanup\n\n* more cleanup\n\n* rm mkdocs\n\n* formatting changes\n\n* add typing to corpora latin\n\n* add types to stanza wrapper\n\n* rm check of spacy vers from build\n\n* add more types\n\n* change all old Dict[] List[] Tuple[] types\n\n* change deepcopy to copy for 3.11 error\n\n* more type improvements\n\n* change all type annotations to hints\n\n* change all type annotations to hints\n\n* fix four errors after code analysis"
    },
    {
        "repo_url": "github.com/cltk/cltk",
        "filepath": "src/cltk/dependency/stanza_wrapper.py",
        "commit_date": "2023-12-27T07:55:02Z",
        "message": "Updates to Clemsciences's spacy process (#1239)\n\n* Added spaCy process\n\n* Improved spaCy to CLTK wrapper\n\n* Use correct Token attributes\n\n* Use spaCy download function instead of shell command\n\n* Update poetry.lock\n\n* Removed unrelated changes\n\n* Fixed SpacyWrapper, StanzaWrapper and download_all_models.py\n\n* Improved SpacyWrapper\n\n* update dependencies\n\n* demo notebook not working\n\n* Added spaCy process\n\n* Improved spaCy to CLTK wrapper\n\n* Use correct Token attributes\n\n* Use spaCy download function instead of shell command\n\n* Removed unrelated changes\n\n* Fixed SpacyWrapper, StanzaWrapper and download_all_models.py\n\n* Improved SpacyWrapper\n\n* Improved SpacyWrapper\n\n* Renamed spacy_dep.py to spacy_wrapper.py and lint fixes\n\n* add morphology from spacy to cltk Doc\n\n* downgrade spacy to 3.6.1\n\n* Download spaCy model if the model is absent\n\n* update dev dependencies, improve spacy wrapper\n\n* spacy wrapper working\n\n* fix .get_dependencies()\n\n* make e2e work with LatinCy\n\n* re-add Latin Stops Process\n\n* add spacy dl to build script\n\n* trigger CI rerun\n\n* load spacy directly\n\n* dl spacy model with subprocess\n\n* load model after dl\n\n* load w/ spacy (wrapper seems to err\n\n* don't check only dl\n\n* bump spacy\n\n* why old spacy on ci?\n\n* deepcopy to copy\n\n* basic lat tests pass\n\n* rewrite bad UD types from Mood to VerbForm\n\n* finish more cleanup or LatinCy release\n\n* add citation printing\n\n* rename latincy proc\n\n---------\n\nCo-authored-by: Cl\u00e9ment Besnier <clemsciences@aol.com>\nCo-authored-by: Cl\u00e9ment Besnier <clem@clementbesnier.fr>"
    },
    {
        "repo_url": "github.com/cltk/cltk",
        "filepath": "src/cltk/dependency/stanza_wrapper.py",
        "commit_date": "2023-11-04T10:15:39Z",
        "message": "Fixed type in stanza_wrapper.py that prevented using stanza"
    },
    {
        "repo_url": "github.com/cltk/cltk",
        "filepath": "src/cltk/dependency/stanza_wrapper.py",
        "commit_date": "2023-11-04T10:15:39Z",
        "message": "Version 1.1.6a1"
    },
    {
        "repo_url": "github.com/cltk/cltk",
        "filepath": "src/cltk/dependency/stanza_wrapper.py",
        "commit_date": "2023-11-04T10:15:39Z",
        "message": "Make path cross-platform"
    },
    {
        "repo_url": "github.com/cltk/cltk",
        "filepath": "src/cltk/dependency/stanza_wrapper.py",
        "commit_date": "2023-11-04T10:15:39Z",
        "message": "Improved annotations and let less tasks to dataclasses"
    },
    {
        "repo_url": "github.com/thunlp/OpenBackdoor",
        "filepath": "openbackdoor/data/plain_dataset.py",
        "commit_date": "2022-04-07T06:06:28Z",
        "message": "fix trojanlm"
    },
    {
        "repo_url": "github.com/thunlp/OpenBackdoor",
        "filepath": "openbackdoor/data/plain_dataset.py",
        "commit_date": "2022-03-28T16:21:58Z",
        "message": "add plain dataset"
    },
    {
        "repo_url": "github.com/alexandrainst/danlp",
        "filepath": "examples/benchmarks/dependency_benchmarks.py",
        "commit_date": "2021-09-29T11:39:16Z",
        "message": "Fixing errors for the documentation page (Tasks, Datasets, Frameworks) (#139)\n\n* Fix Windows compatibility error\n\n* Fix Windows compatibility error 2\n\n* added test script to gitignore\n\n* Accidentally pushed local test artifacts.\n\n* Fix Windows compatibility error 3\n\n* Removed a parentheses\n\n* Fix windows Compatibility error 4\n\n* Updated comments and changed input string\n\n* Updated another comment\n\n* Removed unexpected key word arguments\n\n* example sentences for Frameworks flair 'Your dataset'\n\n* Fixed typo and updated documentation to fit with flair 0.8\n\n* Now includes line to download the required stanza model\n\n* Avoiding division by zero\n\n* added comment that describes how to run the dacy benchmark functions\n\n* removed relative import, which doesn't work for Windows and Mac\n\n* Updated version of pyicu\n\n* Updated print on how to download and where to place model\n\n* added line that downloads needed resources\n\n* compatibility with flair 0.8\n\n* Windows compatibility\n\n* reverting gitignore\n\n* fix pycache ignore\n\n* compatibility with flair 0.8\n\n* coherence between model_path and output\n\n* coherence between model_path and output\n\n* removed relative import\n\n* Revert changed version of PyICU\n\n* Fixed comments\n\nCo-authored-by: Oliver Kinch <oliver@alex5925.local>"
    },
    {
        "repo_url": "github.com/alexandrainst/danlp",
        "filepath": "examples/benchmarks/dependency_benchmarks.py",
        "commit_date": "2021-04-20T09:31:13Z",
        "message": "Add Stanza benchmark (#128)\n\n* Add stanza benchmarking\n\n* Update benchmark requirements\n\n* Add documentation"
    },
    {
        "repo_url": "github.com/arianhosseini/negation-learning",
        "filepath": "transformers/examples/get_conll.py",
        "commit_date": "2020-09-27T16:25:48Z",
        "message": "stable version emnlp"
    },
    {
        "repo_url": "github.com/arianhosseini/negation-learning",
        "filepath": "transformers/examples/new_get_conll.py",
        "commit_date": "2020-09-27T16:25:48Z",
        "message": "stable version emnlp"
    },
    {
        "repo_url": "github.com/deeppavlov/dream",
        "filepath": "annotators/bot_emotion_classifier/dscript_scheme_classifier.py",
        "commit_date": "2023-11-16T08:59:51Z",
        "message": "Dream emotion dist (#568)\n\n* add dream_emotion dist with annotators\n\n* add component cards\n\n* fix codestyle flake8\n\n* fix codestyle\n\n* another codestyle fix\n\n* PR fixes\n\n* fixed batches\n\n* move preprocessing to annotator\n\n* fix codestyle black\n\n* minor fixes\n\n* removed unnecessary api_configs\n\n* changed type of annotator to bot utts annotator\n\n* fixed component file\n\n* fixed service.yml files\n\n* fixed formatters\n\n* fixed emotional response server and dist files\n\n* moved prompt into separate file\n\n* fixed annotators\n\n* fixed annotators again\n\n* fixed emotion response selector\n\n* fixed emotion dist and components\n\n* fixed codestyle\n\n* fixed codestyle again\n\n* fixed codestyle black\n\n* fixed naming of emotion response selector\n\n* fixed emotional_bot_response component\n\n* fixed emotion response selector server\n\n* added prev_services in annotator in pipeline_conf\n\n* fixed result formatting of bot_emo_cls\n\n* fixed codestyle\n\n* fixed codestyle again\n\n* changed description of emo_bot_response\n\n* added necessary component cards\n\n* fix: timeout\n\n* fix: timeout\n\n---------\n\nCo-authored-by: Dilyara Zharikova (Baymurzina) <dilyara.rimovna@gmail.com>"
    },
    {
        "repo_url": "github.com/zide05/AdvFact",
        "filepath": "adversarial_transformation/Antonyms_stress_test.py",
        "commit_date": "2021-02-02T10:51:33Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/zide05/AdvFact",
        "filepath": "adversarial_transformation/Named_entity_stress_test.py",
        "commit_date": "2021-02-02T10:51:33Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/zide05/AdvFact",
        "filepath": "adversarial_transformation/Numerical_inference_strss_test.py",
        "commit_date": "2021-02-02T10:51:33Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/qijimrc/ROBUST",
        "filepath": "src/analysis/hws_distance.py",
        "commit_date": "2023-10-19T11:59:12Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/coreference/v1/coreference_api.py",
        "commit_date": "2023-04-03T09:36:57Z",
        "message": "Fixed initialize_coreference_components: added possibility to specify resources_root_dir"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/coreference/v1/coreference_api.py",
        "commit_date": "2023-03-31T14:56:09Z",
        "message": "Bugfix in coreference_api: do not predict on empty input"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/coreference/v1/coreference_api.py",
        "commit_date": "2023-03-30T15:12:28Z",
        "message": "Added EstonianCoreferenceSystem (v1.0.0) to estnltk_neural"
    },
    {
        "repo_url": "github.com/ly19965/CVPR_Anti_UAV",
        "filepath": "modelscope/preprocessors/nlp/space_T_en/fields/common_utils.py",
        "commit_date": "2023-01-18T12:38:53Z",
        "message": "add baseline"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/stanza_tagger.py",
        "commit_date": "2023-03-09T09:39:14Z",
        "message": "Refactored StanzaSyntax(Ensemble)Tagger: moved prepare_input_doc & feats_to_ordereddict to common_utils"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/stanza_tagger.py",
        "commit_date": "2023-03-08T16:16:18Z",
        "message": "Refactored & fixed StanzaSyntax(Ensemble)Tagger's input preparation: should now also work on detached layers"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/stanza_tagger.py",
        "commit_date": "2023-01-27T13:58:15Z",
        "message": "Updated StanzaSyntax(Ensemble)Tagger: added guarding exceptions against processing too long sentences with GPU / CUDA"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/stanza_tagger.py",
        "commit_date": "2023-01-19T12:05:04Z",
        "message": "Updated StanzaSyntax(Ensemble)Tagger: relocated initialization of Random obj into the constructor. Also made ensemble_tagger's random choice of max score dependency parses configurable by seed"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/stanza_tagger.py",
        "commit_date": "2023-01-18T09:54:46Z",
        "message": "Updated StanzaSyntax(Ensemble)Tagger: made random pick seed of ambiguous analyses configurable"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/stanza_tagger.py",
        "commit_date": "2022-04-22T10:41:17Z",
        "message": "Fixed StanzaSyntax(Ensemble)Tagger's conf_param (removed missing parameters, set default values to None)"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/stanza_tagger.py",
        "commit_date": "2022-04-22T09:15:51Z",
        "message": "Made StanzaSyntax(Ensemble)Tagger to automatically download missing resources + removed usage of environment variables"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/stanza_tagger.py",
        "commit_date": "2022-03-30T15:01:35Z",
        "message": "Fixed estnltk_neural setup: added missing stanza_resources json file"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/stanza_tagger.py",
        "commit_date": "2022-03-30T14:37:16Z",
        "message": "Made StanzaSyntax(Ensemble)Tagger to use STANZA_SYNTAX_MODELS_PATH & added smoke test for ensemble_tagger"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/stanza_tagger.py",
        "commit_date": "2021-11-17T13:57:32Z",
        "message": "Updated StanzaSyntaxTagger's docstring and added check for mismatching input_type and input_morph_layer"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/stanza_tagger.py",
        "commit_date": "2021-11-17T09:57:04Z",
        "message": "Relocated morph_analysis, text_segmentation, syntax, ner, timexes into estnltk.taggers.standard"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/stanza_tagger.py",
        "commit_date": "2021-10-22T15:24:58Z",
        "message": "estnltk neural additions and moving around other files"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/ensemble_tagger.py",
        "commit_date": "2023-04-24T08:16:32Z",
        "message": "Updated StanzaSyntaxEnsembleTagger: added majority_voting aggregation algorithm"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/ensemble_tagger.py",
        "commit_date": "2023-03-09T09:39:14Z",
        "message": "Refactored StanzaSyntax(Ensemble)Tagger: moved prepare_input_doc & feats_to_ordereddict to common_utils"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/ensemble_tagger.py",
        "commit_date": "2023-03-08T16:16:18Z",
        "message": "Refactored & fixed StanzaSyntax(Ensemble)Tagger's input preparation: should now also work on detached layers"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/ensemble_tagger.py",
        "commit_date": "2023-01-27T13:58:15Z",
        "message": "Updated StanzaSyntax(Ensemble)Tagger: added guarding exceptions against processing too long sentences with GPU / CUDA"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/ensemble_tagger.py",
        "commit_date": "2023-01-19T12:05:04Z",
        "message": "Updated StanzaSyntax(Ensemble)Tagger: relocated initialization of Random obj into the constructor. Also made ensemble_tagger's random choice of max score dependency parses configurable by seed"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/ensemble_tagger.py",
        "commit_date": "2023-01-18T09:54:46Z",
        "message": "Updated StanzaSyntax(Ensemble)Tagger: made random pick seed of ambiguous analyses configurable"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/ensemble_tagger.py",
        "commit_date": "2022-04-22T10:41:17Z",
        "message": "Fixed StanzaSyntax(Ensemble)Tagger's conf_param (removed missing parameters, set default values to None)"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/ensemble_tagger.py",
        "commit_date": "2022-04-22T09:15:51Z",
        "message": "Made StanzaSyntax(Ensemble)Tagger to automatically download missing resources + removed usage of environment variables"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/ensemble_tagger.py",
        "commit_date": "2022-03-30T14:37:16Z",
        "message": "Made StanzaSyntax(Ensemble)Tagger to use STANZA_SYNTAX_MODELS_PATH & added smoke test for ensemble_tagger"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/ensemble_tagger.py",
        "commit_date": "2022-03-05T09:03:17Z",
        "message": "Import fixes in estnltk_neural + added missing package_data"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/ensemble_tagger.py",
        "commit_date": "2021-11-17T09:57:04Z",
        "message": "Relocated morph_analysis, text_segmentation, syntax, ner, timexes into estnltk.taggers.standard"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/ensemble_tagger.py",
        "commit_date": "2021-10-22T15:24:58Z",
        "message": "estnltk neural additions and moving around other files"
    },
    {
        "repo_url": "github.com/RobustNLP/TestIC",
        "filepath": "ROME/gt_err/synonym_final.py",
        "commit_date": "2023-06-08T01:31:08Z",
        "message": "add Rome"
    },
    {
        "repo_url": "github.com/RobustNLP/TestIC",
        "filepath": "ROME/error_detection/synonym_clean.py",
        "commit_date": "2023-06-08T01:31:08Z",
        "message": "add Rome"
    },
    {
        "repo_url": "github.com/RobustNLP/TestIC",
        "filepath": "ROME/metaic_error_detection/ofa_base_meta/meta_relation_error_report_0.py",
        "commit_date": "2023-06-08T01:31:08Z",
        "message": "add Rome"
    },
    {
        "repo_url": "github.com/RobustNLP/TestIC",
        "filepath": "ROME/metaic_error_detection/ofa_base_meta/meta_relation_error_report_bar2.py",
        "commit_date": "2023-06-08T01:31:08Z",
        "message": "add Rome"
    },
    {
        "repo_url": "github.com/RobustNLP/TestIC",
        "filepath": "MetaIC/Report_erroneous_issues/azure_meta_relation_error_report_dir/meta_relation_error_report_0.py",
        "commit_date": "2023-05-26T14:30:27Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/RobustNLP/TestIC",
        "filepath": "MetaIC/Report_erroneous_issues/azure_meta_relation_error_report_dir/meta_relation_error_report_bar1.py",
        "commit_date": "2023-05-26T14:30:27Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/RobustNLP/TestIC",
        "filepath": "MetaIC/Report_erroneous_issues/azure_meta_relation_error_report_dir/meta_relation_error_report_bar2.py",
        "commit_date": "2023-05-26T14:30:27Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/RobustNLP/TestIC",
        "filepath": "MetaIC/Report_erroneous_issues/azure_meta_relation_error_report_dir/meta_relation_error_report_bar3.py",
        "commit_date": "2023-05-26T14:30:27Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/jcyk/XAMR",
        "filepath": "xl-amr/xlamr_stog/data/dataset_readers/amr_parsing/preprocess/feature_annotator_multilingual.py",
        "commit_date": "2021-12-11T08:14:38Z",
        "message": "ready"
    },
    {
        "repo_url": "github.com/huashen218/convxai",
        "filepath": "convxai/utils/utils.py",
        "commit_date": "2023-06-19T04:17:26Z",
        "message": "your comments here"
    },
    {
        "repo_url": "github.com/huashen218/convxai",
        "filepath": "convxai/utils/utils.py",
        "commit_date": "2022-12-05T06:35:26Z",
        "message": "counterfactual"
    },
    {
        "repo_url": "github.com/huashen218/convxai",
        "filepath": "convxai/utils/utils.py",
        "commit_date": "2022-11-29T19:56:05Z",
        "message": "round"
    },
    {
        "repo_url": "github.com/huashen218/convxai",
        "filepath": "convxai/utils/utils.py",
        "commit_date": "2022-11-28T03:21:42Z",
        "message": "counterfactual"
    },
    {
        "repo_url": "github.com/huashen218/convxai",
        "filepath": "convxai/utils/utils.py",
        "commit_date": "2022-11-26T07:40:24Z",
        "message": "universal-api"
    },
    {
        "repo_url": "github.com/huashen218/convxai",
        "filepath": "convxai/utils/utils.py",
        "commit_date": "2022-11-23T02:52:42Z",
        "message": "run"
    },
    {
        "repo_url": "github.com/huashen218/convxai",
        "filepath": "notebook_unified_XAI_API/convxai/utils/utils.py",
        "commit_date": "2023-06-22T02:50:32Z",
        "message": "cleanup"
    },
    {
        "repo_url": "github.com/huashen218/convxai",
        "filepath": "convxai/xai_models/preprocessing/global_xai_statistics/data_generator.py",
        "commit_date": "2023-06-22T02:48:28Z",
        "message": "cleanup"
    },
    {
        "repo_url": "github.com/huashen218/convxai",
        "filepath": "convxai/xai_models/preprocessing/global_xai_statistics/data_generator.py",
        "commit_date": "2023-06-19T04:17:26Z",
        "message": "your comments here"
    },
    {
        "repo_url": "github.com/huashen218/convxai",
        "filepath": "convxai/xai_models/preprocessing/global_xai_statistics/data_generator.py",
        "commit_date": "2022-11-29T19:56:05Z",
        "message": "round"
    },
    {
        "repo_url": "github.com/AlibabaResearch/DAMO-ConvAI",
        "filepath": "proton/preprocess/test_g.py",
        "commit_date": "2022-08-06T11:43:17Z",
        "message": "add: proton"
    },
    {
        "repo_url": "github.com/AlibabaResearch/DAMO-ConvAI",
        "filepath": "proton/preprocess/test_grappa_prob.py",
        "commit_date": "2022-08-06T11:43:17Z",
        "message": "add: proton"
    },
    {
        "repo_url": "github.com/AlibabaResearch/DAMO-ConvAI",
        "filepath": "proton/preprocess/test_grappa_prob_cmp.py",
        "commit_date": "2022-08-06T11:43:17Z",
        "message": "add: proton"
    },
    {
        "repo_url": "github.com/vvhg1/guided-text-generation-with-classifier-free-language-diffusion",
        "filepath": "improved-diffusion/control_gen/eval_control.py",
        "commit_date": "2022-08-17T13:29:10Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/vvhg1/guided-text-generation-with-classifier-free-language-diffusion",
        "filepath": "transformers/examples/pytorch/language-modeling/run_clm.py",
        "commit_date": "2022-08-17T13:29:10Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/hafriedlander/stable-diffusion-grpcserver",
        "filepath": "sdgrpcserver/pipeline/text_embedding/structured_text_embedding.py",
        "commit_date": "2022-11-03T09:44:04Z",
        "message": "Add ToMe support (optimised UNet), big improvement to CLIP guided speed and memory usage"
    },
    {
        "repo_url": "github.com/hafriedlander/stable-diffusion-grpcserver",
        "filepath": "sdgrpcserver/pipeline/text_embedding/structured_text_embedding.py",
        "commit_date": "2022-10-28T10:11:26Z",
        "message": "Once a year a cricket must slough its skin"
    },
    {
        "repo_url": "github.com/john-hewitt/model-editing-canonical-examples",
        "filepath": "data/temporal/convert_eval.py",
        "commit_date": "2024-02-12T00:13:19Z",
        "message": "arxiv commit"
    },
    {
        "repo_url": "github.com/john-hewitt/model-editing-canonical-examples",
        "filepath": "data/temporal/convert_eval_train.py",
        "commit_date": "2024-02-12T00:13:19Z",
        "message": "arxiv commit"
    },
    {
        "repo_url": "github.com/john-hewitt/model-editing-canonical-examples",
        "filepath": "data/stereoset/make_stereoset_hard_neg.py",
        "commit_date": "2024-02-12T00:13:19Z",
        "message": "arxiv commit"
    },
    {
        "repo_url": "github.com/Tbabm/CUP2",
        "filepath": "common.py",
        "commit_date": "2021-08-27T12:32:37Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/bigscience-workshop/data-preparation",
        "filepath": "preprocessing/training/01a_catalogue_cleaning_and_filtering/clean_helpers/sentence_splitter.py",
        "commit_date": "2022-08-22T14:56:15Z",
        "message": "put oscar in training (#27)"
    },
    {
        "repo_url": "github.com/bigscience-workshop/catalogue_data",
        "filepath": "clean_helpers/sentence_splitter.py",
        "commit_date": "2022-03-10T09:28:41Z",
        "message": "Update preprocessing key to use the new value from the google sheet (#48)\n\n* Rename maps-and-filters to preprocessing\n\n* Update to_jsonl script\n\n* Update the jsonl script\n\n* Nit\n\n* Factorise dataset name normalisation\n\n* New training dataset\n\n* Update soft deduplication\n\n* Fix weird bug\n\n* Update training.csv\n\n* Remove CUDA\n\n* - Fix empty datasets size estimation\n\nCo-authored-by: SaulLu <55560583+SaulLu@users.noreply.github.com>\n\n* Improve logging\n\n* Fix vi sent tokenizer (#54)\n\n* add dedicated vi tokenizer\n\n* clean imports\n\n* update requirements\n\n* Improve logging\n\n* Bump stanza dependency to 1.2.3\n\n* Replace filter with map (#55)\n\n* Bump stanza dependency to 1.2.3\n\n* .filter is really slow\n\n* Make new experiment concerning filtering (#56)\n\n* Make new experiment concerning filtering\n\n* nan becomes empty string\n\n* New short document filter\n\n* Remove unecessary rows\n\n* Update filter choices\n\n* Update slurm array\n\n* Nit\n\n* Remove empty strings\n\n* We need to remove all columns\n\n* Not sure why I need to add\n\n* Generalise deduplicate pattern. (#58)\n\n* Generalise deduplicate at document level\n\n* Example for lm_es_pseudocrawl_filtered_401_www_elperiodicodemexico_com\n\n* Fix naming\n\n* Update training.csv\n\n* new way to simplify dedup url (#57)\n\n* new way to simplify dedup url\n\n* Update clean_helpers/deduplication.py\n\nCo-authored-by: Thomas Wang <24695242+thomasw21@users.noreply.github.com>\n\nCo-authored-by: Thomas Wang <24695242+thomasw21@users.noreply.github.com>\n\n* Install old version of stanza\n\n* Update training.csv\n\n* Update training.csv\n\n* Concatenate ester dataset (#59)\n\n* init new concatenation fct\n\n* add concatenation\n\n* add now proc and batch size\n\n* last edits\n\n* Making sure that things are sorted (#60)\n\nCo-authored-by: Thomas Wang <24695242+thomasw21@users.noreply.github.com>\n\n* Attempt to make a v2 version that's better\n\n* Kill bloom\n\nCo-authored-by: SaulLu <55560583+SaulLu@users.noreply.github.com>\nCo-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bigscience-workshop/catalogue_data",
        "filepath": "clean_helpers/sentence_splitter.py",
        "commit_date": "2022-03-08T00:11:23Z",
        "message": "add sentence splitter functions (#49)\n\n* add sentence splitter functions\n\n* remove redundant newline remover\n\n* document downloads in readme\n\n* remove downloads\n\n* standardize naming\n\n* raise error when unsopported language is called\n\n* rename stanza pipeline\n\n* remove redundant newline remover from init\n\n* Update clean_helpers/sentence_splitter.py\n\nCo-authored-by: Thomas Wang <24695242+thomasw21@users.noreply.github.com>\n\n* fix hindi --> indic\n\n* Use torch.num_threads(1) (#53)\n\nCo-authored-by: Thomas Wang <24695242+thomasw21@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/ncbi/SpeciesAssignment",
        "filepath": "src/Species_Assignment.py",
        "commit_date": "2022-05-03T12:52:06Z",
        "message": "upload"
    },
    {
        "repo_url": "github.com/ncbi/SpeciesAssignment",
        "filepath": "src/species_assignment_eval_test.py",
        "commit_date": "2022-05-03T12:52:06Z",
        "message": "upload"
    },
    {
        "repo_url": "github.com/yerfor/SyntaSpeech",
        "filepath": "modules/tts/syntaspeech/syntactic_graph_buider.py",
        "commit_date": "2022-05-12T12:47:59Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/parse.py",
        "commit_date": "2020-09-02T09:03:56Z",
        "message": "Finish data pipeline for final SQuAD + Rewrites dataset  with Ass2s and QGPen."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/parse.py",
        "commit_date": "2020-08-25T02:01:34Z",
        "message": "Allow to mix synthetic and organic data during training. Add support for SQuAD triples and SQuAD mapped triples."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/parse.py",
        "commit_date": "2020-08-12T02:08:29Z",
        "message": "Implement POS and NE tag features. Fix attention mechanism by computing softmax on correct axis."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/parse.py",
        "commit_date": "2020-07-13T02:11:29Z",
        "message": "Modify Repeat Q data processing pipeline to work with the new format"
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/parse.py",
        "commit_date": "2020-07-03T03:22:11Z",
        "message": "Prototype of our RL network but without the RL part (for now trains like a regular supervised network)"
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/parse.py",
        "commit_date": "2020-06-24T10:55:13Z",
        "message": "Build data pipeline for RepeatQ. Implement embedding layer."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/parse.py",
        "commit_date": "2020-06-01T08:13:59Z",
        "message": "Reduce amount of parsing when reading SQuAD."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/parse.py",
        "commit_date": "2020-05-27T02:27:46Z",
        "message": "Integrate SG-Deep-Question-Generation project and reverse engineer initial graph construction steps"
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/parse.py",
        "commit_date": "2020-05-18T18:11:23Z",
        "message": "Integrate GDP2 as a model. Add evaluation and testing scripts for different datasets."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/parse.py",
        "commit_date": "2020-05-04T08:51:39Z",
        "message": "Add handmade medical dataset and add testing scripts for both this latter and MedQuad."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/parse.py",
        "commit_date": "2020-05-01T07:44:12Z",
        "message": "Add MedQuad as a usable dataset to train and evaluate models"
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "knowledge_graph/kg_api.py",
        "commit_date": "2020-06-06T03:02:29Z",
        "message": "Implement Google Graph Knowledge query engine and add some classes to represent knowledge."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/nqg_dataset.py",
        "commit_date": "2020-06-24T10:55:13Z",
        "message": "Build data pipeline for RepeatQ. Implement embedding layer."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/nqg_dataset.py",
        "commit_date": "2020-06-10T06:47:47Z",
        "message": "Multiple fixes  in eval mode of ASs2s."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/nqg_dataset.py",
        "commit_date": "2020-05-28T10:58:18Z",
        "message": "Manage to make predictions for SQuAD using HotpotQA pretrained model. Answer zones seem to not be working."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/nqg_dataset.py",
        "commit_date": "2020-05-27T02:27:46Z",
        "message": "Integrate SG-Deep-Question-Generation project and reverse engineer initial graph construction steps"
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/nqg_dataset.py",
        "commit_date": "2020-05-18T18:11:23Z",
        "message": "Integrate GDP2 as a model. Add evaluation and testing scripts for different datasets."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/nqg_dataset.py",
        "commit_date": "2020-05-04T08:51:39Z",
        "message": "Add handmade medical dataset and add testing scripts for both this latter and MedQuad."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/nqg_dataset.py",
        "commit_date": "2020-05-01T07:44:12Z",
        "message": "Add MedQuad as a usable dataset to train and evaluate models"
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/data_generator.py",
        "commit_date": "2020-09-13T08:04:02Z",
        "message": "Fix pad tokens masking for logits. Add some of the analyses files from the thesis."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/data_generator.py",
        "commit_date": "2020-09-07T02:07:10Z",
        "message": "Minor modifications to ASs2s code"
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/data_generator.py",
        "commit_date": "2020-09-02T09:03:56Z",
        "message": "Finish data pipeline for final SQuAD + Rewrites dataset  with Ass2s and QGPen."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/data_generator.py",
        "commit_date": "2020-08-25T02:01:34Z",
        "message": "Allow to mix synthetic and organic data during training. Add support for SQuAD triples and SQuAD mapped triples."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/data_generator.py",
        "commit_date": "2020-08-21T03:49:17Z",
        "message": "Add \"origin probability distribution\" to tell if a word shall be predicted, copied from the base question or copied from one of the facts. Split up facts into fact sentences and run them through the encoder individually."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/data_generator.py",
        "commit_date": "2020-08-19T07:53:45Z",
        "message": "Add option to train a certain number of epochs on the synthetic dataset and a certain number of epochs on the Amazon Turk one. Modify benchmark to compare each prediction with all possible references for that prediction."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/data_generator.py",
        "commit_date": "2020-08-15T09:13:14Z",
        "message": "Add base question encoder and use hidden states of encoder in place of embeddings across the model."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/data_generator.py",
        "commit_date": "2020-08-12T02:08:29Z",
        "message": "Implement POS and NE tag features. Fix attention mechanism by computing softmax on correct axis."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/data_generator.py",
        "commit_date": "2020-07-23T03:17:01Z",
        "message": "Merge branch 'rl_model_squad' into dev\n\n# Conflicts:\n#\t.idea/deployment.xml\n#\tdata_processing/data_generator.py\n#\tdefs.py\n#\tevaluating/model_benchmark.py"
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/data_generator.py",
        "commit_date": "2020-07-13T02:11:29Z",
        "message": "Modify Repeat Q data processing pipeline to work with the new format"
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/data_generator.py",
        "commit_date": "2020-07-03T03:22:11Z",
        "message": "Prototype of our RL network but without the RL part (for now trains like a regular supervised network)"
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/data_generator.py",
        "commit_date": "2020-06-24T10:55:13Z",
        "message": "Build data pipeline for RepeatQ. Implement embedding layer."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/data_generator.py",
        "commit_date": "2020-06-22T06:43:32Z",
        "message": "Fix a few pieces of code for SQG to be able to train the model. Results are very bad, there has to be something wrong somewhere."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/data_generator.py",
        "commit_date": "2020-06-15T10:43:58Z",
        "message": "Fix some bugs and model inconsistencies with ASs2s + fix its data preprocessing scripts."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/data_generator.py",
        "commit_date": "2020-06-10T06:47:47Z",
        "message": "Multiple fixes  in eval mode of ASs2s."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/data_generator.py",
        "commit_date": "2020-05-28T10:58:18Z",
        "message": "Manage to make predictions for SQuAD using HotpotQA pretrained model. Answer zones seem to not be working."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/data_generator.py",
        "commit_date": "2020-05-18T18:11:23Z",
        "message": "Integrate GDP2 as a model. Add evaluation and testing scripts for different datasets."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/data_generator.py",
        "commit_date": "2020-05-05T07:08:42Z",
        "message": "Add possibility to generate the NQG dataset with most up to date NER or the original tags. Clean up seq2seq."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/data_generator.py",
        "commit_date": "2020-05-04T08:51:39Z",
        "message": "Add handmade medical dataset and add testing scripts for both this latter and MedQuad."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/data_generator.py",
        "commit_date": "2020-05-01T07:44:12Z",
        "message": "Add MedQuad as a usable dataset to train and evaluate models"
    },
    {
        "repo_url": "github.com/thunlp/OpenBackdoor",
        "filepath": "openbackdoor/attackers/poisoners/trojanlm_poisoner.py",
        "commit_date": "2022-08-14T00:04:23Z",
        "message": "dmodify ripple_config and sos_poisoner"
    },
    {
        "repo_url": "github.com/thunlp/OpenBackdoor",
        "filepath": "openbackdoor/attackers/poisoners/trojanlm_poisoner.py",
        "commit_date": "2022-08-10T12:27:29Z",
        "message": "doc"
    },
    {
        "repo_url": "github.com/thunlp/OpenBackdoor",
        "filepath": "openbackdoor/attackers/poisoners/trojanlm_poisoner.py",
        "commit_date": "2022-08-10T12:17:34Z",
        "message": "update doc"
    },
    {
        "repo_url": "github.com/thunlp/OpenBackdoor",
        "filepath": "openbackdoor/attackers/poisoners/trojanlm_poisoner.py",
        "commit_date": "2022-08-08T16:47:02Z",
        "message": "update poisoner"
    },
    {
        "repo_url": "github.com/thunlp/OpenBackdoor",
        "filepath": "openbackdoor/attackers/poisoners/trojanlm_poisoner.py",
        "commit_date": "2022-08-06T15:15:40Z",
        "message": "fix bugs"
    },
    {
        "repo_url": "github.com/thunlp/OpenBackdoor",
        "filepath": "openbackdoor/attackers/poisoners/trojanlm_poisoner.py",
        "commit_date": "2022-08-05T09:44:12Z",
        "message": "fix bugs"
    },
    {
        "repo_url": "github.com/thunlp/OpenBackdoor",
        "filepath": "openbackdoor/attackers/poisoners/trojanlm_poisoner.py",
        "commit_date": "2022-04-21T11:53:18Z",
        "message": "fix trojanlm, por, neuba"
    },
    {
        "repo_url": "github.com/thunlp/OpenBackdoor",
        "filepath": "openbackdoor/attackers/poisoners/trojanlm_poisoner.py",
        "commit_date": "2022-04-07T06:06:28Z",
        "message": "fix trojanlm"
    },
    {
        "repo_url": "github.com/thunlp/OpenBackdoor",
        "filepath": "openbackdoor/attackers/poisoners/trojanlm_poisoner.py",
        "commit_date": "2022-04-02T14:52:09Z",
        "message": "fix bugs"
    },
    {
        "repo_url": "github.com/thunlp/OpenBackdoor",
        "filepath": "openbackdoor/attackers/poisoners/trojanlm_poisoner.py",
        "commit_date": "2022-03-20T13:28:42Z",
        "message": "add neuba and trojanlm"
    },
    {
        "repo_url": "github.com/thunlp/OpenBackdoor",
        "filepath": "openbackdoor/attackers/poisoners/trojanlm_poisoner.py",
        "commit_date": "2022-01-09T11:52:26Z",
        "message": "update ep sos"
    },
    {
        "repo_url": "github.com/jackfromeast/CyberCoref",
        "filepath": "dataLoader.py",
        "commit_date": "2022-06-20T03:11:55Z",
        "message": "for gold types"
    },
    {
        "repo_url": "github.com/jackfromeast/CyberCoref",
        "filepath": "dataLoader.py",
        "commit_date": "2022-06-20T01:35:33Z",
        "message": "update 0620"
    },
    {
        "repo_url": "github.com/utopia-group/WebQA",
        "filepath": "lib/nlp/nlp.py",
        "commit_date": "2021-06-07T00:57:19Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/misc.py",
        "commit_date": "2023-11-20T14:22:05Z",
        "message": "importlib_resources for Python<3.9"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/misc.py",
        "commit_date": "2023-11-20T04:20:39Z",
        "message": "clean up tests; fix phonetic()"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/misc.py",
        "commit_date": "2021-04-22T22:32:47Z",
        "message": "debug action"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/misc.py",
        "commit_date": "2021-04-22T03:49:50Z",
        "message": "use gzip to avoid git-lfs"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/misc.py",
        "commit_date": "2021-04-13T16:37:25Z",
        "message": "for reals"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/misc.py",
        "commit_date": "2021-04-13T09:07:43Z",
        "message": "modern structure/tools (src/ and setup.cfg/pyproject.toml)"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/sentence.py",
        "commit_date": "2023-11-20T04:20:39Z",
        "message": "clean up tests; fix phonetic()"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/sentence.py",
        "commit_date": "2023-08-04T04:34:48Z",
        "message": "to_html()"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/sentence.py",
        "commit_date": "2022-09-18T01:16:06Z",
        "message": "don\u2019t attempt to re-download punkt tokenizer (#50)\n\n* don\u2019t attempt to re-download punkt tokenizer\n\nPreviously an attempt was made to redownload `punkt` with each request for the in-use tokenizer, causing nltk to emit warnings about `punkt` already being downloaded.  This change attempts to find `punkt` and if not present to _then_ download it.\n\n* Update sentence.py\n\nCo-authored-by: Rob Reynolds <reynoldsnlp@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/sentence.py",
        "commit_date": "2021-08-20T22:33:51Z",
        "message": "code formatting"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/sentence.py",
        "commit_date": "2021-06-08T16:38:28Z",
        "message": "force utf8 for cg3"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/sentence.py",
        "commit_date": "2021-05-26T15:36:16Z",
        "message": "to_json()"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/sentence.py",
        "commit_date": "2021-04-22T20:51:04Z",
        "message": "fix pmhfst path"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/sentence.py",
        "commit_date": "2021-04-13T09:07:43Z",
        "message": "modern structure/tools (src/ and setup.cfg/pyproject.toml)"
    },
    {
        "repo_url": "github.com/nju-websoft/AdaLoGN",
        "filepath": "utils/data_utils_preprocess.py",
        "commit_date": "2022-03-16T08:23:01Z",
        "message": "code update"
    },
    {
        "repo_url": "github.com/jinpeng01/AIG_CL",
        "filepath": "src/prepro/data_builder.py",
        "commit_date": "2022-03-04T16:27:59Z",
        "message": "upload"
    },
    {
        "repo_url": "github.com/jinpeng01/AIG_CL",
        "filepath": "graph_construction/graph_construction.py",
        "commit_date": "2022-03-05T03:00:47Z",
        "message": "second commit"
    },
    {
        "repo_url": "github.com/text-processing/prose-rhythm-detector",
        "filepath": "app/models/document_parser.py",
        "commit_date": "2020-11-18T14:33:42Z",
        "message": "Add the new version of ProseRhythmDetector"
    },
    {
        "repo_url": "github.com/text-processing/prose-rhythm-detector",
        "filepath": "app/models/document_loader.py",
        "commit_date": "2020-11-18T14:33:42Z",
        "message": "Add the new version of ProseRhythmDetector"
    },
    {
        "repo_url": "github.com/text-processing/prose-rhythm-detector",
        "filepath": "app/statistic_utils/PoS_ngram.py",
        "commit_date": "2022-05-13T13:43:59Z",
        "message": "Update PoS_ngram.py"
    },
    {
        "repo_url": "github.com/text-processing/prose-rhythm-detector",
        "filepath": "app/statistic_utils/PoS_ngram.py",
        "commit_date": "2022-05-12T10:35:00Z",
        "message": "Add PoS count utils"
    },
    {
        "repo_url": "github.com/text-processing/prose-rhythm-detector",
        "filepath": "app/statistic_utils/PoS_count.py",
        "commit_date": "2022-05-13T13:43:45Z",
        "message": "Update PoS_count.py"
    },
    {
        "repo_url": "github.com/text-processing/prose-rhythm-detector",
        "filepath": "app/statistic_utils/PoS_count.py",
        "commit_date": "2022-05-12T10:35:00Z",
        "message": "Add PoS count utils"
    },
    {
        "repo_url": "github.com/text-processing/prose-rhythm-detector",
        "filepath": "app/statistic_utils/statistic_util.py",
        "commit_date": "2020-11-18T14:33:42Z",
        "message": "Add the new version of ProseRhythmDetector"
    },
    {
        "repo_url": "github.com/text-processing/prose-rhythm-detector",
        "filepath": "app/statistic_utils/statistic_util.py",
        "commit_date": "2020-06-05T13:52:49Z",
        "message": "Add classification"
    },
    {
        "repo_url": "github.com/text-processing/prose-rhythm-detector",
        "filepath": "app/statistic_utils/statistic_util.py",
        "commit_date": "2020-02-24T16:05:52Z",
        "message": "Add the license to all files"
    },
    {
        "repo_url": "github.com/pmchozas/termitup",
        "filepath": "modules_api/postprocess.py",
        "commit_date": "2021-07-04T16:09:25Z",
        "message": "fixed non-term pattern deletion + updated req"
    },
    {
        "repo_url": "github.com/pmchozas/termitup",
        "filepath": "modules_api/postprocess.py",
        "commit_date": "2021-03-04T16:31:00Z",
        "message": "added postproc"
    },
    {
        "repo_url": "github.com/pmchozas/termitup",
        "filepath": "modules_api/postprocess.py",
        "commit_date": "2021-03-04T16:30:17Z",
        "message": "Delete postprocess.py"
    },
    {
        "repo_url": "github.com/pmchozas/termitup",
        "filepath": "modules_api/postprocess.py",
        "commit_date": "2021-02-17T11:49:09Z",
        "message": "tag verb"
    },
    {
        "repo_url": "github.com/pmchozas/termitup",
        "filepath": "modules_api/postprocess.py",
        "commit_date": "2021-02-15T11:42:24Z",
        "message": "tags"
    },
    {
        "repo_url": "github.com/pmchozas/termitup",
        "filepath": "modules_api/postprocess.py",
        "commit_date": "2021-02-12T16:21:54Z",
        "message": "corenlp server updated"
    },
    {
        "repo_url": "github.com/pmchozas/termitup",
        "filepath": "modules_api/postprocess.py",
        "commit_date": "2021-02-09T11:42:19Z",
        "message": "corenlp server updated"
    },
    {
        "repo_url": "github.com/pmchozas/termitup",
        "filepath": "modules_api/postprocess.py",
        "commit_date": "2020-10-16T13:18:49Z",
        "message": "bug fixed"
    },
    {
        "repo_url": "github.com/pmchozas/termitup",
        "filepath": "modules_api/postprocess.py",
        "commit_date": "2020-10-15T17:59:25Z",
        "message": "changes in swagger methods"
    },
    {
        "repo_url": "github.com/pmchozas/termitup",
        "filepath": "modules_api/postprocess.py",
        "commit_date": "2020-10-14T17:56:23Z",
        "message": "many changes with small success"
    },
    {
        "repo_url": "github.com/pmchozas/termitup",
        "filepath": "modules_api/postprocess.py",
        "commit_date": "2020-09-14T14:26:13Z",
        "message": "API advances"
    },
    {
        "repo_url": "github.com/pmchozas/termitup",
        "filepath": "modules_api/delete_patterns_flair.py",
        "commit_date": "2021-07-04T16:09:25Z",
        "message": "fixed non-term pattern deletion + updated req"
    },
    {
        "repo_url": "github.com/amazon-science/masked-diffusion-lm",
        "filepath": "improved-diffusion/scripts/infill.py",
        "commit_date": "2023-04-10T16:56:50Z",
        "message": "Upload code"
    },
    {
        "repo_url": "github.com/amazon-science/masked-diffusion-lm",
        "filepath": "improved-diffusion/control_gen/eval_control.py",
        "commit_date": "2023-04-10T16:56:50Z",
        "message": "Upload code"
    },
    {
        "repo_url": "github.com/amazon-science/masked-diffusion-lm",
        "filepath": "transformers/examples/pytorch/language-modeling/run_clm.py",
        "commit_date": "2023-04-10T16:56:50Z",
        "message": "Upload code"
    },
    {
        "repo_url": "github.com/luohongyin/RGX",
        "filepath": "rgx_doc.py",
        "commit_date": "2022-06-29T15:30:47Z",
        "message": "huggingface hub and demo"
    },
    {
        "repo_url": "github.com/luohongyin/RGX",
        "filepath": "rgx_doc.py",
        "commit_date": "2022-06-28T22:12:23Z",
        "message": "stanza download"
    },
    {
        "repo_url": "github.com/luohongyin/RGX",
        "filepath": "rgx_doc.py",
        "commit_date": "2022-06-27T23:16:10Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/luohongyin/RGX",
        "filepath": "search_rgx.py",
        "commit_date": "2022-06-27T23:16:10Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/luohongyin/RGX",
        "filepath": "aer_ext_deptree.py",
        "commit_date": "2022-06-28T01:49:29Z",
        "message": "fix aer"
    },
    {
        "repo_url": "github.com/luohongyin/RGX",
        "filepath": "aer_ext_deptree.py",
        "commit_date": "2022-06-27T23:16:10Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2022-05-14T09:21:27Z",
        "message": "Use the nltk tokenizer for libraries that don't have their own"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2022-05-14T09:09:50Z",
        "message": "Added Raudikko"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2022-05-03T18:13:22Z",
        "message": "Rename all models as lower case for consistency"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2022-05-03T17:13:26Z",
        "message": "More surface form fixes"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2022-05-02T18:54:20Z",
        "message": "simplemma surface forms"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2022-05-02T18:48:02Z",
        "message": "Update to spacy 3.3.0"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2022-05-02T18:32:51Z",
        "message": "Merge branch 'spacy3.3'"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2022-05-02T18:22:07Z",
        "message": "Fix surface forms by inserting multi-word tokens"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2022-05-02T17:16:33Z",
        "message": "Separate predict and evaluate steps"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2022-05-01T17:47:57Z",
        "message": "Switch to the official CoNLL18 evaluation script\n\nTODO: Some models are broken"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2022-04-18T18:02:49Z",
        "message": "Trankit large"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2022-04-14T16:50:43Z",
        "message": "Write the output in the CONLL-U format"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2022-04-09T08:42:45Z",
        "message": "Updated the experimental spacy model to be compatbile with spacy 3.3.0"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2022-04-08T14:43:37Z",
        "message": "Spacy 3.3.0.dev0"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2022-04-03T17:25:59Z",
        "message": "Run the Turku pipeline as a server"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2022-04-03T09:13:24Z",
        "message": "Move the Turku data out of the checkout directory"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2022-04-03T09:04:57Z",
        "message": "Turku pipeline as a git submodule"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2022-04-03T09:01:27Z",
        "message": "FinnPos as git submodule"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2022-04-03T08:51:32Z",
        "message": "move Stanza and Trankit to the models directory"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2022-04-03T08:47:52Z",
        "message": "UralicNLP POS"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2022-04-02T16:28:21Z",
        "message": "UralicNLP, only lemmatization"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2022-04-02T15:48:19Z",
        "message": "simplemma"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2022-04-02T13:33:07Z",
        "message": "trankit"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2022-04-02T12:09:38Z",
        "message": "Updated Turku neural pipeline"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2022-04-02T06:39:14Z",
        "message": "Remove spacy_stanfordnlp, add Stanza"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2022-04-01T17:30:46Z",
        "message": "Updated models. Optionally select models to evaluate\n\nUpdated spacy, spacy_fi_experimental_web_md and spacy-udpipe.\n\nDisabled spacy-stanfordnlp, because it's not supported on recent spacy versions anymore."
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2020-01-19T16:30:07Z",
        "message": "Warmup the Turku parser"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2020-01-19T16:09:38Z",
        "message": "Merge sentences incorrectly split by the Turku parser"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2020-01-19T15:52:13Z",
        "message": "Let the models do the tokenization"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2020-01-19T15:10:48Z",
        "message": "Process the whole dataset in one batch"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2020-01-14T18:06:14Z",
        "message": "spacy-fi"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2019-11-02T19:03:56Z",
        "message": "FinnPos"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2019-11-02T17:33:24Z",
        "message": "Fix sentence alignment. WER, F1-score and aligned accuracy metrics"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2019-10-30T20:01:17Z",
        "message": "Only use sudo if configured. Explain the need for sudo on README"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2019-10-30T19:32:31Z",
        "message": "Run the Turku-neural-parser-pipeline on a Docker"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2019-10-29T19:18:39Z",
        "message": "Analyze with Turku neural pipeline"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2019-10-28T18:03:40Z",
        "message": "Move stanfordnlp resources under data"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2019-10-27T17:02:54Z",
        "message": "Refactor Voikko analyzer"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "nlpmodels.py",
        "commit_date": "2019-10-27T16:50:40Z",
        "message": "Rename file"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "download_models.py",
        "commit_date": "2022-05-02T18:48:02Z",
        "message": "Update to spacy 3.3.0"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "download_models.py",
        "commit_date": "2022-04-18T18:02:49Z",
        "message": "Trankit large"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "download_models.py",
        "commit_date": "2022-04-03T08:51:32Z",
        "message": "move Stanza and Trankit to the models directory"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "download_models.py",
        "commit_date": "2022-04-02T16:28:21Z",
        "message": "UralicNLP, only lemmatization"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "download_models.py",
        "commit_date": "2022-04-02T13:33:07Z",
        "message": "trankit"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "download_models.py",
        "commit_date": "2022-04-02T06:39:14Z",
        "message": "Remove spacy_stanfordnlp, add Stanza"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "download_models.py",
        "commit_date": "2019-10-28T18:03:40Z",
        "message": "Move stanfordnlp resources under data"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "download_models.py",
        "commit_date": "2019-10-27T16:49:41Z",
        "message": "spacy-stanfordnlp"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "download_models.py",
        "commit_date": "2019-10-27T16:28:10Z",
        "message": "udpipe Finnish FTB model"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "download_models.py",
        "commit_date": "2019-10-27T13:19:19Z",
        "message": "udpipe evaluation"
    },
    {
        "repo_url": "github.com/ShushanArakelyan/modular_code_search",
        "filepath": "layout_assembly/layout_w_orig_verbs.py",
        "commit_date": "2021-12-18T17:56:10Z",
        "message": "add option to finetune scoring module in layout net"
    },
    {
        "repo_url": "github.com/ShushanArakelyan/modular_code_search",
        "filepath": "layout_assembly/layout_w_orig_verbs.py",
        "commit_date": "2021-12-08T05:51:08Z",
        "message": "fix extra new line at the end"
    },
    {
        "repo_url": "github.com/ShushanArakelyan/modular_code_search",
        "filepath": "layout_assembly/layout_w_orig_verbs.py",
        "commit_date": "2021-11-01T03:34:54Z",
        "message": "removing eval param from constructors and adding action v7"
    },
    {
        "repo_url": "github.com/ShushanArakelyan/modular_code_search",
        "filepath": "layout_assembly/layout_w_orig_verbs.py",
        "commit_date": "2021-10-31T22:55:07Z",
        "message": "adding models with batching and different experiment code"
    },
    {
        "repo_url": "github.com/VPeterV/Structured-MFVI",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2024-01-05T10:39:05Z",
        "message": "Structured MFVI"
    },
    {
        "repo_url": "github.com/Bibliome/alvisnlp",
        "filepath": "alvisnlp-bibliome/src/main/resources/fr/inra/maiage/bibliome/alvisnlp/bibliomefactory/modules/python/stanza-alvisnlp.py",
        "commit_date": "2022-09-02T11:56:42Z",
        "message": "fixed Stanza, crashed when constituency param was not set"
    },
    {
        "repo_url": "github.com/Bibliome/alvisnlp",
        "filepath": "alvisnlp-bibliome/src/main/resources/fr/inra/maiage/bibliome/alvisnlp/bibliomefactory/modules/python/stanza-alvisnlp.py",
        "commit_date": "2022-07-20T16:34:09Z",
        "message": "added parameter constituency to Stanza"
    },
    {
        "repo_url": "github.com/Bibliome/alvisnlp",
        "filepath": "alvisnlp-bibliome/src/main/resources/fr/inra/maiage/bibliome/alvisnlp/bibliomefactory/modules/python/stanza-alvisnlp.py",
        "commit_date": "2022-06-15T15:26:34Z",
        "message": "added Stanza module"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-11-13T12:39:27Z",
        "message": "Changed default mongoDB & remove unused method in analysis"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-11-11T16:06:38Z",
        "message": "Similarity now returns -1 instead of raising an error. The error should be handled in the frontend."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-11-11T14:38:11Z",
        "message": "Removed an unnecessary parameter & fixed token counting"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-10-30T10:41:07Z",
        "message": "Merge branch 'multi-processing' into master"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-10-22T16:25:36Z",
        "message": "Removed deprecated keywords"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-10-21T17:39:52Z",
        "message": "Api calls now respect default values correctly (done by **kwargs mapping).\nSome api keywords have been changed to match the function keywords of analysis.py.\nFixed some minor bugs in analysis & api.\nTested all api functions."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-10-13T12:20:25Z",
        "message": "Added two different execution queues for celery (small & big corpora).\nAdded configuration file:\nServer mode (=> use celery task queue or not)\nMongoDB constants\nAnalysis threads\nCelery configuration"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-10-11T12:47:13Z",
        "message": "Added a self-compiled version of blackstone.\nThe project is now compatible with spaCy 2.1.9.\nUpdated requirments.txt\nExchanged a linux-only cpu core determiner function with a cross-platform one (however this function does not account for artifical cpu restrictions of the process!)."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-10-09T16:09:01Z",
        "message": "Added filtering for part of speech tags."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-10-07T15:26:11Z",
        "message": "Added multi-threaded pipeline execution.\nCelex numbers are now part of the corpus (get via get_celex_numbers).\nAdjusted api keywords and fixed none type errors in api_functions for _per_doc functions."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-30T17:23:31Z",
        "message": "Deleted duplicate functions from analysis (e.g. word & token count use the same logic with other params).\nUpdated api functions accordingly.\nAdded initalisation of analysers to flask start up."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-29T11:37:55Z",
        "message": "Added n_grams_per_doc.\nAdded imports for model packages (so python throws an error, when they are not correctly installed)\nUpdated pre-processing\nAdded stanza downloading every setup (no other way currently known to download them via pip)"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-28T15:02:46Z",
        "message": "Adjusted api functions to work with modified analysis."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-25T13:29:49Z",
        "message": "Generalized CorpusAnalysis class to also work for single documents.\nRemoved obsolete Analysis class.\nAdded detailed Python docstrings to public functions."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-25T10:07:25Z",
        "message": "Added sentiment calculation for a whole corpus.\nAdded readbility score for each document in a corpus.\nCode clean up."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-22T17:52:01Z",
        "message": "Added compound cases to NER detection"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-22T15:36:46Z",
        "message": "Added sentiment analysis to single documents.\nImproved pre-processing with another RegEx for parentheses.\nAdded compound cases to NER detection for english texts.\nAdded legal text sentencer to german pipeline.\nShould have fixed problems with sentence segementer being already present."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-21T14:55:12Z",
        "message": "Generalized get_tokens to also account for various part of speech tags & frequency.\nFixed a bug, where empty tokens haven't been removed.\nFixed n_grams for CorpusAnalysis.\nAdded key word extraction for single documents."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-19T18:01:13Z",
        "message": "Added a new regex to remove old headers.\nFixed some typos"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-16T16:49:48Z",
        "message": "Removed deprecated files.\nAdded sentence counting."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-16T16:23:50Z",
        "message": "Added _per_doc functions for corpus analysis:\n- tokens\n- sentences\n- part of speech tags\n- lemmata\n- named entities\nFixed list comprehention errors."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-15T19:54:23Z",
        "message": "Added n-grams for single documents.\nAdded a class for corpus representation and analysis (n_doc > 1)"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-14T22:06:38Z",
        "message": "Rewrote readbility scores.\nAdded working lda implementation for a corpus."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-14T14:10:44Z",
        "message": "Rewrote the implementation which is now based on another library (old were too slow).\nAdded document similarity based on word-embeddings (aka word2vec).\nAdditionally added more sophisticated preprocessing (mostly regex):\n- older texts are now getting tokenized better\n- headlines of all older formats (until 2003) are getting removed\n- all paragraphs numbers are getting removed correctly (except for lines with in-text headlines)"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-02T13:44:32Z",
        "message": "Added german lemmatisation and pos tagging"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-02T12:29:20Z",
        "message": "Added text complexity & basic LDA"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-01T16:23:08Z",
        "message": "Added most frequent words & avg token length"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-01T15:46:08Z",
        "message": "Added tokenisation, lemmatisation & POS tagging"
    },
    {
        "repo_url": "github.com/microsoft/presidio-research",
        "filepath": "presidio_evaluator/models/stanza_model.py",
        "commit_date": "2023-12-27T19:45:04Z",
        "message": "refresh to various things"
    },
    {
        "repo_url": "github.com/microsoft/presidio-research",
        "filepath": "presidio_evaluator/models/stanza_model.py",
        "commit_date": "2022-01-19T22:04:18Z",
        "message": "black and flake8-ing the entire code"
    },
    {
        "repo_url": "github.com/microsoft/presidio-research",
        "filepath": "presidio_evaluator/models/stanza_model.py",
        "commit_date": "2022-01-14T22:42:14Z",
        "message": "updates to notebooks and some evaluation logic, experiment tracking"
    },
    {
        "repo_url": "github.com/microsoft/presidio-research",
        "filepath": "presidio_evaluator/models/stanza_model.py",
        "commit_date": "2022-01-08T00:30:23Z",
        "message": "more updates"
    },
    {
        "repo_url": "github.com/shunk031/training-free-structured-diffusion-guidance",
        "filepath": "tfsdg/pipelines/tfsdg_pipeline.py",
        "commit_date": "2022-10-29T08:38:48Z",
        "message": "Add tensor shape (#7)\n\n* add tensor shape\n\n* update README"
    },
    {
        "repo_url": "github.com/shunk031/training-free-structured-diffusion-guidance",
        "filepath": "tfsdg/pipelines/tfsdg_pipeline.py",
        "commit_date": "2022-10-10T08:59:04Z",
        "message": "Initialize (#1)\n\n* add README.md\n\n* add poetry files\n\n* add files for tfsdg library\n\n* add files for tests\n\n* add .gitignore\n\n* add settings for CI\n\n* fix branch name in the CI file"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_main.py",
        "commit_date": "2024-01-11T11:31:01Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI;\n3. fixed wrong warning in SVO GUI for gender and quote annotators;\n4. added Sankey charts to SVO output."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_main.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_main.py",
        "commit_date": "2023-11-27T12:58:43Z",
        "message": "1. fixed a bug in the creation of a wordcloud with an empty input file;\n2. added an Organizations column in the SVO output;\n3. improved Stanford CoreNLP SVO using entitymentions values for Subjects and Objects;\n4. introduced a new GUI for a pipeline of data quality algorithms (file_checker_pre_processing_pipeline_main.py)."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_main.py",
        "commit_date": "2023-11-25T11:01:41Z",
        "message": "1. fixed a bug in the creation of a wordcloud with an empty input file."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_main.py",
        "commit_date": "2023-11-17T13:59:01Z",
        "message": "1. added CLOSE button to coref results and passed the right file back to SVO when doing manual coreferencing."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_main.py",
        "commit_date": "2023-11-16T20:59:23Z",
        "message": "1. fixed a bug with Plotly capitalized."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_main.py",
        "commit_date": "2023-10-22T18:52:05Z",
        "message": "1. added SVO_main.py."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_main.py",
        "commit_date": "2023-10-20T00:21:16Z",
        "message": "1. hapax"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_main.py",
        "commit_date": "2023-10-07T20:12:07Z",
        "message": "1. fixed bugs in the export of files for the SVO algorithm."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_main.py",
        "commit_date": "2023-06-14T16:08:15Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_main.py",
        "commit_date": "2023-06-14T07:27:54Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_main.py",
        "commit_date": "2023-06-10T18:01:13Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_main.py",
        "commit_date": "2023-06-04T17:32:29Z",
        "message": "1. rewrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI;\n4. added the chart visualization to BERT NER;\n5. uniformed the output layout of spaCy NER to all other NER packages;\n6. fixed a bug in reminders;\n7 fixed bugs in the opening of output files in the CoNLL_table_analyzer_main GUI;\n8. fixed bugs in the statistics_csv_main GUIs;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_main.py",
        "commit_date": "2023-05-30T16:00:54Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI;\n4. added the chart visualization to BERT NER;\n5. uniformed the output layout of spaCy NER to all other NER packages;\n6. fixed a bug in reminders;\n7 fixed bugs in the opening of output files in the CoNLL_table_analyzer_main GUI;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_main.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_util.py",
        "commit_date": "2024-01-11T11:31:01Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI;\n3. fixed wrong warning in SVO GUI for gender and quote annotators;\n4. added Sankey charts to SVO output."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_util.py",
        "commit_date": "2023-11-27T12:58:43Z",
        "message": "1. fixed a bug in the creation of a wordcloud with an empty input file;\n2. added an Organizations column in the SVO output;\n3. improved Stanford CoreNLP SVO using entitymentions values for Subjects and Objects;\n4. introduced a new GUI for a pipeline of data quality algorithms (file_checker_pre_processing_pipeline_main.py)."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_util.py",
        "commit_date": "2023-11-27T12:12:33Z",
        "message": "1. fixed a bug in the creation of a wordcloud with an empty input file;\n2. added an Organizations column in the SVO output;\n3. improved Stanford CoreNLP SVO using entitymentions values for Subjects and Objects;\n4. introduced a new GUI for a pipeline of data quality algorithms (file_checker_pre_processing_pipeline_main.py)."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_util.py",
        "commit_date": "2023-11-25T11:01:41Z",
        "message": "1. fixed a bug in the creation of a wordcloud with an empty input file."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_util.py",
        "commit_date": "2023-11-18T20:50:39Z",
        "message": "1. bugs in search functions"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_util.py",
        "commit_date": "2023-11-17T13:59:01Z",
        "message": "1. added CLOSE button to coref results and passed the right file back to SVO when doing manual coreferencing."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_util.py",
        "commit_date": "2023-11-17T02:48:00Z",
        "message": "1. fixed bug in verb analysis in the CoNLL table analyzer;\n2. improved the layout of the  NGrams_CoOccurrences_main GUI;\n3. improved the user interaction for NGrams_CoOccurrences_main GUI;\n4. fixed a bug in SVO for GIS information;\n5. improved the display of GIS information in SVO to take into account multi-word locations;\n6. improved the handling of NER PERSON in SVO;\n7. changed the default display of wordcloud setting collocation to False."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_util.py",
        "commit_date": "2023-11-16T20:59:23Z",
        "message": "1. fixed a bug with Plotly capitalized."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_util.py",
        "commit_date": "2023-10-07T20:12:07Z",
        "message": "1. fixed bugs in the export of files for the SVO algorithm."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_util.py",
        "commit_date": "2023-10-05T00:06:18Z",
        "message": "1. fixed a bug with Gephi;\n2. fixed bugs in the function statistics_csv_util.compute_csv_column_frequencies with no groups;\n3. fixed the wrong display of Infinitive verbs frequency."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_util.py",
        "commit_date": "2023-09-30T15:43:33Z",
        "message": "1. fixed bugs in the creation of charts for document statistics and other charts;\n2. improved the output layout for sentiment analysis algorithms."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_util.py",
        "commit_date": "2023-08-09T16:58:34Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_util.py",
        "commit_date": "2023-06-14T16:08:15Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_util.py",
        "commit_date": "2023-06-12T12:59:48Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_util.py",
        "commit_date": "2023-06-10T18:01:13Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SVO_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/BERT_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/BERT_util.py",
        "commit_date": "2023-09-29T16:49:39Z",
        "message": "1. fixed an inconsequential bug in the export of sentiment analysis scores for Stanza;\n2. fixed a bug in the export of charts for sentiment analysis scores."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/BERT_util.py",
        "commit_date": "2023-09-21T12:05:01Z",
        "message": "1. uniformed the width of Open file/dictionary buttons in all GUIs;\n2. improved the layout of the html_gender_annotator GUI;\n3. fixed a bug with language selection in Stanza."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/BERT_util.py",
        "commit_date": "2023-06-12T12:59:48Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/BERT_util.py",
        "commit_date": "2023-06-10T18:01:13Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/BERT_util.py",
        "commit_date": "2023-05-30T16:00:54Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI;\n4. added the chart visualization to BERT NER;\n5. uniformed the output layout of spaCy NER to all other NER packages;\n6. fixed a bug in reminders;\n7 fixed bugs in the opening of output files in the CoNLL_table_analyzer_main GUI;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/BERT_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SENNA_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SENNA_util.py",
        "commit_date": "2023-05-30T16:00:54Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI;\n4. added the chart visualization to BERT NER;\n5. uniformed the output layout of spaCy NER to all other NER packages;\n6. fixed a bug in reminders;\n7 fixed bugs in the opening of output files in the CoNLL_table_analyzer_main GUI;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SENNA_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2024-03-01T20:58:47Z",
        "message": "1. added headers to MALLET output csv files;\n2. fixed bugs with docx to txt converter."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2024-02-27T02:03:36Z",
        "message": "1. added headers to MALLET output csv files."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2024-01-29T11:32:14Z",
        "message": "1. fixed a bug in a function call in the file_checker_converter_cleaner_main due to wrong number of parameters passed."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2024-01-17T14:35:37Z",
        "message": "1. edited the HELP? and hover-over info for normalization and data transformation. Both options are now available.\n2. added Mac and Windows readme beta versions to help with installations."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2024-01-13T10:10:58Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI;\n3. fixed wrong warning in SVO GUI for gender and quote annotators;\n4. added Sankey charts to SVO output."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2024-01-11T11:31:01Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI;\n3. fixed wrong warning in SVO GUI for gender and quote annotators;\n4. added Sankey charts to SVO output."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-11-12T22:45:35Z",
        "message": "1. fixed a bug with Plotly capitalized."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-11-12T15:31:40Z",
        "message": "1. added a check on the number of variables required by the various Excel/plotly charts;\n2. improved the layout of the data_visualization_2_main GUI;\n3. improved the user interaction in dealing with charts with improved dropdown menus."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-11-10T18:42:56Z",
        "message": "1. fixed various bugs in the search functions;\n2. added the option of filtering data when drawing a sunburst or treemap charts;\n3. uniformed the name of output directories for searches of various types (word, sentence, n-grams);\n4. added the option of computing and visualizing WordNet aggregated lemma values for POS nouns and POS verbs in the CoNLL_table_analyzer;\n5. added the Excel/plotly option in data_visualization_2_main GUI;\n6. added the option of searching for co-occurring words with sentence (rather than document) in NGrams_CoOccurrences_main."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-11-09T23:52:42Z",
        "message": "1. fixed various bugs in the search functions;\n2. added the option of filtering data when drawing a sunburst or treemap charts;\n3. uniformed the name of output directories for searches of various types (word, sentence, n-grams);\n4. added the option of computing and visualizing WordNet aggregated lemma values for POS nouns and POS verbs in the CoNLL_table_analyzer;\n5. added the Excel/plotly option in data_visualization_2_main GUI"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-11-07T11:58:53Z",
        "message": "1. fixed various bugs in the search functions;\n2. added the option of filtering data when drawing a sunburst or treemap charts."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-11-05T01:12:59Z",
        "message": "1. added the option of creating Excel/Plotly charts in the data_visualization_2_main GUI;\n2. fixed bug in N-grams VIEWER GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-11-03T01:41:09Z",
        "message": "1. added the option of users selecting the type of chart t be displayed (bar, line, radar);\n2. fixed bugs with plotLy charts;\n3. fixed bugs in ngrams and file searches when users select the wrong file/search word."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-10-31T13:52:01Z",
        "message": "1. added the exact match in the file_search_byWord functions"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-10-29T10:50:58Z",
        "message": "1. improved the n-grams scripts;\n2. added a line chart to the n-gram search for 1-grams;\n3. corrected the sort in the function  get_data_to_be_plotted_with_counts leaving sorting in the order of the x-axis values."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-10-28T20:52:14Z",
        "message": "1. improved the n-grams scripts;\n2. added a line chart to the n-gram search for 1-grams;\n3. corrected the sort in the function  get_data_to_be_plotted_with_counts leaving sorting in the order of the x-axis values."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-10-28T12:41:58Z",
        "message": "1. improved the n-grams scripts."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-10-27T02:08:00Z",
        "message": "1. completed the work on generalizing the special charts to process any number and types of csv file fields."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-10-26T02:57:04Z",
        "message": "1. completed the work on generalizing the special charts to process any number and types of csv file fields."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-10-25T23:02:06Z",
        "message": "1. Test"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-10-25T16:23:00Z",
        "message": "1. improved both the nominalization_main algorithms and the nominalization TIPS file."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-10-25T03:03:34Z",
        "message": "1. prepared the data_visualization_1_main.py GUI to process multiple selections of fields/search values. TO BE CONTINUED;\n2. added Sankey charts to the n-grams search in NGrams_CoOccurrences_main GUI;\n3. fixed an Excel chart display with MALLET topic modelling;\n4. fixed a bug in style_analysis_main for the hapax_words variable not assigned;\n4. added the colormap chart option to data_visualization1_main;\n5. added Samkey charts to coreference output."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-10-25T01:47:36Z",
        "message": "1. prepared the data_visualization_1_main.py GUI to process multiple selections of fields/search values. TO BE CONTINUED;\n2. added Sankey charts to the n-grams search in NGrams_CoOccurrences_main GUI;\n3. fixed an Excel chart display with MALLET topic modelling;\n4. fixed a bug in style_analysis_main for the hapax_words variable not assigned;\n4. added the colormap chart option to data_visualization1_main."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-10-24T14:19:27Z",
        "message": "1. prepared the data_visualization_1_main.py GUI to process multiple selections of fields/search values. TO BE CONTINUED;\n2. added Sankey charts to the n-grams search in NGrams_CoOccurrences_main GUI;\n3. fixed an Excel chart display with MALLET topic modelling;\n4. fixed a bug in style_analysis_main for the hapax_words variable not assigned;\n4. added the colormap chart option to data_visualization1_main."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-10-05T00:06:18Z",
        "message": "1. fixed a bug with Gephi;\n2. fixed bugs in the function statistics_csv_util.compute_csv_column_frequencies with no groups;\n3. fixed the wrong display of Infinitive verbs frequency."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-10-03T16:48:22Z",
        "message": "1. added a series of checks in the data visualization 2 GUI for Boxplots;\n2. added two updated TIPS files for data visualization;\n3. added a try/except for the Sankey charts."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-09-30T15:43:33Z",
        "message": "1. fixed bugs in the creation of charts for document statistics and other charts;\n2. improved the output layout for sentiment analysis algorithms."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-09-29T21:33:51Z",
        "message": "1. fixed a display problem with charts by group columns (e.g., By Document)."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-09-29T16:49:39Z",
        "message": "1. fixed an inconsequential bug in the export of sentiment analysis scores for Stanza;\n2. fixed a bug in the export of charts for sentiment analysis scores."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-09-28T11:38:02Z",
        "message": "1. Further improved error trapping for the sunburst visualization option;\n2. further improved the layout design of the DB_PCACE_data_analyzer_main GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-09-28T00:53:50Z",
        "message": "1. fixed all bugs in the data_visualization_1_main.py GUI;\n2. fixed bugs with the Gephi_util.py."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-09-27T11:55:47Z",
        "message": "1. trapped user errors in Sunburst and Treemap."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-09-26T02:14:44Z",
        "message": "1. improved STEP2 for Mac;\n2. fixed a display overlap between Document and Sentence in GIS description field;\n3. added 2 TIPS files;\n4. fixed a display overlap in all GUIs with the Release version."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-09-18T11:12:26Z",
        "message": "1. added a series of checks to the NLP_setup_package_language_main.py to avoid setup errors;\n2. Added the POTUS_webscraper based on beautifulsoup to extract POTUS speeches;\n3. fixed bugs in the creation of Excel charts;\n4. fixed bugs in TIPS filenames."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-09-16T18:20:12Z",
        "message": "1. Added the POTUS_webscraper based on beautifulsoup to extract POTUS speeches;\n2.fixed an inconsequential display but in NLP_setup_package_language_main\n3. fixed bugs in the creation of Excel charts;\n4. fixed bugs in TIPS filenames;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-09-10T22:16:07Z",
        "message": "1.improved the user interaction when there are missing IO values;\n2. removed the line import RF_charts_treemaper_util from data_visualization util;\n3. fixed a bug in file_search_byWord_util."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-08-27T20:19:25Z",
        "message": "1. Fixed a bug in NLP_menu_main when selecting the option \"Sample corpus (ALL options GUI)\" under Pre-processing tools;\n2. improved the display of user messages about the RUN button with wrong/missing IO information;\n3. fixed a bug of missing file for timechart in data_visualization_main;\n4. added TIPS file for specialized visuals to the data_visualization GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-08-13T08:49:24Z",
        "message": "Release 3.8.8 reflects a large number of bug fixes and improvements, the result of summer 2023 work on the NLP Suite."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-07-19T13:33:58Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-06-25T14:41:05Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-06-17T08:29:03Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-06-12T12:59:48Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-06-10T18:01:13Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-06-05T20:37:47Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-06-04T17:32:29Z",
        "message": "1. rewrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI;\n4. added the chart visualization to BERT NER;\n5. uniformed the output layout of spaCy NER to all other NER packages;\n6. fixed a bug in reminders;\n7 fixed bugs in the opening of output files in the CoNLL_table_analyzer_main GUI;\n8. fixed bugs in the statistics_csv_main GUIs;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/charts_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2024-02-06T02:24:23Z",
        "message": "1. added a vocab csv output file when computing 1-grams;\n2. fixed potential bug with Stanza not installed in NLP_menu_main when selecting Setup default NLP parsers & annotators...\n3. fixed a bug in data_visualization_main_1 with the Treemap option."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2023-11-21T19:51:41Z",
        "message": "1. fixed a bug with wordcloud visualization when using a csv file in input;\n2. extended the POS NPOUN to include proper nouns in Wordclouds;\n3. added a checkbox to compute corpus statistics by POS (Part of Speech) tag value in the statistics_txt_main GUI;\n4. uniformed the handling of ,'GUIs available for more options' in all GUIs that rely on the checkbox and menu."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2023-10-05T00:06:18Z",
        "message": "1. fixed a bug with Gephi;\n2. fixed bugs in the function statistics_csv_util.compute_csv_column_frequencies with no groups;\n3. fixed the wrong display of Infinitive verbs frequency."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2023-09-30T15:43:33Z",
        "message": "1. fixed bugs in the creation of charts for document statistics and other charts;\n2. improved the output layout for sentiment analysis algorithms."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2023-09-29T16:49:39Z",
        "message": "1. fixed an inconsequential bug in the export of sentiment analysis scores for Stanza;\n2. fixed a bug in the export of charts for sentiment analysis scores."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2023-09-22T13:41:01Z",
        "message": "1. Fixed a minor issue in the parsers_annotators_main with the CoNLL_table_analyzer checkbox state (normal/disabled);\n2. uniformed the MAC & Windows width for OK + Reset Show buttons in all GUIs."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2023-09-21T12:05:01Z",
        "message": "1. uniformed the width of Open file/dictionary buttons in all GUIs;\n2. improved the layout of the html_gender_annotator GUI;\n3. fixed a bug with language selection in Stanza."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2023-06-17T08:29:03Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2023-06-14T07:27:54Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2023-06-12T12:59:48Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2023-06-04T17:32:29Z",
        "message": "1. rewrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI;\n4. added the chart visualization to BERT NER;\n5. uniformed the output layout of spaCy NER to all other NER packages;\n6. fixed a bug in reminders;\n7 fixed bugs in the opening of output files in the CoNLL_table_analyzer_main GUI;\n8. fixed bugs in the statistics_csv_main GUIs;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2023-05-30T16:00:54Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI;\n4. added the chart visualization to BERT NER;\n5. uniformed the output layout of spaCy NER to all other NER packages;\n6. fixed a bug in reminders;\n7 fixed bugs in the opening of output files in the CoNLL_table_analyzer_main GUI;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_util.py",
        "commit_date": "2024-02-01T01:52:25Z",
        "message": "1. fixed a bug in the merge_main.py with wrong number of parameters;\n2. fixed bug with missing TIPS;\n3. fixed a bug with N-grams option for determiners."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_util.py",
        "commit_date": "2023-10-28T20:52:14Z",
        "message": "1. improved the n-grams scripts;\n2. added a line chart to the n-gram search for 1-grams;\n3. corrected the sort in the function  get_data_to_be_plotted_with_counts leaving sorting in the order of the x-axis values."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_util.py",
        "commit_date": "2023-10-28T15:49:40Z",
        "message": "1. improved the n-grams scripts."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_util.py",
        "commit_date": "2023-10-28T15:36:33Z",
        "message": "1. improved the n-grams scripts."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/wordclouds_util.py",
        "commit_date": "2023-11-25T11:01:41Z",
        "message": "1. fixed a bug in the creation of a wordcloud with an empty input file."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/wordclouds_util.py",
        "commit_date": "2023-11-22T09:13:51Z",
        "message": "1. in the SVO swordclouds visualization, set the collocations parameter to False to avoid potential repetition of the same words;\n2. added new options to the dropdown menus in NL_menu_main for statistical tools of textual analysis;\n 3. fixed chart display bugs in the computation of clause, noun, verb, function words in the CoNLL table analyzer;\n4. added computation of overall noun and verb lists and frequencies in the CoNLL table analyzer;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/wordclouds_util.py",
        "commit_date": "2023-11-21T19:51:41Z",
        "message": "1. fixed a bug with wordcloud visualization when using a csv file in input;\n2. extended the POS NPOUN to include proper nouns in Wordclouds;\n3. added a checkbox to compute corpus statistics by POS (Part of Speech) tag value in the statistics_txt_main GUI;\n4. uniformed the handling of ,'GUIs available for more options' in all GUIs that rely on the checkbox and menu."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/wordclouds_util.py",
        "commit_date": "2023-10-12T13:34:06Z",
        "message": "1. added M.K.A. Halliday's high-value, median-value, and low-value classification of modal verbs in CoNLL_verb_analysis_util;\n2. added a wordcloud display to the output of a CoNLL_table_search_util.py."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/wordclouds_util.py",
        "commit_date": "2023-10-11T15:48:06Z",
        "message": "1. In the Style analysis GUI, modified the Vocabulary analysis option for Short words (<4 characters) to compute, instead, the word length for all words in a corpus;\n2. fixed bug in wordclouds_util word_str referenced before assignment."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/wordclouds_util.py",
        "commit_date": "2023-10-08T17:24:19Z",
        "message": "1. fixed a config filename bug in coreference_main.py."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/wordclouds_util.py",
        "commit_date": "2023-09-08T02:05:20Z",
        "message": "1. fixed a bug in wordclouds_main GUI with wrong filename;\n2. fixed a bug with Mac external software installation."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/wordclouds_util.py",
        "commit_date": "2023-06-04T17:32:29Z",
        "message": "1. rewrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI;\n4. added the chart visualization to BERT NER;\n5. uniformed the output layout of spaCy NER to all other NER packages;\n6. fixed a bug in reminders;\n7 fixed bugs in the opening of output files in the CoNLL_table_analyzer_main GUI;\n8. fixed bugs in the statistics_csv_main GUIs;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/wordclouds_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_functions.py",
        "commit_date": "2024-02-07T03:09:41Z",
        "message": "1. fixed bug in file_search_byWord_main when running the case sensitive/insensitive option."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_functions.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_cleaner_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_cleaner_util.py",
        "commit_date": "2023-10-18T10:53:21Z",
        "message": "1. completed the n-grams/co-occurence algorithms;\n2. added error trapping in the knowledge_graphs_WordNet_main;\n3. added a chart of countries found by the geocoder used for GIS."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_cleaner_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2024-02-11T15:52:51Z",
        "message": "1. fixed a bug in the display of headers in file_search_byWord;\n2. added the lemmatize option in the file_search_byWord search."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2024-02-06T02:24:23Z",
        "message": "1. added a vocab csv output file when computing 1-grams;\n2. fixed potential bug with Stanza not installed in NLP_menu_main when selecting Setup default NLP parsers & annotators...\n3. fixed a bug in data_visualization_main_1 with the Treemap option."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2024-01-29T11:32:14Z",
        "message": "1. fixed a bug in a function call in the file_checker_converter_cleaner_main due to wrong number of parameters passed."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2024-01-26T03:30:54Z",
        "message": "1. fixed potential bug in importing stanza;\n2. fixed bug in computing document(s) statistics."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2024-01-11T11:31:01Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI;\n3. fixed wrong warning in SVO GUI for gender and quote annotators;\n4. added Sankey charts to SVO output."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-11-27T12:12:33Z",
        "message": "1. fixed a bug in the creation of a wordcloud with an empty input file;\n2. added an Organizations column in the SVO output;\n3. improved Stanford CoreNLP SVO using entitymentions values for Subjects and Objects;\n4. introduced a new GUI for a pipeline of data quality algorithms (file_checker_pre_processing_pipeline_main.py)."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-11-25T11:01:41Z",
        "message": "1. fixed a bug in the creation of a wordcloud with an empty input file."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-11-21T19:51:41Z",
        "message": "1. fixed a bug with wordcloud visualization when using a csv file in input;\n2. extended the POS NPOUN to include proper nouns in Wordclouds;\n3. added a checkbox to compute corpus statistics by POS (Part of Speech) tag value in the statistics_txt_main GUI;\n4. uniformed the handling of ,'GUIs available for more options' in all GUIs that rely on the checkbox and menu."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-30T16:21:03Z",
        "message": "1. moved the corpus statistics under the text statistics GUI and out of style analysis."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-29T12:07:38Z",
        "message": "1. improved the n-grams scripts;\n2. added a line chart to the n-gram search for 1-grams;\n3. corrected the sort in the function  get_data_to_be_plotted_with_counts leaving sorting in the order of the x-axis values;\n4. organized N-grams output ny n-gram number."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-28T15:36:33Z",
        "message": "1. improved the n-grams scripts."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-28T12:41:58Z",
        "message": "1. improved the n-grams scripts."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-27T02:08:00Z",
        "message": "1. completed the work on generalizing the special charts to process any number and types of csv file fields."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-24T12:36:09Z",
        "message": "1. prepared the data_visualization_1_main.py GUI to process multiple selections of fields/search values. TO BE CONTINUED;\n2. added Sankey charts to the n-grams search in NGrams_CoOccurrences_main GUI;\n3. fixed an Excel chart display with MALLET topic modelling;\n4. fixed a bug in style_analysis_main for the hapax_words variable not assigned."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-22T14:30:14Z",
        "message": "1. added the N-grams search option in the NGrams_CoOccurrences_main GUI;\n2. added the code to deal with multi-word NER values."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-21T12:55:43Z",
        "message": "1. reorganized the style_analysis and NGrams_CoOccurrences GUI;\n2. in the CoNLL_table_analyzer and file_search_byWord GUIs added the options of searching for a word and extracting neighboring words (TO BE COMPLETED);\n3. improved the efficiency of the ngrams algorithms."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-20T00:21:16Z",
        "message": "1. hapax"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-19T23:28:20Z",
        "message": "1. hapax"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-18T10:53:21Z",
        "message": "1. completed the n-grams/co-occurence algorithms;\n2. added error trapping in the knowledge_graphs_WordNet_main;\n3. added a chart of countries found by the geocoder used for GIS."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-17T23:32:23Z",
        "message": "1. completed the n-grams algorithms."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-17T23:26:53Z",
        "message": "1. completed the n-grams algorithms."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-17T17:41:19Z",
        "message": "1. fixed a bug in the N-grams VIEWER when splitting multiple-word search words (e.g., Hong Kong)."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-17T00:28:57Z",
        "message": "1. completed the development of GIS pipeline;\n2. rewrote the n-grams algorithms leading to greater efficiency (15 minutes instead of 5 hours for the GWR corpus)."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-16T21:39:03Z",
        "message": "1. completed the development of GIS pipeline;\n2. rewrote the n-grams algorithms leading to greater efficiency (15 minutes instead of 5 hours for the GWR corpus)."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-11T15:48:06Z",
        "message": "1. In the Style analysis GUI, modified the Vocabulary analysis option for Short words (<4 characters) to compute, instead, the word length for all words in a corpus;\n2. fixed bug in wordclouds_util word_str referenced before assignment."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-09-30T15:43:33Z",
        "message": "1. fixed bugs in the creation of charts for document statistics and other charts;\n2. improved the output layout for sentiment analysis algorithms."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-09-29T21:33:51Z",
        "message": "1. fixed a display problem with charts by group columns (e.g., By Document)."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-09-29T16:49:39Z",
        "message": "1. fixed an inconsequential bug in the export of sentiment analysis scores for Stanza;\n2. fixed a bug in the export of charts for sentiment analysis scores."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-09-16T18:20:12Z",
        "message": "1. Added the POTUS_webscraper based on beautifulsoup to extract POTUS speeches;\n2.fixed an inconsequential display but in NLP_setup_package_language_main\n3. fixed bugs in the creation of Excel charts;\n4. fixed bugs in TIPS filenames;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-08-13T08:49:24Z",
        "message": "Release 3.8.8 reflects a large number of bug fixes and improvements, the result of summer 2023 work on the NLP Suite."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-08-09T16:58:34Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-08-07T13:49:20Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-08-06T14:00:48Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-06-14T16:57:31Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-06-14T16:08:15Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-06-12T12:59:48Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-06-10T18:01:13Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-05-30T16:00:54Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI;\n4. added the chart visualization to BERT NER;\n5. uniformed the output layout of spaCy NER to all other NER packages;\n6. fixed a bug in reminders;\n7 fixed bugs in the opening of output files in the CoNLL_table_analyzer_main GUI;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/nominalization_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/nominalization_util.py",
        "commit_date": "2023-10-28T12:41:58Z",
        "message": "1. improved the n-grams scripts."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/nominalization_util.py",
        "commit_date": "2023-10-28T00:20:05Z",
        "message": "1. nominalization creates a line chart with values by date if the files embed a date."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/nominalization_util.py",
        "commit_date": "2023-10-27T14:19:10Z",
        "message": "1. nominalization creates a line chart with values by date if the files embed a date."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/nominalization_util.py",
        "commit_date": "2023-10-27T12:56:45Z",
        "message": "1.completed the work on nominalization."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/nominalization_util.py",
        "commit_date": "2023-10-27T02:08:00Z",
        "message": "1. completed the work on generalizing the special charts to process any number and types of csv file fields."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/word2vec_Gensim_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/word2vec_Gensim_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_functions_util.py",
        "commit_date": "2024-02-07T03:09:41Z",
        "message": "1. fixed bug in file_search_byWord_main when running the case sensitive/insensitive option."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_functions_util.py",
        "commit_date": "2024-01-26T03:30:54Z",
        "message": "1. fixed potential bug in importing stanza;\n2. fixed bug in computing document(s) statistics."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_functions_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentence_analysis_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentence_analysis_util.py",
        "commit_date": "2023-06-10T18:01:13Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentence_analysis_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/CoNLL_k_sentences_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/CoNLL_k_sentences_util.py",
        "commit_date": "2023-11-25T11:01:41Z",
        "message": "1. fixed a bug in the creation of a wordcloud with an empty input file."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/CoNLL_k_sentences_util.py",
        "commit_date": "2023-06-12T12:59:48Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/CoNLL_k_sentences_util.py",
        "commit_date": "2023-06-10T18:01:13Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/CoNLL_k_sentences_util.py",
        "commit_date": "2023-06-04T17:32:29Z",
        "message": "1. rewrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI;\n4. added the chart visualization to BERT NER;\n5. uniformed the output layout of spaCy NER to all other NER packages;\n6. fixed a bug in reminders;\n7 fixed bugs in the opening of output files in the CoNLL_table_analyzer_main GUI;\n8. fixed bugs in the statistics_csv_main GUIs;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/CoNLL_k_sentences_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util.py",
        "commit_date": "2023-11-08T11:34:46Z",
        "message": "1. fixed various bugs in the search functions;\n2. added the option of filtering data when drawing a sunburst or treemap charts."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util.py",
        "commit_date": "2023-09-30T15:43:33Z",
        "message": "1. fixed bugs in the creation of charts for document statistics and other charts;\n2. improved the output layout for sentiment analysis algorithms."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util.py",
        "commit_date": "2023-09-01T18:08:08Z",
        "message": "1. Improved the display of information when clicking on a Google Earth Pro pin in GIS_main;\n2. improved the user interface in the functions behind the NLP_setup_external_software_main.py GUI;\n3. fixed a filename bug in opening the config file NLP_default_package_language_config.csv;\n4. fixed a bug in GUI-specific I/O configuration not updating."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util.py",
        "commit_date": "2023-08-13T08:49:24Z",
        "message": "Release 3.8.8 reflects a large number of bug fixes and improvements, the result of summer 2023 work on the NLP Suite."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util.py",
        "commit_date": "2023-08-07T13:49:20Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util.py",
        "commit_date": "2023-06-14T16:08:15Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util.py",
        "commit_date": "2023-06-12T12:59:48Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util.py",
        "commit_date": "2023-06-10T18:01:13Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util.py",
        "commit_date": "2023-05-30T16:00:54Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI;\n4. added the chart visualization to BERT NER;\n5. uniformed the output layout of spaCy NER to all other NER packages;\n6. fixed a bug in reminders;\n7 fixed bugs in the opening of output files in the CoNLL_table_analyzer_main GUI;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2024-02-11T15:52:51Z",
        "message": "1. fixed a bug in the display of headers in file_search_byWord;\n2. added the lemmatize option in the file_search_byWord search."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2024-02-07T03:09:41Z",
        "message": "1. fixed bug in file_search_byWord_main when running the case sensitive/insensitive option."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2023-11-22T09:13:51Z",
        "message": "1. in the SVO swordclouds visualization, set the collocations parameter to False to avoid potential repetition of the same words;\n2. added new options to the dropdown menus in NL_menu_main for statistical tools of textual analysis;\n 3. fixed chart display bugs in the computation of clause, noun, verb, function words in the CoNLL table analyzer;\n4. added computation of overall noun and verb lists and frequencies in the CoNLL table analyzer;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2023-11-19T21:34:14Z",
        "message": "1. fixed minor bugs in the search functions;\n2. added the heatmap for topic composition and keys to MALLET topic modeling;\n3. added the script charts_matplotlib_seaborn_util to the NLP Suite."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2023-11-18T20:50:39Z",
        "message": "1. bugs in search functions"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2023-11-16T20:59:23Z",
        "message": "1. fixed a bug with Plotly capitalized."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2023-11-09T23:52:42Z",
        "message": "1. fixed various bugs in the search functions;\n2. added the option of filtering data when drawing a sunburst or treemap charts;\n3. uniformed the name of output directories for searches of various types (word, sentence, n-grams);\n4. added the option of computing and visualizing WordNet aggregated lemma values for POS nouns and POS verbs in the CoNLL_table_analyzer;\n5. added the Excel/plotly option in data_visualization_2_main GUI"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2023-11-08T12:42:18Z",
        "message": "1. fixed various bugs in the search functions;\n2. added the option of filtering data when drawing a sunburst or treemap charts;\n3. uniformed the name of output directories for searches of various types (word, sentence, n-grams)."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2023-11-08T11:34:46Z",
        "message": "1. fixed various bugs in the search functions;\n2. added the option of filtering data when drawing a sunburst or treemap charts."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2023-11-07T11:58:53Z",
        "message": "1. fixed various bugs in the search functions;\n2. added the option of filtering data when drawing a sunburst or treemap charts."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2023-11-06T12:49:46Z",
        "message": "1. removed path from document in plotly bar charts with document as the X-axis;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2023-11-02T03:11:05Z",
        "message": "1. fixed bugs in N-grams search function with Sankey visualization;\n2. improved the layout of the style analysis GUI;\n3. completed the development work of the file search by word with wordclouds visualization;\n4. completed the development work of the n-grams search with wordclouds visualization."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2023-10-31T14:10:24Z",
        "message": "1. added the exact match in the file_search_byWord functions"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2023-10-31T13:52:01Z",
        "message": "1. added the exact match in the file_search_byWord functions"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2023-10-30T16:21:03Z",
        "message": "1. moved the corpus statistics under the text statistics GUI and out of style analysis."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2023-10-30T03:29:28Z",
        "message": "1. completed the search function for N-grams;\n2. added wordcloud display in the word search functions."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2023-10-29T20:39:05Z",
        "message": "1. completed the search function for N-grams;\n2. added wordcloud display in the word search functions."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2023-10-19T23:28:20Z",
        "message": "1. hapax"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2023-10-19T11:27:34Z",
        "message": "Ngrams_CoOccurrences_Viewer functions."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2023-10-17T15:49:29Z",
        "message": "1. fixed a bug in the N-grams VIEWER when splitting multiple-word search words (e.g., Hong Kong);"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2023-10-17T13:44:10Z",
        "message": "1. fixed a bug in the N-grams VIEWER when splitting multiple-word search words (e.g., Hong Kong);"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2023-10-15T02:58:33Z",
        "message": "1. improved the user interaction of the file_search_byWord_util algorithms."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2023-09-10T22:16:07Z",
        "message": "1.improved the user interaction when there are missing IO values;\n2. removed the line import RF_charts_treemaper_util from data_visualization util;\n3. fixed a bug in file_search_byWord_util."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2023-06-12T12:59:48Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2023-06-10T18:01:13Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_search_byWord_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_classifier_NER_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_classifier_NER_util.py",
        "commit_date": "2023-09-01T18:08:08Z",
        "message": "1. Improved the display of information when clicking on a Google Earth Pro pin in GIS_main;\n2. improved the user interface in the functions behind the NLP_setup_external_software_main.py GUI;\n3. fixed a filename bug in opening the config file NLP_default_package_language_config.csv;\n4. fixed a bug in GUI-specific I/O configuration not updating."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_classifier_NER_util.py",
        "commit_date": "2023-06-12T12:59:48Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_classifier_NER_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_CoOccurrences_util.py",
        "commit_date": "2024-01-25T17:09:34Z",
        "message": "1. fixed a bug in the NLP_menu_main when selecting the Co-Occurrences VIEWER and N-Grams VIEWER;\n3. removed the unnamed column in n-grams csv output thuus removing a bug in the Search word(s) option in the N-grams Co-Occurrences GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_CoOccurrences_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_CoOccurrences_util.py",
        "commit_date": "2023-11-18T20:50:39Z",
        "message": "1. bugs in search functions"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_CoOccurrences_util.py",
        "commit_date": "2023-11-16T20:59:23Z",
        "message": "1. fixed a bug with Plotly capitalized."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_CoOccurrences_util.py",
        "commit_date": "2023-11-11T14:22:51Z",
        "message": "1. fixed various bugs in the search functions;\n2. added the option of filtering data when drawing a sunburst or treemap charts;\n3. uniformed the name of output directories for searches of various types (word, sentence, n-grams);\n4. added the option of computing and visualizing WordNet aggregated lemma values for POS nouns and POS verbs in the CoNLL_table_analyzer;\n5. added the Excel/plotly option in data_visualization_2_main GUI;\n6. added the option of searching for co-occurring words with sentence (rather than document) in NGrams_CoOccurrences_main;\n7. added a Sankey chart to the gender annotator;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_CoOccurrences_util.py",
        "commit_date": "2023-11-08T12:42:18Z",
        "message": "1. fixed various bugs in the search functions;\n2. added the option of filtering data when drawing a sunburst or treemap charts;\n3. uniformed the name of output directories for searches of various types (word, sentence, n-grams)."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_CoOccurrences_util.py",
        "commit_date": "2023-11-07T11:58:53Z",
        "message": "1. fixed various bugs in the search functions;\n2. added the option of filtering data when drawing a sunburst or treemap charts."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_CoOccurrences_util.py",
        "commit_date": "2023-11-02T13:10:33Z",
        "message": "1. fixed bugs in ngrams and file searches when users select the wrong file/search word."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_CoOccurrences_util.py",
        "commit_date": "2023-11-02T03:11:05Z",
        "message": "1. fixed bugs in N-grams search function with Sankey visualization;\n2. improved the layout of the style analysis GUI;\n3. completed the development work of the file search by word with wordclouds visualization;\n4. completed the development work of the n-grams search with wordclouds visualization."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_CoOccurrences_util.py",
        "commit_date": "2023-10-31T20:26:24Z",
        "message": "1. fixed bugs in data_visualization1 functions"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_CoOccurrences_util.py",
        "commit_date": "2023-10-30T03:29:28Z",
        "message": "1. completed the search function for N-grams;\n2. added wordcloud display in the word search functions."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_CoOccurrences_util.py",
        "commit_date": "2023-10-29T20:39:05Z",
        "message": "1. completed the search function for N-grams;\n2. added wordcloud display in the word search functions."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_CoOccurrences_util.py",
        "commit_date": "2023-10-29T14:55:34Z",
        "message": "1. completed the search function for N-grams"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_CoOccurrences_util.py",
        "commit_date": "2023-10-29T10:50:58Z",
        "message": "1. improved the n-grams scripts;\n2. added a line chart to the n-gram search for 1-grams;\n3. corrected the sort in the function  get_data_to_be_plotted_with_counts leaving sorting in the order of the x-axis values."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_CoOccurrences_util.py",
        "commit_date": "2023-10-29T03:26:19Z",
        "message": "1. improved the n-grams scripts;\n2. added a line chart to the n-gram search for 1-grams;\n3. corrected the sort in the function  get_data_to_be_plotted_with_counts leaving sorting in the order of the x-axis values."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_CoOccurrences_util.py",
        "commit_date": "2023-10-28T20:52:14Z",
        "message": "1. improved the n-grams scripts;\n2. added a line chart to the n-gram search for 1-grams;\n3. corrected the sort in the function  get_data_to_be_plotted_with_counts leaving sorting in the order of the x-axis values."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_CoOccurrences_util.py",
        "commit_date": "2023-10-28T15:36:33Z",
        "message": "1. improved the n-grams scripts."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_CoOccurrences_util.py",
        "commit_date": "2023-10-26T02:57:04Z",
        "message": "1. completed the work on generalizing the special charts to process any number and types of csv file fields."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_CoOccurrences_util.py",
        "commit_date": "2023-10-25T23:02:06Z",
        "message": "1. Test"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_CoOccurrences_util.py",
        "commit_date": "2023-10-25T16:23:00Z",
        "message": "1. improved both the nominalization_main algorithms and the nominalization TIPS file."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_CoOccurrences_util.py",
        "commit_date": "2023-10-25T01:47:36Z",
        "message": "1. prepared the data_visualization_1_main.py GUI to process multiple selections of fields/search values. TO BE CONTINUED;\n2. added Sankey charts to the n-grams search in NGrams_CoOccurrences_main GUI;\n3. fixed an Excel chart display with MALLET topic modelling;\n4. fixed a bug in style_analysis_main for the hapax_words variable not assigned;\n4. added the colormap chart option to data_visualization1_main."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_CoOccurrences_util.py",
        "commit_date": "2023-10-24T01:37:47Z",
        "message": "1. prepared the data_visualization_1_main.py GUI to process multiple selections of fields/search values. TO BE CONTINUED."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_CoOccurrences_util.py",
        "commit_date": "2023-10-23T12:58:29Z",
        "message": "1. prepared the data_visualization_1_main.py GUI to process multiple selections of fields/search values. TO BE CONTINUED."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_CoOccurrences_util.py",
        "commit_date": "2023-10-22T17:56:01Z",
        "message": "1. added the N-grams search option in the NGrams_CoOccurrences_main GUI;\n2. added the code to deal with multi-word NER values."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_CoOccurrences_util.py",
        "commit_date": "2023-10-22T14:30:14Z",
        "message": "1. added the N-grams search option in the NGrams_CoOccurrences_main GUI;\n2. added the code to deal with multi-word NER values."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NGrams_CoOccurrences_util.py",
        "commit_date": "2023-10-21T12:55:43Z",
        "message": "1. reorganized the style_analysis and NGrams_CoOccurrences GUI;\n2. in the CoNLL_table_analyzer and file_search_byWord GUIs added the options of searching for a word and extracting neighboring words (TO BE COMPLETED);\n3. improved the efficiency of the ngrams algorithms."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/knowledge_graphs_YAGO_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util_RF.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util_RF.py",
        "commit_date": "2023-09-01T11:50:28Z",
        "message": "1. Improved the display of information when clicking on a Google Earth Pro pin in GIS_main;\n2. improved the user interface in the functions behind the NLP_setup_external_software_main.py GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util_RF.py",
        "commit_date": "2023-06-12T12:59:48Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util_RF.py",
        "commit_date": "2023-05-30T16:00:54Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI;\n4. added the chart visualization to BERT NER;\n5. uniformed the output layout of spaCy NER to all other NER packages;\n6. fixed a bug in reminders;\n7 fixed bugs in the opening of output files in the CoNLL_table_analyzer_main GUI;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util_RF.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_splitter_ByLength_util.py",
        "commit_date": "2023-05-30T16:00:54Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI;\n4. added the chart visualization to BERT NER;\n5. uniformed the output layout of spaCy NER to all other NER packages;\n6. fixed a bug in reminders;\n7 fixed bugs in the opening of output files in the CoNLL_table_analyzer_main GUI;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_splitter_ByLength_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/TensorFlow_semantic_analysis.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentiment_analysis_ANEW_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentiment_analysis_ANEW_util.py",
        "commit_date": "2023-09-30T15:43:33Z",
        "message": "1. fixed bugs in the creation of charts for document statistics and other charts;\n2. improved the output layout for sentiment analysis algorithms."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentiment_analysis_ANEW_util.py",
        "commit_date": "2023-09-29T16:49:39Z",
        "message": "1. fixed an inconsequential bug in the export of sentiment analysis scores for Stanza;\n2. fixed a bug in the export of charts for sentiment analysis scores."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentiment_analysis_ANEW_util.py",
        "commit_date": "2023-06-12T12:59:48Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentiment_analysis_ANEW_util.py",
        "commit_date": "2023-06-10T18:01:13Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentiment_analysis_ANEW_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentiment_analysis_VADER_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentiment_analysis_VADER_util.py",
        "commit_date": "2023-09-30T15:43:33Z",
        "message": "1. fixed bugs in the creation of charts for document statistics and other charts;\n2. improved the output layout for sentiment analysis algorithms."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentiment_analysis_VADER_util.py",
        "commit_date": "2023-09-29T16:49:39Z",
        "message": "1. fixed an inconsequential bug in the export of sentiment analysis scores for Stanza;\n2. fixed a bug in the export of charts for sentiment analysis scores."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentiment_analysis_VADER_util.py",
        "commit_date": "2023-08-13T08:49:24Z",
        "message": "Release 3.8.8 reflects a large number of bug fixes and improvements, the result of summer 2023 work on the NLP Suite."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentiment_analysis_VADER_util.py",
        "commit_date": "2023-06-12T12:59:48Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentiment_analysis_VADER_util.py",
        "commit_date": "2023-06-10T18:01:13Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentiment_analysis_VADER_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_splitter_ByKeyword_txt_util.py",
        "commit_date": "2023-10-05T00:06:18Z",
        "message": "1. fixed a bug with Gephi;\n2. fixed bugs in the function statistics_csv_util.compute_csv_column_frequencies with no groups;\n3. fixed the wrong display of Infinitive verbs frequency."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_splitter_ByKeyword_txt_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/html_annotator_extractor_util_NEW.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/html_annotator_extractor_util_NEW.py",
        "commit_date": "2023-06-10T18:01:13Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/html_annotator_extractor_util_NEW.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_splitter_ByKeyword_conll_util.py",
        "commit_date": "2023-10-05T00:06:18Z",
        "message": "1. fixed a bug with Gephi;\n2. fixed bugs in the function statistics_csv_util.compute_csv_column_frequencies with no groups;\n3. fixed the wrong display of Infinitive verbs frequency."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_splitter_ByKeyword_conll_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/abstract_concreteness_analysis_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/abstract_concreteness_analysis_util.py",
        "commit_date": "2023-10-05T00:06:18Z",
        "message": "1. fixed a bug with Gephi;\n2. fixed bugs in the function statistics_csv_util.compute_csv_column_frequencies with no groups;\n3. fixed the wrong display of Infinitive verbs frequency."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/abstract_concreteness_analysis_util.py",
        "commit_date": "2023-09-30T15:43:33Z",
        "message": "1. fixed bugs in the creation of charts for document statistics and other charts;\n2. improved the output layout for sentiment analysis algorithms."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/abstract_concreteness_analysis_util.py",
        "commit_date": "2023-06-12T12:59:48Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/abstract_concreteness_analysis_util.py",
        "commit_date": "2023-06-10T18:01:13Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/abstract_concreteness_analysis_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentiment_analysis_hedonometer_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentiment_analysis_hedonometer_util.py",
        "commit_date": "2023-09-30T15:43:33Z",
        "message": "1. fixed bugs in the creation of charts for document statistics and other charts;\n2. improved the output layout for sentiment analysis algorithms."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentiment_analysis_hedonometer_util.py",
        "commit_date": "2023-09-29T16:49:39Z",
        "message": "1. fixed an inconsequential bug in the export of sentiment analysis scores for Stanza;\n2. fixed a bug in the export of charts for sentiment analysis scores."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentiment_analysis_hedonometer_util.py",
        "commit_date": "2023-06-12T12:59:48Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentiment_analysis_hedonometer_util.py",
        "commit_date": "2023-06-10T18:01:13Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentiment_analysis_hedonometer_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/knowledge_graphs_DBpedia_util_SPARQL.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentiment_analysis_SentiWordNet_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentiment_analysis_SentiWordNet_util.py",
        "commit_date": "2023-09-30T15:43:33Z",
        "message": "1. fixed bugs in the creation of charts for document statistics and other charts;\n2. improved the output layout for sentiment analysis algorithms."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentiment_analysis_SentiWordNet_util.py",
        "commit_date": "2023-09-29T16:49:39Z",
        "message": "1. fixed an inconsequential bug in the export of sentiment analysis scores for Stanza;\n2. fixed a bug in the export of charts for sentiment analysis scores."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentiment_analysis_SentiWordNet_util.py",
        "commit_date": "2023-08-13T08:49:24Z",
        "message": "Release 3.8.8 reflects a large number of bug fixes and improvements, the result of summer 2023 work on the NLP Suite."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentiment_analysis_SentiWordNet_util.py",
        "commit_date": "2023-06-12T12:59:48Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentiment_analysis_SentiWordNet_util.py",
        "commit_date": "2023-06-10T18:01:13Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/sentiment_analysis_SentiWordNet_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_find_non_related_documents_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_find_non_related_documents_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_splitter_ByBME_K_sentences_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_splitter_ByBME_K_sentences_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/html_annotator_gender_dictionary_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/html_annotator_gender_dictionary_util.py",
        "commit_date": "2023-10-05T00:06:18Z",
        "message": "1. fixed a bug with Gephi;\n2. fixed bugs in the function statistics_csv_util.compute_csv_column_frequencies with no groups;\n3. fixed the wrong display of Infinitive verbs frequency."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/html_annotator_gender_dictionary_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/xjtu-intsoft/chase",
        "filepath": "Benchmark_Approaches/DuoratChar/duorat/utils/tokenization.py",
        "commit_date": "2021-08-02T03:02:02Z",
        "message": "Add benchmark approaches"
    },
    {
        "repo_url": "github.com/princeton-nlp/MultilingualAnalysis",
        "filepath": "preprocessing/xnli/convert_dataset_to_dependency.py",
        "commit_date": "2021-04-08T06:05:05Z",
        "message": "XNLI syntax modifications"
    },
    {
        "repo_url": "github.com/princeton-nlp/MultilingualAnalysis",
        "filepath": "preprocessing/ner_pos/convert_dataset_to_dependency.py",
        "commit_date": "2021-10-06T04:28:02Z",
        "message": "Modified unicode handling"
    },
    {
        "repo_url": "github.com/princeton-nlp/MultilingualAnalysis",
        "filepath": "preprocessing/ner_pos/convert_dataset_to_dependency.py",
        "commit_date": "2021-04-09T04:56:12Z",
        "message": "NER and POS syntax modif"
    },
    {
        "repo_url": "github.com/princeton-nlp/MultilingualAnalysis",
        "filepath": "preprocessing/sentence_retrieval/convert_dataset_to_dependency.py",
        "commit_date": "2021-04-11T02:20:45Z",
        "message": "Preprocessing for tatoeba"
    },
    {
        "repo_url": "github.com/princeton-nlp/MultilingualAnalysis",
        "filepath": "preprocessing/dependency_parsing/convert_sentences_to_dependency.py",
        "commit_date": "2021-04-04T17:35:01Z",
        "message": "Slurm files"
    },
    {
        "repo_url": "github.com/princeton-nlp/MultilingualAnalysis",
        "filepath": "preprocessing/dependency_parsing/convert_sentences_to_dependency.py",
        "commit_date": "2021-03-22T04:33:09Z",
        "message": "Convert MNLI to syntax synthetic"
    },
    {
        "repo_url": "github.com/princeton-nlp/MultilingualAnalysis",
        "filepath": "preprocessing/dependency_parsing/convert_sentences_to_dependency.py",
        "commit_date": "2021-03-21T05:02:06Z",
        "message": "Fixed error in dependency parsing"
    },
    {
        "repo_url": "github.com/princeton-nlp/MultilingualAnalysis",
        "filepath": "preprocessing/dependency_parsing/convert_sentences_to_dependency.py",
        "commit_date": "2021-03-20T06:03:04Z",
        "message": "File clean up"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/demo/pipeline_demo.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/stanza/server/semgrex.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/stanza/server/ud_enhancer.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/stanza/server/tokensregex.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/scripts/sentiment/process_sb10k.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/scripts/sentiment/process_scare.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/stanza/pipeline/demo/demo_server.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/scripts/sentiment/process_ren_chinese.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/scripts/sentiment/process_usage_german.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/stanza/utils/datasets/ner/convert_bsnlp.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/stanza/utils/datasets/constituency/convert_it_turin.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/stanza/tests/test_core.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/stanza/tests/test_tagger.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/stanza/tests/test_depparse.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/stanza/tests/test_tokenizer.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/stanza/tests/test_decorators.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/stanza/tests/test_lemmatizer.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/stanza/tests/test_ner_tagger.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/stanza/tests/test_ud_enhancer.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/stanza/tests/test_data_objects.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/stanza/tests/test_mwt_expander.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/stanza/tests/test_requirements.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/stanza/tests/test_french_pipeline.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/stanza/tests/test_english_pipeline.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/stanza/tests/resources/test_common.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/stanza/tests/test_pipeline_ner_processor.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/stanza/tests/resources/test_installation.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/stanza/tests/test_pipeline_sentiment_processor.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/zhihao-chen/NLP-experiments",
        "filepath": "nlp/utils/tokenizers.py",
        "commit_date": "2021-12-10T08:31:14Z",
        "message": "modify mytokenizer"
    },
    {
        "repo_url": "github.com/zhihao-chen/NLP-experiments",
        "filepath": "nlp/utils/tokenizers.py",
        "commit_date": "2021-12-01T07:38:49Z",
        "message": "add a new tokenizer that avoid split digits to sub_word"
    },
    {
        "repo_url": "github.com/zhihao-chen/NLP-experiments",
        "filepath": "nlp/utils/tokenizers.py",
        "commit_date": "2021-12-01T07:37:53Z",
        "message": "add a new tokenizer that avoid split digits to sub_word"
    },
    {
        "repo_url": "github.com/zhihao-chen/NLP-experiments",
        "filepath": "nlp/utils/tokenizers.py",
        "commit_date": "2021-11-30T10:16:13Z",
        "message": "add a new tokenizer that avoid split digits to sub_word"
    },
    {
        "repo_url": "github.com/zhihao-chen/NLP-experiments",
        "filepath": "nlp/utils/tokenizers.py",
        "commit_date": "2021-11-10T03:45:49Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/philip-mueller/lovt",
        "filepath": "src/data/text_utils/sentence_splitting.py",
        "commit_date": "2021-11-05T09:06:35Z",
        "message": "Added code"
    },
    {
        "repo_url": "github.com/centre-for-humanities-computing/newsFluxus",
        "filepath": "src/tekisuto/preprocessing/lemmatizer.py",
        "commit_date": "2021-04-29T11:40:15Z",
        "message": "fix: spacy lemmatizer imports, typos"
    },
    {
        "repo_url": "github.com/centre-for-humanities-computing/newsFluxus",
        "filepath": "src/tekisuto/preprocessing/lemmatizer.py",
        "commit_date": "2021-04-29T10:56:46Z",
        "message": "added LemmatizerSpacy"
    },
    {
        "repo_url": "github.com/centre-for-humanities-computing/newsFluxus",
        "filepath": "src/tekisuto/preprocessing/lemmatizer.py",
        "commit_date": "2021-01-14T15:02:12Z",
        "message": "add stanze lemmatizer"
    },
    {
        "repo_url": "github.com/centre-for-humanities-computing/newsFluxus",
        "filepath": "src/tekisuto/preprocessing/lemmatizer.py",
        "commit_date": "2020-06-04T09:21:59Z",
        "message": "add teki repo"
    },
    {
        "repo_url": "github.com/griff4692/calibrating-summaries",
        "filepath": "corruptions/entity/stanza_entities.py",
        "commit_date": "2023-05-09T17:27:20Z",
        "message": "Initial commit."
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-04-05T17:58:13Z",
        "message": "Fix frequency weighting"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-04-03T14:40:35Z",
        "message": "Clean up"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-03-31T12:47:58Z",
        "message": "Minor"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-03-29T22:48:44Z",
        "message": "Using JSONL in JSD script"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-03-29T12:14:18Z",
        "message": "Use JSONL format for the entire pipeline"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-03-28T21:58:03Z",
        "message": "Added UDPipe to the lemmatizer\nFixes #2"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-03-20T17:37:34Z",
        "message": "Gensim vectors and ELMo backward-forward comb"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-02-28T13:02:39Z",
        "message": "Use numpy log function"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-02-28T12:53:26Z",
        "message": "Optional frequency correction"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-02-28T12:34:05Z",
        "message": "Prior word probability distributions"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-02-23T14:44:46Z",
        "message": "Fix log error"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-02-23T13:48:55Z",
        "message": "Fix log/exp issues"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-02-23T13:29:30Z",
        "message": "Fix log/exp issues"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-02-08T17:14:34Z",
        "message": "Inject lexical similarity with static embedding model"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-01-12T18:08:39Z",
        "message": "Use argparse for script arguments"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-01-12T14:44:39Z",
        "message": "Divide by temperature in postprocessing"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-01-07T19:08:40Z",
        "message": "Correct Swedish language code"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-01-07T16:43:59Z",
        "message": "Obtain all forms of target words given a corpus"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2020-12-28T12:53:54Z",
        "message": "Minor"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2020-12-28T10:22:15Z",
        "message": "Use stanza lemmatizer"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2020-12-28T09:20:01Z",
        "message": "Use stanza lemmatizer"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2020-12-27T11:27:29Z",
        "message": "Fix options for frequency correction"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2020-12-27T09:29:14Z",
        "message": "Clean up"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2020-12-27T09:28:39Z",
        "message": "Clean up"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2020-12-27T09:24:46Z",
        "message": "Refactor"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/processing/find_word_forms.py",
        "commit_date": "2021-01-07T19:08:40Z",
        "message": "Correct Swedish language code"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/processing/find_word_forms.py",
        "commit_date": "2021-01-07T18:41:57Z",
        "message": "Minor"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/processing/find_word_forms.py",
        "commit_date": "2021-01-07T18:07:35Z",
        "message": "Use pos tags for english to filter out irrelevant forms"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/processing/find_word_forms.py",
        "commit_date": "2021-01-07T17:49:32Z",
        "message": "Obtain all forms of target words given a corpus"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/processing/find_word_forms.py",
        "commit_date": "2021-01-07T17:44:31Z",
        "message": "Obtain all forms of target words given a corpus"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/processing/find_word_forms.py",
        "commit_date": "2021-01-07T16:43:59Z",
        "message": "Obtain all forms of target words given a corpus"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/ling_features/stanza_process.py",
        "commit_date": "2021-05-12T23:15:54Z",
        "message": "Readme for linguistic features"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/ling_features/stanza_process.py",
        "commit_date": "2021-05-12T23:07:34Z",
        "message": "Linguistic features"
    },
    {
        "repo_url": "github.com/AlonEirew/CoreSearch",
        "filepath": "scripts/extract_min_span.py",
        "commit_date": "2023-03-23T09:48:18Z",
        "message": "Adding scripts to clean the mentions spans and release a new version CoreSearchV2 dataset"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_parser.py",
        "commit_date": "2023-04-17T19:22:47Z",
        "message": "Updated for mtap 1.2.1 (#167)\n\n* Updated for mtap 1.2.1\n\n* Added a scaleout test\n\n* Tweaked the default scaleout settings a little\n\n* Made write-config options consistent"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_parser.py",
        "commit_date": "2023-03-16T18:44:26Z",
        "message": "Issues/138 (#144)\n\n* Fix for deepen error\n\n* Fix for sentences failing when CUDA available"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_parser.py",
        "commit_date": "2020-09-25T17:55:48Z",
        "message": "Updates for mtap 0.7.0"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_parser.py",
        "commit_date": "2020-09-10T12:42:38Z",
        "message": "Fixes to the stanza dependencies\n\nSwitched the method for creating the referential dependency graph from a queue over stanza dependencies to graph construction followed by breadth-first search."
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_parser.py",
        "commit_date": "2020-05-11T15:29:40Z",
        "message": "Features/dependencies (#55)\n\n* Fixed mipacq / ptb / concepts performance tests\n\n* Minor import cleanup\n\n* Last little bits of non-PHI performance test fixes\n\n* Stanza dependency parser, updates for MTAP\n\n* Removed test-results.yml files"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_selective_parser.py",
        "commit_date": "2023-04-17T19:22:47Z",
        "message": "Updated for mtap 1.2.1 (#167)\n\n* Updated for mtap 1.2.1\n\n* Added a scaleout test\n\n* Tweaked the default scaleout settings a little\n\n* Made write-config options consistent"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_selective_parser.py",
        "commit_date": "2023-03-16T18:44:26Z",
        "message": "Issues/138 (#144)\n\n* Fix for deepen error\n\n* Fix for sentences failing when CUDA available"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_selective_parser.py",
        "commit_date": "2023-02-07T16:20:12Z",
        "message": "Release/v3.0.0 rc.7 (#126)\n\n* Various\n\n- Made default and rtf-to-text deployment scripts consistent\n- Fixed issues with the rtf-to-text deployment script.\n- Cleaned up some issues with resource loading.\n- Made all processors use standard non-multiprocessing server by default.\n- Parameterized bi_lstm and selective dependencies to allow use of multiprocessing.\n\n* Clean up flake errors\n* Attempt to re-enable integration tests"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_selective_parser.py",
        "commit_date": "2023-01-13T02:05:50Z",
        "message": "biomedicus-client and updates for mtap compatibility (#120)\n\n* biomedicus-client and updates for mtap compatibility\n\n* Update build action to install biomedicus_client"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_selective_parser.py",
        "commit_date": "2022-12-08T22:34:26Z",
        "message": "Various changes to support mtap>=1.0rc3 (#119)\n\n- Pyproject.toml support\n- Works with Python 3.10 / newest pytorch"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_selective_parser.py",
        "commit_date": "2022-08-17T19:17:22Z",
        "message": "Fixed various RTF issues (#111)"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_selective_parser.py",
        "commit_date": "2021-09-07T15:49:30Z",
        "message": "Update for mtap 1.0"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_selective_parser.py",
        "commit_date": "2021-09-07T15:49:30Z",
        "message": "Various rtf and performance related changes"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_selective_parser.py",
        "commit_date": "2021-03-08T15:18:22Z",
        "message": "Batched sentences for dependency parsing"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_selective_parser.py",
        "commit_date": "2020-09-25T17:55:48Z",
        "message": "Updates for mtap 0.7.0"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_selective_parser.py",
        "commit_date": "2020-09-10T12:42:38Z",
        "message": "Fixes to the stanza dependencies\n\nSwitched the method for creating the referential dependency graph from a queue over stanza dependencies to graph construction followed by breadth-first search."
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_selective_parser.py",
        "commit_date": "2020-07-20T17:38:18Z",
        "message": "deepen (#63)"
    },
    {
        "repo_url": "github.com/pengr/IKD-mmt",
        "filepath": "scripts/stanza_batch.py",
        "commit_date": "2023-10-12T07:03:37Z",
        "message": "new commit"
    },
    {
        "repo_url": "github.com/pengr/IKD-mmt",
        "filepath": "scripts/stanza_batch.py",
        "commit_date": "2023-10-12T06:56:07Z",
        "message": "Delete scripts directory"
    },
    {
        "repo_url": "github.com/pengr/IKD-mmt",
        "filepath": "scripts/stanza_batch.py",
        "commit_date": "2022-10-08T14:10:12Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/pengr/IKD-mmt",
        "filepath": "scripts/extract_image_labels.py",
        "commit_date": "2023-10-12T07:03:37Z",
        "message": "new commit"
    },
    {
        "repo_url": "github.com/pengr/IKD-mmt",
        "filepath": "scripts/extract_image_labels.py",
        "commit_date": "2023-10-12T06:56:07Z",
        "message": "Delete scripts directory"
    },
    {
        "repo_url": "github.com/pengr/IKD-mmt",
        "filepath": "scripts/extract_image_labels.py",
        "commit_date": "2022-10-08T14:10:12Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/lematizacion.py",
        "commit_date": "2021-07-13T22:23:29Z",
        "message": "Ajustes menores en la documentaci\u00f3n"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/lematizacion.py",
        "commit_date": "2021-07-06T14:29:47Z",
        "message": "Correci\u00f3n de errores de documentaci\u00f3n"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/lematizacion.py",
        "commit_date": "2021-07-01T19:25:19Z",
        "message": "Corrreci\u00f3n de bugs para nueva version de spacy"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/lematizacion.py",
        "commit_date": "2021-06-29T21:45:50Z",
        "message": "Actualizacci\u00f3n documentaci\u00f3n ejemplos\\03_Exploraci\u00f3n y visualizaci\u00f3n\n\n- Se actualizan gr\u00e1ficos del ejemplo ejemplos\\03_Exploraci\u00f3n y visualizaci\u00f3n\n- Se corrigen errores menores en documentaci\u00f3n"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/lematizacion.py",
        "commit_date": "2021-06-22T22:45:33Z",
        "message": "Cambios en la documentaci\u00f3n"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/lematizacion.py",
        "commit_date": "2021-03-29T23:59:34Z",
        "message": "Correcci\u00f3n de errores de autopep8 funciones principales"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/lematizacion.py",
        "commit_date": "2021-03-12T00:46:53Z",
        "message": "lematizacion.py con formato pep8"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/lematizacion.py",
        "commit_date": "2020-12-07T13:30:03Z",
        "message": "modificar scripts para que queden con formato pep8, utilizando autopep8"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/lematizacion.py",
        "commit_date": "2020-10-15T23:58:50Z",
        "message": "Ajustes en la documentaci\u00f3n"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/lematizacion.py",
        "commit_date": "2020-10-14T12:17:20Z",
        "message": "se a\u00f1ade tokenizador a lematizaci\u00f3n, se a\u00f1ade par\u00e1metro de maxima_longitud a lematizadorSpacy y algunos cambios a la documentaci\u00f3n"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/lematizacion.py",
        "commit_date": "2020-10-13T23:03:41Z",
        "message": "se deshabilitan procesos del lematizador de spacy, para que corra m\u00e1s r\u00e1pido"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/lematizacion.py",
        "commit_date": "2020-09-29T21:14:48Z",
        "message": "modificaciones peque\u00f1as en documentaci\u00f3n"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/lematizacion.py",
        "commit_date": "2020-09-29T01:13:41Z",
        "message": "Actualizaci\u00f3n documentaci\u00f3n"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/lematizacion.py",
        "commit_date": "2020-09-25T14:27:29Z",
        "message": "Actualizaci\u00f3n documentaci\u00f3n - secciones OCR y Lenguajes soportados\n\n- Se agregan secciones OCR y Lenguajes soportados\n- Se hacen ajustes en la documentaci\u00f3n de las funciones acorde a las secciones agregadas"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/lematizacion.py",
        "commit_date": "2020-09-24T21:22:33Z",
        "message": "Se cambiaron algunos par\u00e1metros de las funciones de lenguajes"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/lematizacion.py",
        "commit_date": "2020-09-15T02:44:21Z",
        "message": "Ajustes menores en documentaci\u00f3n"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/lematizacion.py",
        "commit_date": "2020-09-08T23:45:27Z",
        "message": "Ajustes menores a la documentaci\u00f3n"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/lematizacion.py",
        "commit_date": "2020-09-01T03:10:42Z",
        "message": "Se ajustan las listas de lenguajes como listas no ordenadas, usando raw html en los .rst"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/lematizacion.py",
        "commit_date": "2020-08-27T16:31:29Z",
        "message": "modificaciones documentaci\u00f3n - m\u00f3dulo lematizaci\u00f3n"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/lematizacion.py",
        "commit_date": "2020-08-25T23:11:59Z",
        "message": "Se hizo la documentaci\u00f3n de las funciones de lematizaci\u00f3n"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/lematizacion.py",
        "commit_date": "2020-08-24T18:31:32Z",
        "message": "avances en documentaci\u00f3n"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/lematizacion.py",
        "commit_date": "2020-08-24T02:12:07Z",
        "message": "Merge branch 'master' of https://github.com/ucd-dnp/analitica_texto"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/lematizacion.py",
        "commit_date": "2020-08-18T12:39:55Z",
        "message": "cambios en importaciones y en el nombre del m\u00f3dulo de exploraci\u00f3n"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/lematizacion.py",
        "commit_date": "2020-08-13T15:32:41Z",
        "message": "renombrar clases de acuerdo a convenci\u00f3n pep8"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/lematizacion.py",
        "commit_date": "2020-08-12T19:03:56Z",
        "message": "se renombr\u00f3 la librer\u00eda como contexto"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/utils/stanza_funcs.py",
        "commit_date": "2021-07-08T14:03:48Z",
        "message": "Correcci\u00f3n bugs lematizador Stanza"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/utils/stanza_funcs.py",
        "commit_date": "2021-06-22T22:45:33Z",
        "message": "Cambios en la documentaci\u00f3n"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/utils/stanza_funcs.py",
        "commit_date": "2021-03-30T02:42:00Z",
        "message": "corecci\u00f3n errores autopep8 para funciones auxiliares"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/utils/stanza_funcs.py",
        "commit_date": "2021-03-09T23:40:51Z",
        "message": "stanza_func.py en formato pep8"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/utils/stanza_funcs.py",
        "commit_date": "2020-12-07T13:30:03Z",
        "message": "modificar scripts para que queden con formato pep8, utilizando autopep8"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/utils/stanza_funcs.py",
        "commit_date": "2020-09-15T15:38:10Z",
        "message": "terminar documentaci\u00f3n de scripts auxiliares"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/utils/stanza_funcs.py",
        "commit_date": "2020-08-12T19:03:56Z",
        "message": "se renombr\u00f3 la librer\u00eda como contexto"
    },
    {
        "repo_url": "github.com/microsoft/ContextualSP",
        "filepath": "unified_parser_text_to_sql/interactive.py",
        "commit_date": "2022-04-14T04:34:57Z",
        "message": "init unisar"
    },
    {
        "repo_url": "github.com/microsoft/ContextualSP",
        "filepath": "unified_parser_text_to_sql/step1_schema_linking.py",
        "commit_date": "2022-04-14T04:34:57Z",
        "message": "init unisar"
    },
    {
        "repo_url": "github.com/microsoft/ContextualSP",
        "filepath": "unified_parser_text_to_sql/semparse/sql/spider_utils.py",
        "commit_date": "2022-04-14T04:34:57Z",
        "message": "init unisar"
    },
    {
        "repo_url": "github.com/rashad101/RoMe",
        "filepath": "components/ted_se.py",
        "commit_date": "2023-08-13T09:18:57Z",
        "message": ":bug: fixed memory allocation issue"
    },
    {
        "repo_url": "github.com/rashad101/RoMe",
        "filepath": "components/ted_se.py",
        "commit_date": "2022-05-26T22:36:01Z",
        "message": ":rocket: - added files. more cleanup required"
    },
    {
        "repo_url": "github.com/SapienzaNLP/xl-amr",
        "filepath": "xlamr_stog/data/dataset_readers/amr_parsing/preprocess/feature_annotator_multilingual.py",
        "commit_date": "2020-06-04T22:14:59Z",
        "message": "EMNLP2020"
    },
    {
        "repo_url": "github.com/mo-xiaoxi/HDiff",
        "filepath": "SR_Finder.py",
        "commit_date": "2022-05-04T09:51:23Z",
        "message": "refactor: first commit"
    },
    {
        "repo_url": "github.com/mo-xiaoxi/HDiff",
        "filepath": "SR_Finder.py",
        "commit_date": "2022-05-04T09:50:43Z",
        "message": "refactor: first commit"
    },
    {
        "repo_url": "github.com/UnderstandLingBV/LLaMa2lang",
        "filepath": "translators/seamless_m4t_v2.py",
        "commit_date": "2024-02-07T14:03:25Z",
        "message": "Parameterized version"
    },
    {
        "repo_url": "github.com/UnderstandLingBV/LLaMa2lang",
        "filepath": "translators/seamless_m4t_v2.py",
        "commit_date": "2024-02-01T11:51:20Z",
        "message": "Update seamless_m4t_v2.py"
    },
    {
        "repo_url": "github.com/UnderstandLingBV/LLaMa2lang",
        "filepath": "translators/seamless_m4t_v2.py",
        "commit_date": "2024-01-19T14:59:32Z",
        "message": "fix: fixed errors and whitespaces during translation."
    },
    {
        "repo_url": "github.com/UnderstandLingBV/LLaMa2lang",
        "filepath": "translators/seamless_m4t_v2.py",
        "commit_date": "2024-01-19T14:20:04Z",
        "message": "feat: added sentence splitting with possibility to conduct sen2sen trasnlation, and combining resul after that."
    },
    {
        "repo_url": "github.com/UnderstandLingBV/LLaMa2lang",
        "filepath": "translators/seamless_m4t_v2.py",
        "commit_date": "2024-01-17T21:19:17Z",
        "message": "Added new seamless translator. Need to conduct some effort to allow translate long text\nAlso added possibility to use source lang for selecting most efficient languages for translation."
    },
    {
        "repo_url": "github.com/Lou1sM/video_annotation",
        "filepath": "src/semantic_parser.py",
        "commit_date": "2023-08-23T12:19:05Z",
        "message": "allow parsing on vid-captioning output"
    },
    {
        "repo_url": "github.com/Lou1sM/video_annotation",
        "filepath": "src/semantic_parser.py",
        "commit_date": "2022-07-07T11:36:14Z",
        "message": "several changes to dataset creation, including adding new ignore-words, identifying words just by wordnet id (not string itself too), and filtering out non-english captions from semantic parsing"
    },
    {
        "repo_url": "github.com/Lou1sM/video_annotation",
        "filepath": "src/semantic_parser.py",
        "commit_date": "2022-06-25T14:02:54Z",
        "message": "feat: check sentences are english before parsing; pass dset name by sys.argv instead of running on both"
    },
    {
        "repo_url": "github.com/Lou1sM/video_annotation",
        "filepath": "src/semantic_parser.py",
        "commit_date": "2021-02-02T17:31:37Z",
        "message": "update semantic parser to new stanze corenlp, make new pats if they don't exist in msvd preproc, download nltk data if not already there in w2v_links"
    },
    {
        "repo_url": "github.com/Lou1sM/video_annotation",
        "filepath": "src/semantic_parser.py",
        "commit_date": "2021-01-09T17:59:48Z",
        "message": "clean up scripts for generating datasets of logical captions, now just run semantic_parser.py, w2v_wn_links.py and make_new_dset.py once each, in that order"
    },
    {
        "repo_url": "github.com/Lou1sM/video_annotation",
        "filepath": "src/semantic_parser.py",
        "commit_date": "2021-01-05T12:48:22Z",
        "message": "make preprocessing and i3d extraction work, resize i3d vector from penultimate conv layer so size is now 4096, adjust network sizes to match, fix some formatting errors in main and train, add w2v_limit CL option"
    },
    {
        "repo_url": "github.com/Lou1sM/video_annotation",
        "filepath": "src/semantic_parser.py",
        "commit_date": "2020-12-03T14:01:48Z",
        "message": "make dataset path a CL argument"
    },
    {
        "repo_url": "github.com/Lou1sM/video_annotation",
        "filepath": "src/semantic_parser.py",
        "commit_date": "2020-01-13T11:35:03Z",
        "message": "make work up to point of memory error"
    },
    {
        "repo_url": "github.com/quadrismegistus/lltk",
        "filepath": "lltk/model/ner.py",
        "commit_date": "2022-07-11T08:06:11Z",
        "message": "major"
    },
    {
        "repo_url": "github.com/quadrismegistus/lltk",
        "filepath": "lltk/model/ner.py",
        "commit_date": "2021-03-09T15:51:13Z",
        "message": "bunch of new changes, temp. in new branch"
    },
    {
        "repo_url": "github.com/quadrismegistus/lltk",
        "filepath": "lltk/text/utils.py",
        "commit_date": "2022-07-11T08:06:11Z",
        "message": "major"
    },
    {
        "repo_url": "github.com/quadrismegistus/lltk",
        "filepath": "lltk/text/utils.py",
        "commit_date": "2022-03-08T20:47:19Z",
        "message": "lots of changes"
    },
    {
        "repo_url": "github.com/quadrismegistus/lltk",
        "filepath": "lltk/text/utils.py",
        "commit_date": "2021-04-20T07:37:43Z",
        "message": "fixes"
    },
    {
        "repo_url": "github.com/quadrismegistus/lltk",
        "filepath": "lltk/text/utils.py",
        "commit_date": "2021-04-18T07:30:10Z",
        "message": "switching over to pkl from ft"
    },
    {
        "repo_url": "github.com/quadrismegistus/lltk",
        "filepath": "lltk/text/utils.py",
        "commit_date": "2021-03-10T16:43:51Z",
        "message": "fixes"
    },
    {
        "repo_url": "github.com/quadrismegistus/lltk",
        "filepath": "lltk/text/utils.py",
        "commit_date": "2021-03-10T14:38:58Z",
        "message": "readme ipynb"
    },
    {
        "repo_url": "github.com/quadrismegistus/lltk",
        "filepath": "lltk/text/utils.py",
        "commit_date": "2021-03-10T10:12:31Z",
        "message": "spacy"
    },
    {
        "repo_url": "github.com/quadrismegistus/lltk",
        "filepath": "lltk/text/utils.py",
        "commit_date": "2021-03-10T01:50:17Z",
        "message": "fixes"
    },
    {
        "repo_url": "github.com/quadrismegistus/lltk",
        "filepath": "lltk/text/utils.py",
        "commit_date": "2021-03-09T18:53:44Z",
        "message": "many fixes"
    },
    {
        "repo_url": "github.com/quadrismegistus/lltk",
        "filepath": "lltk/text/utils.py",
        "commit_date": "2021-03-09T15:51:13Z",
        "message": "bunch of new changes, temp. in new branch"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/LiudmylaSlava/project/fem/fem.py",
        "commit_date": "2020-06-11T13:55:06Z",
        "message": "project work"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/LiudmylaSlava/project/fem/app.py",
        "commit_date": "2020-06-11T13:55:06Z",
        "message": "project work"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/BohdanYatsyna/homework5/base_line.py",
        "commit_date": "2020-04-20T09:29:48Z",
        "message": "\u0434\u043e\u0434\u0430\u0432 \u0435\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u0438 \u0442\u0430 \u043e\u043f\u0438\u0441 \u043f\u043e\u0441\u043b\u0456\u0434\u043e\u0432\u043d\u043e\u0441\u0442\u0456 \u0437\u0430\u0434\u0430\u0447\u0456"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/BohdanYatsyna/homework5/base_line.py",
        "commit_date": "2020-04-18T08:56:07Z",
        "message": "\u043d\u0430\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u043f\u043e \u0447\u0435\u0442\u0432\u0435\u0440\u0442\u0456\u0439 \u0434\u043e\u043c\u0430\u0448\u0446\u0456"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/BohdanYatsyna/homework2/task-02-3-2.py",
        "commit_date": "2020-03-26T15:51:25Z",
        "message": "\u0432\u0438\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u044f \u0437\u0430\u0443\u0432\u0430\u0436\u0435\u043d\u044c \u041c\u0430\u0440\u044f\u043d\u0438\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398049316\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398050175\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398053698\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398054208\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398058079\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398058962\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398060221\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398064295\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398065397"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/BohdanYatsyna/homework2/task-02-3-2.py",
        "commit_date": "2020-03-26T15:46:43Z",
        "message": "\u0432\u0438\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u044f \u0437\u0430\u0443\u0432\u0430\u0436\u0435\u043d\u044c \u041c\u0430\u0440\u044f\u043d\u0438\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398049316\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398050175\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398053698\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398054208\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398058079\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398058962\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398060221\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398064295\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398065397"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/BohdanYatsyna/homework2/task-02-3-2.py",
        "commit_date": "2020-03-23T17:57:51Z",
        "message": "\u0437\u0430\u043c\u0435\u0440\u0436\u0438\u0432 2\u0433\u0443 \u0434\u043e\u043c\u0430\u0448\u043a\u0443 \u0432 \u043c\u0430\u0441\u0442\u0435\u0440"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/BohdanYatsyna/homework2/task-02-3-2.py",
        "commit_date": "2020-03-23T17:53:00Z",
        "message": "\u0437\u0430\u043c\u0435\u0440\u0436\u0438\u0432 2\u0433\u0443 \u0434\u043e\u043c\u0430\u0448\u043a\u0443 \u0432 \u043c\u0430\u0441\u0442\u0435\u0440"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/BohdanYatsyna/homework2/task-02-3-2.py",
        "commit_date": "2020-03-23T17:45:46Z",
        "message": "3\u044f \u0447\u0430\u0441\u0442\u0438\u043d\u0430 \u0434\u043e\u043c\u0430\u0448\u043a\u0438"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/BohdanYatsyna/homework5/prepare_data.py",
        "commit_date": "2020-04-20T09:29:48Z",
        "message": "\u0434\u043e\u0434\u0430\u0432 \u0435\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u0438 \u0442\u0430 \u043e\u043f\u0438\u0441 \u043f\u043e\u0441\u043b\u0456\u0434\u043e\u0432\u043d\u043e\u0441\u0442\u0456 \u0437\u0430\u0434\u0430\u0447\u0456"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/BohdanYatsyna/homework5/prepare_data.py",
        "commit_date": "2020-04-18T08:56:07Z",
        "message": "\u043d\u0430\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u043f\u043e \u0447\u0435\u0442\u0432\u0435\u0440\u0442\u0456\u0439 \u0434\u043e\u043c\u0430\u0448\u0446\u0456"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/AlisaMansurova/08-syntactic-parsing/parser.py",
        "commit_date": "2020-05-04T22:13:31Z",
        "message": "cleanup"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/AlisaMansurova/08-syntactic-parsing/parser.py",
        "commit_date": "2020-05-03T10:14:06Z",
        "message": "Cleanup"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/DmytroMindra/05-bag-of-words/src/04-frequency_analysis.py",
        "commit_date": "2020-04-15T12:18:59Z",
        "message": "Filtered input added notes to readme.md"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/DmytroMindra/05-bag-of-words/src/04-frequency_analysis.py",
        "commit_date": "2020-04-14T20:06:57Z",
        "message": "Added more data to the experiment"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/DmytroMindra/05-bag-of-words/src/04-frequency_analysis.py",
        "commit_date": "2020-04-11T20:44:05Z",
        "message": "Adding homework-05"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/OleksandrPetrov/02-structural-linguistics/3.2.collocations.py",
        "commit_date": "2020-03-28T14:07:44Z",
        "message": "Homework 2: problem 3.2: fix bug with parent word"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/OleksandrPetrov/02-structural-linguistics/3.2.collocations.py",
        "commit_date": "2020-03-28T12:25:01Z",
        "message": "Homework 2: problem 3.2: fix anim nouns detection"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/OleksandrPetrov/02-structural-linguistics/3.2.collocations.py",
        "commit_date": "2020-03-28T12:25:01Z",
        "message": "Add: homework 2: problem 3.2"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/DmytroMindra/06-language-as-sequence/src/02-prepare-datasets.py",
        "commit_date": "2020-04-20T07:31:11Z",
        "message": "Added the evaluation on run-on-test.json"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/DmytroMindra/06-language-as-sequence/src/02-prepare-datasets.py",
        "commit_date": "2020-04-16T21:35:47Z",
        "message": "Added more features. Improved score."
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/DmytroMindra/06-language-as-sequence/src/02-prepare-datasets.py",
        "commit_date": "2020-04-16T09:23:18Z",
        "message": "Baseline"
    },
    {
        "repo_url": "github.com/PlusLabNLP/DEGREE",
        "filepath": "preprocessing/process_ace05ep.py",
        "commit_date": "2022-10-14T12:55:10Z",
        "message": "update readme for clearer env setup"
    },
    {
        "repo_url": "github.com/PlusLabNLP/DEGREE",
        "filepath": "preprocessing/process_ace05ep.py",
        "commit_date": "2022-04-08T06:27:21Z",
        "message": "NAACL 2022 Code"
    },
    {
        "repo_url": "github.com/VegB/Diagnose_VLN",
        "filepath": "rxr/model/CLIP-ViL-VLN/rxr_src/utils.py",
        "commit_date": "2022-04-30T09:57:17Z",
        "message": "Update code and scripts"
    },
    {
        "repo_url": "github.com/VegB/Diagnose_VLN",
        "filepath": "rxr/model/CLIP-ViL-VLN/rxr_src/utils.py",
        "commit_date": "2022-04-30T09:14:23Z",
        "message": "(1) add instruction processing scripts; (2) add original agent code"
    },
    {
        "repo_url": "github.com/utcsnlp/lfqa_discourse",
        "filepath": "role_classifier/generate_t5_input_csv_file.py",
        "commit_date": "2022-03-21T02:21:04Z",
        "message": "add code and data"
    },
    {
        "repo_url": "github.com/utcsnlp/lfqa_discourse",
        "filepath": "role_classifier/generate_t5_input_json_file.py",
        "commit_date": "2023-10-13T14:29:31Z",
        "message": "update role prediction script"
    },
    {
        "repo_url": "github.com/loriqing/Label-Reasoning-Network",
        "filepath": "probe_experiment/data/create_memory.py",
        "commit_date": "2021-09-06T05:47:30Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/BBN-E/ZS4IE",
        "filepath": "serif/model/impl/stanza_adapter2/stanza_driver.py",
        "commit_date": "2022-03-23T23:58:13Z",
        "message": "initial release"
    },
    {
        "repo_url": "github.com/sarves/thamizhi-morph",
        "filepath": "thamizhi-morph-parsing.py",
        "commit_date": "2020-12-07T22:55:55Z",
        "message": "Morphology - POS alignment - output will be in CoNLL-U"
    },
    {
        "repo_url": "github.com/phsfr/UD_Persian-PerDT",
        "filepath": "not-to-release/stanza_decode.py",
        "commit_date": "2020-09-10T15:56:47Z",
        "message": "Move stuff to not-to-release"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2023-02-20T13:00:32Z",
        "message": "Move ISO 639 functions to util package"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2023-02-20T12:59:59Z",
        "message": "refactor: use LangaugeRegistry"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2023-01-17T15:17:48Z",
        "message": "Make Stanza select the GPU with the most free memory"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2022-09-02T09:04:25Z",
        "message": "Prevent Stanza from automatically downloading models"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2021-06-01T15:52:07Z",
        "message": "Move util package to api"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2021-05-17T15:12:30Z",
        "message": "Move everything needed by Sparv modules to sparv.api package"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2021-05-03T10:07:20Z",
        "message": "allow whitespaces within tokens in Stanza"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2021-04-30T13:11:43Z",
        "message": "stanza: introduce some checks for tokenisation and sentence segmentation"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2021-04-29T14:22:05Z",
        "message": "Remove Stanza GPU warning workaround (fixes #82)"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2021-04-29T13:46:31Z",
        "message": "add Stanza support for English"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2021-03-26T13:30:02Z",
        "message": "make Stanza annotator functions unavailable for other languages than Swedish"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2021-03-22T10:50:05Z",
        "message": "Add friendly error message when Stanza runs out of memory when not using GPU"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2021-01-15T17:20:45Z",
        "message": "Add cpu_fallback setting for Stanza to allow dependency parsing of sentences longer than max_sentence_length"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2021-01-15T15:03:41Z",
        "message": "Add max_sentence_length setting to Stanza to exclude long sentences from dependency parsing"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2020-12-11T18:45:07Z",
        "message": "Add new config options for Stanza module, and error message to help mitigate memory problems"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2020-12-11T17:23:43Z",
        "message": "Move Stanza imports to shave off half a second from Sparv startup time"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2020-12-11T17:16:34Z",
        "message": "Don't annotate orphaned tokens with dependency relations in Stanza module"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2020-12-11T17:04:54Z",
        "message": "Fix dephead annotation in Stanza module"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2020-12-11T14:45:19Z",
        "message": "Fix Stanza annotations for documents with tokens not belonging to any sentence"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2020-12-10T12:22:25Z",
        "message": "Fix class name in Stanza module"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2020-12-09T18:36:24Z",
        "message": "Minor code improvements"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2020-12-04T11:31:26Z",
        "message": "Suppress GPU warning in Stanza"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2020-11-25T15:44:54Z",
        "message": "add lemmatiser to stanza, re-organise module"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2020-11-20T09:11:37Z",
        "message": "add classes to dependency annotations"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2020-11-18T16:47:30Z",
        "message": "stanza exports pos, not upos!"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2020-11-17T16:42:26Z",
        "message": "add stanza POS tagging"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2020-11-16T14:00:00Z",
        "message": "tiny fix"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza.py",
        "commit_date": "2020-11-16T14:00:00Z",
        "message": "add stanza dependency parser"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza_swe.py",
        "commit_date": "2023-10-30T14:35:26Z",
        "message": "Use lazy formatting in some logging"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza_swe.py",
        "commit_date": "2023-10-27T15:10:10Z",
        "message": "Remove single comma that prevented Stanza from using GPU \ud83d\ude29"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza_swe.py",
        "commit_date": "2023-01-17T15:17:48Z",
        "message": "Make Stanza select the GPU with the most free memory"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza_swe.py",
        "commit_date": "2022-09-02T09:04:25Z",
        "message": "Prevent Stanza from automatically downloading models"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza_swe.py",
        "commit_date": "2021-12-09T12:51:37Z",
        "message": "Fix crash on empty sentences in Stanza"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza_swe.py",
        "commit_date": "2021-09-24T14:30:48Z",
        "message": "add Hunpos-backoff annotation for Stanza msd and pos and add it to SB_DEFAULT.yaml"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza_swe.py",
        "commit_date": "2021-09-09T14:30:13Z",
        "message": "Add stanza.max_token_length option\n\nStanza would crash on extremely long tokens. This option lets the user\nexclude sentences containing such tokens."
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza_swe.py",
        "commit_date": "2021-06-01T15:52:07Z",
        "message": "Move util package to api"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza_swe.py",
        "commit_date": "2021-06-01T13:23:44Z",
        "message": "make explicit ref annotators for stanza, malt, stanford and misc (fixes #132)"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza_swe.py",
        "commit_date": "2021-05-17T15:12:30Z",
        "message": "Move everything needed by Sparv modules to sparv.api package"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza_swe.py",
        "commit_date": "2021-05-03T10:07:20Z",
        "message": "allow whitespaces within tokens in Stanza"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza_swe.py",
        "commit_date": "2021-04-30T13:11:43Z",
        "message": "stanza: introduce some checks for tokenisation and sentence segmentation"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza_swe.py",
        "commit_date": "2021-04-29T14:22:05Z",
        "message": "Remove Stanza GPU warning workaround (fixes #82)"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/stanza_swe.py",
        "commit_date": "2021-04-29T13:46:31Z",
        "message": "add Stanza support for English"
    },
    {
        "repo_url": "github.com/Eric3911/OpenAGI",
        "filepath": "AudioGPT-master/NeuralSeq/modules/syntaspeech/syntactic_graph_buider.py",
        "commit_date": "2023-05-06T06:44:54Z",
        "message": "\u6a21\u578b\u66f4\u65b0\n\nupdate"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-11-07T11:03:53Z",
        "message": "add doc string to krnnt analyzer, add gsutil command to synchronize data"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-05-19T14:12:35Z",
        "message": "add warning swallow for polylot detect lang function"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-05-19T13:54:32Z",
        "message": "fox jsondecoderError in krnnt tagger"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-05-18T13:55:31Z",
        "message": "fix quotes"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-05-18T13:54:10Z",
        "message": "add passing url parma to krnnt analyzer"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-05-16T10:55:07Z",
        "message": "comment morfeusz2 import, it should be removed"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-05-15T14:22:29Z",
        "message": "add stats to process lines"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-05-15T07:39:41Z",
        "message": "add oscar processing, move data procesing to another notebook"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-05-11T05:59:04Z",
        "message": "add new test sentences to check"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-05-08T14:56:43Z",
        "message": "add krnnttagger,add aux and propn"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-05-07T18:11:57Z",
        "message": "add stanza sentence validator, add linguistics heuristic for sentences"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-04-08T15:39:09Z",
        "message": "add testing pos taggers"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-04-05T11:48:10Z",
        "message": "add checking if sentence is in polish"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-03-30T14:48:16Z",
        "message": "move wiki and text data processing to another jupyter, test sentence validator"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-03-26T21:46:22Z",
        "message": "wip: refactoring and checking sentence"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "playground_taggers.py",
        "commit_date": "2020-05-15T07:39:41Z",
        "message": "add oscar processing, move data procesing to another notebook"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "playground_taggers.py",
        "commit_date": "2020-05-11T05:59:04Z",
        "message": "add new test sentences to check"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "playground_taggers.py",
        "commit_date": "2020-05-08T14:56:43Z",
        "message": "add krnnttagger,add aux and propn"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "playground_taggers.py",
        "commit_date": "2020-05-07T18:11:57Z",
        "message": "add stanza sentence validator, add linguistics heuristic for sentences"
    },
    {
        "repo_url": "github.com/humlab/penelope",
        "filepath": "penelope/vendor/archive/stanza/utility.py",
        "commit_date": "2022-02-10T12:26:49Z",
        "message": "Safer extras"
    },
    {
        "repo_url": "github.com/humlab/penelope",
        "filepath": "tests/vendor/stanza_test.py",
        "commit_date": "2020-12-18T11:58:51Z",
        "message": "Tests overhaul - sorted and added GUI tests"
    },
    {
        "repo_url": "github.com/INDElab/conversationkg",
        "filepath": "conversationkg/conversations/factories.py",
        "commit_date": "2020-12-14T14:46:16Z",
        "message": "fixed parallelisation issues"
    },
    {
        "repo_url": "github.com/INDElab/conversationkg",
        "filepath": "conversationkg/conversations/factories.py",
        "commit_date": "2020-12-10T15:00:51Z",
        "message": "moved address and link extraction also to factories (RegexFactory)"
    },
    {
        "repo_url": "github.com/INDElab/conversationkg",
        "filepath": "conversationkg/conversations/factories.py",
        "commit_date": "2020-12-09T17:17:01Z",
        "message": "more parallelisation, and factory work"
    },
    {
        "repo_url": "github.com/INDElab/conversationkg",
        "filepath": "conversationkg/conversations/factories.py",
        "commit_date": "2020-12-08T00:10:43Z",
        "message": "parallelised EmailCorpus instantiation and Factory call"
    },
    {
        "repo_url": "github.com/INDElab/conversationkg",
        "filepath": "conversationkg/conversations/factories.py",
        "commit_date": "2020-12-04T19:04:48Z",
        "message": "updated tests.py to reflect recent changes, added parallelise.py, made changes to EmailCorpusCollection in corpus.py and to ledger.py"
    },
    {
        "repo_url": "github.com/INDElab/conversationkg",
        "filepath": "conversationkg/conversations/factories.py",
        "commit_date": "2020-12-03T14:10:09Z",
        "message": "refurbished conversations: topics in entities.py now; vectoriser, topic modelling, NER and keyword extraction functions in factories.py"
    },
    {
        "repo_url": "github.com/INDElab/conversationkg",
        "filepath": "conversationkg/conversations/factories.py",
        "commit_date": "2020-11-20T21:38:12Z",
        "message": "modified topics, writers and conversations; revamped conversations.entities"
    },
    {
        "repo_url": "github.com/INDElab/conversationkg",
        "filepath": "conversationkg/conversations/factories.py",
        "commit_date": "2020-11-13T13:52:12Z",
        "message": "changes and improvementsto the corpus structure"
    },
    {
        "repo_url": "github.com/dpalmasan/TRUNAJOD2.0",
        "filepath": "stanza_example.py",
        "commit_date": "2021-04-03T12:20:05Z",
        "message": "Update pre-commit hooks to use black (#14)"
    },
    {
        "repo_url": "github.com/dpalmasan/TRUNAJOD2.0",
        "filepath": "stanza_example.py",
        "commit_date": "2021-03-29T09:41:48Z",
        "message": "test conflict resolve and enum implementation"
    },
    {
        "repo_url": "github.com/heqi2015/CA_GCN",
        "filepath": "data/MNLI/dependency_output_mnli.py",
        "commit_date": "2022-02-28T11:41:51Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/passing2961/PersonaChatGen",
        "filepath": "profile_filtering.py",
        "commit_date": "2023-05-18T08:24:15Z",
        "message": "update codes"
    },
    {
        "repo_url": "github.com/passing2961/PersonaChatGen",
        "filepath": "profile_filtering.py",
        "commit_date": "2023-05-18T07:38:20Z",
        "message": "update codes"
    },
    {
        "repo_url": "github.com/eric11eca/Udep2Mono",
        "filepath": "udep2mono/dependency_parse.py",
        "commit_date": "2021-05-06T08:17:55Z",
        "message": "codebase for IWCS2021 camera ready version"
    },
    {
        "repo_url": "github.com/eric11eca/Udep2Mono",
        "filepath": "udep2mono/dependency_parse.py",
        "commit_date": "2021-02-22T19:38:04Z",
        "message": "package 0.2.0 version release"
    },
    {
        "repo_url": "github.com/lang-uk/ner-uk",
        "filepath": "scripts/eval_ner_models.py",
        "commit_date": "2021-08-02T19:52:43Z",
        "message": "fix: Cleanup of eval script\n\n- whitespaces\n- more docs\n- cleanup of debug output"
    },
    {
        "repo_url": "github.com/lang-uk/ner-uk",
        "filepath": "scripts/eval_ner_models.py",
        "commit_date": "2021-07-28T07:36:24Z",
        "message": "Minor cleanup and doc udpate"
    },
    {
        "repo_url": "github.com/lang-uk/ner-uk",
        "filepath": "scripts/eval_ner_models.py",
        "commit_date": "2021-07-28T07:29:40Z",
        "message": "feat: Evaluation logic for mitie and stanza models\n\n- converting model output to iob format\n- classification report with sklearn"
    },
    {
        "repo_url": "github.com/EclipseCN/PhenoBERT",
        "filepath": "phenobert/utils/api.py",
        "commit_date": "2021-05-10T02:25:45Z",
        "message": "fix api func"
    },
    {
        "repo_url": "github.com/EclipseCN/PhenoBERT",
        "filepath": "phenobert/utils/api.py",
        "commit_date": "2021-03-23T11:36:17Z",
        "message": "fix for long text"
    },
    {
        "repo_url": "github.com/EclipseCN/PhenoBERT",
        "filepath": "phenobert/utils/api.py",
        "commit_date": "2021-03-06T09:09:52Z",
        "message": "add -al function & add score to output"
    },
    {
        "repo_url": "github.com/EclipseCN/PhenoBERT",
        "filepath": "phenobert/utils/api.py",
        "commit_date": "2020-12-16T09:37:21Z",
        "message": "add gene_reviews search func"
    },
    {
        "repo_url": "github.com/EclipseCN/PhenoBERT",
        "filepath": "phenobert/utils/api.py",
        "commit_date": "2020-12-01T06:59:56Z",
        "message": "ver1.0.5"
    },
    {
        "repo_url": "github.com/EclipseCN/PhenoBERT",
        "filepath": "phenobert/utils/annotate.py",
        "commit_date": "2021-03-23T07:57:40Z",
        "message": "add text process"
    },
    {
        "repo_url": "github.com/EclipseCN/PhenoBERT",
        "filepath": "phenobert/utils/annotate.py",
        "commit_date": "2021-03-06T09:09:52Z",
        "message": "add -al function & add score to output"
    },
    {
        "repo_url": "github.com/EclipseCN/PhenoBERT",
        "filepath": "phenobert/utils/annotate.py",
        "commit_date": "2020-12-01T07:35:11Z",
        "message": "ver1.0.5"
    },
    {
        "repo_url": "github.com/ptarau/natlog",
        "filepath": "apps/natgpt/textual.py",
        "commit_date": "2023-04-01T22:08:29Z",
        "message": "refined processing of raw text seen as a Natlog textual db"
    },
    {
        "repo_url": "github.com/princeton-nlp/align-mlm",
        "filepath": "preprocessing/xnli/convert_dataset_to_dependency.py",
        "commit_date": "2022-01-25T01:34:48Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/princeton-nlp/align-mlm",
        "filepath": "preprocessing/ner_pos/convert_dataset_to_dependency.py",
        "commit_date": "2022-01-25T01:34:48Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/princeton-nlp/align-mlm",
        "filepath": "preprocessing/sentence_retrieval/convert_dataset_to_dependency.py",
        "commit_date": "2022-01-25T01:34:48Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/princeton-nlp/align-mlm",
        "filepath": "preprocessing/dependency_parsing/convert_sentences_to_dependency.py",
        "commit_date": "2022-01-25T01:34:48Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/IreneZihuiLi/EHRKit-2022",
        "filepath": "wrapper_functions/utils.py",
        "commit_date": "2022-04-05T16:00:34Z",
        "message": "First Commit"
    },
    {
        "repo_url": "github.com/IreneZihuiLi/EHRKit-2022",
        "filepath": "wrapper_functions/stanza_functions.py",
        "commit_date": "2022-04-05T16:00:34Z",
        "message": "First Commit"
    },
    {
        "repo_url": "github.com/IreneZihuiLi/EHRKit-2022",
        "filepath": "collated_tasks/tasks/utils/get_clusters.py",
        "commit_date": "2022-04-05T16:00:34Z",
        "message": "First Commit"
    },
    {
        "repo_url": "github.com/IreneZihuiLi/EHRKit-2022",
        "filepath": "collated_tasks/tasks/utils/get_sentences.py",
        "commit_date": "2022-04-05T16:00:34Z",
        "message": "First Commit"
    },
    {
        "repo_url": "github.com/kenantang/petci",
        "filepath": "data/dataset.py",
        "commit_date": "2022-02-19T02:41:50Z",
        "message": "update data and code"
    },
    {
        "repo_url": "github.com/kenantang/petci",
        "filepath": "models/lstm/dataloader.py",
        "commit_date": "2022-02-19T02:41:50Z",
        "message": "update data and code"
    },
    {
        "repo_url": "github.com/s-nlp/multilingual-fake-news",
        "filepath": "tools/features_extraction.py",
        "commit_date": "2021-02-01T21:47:44Z",
        "message": "ACL 2021 update"
    },
    {
        "repo_url": "github.com/wkvong/multimodal-baby",
        "filepath": "analysis_tools/processing.py",
        "commit_date": "2023-04-06T01:49:39Z",
        "message": "fix a typo"
    },
    {
        "repo_url": "github.com/wkvong/multimodal-baby",
        "filepath": "analysis_tools/processing.py",
        "commit_date": "2023-03-19T15:36:36Z",
        "message": "accelerate Captioning LSTM on data sharing the same image by preprocessing the image features and sharing them across batches"
    },
    {
        "repo_url": "github.com/wkvong/multimodal-baby",
        "filepath": "analysis_tools/processing.py",
        "commit_date": "2023-02-18T06:27:31Z",
        "message": "tried spacy pos tagger but find not better; remove erroneous checkpoint"
    },
    {
        "repo_url": "github.com/wkvong/multimodal-baby",
        "filepath": "analysis_tools/processing.py",
        "commit_date": "2022-08-13T18:42:52Z",
        "message": "abstract get_model_losses_on_data_batches"
    },
    {
        "repo_url": "github.com/wkvong/multimodal-baby",
        "filepath": "analysis_tools/processing.py",
        "commit_date": "2022-07-25T04:16:13Z",
        "message": "get better pos stats; filter out some ambiguous words; filter out templates seen in train set; fix word category: marmite is the name of their cat, not a kind of cooking pot"
    },
    {
        "repo_url": "github.com/wkvong/multimodal-baby",
        "filepath": "analysis_tools/processing.py",
        "commit_date": "2022-07-13T03:40:24Z",
        "message": "initial attempt to pos analysis tests; further decouple run_model_on_data"
    },
    {
        "repo_url": "github.com/wkvong/multimodal-baby",
        "filepath": "analysis_tools/processing.py",
        "commit_date": "2022-07-11T20:17:44Z",
        "message": "build stanza.Pipeline in get_pos_tags in order to save GPU memory"
    },
    {
        "repo_url": "github.com/wkvong/multimodal-baby",
        "filepath": "analysis_tools/processing.py",
        "commit_date": "2022-07-10T15:13:52Z",
        "message": "break notebooks/lm_clustering.ipynb into modules in analysis_tools/ so all notebooks can share them"
    },
    {
        "repo_url": "github.com/mehdi-mirzapour/French-CRS",
        "filepath": "french_crs/fast_text2corefchains.py",
        "commit_date": "2020-06-16T02:00:49Z",
        "message": "Closest-First Strategy Added"
    },
    {
        "repo_url": "github.com/elisaF/subjective_discourse",
        "filepath": "code/analyses/question_classifier.py",
        "commit_date": "2020-07-17T10:18:11Z",
        "message": "add analyses folder"
    },
    {
        "repo_url": "github.com/eric-ai-lab/CPL",
        "filepath": "CPL/utils/_utils/vqa_clip_inference.py",
        "commit_date": "2022-11-12T18:51:37Z",
        "message": "utils"
    },
    {
        "repo_url": "github.com/zsLin177/SRL-as-GP",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2021-04-14T07:28:25Z",
        "message": "Integrated tokenizer (#47)"
    },
    {
        "repo_url": "github.com/dair-iitd/moie",
        "filepath": "code/aligner.py",
        "commit_date": "2022-06-02T10:11:14Z",
        "message": "pushed all code files in code directory; updated README with basic steps for data preparation"
    },
    {
        "repo_url": "github.com/novoic/blabla",
        "filepath": "blabla/document_processor.py",
        "commit_date": "2020-07-21T11:41:00Z",
        "message": ":zap: Remove client.start() behaviour"
    },
    {
        "repo_url": "github.com/novoic/blabla",
        "filepath": "blabla/document_processor.py",
        "commit_date": "2020-07-15T15:56:25Z",
        "message": ":zap: Added flag to split sentences on newline or with stanza"
    },
    {
        "repo_url": "github.com/novoic/blabla",
        "filepath": "blabla/document_processor.py",
        "commit_date": "2020-05-21T13:53:45Z",
        "message": ":tada: Initial public commit"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-10-07T09:58:10Z",
        "message": "make style"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-06-15T12:37:34Z",
        "message": "make logging level adjustable for stanza"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-04-02T12:53:18Z",
        "message": "allow loading spacy shortcuts\n\neven though that is not possible anymore in v3. We look for available models and load the first one that starts with \"{model_name}_\""
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-04-01T14:01:46Z",
        "message": "fix spacy warning"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-04-01T10:44:01Z",
        "message": "check spaCy version\n\nregistering Language.component is a v3 feature"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-04-01T10:08:27Z",
        "message": "make style & quality"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-04-01T10:01:11Z",
        "message": "typing improvements"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-04-01T08:36:42Z",
        "message": "allow disabling auto_download\n\nThis may help in scenarios where there is no more access to the internet"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-03-31T17:27:51Z",
        "message": "do not exclude tok2vec which seems important"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-03-31T16:46:01Z",
        "message": "Download stanza model if not available"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-03-31T16:32:50Z",
        "message": "cached_property for Python <3.8"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-03-31T15:31:35Z",
        "message": "allow instantiation of spaCy (but require 3+)"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-03-31T12:50:16Z",
        "message": "make stanza, aligner, spacy optional"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-03-31T09:54:05Z",
        "message": "make style & quality"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-02-24T14:05:04Z",
        "message": "Do not show verbose INFO logs for stanza"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-02-17T17:48:03Z",
        "message": "MWT should be automatically added in new stanza"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-02-08T16:55:25Z",
        "message": "new stanza does not silently ignore missing processors anymore"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-01-13T08:47:12Z",
        "message": "style and typing"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-01-07T15:41:17Z",
        "message": "full refactor"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2020-06-04T14:47:08Z",
        "message": "refactoring n1"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2020-06-02T09:55:55Z",
        "message": "fix typing"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2020-06-02T09:37:28Z",
        "message": "Rename AlignmentPair to AlignedIdxs and add Alignment class"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2020-03-25T13:13:28Z",
        "message": "Make style"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2020-03-25T11:13:50Z",
        "message": "include MWT processor for languages that support it"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2020-03-25T10:17:13Z",
        "message": "move get_distance to utils"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2020-03-19T13:49:58Z",
        "message": "remove unused import"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2020-03-19T13:47:22Z",
        "message": "style"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2020-03-18T15:57:39Z",
        "message": "Don't download\n\nModels need to be downloaded manually by users"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2020-03-18T15:00:59Z",
        "message": "use astred as package name"
    },
    {
        "repo_url": "github.com/QtacierP/PRIOR",
        "filepath": "codes/prior/data/pretrain/text_process.py",
        "commit_date": "2023-08-03T06:36:32Z",
        "message": "pre-train codes"
    },
    {
        "repo_url": "github.com/khuangaf/ZeroFEC",
        "filepath": "zerofec/models/answer_selector.py",
        "commit_date": "2023-08-13T05:24:12Z",
        "message": "restructure repo"
    },
    {
        "repo_url": "github.com/changzheng123/L-CAD",
        "filepath": "cldm/struct_tool.py",
        "commit_date": "2023-12-07T11:16:50Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/codingsoo/nlp2rest",
        "filepath": "rule_extractor/rule_extractor.py",
        "commit_date": "2023-05-28T17:35:48Z",
        "message": "Change interface"
    },
    {
        "repo_url": "github.com/codingsoo/nlp2rest",
        "filepath": "rule_extractor/rule_extractor.py",
        "commit_date": "2023-05-28T14:51:11Z",
        "message": "Change APIs-guru path"
    },
    {
        "repo_url": "github.com/codingsoo/nlp2rest",
        "filepath": "rule_extractor/rule_extractor.py",
        "commit_date": "2023-05-28T14:41:51Z",
        "message": "Make a separate directory for NLP part"
    },
    {
        "repo_url": "github.com/lxucs/multilingual-mrc-isdg",
        "filepath": "isdg/preprocess.py",
        "commit_date": "2021-12-02T15:06:44Z",
        "message": "Save"
    },
    {
        "repo_url": "github.com/mayubo2333/fewshot_ED",
        "filepath": "dataset_processing/k_shot/construct_fewshot_dataset.py",
        "commit_date": "2023-05-07T07:57:02Z",
        "message": "add dataset processing modules"
    },
    {
        "repo_url": "github.com/PlusLabNLP/TagPrime",
        "filepath": "preprocessing/process_ace05ep.py",
        "commit_date": "2023-07-11T20:18:30Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/THUNLP-MT/Template-NMT",
        "filepath": "lexical_scripts/terminology_evaluation/evaluate_term_plain.py",
        "commit_date": "2022-11-06T11:54:16Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/tanfiona/CauseEffectDetection",
        "filepath": "preprocess.py",
        "commit_date": "2021-09-07T12:54:40Z",
        "message": "Upload main codes"
    },
    {
        "repo_url": "github.com/tanfiona/CauseEffectDetection",
        "filepath": "src/process/dataset.py",
        "commit_date": "2021-09-07T12:54:40Z",
        "message": "Upload main codes"
    },
    {
        "repo_url": "github.com/FarimaFatahi/CompactIE",
        "filepath": "data/compactness_measurements.py",
        "commit_date": "2023-04-24T05:13:48Z",
        "message": "Add files via upload\n\nThis script measures compactness metrics (ACL, NCC, RPA) for a CompactIE output file."
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-08-28T14:21:00Z",
        "message": "Whole word masking"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-08-09T11:51:49Z",
        "message": "Fasttext imports moved inside methods"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-07-23T11:54:00Z",
        "message": "Transformer attention mask support"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-07-18T11:23:14Z",
        "message": "Bug fixes"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-07-18T11:03:34Z",
        "message": "Bug fixes"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-07-17T09:21:35Z",
        "message": "Autocast switch, dev for testing, model cache"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-07-15T12:53:29Z",
        "message": "Enable control of loss variables"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-07-09T16:08:38Z",
        "message": "Regularization scheduling facility"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-07-06T14:58:46Z",
        "message": "Control num encoders and decoders in Transformer"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-07-04T13:08:43Z",
        "message": "Reduce timing"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-07-04T09:48:56Z",
        "message": "Try Lang features speed up"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-06-30T16:21:17Z",
        "message": "Adding Ensemble Transformer, Adding direct word vector getting for MultiImageMultiText"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-06-30T11:45:06Z",
        "message": "Bug fix"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-06-30T11:04:46Z",
        "message": "Bug fix"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-06-30T11:00:29Z",
        "message": "Bug fix"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-06-30T10:56:41Z",
        "message": "Bug fixes"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-06-30T10:39:04Z",
        "message": "Bug fixes"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-06-30T10:23:36Z",
        "message": "Bug fix"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-06-29T14:00:33Z",
        "message": "Fixing LangFeatures to remove multi-rake as strict dependency"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-06-25T10:56:01Z",
        "message": "Updating Setup process"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-06-19T11:42:34Z",
        "message": "Detectron Features based Attention Support"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-06-17T18:26:26Z",
        "message": "Adding VisualBert and VilBert model retrieve and Detectron Feature Retrieve"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-06-14T20:19:39Z",
        "message": "Multi-Image Multi-Text Attention Early Fusion model"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-06-14T15:33:53Z",
        "message": "Multi-task support"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-06-14T08:56:10Z",
        "message": "Image features from multiple pytorch pretrained classification models"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-06-14T07:27:17Z",
        "message": "Image features from multiple pytorch pretrained classification models"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-06-13T08:16:52Z",
        "message": "Augmentation and multi-eval preparation"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-06-11T07:11:44Z",
        "message": "Augmentation and multi-eval preparation"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-06-10T09:22:15Z",
        "message": "Augmentation and multi-eval preparation"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-06-08T15:24:34Z",
        "message": "Augmentation and multi-eval preparation"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-06-06T18:55:16Z",
        "message": "Augmentation and multi-eval preparation"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-06-02T19:03:57Z",
        "message": "Text Aug"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-06-01T15:13:36Z",
        "message": "Changes"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-05-31T12:06:00Z",
        "message": "Fix CNN1D"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-05-30T20:10:38Z",
        "message": "Refine CNN 1d"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-05-30T14:34:59Z",
        "message": "Refactor"
    },
    {
        "repo_url": "github.com/faizanahemad/facebook-hateful-memes",
        "filepath": "facebook_hateful_memes_detector/models/text_models/LangFeatures.py",
        "commit_date": "2020-05-29T18:25:46Z",
        "message": "Gensim Model"
    },
    {
        "repo_url": "github.com/awebson/congressional_adversary",
        "filepath": "src/preprocess_PN/S1_tokenize.py",
        "commit_date": "2020-08-25T13:56:59Z",
        "message": "refactor models.dual_grounded"
    },
    {
        "repo_url": "github.com/awebson/congressional_adversary",
        "filepath": "src/preprocess_PN/S1_tokenize_with_NER.py",
        "commit_date": "2020-08-25T13:56:59Z",
        "message": "refactor models.dual_grounded"
    },
    {
        "repo_url": "github.com/AnzorGozalishvili/sifrank_serving",
        "filepath": "api.py",
        "commit_date": "2020-03-28T18:41:53Z",
        "message": "Update requirements with waitress. Add waitress in Flask API. Update readme."
    },
    {
        "repo_url": "github.com/AnzorGozalishvili/sifrank_serving",
        "filepath": "api.py",
        "commit_date": "2020-03-28T12:24:52Z",
        "message": "Update API to support post request with json content. Update README with code snippets and curl example."
    },
    {
        "repo_url": "github.com/AnzorGozalishvili/sifrank_serving",
        "filepath": "api.py",
        "commit_date": "2020-03-20T15:56:51Z",
        "message": "update data directory usage in scripts."
    },
    {
        "repo_url": "github.com/AnzorGozalishvili/sifrank_serving",
        "filepath": "api.py",
        "commit_date": "2020-03-20T15:49:05Z",
        "message": "udapte dockerfile. update api"
    },
    {
        "repo_url": "github.com/AnzorGozalishvili/sifrank_serving",
        "filepath": "api.py",
        "commit_date": "2020-03-20T15:45:55Z",
        "message": "remove unnecessary cuda device argument."
    },
    {
        "repo_url": "github.com/AnzorGozalishvili/sifrank_serving",
        "filepath": "api.py",
        "commit_date": "2020-03-20T14:39:53Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/AnzorGozalishvili/sifrank_serving",
        "filepath": "test/test.py",
        "commit_date": "2020-03-20T14:39:53Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/sunnweiwei/MixCL",
        "filepath": "mixup.py",
        "commit_date": "2022-08-15T05:26:07Z",
        "message": "Readme"
    },
    {
        "repo_url": "github.com/asyml/forte-wrappers",
        "filepath": "src/stanza/fortex/stanza/stanza_processor.py",
        "commit_date": "2022-06-24T02:56:50Z",
        "message": "Fix stanfordnlp_processor_test.py (#112)\n\n* Fix outdated stanza\n\n* Fix stanza test\n\nCo-authored-by: Suqi Sun <suqi.sun@petuum.com>"
    },
    {
        "repo_url": "github.com/asyml/forte-wrappers",
        "filepath": "src/stanza/fortex/stanza/stanza_processor.py",
        "commit_date": "2022-06-18T19:07:58Z",
        "message": "Support Bio NER using stanza processor (#111)\n\n* Support NER and Bio NER using Stanza processor\n\n* update stanfordnlp_processor_test.py\n\n* update standfordnlp_processor_test.py\n\n* support Bio NER using stanza processor\n\n* add test for bio ner in stanza processor\n\n* Minor changes in stanza_processor\n\n* Update stanfordnlp_processor_test.py\n\n* Fix minor build error\n\n* fix an autoformatting issue provided by Black"
    },
    {
        "repo_url": "github.com/asyml/forte-wrappers",
        "filepath": "src/stanza/fortex/stanza/stanza_processor.py",
        "commit_date": "2021-09-08T14:31:02Z",
        "message": "Change wrapper package from Forte to Fortex (#73)\n\n* Move to fortex\n\n* Fix import in test.\n\n* Rename package.\n\n* Fix examples.\n\n* Update init imports.\n\n* Update test strings.\n\n* Fix imports in elastics.\n\n* Fix imports in allennlp.\n\n* Typo\n\n* Fix class name in docs.\n\n* Clean up documentation.\n\n* Add docuemntation back.\n\n* Fixing documentation and gpu setups.\n\n* Black all setups.\n\n* Remove obsolete configs"
    },
    {
        "repo_url": "github.com/john-hewitt/backpacks-flash-attn",
        "filepath": "training/src/test_topic.py",
        "commit_date": "2023-05-28T01:18:45Z",
        "message": "acl2023 commit"
    },
    {
        "repo_url": "github.com/lucy3/gpt3_gender",
        "filepath": "code/segment_original_books.py",
        "commit_date": "2021-02-02T05:53:10Z",
        "message": "word embed"
    },
    {
        "repo_url": "github.com/lucy3/gpt3_gender",
        "filepath": "code/segment_original_books.py",
        "commit_date": "2021-01-21T00:43:12Z",
        "message": "get baby names"
    },
    {
        "repo_url": "github.com/lucy3/gpt3_gender",
        "filepath": "code/segment_original_books.py",
        "commit_date": "2020-11-25T21:00:43Z",
        "message": "it hopefully works now"
    },
    {
        "repo_url": "github.com/lucy3/gpt3_gender",
        "filepath": "code/segment_original_books.py",
        "commit_date": "2020-11-25T04:44:06Z",
        "message": "my best attempt yet, but it is still no good..."
    },
    {
        "repo_url": "github.com/lucy3/gpt3_gender",
        "filepath": "code/segment_original_books.py",
        "commit_date": "2020-11-25T03:35:02Z",
        "message": "seems like sentence IDs in booknlp are not deterministic"
    },
    {
        "repo_url": "github.com/lucy3/gpt3_gender",
        "filepath": "code/segment_original_books.py",
        "commit_date": "2020-11-25T00:17:53Z",
        "message": "this stanza thing doesn't seem to sentence tokenize well, so will use tokens instead"
    },
    {
        "repo_url": "github.com/jinpeng01/WGSum",
        "filepath": "src/prepro/data_builder.py",
        "commit_date": "2021-12-18T09:19:39Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/jinpeng01/WGSum",
        "filepath": "graph_construction/graph_construction.py",
        "commit_date": "2021-12-18T09:19:39Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/tsproisl/textcomplexity",
        "filepath": "utils/run_stanza.py",
        "commit_date": "2024-01-22T20:00:26Z",
        "message": "Replace deprecated convert_dict() with write_doc2conll() (#6)"
    },
    {
        "repo_url": "github.com/tsproisl/textcomplexity",
        "filepath": "utils/run_stanza.py",
        "commit_date": "2021-07-16T06:14:46Z",
        "message": "Download stanza resources file, if it does not exist (issue #1)"
    },
    {
        "repo_url": "github.com/tsproisl/textcomplexity",
        "filepath": "utils/run_stanza.py",
        "commit_date": "2020-09-29T09:51:02Z",
        "message": "Utility script for parsing texts with stanza"
    },
    {
        "repo_url": "github.com/eslambakr/HRS_benchmark",
        "filepath": "codes/t2i_models/Structured-Diffusion-Guidance-master/scripts/txt2img_demo.py",
        "commit_date": "2023-04-11T11:09:43Z",
        "message": "add struct-diff model codes"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/dep_parser.py",
        "commit_date": "2021-07-26T19:36:16Z",
        "message": "new parser module"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/dep_parser.py",
        "commit_date": "2020-07-21T09:04:42Z",
        "message": "some fixes and freezing libs"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/dep_parser.py",
        "commit_date": "2020-07-15T12:48:29Z",
        "message": "double os.sep"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/dep_parser.py",
        "commit_date": "2020-07-14T20:37:56Z",
        "message": "download dep parser models to parser-dependencies"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/dep_parser.py",
        "commit_date": "2020-04-17T19:23:10Z",
        "message": "fix gpu memory leak"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/dep_parser.py",
        "commit_date": "2020-03-02T17:46:18Z",
        "message": "leave out dep_rel"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/dep_parser.py",
        "commit_date": "2020-03-01T16:40:56Z",
        "message": "replace None lemmas with %NONE%"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/dep_parser.py",
        "commit_date": "2020-02-29T22:08:28Z",
        "message": "give nlp1 lists instead of string\n\nsome tokens have whitespace in them and we need to use a list to make sure snlp understands its tokenization properly"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/dep_parser.py",
        "commit_date": "2020-02-29T16:56:33Z",
        "message": "accept '<s ' or '<s>' as start of a sent"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/dep_parser.py",
        "commit_date": "2020-02-29T16:55:09Z",
        "message": "lemma None handling + a very important space"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/dep_parser.py",
        "commit_date": "2020-02-29T04:15:15Z",
        "message": "add dep parser"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/dep_parser.py",
        "commit_date": "2020-02-28T22:59:52Z",
        "message": "dep parser v1.1 test passed"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/dep_parser.py",
        "commit_date": "2020-02-28T01:59:24Z",
        "message": "dep parser v1.0"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/dep_parser.py",
        "commit_date": "2020-02-27T19:13:41Z",
        "message": "dep parser v1.0"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/dep_parser.py",
        "commit_date": "2020-02-27T14:45:29Z",
        "message": "dep parser v0.21"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/dep_parser.py",
        "commit_date": "2020-02-27T03:48:01Z",
        "message": "dep parser v0.2"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/dep_parser.py",
        "commit_date": "2020-02-26T06:02:14Z",
        "message": "dep parser v0.1"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/pos_tagger.py",
        "commit_date": "2021-07-12T13:34:16Z",
        "message": "rework ensemble POS tagger\n  * Make trainable in repo code\n  * Add flair OntoNotes NER model as predictor\n  * Replace StanfordNLP with newer Stanza\n  * Use trigram context for ensemble prediction"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/pos_tagger.py",
        "commit_date": "2020-07-14T20:58:38Z",
        "message": "support newer flair void predict implementation\n  * flair >=0.5 just modifies the input Sentence objects by ref\n  * does not return prediction for .predict() method"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/pos_tagger.py",
        "commit_date": "2020-06-22T20:06:53Z",
        "message": "support flair 0.5 label syntax"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/pos_tagger.py",
        "commit_date": "2020-06-22T19:46:20Z",
        "message": "Use io to explicitly set encoding"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/pos_tagger.py",
        "commit_date": "2020-06-22T19:45:19Z",
        "message": "Use global config\n  * enable --use-gpu flag\n  * test dependencies on module init"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/pos_tagger.py",
        "commit_date": "2020-06-22T19:44:28Z",
        "message": "verify or get models from corpling"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/pos_tagger.py",
        "commit_date": "2020-06-22T19:43:51Z",
        "message": "Rename models\n  * Explicitly use versioned flair modeld, do not rely on what flair refers to by 'pos'"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/pos_tagger.py",
        "commit_date": "2020-02-29T21:01:30Z",
        "message": "fix whitespace in token bug"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/pos_tagger.py",
        "commit_date": "2020-02-29T04:16:17Z",
        "message": "fix pos tagger deps"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/pos_tagger.py",
        "commit_date": "2020-02-27T19:57:42Z",
        "message": "add with statements"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/pos_tagger.py",
        "commit_date": "2020-02-27T19:48:18Z",
        "message": "keep some models in memory"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/pos_tagger.py",
        "commit_date": "2020-02-27T19:10:10Z",
        "message": "cleaned up tagging code and changed output format"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/pos_tagger.py",
        "commit_date": "2020-02-27T19:09:06Z",
        "message": "Merge branch 'master' of https://github.com/gucorpling/autogum"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/pos_tagger.py",
        "commit_date": "2020-02-27T19:08:39Z",
        "message": "cleaned up tagging code and changed output format"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/pos_tagger.py",
        "commit_date": "2020-02-27T19:07:58Z",
        "message": "update dependencies for pos tagger module"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/pos_tagger.py",
        "commit_date": "2020-02-23T21:16:44Z",
        "message": "updated tagger test_dependencies"
    },
    {
        "repo_url": "github.com/gucorpling/amalgum",
        "filepath": "nlp_modules/pos_tagger.py",
        "commit_date": "2020-02-23T01:38:14Z",
        "message": "first version of tagger"
    },
    {
        "repo_url": "github.com/jkallini/mission-impossible-language-models",
        "filepath": "data/tag.py",
        "commit_date": "2024-01-15T20:06:28Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/jkallini/mission-impossible-language-models",
        "filepath": "edge_probing/get_constituency_parses.py",
        "commit_date": "2024-01-15T20:06:28Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/KaijuML/dtt-multi-branch",
        "filepath": "onmt/metrics/hss.py",
        "commit_date": "2020-09-10T20:42:48Z",
        "message": "bugfix HS for empty sentences"
    },
    {
        "repo_url": "github.com/KaijuML/dtt-multi-branch",
        "filepath": "onmt/metrics/hss.py",
        "commit_date": "2020-09-10T20:40:07Z",
        "message": "bugfix HS for empty sentences"
    },
    {
        "repo_url": "github.com/KaijuML/dtt-multi-branch",
        "filepath": "onmt/metrics/hss.py",
        "commit_date": "2020-09-10T18:53:39Z",
        "message": "bugfix HS for empty sentences"
    },
    {
        "repo_url": "github.com/KaijuML/dtt-multi-branch",
        "filepath": "onmt/metrics/hss.py",
        "commit_date": "2020-09-10T09:18:45Z",
        "message": "HSA RL metric"
    },
    {
        "repo_url": "github.com/KaijuML/dtt-multi-branch",
        "filepath": "onmt/metrics/hss.py",
        "commit_date": "2020-09-10T06:05:40Z",
        "message": "HSS is working"
    },
    {
        "repo_url": "github.com/KaijuML/dtt-multi-branch",
        "filepath": "onmt/metrics/hss.py",
        "commit_date": "2020-09-10T06:04:10Z",
        "message": "HSS is working"
    },
    {
        "repo_url": "github.com/KaijuML/dtt-multi-branch",
        "filepath": "onmt/metrics/hss.py",
        "commit_date": "2020-09-09T16:31:35Z",
        "message": "fix RL"
    },
    {
        "repo_url": "github.com/KaijuML/dtt-multi-branch",
        "filepath": "onmt/metrics/hss.py",
        "commit_date": "2020-09-09T16:30:47Z",
        "message": "fix RL"
    },
    {
        "repo_url": "github.com/KaijuML/dtt-multi-branch",
        "filepath": "onmt/metrics/hss.py",
        "commit_date": "2020-09-09T16:17:06Z",
        "message": "HSS reinforcement learning metric"
    },
    {
        "repo_url": "github.com/KaijuML/dtt-multi-branch",
        "filepath": "onmt/metrics/hss.py",
        "commit_date": "2020-09-08T08:43:38Z",
        "message": "added possibility of training with RL"
    },
    {
        "repo_url": "github.com/KaijuML/dtt-multi-branch",
        "filepath": "data/dep_parsing_gpu.py",
        "commit_date": "2021-07-01T09:39:16Z",
        "message": "Solving issue #3"
    },
    {
        "repo_url": "github.com/KaijuML/dtt-multi-branch",
        "filepath": "data/dep_parsing_gpu.py",
        "commit_date": "2021-07-01T09:27:15Z",
        "message": "Solving issue #3"
    },
    {
        "repo_url": "github.com/taishan1994/python_common_code_collection",
        "filepath": "src/segment.py",
        "commit_date": "2023-03-15T02:49:43Z",
        "message": "Create"
    },
    {
        "repo_url": "github.com/HCDM/XRec",
        "filepath": "SAER/data/extact_exp_data.py",
        "commit_date": "2022-02-28T00:43:37Z",
        "message": "init SAER"
    },
    {
        "repo_url": "github.com/HCDM/XRec",
        "filepath": "SAER/data/extract_features.py",
        "commit_date": "2022-02-28T00:43:37Z",
        "message": "init SAER"
    },
    {
        "repo_url": "github.com/HCDM/XRec",
        "filepath": "CompExp/data/extract_exp_data.py",
        "commit_date": "2024-01-18T07:47:24Z",
        "message": "add missing config & extract trainer"
    },
    {
        "repo_url": "github.com/HCDM/XRec",
        "filepath": "CompExp/data/extract_features.py",
        "commit_date": "2024-01-18T07:47:24Z",
        "message": "add missing config & extract trainer"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2021-02-01T09:10:02Z",
        "message": "Use stanza instead of #22"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2021-01-21T15:40:09Z",
        "message": "Fix up datapaths and log paths, cache vectors"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2021-01-21T09:12:13Z",
        "message": "Hook up rough version of a stanza Processor"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2021-01-14T13:40:09Z",
        "message": "Changed how logging is done"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2021-01-14T13:16:48Z",
        "message": "Added logging info and raise error #24"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2021-01-08T13:08:57Z",
        "message": "Move default data dir and create if not exists"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2020-12-16T16:08:57Z",
        "message": "Made predictor class and moved around data config\""
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2020-10-27T15:07:56Z",
        "message": "conform to pep8"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2020-10-07T13:52:17Z",
        "message": "split config in 2 files"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2020-09-18T13:52:56Z",
        "message": "Move nltk import into function that uses it"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2020-09-18T12:52:29Z",
        "message": "Added option to specify cfg file in scripts"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2020-07-15T11:33:45Z",
        "message": "Added possibility to predict on untokenized txt file"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2020-07-08T12:47:56Z",
        "message": "Use BERT embedder on the fly if not cached #1"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2020-07-06T15:22:14Z",
        "message": "Make training script work"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2020-07-06T14:07:36Z",
        "message": "Autopeped the code"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2020-07-06T12:49:38Z",
        "message": "Improved interface of predict script"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2020-07-03T17:21:41Z",
        "message": "copied code from e2e and made some small adjustments"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "test/test_stanza.py",
        "commit_date": "2021-01-25T18:58:21Z",
        "message": "Reorder imports"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "test/test_stanza.py",
        "commit_date": "2021-01-25T13:52:45Z",
        "message": "Add requirement and test"
    },
    {
        "repo_url": "github.com/AutumnCrocus/shadow_sim",
        "filepath": "card2vec.py",
        "commit_date": "2021-07-06T09:07:39Z",
        "message": "test"
    },
    {
        "repo_url": "github.com/lingmod-tue/stanza-api",
        "filepath": "main.py",
        "commit_date": "2020-06-22T14:14:56Z",
        "message": "Add support for specifying language packages"
    },
    {
        "repo_url": "github.com/lingmod-tue/stanza-api",
        "filepath": "main.py",
        "commit_date": "2020-06-20T20:31:00Z",
        "message": "simplification"
    },
    {
        "repo_url": "github.com/lingmod-tue/stanza-api",
        "filepath": "main.py",
        "commit_date": "2020-06-19T16:06:59Z",
        "message": "better ordering"
    },
    {
        "repo_url": "github.com/lingmod-tue/stanza-api",
        "filepath": "main.py",
        "commit_date": "2020-06-19T15:57:36Z",
        "message": "Basic working version, including Dockerfile."
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-09-08T14:10:51Z",
        "message": "Dockerise the application (#77)\n\n* feat: add Dockerfile for app deployement\n\n* Update .gitignore\n\n* fix: correct model path for Dockerfile\n\n* fix: allow deployement of docker with nginx"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-08-31T10:58:52Z",
        "message": "improved streamlit app"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-08-29T12:14:31Z",
        "message": "improved returned json"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-08-26T09:51:23Z",
        "message": "Merge remote-tracking branch 'origin/master'"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-08-26T09:50:33Z",
        "message": "improve streamlit app"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-08-25T14:10:10Z",
        "message": "Create DockerFile for deploying app  (#76)\n\n* feat: add Dockerfile for app deployement\n\n* Update .gitignore"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-08-24T14:13:52Z",
        "message": "added streamlit app"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-08-24T12:14:01Z",
        "message": "fixed bugs"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-08-09T08:45:38Z",
        "message": "load models in variables"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-08-02T07:03:48Z",
        "message": "fixed bugs"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-07-28T22:39:24Z",
        "message": "fixed bugs"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-07-26T10:05:03Z",
        "message": "fixed bug"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-07-26T10:02:56Z",
        "message": "fixed bug"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-07-26T10:01:55Z",
        "message": "fixed bug"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-07-26T08:05:23Z",
        "message": "added start and end date to tables"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-07-19T08:59:15Z",
        "message": "added rules, show result table"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-07-11T22:21:35Z",
        "message": "fixed pipeline"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-07-09T14:13:16Z",
        "message": "added location extractions from dataset of lacations"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-07-09T11:44:13Z",
        "message": "added scripts, first pipeline"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline_st.py",
        "commit_date": "2021-09-08T14:10:51Z",
        "message": "Dockerise the application (#77)\n\n* feat: add Dockerfile for app deployement\n\n* Update .gitignore\n\n* fix: correct model path for Dockerfile\n\n* fix: allow deployement of docker with nginx"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline_st.py",
        "commit_date": "2021-09-02T08:10:10Z",
        "message": "improved streamlit app"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline_st.py",
        "commit_date": "2021-08-31T10:58:52Z",
        "message": "improved streamlit app"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline_st.py",
        "commit_date": "2021-08-29T12:14:31Z",
        "message": "improved returned json"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline_st.py",
        "commit_date": "2021-08-29T11:56:33Z",
        "message": "improved app"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline_st.py",
        "commit_date": "2021-08-26T09:51:23Z",
        "message": "Merge remote-tracking branch 'origin/master'"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline_st.py",
        "commit_date": "2021-08-26T09:50:33Z",
        "message": "improve streamlit app"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline_st.py",
        "commit_date": "2021-08-25T14:10:10Z",
        "message": "Create DockerFile for deploying app  (#76)\n\n* feat: add Dockerfile for app deployement\n\n* Update .gitignore"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline_st.py",
        "commit_date": "2021-08-24T14:15:43Z",
        "message": "added streamlit app"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline_st.py",
        "commit_date": "2021-08-24T14:13:52Z",
        "message": "added streamlit app"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline_st.py",
        "commit_date": "2021-08-24T12:14:01Z",
        "message": "fixed bugs"
    },
    {
        "repo_url": "github.com/hjkgrp/text_mining_tools",
        "filepath": "text_mining_tools/MOF_stability_scripts/solvent_removal_stability_extraction.py",
        "commit_date": "2021-08-10T21:06:53Z",
        "message": "Cleaned up scripts for text mining MOF stability"
    },
    {
        "repo_url": "github.com/causalNLP/logical-fallacy",
        "filepath": "codes_for_models/experiments_round2/analyze_datasets.py",
        "commit_date": "2022-04-23T20:48:57Z",
        "message": "update readme and saved models"
    },
    {
        "repo_url": "github.com/causalNLP/logical-fallacy",
        "filepath": "codes_for_models/experiments_round2/logicclimate_split.py",
        "commit_date": "2022-04-23T20:48:57Z",
        "message": "update readme and saved models"
    },
    {
        "repo_url": "github.com/causalNLP/logical-fallacy",
        "filepath": "codes_for_models/experiments_round2/remove_content_words.py",
        "commit_date": "2023-12-11T18:36:37Z",
        "message": "Update remove_content_words.py"
    },
    {
        "repo_url": "github.com/causalNLP/logical-fallacy",
        "filepath": "codes_for_models/experiments_round2/remove_content_words.py",
        "commit_date": "2022-04-23T20:48:57Z",
        "message": "update readme and saved models"
    },
    {
        "repo_url": "github.com/texttechnologylab/textimager-uima",
        "filepath": "textimager-uima-stanza/src/main/docker/stanza_service.py",
        "commit_date": "2021-09-17T11:24:17Z",
        "message": "Update stanza_service.py for Multi and Sentiment, Add StanzaMultiTagger and StanzaMultiTaggerTest"
    },
    {
        "repo_url": "github.com/texttechnologylab/textimager-uima",
        "filepath": "textimager-uima-stanza/src/main/docker/stanza_service.py",
        "commit_date": "2021-09-16T19:22:02Z",
        "message": "Change Route in Stanza to \\multi (doens't work yet, why?) and seperate Tests for SpaCyMultiTagger3Tests. Cointains up to Greek until now"
    },
    {
        "repo_url": "github.com/texttechnologylab/textimager-uima",
        "filepath": "textimager-uima-stanza/src/main/docker/stanza_service.py",
        "commit_date": "2021-09-16T15:43:45Z",
        "message": "First working Test on SpaCyMultiTagger3Test"
    },
    {
        "repo_url": "github.com/texttechnologylab/textimager-uima",
        "filepath": "textimager-uima-stanza/src/main/docker/stanza_service.py",
        "commit_date": "2021-08-20T13:40:31Z",
        "message": "Updated Sentiment Tools"
    },
    {
        "repo_url": "github.com/texttechnologylab/textimager-uima",
        "filepath": "textimager-uima-stanza/src/main/docker/stanza_service.py",
        "commit_date": "2021-08-13T20:31:25Z",
        "message": "Easier Docker image handling, Sentiment base class with annotator comments, German Bert Sentiment, GerVader, Stanza Sentiment, TextBlob Sentiment"
    },
    {
        "repo_url": "github.com/texttechnologylab/textimager-uima",
        "filepath": "textimager-uima-stanza/src/main/docker/stanza_service.py",
        "commit_date": "2021-07-21T16:22:18Z",
        "message": "Stanza sentiment: Fix switched key error"
    },
    {
        "repo_url": "github.com/texttechnologylab/textimager-uima",
        "filepath": "textimager-uima-stanza/src/main/docker/stanza_service.py",
        "commit_date": "2021-07-21T13:52:00Z",
        "message": "Added Stanza sentiment"
    },
    {
        "repo_url": "github.com/natasha/naeval",
        "filepath": "docker/stanza/app.py",
        "commit_date": "2020-06-23T05:43:33Z",
        "message": "Add stanza ner, morph, syntax"
    },
    {
        "repo_url": "github.com/GateNLP/gateplugin-Python",
        "filepath": "src/main/resources/resources/pipelines/python-stanford-stanza.py",
        "commit_date": "2021-02-20T09:28:08Z",
        "message": "Update pipeline parameters, update gatenlp."
    },
    {
        "repo_url": "github.com/GateNLP/gateplugin-Python",
        "filepath": "src/main/resources/resources/pipelines/python-stanford-stanza.py",
        "commit_date": "2020-08-22T17:21:39Z",
        "message": "Stanza resources and some empty documentation files"
    },
    {
        "repo_url": "github.com/AdrianBZG/SQLformer",
        "filepath": "preprocessing/vocabulary_handler.py",
        "commit_date": "2023-12-13T10:27:50Z",
        "message": "Update"
    },
    {
        "repo_url": "github.com/AdrianBZG/SQLformer",
        "filepath": "preprocessing/vocabulary_handler.py",
        "commit_date": "2023-09-12T10:41:03Z",
        "message": "Upload"
    },
    {
        "repo_url": "github.com/AdrianBZG/SQLformer",
        "filepath": "preprocessing/spider_questions_to_graph.py",
        "commit_date": "2023-12-14T01:33:39Z",
        "message": "Update"
    },
    {
        "repo_url": "github.com/AdrianBZG/SQLformer",
        "filepath": "preprocessing/spider_questions_to_graph.py",
        "commit_date": "2023-12-13T10:27:50Z",
        "message": "Update"
    },
    {
        "repo_url": "github.com/AdrianBZG/SQLformer",
        "filepath": "preprocessing/spider_questions_to_graph.py",
        "commit_date": "2023-09-12T10:41:03Z",
        "message": "Upload"
    }
]