[
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-03-01T17:00:29Z",
        "message": "Fix deprecated arg issue (#29372)\n\n* Fix deprecated arg issue\n\n* Trainer check too\n\n* Check for dict or dataclass\n\n* Simplify, make config always AcceleratorConfig\n\n* Upstream to Trainer"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-02-26T10:35:37Z",
        "message": "Add feature extraction mapping for automatic metadata update (#28944)\n\n* add feature extraction mapping\n\n* added prefix\n\n* ruff check\n\n* minor fix\n\n* Update modeling_auto.py\n\n* fix typo\n\n* remove prefix to make variable public/importable\n\n* Update src/transformers/models/auto/modeling_auto.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* fixes\n\n* addressed comments\n\n* nit\n\n* fix-copies\n\n* remove from tests\n\n* this should fix\n\n* Update tests/models/convnextv2/test_modeling_convnextv2.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* nits\n\n---------\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-02-20T11:45:08Z",
        "message": "FIX [`PEFT` / `Trainer` ] Handle better peft + quantized compiled models (#29055)\n\n* handle peft + compiled models\n\n* add tests\n\n* fixup\n\n* adapt from suggestions\n\n* clarify comment"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-02-20T01:43:02Z",
        "message": "FEAT [`Trainer` / `bnb`]: Add RMSProp from `bitsandbytes` to HF `Trainer` (#29082)\n\n* add RMSProp to Trainer\n\n* revert some change\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n---------\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-02-19T19:07:41Z",
        "message": "storing & logging gradient norm in trainer (#27326)\n\n* report grad_norm during training\n\n* support getting grad_norm from deepspeed"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-02-16T12:41:09Z",
        "message": "`auto_find_batch_size` isn't yet supported with DeepSpeed/FSDP. Raise error accrodingly. (#29058)\n\nUpdate trainer.py"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-02-14T22:56:35Z",
        "message": "FIX [`Trainer` / tags]: Fix trainer + tags when users do not pass `\"tags\"` to `trainer.push_to_hub()` (#29009)\n\n* fix trainer tags\n\n* add test"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-02-14T21:44:49Z",
        "message": "[TPU] Support PyTorch/XLA FSDP via SPMD (#28949)\n\n* Initial commit\n\n* Add guards for the global mesh\n\n* Address more comments\n\n* Move the dataloader into integrations/tpu.py\n\n* Fix linters\n\n* Make karg more explicitly\n\n* Remove the move device logic\n\n* Fix the CI\n\n* Fix linters\n\n* Re-enable checkpointing"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-02-14T15:18:09Z",
        "message": "Introduce AcceleratorConfig dataclass (#28664)\n\n* Introduce acceleratorconfig dataclass\n\n* Extra second warn\n\n* Move import\n\n* Try moving import under is_accelerate_available\n\n* Quality\n\n* Apply suggestions from code review\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Clean\n\n* Remove to_kwargs\n\n* Change version\n\n* Improve tests by including dispatch and split batches\n\n* Improve reliability\n\n* Update tests/trainer/test_trainer.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Fixup tests and review nits\n\n* Make tests pass\n\n* protect import\n\n* Protect import\n\n* Empty-Commit\n\n* Make training_args.to_dict handle the AcceleratorConfig\n\n---------\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-02-14T00:30:23Z",
        "message": "ENH [`AutoQuantizer`]: enhance trainer + not supported quant methods (#28991)\n\n* enhance trainer + not support quant methods\n\n* remove all old logic\n\n* add version"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-02-12T15:47:21Z",
        "message": "Clean up staging tmp checkpoint directory (#28848)\n\nclean up remaining tmp checkpoint dir\n\nSigned-off-by: woshiyyya <xiaoyunxuan1998@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-02-06T05:55:44Z",
        "message": "Raise error when using `save_only_model` with `load_best_model_at_end` for DeepSpeed/FSDP (#28866)\n\n* Raise error when using `save_only_model` with `load_best_model_at_end` for DeepSpeed/FSDP\n\n* Update trainer.py"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-02-06T02:21:50Z",
        "message": "Do not use mtime for checkpoint rotation. (#28862)\n\nResolve https://github.com/huggingface/transformers/issues/26961"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-02-02T15:48:01Z",
        "message": "Reduce GPU memory usage when using FSDP+PEFT (#28830)\n\nsupport FSDP+PEFT"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-01-31T12:58:26Z",
        "message": "Resolve DeepSpeed cannot resume training with PeftModel (#28746)\n\n* fix: resolve deepspeed resume peft model issues\n\n* chore: update something\n\n* chore: update model instance pass into is peft model checks\n\n* chore: remove hard code value to tests\n\n* fix: format code"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-01-29T17:10:15Z",
        "message": "Support saving only PEFT adapter in checkpoints when using PEFT + FSDP (#28297)\n\n* Update trainer.py\n\n* Revert \"Update trainer.py\"\n\nThis reverts commit 0557e2cc9effa3a41304322032239a3874b948a7.\n\n* Make trainer.py use adapter_only=True when using FSDP + PEFT\n\n* Support load_best_model with adapter_only=True\n\n* Ruff format\n\n* Inspect function args for save_ load_ fsdp utility functions and only pass adapter_only=True if they support it"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-01-26T12:00:49Z",
        "message": "Fix `weights_only` (#28725)\n\nfix\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-01-26T11:05:01Z",
        "message": "support PeftMixedModel signature inspect (#28321)\n\n* support PeftMixedModel signature inspect\n\n* import PeftMixedModel only peft>=0.7.0\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* fix styling\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* style fixup\n\n* fix note\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-01-25T09:34:52Z",
        "message": "[`chore`] Add missing space in warning (#28695)\n\nAdd missing space in warning"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-01-24T11:57:45Z",
        "message": "Use save_safetensor to disable safe serialization for XLA (#28669)\n\n* Use save_safetensor to disable safe serialization for XLA\n\nhttps://github.com/huggingface/transformers/issues/28438\n\n* Style fixup"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-01-23T15:08:18Z",
        "message": "add dataloader prefetch factor in training args and trainer (#28498)\n\n* add dataloader prefetch factor in training args and trainer\n\n* remove trailing spaces\n\n* prevent dataloader_num_workers == 0 and dataloader_prefetch_factor != None\n\ndataloader_prefetch_factor works only when data is loaded in a different process as the main one. This commit adds the necessary checks to avoid having prefetch_factor set when there is no such process.\n\n* Remove whitespaces in empty line\n\n* Update src/transformers/training_args.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/training_args.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/training_args.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/training_args.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n---------\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-01-23T13:30:36Z",
        "message": "Fix windows err with checkpoint race conditions (#28637)\n\nFix windows err"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-01-18T10:55:29Z",
        "message": "Use `weights_only` only if torch >= 1.13 (#28506)\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-01-15T13:48:07Z",
        "message": "[`core`/ FEAT] Add the possibility to push custom tags using `PreTrainedModel` itself (#28405)\n\n* v1 tags\n\n* remove unneeded conversion\n\n* v2\n\n* rm unneeded warning\n\n* add more utility methods\n\n* Update src/transformers/utils/hub.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/utils/hub.py\n\nCo-authored-by: Lucain <lucainp@gmail.com>\n\n* Update src/transformers/utils/hub.py\n\nCo-authored-by: Lucain <lucainp@gmail.com>\n\n* more enhancements\n\n* oops\n\n* merge tags\n\n* clean up\n\n* revert unneeded change\n\n* add extensive docs\n\n* more docs\n\n* more kwargs\n\n* add test\n\n* oops\n\n* fix test\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Omar Sanseviero <osanseviero@gmail.com>\n\n* Update src/transformers/utils/hub.py\n\nCo-authored-by: Lucain <lucainp@gmail.com>\n\n* Update src/transformers/modeling_utils.py\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* add more conditions\n\n* more logic\n\n---------\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\nCo-authored-by: Lucain <lucainp@gmail.com>\nCo-authored-by: Omar Sanseviero <osanseviero@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-01-11T15:18:27Z",
        "message": "Byebye torch 1.10 (#28207)\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-01-10T15:55:42Z",
        "message": "Fix for checkpoint rename race condition (#28364)\n\n* Changed logic for renaming staging directory when saving checkpoint to only operate with the main process.\nAdded fsync functionality to attempt to flush the write changes in case os.rename is not atomic.\n\n* Updated styling using make fixup\n\n* Updated check for main process to use built-in versions from trainer\n\nCo-authored-by: Zach Mueller <muellerzr@gmail.com>\n\n* Fixed incorrect usage of trainer main process checks\nAdded with open usage to ensure better file closing as suggested from PR\nAdded rotate_checkpoints into main process logic\n\n* Removed \"with open\" due to not working with directory. os.open seems to work for directories.\n\n---------\n\nCo-authored-by: Zach Mueller <muellerzr@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-01-10T11:03:13Z",
        "message": "Support `DeepSpeed` when using auto find batch size (#28088)\n\nFixup test"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-01-02T14:19:42Z",
        "message": "fix bug:divide by zero in _maybe_log_save_evaluate() (#28251)\n\nCo-authored-by: liujizhong1 <liujizhong1@xiaomi.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2024-01-02T12:58:29Z",
        "message": "Fix trainer saving safetensors: metadata is None (#28219)\n\n* Update trainer.py\n\n* format"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-12-20T09:55:56Z",
        "message": "move code to Trainer.evaluate to enable use of that function with multiple datasets (#27844)\n\n* move code to Trainer.evaluate to enable use of that function with multiple datasets\n\n* test\n\n* update doc string\n\n* and a tip\n\n* forgot the type\n\n---------\n\nCo-authored-by: Prof. Peter Schneider-Kamp <jps@ordbogen.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-12-19T10:08:51Z",
        "message": "When save a model on TPU, make a copy to be moved to CPU (#27993)\n\n* When save a model, make a copy to be moved to CPU, dont move the original\nmodel\n\n* make deepcopy inside of _save_tpu\n\n* Move to tpu without copy"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-12-18T14:27:05Z",
        "message": "in peft finetune, only the trainable parameters need to be saved (#27825)\n\nto reduce the storage size and also save the time of checkpoint saving while using deepspeed for training\n\nSigned-off-by: Wang, Yi <yi.a.wang@intel.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-12-16T14:11:43Z",
        "message": "fix resuming from ckpt when using FSDP with FULL_STATE_DICT (#27891)\n\n* fix resuming from ckpt when suing FSDP with FULL_STATE_DICT\n\n* update tests\n\n* fix tests"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-12-15T16:18:56Z",
        "message": "Fix bug for checkpoint saving on multi node training setting (#28078)\n\n* add multi-node traning setting\n\n* fix style"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-12-15T15:01:18Z",
        "message": "make torch.load a bit safer (#27282)\n\n* make torch.load a bit safer\n\n* Fixes\n\n---------\n\nCo-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-12-13T17:17:30Z",
        "message": "Fix bug with rotating checkpoints (#28009)\n\n* Fix bug\n\n* Write test\n\n* Keep back old modification for grad accum steps\n\n* Whitespace...\n\n* Whitespace again\n\n* Race condition\n\n* Wait for everyone"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-12-11T19:30:11Z",
        "message": "Support PeftModel signature inspect (#27865)\n\n* Support PeftModel signature inspect\n\n* Use get_base_model() to get the base model\n\n---------\n\nCo-authored-by: shujunhua1 <shujunhua1@jd.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-12-09T10:04:13Z",
        "message": "[integration] Update Ray Tune integration for Ray 2.7 (#26499)\n\n* fix tune integration for ray 2.7+\n\nSigned-off-by: Justin Yu <justinvyu@anyscale.com>\n\n* add version check for ray tune backend availability\n\nSigned-off-by: Justin Yu <justinvyu@anyscale.com>\n\n* missing import\n\nSigned-off-by: Justin Yu <justinvyu@anyscale.com>\n\n* pin min version instead\n\nSigned-off-by: Justin Yu <justinvyu@anyscale.com>\n\n* address comments\n\nSigned-off-by: Justin Yu <justinvyu@anyscale.com>\n\n* some fixes\n\nSigned-off-by: Justin Yu <justinvyu@anyscale.com>\n\n* fix unnecessary final checkpoint\n\nSigned-off-by: Justin Yu <justinvyu@anyscale.com>\n\n* fix lint\n\nSigned-off-by: Justin Yu <justinvyu@anyscale.com>\n\n* dep table fix\n\nSigned-off-by: Justin Yu <justinvyu@anyscale.com>\n\n* fix lint\n\nSigned-off-by: Justin Yu <justinvyu@anyscale.com>\n\n---------\n\nSigned-off-by: Justin Yu <justinvyu@anyscale.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-12-08T16:51:02Z",
        "message": "Allow `resume_from_checkpoint` to handle `auto_find_batch_size` (#27568)\n\n* Fuffill request\n\n* Add test\n\n* Better test\n\n* Apply suggestions from code review\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Better test\n\n* Better test\n\n* MOre comments\n\n---------\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-12-08T13:08:54Z",
        "message": "fix: non-atomic checkpoint save (#27820)"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-12-07T12:06:02Z",
        "message": "update `create_model_card` to properly save peft details when using Trainer with PEFT (#27754)\n\n* update `create_model_card` to properly save peft details when using Trainer with PEFT\n\n* nit\n\n* Apply suggestions from code review\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-12-04T13:56:00Z",
        "message": "[Hot-Fix][XLA] Re-enable broken _tpu_save for XLATensors (#27799)\n\n* [XLA] Re-enable broken _tpu_save for XLATensors, by explicitly moving to cpu\n\n* linter-fix"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-12-04T06:43:32Z",
        "message": "Add `persistent_workers` parameter to `TrainingArguments` (#27189)\n\nadded param\n\nCo-authored-by: Ilya Fedorov <ilyaf@nvidia.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-11-28T16:40:44Z",
        "message": "Docs: Fix broken cross-references, i.e. `~transformer.` -> `~transformers.` (#27740)\n\n~transformer. -> ~transformers."
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-11-28T07:33:45Z",
        "message": "Fixed passing scheduler-specific kwargs via TrainingArguments lr_scheduler_kwargs (#27595)\n\n* Fix passing scheduler-specific kwargs through TrainingArguments `lr_scheduler_kwargs`\n\n* Added test for lr_scheduler_kwargs"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-11-27T16:26:33Z",
        "message": "docs: replace torch.distributed.run by torchrun (#27528)\n\n* docs: replace torch.distributed.run by torchrun\n\n `transformers` now officially support pytorch >= 1.10.\n The entrypoint `torchrun`` is present from 1.10 onwards.\n\nSigned-off-by: Peter Pan <Peter.Pan@daocloud.io>\n\n* Update src/transformers/trainer.py\n\nwith @ArthurZucker's suggestion\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Peter Pan <Peter.Pan@daocloud.io>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-11-24T06:10:52Z",
        "message": "Refactoring Trainer, adds `save_only_model` arg and simplifying FSDP integration (#27652)\n\n* add code changes\n\n1. Refactor FSDP\n2. Add `--save_only_model` option: When checkpointing, whether to only save the model, or also the optimizer, scheduler & rng state.\n3. Bump up the minimum `accelerate` version to `0.21.0`\n\n* quality\n\n* fix quality?\n\n* Revert \"fix quality?\"\n\nThis reverts commit 149330a6abc078827be274db84c8a2d26a76eba1.\n\n* fix fsdp doc strings\n\n* fix quality\n\n* Update src/transformers/training_args.py\n\nCo-authored-by: Zach Mueller <muellerzr@gmail.com>\n\n* please fix the quality issue \ud83d\ude05\n\n* Apply suggestions from code review\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* address comment\n\n* simplify conditional check as per the comment\n\n* update documentation\n\n---------\n\nCo-authored-by: Zach Mueller <muellerzr@gmail.com>\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-11-21T16:09:35Z",
        "message": "remove the deprecated method `init_git_repo` (#27617)\n\n* remove deprecated method `init_git_repo`\n\n* make style"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-11-14T20:31:04Z",
        "message": "Track the number of tokens seen to metrics (#27274)\n\n* Add tokens seen\n\n* Address comments, add to TrainingArgs\n\n* Update log\n\n* Apply suggestions from code review\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Use self.args\n\n* Fix docstring\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n---------\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-11-14T19:54:44Z",
        "message": "Have seq2seq just use gather (#27025)\n\n* Have seq2seq just use gather\n\n* Change\n\n* Reset after\n\n* Make slow\n\n* Apply suggestions from code review\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Clean\n\n* Simplify and just use gather\n\n* Update tests/trainer/test_trainer_seq2seq.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* gather always for seq2seq\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-11-07T21:40:00Z",
        "message": "Allow scheduler parameters (#26480)\n\n* Allow for scheduler kwargs\n\n* Formatting\n\n* Arguments checks, passing the tests\n\n* Black failed somehow\n\n---------\n\nCo-authored-by: Pierre <pierre@avatarin.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-11-02T20:08:03Z",
        "message": "Fixed base model class name extraction from PeftModels (#27162)\n\n* Fixed base model class name extraction from PeftModels\n\n* Changes to first unwrap the model then extract the base model name\n\n* Changed base_model to base_model.model to stay consistent with peft model abstractions"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-11-02T10:27:13Z",
        "message": "Reproducible checkpoint for npu (#27208)\n\n* save NPU's RNG states when saving a checkpoint and set after all the\ndata skip phase when resuming training.\n\n* re-trigger ci\n\n* re-trigger ci"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-11-01T18:42:38Z",
        "message": "Enable split_batches through TrainingArguments (#26798)\n\n* Enable split_batches through TrainingArguments\n\n* Extra dispatch_batches\n\n* Keep as default false\n\n* Add to docstring\n\n* Add to docstring\n\n* Remove the capturewarnings change\n\n* Comma"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-10-31T15:03:59Z",
        "message": "[FEAT] Add Neftune into transformers Trainer (#27141)\n\n* add v1 neftune\n\n* use `unwrap_model` instead\n\n* add test + docs\n\n* Apply suggestions from code review\n\nCo-authored-by: Zach Mueller <muellerzr@gmail.com>\n\n* more details\n\n* fixup\n\n* Update docs/source/en/main_classes/trainer.md\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* refactor a bit\n\n* more elaborated test\n\n* fix unwrap issue\n\n---------\n\nCo-authored-by: Zach Mueller <muellerzr@gmail.com>\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-10-30T11:55:03Z",
        "message": "remove the obsolete code related to fairscale FSDP (#26651)\n\n* remove the obsolete code related to fairscale FSDP\n\n* apple review suggestion"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-10-30T11:41:48Z",
        "message": "[`Trainer` / `GC`] Add `gradient_checkpointing_kwargs` in trainer and training arguments (#27068)\n\n* add `gradient_checkpointing_kwargs` in trainer and training arguments\n\n* add comment\n\n* add test - currently failing\n\n* now tests pass"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-10-26T16:13:19Z",
        "message": "Save TB logs as part of push_to_hub (#27022)\n\n* Support runs/\n\n* Upload runs folder as part of push to hub\n\n* Add a test\n\n* Add to test deps\n\n* Update with proposed solution from Slack\n\n* Ensure that repo gets deleted in tests"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-10-26T15:46:17Z",
        "message": "Correct docstrings and a typo in comments (#27047)\n\n* docs(training_args): correct docstrings\n\nCorrect docstrings of these methods in `TrainingArguments`:\n\n- `set_save`\n- `set_logging`\n\n* docs(training_args): adjust words in docstrings\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* docs(trainer): correct a typo in comments\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-10-26T09:20:11Z",
        "message": "Bring back `set_epoch` for Accelerate-based dataloaders (#26850)\n\n* Working tests!\n\n* Fix sampler\n\n* Fix\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Fix check\n\n* Clean\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-10-16T13:29:47Z",
        "message": "fix resume_from_checkpoint bug (#26739)\n\n* fix resume_from_checkpoint bug\n\n* update code"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-10-12T08:28:40Z",
        "message": "Add many missing spaces in adjacent strings (#26751)\n\nAdd missing spaces in adjacent strings"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-10-06T14:03:11Z",
        "message": "remove SharedDDP as it is deprecated (#25702)\n\n* remove SharedDDP as it was drepracated\n\n* apply review suggestion\n\n* make style\n\n* Oops,forgot to remove the compute_loss context manager in Seq2SeqTrainer.\n\n* remove the unnecessary conditional statement\n\n* keep the logic of IPEX\n\n* clean code\n\n* mix precision setup & make fixup\n\n---------\n\nCo-authored-by: statelesshz <jihuazhong1@huawei.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-10-04T13:13:37Z",
        "message": "Docstring check (#26052)\n\n* Fix number of minimal calls to the Hub with peft integration\n\n* Alternate design\n\n* And this way?\n\n* Revert\n\n* Nits to fix\n\n* Add util\n\n* Print when changes are made\n\n* Add list to ignore\n\n* Add more rules\n\n* Manual fixes\n\n* deal with kwargs\n\n* deal with enum defaults\n\n* avoid many digits for floats\n\n* Manual fixes\n\n* Fix regex\n\n* Fix regex\n\n* Auto fix\n\n* Style\n\n* Apply script\n\n* Add ignored list\n\n* Add check that templates are filled\n\n* Adding to CI checks\n\n* Add back semi-fix\n\n* Ignore more objects\n\n* More auto-fixes\n\n* Ignore missing objects\n\n* Remove temp semi-fix\n\n* Fixes\n\n* Update src/transformers/models/pvt/configuration_pvt.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update utils/check_docstrings.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update src/transformers/utils/quantization_config.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Deal with float defaults\n\n* Fix small defaults\n\n* Address review comment\n\n* Treat\n\n* Post-rebase cleanup\n\n* Address review comment\n\n* Update src/transformers/models/deprecated/mctct/configuration_mctct.py\n\nCo-authored-by: Lysandre Debut <lysandre.debut@reseau.eseo.fr>\n\n* Address review comment\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\nCo-authored-by: Lysandre Debut <lysandre.debut@reseau.eseo.fr>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-10-04T12:57:11Z",
        "message": "Extend Trainer to enable Ascend NPU to use the fused Adamw optimizer when training (#26194)"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-26T17:27:09Z",
        "message": "Add torch `RMSProp` optimizer (#26425)\n\nadd rmsprop"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-22T01:33:29Z",
        "message": "[QUICK FIX LINK] Update trainer.py (#26293)\n\n* Update trainer.py\n\nFix link\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update trainer.py\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-20T15:38:59Z",
        "message": "[`Trainer`] Refactor trainer + bnb logic (#26248)\n\n* refactor trainer + bnb logic\n\n* remove logger.info\n\n* oops"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-20T04:56:16Z",
        "message": "FSDP tests and checkpointing fixes (#26180)\n\n* add fsdp tests\n\n* Update test_fsdp.py\n\n* Update test_fsdp.py\n\n* fixes\n\n* checks\n\n* Update trainer.py\n\n* fix\n\n* fixes for saving/resuming checkpoints\n\n* fixes\n\n* add tests and delete debug statements\n\n* fixing tests\n\n* Update test_fsdp.py\n\n* fix tests\n\n* fix tests\n\n* minor nits\n\n* fix code style and quality\n\n* refactor and modularize test code\n\n* reduce the time of tests\n\n* reduce the test time\n\n* fix test\n\n* reduce test time\n\n* reduce test time\n\n* fix failing tests\n\n* fix\n\n* Apply suggestions from code review\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* resolve comments\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-18T15:40:11Z",
        "message": "refactor decay_parameters production into its own function (#26152)"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-14T09:57:47Z",
        "message": "Fix eval accumulation when `accelerate` > 0.20.3 (#26060)\n\nAs mentioned in: https://github.com/huggingface/transformers/issues/25641\n\nEval accumulation will never happen with `accelerate > 0.20.3`, so this change ensures that `sync_gradients` is ignored if accelerate is > 0.20.3"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-12T17:01:22Z",
        "message": "enable optuna multi-objectives feature (#25969)\n\n* enable optuna multi-objectives feature\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* update hpo doc\n\n* update docstring\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>\n\n* extend direction to List[str] type\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>\n\n* Update src/transformers/integrations/integration_utils.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-11T11:56:36Z",
        "message": "only main process should call _save on deepspeed zero3 (#25959)\n\nonly main process should call _save when deepspeed zero3"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-07T19:00:22Z",
        "message": "Try to fix training Loss inconsistent after resume from old checkpoint (#25872)\n\n* fix loss inconsistent after resume  #25340\n\n* fix typo\n\n* clean code\n\n* reformatted code\n\n* adjust code according to comments\n\n* adjust check_dataloader_randomsampler location\n\n* return sampler only\n\n* handle sampler is None\n\n* Update src/transformers/trainer_pt_utils.py\n\nthanks @amyeroberts\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n---------\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-07T16:17:30Z",
        "message": "Add `tgs` speed metrics (#25858)\n\n* Add tgs metrics\n\n* bugfix and black formatting\n\n* workaround for tokens counting\n\n* formating and bugfix\n\n* Fix\n\n* Add opt-in for tgs metrics\n\n* make style and fix error\n\n* Fix doc\n\n* fix docbuild\n\n* hf-doc-build\n\n* fix\n\n* test\n\n* Update src/transformers/training_args.py\n\nrenaming\n\nCo-authored-by: Zach Mueller <muellerzr@gmail.com>\n\n* Update src/transformers/training_args.py\n\nrenaming\n\nCo-authored-by: Zach Mueller <muellerzr@gmail.com>\n\n* Fix some symbol\n\n* test\n\n* Update src/transformers/trainer_utils.py\n\nmatch nameing patterns\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/training_args.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/trainer.py\n\nnice\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Fix reviews\n\n* Fix\n\n* Fix black\n\n---------\n\nCo-authored-by: Zach Mueller <muellerzr@gmail.com>\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-07T04:22:53Z",
        "message": "Fix err with FSDP (#25991)\n\n* Fix err\n\n* Use version check"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-05T17:01:20Z",
        "message": "deepspeed resume from ckpt fixes and adding support for deepspeed optimizer and HF scheduler (#25863)\n\n* Add support for deepspeed optimizer and HF scheduler\n\n* fix bug\n\n* fix the import\n\n* fix issue with deepspeed scheduler saving for hf optim + hf scheduler scenario\n\n* fix loading of hf scheduler when loading deepspeed checkpoint\n\n* fix import of `DeepSpeedSchedulerWrapper`\n\n* add tests\n\n* add the comment and skip the failing tests\n\n* address comment"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-01T15:24:12Z",
        "message": "Revert frozen training arguments (#25903)\n\n* Revert frozen training arguments\n\n* TODO"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-01T09:50:42Z",
        "message": "fix FSDP model resume optimizer & scheduler (#25852)\n\n* fix FSDP resume optimizer & scheduler\n\n* improve trainer code quality\n\n---------\n\nCo-authored-by: machi04 <machi04@meituan.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-31T09:47:53Z",
        "message": "fix ds z3 checkpointing when  `stage3_gather_16bit_weights_on_model_save=False` (#25817)\n\n* fix ds z3 checkpointing when  `stage3_gather_16bit_weights_on_model_save=False`\n\n* refactoring"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-29T14:06:41Z",
        "message": "Error with checking args.eval_accumulation_steps to gather tensors (#25819)\n\n* Update trainer.py (error with checking steps in args.eval_accumulation_steps to gather tensors)\n\nWhile the deprecated code has the correct check (line 3772): \n\"if args.eval_accumulation_steps is not None and (step + 1) % args.eval_accumulation_steps == 0:\"\n\nThe current code does not (line 3196):\n\"if args.eval_accumulation_steps is not None and self.accelerator.sync_gradients:\"\n\nWe need to check \"(step + 1) % args.eval_accumulation_steps == 0\". Hence, the line 3196 should be modified to:\n\"if args.eval_accumulation_steps is not None and (step + 1) % args.eval_accumulation_steps == 0 and self.accelerator.sync_gradients:\"\n\n* Fix error with checking args.eval_accumulation_steps to gather tensors"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-29T07:22:14Z",
        "message": "Arde/fsdp activation checkpointing (#25771)\n\n* add FSDP config option to enable activation-checkpointing\n\n* update docs\n\n* add checks and remove redundant code\n\n* fix formatting error"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-25T15:13:34Z",
        "message": "\ud83d\udea8\ud83d\udea8\ud83d\udea8 [`Refactor`] Move third-party related utility files into `integrations/` folder \ud83d\udea8\ud83d\udea8\ud83d\udea8 (#25599)\n\n* move deepspeed to `lib_integrations.deepspeed`\n\n* more refactor\n\n* oops\n\n* fix slow tests\n\n* Fix docs\n\n* fix docs\n\n* addess feedback\n\n* address feedback\n\n* final modifs for PEFT\n\n* fixup\n\n* ok now\n\n* trigger CI\n\n* trigger CI again\n\n* Update docs/source/en/main_classes/deepspeed.md\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* import from `integrations`\n\n* address feedback\n\n* revert removal of `deepspeed` module\n\n* revert removal of `deepspeed` module\n\n* fix conflicts\n\n* ooops\n\n* oops\n\n* add deprecation warning\n\n* place it on the top\n\n* put `FutureWarning`\n\n* fix conflicts with not_doctested.txt\n\n* add back `bitsandbytes` module with a depr warning\n\n* fix\n\n* fix\n\n* fixup\n\n* oops\n\n* fix doctests\n\n---------\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-18T17:08:03Z",
        "message": "[`PEFT`] Peft integration alternative design  (#25077)\n\n* a draft version\n\n* v2 integration\n\n* fix\n\n* make it more generic and works for IA3\n\n* add set adapter and multiple adapters support\n\n* fixup\n\n* adapt a bit\n\n* oops\n\n* oops\n\n* oops\n\n* adapt more\n\n* fix\n\n* add more refactor\n\n* now works with model class\n\n* change it to instance method as it causes issues with `jit`.\n\n* add CR\n\n* change method name\n\n* add `add_adapter` method\n\n* clean up\n\n* Update src/transformers/adapters/peft_mixin.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* add moe utils\n\n* fixup\n\n* Update src/transformers/adapters/peft_mixin.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* adapt\n\n* oops\n\n* fixup\n\n* add is_peft_available\n\n* remove `requires_backend`\n\n* trainer compatibility\n\n* fixup + docstring\n\n* more details\n\n* trigger CI\n\n* Apply suggestions from code review\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* Update src/transformers/modeling_utils.py\n\n* fixup + is_main_process\n\n* added `save_peft_format` in save_pretrained\n\n* up\n\n* fix nits here and there\n\n* nits here and there.\n\n* docs\n\n* revert `encoding=\"utf-8\"`\n\n* comment\n\n* added slow tests before the PEFT release.\n\n* fixup and nits\n\n* let's be on the safe zone\n\n* added more comments\n\n* v1 docs\n\n* add remaining docs\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* move to `lib_integrations`\n\n* fixup\n\n* this time fixup\n\n* Apply suggestions from code review\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* address final comments\n\n* refactor to use `token`\n\n* add PEFT to DockerFile for slow tests.\n\n* added pipeline support.\n\n---------\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-17T18:48:58Z",
        "message": "add warning for 8bit optimizers (#25575)\n\n* add warning for 8bit optimizers\n\n* protect import"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-17T16:23:34Z",
        "message": "add util for ram efficient loading of model when using fsdp (#25107)\n\n* add util for ram efficient loading of model when using fsdp\n\n* make fix-copies\n\n* fixes \ud83d\ude05\n\n* docs\n\n* making it further easier to use\n\n* rename the function\n\n* refactor to handle fsdp ram efficiency in `from_pretrained`\n\n* fixes\n\n* fixes\n\n* fixes\n\n* update\n\n* fixes\n\n* revert `load_pretrained_model_only_on_rank0`\n\n* resolve `load_from_checkpoint`"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-17T15:44:01Z",
        "message": "Revert \"change version (#25387)\" (#25573)\n\nThis reverts commit 3a05e010e0c7e8abd3e5357dd4e89e28cc69003e."
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-17T06:10:33Z",
        "message": "Update trainer.py (#25553)"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-16T16:19:51Z",
        "message": "More frozen args (#25540)"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-10T20:06:29Z",
        "message": "GPTQ integration (#25062)\n\n* GTPQ integration\n\n* Add tests for gptq\n\n* support for more quantization model\n\n* fix style\n\n* typo\n\n* fix method\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* add dataclass and fix quantization_method\n\n* fix doc\n\n* Update tests/quantization/gptq/test_gptq.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* modify dataclass\n\n* add gtpqconfig import\n\n* fix typo\n\n* fix tests\n\n* remove dataset as req arg\n\n* remove tokenizer import\n\n* add offload cpu quantization test\n\n* fix check dataset\n\n* modify dockerfile\n\n* protect trainer\n\n* style\n\n* test for config\n\n* add more log\n\n* overwrite torch_dtype\n\n* draft doc\n\n* modify quantization_config docstring\n\n* fix class name in docstring\n\n* Apply suggestions from code review\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* more warning\n\n* fix 8bit kwargs tests\n\n* peft compatibility\n\n* remove var\n\n* fix is_gptq_quantized\n\n* remove is_gptq_quantized\n\n* fix wrap\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* add exllama\n\n* skip test\n\n* overwrite float16\n\n* style\n\n* fix skip test\n\n* Apply suggestions from code review\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* fix docsting formatting\n\n* add doc\n\n* better test\n\n---------\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-10T15:07:32Z",
        "message": "Fix issue with ratio evaluation steps and auto find batch size (#25436)\n\n* Fully rebased solution\n\n* 500"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-09T07:31:24Z",
        "message": "rm useless condition since the previous condition contains it. (#25403)"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-08T17:05:41Z",
        "message": "change version (#25387)"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-07T15:47:22Z",
        "message": "Migrate Trainer from `Repository` to `upload_folder` (#25095)\n\n* First draft\n\n* Deal with progress bars\n\n* Update src/transformers/utils/hub.py\n\nCo-authored-by: Lucain <lucainp@gmail.com>\n\n* Address review comments\n\n* Forgot one\n\n* Pin hf_hub\n\n* Add argument for push all and fix tests\n\n* Fix tests\n\n* Address review comments\n\n---------\n\nCo-authored-by: Lucain <lucainp@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-02T07:29:00Z",
        "message": "Fix set of model parallel in the Trainer when no GPUs are available (#25239)"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-07-28T09:40:08Z",
        "message": "Fix `.push_to_hub` and cleanup `get_full_repo_name` usage (#25120)\n\n* Fix .push_to_hub and cleanup get_full_repo_name usage\n\n* Do not rely on Python bool conversion magic\n\n* request changes"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-07-27T12:34:02Z",
        "message": "fix delete all checkpoints when save_total_limit is set to 1 (#25136)"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-07-27T01:41:43Z",
        "message": "fix deepspeed load best model at end when the model gets sharded (#25057)"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-07-24T14:53:10Z",
        "message": "compute_loss in trainer failing to label shift for PEFT model when label smoothing enabled. (#25044)\n\n* added PeftModelForCausalLM to MODEL_FOR_CAUSAL_LM_MAPPING_NAMES dict\n\n* check for PEFT model in compute_loss section\n\n---------\n\nCo-authored-by: Nathan Brake <nbrake3@mmm.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-07-24T13:27:19Z",
        "message": "Add dispatch_batches to training arguments (#25038)\n\n* Dispatch batches\n\n* Copy items"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-03-01T01:12:51Z",
        "message": "Expose `offload_buffers` parameter of `accelerate` to `PreTrainedModel.from_pretrained` method (#28755)\n\nExpose offload_buffers parameter to from_pretrained method"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-02-27T09:43:01Z",
        "message": "Fix `attn_implementation` documentation (#29295)\n\nfix"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-02-20T01:23:25Z",
        "message": "[`gradient_checkpointing`] default to use it for torch 2.3 (#28538)\n\n* default to use it\n\n* style"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-02-16T07:16:58Z",
        "message": "Update all references to canonical models (#29001)\n\n* Script & Manual edition\n\n* Update"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-02-15T14:33:26Z",
        "message": "FIX: Fix error with `logger.warning` + inline with recent refactor (#29039)\n\nUpdate modeling_utils.py"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-02-14T00:30:23Z",
        "message": "ENH [`AutoQuantizer`]: enhance trainer + not supported quant methods (#28991)\n\n* enhance trainer + not support quant methods\n\n* remove all old logic\n\n* add version"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-02-12T15:47:08Z",
        "message": "Always initialize tied output_embeddings if it has a bias term (#28947)\n\nContinue to initialize tied output_embeddings if it has a bias term\n\nThe bias term is not tied, and so will need to be initialized accordingly."
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-02-06T16:18:30Z",
        "message": "Revert \"[WIP] Hard error when ignoring tensors.\" (#28898)\n\nRevert \"[WIP] Hard error when ignoring tensors. (#27484)\"\n\nThis reverts commit 2da28c4b41bba23969a8afe97c3dfdcbc47a57dc."
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-02-05T08:17:24Z",
        "message": "[WIP] Hard error when ignoring tensors. (#27484)\n\n* [WIP] Hard error when ignoring tensors.\n\n* Better selection/error when saving a checkpoint.\n\n- Find all names we should normally drop (those are in the transformers\n  config)\n- Find all disjoint tensors (for those we can safely trigger a copy to\n  get rid of the sharing before saving)\n- Clone those disjoint tensors getting rid of the issue\n- Find all identical names (those should be declared in the config\n  but we try to find them all anyway.)\n- For all identical names:\n  - If they are in the config, just ignore them everything is fine\n  - If they are not, warn about them.\n- For all remainder tensors which are shared yet neither identical NOR\n  disjoint. raise a hard error.\n\n* Adding a failing test on `main` that passes here.\n\n* We don't need to keep the subfolder logic in this test.\n\n* Apply suggestions from code review\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-02-02T08:34:12Z",
        "message": "Add missing None check for hf_quantizer (#28804)\n\n* Add missing None check for hf_quantizer\n\n* Add test, fix logic.\n\n* make style\n\n* Switch test model to Mistral\n\n* Comment\n\n* Update tests/test_modeling_utils.py\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-02-02T07:45:00Z",
        "message": "[Docs] Fix spelling and grammar mistakes (#28825)\n\n* Fix typos and grammar mistakes in docs and examples\n\n* Fix typos in docstrings and comments\n\n* Fix spelling of `tokenizer` in model tests\n\n* Remove erroneous spaces in decorators\n\n* Remove extra spaces in Markdown link texts"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-01-31T01:19:18Z",
        "message": "don't initialize the output embeddings if we're going to tie them to input embeddings (#28192)\n\n* test that tied output embeddings aren't initialized on load\n\n* don't initialize the output embeddings if we're going to tie them to the input embeddings"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-01-30T01:48:25Z",
        "message": "`HfQuantizer` class for quantization-related stuff in `modeling_utils.py` (#26610)\n\n* squashed earlier commits for easier rebase\n\n* rm rebase leftovers\n\n* 4bit save enabled @quantizers\n\n* TMP gptq test use exllama\n\n* fix AwqConfigTest::test_wrong_backend for A100\n\n* quantizers AWQ fixes\n\n* _load_pretrained_model low_cpu_mem_usage branch\n\n* quantizers style\n\n* remove require_low_cpu_mem_usage attr\n\n* rm dtype arg from process_model_before_weight_loading\n\n* rm config_origin from Q-config\n\n* rm inspect from q_config\n\n* fixed docstrings in QuantizationConfigParser\n\n* logger.warning fix\n\n* mv is_loaded_in_4(8)bit to BnbHFQuantizer\n\n* is_accelerate_available error msg fix in quantizer\n\n* split is_model_trainable in bnb quantizer class\n\n* rm llm_int8_skip_modules as separate var in Q\n\n* Q rm todo\n\n* fwd ref to HFQuantizer in type hint\n\n* rm note re optimum.gptq.GPTQQuantizer\n\n* quantization_config in __init__ simplified\n\n* replaced NonImplemented with  create_quantized_param\n\n* rm load_in_4/8_bit deprecation warning\n\n* QuantizationConfigParser refactoring\n\n* awq-related minor changes\n\n* awq-related changes\n\n* awq config.modules_to_not_convert\n\n* raise error if no q-method in q-config in args\n\n* minor cleanup\n\n* awq quantizer docstring\n\n* combine common parts in bnb process_model_before_weight_loading\n\n* revert test_gptq\n\n* .process_model_ cleanup\n\n* restore dict config warning\n\n* removed typevars in quantizers.py\n\n* cleanup post-rebase 16 jan\n\n* QuantizationConfigParser classmethod refactor\n\n* rework of handling of unexpected aux elements of bnb weights\n\n* moved q-related stuff from save_pretrained to quantizers\n\n* refactor v1\n\n* more changes\n\n* fix some tests\n\n* remove it from main init\n\n* ooops\n\n* Apply suggestions from code review\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* fix awq issues\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* add docs\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update docs/source/en/hf_quantizer.md\n\n* address comments\n\n* fix\n\n* fixup\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* address final comment\n\n* update\n\n* Update src/transformers/quantizers/base.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update src/transformers/quantizers/auto.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* fix\n\n* add kwargs update\n\n* fixup\n\n* add `optimum_quantizer` attribute\n\n* oops\n\n* rm unneeded file\n\n* fix doctests\n\n---------\n\nCo-authored-by: younesbelkada <younesbelkada@gmail.com>\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-01-26T16:25:08Z",
        "message": "fix: suppress `GatedRepoError` to use cache file (fix #28558). (#28566)\n\n* fix: suppress `GatedRepoError` to use cache file (fix #28558).\n\n* move condition_to_return parameter back to outside."
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-01-26T12:00:49Z",
        "message": "Fix `weights_only` (#28725)\n\nfix\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-01-26T11:52:53Z",
        "message": "fix: corrected misleading log message in save_pretrained function (#28699)"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-01-26T08:37:04Z",
        "message": "Fix duplicate & unnecessary flash attention warnings (#28557)\n\n* fix duplicate & unnecessary flash warnings\n\n* trigger ci\n\n* warning_once\n\n* if/else order\n\n---------\n\nCo-authored-by: Your Name <you@example.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-01-18T10:55:29Z",
        "message": "Use `weights_only` only if torch >= 1.13 (#28506)\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-01-16T18:31:01Z",
        "message": "Config: warning when saving generation kwargs in the model config (#28514)"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-01-16T16:10:44Z",
        "message": "Clearer error for SDPA when explicitely requested (#28006)\n\n* clearer error for sdpa\n\n* better message"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-01-16T13:29:51Z",
        "message": "Fix mismatching loading in from_pretrained with/without accelerate (#28414)\n\n* fix mismatching behavior in from_pretrained with/without accelerate\n\n* meaningful refactor\n\n* remove added space\n\n* add test\n\n* fix model on the hub\n\n* comment\n\n* use tiny model\n\n* style"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-01-15T13:48:07Z",
        "message": "[`core`/ FEAT] Add the possibility to push custom tags using `PreTrainedModel` itself (#28405)\n\n* v1 tags\n\n* remove unneeded conversion\n\n* v2\n\n* rm unneeded warning\n\n* add more utility methods\n\n* Update src/transformers/utils/hub.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/utils/hub.py\n\nCo-authored-by: Lucain <lucainp@gmail.com>\n\n* Update src/transformers/utils/hub.py\n\nCo-authored-by: Lucain <lucainp@gmail.com>\n\n* more enhancements\n\n* oops\n\n* merge tags\n\n* clean up\n\n* revert unneeded change\n\n* add extensive docs\n\n* more docs\n\n* more kwargs\n\n* add test\n\n* oops\n\n* fix test\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Omar Sanseviero <osanseviero@gmail.com>\n\n* Update src/transformers/utils/hub.py\n\nCo-authored-by: Lucain <lucainp@gmail.com>\n\n* Update src/transformers/modeling_utils.py\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* add more conditions\n\n* more logic\n\n---------\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\nCo-authored-by: Lucain <lucainp@gmail.com>\nCo-authored-by: Omar Sanseviero <osanseviero@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-01-12T13:29:35Z",
        "message": "[`Mixtral` / `Awq`] Add mixtral fused modules for Awq  (#28240)\n\n* add mixtral fused modules\n\n* add changes from modeling utils\n\n* add test\n\n* fix test + rope theta issue\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* add tests\n\n---------\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-01-12T05:55:54Z",
        "message": "[`Awq`] Add llava fused modules support (#28239)\n\n* add llava + fused modules\n\n* Update src/transformers/models/llava/modeling_llava.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-01-11T15:18:27Z",
        "message": "Byebye torch 1.10 (#28207)\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-01-10T08:57:30Z",
        "message": "Use mmap option to load_state_dict (#28331)\n\nUse mmap option to load_state_dict (#28331)"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2024-01-09T14:58:21Z",
        "message": "Fix initialization for missing parameters in `from_pretrained` under ZeRO-3 (#28245)\n\n* Fix initialization for missing parameters in `from_pretrained` under ZeRO-3\n\n* Test initialization for missing parameters under ZeRO-3\n\n* Add more tests\n\n* Only enable deepspeed context for per-module level parameters\n\n* Enable deepspeed context only once\n\n* Move class definition inside test case body"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-12-26T20:52:10Z",
        "message": "small typo (#28229)\n\nUpdate modeling_utils.py"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-12-25T10:06:56Z",
        "message": "[`Awq`] Enable the possibility to skip quantization for some target modules (#27950)\n\n* v1\n\n* add docstring\n\n* add tests\n\n* add awq 0.1.8\n\n* oops\n\n* fix test"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-12-22T15:05:10Z",
        "message": "update the logger message with accordant weights_file_name (#28181)\n\nCo-authored-by: yudong.lin <yudong.lin@funplus.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-12-21T10:54:44Z",
        "message": "[bnb] Let's make serialization of 4bit models possible  (#26037)\n\n* updated bitsandbytes.py\n\n* rm test_raise_* from test_4bit.py\n\n* add test_4bit_serialization.py\n\n* modeling_utils bulk edits\n\n* bnb_ver 0.41.3 in integrations/bitsandbytes.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* @slow reinstated\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* bnb ver 0.41.3 in  src/transformers/modeling_utils.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* rm bnb version todo in  integrations/bitsandbytes.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* moved 4b serialization tests to test_4bit\n\n* tests upd for opt\n\n* to torch_device\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* ruff fixes to tests\n\n* rm redundant bnb version check in mod_utils\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* restore _hf_peft_config_loaded  modeling_utils.py::2188\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* restore _hf_peft_config_loaded  test in modeling_utils.py::2199\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* fixed NOT getattr(self, \"is_8bit_serializable\")\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* setting model.is_4bit_serializable\n\n* rm separate fp16_statistics arg from set_module...\n\n* rm else branch in integrations::bnb::set_module\n\n* bnb 4bit dtype check\n\n* upd comment on 4bit weights\n\n* upd tests for FP4 safe\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-12-20T16:52:16Z",
        "message": "Update FA2 exception msg to point to hub discussions (#28161)\n\n* Update FA2 exception msg to point to hub discussions\n\n* Use path for hub url"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-12-20T13:20:02Z",
        "message": "Fix weights not properly initialized due to shape mismatch (#28122)\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-12-20T08:55:07Z",
        "message": "Fix FA2 integration (#28142)\n\n* fix fa2\n\n* fix FA2 for popular models\n\n* improve warning and add Younes as co-author\n\nCo-Authored-By: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* fix the warning\n\n* Add Tip\n\n* typo fix\n\n* nit\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-12-19T17:07:57Z",
        "message": "Update modeling_utils.py (#28127)\n\nIn docstring for PreTrainedModel.resize_token_embeddings, correct definition of new_num_tokens parameter to read \"the new number of tokens\" (meaning the new size of the vocab) rather than \"the number of new tokens\" (number of newly added tokens only)."
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-12-15T17:03:41Z",
        "message": "Fix `low_cpu_mem_usage` Flag Conflict with DeepSpeed Zero 3 in `from_pretrained` for Models with `keep_in_fp32_modules`\" (#27762)\n\nFix `from_pretrained` Logic\nfor `low_cpu_mem_usage` with DeepSpeed Zero3"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-12-15T15:01:18Z",
        "message": "make torch.load a bit safer (#27282)\n\n* make torch.load a bit safer\n\n* Fixes\n\n---------\n\nCo-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-12-15T10:08:27Z",
        "message": "[`FA-2`] Fix fa-2 issue when passing `config` to `from_pretrained` (#28043)\n\n* fix fa-2 issue\n\n* fix test\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: fxmarty <9808326+fxmarty@users.noreply.github.com>\n\n* clenaer fix\n\n* up\n\n* add more robust tests\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: fxmarty <9808326+fxmarty@users.noreply.github.com>\n\n* fixup\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* pop\n\n* add test\n\n---------\n\nCo-authored-by: fxmarty <9808326+fxmarty@users.noreply.github.com>\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-12-11T11:38:17Z",
        "message": "[`from_pretrained`] Make from_pretrained fast again (#27709)\n\n* Skip nn.Module.reset_parameters\n\n* Actually skip\n\n* Check quality\n\n* Maybe change all inits\n\n* Fix init issues: only modify public functions\n\n* Add a small test for now\n\n* Style\n\n* test updates\n\n* style\n\n* nice tes\n\n* style\n\n* make it even faster\n\n* one more second\n\n* remove fx icompatible\n\n* Update tests/test_modeling_common.py\n\nCo-authored-by: Lysandre Debut <hi@lysand.re>\n\n* Update tests/test_modeling_common.py\n\nCo-authored-by: Lysandre Debut <hi@lysand.re>\n\n* skip\n\n* fix quality\n\n* protect the import\n\n---------\n\nCo-authored-by: Lysandre Debut <hi@lysand.re>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-12-11T09:56:38Z",
        "message": "Fix SDPA dispatch & make SDPA CI compatible with torch<2.1.1 (#27940)\n\nfix sdpa dispatch"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-12-08T20:38:14Z",
        "message": "F.scaled_dot_product_attention support (#26572)\n\n* add sdpa\n\n* wip\n\n* cleaning\n\n* add ref\n\n* yet more cleaning\n\n* and more :)\n\n* wip llama\n\n* working llama\n\n* add output_attentions=True support\n\n* bigcode sdpa support\n\n* fixes\n\n* gpt-bigcode support, require torch>=2.1.1\n\n* add falcon support\n\n* fix conflicts falcon\n\n* style\n\n* fix attention_mask definition\n\n* remove output_attentions from attnmaskconverter\n\n* support whisper without removing any Copied from statement\n\n* fix mbart default to eager renaming\n\n* fix typo in falcon\n\n* fix is_causal in SDPA\n\n* check is_flash_attn_2_available in the models init as well in case the model is not initialized through from_pretrained\n\n* add warnings when falling back on the manual implementation\n\n* precise doc\n\n* wip replace _flash_attn_enabled by config.attn_implementation\n\n* fix typo\n\n* add tests\n\n* style\n\n* add a copy.deepcopy on the config in from_pretrained, as we do not want to modify it inplace\n\n* obey to config.attn_implementation if a config is passed in from_pretrained\n\n* fix is_torch_sdpa_available when torch is not installed\n\n* remove dead code\n\n* Update src/transformers/modeling_attn_mask_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update src/transformers/modeling_attn_mask_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update src/transformers/modeling_attn_mask_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update src/transformers/modeling_attn_mask_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update src/transformers/modeling_attn_mask_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update src/transformers/models/bart/modeling_bart.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* remove duplicate pretraining_tp code\n\n* add dropout in llama\n\n* precise comment on attn_mask\n\n* add fmt: off for _unmask_unattended docstring\n\n* precise num_masks comment\n\n* nuke pretraining_tp in LlamaSDPAAttention following Arthur's suggestion\n\n* cleanup modeling_utils\n\n* backward compatibility\n\n* fix style as requested\n\n* style\n\n* improve documentation\n\n* test pass\n\n* style\n\n* add _unmask_unattended tests\n\n* skip meaningless tests for idefics\n\n* hard_check SDPA requirements when specifically requested\n\n* standardize the use if XXX_ATTENTION_CLASSES\n\n* fix SDPA bug with mem-efficient backend on CUDA when using fp32\n\n* fix test\n\n* rely on SDPA is_causal parameter to handle the causal mask in some cases\n\n* fix FALCON_ATTENTION_CLASSES\n\n* remove _flash_attn_2_enabled occurences\n\n* fix test\n\n* add OPT to the list of supported flash models\n\n* improve test\n\n* properly test on different SDPA backends, on different dtypes & properly handle separately the pad tokens in the test\n\n* remove remaining _flash_attn_2_enabled occurence\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update src/transformers/modeling_attn_mask_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update docs/source/en/perf_infer_gpu_one.md\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* remove use_attn_implementation\n\n* fix docstring & slight bug\n\n* make attn_implementation internal (_attn_implementation)\n\n* typos\n\n* fix tests\n\n* deprecate use_flash_attention_2=True\n\n* fix test\n\n* add back llama that was removed by mistake\n\n* fix tests\n\n* remove _flash_attn_2_enabled occurences bis\n\n* add check & test that passed attn_implementation is valid\n\n* fix falcon torchscript export\n\n* fix device of mask in tests\n\n* add tip about torch.jit.trace and move bt doc below sdpa\n\n* fix parameterized.expand order\n\n* move tests from test_modeling_attn_mask_utils to test_modeling_utils as a relevant test class is already there\n\n* update sdpaattention class with the new cache\n\n* Update src/transformers/configuration_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update src/transformers/models/bark/modeling_bark.py\n\n* address review comments\n\n* WIP torch.jit.trace fix. left: test both eager & sdpa\n\n* add test for torch.jit.trace for both eager/sdpa\n\n* fix falcon with torch==2.0 that needs to use sdpa\n\n* fix doc\n\n* hopefully last fix\n\n* fix key_value_length that has no default now in mask converter\n\n* is it flacky?\n\n* fix speculative decoding bug\n\n* tests do pass\n\n* fix following #27907\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-12-08T08:00:17Z",
        "message": "Generate: New `Cache` abstraction and Attention Sinks support (#26681)\n\n* Draft version of new KV Caching\n\nThis should allow Attention Sinks (https://github.com/tomaarsen/attention_sinks)\n/ StreamingLLM (https://arxiv.org/abs/2309.17453) to be easily implemented\nin a third-party or in transformers directly\n\n* Address numerous PR suggestions\n\n1. Move layer_idx from cache to ...Attention. Removes confusing set_layer_idx magic.\n2. Always convert past_key_values to Cache instance at the start of ...Attention, removes all other isinstance calls.\n3. Remove __bool__ and __getitem__ magic as they're confusing.\n4. past_key_values.update(key, value, idx) now returns key, value.\n5. Add use_legacy_cache flag, defaults to None, i.e. Falsey. This breaks generate for now, until 1) the cache is used is generate() or 2) use_legacy_cache is defaulted to True in generate() until we change it in another PR.\n6. Separate key_cache and value_cache.\n\nSome work is still needed to see if the SinkCache can conveniently be implemented with just one update method.\n\n* Implement the SinkCache through backward+forward rotations\n\n* Integrate (Sink)Cache with Llama FA2\n\n* Set use_legacy_cache=True as default, allows for test passes\n\n* Move from/to_legacy_cache to ...Model class\n\n* Undo unnecessary newline change\n\n* Remove copy utility from deprecated OpenLlama\n\n* Match import style\n\n* manual rebase with main\n\n* Cache class working with generate (#1)\n\n* Draft version of new KV Caching\n\nThis should allow Attention Sinks (https://github.com/tomaarsen/attention_sinks)\n/ StreamingLLM (https://arxiv.org/abs/2309.17453) to be easily implemented\nin a third-party or in transformers directly\n\n* Address numerous PR suggestions\n\n1. Move layer_idx from cache to ...Attention. Removes confusing set_layer_idx magic.\n2. Always convert past_key_values to Cache instance at the start of ...Attention, removes all other isinstance calls.\n3. Remove __bool__ and __getitem__ magic as they're confusing.\n4. past_key_values.update(key, value, idx) now returns key, value.\n5. Add use_legacy_cache flag, defaults to None, i.e. Falsey. This breaks generate for now, until 1) the cache is used is generate() or 2) use_legacy_cache is defaulted to True in generate() until we change it in another PR.\n6. Separate key_cache and value_cache.\n\nSome work is still needed to see if the SinkCache can conveniently be implemented with just one update method.\n\n* Integrate (Sink)Cache with Llama FA2\n\n* Move from/to_legacy_cache to ...Model class\n\n* Undo unnecessary newline change\n\n* Match import style\n\n* working generate\n\n* Add tests; Simplify code; Apply changes to Mistral and Persimmon\n\n* fix rebase mess\n\n* a few more manual fixes\n\n* last manual fix\n\n* propagate changes to phi\n\n* upgrade test\n\n* add use_legacy_cache docstring; beef up tests\n\n* reintroduce unwanted deletes\n\n---------\n\nCo-authored-by: Tom Aarsen <Cubiegamedev@gmail.com>\n\n* move import\n\n* add default to model_kwargs.get('use_legacy_cache')\n\n* correct failing test\n\n* Apply suggestions from code review\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* apply PR suggestions\n\n* fix failing test\n\n* Apply suggestions from code review\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: Tom Aarsen <37621491+tomaarsen@users.noreply.github.com>\n\n* PR comments\n\n* tmp commit\n\n* add docstrings\n\n* more tests, more docstrings, add to docs\n\n* derp\n\n* tmp commit\n\n* tmp dbg\n\n* more dbg\n\n* fix beam search bug\n\n* cache can be a list of tuples in some models\n\n* fix group beam search\n\n* all but sinkcache integration tests\n\n* fix sink cache and add hard integration test\n\n* now also compatible with input_embeds input\n\n* PR comments\n\n* add Cache support to Phi+FA2\n\n* make fixup\n\n---------\n\nCo-authored-by: Joao Gante <joao@huggingface.co>\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-12-06T16:19:44Z",
        "message": "Avoid class attribute `_keep_in_fp32_modules` being modified (#27867)\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-12-05T11:14:45Z",
        "message": "Faster generation using AWQ + Fused modules (#27411)\n\n* v1 fusing modules\n\n* add fused mlp support\n\n* up\n\n* fix CI\n\n* block save_pretrained\n\n* fixup\n\n* small fix\n\n* add new condition\n\n* add v1 docs\n\n* add some comments\n\n* style\n\n* fix nit\n\n* adapt from suggestion\n\n* add check\n\n* change arg names\n\n* change variables name\n\n* Update src/transformers/integrations/awq.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* style\n\n* split up into 3 different private methods\n\n* more conditions\n\n* more checks\n\n* add fused tests for custom models\n\n* fix\n\n* fix tests\n\n* final update docs\n\n* final fixes\n\n* fix importlib metadata\n\n* Update src/transformers/utils/quantization_config.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* change it to `do_fuse`\n\n* nit\n\n* Update src/transformers/utils/quantization_config.py\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* Update src/transformers/utils/quantization_config.py\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* Update src/transformers/utils/quantization_config.py\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* few fixes\n\n* revert\n\n* fix test\n\n* fix copies\n\n* raise error if model is not quantized\n\n* add test\n\n* use quantization_config.config when fusing\n\n* Update src/transformers/modeling_utils.py\n\n---------\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-12-04T12:52:17Z",
        "message": "Flash Attention 2 support for RoCm (#27611)\n\n* support FA2\n\n* fix typo\n\n* fix broken tests\n\n* fix more test errors\n\n* left/right\n\n* fix bug\n\n* more test\n\n* typo\n\n* fix layout flash attention falcon\n\n* do not support this case\n\n* use allclose instead of equal\n\n* fix various bugs with flash attention\n\n* bump\n\n* fix test\n\n* fix mistral\n\n* use skiptest instead of return that may be misleading\n\n* add fix causal arg flash attention\n\n* fix copies\n\n* more explicit comment\n\n* still use self.is_causal\n\n* fix causal argument\n\n* comment\n\n* fixes\n\n* update documentation\n\n* add link\n\n* wrong test\n\n* simplify FA2 RoCm requirements\n\n* update opt\n\n* make flash_attn_uses_top_left_mask attribute private and precise comment\n\n* better error handling\n\n* fix copy & mistral\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/utils/import_utils.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* use is_flash_attn_greater_or_equal_2_10 instead of is_flash_attn_greater_or_equal_210\n\n* fix merge\n\n* simplify\n\n* inline args\n\n---------\n\nCo-authored-by: Felix Marty <felix@hf.co>\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-12-01T16:59:14Z",
        "message": "Better error message for bitsandbytes import  (#27764)\n\n* better error message\n\n* fix logic\n\n* fix log"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-12-01T14:51:10Z",
        "message": "Make using safetensors files automated. (#27571)\n\n* [WIP] Make using safetensors files automated.\n\nIf `use_safetensors=True` is used, and it doesn't exist:\n\n- Don't crash just yet\n- Lookup for an open PR containing it.\n- If yes, use that instead\n- If not, touch the space to convert, wait for conversion to be finished\n  and the PR to be opened\n- Use that new PR\n- Profit.\n\n* Remove the token.\n\n* [Auto Safetensors] Websocket -> SSE (#27656)\n\n* Websocket -> SSE\n\n* Support sharded + tests +cleanup\n\na\n\n* env var\n\n* Apply suggestions from code review\n\n* Thanks Simon\n\n* Thanks Wauplin\n\nCo-authored-by: Wauplin <lucainp@gmail.com>\n\n* Cleanup\n\n* Update tests\n\n* Tests should pass\n\n* Apply to other tests\n\n* Extend extension\n\n* relax requirement on latest hfh\n\n* Revert\n\n* Correct private handling & debug statements\n\n* Skip gated repos as of now\n\n* Address review comments\n\nCo-authored-by: ArthurZucker <arthur.zucker@gmail.com>\n\n---------\n\nCo-authored-by: Lysandre Debut <hi@lysand.re>\nCo-authored-by: Lysandre <lysandre@huggingface.co>\nCo-authored-by: Wauplin <lucainp@gmail.com>\nCo-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>\nCo-authored-by: ArthurZucker <arthur.zucker@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-11-24T06:10:52Z",
        "message": "Refactoring Trainer, adds `save_only_model` arg and simplifying FSDP integration (#27652)\n\n* add code changes\n\n1. Refactor FSDP\n2. Add `--save_only_model` option: When checkpointing, whether to only save the model, or also the optimizer, scheduler & rng state.\n3. Bump up the minimum `accelerate` version to `0.21.0`\n\n* quality\n\n* fix quality?\n\n* Revert \"fix quality?\"\n\nThis reverts commit 149330a6abc078827be274db84c8a2d26a76eba1.\n\n* fix fsdp doc strings\n\n* fix quality\n\n* Update src/transformers/training_args.py\n\nCo-authored-by: Zach Mueller <muellerzr@gmail.com>\n\n* please fix the quality issue \ud83d\ude05\n\n* Apply suggestions from code review\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* address comment\n\n* simplify conditional check as per the comment\n\n* update documentation\n\n---------\n\nCo-authored-by: Zach Mueller <muellerzr@gmail.com>\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-11-21T17:51:48Z",
        "message": "Fix `resize_token_embeddings` (#26861) (#26865)\n\n* Fix `resize_token_embeddings` about `requires_grad`\n\nThe method `resize_token_embeddings` should keep `requires_grad`\nunchanged for all parameters in embeddings.\n\nPreviously, `resize_token_embeddings` always set `requires_grad`\nto `True`. After fixed, `resize_token_embeddings` copy the\n`requires_grad` attribute in the old embeddings."
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-11-21T10:03:30Z",
        "message": "[`core` / `gradient_checkpointing`] add support for old GC method (#27610)\n\n* add support for old GC method\n\n* add also disable\n\n* up\n\n* oops"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-11-20T15:45:55Z",
        "message": "[`FA-2`] Add fa2 support for `from_config` (#26914)\n\n* add fa2 support for from_config\n\n* Update test_modeling_common.py"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-11-16T16:43:19Z",
        "message": "[`Styling`] stylify using ruff (#27144)\n\n* try to stylify using ruff\n\n* might need to remove these changes?\n\n* use ruf format andruff check\n\n* use isinstance instead of type comparision\n\n* use # fmt: skip\n\n* use # fmt: skip\n\n* nits\n\n* soem styling changes\n\n* update ci job\n\n* nits isinstance\n\n* more files update\n\n* nits\n\n* more nits\n\n* small nits\n\n* check and format\n\n* revert wrong changes\n\n* actually use formatter instead of checker\n\n* nits\n\n* well docbuilder is overwriting this commit\n\n* revert notebook changes\n\n* try to nuke docbuilder\n\n* style\n\n* fix feature exrtaction test\n\n* remve `indent-width = 4`\n\n* fixup\n\n* more nits\n\n* update the ruff version that we use\n\n* style\n\n* nuke docbuilder styling\n\n* leve the print for detected changes\n\n* nits\n\n* Remove file I/O\n\nCo-authored-by: charliermarsh\n <charlie.r.marsh@gmail.com>\n\n* style\n\n* nits\n\n* revert notebook changes\n\n* Add # fmt skip when possible\n\n* Add # fmt skip when possible\n\n* Fix\n\n* More `  # fmt: skip` usage\n\n* More `  # fmt: skip` usage\n\n* More `  # fmt: skip` usage\n\n* NIts\n\n* more fixes\n\n* fix tapas\n\n* Another way to skip\n\n* Recommended way\n\n* Fix two more fiels\n\n* Remove asynch\nRemove asynch\n\n---------\n\nCo-authored-by: charliermarsh <charlie.r.marsh@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-11-16T15:35:40Z",
        "message": "Raise error when quantizing a quantized model (#27500)\n\nadd error msg"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-11-15T19:58:08Z",
        "message": "Fix offload disk for loading derivated model checkpoint into base model (#27253)\n\n* fix\n\n* style\n\n* add test"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-11-02T11:03:51Z",
        "message": "[`core` / `Quantization`] Fix for 8bit serialization tests (#27234)\n\n* fix for 8bit serialization\n\n* added regression tests.\n\n* fixup"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-11-01T18:25:23Z",
        "message": "Fix CPU offload + disk offload tests (#27204)\n\nFix disk offload tests + weight sharing issues"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-11-01T17:09:21Z",
        "message": "Add exllamav2 better (#27111)\n\n* add_ xllamav2 arg\n\n* add test\n\n* style\n\n* add check\n\n* add doc\n\n* replace by use_exllama_v2\n\n* fix tests\n\n* fix doc\n\n* style\n\n* better condition\n\n* fix logic\n\n* add deprecate msg\n\n* deprecate exllama\n\n* remove disable_exllama from the linter\n\n* remove\n\n* fix warning\n\n* Revert the commits deprecating exllama\n\n* deprecate disable_exllama for use_exllama\n\n* fix\n\n* fix loading attribute\n\n* better handling of args\n\n* remove disable_exllama from init and linter\n\n* Apply suggestions from code review\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* better arg\n\n* fix warning\n\n* Apply suggestions from code review\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* switch to dict\n\n* Apply suggestions from code review\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* style\n\n* nits\n\n* style\n\n* better tests\n\n* style\n\n---------\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-11-01T08:06:31Z",
        "message": "[`core` / `Quantization` ] AWQ integration (#27045)\n\n* working v1\n\n* oops\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* fixup\n\n* oops\n\n* push\n\n* more changes\n\n* add docs\n\n* some fixes\n\n* fix copies\n\n* add v1 doc\n\n* added installation guide\n\n* relax constraints\n\n* revert\n\n* attempt llm-awq\n\n* oops\n\n* oops\n\n* fixup\n\n* raise error when incorrect cuda compute capability\n\n* nit\n\n* add instructions for llm-awq\n\n* fixup\n\n* fix copies\n\n* fixup and docs\n\n* change\n\n* few changes + add demo\n\n* add v1 tests\n\n* add autoawq in dockerfile\n\n* finalize\n\n* Update tests/quantization/autoawq/test_awq.py\n\n* fix test\n\n* fix\n\n* fix issue\n\n* Update src/transformers/integrations/awq.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update docs/source/en/main_classes/quantization.md\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update docs/source/en/main_classes/quantization.md\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update src/transformers/integrations/awq.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update src/transformers/integrations/awq.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* add link to example script\n\n* Update docs/source/en/main_classes/quantization.md\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* add more content\n\n* add more details\n\n* add link to quantization docs\n\n* camel case + change backend class name\n\n* change to string\n\n* fixup\n\n* raise errors if libs not installed\n\n* change to `bits` and `group_size`\n\n* nit\n\n* nit\n\n* Apply suggestions from code review\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* disable training\n\n* address some comments and fix nits\n\n* fix\n\n* final nits and fix tests\n\n* adapt to our new runners\n\n* make fix-copies\n\n* Update src/transformers/utils/quantization_config.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/utils/quantization_config.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/integrations/awq.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/integrations/awq.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* move to top\n\n* add conversion test\n\n* final nit\n\n* add more elaborated test\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-31T18:16:49Z",
        "message": "Safetensors serialization by default (#27064)\n\n* Safetensors serialization by default\n\n* First pass on the tests\n\n* Second pass on the tests\n\n* Third pass on the tests\n\n* Fix TF weight loading from TF-format safetensors\n\n* Specific encoder-decoder fixes for weight crossloading\n\n* Add VisionEncoderDecoder fixes for TF too\n\n* Change filename test for pt-to-tf\n\n* One missing fix for TFVisionEncoderDecoder\n\n* Fix the other crossload test\n\n* Support for flax + updated tests\n\n* Apply suggestions from code review\n\nCo-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>\n\n* Sanchit's comments\n\n* Sanchit's comments 2\n\n* Nico's comments\n\n* Fix tests\n\n* cleanup\n\n* Apply suggestions from code review\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Matt <rocketknight1@gmail.com>\nCo-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-31T13:45:23Z",
        "message": "Add support for loading GPTQ models on CPU (#26719)\n\n* Add support for loading GPTQ models on CPU\n\nRight now, we can only load the GPTQ Quantized model on the CUDA\ndevice. The attribute `gptq_supports_cpu` checks if the current\nauto_gptq version is the one which has the cpu support for the\nmodel or not.\nThe larger variants of the model are hard to load/run/trace on\nthe GPU and that's the rationale behind adding this attribute.\n\nSigned-Off By: Vivek Khandelwal <vivek@nod-labs.com>\n\n* Update quantization.md\n\n* Update quantization.md\n\n* Update quantization.md"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-30T20:08:29Z",
        "message": "Fix import of torch.utils.checkpoint (#27155)\n\n* Fix import\n\n* Apply suggestions from code review\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-27T14:15:22Z",
        "message": "[`core`/ `gradient_checkpointing`] Refactor GC - part 2 (#27073)\n\n* fix\n\n* more fixes\n\n* fix other models\n\n* fix long t5\n\n* use `gradient_checkpointing_func` instead\n\n* fix copies\n\n* set `gradient_checkpointing_func` as a private attribute and retrieve previous behaviour\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* replace it with `is_gradient_checkpointing_set`\n\n* remove default\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* fixup\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-27T13:49:20Z",
        "message": "Fix no split modules underlying modules (#27090)\n\n* fix no split\n\n* style\n\n* remove comm\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* rename modules\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-27T12:32:54Z",
        "message": "Provide alternative when warning on use_auth_token (#27105)"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-27T09:23:06Z",
        "message": "Revert \"add exllamav2 arg\" (#27102)\n\nRevert \"add exllamav2 arg (#26437)\"\n\nThis reverts commit 8214d6e7b1d6ac25859ad745ccebdf73434e166d."
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-26T14:15:05Z",
        "message": "add exllamav2 arg (#26437)\n\n* add_ xllamav2 arg\n\n* add test\n\n* style\n\n* add check\n\n* add doc\n\n* replace by use_exllama_v2\n\n* fix tests\n\n* fix doc\n\n* style\n\n* better condition\n\n* fix logic\n\n* add deprecate msg"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-26T09:21:04Z",
        "message": "Bump`flash_attn` version to `2.1` (#27079)\n\n* pin FA-2 to `2.1`\n\n* fix on modeling"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-25T10:16:15Z",
        "message": "[`core`] Refactor of `gradient_checkpointing` (#27020)\n\n* v1\n\n* fix\n\n* remove `create_custom_forward`\n\n* fixup\n\n* fixup\n\n* add test and fix all failing GC tests\n\n* remove all remaining `create_custom_forward` methods\n\n* fix idefics bug\n\n* fixup\n\n* replace with `__call__`\n\n* add comment\n\n* quality"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-24T17:05:37Z",
        "message": "Fix config silent copy in from_pretrained (#27043)\n\n* Fix config modeling utils\n\n* fix more\n\n* fix attn mask bug\n\n* Update src/transformers/modeling_utils.py"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-24T13:10:23Z",
        "message": "Add fuyu device map (#26949)\n\n* add _no_split_modules\n\n* style\n\n* fix _no_split_modules\n\n* add doc"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-23T12:25:48Z",
        "message": "Change default `max_shard_size` to smaller value (#26942)\n\n* Update modeling_utils.py\n\n* fixup\n\n* let's change it to 5GB\n\n* fix"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-16T17:56:53Z",
        "message": "\ud83d\udea8\ud83d\udea8\ud83d\udea8 [`Quantization`] Store the original dtype in the config as a private attribute \ud83d\udea8\ud83d\udea8\ud83d\udea8 (#26761)\n\n* First step\n\n* fix\n\n* add adjustements for gptq\n\n* change to `_pre_quantization_dtype`\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* fix serialization\n\n* Apply suggestions from code review\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* fixup\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-16T13:29:01Z",
        "message": "Make fsdp ram efficient loading optional (#26631)\n\nmake fsdp ram efficient loading optional"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-13T10:56:50Z",
        "message": "[`core`] Fix fa-2 import (#26785)\n\n* fix fa-2 import\n\n* nit"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-12T08:28:40Z",
        "message": "Add many missing spaces in adjacent strings (#26751)\n\nAdd missing spaces in adjacent strings"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-05T12:44:31Z",
        "message": "[`core`] fix silent bug `keep_in_fp32` modules (#26589)\n\n* fix silent bug `keep_in_fp32` modules\n\n* final fix\n\n* added a common test.\n\n* Trigger CI\n\n* revert"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-03T12:53:09Z",
        "message": "[`PEFT`] Final fixes (#26559)\n\n* fix issues with PEFT\n\n* logger warning futurewarning issues\n\n* fixup\n\n* adapt from suggestions\n\n* oops\n\n* rm test"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-03T06:55:39Z",
        "message": "[RFC, Logging] Change warning to info (#26545)\n\n[Logging] Change warning to info"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-02T12:59:24Z",
        "message": "[`PEFT`] Protect `adapter_kwargs` check (#26537)\n\nUpdate modeling_utils.py"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-02T09:23:03Z",
        "message": "[`PEFT`] Pass token when calling `find_adapter_config` (#26488)\n\n* try\n\n* nit\n\n* nits"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-28T09:13:03Z",
        "message": "[`PEFT`]\u00a0introducing `adapter_kwargs` for loading adapters from different Hub location (`subfolder`, `revision`) than the base model (#26270)\n\n* make use of adapter_revision\n\n* v1 adapter kwargs\n\n* fix CI\n\n* fix CI\n\n* fix CI\n\n* fixup\n\n* add BC\n\n* Update src/transformers/integrations/peft.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* fixup\n\n* change it to error\n\n* Update src/transformers/modeling_utils.py\n\n* Update src/transformers/modeling_utils.py\n\n* fixup\n\n* change\n\n* Update src/transformers/integrations/peft.py\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-27T14:45:31Z",
        "message": "[`PEFT`] Fix PEFT multi adapters support (#26407)\n\n* fix PEFT multi adapters support\n\n* refactor a bit\n\n* save pretrained + BC + added tests\n\n* Update src/transformers/integrations/peft.py\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* add more tests\n\n* add suggestion\n\n* final changes\n\n* adapt a bit\n\n* fixup\n\n* Update src/transformers/integrations/peft.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* adapt from suggestions\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-22T15:42:10Z",
        "message": "[`core` ]\u00a0Integrate Flash attention 2 in most used models (#25598)\n\n* v1\n\n* oops\n\n* working v1\n\n* fixup\n\n* add some TODOs\n\n* fixup\n\n* padding support + try with module replacement\n\n* nit\n\n* alternative design\n\n* oops\n\n* add `use_cache` support for llama\n\n* v1 falcon\n\n* nit\n\n* a bit of refactor\n\n* nit\n\n* nits nits\n\n* add v1 padding support falcon (even though it seemed to work before)\n\n* nit\n\n* falcon works\n\n* fixup\n\n* v1 tests\n\n* nit\n\n* fix generation llama flash\n\n* update tests\n\n* fix tests + nits\n\n* fix copies\n\n* fix nit\n\n* test- padding mask\n\n* stype\n\n* add more mem efficient support\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fixup\n\n* nit\n\n* fixup\n\n* remove it from config when saving\n\n* fixup\n\n* revert docstring\n\n* add more checks\n\n* use values\n\n* oops\n\n* new version\n\n* fixup\n\n* add same trick for falcon\n\n* nit\n\n* add another test\n\n* change tests\n\n* fix issues with GC and also falcon\n\n* fixup\n\n* oops\n\n* Update src/transformers/models/falcon/modeling_falcon.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* add init_rope\n\n* updates\n\n* fix copies\n\n* fixup\n\n* fixup\n\n* more clarification\n\n* fixup\n\n* right padding tests\n\n* add docs\n\n* add FA in docker image\n\n* more clarifications\n\n* add some figures\n\n* add todo\n\n* rectify comment\n\n* Change to FA2\n\n* Update docs/source/en/perf_infer_gpu_one.md\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* split in two lines\n\n* change test name\n\n* add more tests\n\n* some clean up\n\n* remove `rearrange` deps\n\n* add more docs\n\n* revert changes on dockerfile\n\n* Revert \"revert changes on dockerfile\"\n\nThis reverts commit 8d72a66b4b9b771abc3f15a9b9506b4246d62d8e.\n\n* revert changes on dockerfile\n\n* Apply suggestions from code review\n\nCo-authored-by: Lysandre Debut <hi@lysand.re>\n\n* address some comments\n\n* docs\n\n* use inheritance\n\n* Update src/transformers/testing_utils.py\n\nCo-authored-by: Lysandre Debut <hi@lysand.re>\n\n* fixup\n\n* Apply suggestions from code review\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update src/transformers/modeling_utils.py\n\n* final comments\n\n* clean up\n\n* style\n\n* add cast + warning for PEFT models\n\n* fixup\n\n---------\n\nCo-authored-by: Felix Marty <9808326+fxmarty@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\nCo-authored-by: Lysandre Debut <hi@lysand.re>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-21T10:00:03Z",
        "message": "Keep relevant weights in fp32 when `model._keep_in_fp32_modules` is set even when `accelerate` is not installed (#26225)\n\n* fix bug where weight would not be kept in fp32\n\n* nit\n\n* address review comments\n\n* fix test"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-20T04:56:16Z",
        "message": "FSDP tests and checkpointing fixes (#26180)\n\n* add fsdp tests\n\n* Update test_fsdp.py\n\n* Update test_fsdp.py\n\n* fixes\n\n* checks\n\n* Update trainer.py\n\n* fix\n\n* fixes for saving/resuming checkpoints\n\n* fixes\n\n* add tests and delete debug statements\n\n* fixing tests\n\n* Update test_fsdp.py\n\n* fix tests\n\n* fix tests\n\n* minor nits\n\n* fix code style and quality\n\n* refactor and modularize test code\n\n* reduce the time of tests\n\n* reduce the test time\n\n* fix test\n\n* reduce test time\n\n* reduce test time\n\n* fix failing tests\n\n* fix\n\n* Apply suggestions from code review\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* resolve comments\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-19T19:44:41Z",
        "message": "[FIX] resize_token_embeddings (#26102)\n\n* fix roundup command\n\n* add test for resize_token_embeddings\n\n* Update tests/test_modeling_common.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* style\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-19T19:04:56Z",
        "message": "DeepSpeed ZeRO-3 handling when resizing embedding layers (#26259)\n\n* fix failing deepspeed slow tests\n\n* fixes"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-15T15:53:39Z",
        "message": "Fix pad to multiple of (#25732)\n\n* nits\n\n* update the test\n\n* nits\n\n* update\n\n* fix bark\n\n* fix bark tests and allow padding to multiple of without new tokens"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-14T11:01:58Z",
        "message": "[`PEFT`] Fix PEFT + gradient checkpointing (#25846)\n\n* fix PEFT + gradient checkpointing\n\n* add disable RG\n\n* polish tests\n\n* fix comment\n\n* Revert \"fix comment\"\n\nThis reverts commit b85386f50d2b104bac522e823c47b7e232116a47.\n\n* final explanations and tests"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-13T12:12:35Z",
        "message": "[`core`] fix 4bit `num_parameters` (#26132)\n\n* fix 4bit `num_parameters`\n\n* stronger check"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-13T04:56:37Z",
        "message": "safeguard torch distributed check (#26056)"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-08T19:13:33Z",
        "message": "Skip warning if tracing with dynamo (#25581)\n\n* Ignore warning if tracing with dynamo\n\n* fix import error\n\n* separate to function\n\n* add test"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-07T09:10:40Z",
        "message": "fix _resize_token_embeddings will set lm head size to 0 when enabled deepspeed zero3 (#26024)"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-06T15:45:47Z",
        "message": "modify context length for GPTQ + version bump (#25899)\n\n* add new arg for gptq\n\n* add tests\n\n* add min version autogptq\n\n* fix order\n\n* skip test\n\n* fix\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* fix style\n\n* change model path\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-05T09:37:54Z",
        "message": "nn.Identity is not required to be compatible with PyTorch < 1.1.0 as the minimum PyTorch version we currently support is 1.10.0 (#25974)\n\nnn.Identity is not required to be compatible with PyTorch < 1.1.0 as the\nminimum PyTorch version we currently support is 1.10.0"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-31T21:38:14Z",
        "message": "remove torch_dtype override (#25894)\n\n* remove torch_dtype override\n\n* style\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-30T15:00:36Z",
        "message": "fix max_memory for bnb (#25842)"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-29T19:10:46Z",
        "message": "Generate: models with custom `generate()` return `True` in `can_generate()` (#25838)"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-29T11:32:19Z",
        "message": "Resolving Attribute error when using the FSDP ram efficient feature (#25820)\n\nfix bug"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-25T15:46:56Z",
        "message": "fix a typo in docsting (#25759)\n\n* fix a typo in docsting\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n---------\n\nCo-authored-by: statelesshz <jihuazhong1@huawei.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/integrations/peft.py",
        "commit_date": "2023-11-14T09:32:57Z",
        "message": "[`Peft`] `modules_to_save` support for peft integration (#27466)\n\n* `modules_to_save` support for peft integration\n\n* Update docs/source/en/peft.md\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* slightly elaborate test\n\n---------\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/integrations/peft.py",
        "commit_date": "2023-11-13T13:20:54Z",
        "message": "Remove-auth-token (#27060)\n\n* don't use `use_auth_token`internally\n\n* let's use token everywhere\n\n* fixup"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/integrations/peft.py",
        "commit_date": "2023-10-03T12:53:09Z",
        "message": "[`PEFT`] Final fixes (#26559)\n\n* fix issues with PEFT\n\n* logger warning futurewarning issues\n\n* fixup\n\n* adapt from suggestions\n\n* oops\n\n* rm test"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/integrations/peft.py",
        "commit_date": "2023-09-28T09:13:03Z",
        "message": "[`PEFT`]\u00a0introducing `adapter_kwargs` for loading adapters from different Hub location (`subfolder`, `revision`) than the base model (#26270)\n\n* make use of adapter_revision\n\n* v1 adapter kwargs\n\n* fix CI\n\n* fix CI\n\n* fix CI\n\n* fixup\n\n* add BC\n\n* Update src/transformers/integrations/peft.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* fixup\n\n* change it to error\n\n* Update src/transformers/modeling_utils.py\n\n* Update src/transformers/modeling_utils.py\n\n* fixup\n\n* change\n\n* Update src/transformers/integrations/peft.py\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/integrations/peft.py",
        "commit_date": "2023-09-27T14:45:31Z",
        "message": "[`PEFT`] Fix PEFT multi adapters support (#26407)\n\n* fix PEFT multi adapters support\n\n* refactor a bit\n\n* save pretrained + BC + added tests\n\n* Update src/transformers/integrations/peft.py\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* add more tests\n\n* add suggestion\n\n* final changes\n\n* adapt a bit\n\n* fixup\n\n* Update src/transformers/integrations/peft.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* adapt from suggestions\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/integrations/peft.py",
        "commit_date": "2023-09-15T16:22:01Z",
        "message": "[PEFT] Allow PEFT model dict to be loaded (#25721)\n\n* Allow PEFT model dict to be loaded\n\n* make style\n\n* make style\n\n* Apply suggestions from code review\n\n* address comments\n\n* fixup\n\n* final change\n\n* added tests\n\n* fix test\n\n* better logic for handling if adapter has been loaded\n\n* Update tests/peft_integration/test_peft_integration.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n---------\n\nCo-authored-by: younesbelkada <younesbelkada@gmail.com>\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/integrations/peft.py",
        "commit_date": "2023-08-30T10:56:05Z",
        "message": "minor typo fix in PeftAdapterMixin docs (#25829)\n\nfix minor documentation typo"
    },
    {
        "repo_url": "github.com/huggingface/transformers",
        "filepath": "src/transformers/integrations/peft.py",
        "commit_date": "2023-08-25T15:13:34Z",
        "message": "\ud83d\udea8\ud83d\udea8\ud83d\udea8 [`Refactor`] Move third-party related utility files into `integrations/` folder \ud83d\udea8\ud83d\udea8\ud83d\udea8 (#25599)\n\n* move deepspeed to `lib_integrations.deepspeed`\n\n* more refactor\n\n* oops\n\n* fix slow tests\n\n* Fix docs\n\n* fix docs\n\n* addess feedback\n\n* address feedback\n\n* final modifs for PEFT\n\n* fixup\n\n* ok now\n\n* trigger CI\n\n* trigger CI again\n\n* Update docs/source/en/main_classes/deepspeed.md\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* import from `integrations`\n\n* address feedback\n\n* revert removal of `deepspeed` module\n\n* revert removal of `deepspeed` module\n\n* fix conflicts\n\n* ooops\n\n* oops\n\n* add deprecation warning\n\n* place it on the top\n\n* put `FutureWarning`\n\n* fix conflicts with not_doctested.txt\n\n* add back `bitsandbytes` module with a depr warning\n\n* fix\n\n* fix\n\n* fixup\n\n* oops\n\n* fix doctests\n\n---------\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2024-02-06T14:21:36Z",
        "message": "Revert \"Remove non-HF ExLlamaV2 loader (#5431)\"\n\nThis reverts commit cde000d47801fa13c5a88f9e435da64132bd96bc."
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2024-02-04T04:40:10Z",
        "message": "Lint"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2024-02-04T04:15:51Z",
        "message": "Remove non-HF ExLlamaV2 loader (#5431)"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-12-31T04:57:06Z",
        "message": "Remove exllamav1 loaders (#5128)"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-12-20T16:29:19Z",
        "message": "let exllama v1 models load safetensor loras (#4854)"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-12-20T04:54:32Z",
        "message": "Improve several log messages"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-11-19T15:59:29Z",
        "message": "Minor LoRA bug fix"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-11-19T15:55:25Z",
        "message": "Fix PEFT LoRA unloading"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-10-27T02:39:51Z",
        "message": "Intel Gpu support initialization (#4340)"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-10-23T20:07:17Z",
        "message": "Organize"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-10-22T19:06:22Z",
        "message": "transformers loader: multi-LoRAs support (#3120)"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-10-14T19:12:41Z",
        "message": "Exllamav2 lora support (#4229)\n\n\n---------\n\nCo-authored-by: oobabooga <112222186+oobabooga@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-08-10T17:03:12Z",
        "message": "Allow --lora to use an absolute path"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-07-18T00:27:18Z",
        "message": "Use 'torch.backends.mps.is_available' to check if mps is supported (#3164)"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-07-12T18:33:25Z",
        "message": "lint"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-07-09T04:03:43Z",
        "message": "Lora fixes for AutoGPTQ (#2818)"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-07-07T05:24:07Z",
        "message": " Fixed the param name when loading a LoRA using a model loaded in 4 or 8 bits (#3036)"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-07-03T20:40:22Z",
        "message": "Update LoRA.py - avoid potential error (#2953)"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-06-26T03:10:33Z",
        "message": "Add LoRA support to ExLlama_HF"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-06-24T23:24:17Z",
        "message": "Use pre-compiled python module for ExLlama (#2770)"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-06-19T15:31:24Z",
        "message": "Add ExLlama+LoRA support (#2756)"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-06-16T22:00:37Z",
        "message": "Reorganize model loading UI completely (#2720)"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-06-14T21:29:42Z",
        "message": "Add LORA name instead of \"default\" in PeftModel (#2689)"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-06-06T16:05:05Z",
        "message": "Handle the case of older autogptq install"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-06-06T02:32:57Z",
        "message": "Add AutoGPTQ LoRA support"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-05-22T01:42:34Z",
        "message": "Prevent unwanted log messages from modules"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-05-08T19:21:55Z",
        "message": "fixed LoRA loading issue (#1865)"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-05-04T00:43:17Z",
        "message": "Better warning messages"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-04-26T01:58:48Z",
        "message": "Load more than one LoRA with --lora, fix a bug"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-04-26T00:20:26Z",
        "message": "Monkey patch fixes"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-04-17T02:26:52Z",
        "message": "Add 4-bit LoRA support (#1200)"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-04-14T17:52:06Z",
        "message": "initial multi-lora support (#1103)\n\n---------\n\nCo-authored-by: oobabooga <112222186+oobabooga@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-04-08T00:36:04Z",
        "message": "SD Api Pics extension, v.1.1 (#596)"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-04-07T03:15:45Z",
        "message": "Make the code more like PEP8 for readability (#862)"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-03-30T01:50:58Z",
        "message": "Move an import"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-03-28T02:16:44Z",
        "message": "Merge branch 'main' into catalpaaa-lora-and-model-dir"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-03-27T03:04:43Z",
        "message": "Handle unloading LoRA from dropdown menu icon"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-03-25T08:28:33Z",
        "message": "Merge branch 'oobabooga:main' into lora-and-model-dir"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-03-25T04:18:32Z",
        "message": "Fix LoRA on mps"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-03-25T00:30:18Z",
        "message": "lora-dir, model-dir and login auth\n\nAdded lora-dir, model-dir, and a login auth arguments that points to a file contains usernames and passwords in the format of \"u:pw,u:pw,...\""
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-03-24T01:02:09Z",
        "message": "Update LoRA.py"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-03-24T00:56:26Z",
        "message": "Clear cache while switching LoRAs"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-03-23T19:49:41Z",
        "message": "Fix LoRA device map (attempt)"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-03-23T04:05:13Z",
        "message": "Fix LoRA in CPU mode"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-03-23T03:55:33Z",
        "message": "Make LoRAs work in 16-bit mode"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-03-19T15:11:35Z",
        "message": "Make custom LoRAs work by default #385"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-03-18T13:55:24Z",
        "message": "Don't include PeftModel every time"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-03-17T20:45:28Z",
        "message": "Add some LoRA params"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-03-17T16:07:17Z",
        "message": "Add a comment"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-03-17T14:43:11Z",
        "message": "Remove unused import"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-03-17T14:39:48Z",
        "message": "Remove LoRA tab, move it into the Parameters menu"
    },
    {
        "repo_url": "github.com/oobabooga/text-generation-webui",
        "filepath": "modules/LoRA.py",
        "commit_date": "2023-03-17T00:35:53Z",
        "message": "Add files"
    },
    {
        "repo_url": "github.com/nomic-ai/gpt4all",
        "filepath": "gpt4all-training/generate.py",
        "commit_date": "2023-10-24T13:28:21Z",
        "message": "make scripts executable (#1555)"
    },
    {
        "repo_url": "github.com/nomic-ai/gpt4all",
        "filepath": "gpt4all-training/generate.py",
        "commit_date": "2023-05-01T19:45:23Z",
        "message": "mono repo structure"
    },
    {
        "repo_url": "github.com/TabbyML/tabby",
        "filepath": "python/tabby/trainer.py",
        "commit_date": "2023-06-13T19:48:27Z",
        "message": "feat: cleanup trainer with new data format"
    },
    {
        "repo_url": "github.com/mlflow/mlflow",
        "filepath": "mlflow/transformers/peft.py",
        "commit_date": "2024-02-29T00:24:11Z",
        "message": "Merge PEFT feature branch (#11240)\n\nSigned-off-by: B-Step62 <yuki.watanabe@databricks.com>\nSigned-off-by: Yuki Watanabe <31463517+B-Step62@users.noreply.github.com>\nCo-authored-by: Ben Wilson <39283302+BenWilson2@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/apply_lora.py",
        "commit_date": "2023-06-14T09:07:44Z",
        "message": "Add support for QLoRA (#1676)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/apply_lora.py",
        "commit_date": "2023-04-24T10:02:08Z",
        "message": "Add support for Baize and LoRA models (#553)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/auto.py",
        "commit_date": "2024-02-22T04:08:19Z",
        "message": "Allow trust_remote_code for tokenizers when loading AutoPeftModels (#1477)\n\n* feat: Allow tokenizer remote code when loading AutoPeftModels\n\n* style: Merge arguments into one line"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/auto.py",
        "commit_date": "2024-02-12T13:41:35Z",
        "message": "FIX Honor HF_HUB_OFFLINE mode if set by user (#1454)\n\nResolves #1452\n\nIf users enable offline mode, don't perform checks for files on HF Hub,\nas they would fail."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/auto.py",
        "commit_date": "2024-02-09T10:00:03Z",
        "message": "FIX Loading with AutoPeftModel.from_pretrained (#1449)\n\nFixes #1430\n\nWhen Using AutoPeftModel.from_pretrained, there is a check to see if a\ntokenizer can be found. This check will include a search for the\ntokenizer on HF Hub. However, when the model is stored locally, the path\nmay not be a valid HF Hub repo ID. In that case, an error is raised by\nhuggingface_hub.\n\nThis PR consists of catching that error, and assuming that if the error\noccurs, the tokenizer does not exist. This resolves the issue."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/auto.py",
        "commit_date": "2024-02-07T11:52:35Z",
        "message": "MNT Move code quality fully to ruff (#1421)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/auto.py",
        "commit_date": "2024-01-17T15:32:16Z",
        "message": "Handle resizing of embedding layers for AutoPeftModel (#1367)\n\n* handle resizing of embedding layers for AutoPeftModel\n\n* fixes\n\n* add test"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/auto.py",
        "commit_date": "2023-08-07T14:34:54Z",
        "message": "[`core`] PEFT refactor + introducing `inject_adapter_in_model` public method (#749)\n\nRefactors a bit the internals of some PEFT models and introduces a new\nmethod inject_adapter_in_model for users that want to pass a bare model\nand a peft config to inject adapters in-place into the model. These\nchanges are totally BC with the previous PEFT versions.\n\nThis PR makes things easier for the PEFT integration in transformers\nhuggingface/transformers#25077\n\nThe main goal of the PR is to expose a new API for advanced users that\nwant to integrate PEFT method without the need to use the PeftModel\nwrapper. A simple use case could be someone that wants to inject adapters\ninto a model and wants to keep the original class of the model without\nhaving to offload that to peft that will create a PeftModel. I have\nfaced this issue in huggingface/transformers#25077 Among other things,\nthis PR refactors some internals of PEFT library, while keeping it fully\nbackward compatible.\n\nTo tackle the main motivation I propose to differentiate things between\ntwo type of adapters\n\n1- adapters that are injectable (LoRA, AdaLoRA, IA3)\n2- adapters that are not injectable (the rest)\n\nAs a first iteration this API would be supported only for the scenario\n1- / therefore I decided to create 2 abstract classes to make things\neasy to be able to determine if the adapter layer (e.g. LoraLayer) /\nadapter module (e.g. LoraModel) does follow the minimal\nrequirement (i.e. needed attributes, etc.)\n\nOther related changes:\n\n1- Creates a new property method is_prompt_learning to avoid importing\n   PromptLearningConfig all the way down\n2- Introduces a new object TUNERS_MAPPING, which is a mapping of\n   supported pluggable adapters\n3- Creates two abstract classes\n3.1- BaseTunerLayer: a mixin to check for minimal required attributes\n     that a tuner layer should have active_adapter / _is_plugable\n3.2- BaseTuner: a higher level module mixin that should be used for any\n     injectable adapters in the future.\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/auto.py",
        "commit_date": "2023-07-15T12:18:34Z",
        "message": "[`Auto`] Support `AutoPeftModel` for custom HF models (#707)\n\n* support `AutoPeftModel` for custom HF models\n\n* added documentation."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/auto.py",
        "commit_date": "2023-07-14T09:07:09Z",
        "message": "Introducing `AutoPeftModelForxxx` (#694)\n\n* working v1 for LMs\n\n* added tests.\n\n* added documentation.\n\n* fixed ruff issues.\n\n* added `AutoPeftModelForFeatureExtraction` .\n\n* replace with `TypeError`\n\n* address last comments\n\n* added comment."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/config.py",
        "commit_date": "2024-02-07T11:52:35Z",
        "message": "MNT Move code quality fully to ruff (#1421)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/config.py",
        "commit_date": "2024-01-29T06:25:01Z",
        "message": "add peft type constructor (#1398)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/config.py",
        "commit_date": "2023-12-07T22:22:26Z",
        "message": "[docs] PeftConfig and PeftModel (#1211)\n\n* rough draft\n\n* feedback\n\n* feedback"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/config.py",
        "commit_date": "2023-11-22T19:52:26Z",
        "message": "(minor) correct type annotation (#1166)\n\n* add correct type annotation\n\n* make style"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/config.py",
        "commit_date": "2023-10-24T10:06:01Z",
        "message": "Fix target_modules type in config.from_pretrained (#1046)\n\nFixes #1045, supersedes #1041\n\nDescription\n\nWhen loading a config from a file, we currently set the loaded\nattributes on the config directly. However, this sidesteps the\n__post_init__ call, which is required to convert the target_modules to a\nset. This PR fixes this by avoiding to set attributes on the config\nclass directly, instead of going through __init__.\n\nOther changes\n\nWhile working on this, I did a slight refactor of the config tests.\n\n1. All config classes are included now (some where missing before).\n2. Use parameterized instead of looping through the classes.\n3. Added a unit test for the aforementioned bug."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/config.py",
        "commit_date": "2023-10-05T07:57:49Z",
        "message": "Fix lora creation (#993)\n\n* reducing the time for inject lora modules\n\n* fix bugs\n\n* fix bug\n\n* fixes\n\n* Revert \"fixes\"\n\nThis reverts commit c7f30627c1798db11be8a5da8f3c801f9469a5e3.\n\n* refactor\n\n* fix failing tests\n\n* fix tests\n\n* fix tests\n\n* fix tests\n\n* fix tests\n\n* Apply suggestions from code review\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* address comments\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/config.py",
        "commit_date": "2023-09-25T14:42:51Z",
        "message": "feat: add type hints (#858)\n\n* feat: add type hints\n\n* build: trigger ci"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/config.py",
        "commit_date": "2023-08-07T14:34:54Z",
        "message": "[`core`] PEFT refactor + introducing `inject_adapter_in_model` public method (#749)\n\nRefactors a bit the internals of some PEFT models and introduces a new\nmethod inject_adapter_in_model for users that want to pass a bare model\nand a peft config to inject adapters in-place into the model. These\nchanges are totally BC with the previous PEFT versions.\n\nThis PR makes things easier for the PEFT integration in transformers\nhuggingface/transformers#25077\n\nThe main goal of the PR is to expose a new API for advanced users that\nwant to integrate PEFT method without the need to use the PeftModel\nwrapper. A simple use case could be someone that wants to inject adapters\ninto a model and wants to keep the original class of the model without\nhaving to offload that to peft that will create a PeftModel. I have\nfaced this issue in huggingface/transformers#25077 Among other things,\nthis PR refactors some internals of PEFT library, while keeping it fully\nbackward compatible.\n\nTo tackle the main motivation I propose to differentiate things between\ntwo type of adapters\n\n1- adapters that are injectable (LoRA, AdaLoRA, IA3)\n2- adapters that are not injectable (the rest)\n\nAs a first iteration this API would be supported only for the scenario\n1- / therefore I decided to create 2 abstract classes to make things\neasy to be able to determine if the adapter layer (e.g. LoraLayer) /\nadapter module (e.g. LoraModel) does follow the minimal\nrequirement (i.e. needed attributes, etc.)\n\nOther related changes:\n\n1- Creates a new property method is_prompt_learning to avoid importing\n   PromptLearningConfig all the way down\n2- Introduces a new object TUNERS_MAPPING, which is a mapping of\n   supported pluggable adapters\n3- Creates two abstract classes\n3.1- BaseTunerLayer: a mixin to check for minimal required attributes\n     that a tuner layer should have active_adapter / _is_plugable\n3.2- BaseTuner: a higher level module mixin that should be used for any\n     injectable adapters in the future.\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/helpers.py",
        "commit_date": "2023-09-25T14:42:51Z",
        "message": "feat: add type hints (#858)\n\n* feat: add type hints\n\n* build: trigger ci"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/helpers.py",
        "commit_date": "2023-08-10T10:14:40Z",
        "message": "Helper function to update model signature (#784)\n\nProvides helper functions in peft.helpers to update the signature of the\nforward or generate method of a PeftModel (or subclass). This can be\nuseful because the wrapping class may override the docstring and type\nannotations of the underlying base model. Applying the helper functions\nwill restore those, leading to better tab completion, help text, etc.\n\nFor the time being, these helper functions are purely optional to use.\nAt a later stage, we may consider applying them automatically, but that\nwould require testing to ensure that nothing breaks."
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2024-02-28T19:26:39Z",
        "message": "refactor: move model helper function in pipeline to a mixin class (#6571)\n\n* move model helper function in pipeline to EfficiencyMixin\n\n---------\n\nCo-authored-by: YiYi Xu <yixu310@gmail.com>\nCo-authored-by: Sayak Paul <spsayakpaul@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2024-02-27T03:22:38Z",
        "message": "[Core] pass revision in the loading_kwargs. (#7019)\n\n* pass revision in the loading_kwarhs.\n\n* remove revision from load_sub_model."
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2024-02-23T17:24:51Z",
        "message": "Fix typos (#7068)"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2024-02-12T18:38:13Z",
        "message": "[docs] Community pipelines (#6929)\n\nfix"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2024-02-12T08:12:32Z",
        "message": "Pass device to enable_model_cpu_offload in maybe_free_model_hooks (#6937)"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2024-02-08T18:19:31Z",
        "message": "change to 2024 in the license (#6902)\n\nchange to 2024"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2024-02-08T04:08:57Z",
        "message": "Remove `torch_dtype` in to() to end deprecation (#6886)\n\n* remove torch_dtype from to()\n\n* remove torch_dtype from usage scripts.\n\n* remove old lora backend\n\n* Revert \"remove old lora backend\"\n\nThis reverts commit adcddf6ba421f847e7da2a0ce57b9456cae43356."
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2024-01-26T17:31:48Z",
        "message": "[Hub] feat: explicitly tag to diffusers when using push_to_hub (#6678)\n\n* feat: explicitly tag to diffusers when using push_to_hub\n\n* remove tags.\n\n* reset repo.\n\n* Apply suggestions from code review\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix: tests\n\n* fix: push_to_hub behaviour for tagging from save_pretrained\n\n* Apply suggestions from code review\n\nCo-authored-by: Lucain <lucainp@gmail.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Lucain <lucainp@gmail.com>\n\n* import fixes.\n\n* add library name to existing model card.\n\n* add: standalone test for generate_model_card\n\n* fix tests for standalone method\n\n* moved library_name to a better place.\n\n* merge create_model_card and generate_model_card.\n\n* fix test\n\n* address lucain's comments\n\n* fix return identation\n\n* Apply suggestions from code review\n\nCo-authored-by: Lucain <lucainp@gmail.com>\n\n* address further comments.\n\n* Update src/diffusers/pipelines/pipeline_utils.py\n\nCo-authored-by: Lucain <lucainp@gmail.com>\n\n---------\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: Lucain <lucainp@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2024-01-23T09:12:03Z",
        "message": "[Refactor] Update from single file (#6428)\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update'\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* up\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* up\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update'\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* clean\n\n* update\n\n* update\n\n* clean up\n\n* clean up\n\n* update\n\n* clean\n\n* clean\n\n* update\n\n* updaet\n\n* clean up\n\n* fix docs\n\n* update\n\n* update\n\n* Revert \"update\"\n\nThis reverts commit dbfb8f1ea9c61a2b4e02f926245be2b3d387e577.\n\n* update\n\n* update\n\n* update\n\n* update\n\n* fix controlnet\n\n* fix scheduler\n\n* fix controlnet tests"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2024-01-05T12:32:09Z",
        "message": "Correctly handle creating model index json files when setting compiled modules in pipelines.  (#6436)\n\nupdate"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2024-01-04T16:05:55Z",
        "message": "Respect offline mode when loading pipeline (#6456)\n\n* Respect offline mode when loading model\n\n* default to local entry if connectionerror"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-12-06T21:22:31Z",
        "message": "Harmonize HF environment variables + deprecate use_auth_token (#6066)\n\n* Harmonize HF environment variables + deprecate use_auth_token\n\n* fix import\n\n* fix"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-12-01T17:43:44Z",
        "message": "Post Release: v0.24.0 (#5985)\n\n* Post Release: v0.24.0\n\n* post pone deprecation\n\n* post pone deprecation\n\n* Add model_index.json"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-11-29T15:33:04Z",
        "message": "Fixed custom module importing on Windows (#5891)\n\n* Fixed custom module importing on Windows\n\nWindows use back slash and `os.path.join()` follows that convention.\n\n* Apply suggestions from code review\n\nCo-authored-by: Lucain <lucainp@gmail.com>\n\n* Update pipeline_utils.py\n\n---------\n\nCo-authored-by: Sayak Paul <spsayakpaul@gmail.com>\nCo-authored-by: Lucain <lucainp@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-11-27T13:35:19Z",
        "message": "[From_pretrained] Fix warning (#5948)"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-11-21T17:34:30Z",
        "message": "[feat] IP Adapters (author @okotaku ) (#5713)\n\n* add ip-adapter\n\n\n---------\n\nCo-authored-by: okotaku <to78314910@gmail.com>\nCo-authored-by: sayakpaul <spsayakpaul@gmail.com>\nCo-authored-by: yiyixuxu <yixu310@gmail,com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-11-20T10:48:34Z",
        "message": "[Styling] stylify using ruff (#5841)\n\n* ruff format\n\n* not need to use doc-builder's black styling as the doc is styled in ruff\n\n* make fix-copies\n\n* comment\n\n* use run_ruff"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-11-14T11:08:03Z",
        "message": "Unwrap models everywhere (#5789)\n\nmore debug"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-11-14T10:40:35Z",
        "message": "[Enhacne] Support maybe_raise_or_warn for peft (#5653)\n\n* Support maybe_raise_or_warn for peft\n\n* fix by comment\n\n* unwrap function"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-11-08T10:51:15Z",
        "message": "Fixed is_safetensors_compatible() handling of windows path separators (#5650)\n\nCloses #4665"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-11-06T14:11:48Z",
        "message": "[Custom Pipelines] Make sure that community pipelines can use repo revision (#5659)\n\nfix custom pipelines"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-10-30T18:32:11Z",
        "message": "Fix incorrect loading of custom pipeline (#5568)\n\n* update\n\n* update\n\n* update\n\n* update"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-10-26T15:11:49Z",
        "message": "[Remote code] Add functionality to run remote models, schedulers, pipelines (#5472)\n\n* upload custom remote poc\n\n* up\n\n* make style\n\n* finish\n\n* better name\n\n* Apply suggestions from code review\n\n* Update tests/pipelines/test_pipelines.py\n\n* more fixes\n\n* remove ipdb\n\n* more fixes\n\n* fix more\n\n* finish tests\n\n---------\n\nCo-authored-by: Sayak Paul <spsayakpaul@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-09-27T12:04:57Z",
        "message": "Fix memory issues in tests (#5183)\n\n* fix memory issues\n\n* set _offload_gpu_id\n\n* set gpu offload id"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-09-25T18:24:03Z",
        "message": "make style"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-09-25T18:22:41Z",
        "message": "[pipeline utils] sanitize pretrained_model_name_or_path (#5173)\n\nMake sure the repo_id is valid before sending it to huggingface_hub to get a more understandable error message.\n\nRe #5110\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-09-25T17:26:39Z",
        "message": "Fix type annotation (#5146)\n\n* Fix type annotation on Scheduler.from_pretrained\n\n* Fix type annotation on PIL.Image"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-09-25T12:10:18Z",
        "message": "[Core] Improve `.to(...)` method, fix offloads multi-gpu, add docstring, add dtype (#5132)\n\n* fix cpu offload\n\n* fix\n\n* fix\n\n* Update src/diffusers/pipelines/pipeline_utils.py\n\n* make style\n\n* Apply suggestions from code review\n\nCo-authored-by: YiYi Xu <yixu310@gmail.com>\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* fix more\n\n* fix more\n\n---------\n\nCo-authored-by: YiYi Xu <yixu310@gmail.com>\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-09-14T16:28:57Z",
        "message": "[Release 0.21] Bump version (#5018)\n\n* [Release 0.21] Bump version\n\n* fix & remove\n\n* fix more\n\n* fix all, upload"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-09-14T14:36:19Z",
        "message": "Allow disabling `from_pretrained` tqdm (#5007)"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-09-14T09:02:06Z",
        "message": "Fix model offload bug when key isn't present (#5030)\n\n* fix model offload bug when key isn't present\n\n* make style"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-09-13T09:29:13Z",
        "message": "[Flax->PT] Fix flaky testing (#5011)\n\nfix flaky flax class name"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-09-12T15:58:47Z",
        "message": "[docs] Fix DiffusionPipeline.enable_sequential_cpu_offload docstring (#4952)\n\n* Fix an unmatched backtick and make description more general for DiffusionPipeline.enable_sequential_cpu_offload.\n\n* make style\n\n* _exclude_from_cpu_offload -> self._exclude_from_cpu_offload\n\n* make style\n\n* apply suggestions from review\n\n* make style"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-09-11T17:39:26Z",
        "message": "Refactor model offload (#4514)\n\n* [Draft] Refactor model offload\n\n* [Draft] Refactor model offload\n\n* Apply suggestions from code review\n\n* cpu offlaod updates\n\n* remove model cpu offload from individual pipelines\n\n* add hook to offload models to cpu\n\n* clean up\n\n* model offload\n\n* add model cpu offload string\n\n* make style\n\n* clean up\n\n* fixes for offload issues\n\n* fix tests issues\n\n* resolve merge conflicts\n\n* update src/diffusers/pipelines/pipeline_utils.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* make style\n\n* Update src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py\n\n---------\n\nCo-authored-by: Dhruv Nair <dhruv.nair@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-09-11T10:03:49Z",
        "message": "Make sure Flax pipelines can be loaded into PyTorch (#4971)\n\n* Make sure Flax pipelines can be loaded into PyTorch\n\n* add test\n\n* Update src/diffusers/pipelines/pipeline_utils.py"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-09-11T07:56:22Z",
        "message": "Lazy Import for Diffusers (#4829)\n\n* initial commit\n\n* move modules to import struct\n\n* add dummy objects and _LazyModule\n\n* add lazy import to schedulers\n\n* clean up unused imports\n\n* lazy import on models module\n\n* lazy import for schedulers module\n\n* add lazy import to pipelines module\n\n* lazy import altdiffusion\n\n* lazy import audio diffusion\n\n* lazy import audioldm\n\n* lazy import consistency model\n\n* lazy import controlnet\n\n* lazy import dance diffusion ddim ddpm\n\n* lazy import deepfloyd\n\n* lazy import kandinksy\n\n* lazy imports\n\n* lazy import semantic diffusion\n\n* lazy imports\n\n* lazy import stable diffusion\n\n* move sd output to its own module\n\n* clean up\n\n* lazy import t2iadapter\n\n* lazy import unclip\n\n* lazy import versatile and vq diffsuion\n\n* lazy import vq diffusion\n\n* helper to fetch objects from modules\n\n* lazy import sdxl\n\n* lazy import txt2vid\n\n* lazy import stochastic karras\n\n* fix model imports\n\n* fix bug\n\n* lazy import\n\n* clean up\n\n* clean up\n\n* fixes for tests\n\n* fixes for tests\n\n* clean up\n\n* remove import of torch_utils from utils module\n\n* clean up\n\n* clean up\n\n* fix mistake import statement\n\n* dedicated modules for exporting and loading\n\n* remove testing utils from utils module\n\n* fixes from  merge conflicts\n\n* Update src/diffusers/pipelines/kandinsky2_2/__init__.py\n\n* fix docs\n\n* fix alt diffusion copied from\n\n* fix check dummies\n\n* fix more docs\n\n* remove accelerate import from utils module\n\n* add type checking\n\n* make style\n\n* fix check dummies\n\n* remove torch import from xformers check\n\n* clean up error message\n\n* fixes after upstream merges\n\n* dummy objects fix\n\n* fix tests\n\n* remove unused module import\n\n---------\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-09-04T16:21:36Z",
        "message": "allow passing components to connected pipelines when use the combined pipeline (#4883)\n\n* fix\n\n* add test\n\n---------\n\nCo-authored-by: yiyixuxu <yixu310@gmail,com>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-08-25T16:25:48Z",
        "message": "fix a bug in `from_pretrained` when load optional components (#4745)\n\n* fix\n---------\n\nCo-authored-by: yiyixuxu <yixu310@gmail,com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-08-25T09:13:32Z",
        "message": "Torch device (#4755)"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-08-17T06:19:32Z",
        "message": "Fix `use_onnx` parameter usage in `from_pretrained` func and update `test_download_no_onnx_by_default` test (#4508)\n\n* add missing use_onnx in from_pretrained func\n\n* fix test_download_no_onnx_by_default test func\n\n* address comments\n\n* split test cases"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-08-17T05:24:28Z",
        "message": "[Safetensors] Make safetensors the default way of saving weights (#4235)\n\n* make safetensors default\n\n* set default save method as safetensors\n\n* update tests\n\n* update to support saving safetensors\n\n* update test to account for safetensors default\n\n* update example tests to use safetensors\n\n* update example to support safetensors\n\n* update unet tests for safetensors\n\n* fix failing loader tests\n\n* fix qc issues\n\n* fix pipeline tests\n\n* fix example test\n\n---------\n\nCo-authored-by: Dhruv Nair <dhruv.nair@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-08-15T02:09:22Z",
        "message": "[Pipeline utils] feat: implement push_to_hub for standalone models, schedulers as well as pipelines (#4128)\n\n* feat: implement push_to_hub for standalone models.\n\n* address PR feedback.\n\n* Apply suggestions from code review\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* remove max_shard_size.\n\n* add: support for scheduler push_to_hub\n\n* enable push_to_hub support for flax schedulers.\n\n* enable push_to_hub for pipelines.\n\n* Apply suggestions from code review\n\nCo-authored-by: Lucain <lucainp@gmail.com>\n\n* reflect pr feedback.\n\n* address another round of deedback.\n\n* better handling of kwargs.\n\n* add: tests\n\n* Apply suggestions from code review\n\nCo-authored-by: Lucain <lucainp@gmail.com>\n\n* setting hub staging to False for now.\n\n* incorporate staging test as a separate job.\n\nCo-authored-by: ydshieh <2521628+ydshieh@users.noreply.github.com>\n\n* fix: tokenizer loading.\n\n* fix: json dumping.\n\n* move is_staging_test to a better location.\n\n* better treatment to tokens.\n\n* define repo_id to better handle concurrency\n\n* style\n\n* explicitly set token\n\n* Empty-Commit\n\n* move SUER, TOKEN to test\n\n* collate org_repo_id\n\n* delete repo\n\n---------\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: Lucain <lucainp@gmail.com>\nCo-authored-by: ydshieh <2521628+ydshieh@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-08-11T05:35:22Z",
        "message": "Remove code snippets containing `is_safetensors_available()` (#4521)\n\n* [WIP] Remove code snippets containing `is_safetensors_available()`\n\n* Modifying `import_utils.py`\n\n* update pipeline tests for safetensor default\n\n* fix test related to cached requests\n\n* address import nits\n\n---------\n\nCo-authored-by: Dhruv Nair <dhruv.nair@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-08-10T18:00:03Z",
        "message": "[docs] Remove attention slicing (#4518)\n\n* remove attention slicing\n\n* apply feedback"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-07-28T12:02:48Z",
        "message": "[ONNX] Don't download ONNX model by default (#4338)\n\n* [Download] Don't download ONNX weights by default\n\n* [Download] Don't download ONNX weights by default\n\n* [Download] Don't download ONNX weights by default\n\n* fix more\n\n* finish\n\n* finish\n\n* finish"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-07-27T14:16:46Z",
        "message": "[Local loading] Correct bug with local files only (#4318)\n\n* [Local loading] Correct bug with local files only\n\n* file not found error\n\n* fix\n\n* finish"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-07-26T21:06:18Z",
        "message": "0.20.0dev0 (#4299)\n\n* 0.20.0dev0\n\n* make style"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-07-26T15:13:55Z",
        "message": "[Kandinsky] Add combined pipelines / Fix cpu model offload / Fix inpainting (#4207)\n\n* Add combined pipeline\n\n* Download readme\n\n* Upload\n\n* up\n\n* up\n\n* fix final\n\n* Add enable model cpu offload kandinsky\n\n* finish\n\n* finish\n\n* Fix\n\n* fix more\n\n* make style\n\n* fix kandinsky mask\n\n* fix inpainting test\n\n* add callbacks\n\n* add tests\n\n* fix tests\n\n* Apply suggestions from code review\n\nCo-authored-by: YiYi Xu <yixu310@gmail.com>\n\n* docs\n\n* docs\n\n* correct docs\n\n* fix tests\n\n* add warning\n\n* correct docs\n\n---------\n\nCo-authored-by: YiYi Xu <yixu310@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-07-24T18:24:29Z",
        "message": "[Docs] Fix from pretrained docs (#4240)\n\n* [Docs] Fix from pretrained docs\n\n* [Docs] Fix from pretrained docs"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-07-24T13:35:16Z",
        "message": "Raise initial HTTPError if pipeline is not cached locally (#4230)\n\n* Raise initial HTTPError if pipeline is not cached locally\n\n* make style"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-07-21T18:01:34Z",
        "message": "[docs] Clean up pipeline apis (#3905)\n\n* start with stable diffusion\n\n* fix\n\n* finish stable diffusion pipelines\n\n* fix path to pipeline output\n\n* fix flax paths\n\n* fix copies\n\n* add up to score sde ve\n\n* finish first pass of pipelines\n\n* fix copies\n\n* second review\n\n* align doc titles\n\n* more review fixes\n\n* final review"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-07-21T11:45:09Z",
        "message": "make enable_sequential_cpu_offload more generic for third-party devices (#4191)\n\n* make enable_sequential_cpu_offload more generic for third-party devices\n\n* make style"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-07-18T09:04:40Z",
        "message": "Refactor execution device & cpu offload (#4114)\n\n* create general cpu offload & execution device\n\n* Remove boiler plate\n\n* finish\n\n* kp\n\n* Correct offload more pipelines\n\n* up\n\n* Update src/diffusers/pipelines/pipeline_utils.py\n\n* make style\n\n* up"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-07-11T10:20:00Z",
        "message": "FIX `force_download` in download utility (#4036)\n\nFIX force_download in download utils"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-07-10T16:05:40Z",
        "message": "Improve loading pipe (#4009)\n\n* improve loading subcomponents\n\n* Add test for logging\n\n* improve loading subcomponents\n\n* make style\n\n* make style\n\n* fix\n\n* finish"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-07-10T10:12:29Z",
        "message": "[DiffusionPipeline] Deprecate not throwing error when loading non-existant variant (#4011)\n\n* Deprecate variant nicely\n\n* make style\n\n* Apply suggestions from code review\n\nCo-authored-by: Sayak Paul <spsayakpaul@gmail.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n---------\n\nCo-authored-by: Sayak Paul <spsayakpaul@gmail.com>\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-07-07T07:33:51Z",
        "message": "typo in safetensors (safetenstors) (#3976)\n\n* Update pipeline_utils.py\n\ntypo in safetensors (safetenstors)\n\n* Update loaders.py\n\ntypo in safetensors (safetenstors)\n\n* Update modeling_utils.py\n\ntypo in safetensors (safetenstors)"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-06-21T18:07:23Z",
        "message": "[docs] More API stuff (#3835)\n\n* clean up loaders\n\n* clean up rest of main class apis\n\n* apply feedback"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-06-08T16:08:49Z",
        "message": "Post 0.17.0 release (#3721)\n\n* Post release\n\n* Post release"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-06-05T16:47:26Z",
        "message": "[docs] More API fixes (#3640)\n\n* part 2 of api fixes\n\n* move randn_tensor\n\n* add to toctree\n\n* apply feedback\n\n* more feedback"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-06-02T17:26:41Z",
        "message": "set config from original module but set compiled module on class (#3650)\n\n* set config from original module but set compiled module on class\n\n* add test"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-05-23T13:22:43Z",
        "message": "Make sure Diffusers works even if Hub is down (#3447)\n\n* Make sure Diffusers works even if Hub is down\n\n* Make sure hub down is well tested"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-05-23T12:20:55Z",
        "message": "Allow custom pipeline loading (#3504)"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-05-22T15:11:08Z",
        "message": "feat: allow disk offload for diffuser models (#3285)\n\n* allow disk offload for diffuser models\n\n* sort import\n\n* add max_memory argument\n\n* Changed sample[0] to images[0] (#3304)\n\nA pipeline object stores the results in `images` not in `sample`.\nCurrent code blocks don't work.\n\n* Typo in tutorial (#3295)\n\n* Torch compile graph fix (#3286)\n\n* fix more\n\n* Fix more\n\n* fix more\n\n* Apply suggestions from code review\n\n* fix\n\n* make style\n\n* make fix-copies\n\n* fix\n\n* make sure torch compile\n\n* Clean\n\n* fix test\n\n* Postprocessing refactor img2img (#3268)\n\n* refactor img2img VaeImageProcessor.postprocess\n\n* remove copy from for init, run_safety_checker, decode_latents\n\nCo-authored-by: Sayak Paul <spsayakpaul@gmail.com>\n\n---------\n\nCo-authored-by: yiyixuxu <yixu@yis-macbook-pro.lan>\nCo-authored-by: Sayak Paul <spsayakpaul@gmail.com>\n\n* [Torch 2.0 compile] Fix more torch compile breaks (#3313)\n\n* Fix more torch compile breaks\n\n* add tests\n\n* Fix all\n\n* fix controlnet\n\n* fix more\n\n* Add Horace He as co-author.\n>\n>\nCo-authored-by: Horace He <horacehe2007@yahoo.com>\n\n* Add Horace He as co-author.\n\nCo-authored-by: Horace He <horacehe2007@yahoo.com>\n\n---------\n\nCo-authored-by: Horace He <horacehe2007@yahoo.com>\n\n* fix: scale_lr and sync example readme and docs. (#3299)\n\n* fix: scale_lr and sync example readme and docs.\n\n* fix doc link.\n\n* Update stable_diffusion.mdx (#3310)\n\nfixed import statement\n\n* Fix missing variable assign in DeepFloyd-IF-II (#3315)\n\nFix missing variable assign\n\nlol\n\n* Correct doc build for patch releases (#3316)\n\nUpdate build_documentation.yml\n\n* Add Stable Diffusion RePaint to community pipelines (#3320)\n\n* Add Stable Diffsuion RePaint to community pipelines\n\n- Adds Stable Diffsuion RePaint to community pipelines\n- Add Readme enty for pipeline\n\n* Fix: Remove wrong import\n\n- Remove wrong import\n- Minor change in comments\n\n* Fix: Code formatting of stable_diffusion_repaint\n\n* Fix: ruff errors in stable_diffusion_repaint\n\n* Fix multistep dpmsolver for cosine schedule (suitable for deepfloyd-if) (#3314)\n\n* fix multistep dpmsolver for cosine schedule (deepfloy-if)\n\n* fix a typo\n\n* Update src/diffusers/schedulers/scheduling_dpmsolver_multistep.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* Update src/diffusers/schedulers/scheduling_dpmsolver_multistep.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* Update src/diffusers/schedulers/scheduling_dpmsolver_multistep.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* Update src/diffusers/schedulers/scheduling_dpmsolver_multistep.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* Update src/diffusers/schedulers/scheduling_dpmsolver_multistep.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* update all dpmsolver (singlestep, multistep, dpm, dpm++) for cosine noise schedule\n\n* add test, fix style\n\n---------\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* [docs] Improve LoRA docs (#3311)\n\n* update docs\n\n* add to toctree\n\n* apply feedback\n\n* Added input pretubation (#3292)\n\n* Added input pretubation\n\n* Fixed spelling\n\n* Update write_own_pipeline.mdx (#3323)\n\n* update controlling generation doc with latest goodies. (#3321)\n\n* [Quality] Make style (#3341)\n\n* Fix config dpm (#3343)\n\n* Add the SDE variant of DPM-Solver and DPM-Solver++ (#3344)\n\n* add SDE variant of DPM-Solver and DPM-Solver++\n\n* add test\n\n* fix typo\n\n* fix typo\n\n* Add upsample_size to AttnUpBlock2D, AttnDownBlock2D (#3275)\n\nThe argument `upsample_size` needs to be added to these modules to allow compatibility with other blocks that require this argument.\n\n* Rename --only_save_embeds to --save_as_full_pipeline (#3206)\n\n* Set --only_save_embeds to False by default\n\nDue to how the option is named, it makes more sense to behave like this.\n\n* Refactor only_save_embeds to save_as_full_pipeline\n\n* [AudioLDM] Generalise conversion script (#3328)\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* Fix TypeError when using prompt_embeds and negative_prompt (#2982)\n\n* test: Added test case\n\n* fix: fixed type checking issue on _encode_prompt\n\n* fix: fixed copies consistency\n\n* fix: one copy was not sufficient\n\n* Fix pipeline class on README (#3345)\n\nUpdate README.md\n\n* Inpainting: typo in docs (#3331)\n\nTypo in docs\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* Add `use_Karras_sigmas` to LMSDiscreteScheduler (#3351)\n\n* add karras sigma to lms discrete scheduler\n\n* add test for lms_scheduler karras\n\n* reformat test lms\n\n* Batched load of textual inversions (#3277)\n\n* Batched load of textual inversions\n\n- Only call resize_token_embeddings once per batch as it is the most expensive operation\n- Allow pretrained_model_name_or_path and token to be an optional list\n- Remove Dict from type annotation pretrained_model_name_or_path as it was not supported in this function\n- Add comment that single files (e.g. .pt/.safetensors) are supported\n- Add comment for token parameter\n- Convert token override log message from warning to info\n\n* Update src/diffusers/loaders.py\n\nCheck for duplicate tokens\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* Update condition for None tokens\n\n---------\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* make fix-copies\n\n* [docs] Fix docstring (#3334)\n\nfix docstring\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* if dreambooth lora (#3360)\n\n* update IF stage I pipelines\n\nadd fixed variance schedulers and lora loading\n\n* added kv lora attn processor\n\n* allow loading into alternative lora attn processor\n\n* make vae optional\n\n* throw away predicted variance\n\n* allow loading into added kv lora layer\n\n* allow load T5\n\n* allow pre compute text embeddings\n\n* set new variance type in schedulers\n\n* fix copies\n\n* refactor all prompt embedding code\n\nclass prompts are now included in pre-encoding code\nmax tokenizer length is now configurable\nembedding attention mask is now configurable\n\n* fix for when variance type is not defined on scheduler\n\n* do not pre compute validation prompt if not present\n\n* add example test for if lora dreambooth\n\n* add check for train text encoder and pre compute text embeddings\n\n* Postprocessing refactor all others (#3337)\n\n* add text2img\n\n* fix-copies\n\n* add\n\n* add all other pipelines\n\n* add\n\n* add\n\n* add\n\n* add\n\n* add\n\n* make style\n\n* style + fix copies\n\n---------\n\nCo-authored-by: yiyixuxu <yixu310@gmail,com>\n\n* [docs] Improve safetensors docstring (#3368)\n\n* clarify safetensor docstring\n\n* fix typo\n\n* apply feedback\n\n* add: a warning message when using xformers in a PT 2.0 env. (#3365)\n\n* add: a warning message when using xformers in a PT 2.0 env.\n\n* Apply suggestions from code review\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n---------\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* StableDiffusionInpaintingPipeline - resize image w.r.t height and width (#3322)\n\n* StableDiffusionInpaintingPipeline now resizes input images and masks w.r.t to passed input height and width. Default is already set to 512. This addresses the common tensor mismatch error. Also moved type check into relevant funciton to keep main pipeline body tidy.\n\n* Fixed StableDiffusionInpaintingPrepareMaskAndMaskedImageTests\n\nDue to previous commit these tests were failing as height and width need to be passed into the prepare_mask_and_masked_image function, I have updated the code and added a height/width variable per unit test as it seemed more appropriate than the current hard coded solution\n\n* Added a resolution test to StableDiffusionInpaintPipelineSlowTests\n\nthis unit test simply gets the input and resizes it into some that would fail (e.g. would throw a tensor mismatch error/not a mult of 8). Then passes it through the pipeline and verifies it produces output with correct dims w.r.t the passed height and width\n\n---------\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* make style\n\n* [docs] Adapt a model (#3326)\n\n* first draft\n\n* apply feedback\n\n* conv_in.weight thrown away\n\n* [docs] Load safetensors (#3333)\n\n* safetensors\n\n* apply feedback\n\n* apply feedback\n\n* Apply suggestions from code review\n\n---------\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* make style\n\n* [Docs] Fix stable_diffusion.mdx typo (#3398)\n\nFix typo in last code block. Correct \"prommpts\" to \"prompt\"\n\n* Support ControlNet v1.1 shuffle properly (#3340)\n\n* add inferring_controlnet_cond_batch\n\n* Revert \"add inferring_controlnet_cond_batch\"\n\nThis reverts commit abe8d6311d4b7f5b9409ca709c7fabf80d06c1a9.\n\n* set guess_mode to True\nwhenever global_pool_conditions is True\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* nit\n\n* add integration test\n\n---------\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* [Tests] better determinism (#3374)\n\n* enable deterministic pytorch and cuda operations.\n\n* disable manual seeding.\n\n* make style && make quality for unet_2d tests.\n\n* enable determinism for the unet2dconditional model.\n\n* add CUBLAS_WORKSPACE_CONFIG for better reproducibility.\n\n* relax tolerance (very weird issue, though).\n\n* revert to torch manual_seed() where needed.\n\n* relax more tolerance.\n\n* better placement of the cuda variable and relax more tolerance.\n\n* enable determinism for 3d condition model.\n\n* relax tolerance.\n\n* add: determinism to alt_diffusion.\n\n* relax tolerance for alt diffusion.\n\n* dance diffusion.\n\n* dance diffusion is flaky.\n\n* test_dict_tuple_outputs_equivalent edit.\n\n* fix two more tests.\n\n* fix more ddim tests.\n\n* fix: argument.\n\n* change to diff in place of difference.\n\n* fix: test_save_load call.\n\n* test_save_load_float16 call.\n\n* fix: expected_max_diff\n\n* fix: paint by example.\n\n* relax tolerance.\n\n* add determinism to 1d unet model.\n\n* torch 2.0 regressions seem to be brutal\n\n* determinism to vae.\n\n* add reason to skipping.\n\n* up tolerance.\n\n* determinism to vq.\n\n* determinism to cuda.\n\n* determinism to the generic test pipeline file.\n\n* refactor general pipelines testing a bit.\n\n* determinism to alt diffusion i2i\n\n* up tolerance for alt diff i2i and audio diff\n\n* up tolerance.\n\n* determinism to audioldm\n\n* increase tolerance for audioldm lms.\n\n* increase tolerance for paint by paint.\n\n* increase tolerance for repaint.\n\n* determinism to cycle diffusion and sd 1.\n\n* relax tol for cycle diffusion \ud83d\udeb2\n\n* relax tol for sd 1.0\n\n* relax tol for controlnet.\n\n* determinism to img var.\n\n* relax tol for img variation.\n\n* tolerance to i2i sd\n\n* make style\n\n* determinism to inpaint.\n\n* relax tolerance for inpaiting.\n\n* determinism for inpainting legacy\n\n* relax tolerance.\n\n* determinism to instruct pix2pix\n\n* determinism to model editing.\n\n* model editing tolerance.\n\n* panorama determinism\n\n* determinism to pix2pix zero.\n\n* determinism to sag.\n\n* sd 2. determinism\n\n* sd. tolerance\n\n* disallow tf32 matmul.\n\n* relax tolerance is all you need.\n\n* make style and determinism to sd 2 depth\n\n* relax tolerance for depth.\n\n* tolerance to diffedit.\n\n* tolerance to sd 2 inpaint.\n\n* up tolerance.\n\n* determinism in upscaling.\n\n* tolerance in upscaler.\n\n* more tolerance relaxation.\n\n* determinism to v pred.\n\n* up tol for v_pred\n\n* unclip determinism\n\n* determinism to unclip img2img\n\n* determinism to text to video.\n\n* determinism to last set of tests\n\n* up tol.\n\n* vq cumsum doesn't have a deterministic kernel\n\n* relax tol\n\n* relax tol\n\n* [docs] Add transformers to install (#3388)\n\nadd transformers to install\n\n* [deepspeed] partial ZeRO-3 support (#3076)\n\n* [deepspeed] partial ZeRO-3 support\n\n* cleanup\n\n* improve deepspeed fixes\n\n* Improve\n\n* make style\n\n---------\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* Add omegaconf for tests (#3400)\n\nAdd omegaconfg\n\n* Fix various bugs with LoRA Dreambooth and Dreambooth script (#3353)\n\n* Improve checkpointing lora\n\n* fix more\n\n* Improve doc string\n\n* Update src/diffusers/loaders.py\n\n* make stytle\n\n* Apply suggestions from code review\n\n* Update src/diffusers/loaders.py\n\n* Apply suggestions from code review\n\n* Apply suggestions from code review\n\n* better\n\n* Fix all\n\n* Fix multi-GPU dreambooth\n\n* Apply suggestions from code review\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* Fix all\n\n* make style\n\n* make style\n\n---------\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* Fix docker file (#3402)\n\n* up\n\n* up\n\n* fix: deepseepd_plugin retrieval from accelerate state (#3410)\n\n* [Docs] Add `sigmoid` beta_scheduler to docstrings of relevant Schedulers (#3399)\n\n* Add `sigmoid` beta scheduler to `DDPMScheduler` docstring\n\n* Add `sigmoid` beta scheduler to `RePaintScheduler` docstring\n\n---------\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* Don't install accelerate and transformers from source (#3415)\n\n* Don't install transformers and accelerate from source (#3414)\n\n* Improve fast tests (#3416)\n\nUpdate pr_tests.yml\n\n* attention refactor: the trilogy  (#3387)\n\n* Replace `AttentionBlock` with `Attention`\n\n* use _from_deprecated_attn_block check re: @patrickvonplaten\n\n* [Docs] update the PT 2.0 optimization doc with latest findings (#3370)\n\n* add: benchmarking stats for A100 and V100.\n\n* Apply suggestions from code review\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* address patrick's comments.\n\n* add: rtx 4090 stats\n\n* \u2694 benchmark reports done\n\n* Apply suggestions from code review\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* 3313 pr link.\n\n* add: plots.\n\nCo-authored-by: Pedro <pedro@huggingface.co>\n\n* fix formattimg\n\n* update number percent.\n\n---------\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* Fix style rendering (#3433)\n\n* Fix style rendering.\n\n* Fix typo\n\n* unCLIP scheduler do not use note (#3417)\n\n* Replace deprecated command with environment file (#3409)\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix warning message pipeline loading (#3446)\n\n* add stable diffusion tensorrt img2img pipeline (#3419)\n\n* add stable diffusion tensorrt img2img pipeline\n\nSigned-off-by: Asfiya Baig <asfiyab@nvidia.com>\n\n* update docstrings\n\nSigned-off-by: Asfiya Baig <asfiyab@nvidia.com>\n\n---------\n\nSigned-off-by: Asfiya Baig <asfiyab@nvidia.com>\n\n* Refactor controlnet and add img2img and inpaint (#3386)\n\n* refactor controlnet and add img2img and inpaint\n\n* First draft to get pipelines to work\n\n* make style\n\n* Fix more\n\n* Fix more\n\n* More tests\n\n* Fix more\n\n* Make inpainting work\n\n* make style and more tests\n\n* Apply suggestions from code review\n\n* up\n\n* make style\n\n* Fix imports\n\n* Fix more\n\n* Fix more\n\n* Improve examples\n\n* add test\n\n* Make sure import is correctly deprecated\n\n* Make sure everything works in compile mode\n\n* make sure authorship is correctly attributed\n\n* [Scheduler] DPM-Solver (++) Inverse Scheduler (#3335)\n\n* Add DPM-Solver Multistep Inverse Scheduler\n\n* Add draft tests for DiffEdit\n\n* Add inverse sde-dpmsolver steps to tune image diversity from inverted latents\n\n* Fix tests\n\n---------\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* [Docs] Fix incomplete docstring for resnet.py (#3438)\n\nFix incomplete docstrings for resnet.py\n\n* fix tiled vae blend extent range (#3384)\n\nfix tiled vae bleand extent range\n\n* Small update to \"Next steps\" section (#3443)\n\nSmall update to \"Next steps\" section:\n\n- PyTorch 2 is recommended.\n- Updated improvement figures.\n\n* Allow arbitrary aspect ratio in IFSuperResolutionPipeline (#3298)\n\n* Update pipeline_if_superresolution.py\n\nAllow arbitrary aspect ratio in IFSuperResolutionPipeline by using the input image shape\n\n* IFSuperResolutionPipeline: allow the user to override the height and width through the arguments\n\n* update IFSuperResolutionPipeline width/height doc string to match StableDiffusionInpaintPipeline conventions\n\n---------\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* Adding 'strength' parameter to StableDiffusionInpaintingPipeline  (#3424)\n\n* Added explanation of 'strength' parameter\n\n* Added get_timesteps function which relies on new strength parameter\n\n* Added `strength` parameter which defaults to 1.\n\n* Swapped ordering so `noise_timestep` can be calculated before masking the image\n\nthis is required when you aren't applying 100% noise to the masked region, e.g. strength < 1.\n\n* Added strength to check_inputs, throws error if out of range\n\n* Changed `prepare_latents` to initialise latents w.r.t strength\n\ninspired from the stable diffusion img2img pipeline, init latents are initialised by converting the init image into a VAE latent and adding noise (based upon the strength parameter passed in), e.g. random when strength = 1, or the init image at strength = 0.\n\n* WIP: Added a unit test for the new strength parameter in the StableDiffusionInpaintingPipeline\n\nstill need to add correct regression values\n\n* Created a is_strength_max to initialise from pure random noise\n\n* Updated unit tests w.r.t new strength parameter + fixed new strength unit test\n\n* renamed parameter to avoid confusion with variable of same name\n\n* Updated regression values for new strength test - now passes\n\n* removed 'copied from' comment as this method is now different and divergent from the cpy\n\n* Update src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* Ensure backwards compatibility for prepare_mask_and_masked_image\n\ncreated a return_image boolean and initialised to false\n\n* Ensure backwards compatibility for prepare_latents\n\n* Fixed copy check typo\n\n* Fixes w.r.t backward compibility changes\n\n* make style\n\n* keep function argument ordering same for backwards compatibility in callees with copied from statements\n\n* make fix-copies\n\n---------\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: William Berman <WLBberman@gmail.com>\n\n* [WIP] Bugfix - Pipeline.from_pretrained is broken when the pipeline is partially downloaded (#3448)\n\nAdded bugfix using f strings.\n\n* Fix gradient checkpointing bugs in freezing part of models (requires_grad=False) (#3404)\n\n* gradient checkpointing bug fix\n\n* bug fix; changes for reviews\n\n* reformat\n\n* reformat\n\n---------\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* Make dreambooth lora more robust to orig unet (#3462)\n\n* Make dreambooth lora more robust to orig unet\n\n* up\n\n* Reduce peak VRAM by releasing large attention tensors (as soon as they're unnecessary) (#3463)\n\nRelease large tensors in attention (as soon as they're no longer required). Reduces peak VRAM by nearly 2 GB for 1024x1024 (even after slicing), and the savings scale up with image size.\n\n* Add min snr to text2img lora training script (#3459)\n\nadd min snr to text2img lora training script\n\n* Add inpaint lora scale support (#3460)\n\n* add inpaint lora scale support\n\n* add inpaint lora scale test\n\n---------\n\nCo-authored-by: yueyang.hyy <yueyang.hyy@alibaba-inc.com>\n\n* [From ckpt] Fix from_ckpt (#3466)\n\n* Correct from_ckpt\n\n* make style\n\n* Update full dreambooth script to work with IF (#3425)\n\n* Add IF dreambooth docs (#3470)\n\n* parameterize pass single args through tuple (#3477)\n\n* attend and excite tests disable determinism on the class level (#3478)\n\n* dreambooth docs torch.compile note (#3471)\n\n* dreambooth docs torch.compile note\n\n* Update examples/dreambooth/README.md\n\nCo-authored-by: Sayak Paul <spsayakpaul@gmail.com>\n\n* Update examples/dreambooth/README.md\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n---------\n\nCo-authored-by: Sayak Paul <spsayakpaul@gmail.com>\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* add: if entry in the dreambooth training docs. (#3472)\n\n* [docs] Textual inversion inference (#3473)\n\n* add textual inversion inference to docs\n\n* add to toctree\n\n---------\n\nCo-authored-by: Sayak Paul <spsayakpaul@gmail.com>\n\n* [docs] Distributed inference (#3376)\n\n* distributed inference\n\n* move to inference section\n\n* apply feedback\n\n* update with split_between_processes\n\n* apply feedback\n\n* [{Up,Down}sample1d] explicit view kernel size as number elements in flattened indices (#3479)\n\nexplicit view kernel size as number elements in flattened indices\n\n* mps & onnx tests rework (#3449)\n\n* Remove ONNX tests from PR.\n\nThey are already a part of push_tests.yml.\n\n* Remove mps tests from PRs.\n\nThey are already performed on push.\n\n* Fix workflow name for fast push tests.\n\n* Extract mps tests to a workflow.\n\nFor better control/filtering.\n\n* Remove --extra-index-url from mps tests\n\n* Increase tolerance of mps test\n\nThis test passes in my Mac (Ventura 13.3) but fails in the CI hardware\n(Ventura 13.2). I ran the local tests following the same steps that\nexist in the CI workflow.\n\n* Temporarily run mps tests on pr\n\nSo we can test.\n\n* Revert \"Temporarily run mps tests on pr\"\n\nTests passed, go back to running on push.\n\n---------\n\nSigned-off-by: Asfiya Baig <asfiyab@nvidia.com>\nCo-authored-by: Ilia Larchenko <41329713+IliaLarchenko@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: YiYi Xu <yixu310@gmail.com>\nCo-authored-by: yiyixuxu <yixu@yis-macbook-pro.lan>\nCo-authored-by: Sayak Paul <spsayakpaul@gmail.com>\nCo-authored-by: Horace He <horacehe2007@yahoo.com>\nCo-authored-by: Umar <55330742+mu94-csl@users.noreply.github.com>\nCo-authored-by: Mylo <36931363+gitmylo@users.noreply.github.com>\nCo-authored-by: Markus Pobitzer <markuspobitzer@gmail.com>\nCo-authored-by: Cheng Lu <lucheng.lc15@gmail.com>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\nCo-authored-by: Isamu Isozaki <isamu.website@gmail.com>\nCo-authored-by: Cesar Aybar <csaybar@gmail.com>\nCo-authored-by: Will Rice <will@spokestack.io>\nCo-authored-by: Adri\u00e0 Arrufat <1671644+arrufat@users.noreply.github.com>\nCo-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>\nCo-authored-by: At-sushi <dkahw210@kyoto.zaq.ne.jp>\nCo-authored-by: Lucca Zen\u00f3bio <luccazen@gmail.com>\nCo-authored-by: Lysandre Debut <lysandre@huggingface.co>\nCo-authored-by: Isotr0py <41363108+Isotr0py@users.noreply.github.com>\nCo-authored-by: pdoane <pdoane2@gmail.com>\nCo-authored-by: Will Berman <wlbberman@gmail.com>\nCo-authored-by: yiyixuxu <yixu310@gmail,com>\nCo-authored-by: Rupert Menneer <71332436+rupertmenneer@users.noreply.github.com>\nCo-authored-by: sudowind <wfpkueecs@163.com>\nCo-authored-by: Takuma Mori <takuma104@gmail.com>\nCo-authored-by: Stas Bekman <stas00@users.noreply.github.com>\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\nCo-authored-by: Laure\u03b7t <laurentfainsin@protonmail.com>\nCo-authored-by: Jongwoo Han <jongwooo.han@gmail.com>\nCo-authored-by: asfiyab-nvidia <117682710+asfiyab-nvidia@users.noreply.github.com>\nCo-authored-by: clarencechen <clarencechenct@gmail.com>\nCo-authored-by: Laure\u03b7t <laurent@fainsin.bzh>\nCo-authored-by: superlabs-dev <133080491+superlabs-dev@users.noreply.github.com>\nCo-authored-by: Dev Aggarwal <devxpy@gmail.com>\nCo-authored-by: Vimarsh Chaturvedi <vimarsh.c@gmail.com>\nCo-authored-by: 7eu7d7 <31194890+7eu7d7@users.noreply.github.com>\nCo-authored-by: cmdr2 <shashank.shekhar.global@gmail.com>\nCo-authored-by: wfng92 <43742196+wfng92@users.noreply.github.com>\nCo-authored-by: Glaceon-Hyy <ffheyy0017@gmail.com>\nCo-authored-by: yueyang.hyy <yueyang.hyy@alibaba-inc.com>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-05-17T10:05:33Z",
        "message": "[WIP] Bugfix - Pipeline.from_pretrained is broken when the pipeline is partially downloaded (#3448)\n\nAdded bugfix using f strings."
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-05-16T11:58:24Z",
        "message": "fix warning message pipeline loading (#3446)"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-05-09T23:15:05Z",
        "message": "[docs] Improve safetensors docstring (#3368)\n\n* clarify safetensor docstring\n\n* fix typo\n\n* apply feedback"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-05-08T19:08:23Z",
        "message": "[docs] Fix docstring (#3334)\n\nfix docstring\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-04-26T16:43:09Z",
        "message": "Post release for 0.16.0 (#3244)\n\n* Post release\n\n* fix more"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-04-25T21:20:43Z",
        "message": "add model (#3230)\n\n* add\n\n* clean\n\n* up\n\n* clean up more\n\n* fix more tests\n\n* Improve docs further\n\n* improve\n\n* more fixes docs\n\n* Improve docs more\n\n* Update src/diffusers/models/unet_2d_condition.py\n\n* fix\n\n* up\n\n* update doc links\n\n* make fix-copies\n\n* add safety checker and watermarker to stage 3 doc page code snippets\n\n* speed optimizations docs\n\n* memory optimization docs\n\n* make style\n\n* add watermarking snippets to doc string examples\n\n* make style\n\n* use pt_to_pil helper functions in doc strings\n\n* skip mps tests\n\n* Improve safety\n\n* make style\n\n* new logic\n\n* fix\n\n* fix bad onnx design\n\n* make new stable diffusion upscale pipeline model arguments optional\n\n* define has_nsfw_concept when non-pil output type\n\n* lowercase linked to notebook name\n\n---------\n\nCo-authored-by: William Berman <WLBberman@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-04-17T16:16:28Z",
        "message": "Fix config deprecation (#3129)\n\n* Better deprecation message\n\n* Better deprecation message\n\n* Better doc string\n\n* Fixes\n\n* fix more\n\n* fix more\n\n* Improve __getattr__\n\n* correct more\n\n* fix more\n\n* fix\n\n* Improve more\n\n* more improvements\n\n* fix more\n\n* Apply suggestions from code review\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* make style\n\n* Fix all rest & add tests & remove old deprecation fns\n\n---------\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-04-17T15:19:11Z",
        "message": "Improve deprecation warnings (#3131)"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-04-13T12:33:11Z",
        "message": "Throw deprecation warning for return_cached_folder (#3092)\n\nThrow deprecation warning"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-04-12T22:25:10Z",
        "message": "[Pipelines] Make sure that None functions are correctly not saved (#3080)"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-04-12T11:11:09Z",
        "message": "fix pipeline __setattr__ value == None (#3063)\n\n* fix pipeline __setattr__\n\n* add test\n\n---------\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-04-11T11:35:42Z",
        "message": "Fix config prints and save, load of pipelines (#2849)\n\n* [Config] Fix config prints and save, load\n\n* Only use potential nn.Modules for dtype and device\n\n* Correct vae image processor\n\n* make sure in_channels is not accessed directly\n\n* make sure in channels is only accessed via config\n\n* Make sure schedulers only access config attributes\n\n* Make sure to access config in SAG\n\n* Fix vae processor and make style\n\n* add tests\n\n* uP\n\n* make style\n\n* Fix more naming issues\n\n* Final fix with vae config\n\n* change more"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-04-05T23:31:09Z",
        "message": "[Pipeline download] Improve pipeline download for index and passed co\u2026 (#2980)\n\n* [Pipeline download] Improve pipeline download for index and passed components\n\n* correct\n\n* add more tests\n\n* up"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-03-28T07:03:21Z",
        "message": "Make dynamo wrapped modules work with save_pretrained  (#2726)\n\n* Workaround for saving dynamo-wrapped models.\n\n* Accept suggestion from code review\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* Apply workaround when overriding pipeline components.\n\n* Ensure the correct config.json is saved to disk.\n\nInstead of the dynamo class.\n\n* Save correct module (not compiled one)\n\n* Add test\n\n* style\n\n* fix docstrings\n\n* Go back to using string comparisons.\n\nPyTorch CPU does not have _dynamo.\n\n* Simple test for save_pretrained of compiled models.\n\n* Helper function to test whether module is compiled.\n\n---------\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-03-27T14:18:57Z",
        "message": "Ruff: apply same rules as in transformers (#2827)\n\n* Apply same ruff settings as in transformers\n\nSee https://github.com/huggingface/transformers/blob/main/pyproject.toml\nCo-authored-by: Aaron Gokaslan <aaronGokaslan@gmail.com>\n\n* Apply new style rules\n\n* Style\n\nCo-authored-by: Aaron Gokaslan <aaronGokaslan@gmail.com>\n\n* style\n\n* remove list, ruff wouldn't auto fix.\n\n---------\n\nCo-authored-by: Aaron Gokaslan <aaronGokaslan@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-03-21T14:21:23Z",
        "message": "Add option to set dtype in pipeline.to() method (#2317)\n\nadd test_to_dtype to check pipe.to(fp16)"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-03-21T12:45:04Z",
        "message": "Fix typos (#2715)\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-03-16T14:57:43Z",
        "message": "Adding `use_safetensors` argument to give more control to users (#2123)\n\n* Adding `use_safetensors` argument to give more control to users\n\nabout which weights they use.\n\n* Doc style.\n\n* Rebased (not functional).\n\n* Rebased and functional with tests.\n\n* Style.\n\n* Apply suggestions from code review\n\n* Style.\n\n* Addressing comments.\n\n* Update tests/test_pipelines.py\n\nCo-authored-by: Will Berman <wlbberman@gmail.com>\n\n* Black ???\n\n---------\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: Will Berman <wlbberman@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-03-10T20:01:59Z",
        "message": "[Pipeline loading] Remove send_telemetry (#2640)\n\n* [Pipeline loading]\n\n* up"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-03-10T10:56:10Z",
        "message": "[From pretrained] Speed-up loading from cache (#2515)\n\n* [From pretrained] Speed-up loading from cache\n\n* up\n\n* Fix more\n\n* fix one more bug\n\n* make style\n\n* bigger refactor\n\n* factor out function\n\n* Improve more\n\n* better\n\n* deprecate return cache folder\n\n* clean up\n\n* improve tests\n\n* up\n\n* upload\n\n* add nice tests\n\n* simplify\n\n* finish\n\n* correct\n\n* fix version\n\n* rename\n\n* Apply suggestions from code review\n\nCo-authored-by: Lucain <lucainp@gmail.com>\n\n* rename\n\n* correct doc string\n\n* correct more\n\n* Apply suggestions from code review\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* apply code suggestions\n\n* finish\n\n---------\n\nCo-authored-by: Lucain <lucainp@gmail.com>\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-03-09T15:13:55Z",
        "message": "Up vesion at which we deprecate \"revision='fp16'\" since `transformers` is not released yet (#2623)\n\n* improve error message\n\n* upload"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-03-09T13:00:36Z",
        "message": "Add cache_dir to docs (#2624)\n\nImprove docs"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-03-03T15:44:10Z",
        "message": "[Model offload] Add nice warning (#2543)\n\n* [Model offload] Add nice warning\n\n* Treat sequential and model offload differently.\n\nSequential raises an error because the operation would fail with a\ncryptic warning later.\n\n* Forcibly move to cpu when offloading.\n\n* make style\n\n* one more fix\n\n* make fix-copies\n\n* up\n\n---------\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-03-03T15:08:56Z",
        "message": "Fix ONNX checkpoint loading (#2544)\n\n* Revert \"Disable ONNX tests (#2509)\"\n\nThis reverts commit a0549fea4469251a8021d20d6550cf061a1cdb84.\n\n* add external weights\n\n* + pb\n\n* style"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-03-01T09:31:00Z",
        "message": "[Copyright] 2023 (#2524)"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-03-01T04:37:42Z",
        "message": "is_safetensors_compatible refactor (#2499)\n\n* is_safetensors_compatible refactor\n\n* files list comma"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-02-20T07:38:30Z",
        "message": "fix transformers naming (#2430)"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-02-20T07:34:30Z",
        "message": "make style"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-02-20T07:34:13Z",
        "message": "Update pipeline_utils.py (#2415)"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-02-19T22:13:03Z",
        "message": "Fix deprecation warning (#2426)\n\nDeprecation warning should only hit at version 0.15"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-02-17T15:27:51Z",
        "message": "Add ddim inversion pix2pix (#2397)\n\n* add\n\n* finish\n\n* add tests\n\n* add tests\n\n* up\n\n* up\n\n* pull from main\n\n* uP\n\n* Apply suggestions from code review\n\n* finish\n\n* Update docs/source/en/_toctree.yml\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* finish\n\n* clean docs\n\n* next\n\n* next\n\n* Apply suggestions from code review\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* up\n\n* up\n\n---------\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-02-16T10:02:58Z",
        "message": "[Variant] Add \"variant\" as input kwarg so to have better UX when downloading no_ema or fp16 weights (#2305)\n\n* [Variant] Add variant loading mechanism\n\n* clean\n\n* improve further\n\n* up\n\n* add tests\n\n* add some first tests\n\n* up\n\n* up\n\n* use path splittetx\n\n* add deprecate\n\n* deprecation warnings\n\n* improve docs\n\n* up\n\n* up\n\n* up\n\n* fix tests\n\n* Apply suggestions from code review\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* Apply suggestions from code review\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* correct code format\n\n* fix warning\n\n* finish\n\n* Apply suggestions from code review\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* Update docs/source/en/using-diffusers/loading.mdx\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Will Berman <wlbberman@gmail.com>\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* correct loading docs\n\n* finish\n\n---------\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\nCo-authored-by: Will Berman <wlbberman@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-02-07T22:46:23Z",
        "message": "Replace flake8 with ruff and update black (#2279)\n\n* before running make style\n\n* remove left overs from flake8\n\n* finish\n\n* make fix-copies\n\n* final fix\n\n* more fixes"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-02-04T19:55:11Z",
        "message": "Show error when loading safety_checker `from_flax` (#2187)\n\n* Show error when loading safety_checker `from_flax`\n\n* fix style"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-01-27T16:23:55Z",
        "message": "[from_pretrained] only load config one time (#2131)"
    },
    {
        "repo_url": "github.com/huggingface/diffusers",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-01-27T07:42:33Z",
        "message": "Don't call the Hub if `local_files_only` is specifiied (#2119)\n\nDon't call the Hub if"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-04-09T21:07:59Z",
        "message": "Update export_hf_checkpoint.py (#302)\n\n* Update export_hf_checkpoint.py\n\n* Update finetune.py\n\nNew tokenizer base model for the current dev branch of transformers\n\n* Update generate.py\n\n* Update export_state_dict_checkpoint.py\n\n* Update export_hf_checkpoint.py"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-04-04T15:05:20Z",
        "message": "Support streaming output on generate (#263)"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-30T15:57:40Z",
        "message": "Fix server_name"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-30T15:57:40Z",
        "message": "Added Dockerfile and docker-compose.yml (#207)\n\n* Added Dockerfile for inference\n\n* Added instructions for Dockerfile\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Pass env through Dockerfile\n\n* Added docker compose setup and instructions\n\n* Added more environment options\n\n* Set a safer default mount point\n\n* add docker-compose changes\n\n* Added Dockerfile for inference\n\n* Added instructions for Dockerfile\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Pass env through Dockerfile\n\n* Added docker compose setup and instructions\n\n* Added more environment options\n\n* Set a safer default mount point\n\n* add to gitignore, update to new generate.py\n\n* add docker ignore, simplify docker compose file\n\n* add back missing requirements\n\n* Adjustments to compose and generate.py, added Docker to README.md\n\n* Linting adjust to Black\n\n* Adjusting import linting\n\n* Update README.md\n\n* Update README.md\n\n* Removed comment by original Dockerfile creator.\n\nComment not necessary.\n\n* cleanup README\n\nCo-authored-by: Francesco Saverio Zuppichini <zuppif@usi.ch>\n\n---------\n\nCo-authored-by: Francesco Saverio Zuppichini <zuppif@usi.ch>\nCo-authored-by: Chris Alexiuk <c.s.alexiuk@gmail.com>\nCo-authored-by: ElRoberto538 <>\nCo-authored-by: Sam Sipe <samsipe@gmail.com>\nCo-authored-by: Eric J. Wang <eric.james.wang@gmail.com>"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-29T23:36:04Z",
        "message": "Templated prompter (#184)\n\n* Templated prompter\n\n* fix dup import\n\n* Set Verbose False by default\n\nI forgot to disable after testing.\n\n* Fix imports order\n\n* Use Black Formatting\n\n* lint\n\n* Re-introduce lost line\n\n* Cleanup\n\n* template default\n\n* isort\n\n---------\n\nCo-authored-by: Eric Wang <eric.james.wang@gmail.com>"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-28T16:43:29Z",
        "message": "Add option to share Gradio demo publicly (#189)\n\n* Add option to share Gradio demo publicly\n\n* gradio_share -> share_gradio\n\n---------\n\nCo-authored-by: Eric Wang <eric.james.wang@gmail.com>"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-28T15:33:47Z",
        "message": "remove asserts"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-27T17:31:44Z",
        "message": "Add HF dataset loading, add linters, pyproject.toml (#175)\n\n* add HF dataset loading, add linters, pyproject.toml\n\n- applied markdownlint\n- add black, black[jupyter], isort\n- fix noqa codes\n- add .github workflow linting\n- update README.md\n\n* restore default settings\n\n* resume_from_checkpoint\n\nCo-authored-by: AngainorDev <54739135+AngainorDev@users.noreply.github.com>\n\n* Print warning on checkpoint not found\n\n* add HF dataset loading, add linters, pyproject.toml\n\n- applied markdownlint\n- add black, black[jupyter], isort\n- fix noqa codes\n- add .github workflow linting\n- update README.md\n\n* Default to local copy and update it\n\n* Typo\n\n* Remove duplicate code block\n\n---------\n\nCo-authored-by: Eric Wang <eric.james.wang@gmail.com>\nCo-authored-by: AngainorDev <54739135+AngainorDev@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-24T21:18:42Z",
        "message": "Use CLI arguments (#159)\n\n* CLI args for finetune\n\n* Update README\n\n* CLI args for generate.py\n\n* reqs.txt\n\n* reorder hyperparams\n\n* lora_target_modules\n\n* cleanup"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-23T20:54:39Z",
        "message": "Remove LLaMA download code, as a precaution"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-23T20:44:45Z",
        "message": "bos, eos in generate.py"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-21T21:31:30Z",
        "message": "fix fp16 inference"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-19T22:53:21Z",
        "message": "slider for tokens generated"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-19T18:22:02Z",
        "message": "Remove messy test code"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-19T06:00:18Z",
        "message": "generate.py tweaks"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-18T23:43:53Z",
        "message": "don't share publicly"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-17T22:07:08Z",
        "message": "min beams = 1"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-17T20:53:21Z",
        "message": "Enable inference on CPU and Mac GPU using pytorch support for MPS (#48)"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-17T02:30:27Z",
        "message": "Update generate.py\n\nAdapting to the input function, a text box for inputting content has been added."
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-16T23:04:06Z",
        "message": "add Gradio interface to generate.py"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-16T19:11:47Z",
        "message": "Catch outdated installs"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-16T19:11:29Z",
        "message": "Update alpaca-lora to use transformers main branch"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-16T16:59:10Z",
        "message": "Expand sampling in generate.py for new test"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-16T07:05:32Z",
        "message": "Add counting test"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-16T00:22:22Z",
        "message": "generate.py memory, perf updates"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-15T18:11:26Z",
        "message": "torch.no_grad"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-15T04:41:02Z",
        "message": "add text-davinci-003 to comparisons"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-15T04:33:12Z",
        "message": "Update README.md with new checkpoint details"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-14T22:10:33Z",
        "message": "Ready to go"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-14T00:23:29Z",
        "message": "decapoda"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-13T22:00:05Z",
        "message": "Licenses and whatnot"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2024-02-23T09:54:10Z",
        "message": "FIX Bug in prompt learning after disabling adapter (#1502)\n\nThere was a big that after using the disable_adapter context, the\nprepare method was not correctly restored, meaning that generations were\nincorrect once the context was exited. This is now fixed."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2024-02-19T12:53:39Z",
        "message": "FIX: Multitask prompt tuning with other tuning init (#1144)\n\nResolves #1082.\n\nAlso, adding tests for prompt_tuning_init != RANDOM.\n\n---------\n\nCo-authored-by: Mayank Mishra <32954280+mayank31398@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2024-02-19T12:33:24Z",
        "message": "FIX [`PromptTuning`] Simple fix for transformers >= 4.38 (#1484)\n\n* fix for transformers >= 4.38\n\n* style"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2024-02-08T13:39:46Z",
        "message": "DOC How to freeze adapter after set_adapter call (#1447)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2024-02-07T11:52:35Z",
        "message": "MNT Move code quality fully to ruff (#1421)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2024-02-06T15:33:15Z",
        "message": "FIX Saving models that don't have _name_or_path in config (#1440)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2024-02-06T00:54:06Z",
        "message": "Fix typos (#1435)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2024-01-30T11:32:39Z",
        "message": "Add positional args to PeftModelForCausalLM.generate (#1393)\n\n* add positional args\n\n* update tests"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2024-01-12T16:19:12Z",
        "message": "FEAT Add Poly Adapter (#1129)\n\nImplement the Poly (Polytropon) adapter.\n\nPapers:\n\n- https://arxiv.org/abs/2202.13914\n- https://arxiv.org/abs/2211.03831\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2024-01-12T10:54:53Z",
        "message": "New transformers caching ETA now v4.38 (#1348)\n\nSee #1252 and #1352 for more context.\n\nThe initial idea was for transformers 4.37 to add the new caching to all\narchitectures, but this was postponed to 4.38. The code needs to be\nadapted for prompt tuning not to break when transformers 4.37 is\nreleased."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2024-01-12T10:48:42Z",
        "message": "fix `prepare_inputs_for_generation` logic for Prompt Learning methods (#1352)\n\n* fix `prepare_inputs_for_generation` logic for Prompt Learning methods\n\n* \ud83d\ude05"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-12-12T14:16:00Z",
        "message": "FIX Issues with transformers 4.36 (#1252)\n\nAdjust for different type of past_key_values when using caching.\n\nAlso: Fix some seeds for flaky tests.\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-12-07T09:56:21Z",
        "message": "Don't set config attribute on custom models (#1200)\n\nInitially, we had the issue that it was sometimes assumed that models\nhad a config attribute, as is given for transformers models. This made\nPEFT fail with custom models, so we made a change to set a dummy config\non those.\n\nHowever, this can lead to issues down the line. For example, when users\nuse the Trainer class from transformers, they can stumble upon lines\nlike this:\n\nhttps://github.com/huggingface/transformers/blob/62ab32b2997846526cdffd629c72c47bc7b4f215/src/transformers/integrations/integration_utils.py#L636-L637\n\nhttps://github.com/huggingface/transformers/blob/62ab32b2997846526cdffd629c72c47bc7b4f215/src/transformers/integrations/integration_utils.py#L729-L730\n\nHere transformers assumes that if config attribute exists on the model,\nit must have a to_json_string method or a to_dict method (as it assumes\nthe config to be a PretrainedConfig instance). Therefore, in order not\nto trip up transformers, it is best not to set any config at all.\n\nAlternative\n\nAlternatively, transformers could be changed to check each time when the\nconfig attributes exists, if it is a PretrainedConfig instance, but that\nwould be a much larger change (albeit a cleaner one)."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-12-04T11:22:03Z",
        "message": "DOC: Update & improve docstrings and type annotations for common methods and classes (#1201)\n\nThe docstrings of the most user-exposed methods and classes have been\nupdated, or added if not already present. Furthermore, type annotations\nhave been updated or added for those methods and classes.\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-11-30T15:58:42Z",
        "message": "[Feature] Support OFT (#1160)\n\n* Support OFT\n\n* add test\n\n* Update README\n\n* fix code quality\n\n* fix test\n\n* Skip 1 test\n\n* fix eps rule and add more test\n\n* feat: added examples to new OFT method\n\n* fix: removed wrong arguments from model example\n\n* fix: changed name of inference file\n\n* fix: changed prompt variable\n\n* fix docs\n\n* fix: dreambooth inference revision based on feedback\n\n* fix: review from BenjaminBossan\n\n* apply safe merge\n\n* del partially\n\n* refactor oft\n\n* refactor oft\n\n* del unused line\n\n* del unused line\n\n* fix skip in windows\n\n* skip test\n\n* Add comments about bias added place\n\n* rename orig_weights to new_weights\n\n* use inverse instead of linalg.inv\n\n* delete alpha and scaling\n\n---------\n\nCo-authored-by: Lukas Kuhn <lukaskuhn.lku@gmail.com>\nCo-authored-by: Lukas Kuhn <lukas.kuhn@deutschebahn.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-11-29T13:58:41Z",
        "message": "Training PEFT models with new tokens being added to the embedding layers and tokenizer (#1147)\n\n* add support for saving base layers weights along with adapter weights\n\n* Update save_and_load.py\n\n* Add an example showing the usage of the added feature\n\n* refactor the functionality\n\n* fix\n\n* refactoring code\n\n1. Add `is_embedding_layer_resized` parameter to `save_pretrained`\n2. Fix the deduplication in README when adding PEFT details.\n3. `save_pretrained` should only save the model when `is_main_process=True` which is one of the parameters of `save_pretrained`.\n\n* update example\n\n* fix the model card\n\n* fix model card\n\n* \ud83d\ude05\n\n* fix model card\n\n* automate setting `is_embedding_layer_resized`\n\n* nits\n\n* Update peft_lora_clm_with_additional_tokens.ipynb\n\n* add test\n\n* fix tests\n\n* maybe fixes the issue?\n\n* address comments\n\nCo-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-11-28T13:17:25Z",
        "message": "FIX Pass HF token when calling PeftModel.from_pretrained (#1076)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-11-15T10:21:23Z",
        "message": "FEAT: Make safe serialization the default one (#1088)\n\n* make safe serialization the default one\n\n* adapt tests\n\n* fix final tests'\n\n* adapt from suggestion"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-11-14T09:30:52Z",
        "message": "FIX: Adding 2 adapters when target_modules is a str fails (#1111)\n\n* Fix adding 2 adapters when target_modules is a str\n\nProblem description\n\nAdding two adapters (e.g. LoRA) when using a list for `target_mdules`\nworks but passing a str fails. The issue is that for str, we do a\n`re.fullmatch`, whereas for list, we just check `endswith`. After adding\nthe first adapter, though, the naming pattern of the modules changes. In\nthe example above, the name for the linear layer changes from `\"lin0\"`\nto `\"base_model.model.lin0\"`, which is why the `fullmatch` fails but the\n`endswith` still works.\n\nReproduction\n\nfrom peft import LoraConfig, get_peft_model\nfrom torch import nn\n\nclass MLP(nn.Module):\n    def __init__(self, bias=True):\n        super().__init__()\n        self.lin0 = nn.Linear(10, 20, bias=bias)\n\ndef test_target_modules_list():\n    config = LoraConfig(target_modules=[\"lin0\"])\n    test_it(config)\n    print(\"Adding two adapters with target_module being a list works\")\n\ndef test_target_modules_str():\n    config = LoraConfig(target_modules=\"lin0\")\n    test_it(config)\n\ndef test_it(config):\n    model = MLP()\n    model = get_peft_model(model, config, \"adapter0\")\n    model.add_adapter(\"adapter1\", config)\n    print(\"Adding two adapters with target_module being a str works\")\n\nif __name__ == \"__main__\":\n    # works\n    test_target_modules_list()\n    # ValueError: Target modules lin0 not found in the base model\n    test_target_modules_str()\n\nI think that most users would be surprised that:\n\n1. Adding the first adapter works but adding the second fails, even\n   though they use the same config.\n2. Using `target_modules=[\"lin0\"]` works but `target_modules=\"lin0\"`\n   fails for the 2nd adapter.\n\nSolution\n\nWe could change the logic of not using `re.fullmatch` for str, but I\nthink that could be tricky to achieve without breaking BC. Instead, I\nchose to change the inject_adapter call in add_adapter to pass the base\nmodel, not the whole peft model. This way, the naming pattern is\npreserved.\n\nTests\n\nI haven't added extra tests for this. The script above could serve as a\ntest. However, it will be sufficient to remove the guard added in #1105:\n\n    if isinstance(config.target_str, modules):\n        # TODO this should be doable\n        self.skipTest(\"Multiple adapters cannot currently be added when target_modules is a string.\")\n\nas that will test exactly this behavior and was how the bug was\noriginally uncovered. Depending on what PR lands first, the guard has to\nremoved in this PR or in #1105.\n\n* Enable tests for adding 2 adapters with str"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-11-10T17:37:38Z",
        "message": "fix import issue transformers (#1116)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-11-09T13:50:35Z",
        "message": "[`core`] Fix safetensors serialization for shared tensors (#1101)\n\n* fix st serialization\n\n* add test\n\n* add CI test\n\n* add comment"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-10-30T14:36:41Z",
        "message": "Add implementation of LyCORIS LoKr for SD&SDXL models (#978)\n\nKronA-like adapter"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-10-25T12:53:45Z",
        "message": "FIX setting active adapter correctly (#1051)\n\nCurrently, when calling set_adapter, the active adapter is not updated.\nTests have been added to trigger the bug and the method updated to fix\nit.\n\nMoreover, I created an active_adapters property on the PeftModel class\nso that it behaves consistently with the underlying models like\nLoraModel."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-10-11T08:34:28Z",
        "message": "FIX Don't assume model_config contains model_type (#1012)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-10-09T12:25:07Z",
        "message": "Fix word_embeddings match for deepspeed wrapped model (#1000)\n\n* vocab size prompt vocab fix\n\n* add comments\n\n* Update src/peft/peft_model.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-10-04T07:44:10Z",
        "message": "Add base model metadata to model card (#975)\n\nResolves #938\n\nThis PR adds the base model metadata, if present, to the model card.\n\nOn top of this, the code for creating the model card has been refactored\nto use the huggingface_hub classes instead of doing ad hoc parsing and\nwriting.\n---------\n\nCo-authored-by: Lucain <lucainp@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-10-02T08:44:51Z",
        "message": "FEAT Add LyCORIS LoHa for SD&SDXL models (#956)\n\nhttps://arxiv.org/abs/2108.06098"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-09-12T09:12:40Z",
        "message": "Make base_model.peft_config single source of truth (#921)\n\nResolves #802, #923\n\nFor the problem description, please check the first issue.\n\nI went with solution 2, i.e. making the base_model.peft_config the\n\"single source of truth\" for the PEFT configuration. That way, we\nminimize the risk of diverging configurations.\n\nThis does not apply for prompt learning, where we don't have a\npeft_config on the base model (which is just the normal model, not a\nPEFT class).\n\nI added a setter for peft_config but from my testing, it isn't being\nused. It's only there for completeness."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-08-30T15:16:22Z",
        "message": "DOC: PeftModel save_pretrained docstring (#881) (#888)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-08-29T08:53:14Z",
        "message": "FIX: seq2seq prompt tuning (#439) (#809)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-08-25T06:12:11Z",
        "message": "\ud83c\udf89 Add Multitask Prompt Tuning (#400)\n\n* mpt\n\n* fix save\n\n* fix save\n\n* add jupyter notebook\n\n* add jupyter notebook\n\n* add jupyter notebook\n\n* drop shuffling\n\n* drop classify_dataset\n\n* drop classify_dataset\n\n* fix keys\n\n* fix keys\n\n* add comments\n\n* use EXACT_SOURCE_TASK in the example\n\n* formatting\n\n* Fix dict index in embedding retrieval\n\n* run style and quality\n\n* run style and quality\n\n* run style and quality\n\n* style\n\n* final fix\n\n* style\n\n* comment out failing tests\n\n* fix generation tests\n\n* fix style and save test\n\n* all testcases\n\n* fix import\n\n* add license header\n\n* reformat\n\n* fix encoder-decoder models\n\n* fix tests running multiple times\n\n* fix paper name for IA3 and add MPT paper\n\n* Trigger CI\n\n* address the recommended changes\n\n* reformat\n\n* address suggestions\n\n* address suggestions\n\n* revert reformatting\n\n* revert reformatting\n\n---------\n\nCo-authored-by: Alex-Brooks <Alex.Brooks@ibm.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-08-11T21:31:17Z",
        "message": "GPTQ Integration (#771)\n\n* add gptq lora\n\n* fix peft gptq\n\n* fix condition\n\n* fix test\n\n* remove unused weights\n\n* check type\n\n* style\n\n* change attribute\n\n* remove print\n\n* add exllama\n\n* make style\n\n* refactor + fix tests\n\n* remove print\n\n* remove dep on transformers"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-08-08T12:38:23Z",
        "message": "Update docstring of PeftModel.from_pretrained (#799)\n\n1. Addresses\nhttps://github.com/huggingface/peft/issues/430#issuecomment-1666312815\n2. Reword docstring to not be LoRA-specific"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-08-08T12:35:19Z",
        "message": "Add adapter error handling (#800)\n\nWhen a user tries to add a 2nd adapter, Lora and AdaLora make some checks to\nensure the new adapter is compatible with existing adapters. Currently, that\ncheck is performed halfway through the method. This means that if the check\nfails, the new adapter is partially applied, leaving the model in a bad state.\nThe main purpose of this PR is to ensure that the model state is correct after\nsuch a failure is encountered.\n\nTests were added to catch this potential bug.\n\nWhile working on this, I also did some related, but not strictly necessary\nchanges to the add_adapter methods:\n\n- Previously, the peft_config from the PeftModel was passed to the base\n  model. This meant that sometimes, the base model would hold a reference\n  to PeftModel.peft_config, but not always, as some base models would\n  create new dicts. This is problematic, because some code would rely on\n  the objects being the same. Now, they are never the same, leading to\n  more consistency.\n- I think that the check if multiple adapters have biases (which is not\n  supported) was accidentally removed by #749. It is added back in.\n- Add some type annotations\n- Extend docstrings to contain adapter_name"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-08-07T14:34:54Z",
        "message": "[`core`] PEFT refactor + introducing `inject_adapter_in_model` public method (#749)\n\nRefactors a bit the internals of some PEFT models and introduces a new\nmethod inject_adapter_in_model for users that want to pass a bare model\nand a peft config to inject adapters in-place into the model. These\nchanges are totally BC with the previous PEFT versions.\n\nThis PR makes things easier for the PEFT integration in transformers\nhuggingface/transformers#25077\n\nThe main goal of the PR is to expose a new API for advanced users that\nwant to integrate PEFT method without the need to use the PeftModel\nwrapper. A simple use case could be someone that wants to inject adapters\ninto a model and wants to keep the original class of the model without\nhaving to offload that to peft that will create a PeftModel. I have\nfaced this issue in huggingface/transformers#25077 Among other things,\nthis PR refactors some internals of PEFT library, while keeping it fully\nbackward compatible.\n\nTo tackle the main motivation I propose to differentiate things between\ntwo type of adapters\n\n1- adapters that are injectable (LoRA, AdaLoRA, IA3)\n2- adapters that are not injectable (the rest)\n\nAs a first iteration this API would be supported only for the scenario\n1- / therefore I decided to create 2 abstract classes to make things\neasy to be able to determine if the adapter layer (e.g. LoraLayer) /\nadapter module (e.g. LoraModel) does follow the minimal\nrequirement (i.e. needed attributes, etc.)\n\nOther related changes:\n\n1- Creates a new property method is_prompt_learning to avoid importing\n   PromptLearningConfig all the way down\n2- Introduces a new object TUNERS_MAPPING, which is a mapping of\n   supported pluggable adapters\n3- Creates two abstract classes\n3.1- BaseTunerLayer: a mixin to check for minimal required attributes\n     that a tuner layer should have active_adapter / _is_plugable\n3.2- BaseTuner: a higher level module mixin that should be used for any\n     injectable adapters in the future.\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-08-02T14:59:11Z",
        "message": "Allow passing inputs_embeds instead of input_ids (#757)\n\nResolves #727\n\nRight now, there is an issue with a few PeftModelForXxx classes when\nusers pass only inputs_embeds but not input_ids. First of all, the batch\nsize was being derived on input_ids, now it is derived from\ninputs_embeds instead if input_ids is None. Furthermore, a few forward\ncalls to the base model were not passing the inputs_embeds along, which\nresulted in errors down the line. These issues have been fixed now."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-08-01T13:46:18Z",
        "message": "Support XPU adapter loading   (#737)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-19T12:57:14Z",
        "message": "[`Patch`]\u00a0patch trainable params for 4bit layers (#733)\n\n* patch trainable params for 4bit layers\n\n* revert\n\n* added tests.\n\n* added comments.\n\n* addressed final comments"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-19T12:29:36Z",
        "message": "[`Llama2`] Add disabling TP behavior (#728)\n\n* add disabling TP behavior\n\n* add comments\n\n* adapt from new changes of transformers PR"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-19T08:59:55Z",
        "message": "revert change (#731)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-19T07:52:25Z",
        "message": "fix the param count when using 4-bit bnb"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-19T05:47:15Z",
        "message": "Fix subfolder issue (#721)\n\n* fix subfolder issue\n\n* added tests"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-17T09:58:57Z",
        "message": "better hub kwargs management (#712)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-17T08:02:30Z",
        "message": "FEAT: Make LoRA work with custom models (#676)\n\nEnable custom models to work with LoRA\n\nThis PR enables custom models to work with LoRA in peft by performing a few\nchanges required for non-transformers models. New tests for linear,\ntransformers conv1d, and conv2d layers were added.\n\nNot yet contained in this PR:\n\n- support for AdaLoRA and IA\u00b3\n- documentation\n- examples\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-15T12:18:34Z",
        "message": "[`Auto`] Support `AutoPeftModel` for custom HF models (#707)\n\n* support `AutoPeftModel` for custom HF models\n\n* added documentation."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-14T14:28:03Z",
        "message": "[`Feature`] Save only selected adapters for LoRA (#705)\n\n* v1 working for LoRA\n\n* more checks\n\n* fix prompt learning issues\n\n* fix failing test\n\n* Apply suggestions from code review\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* fixed indentation\n\n* move the check above\n\n* added tests for adaption prompt, enc-dec and feature extraction\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-14T14:14:51Z",
        "message": "[Core] Enhancements and refactoring of LoRA method (#695)\n\n* refactor lora and add utils\n\n1. Refactor LoRA code\n2. Add method to delete LoRA adapters\n3. Add method to unload the PEFT LoRA model.\n4. Add `svd` weighted adapter support.\n5. minor fixes\n\n* fixes\n\n* fixes\n\n* Update lora.py\n\n* fixes\n\n* Update lora.py\n\n* docstrings for the added public APIs\n\n* docs\n\n* Update src/peft/tuners/lora.py\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* resolve comments, refactoring and adding tests\n\n* fix the remaining failing tests\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-14T12:33:33Z",
        "message": "[WIP] FIX for disabling adapter, adding tests (#683)\n\nThis PR deals with some issues with disabling adapter:\n\n- typo in active.adapter\n- prompt encoder could be on wrong device\n- when using prompt learning + generate, disabling did not work\n\nFor the last point, there is a somewhat ugly fix in place for now,\npending a more comprehensive refactor (a comment was added to that\neffect).\n\nComprehensive tests were added to check that everything works now.\n\nThe following tests still not working:\n\n- adaption prompt\n- seq2seq with prompt tuning/prompt encoding\n- stable diffusion is a little bit flaky but test is hopefully robust enough\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-13T12:34:28Z",
        "message": "add support for Feature Extraction using PEFT (#647)\n\n* add support for embedding with peft\n\n* add example and resolve code quality issues\n\n* update notebook example post fixing the loss\n\n* adding full example with inference notebook\n\n* quality \u2728\n\n* add tests, docs, guide and rename task_type to be inline with Hub\n\n* fixes\n\n* fixes\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update peft_model.py\n\n* fixes\n\n* final fixes\n\n* Update _toctree.yml\n\n* fixes and make style and make quality\n\n* deberta exception with checkpointing\n\n* Update docs/source/task_guides/semantic-similarity-lora.md\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update docs/source/task_guides/semantic-similarity-lora.md\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* resolve comments\n\n* testing prompt learning methods\n\n* Update testing_common.py\n\n* fix the tests\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-13T09:12:40Z",
        "message": "FIX: base_model_torch_dtype when using model.half() after init (#688)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-13T07:45:50Z",
        "message": "Add functionality to support IA3 (#578)\n\n* Added initial ia3 code\n\n* Implemented ia3 correctly for feedforward layers; Fixed regex matching\n\n* Fixed module mapping for mt5\n\n* Merged changes from huggingface:main\n\n* Merged changes\n\n* Fixed lora merge conflicts\n\n* Different bloom config\n\n* Added save option for ia3\n\n* Added loading code for ia3\n\n* Added feedforward implementation in utils and seq cls example\n\n* Added feedforward implementation in utils and seq cls example\n\n* Implemented merge, unmerge, enable/disable adapters functionality\n\n* Fixed feedforward during merge\n\n* Debugging Merge\n\n* Removing debug messages\n\n* Cleaned up repo\n\n* Removed non-IA3 changes\n\n* Refactor save and load\n\n* Added support to all models in tests; Added IA3Config for common tests\n\n* Added half-precision support and test for gradient checkpointing; Formatted jupyter notebooks\n\n* Added target modules for new models GPTBigCode and LLama\n\n* Cleaned up code\n\n* Cleaned up code\n\n* Cleaned up example notebook\n\n* Cleaned up  seq2seq notebook\n\n* Corrected function docstrings; refactored find_and_replace\n\n* Corrected function docstrings; refactored find_and_replace\n\n* Added basic docs for IA3\n\n* Added new conceptual guide in source tree for documentation\n\n* Minor fix to documentation\n\n* Minor fixes to docstrings; Added error handling for 4bit quantization; Cleaned unused merge/unmerge methods\n\n* styling changes after merge from main\n\n* Update src/peft/tuners/ia3.py\n\nRemove unused attribute merge_weights\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Abhishek2304 <abhishekgupta2304@gmail.com>\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-28T07:03:16Z",
        "message": "style: tentatively add hints for some public function (#614)\n\n* style: tentatively add hints for some public function\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* fix: import annotations to evaluate to str\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* fix: style\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-27T21:41:51Z",
        "message": "Update peft_model.py (#644)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-27T12:57:57Z",
        "message": "feat(model): Allow from_pretrained to accept PeftConfig class (#612)\n\n* feat(model): Allow from_pretrained to accept PeftConfig class\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* tests: add test cases for config construction\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: address comments and run tools\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* fix: style\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-27T11:26:47Z",
        "message": "fix ptun and prompt tuning generation issue (#543)\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-27T08:27:21Z",
        "message": "fix Prefix-tuning error in clm Float16 evaluation (#520)\n\nSigned-off-by: Wang, Yi <yi.a.wang@intel.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-27T06:15:49Z",
        "message": "Add seq2seq prompt tuning support (#519)\n\n* Added prompt tuning for seq2seq and corresponding notebook examples\n\n* Added prompt tuning for seq2seq and corresponding notebook examples\n\n* Added prompt tuning for seq2seq and corresponding notebook examples\n\n* Call encoder with get_encoder() and update notebook example\n\n* Style formatting\n\n* Add seq2seq p-tuning support, and improve seq2seq prompt tuning support, enabling the use of generate()\n\n* Fix imports\n\n* Fix imports\n\n* Add co-author.\n\nCo-authored-by: ZhengxiangShi michaelszx117@gmail.com\n\n* Add co-author.\n\nCo-authored-by: ZhengxiangShi <michaelszx117@gmail.com>\n\n---------\n\nCo-authored-by: Thomas SCHILLACI <tschilla@px101.prod.exalead.com>\nCo-authored-by: ZhengxiangShi <michaelszx117@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-19T08:49:41Z",
        "message": "Improve the README when using PEFT (#594)\n\n* add logic\n\n* Update peft_model.py\n\n* fix test failures\n\n* fixes\n\n* fix"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-16T11:23:58Z",
        "message": "feat: Add PeftModelForQuestionAnswering (#473)\n\n* Added first try of supporting QuestionAnswering\n\n* Updated example to be correct\n\n* Added changes from PR 404\n\n* Added missing mapping for task type\n\n* Remove unrelated code\n\n* Run make style"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-16T11:04:07Z",
        "message": "when from_pretrained is called in finetune of lora with flag \"is_trainable\" True, should not call model.eval() (#591)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-16T08:58:51Z",
        "message": "Fix typo at peft_model.py (#588)\n\nFix typo on description:\n- `imputs_embeds` to `inputs_embeds`"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-15T10:23:05Z",
        "message": "[`core`] Correctly passing the kwargs all over the place (#575)\n\n* v1 of the fix\n\n* forward contrib credits from discussions\n\n* add tests\n\n---------\n\nCo-authored-by: winglian <winglian@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-15T07:35:43Z",
        "message": "enable lora for mpt (#576)\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-09T10:33:13Z",
        "message": "[`core`] Add safetensors integration (#553)\n\n* add v1\n\n* clean up\n\n* more improvements\n\n* add device\n\n* final adjustements\n\n* use `EntryNotFoundError`\n\n* better checks\n\n* add tests and final fixes\n\n* make style && make quality\n\n* remove `push_to_hub` because of the release"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-07T12:39:17Z",
        "message": "add thousands separator in print_trainable_parameters (#443)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-05T13:14:40Z",
        "message": "add library name to model card (#549)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-01T09:17:05Z",
        "message": "Fixed problem with duplicate same code. (#517)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-01T09:16:38Z",
        "message": "return load_result when load_adapter (#481)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-01T09:09:54Z",
        "message": "Enable PeftConfig & PeftModel to load from revision (#433)\n\n* Enable PeftConfig to load from revision\n\n* Add revision to PeftModel\n\n* Fix weights download with revision"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-05-31T10:14:27Z",
        "message": "[`core`] Add gradient checkpointing check (#404)\n\n* add automatic input enable gradients when calling `get_peft_model`\n\n* style\n\n* better check\n\n* add 4bit check"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-05-31T06:08:12Z",
        "message": "Remove merge_weights (#392)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-05-20T15:47:15Z",
        "message": "4-bit QLoRA via bitsandbytes (4-bit base model + LoRA) (#476)\n\n* 4bit lora\n\n* 4bit test\n\n* fixing 4bits bugs\n\n* fp4 pass variables\n\n* fix inference datatype and generation config\n\n* updating prep for int8 function to work for 4-bit\n\n* Added FP4 LoRA and FP4 fine-tuning example.\n\n* LinearFP4 -> Linear4bit\n\n* fixes\n\n* Fixed 4-bit example.\n\n* Style changes.\n\n* final changes\n\n---------\n\nCo-authored-by: Artidoro Pagnoni <pagnoni.artidoro@gmail.com>\nCo-authored-by: younesbelkada <younesbelkada@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-05-10T04:39:28Z",
        "message": "do not use self.device. In FSDP cpu offload mode. self.device is \"CPU\" instead of \"cuda\" (#352)\n\nand there's error like \"Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:1\"\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-26T12:56:14Z",
        "message": "Use `try` and `finally` in `disable_adapter()` to catch exceptions (#368)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-25T06:54:18Z",
        "message": "Implement adaption prompt from Llama-Adapter paper (#268)\n\n* Implement adaption prompt from Llama-Adapter paper\n\n* Support multi-adapters\n\n* Refactor adaption prompt to target attn modules instead of layers\n\n* Refactor adaption prompt to be more generic\n\n* Fix adaption prompt not on right device\n\n* Apply suggestions from code review\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>\n\n* Fix style\n\n* Add support for Llama config use_cache=True\n\n* Fix rebase issues\n\n---------\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-20T10:46:13Z",
        "message": "fix lora modules_to_save issue (#343)\n\n* fix lora modules_to_save issue\n\n* fix quality"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-08T08:27:08Z",
        "message": "Merge pull request #283 from huggingface/smangrul/multi-lora-support\n\nfix trainable params setting"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-08T06:15:32Z",
        "message": "Update peft_model.py"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-07T10:48:22Z",
        "message": "add and fix tests"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-07T10:36:58Z",
        "message": "Merge remote-tracking branch 'upstream/main' into fix-half-prec"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-06T22:38:10Z",
        "message": "fixing adalora saving and loading"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-06T14:01:21Z",
        "message": "\ud83d\ude05"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-06T13:35:31Z",
        "message": "Merge branch 'main' into smangrul/multi-lora-support"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-06T06:00:30Z",
        "message": "Merge pull request #233 from QingruZhang/main\n\nThe Implementation of AdaLoRA (ICLR 2023)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-05T20:52:12Z",
        "message": "Run make style and make quality"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-04T20:33:31Z",
        "message": "fix \ud83d\udc1b"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-04T14:14:58Z",
        "message": "fixing \ud83d\udc1b"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-04T12:14:24Z",
        "message": "fix \ud83d\udc1b"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-04T11:34:32Z",
        "message": "Merge branch 'main' into smangrul/multi-lora-support"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-04T10:01:47Z",
        "message": "fix half precision forward"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-03T16:28:11Z",
        "message": "Fixing a bug where a wrong parameter name is used."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-01T12:54:46Z",
        "message": "[`core`] Fix offload issue (#248)\n\n* fix offload dir\n\n* remove offload index\n\n* safety checker\n\n* forward contrib credits from previous PR\n\n---------\n\nCo-authored-by: cosimoiaia <cosimoiaia@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-03-31T21:41:14Z",
        "message": "fix kwargs"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-03-31T21:30:05Z",
        "message": "clean up docstrings"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-03-30T06:24:45Z",
        "message": "Merge branch 'huggingface:main' into main"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-03-29T13:20:14Z",
        "message": "Causal LM generation fix for prefix tuning: GPT2 model (#222)\n\n* expand attention mask after preparing generation inputs for prefix tuning\n\n* reformat\n\n* Update src/peft/peft_model.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* reformat as per black\n\n---------\n\nCo-authored-by: Vineet Kumar <vineeku6@in.ibm.com>\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-03-29T03:19:14Z",
        "message": "merge the conflit'"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-03-28T14:10:06Z",
        "message": "Update peft_model.py"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-03-28T14:02:21Z",
        "message": "Update peft_model.py"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-03-28T13:59:24Z",
        "message": "Update peft_model.py"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/mixed_model.py",
        "commit_date": "2024-02-08T13:39:46Z",
        "message": "DOC How to freeze adapter after set_adapter call (#1447)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/mixed_model.py",
        "commit_date": "2024-02-07T11:52:35Z",
        "message": "MNT Move code quality fully to ruff (#1421)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/mixed_model.py",
        "commit_date": "2024-02-06T17:01:05Z",
        "message": "[docs] Doc maintenance (#1394)\n\n* improvements\n\n* fix name\n\n* feedback\n\n* fix typos\n\n* feedback"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/mixed_model.py",
        "commit_date": "2024-02-06T00:54:06Z",
        "message": "Fix typos (#1435)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/mixed_model.py",
        "commit_date": "2024-01-17T14:25:49Z",
        "message": "Added missing getattr methods for mixed model (#1365)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/mixed_model.py",
        "commit_date": "2023-12-04T11:18:49Z",
        "message": "ENH: Enable OFT adapter for mixed adapter models (#1204)\n\nThis PR makes it possible to use the newly added OFT adapter in mixed\nadapter type models, similar to LoRA, LoHa, etc.\n\nNotes\n\nAdding the integration was pretty straightforward, which is a good sign.\n\nThe difficult part was actually about the tests. This stems from the\nfact that OFT is (if my understanding is correct) never commutative.\nWhat I mean is that even if the adapters are applied to the last layer\nof a model, it makes a difference whether we apply, say, first LoRA,\nthen OFT vs first OFT, then LoRA.\n\nThis is different for the other adapters that were added so far for\nmixed models, as they basically do:\n\n- Xa = X + dXa\n- Xab = Xa + dXb = X + dXa + dXb = X + dXb + dXa = Xb + dXa = Xba\n\nThis is not true for OFT, so when OFT is used, I had to ensure\nthat no test was applied that (implicitly) assumes commutativity.\n\nFurthermore, I had to increase the model size, see this comment:\n\nhttps://github.com/huggingface/peft/pull/1160#issuecomment-1836107235"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/mixed_model.py",
        "commit_date": "2023-11-30T20:58:16Z",
        "message": "Mixed adapter models (#1163)\n\nDescription\n\nThis PR allows to add adapters of different types, e.g. LoRA and LoHa:\n\nbase_model = ...\nconfig0 = LoraConfig(...)\npeft_model = get_peft_model(base_model, config0, mixed=True)\nconfig1 = LoHaConfig(...)\npeft_model.add_adapter(config1, \"other\")\npeft_model.set_adapter([\"default\", \"other\"])\npeft_model(x)\n\nAt this point, both adapters are active at the same time.\n\nExisting code should not be affected by this change, since users need to\nopt into this behavior by setting mixed=True, and a completely different\nclass is being used (PeftMixedModel).\n\nAlso interesting is that this method can be used for a single adapter\ntype but with very different configs. Right now, we have limited support\nfor that (e.g. for LoRA, different r values by using rank_pattern), but\nwith this, we don't need to special case the differing arguments\nanymore.\n\nNot implemented\n\n- [ ] I'm not yet sure if the same logic can be applied to IA\u00b3 or if it\n  may fail because IA\u00b3 can apply its scaling to the input, not the output.\n- [ ] OFT is not supported yet but should work.\n- [ ] It is currently not possible to represent a mixed adapter model as\n  a single config. I think we can come up with a solution but I don't\n  think it is necessary for a first version of this.\n- [ ] Saving and loading is not yet implemented for mixed models.\n\nThose could potentially be added in a future PR.\n\n---------\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2024-02-25T19:10:02Z",
        "message": "Add Gemma (#3078)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2024-02-11T15:47:44Z",
        "message": "Add llava 34b template (#3034)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2024-02-09T19:03:27Z",
        "message": "Upgrade gradio to 4.17 (#3027)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2024-02-05T23:38:31Z",
        "message": "code update (#2997)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2024-02-01T03:31:33Z",
        "message": "update yuan2.0 generation (#2989)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2024-01-24T10:38:22Z",
        "message": "Fix sglang worker (#2953)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2024-01-24T08:22:52Z",
        "message": "SGLang Worker (#2928)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2024-01-24T08:02:31Z",
        "message": "format code"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2024-01-24T07:58:26Z",
        "message": "Fix the pooling method of BGE embedding model (#2926)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2024-01-24T07:52:05Z",
        "message": "feat: support Model Yuan2.0, a new generation Fundamental Large Language Model developed by IEIT System (#2936)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2024-01-18T22:20:41Z",
        "message": "fix specify local path issue use model from www.modelscope.cn (#2934)\n\nCo-authored-by: mulin.lyh <mulin.lyh@taobao.com>"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2024-01-17T07:47:04Z",
        "message": "Bump the version to 0.2.35 (#2927)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2024-01-17T07:28:52Z",
        "message": "nous-hermes-2-mixtral-dpo (#2922)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2024-01-17T06:57:34Z",
        "message": "add support for iei yuan2.0 (https://huggingface.co/IEITYuan) (#2919)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2024-01-17T06:56:20Z",
        "message": "Add TenyxChat-7B-v1 model (#2901)\n\nCo-authored-by: sarath@L3 <[omitted]>"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2024-01-17T06:55:47Z",
        "message": "feat: use variables OPENAI_MODEL_LIST (#2907)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2024-01-08T16:27:41Z",
        "message": "Update model_adapter.py (#2895)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2024-01-07T21:04:31Z",
        "message": "Add `Notus` support (#2813)\n\nCo-authored-by: alvarobartt <alvaro@argilla.io>"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2024-01-07T16:30:22Z",
        "message": "Fix bug that model doesn't automatically switch peft adapter (#2884)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2024-01-07T16:28:57Z",
        "message": "Add TinyLlama (#2889)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-12-28T07:18:45Z",
        "message": "update a new sota model on MT-Bench which touch an 8.8 scores. (#2864)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-12-28T07:18:26Z",
        "message": "Fix the problem of not using the decoding method corresponding to the base model in peft mode (#2865)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-12-24T22:41:28Z",
        "message": "Add new models (Perplexity, gemini) & Separate GPT versions (#2856)\n\nCo-authored-by: Wei-Lin Chiang <infwinston@gmail.com>"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-12-24T06:35:01Z",
        "message": "Format code (#2854)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-12-24T06:01:25Z",
        "message": "Import `accelerate` locally to avoid it as a strong dependency (#2820)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-12-24T05:45:01Z",
        "message": "add bagel model adapter (#2814)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-12-24T05:18:40Z",
        "message": "Fix conv_template of chinese alpaca 2 (#2812)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-12-24T05:11:00Z",
        "message": "add download models from www.modelscope.cn (#2830)\n\nCo-authored-by: mulin.lyh <mulin.lyh@taobao.com>"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-12-24T05:08:03Z",
        "message": "Add support for CatPPT (#2840)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-12-17T22:38:16Z",
        "message": "Add SOLAR-10.7b Instruct Model (#2826)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-12-09T21:58:25Z",
        "message": "add dolphin (#2794)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-12-09T16:07:25Z",
        "message": "Support xDAN-L1-Chat Model  (#2732)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-12-09T16:04:44Z",
        "message": "Support MetaMath (#2748)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-12-09T16:02:01Z",
        "message": "Update UI and new models (#2762)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-12-01T03:18:43Z",
        "message": "Add deepseek chat (#2760)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-11-27T05:13:02Z",
        "message": "add starling support (#2738)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-11-26T21:20:23Z",
        "message": "Fix MPS backend 'index out of range' error (#2737)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-11-26T08:05:02Z",
        "message": "Fix YiAdapter (#2730)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-11-26T06:05:58Z",
        "message": "Fix Hermes2Adapter (#2727)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-11-23T11:42:09Z",
        "message": "Add Hermes 2.5 [fixed] (#2725)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-11-23T07:45:31Z",
        "message": "Add Yi support (#2723)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-11-23T00:12:18Z",
        "message": "Exllama cache 8bit (#2719)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-11-23T00:11:43Z",
        "message": "support stable-vicuna model (#2696)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-11-22T08:36:01Z",
        "message": "add support for Chinese-LLaMA-Alpaca (#2700)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-11-22T08:29:16Z",
        "message": "Add Microsoft/Orca-2-7b and update model support docs (#2714)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-11-12T21:09:49Z",
        "message": "Fix gpt template (#2674)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-11-10T18:19:07Z",
        "message": "add chatglm3 conv template support in conversation.py (#2622)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-11-09T11:43:55Z",
        "message": "added support for CodeGeex(2) (#2645)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-11-07T23:44:38Z",
        "message": "Improve Azure OpenAI interface (#2651)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-11-03T23:59:58Z",
        "message": "xFastTransformer framework support (#2615)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-11-03T23:59:00Z",
        "message": "openchat 3.5 model support (#2638)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-11-02T05:40:05Z",
        "message": "fix: Fix for OpenOrcaAdapter to return correct conversation template (#2613)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-11-01T08:14:00Z",
        "message": "feat: Support model AquilaChat2 (#2616)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-10-28T02:53:35Z",
        "message": "Update qwen and add pygmalion (#2607)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-10-24T01:24:29Z",
        "message": "add trust_remote_code=True in BaseModelAdapter (#2583)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-10-24T01:23:52Z",
        "message": "Add Lemur model (#2584)\n\nCo-authored-by: Roberto Ugolotti <Roberto.UGOLOTTI@ec.europa.eu>"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-10-21T21:06:08Z",
        "message": "Update README.md  (vicuna-v1.3 -> vicuna-1.5) (#2592)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-10-21T11:06:24Z",
        "message": "docs: bit misspell comments model adapter default template name conversation (#2594)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-10-20T18:25:36Z",
        "message": "Add Mistral-7B-OpenOrca conversation_temmplate (#2585)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-10-18T23:12:36Z",
        "message": "Update vigogne template (#2580)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-10-17T20:47:52Z",
        "message": "feat: add claude-v2 (#2571)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-10-15T19:37:17Z",
        "message": "Add Xwin-LM V0.1, V0.2 support (#2566)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-10-15T19:27:15Z",
        "message": "Add airoboros_v3 chat template (llama-2 format) (#2564)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-10-12T21:54:02Z",
        "message": "Revert \"Improve Support for Mistral-Instruct\" (#2552)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-10-12T18:47:50Z",
        "message": "Improve Support for Mistral-Instruct (#2547)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-10-11T18:08:14Z",
        "message": "Add Zephyr 7B Alpha (#2535)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-10-09T20:06:32Z",
        "message": "Improve docs (#2534)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-10-09T19:51:37Z",
        "message": "Add ExllamaV2 Inference Framework Support. (#2455)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-10-09T19:47:31Z",
        "message": "add Llama2ChangAdapter (#2510)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-10-02T21:13:07Z",
        "message": "Add Mistral AI instruction template (#2483)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-09-18T20:41:57Z",
        "message": "Fix docs"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-09-18T20:18:38Z",
        "message": "merge google/flan based adapters: T5Adapter, CodeT5pAdapter, FlanAdapter (#2411)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-09-18T03:06:14Z",
        "message": "add dtype and seed (#2430)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-09-18T02:33:55Z",
        "message": "Improve docs (#2438)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-09-18T02:01:58Z",
        "message": "Add falcon 180B chat conversation template (#2384)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-09-18T01:58:03Z",
        "message": "Add support for Phind-CodeLlama models (#2415) (#2416)\n\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-09-18T01:38:32Z",
        "message": "Add Ascend NPU support (#2422)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-09-13T05:07:34Z",
        "message": "Add support for baichuan2 models (#2408)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-09-12T04:04:46Z",
        "message": "Added google/flan models and fixed AutoModelForSeq2SeqLM when loading T5 compression model (#2402)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-09-11T17:51:51Z",
        "message": "Spicyboros + airoboros 2.2 template update. (#2392)\n\nCo-authored-by: Jon Durbin <jon.durbin@onna.com>"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-09-07T06:29:27Z",
        "message": "Make E5 adapter more restrict to reduce mismatch (#2381)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-09-01T01:34:32Z",
        "message": "Remove hardcode flash-attn disable setting (#2342)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-08-28T09:13:29Z",
        "message": "Add Code Llama Support and Fix empty system prompt for llama 2 (#2326)\n\nSigned-off-by: woshiyyya <xiaoyunxuan1998@gmail.com>"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-08-27T09:21:41Z",
        "message": "Improve gradio demo (#2323)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-08-24T07:05:34Z",
        "message": "Add new model to the arena (#2296)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-08-21T23:37:12Z",
        "message": "Add conversation support for VMware's OpenLLaMa OpenInstruct models (#2278)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-08-21T06:21:56Z",
        "message": "[Minor] Style clean up & Fix embeding (#2272)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-08-21T04:51:11Z",
        "message": "Update embedding logic (#2244)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-08-21T04:49:01Z",
        "message": "Add Intel AMX/AVX512 support to accelerate inference (#2247)\n\nSigned-off-by: LeiZhou-97 <lei.zhou@intel.com>"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-08-21T04:23:16Z",
        "message": "[Minor] Update the warning to follow the new conv_template file (#2248)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-08-16T10:37:27Z",
        "message": "Add support for Vigogne models (#2236)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-08-15T11:26:19Z",
        "message": "Add Llama2-Chinese model support (#2218)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-08-13T00:56:02Z",
        "message": "Improve docs (#2210)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-08-13T00:15:29Z",
        "message": "Add Support of bge model family for embedding generation (#2203)\n\nCo-authored-by: \u201cExtremys\u201d <\u201cExtremys@email.com\u201d>"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-08-11T14:56:13Z",
        "message": "Modify vllm compatible empty special token, and revise qwen (#2200)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-08-10T01:11:01Z",
        "message": "revise qwen adapter (#2191)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-08-09T19:43:15Z",
        "message": "feat: support BAAI/AquilaChat-7B. (#2192)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-08-09T05:25:22Z",
        "message": "Format code (#2189)"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-08-09T02:14:20Z",
        "message": "Add WizardCoder model support (#2166)\n\nCo-authored-by: valentin <valentin.planes@voxaya.com>\nCo-authored-by: Valentin PLANES <valentin@valentins-macbook-air.home>"
    },
    {
        "repo_url": "github.com/lm-sys/FastChat",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-08-08T13:43:05Z",
        "message": "1. add shell scripts for shutdowning serve; 2. add a feature to launch all serve related to openai-api-server in one cmd; (#2141)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2024-02-27T21:48:50Z",
        "message": "fix: use eos token in target tensor for instruction-tuning (#3945)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2024-01-12T06:10:07Z",
        "message": "Add per-step token utilization to tensorboard and progress tracker. (#3867)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-12-15T00:04:03Z",
        "message": "Add LLM Text Encoder (#3828)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-12-14T23:15:28Z",
        "message": "hack: workaround torch bug to unblock mixtral fine-tuning (#3830)\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-12-14T20:58:06Z",
        "message": "Add support for Phi-1 and Phi 1.5 (#3831)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-12-12T20:01:52Z",
        "message": "Remove quantization config before saving dequantized weights to disk (#3825)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-12-12T04:01:34Z",
        "message": "Move reusable LLM model methods to utility functions (#3821)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-12-05T08:22:30Z",
        "message": "Add support for dequantizing 4-bit bitsandbytes base models into fp16 (#3799)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-11-23T00:21:06Z",
        "message": "Add NEFTune implementation for Noised Embedding Instruction Fine-Tuning support (#3744)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-11-20T23:13:17Z",
        "message": "Cleanup: Use existing HFTokenizer to consolidate manual pad token setting. (#3774)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-11-04T02:11:44Z",
        "message": "Pinning Transformers to not include version 4.35.0 because it breaks a method in PEFT."
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-11-04T02:11:44Z",
        "message": "WIP"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-10-28T02:20:41Z",
        "message": "Define initialize_llm() and generate() methods. Remove extra logging in llm.py (#3711)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-10-25T21:00:45Z",
        "message": "Fix: Prevent memory from ballooning during post-training evaluation (#3756)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-10-20T15:45:34Z",
        "message": "Retry LLM model downloads from HF Hub with exponential backoff when there is a Read timeout (#3742)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-10-13T08:17:48Z",
        "message": "Dynamically set `max_new_tokens` based on output feature length, GMSL and model window size (#3713)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-10-11T17:34:55Z",
        "message": "QoL: Only log generation config being used once at inference time (#3715)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-10-05T15:31:36Z",
        "message": "[BUGFIX] Ensure that full base models and not only adapter weights get saved when merge_and_unload is set (#3679)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-10-02T15:35:27Z",
        "message": "Fix dynamic generation config load during `model.predict` (#3666)\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-09-29T13:23:46Z",
        "message": "Fix eos_token and pad_token issue (#3667)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-09-28T18:13:13Z",
        "message": "[MAINTENANCE] Partially reconcile type hints, fix some warnings, and fix comments in parts of the codebase. (#3673)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-09-27T19:29:33Z",
        "message": "[FEATURE] Support Merging LoRA Weights Into Base Model (Issue-3603) (#3649)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-09-22T20:25:16Z",
        "message": "Remove unnecessary peft config updating (#3642)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-09-18T17:40:04Z",
        "message": "QoL: Default to using fast tokenizer for Llama models (#3625)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-09-15T04:01:24Z",
        "message": "fix: Load 8-bit quantized models for eval after fine-tuning (#3606)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-09-12T00:36:39Z",
        "message": "Allow user to specify huggingface link or local path to pretrained lora weights (#3572)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-08-28T09:42:25Z",
        "message": "Set default max_sequence_length to None for LLM text input/output features (#3547)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-08-25T20:18:23Z",
        "message": "Refactor evaluation metrics to support decoded generated text metrics like BLEU and ROUGE. (#3539)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-08-15T19:51:46Z",
        "message": "Improve observability during LLM inference  (#3536)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-08-11T21:40:25Z",
        "message": "Move loss metric to same device as inputs (#3522)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-08-11T15:55:47Z",
        "message": "Add mechanic to override default values for generation during model.predict() (#3520)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-07-30T22:49:30Z",
        "message": "Add RoPE scaling to increase context length up to 8K for training or inference. (#3477)\n\nCo-authored-by: Travis Addair <tgaddair@gmail.com>"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-07-29T19:44:46Z",
        "message": "Enable evaluation with LLMs <7B (#3478)\n\nCo-authored-by: Arnav Garg <arnav@predibase.com>"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-07-29T18:47:29Z",
        "message": "Fixed QLoRA with mutli-gpu and reduce CUDA memory pressure during eval (#3486)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-07-28T16:48:49Z",
        "message": "Exclude frozen weights from checkpoints and fix evaluation using quantized LLMs (#3483)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-07-24T16:18:05Z",
        "message": "Add QLoRA for 4-bit fine-tuning (#3476)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-07-22T08:57:41Z",
        "message": "Zero copy initialization of models onto training workers for LLMs (#3469)\n\nCo-authored-by: Geoffrey Angus <geoffrey@predibase.com>\nCo-authored-by: Travis Addair <tgaddair@gmail.com>"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-06-29T07:38:49Z",
        "message": "[llm] fix device placement issues when using CPUs and GPUs during LLM fine tuning (#3447)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-06-29T00:25:36Z",
        "message": "[llm] Replace `model_name` with *required* `base_model`, add preset LLM registry, update internal adapter modules (#3423)\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Martin Davis <martin@predibase.com>\nCo-authored-by: ksbrar <kabir@brar.org>"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-06-23T18:24:35Z",
        "message": "[LLM] Various fixes for LLM Fine-Tuning issues that caused loss disparity between train and val sets (#3437)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-06-07T07:42:32Z",
        "message": "[LLM] Skip left padding removal when there is no left padding (#3432)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-05-25T01:06:25Z",
        "message": "[llm] Fixed loading when performing full fine-tuning (#3421)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-05-23T06:48:05Z",
        "message": "[llm] Fixed adapter initialization and OOM on checkpoint loading (#3416)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-05-22T16:10:26Z",
        "message": "[llm] Create separate Predictor for LLMs and enable flash attention on CUDA (#3409)\n\nCo-authored-by: Arnav Garg <arnav@predibase.com>"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-05-20T03:30:53Z",
        "message": "[LLM] Fix Loss Computation for LLM Fine-tuning using shifted tensors/new loss function (#3408)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-05-16T00:09:06Z",
        "message": "[LLM] Add Prefix Tuning, PTuning, LoRA, AdaLoRA and Adaption Prompt for LLM fine-tuning (#3386)\n\nCo-authored-by: Arnav Garg <arnav@predibase.com>"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-05-06T07:38:05Z",
        "message": "[LLM] Fine-Tune LLMs via Prompt tuning (#3359)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-05-03T18:28:13Z",
        "message": "[LLM] Few-shot learning via Retrieval-augmented ICL (#3351)\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/models/llm.py",
        "commit_date": "2023-04-15T00:04:06Z",
        "message": "Support zero-shot learning and text generation through LLMs (#3335)\n\nCo-authored-by: Travis Addair <tgaddair@gmail.com>"
    },
    {
        "repo_url": "github.com/nomic-ai/gpt4all",
        "filepath": "gpt4all-training/eval_self_instruct.py",
        "commit_date": "2023-10-24T13:28:21Z",
        "message": "make scripts executable (#1555)"
    },
    {
        "repo_url": "github.com/nomic-ai/gpt4all",
        "filepath": "gpt4all-training/eval_self_instruct.py",
        "commit_date": "2023-05-01T19:45:23Z",
        "message": "mono repo structure"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-03-01T21:51:24Z",
        "message": "Fix allowed_paths"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-28T02:23:31Z",
        "message": "Change to default of document choice in sidebar, on average easier even if not cleaner"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-28T02:03:10Z",
        "message": "save liked chatbots"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-28T00:05:04Z",
        "message": "Control visibility of reviews, keep likeable default always on"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-27T21:03:18Z",
        "message": "Update after transformers assertion"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-27T02:35:32Z",
        "message": "Cleanup"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-26T22:41:17Z",
        "message": "Fix AWQ, old models are not compatible"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-26T22:32:03Z",
        "message": "Update OpenAI server and add client docs&tests for other client parameters like langchain, like inspired by https://github.com/h2oai/h2ogpt/pull/1433/.  Fix AWQ."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-26T17:22:42Z",
        "message": "Ensure don't break h2oGPT when new models come out for openai, anthropic, mistralai, google"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-25T05:51:17Z",
        "message": "Handle case when local server doesn't need to know about Gated repo"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-24T19:18:31Z",
        "message": "Handle auth_access=open or closed for OpenAI server"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-23T05:40:33Z",
        "message": "Close responses for streaming in case incomplete yield, e.g. timeout"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-23T02:22:13Z",
        "message": "Update openai client for https://github.com/openai/openai-python/issues/1181"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-22T23:11:21Z",
        "message": "Close OpenAI connection once done"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-22T04:32:57Z",
        "message": "Groq via OpenAI API"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-22T04:30:02Z",
        "message": "groq fix for temperature=0 using openai api"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-21T00:23:51Z",
        "message": "Wrap gradio __h2oai__"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-20T23:34:18Z",
        "message": "Limit public portal to no more than 4 llms at once"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-20T18:31:01Z",
        "message": "Handle soft linked /tmp/gradio"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-15T19:41:42Z",
        "message": "Add note about --enable_tts and --enable_stt using more GPU memory"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-13T06:52:18Z",
        "message": "If user goes from not logged in to logged in, persist user id from previous, so can still use dbs."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-12T21:54:02Z",
        "message": "Disable streaming chunking if not using model lock, for https://github.com/h2oai/h2ogpt/issues/1367#issuecomment-1938913813"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-12T21:44:02Z",
        "message": "Handle url for TheBloke GGUF"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-12T08:31:21Z",
        "message": "cosmetics"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-12T08:12:37Z",
        "message": "Fixes #1270"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-10T09:11:52Z",
        "message": "Use GradioClient for llava too, so new hash for each user, so image isn't re-used if pass no image"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-10T08:15:56Z",
        "message": "Fix regenerate or not"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-10T06:54:26Z",
        "message": "Make llava faster"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-10T01:51:49Z",
        "message": "Note about https://github.com/gradio-app/gradio/issues/7379"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-08T06:40:10Z",
        "message": "change in gradio4 for image upload, can't push string"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-07T19:04:55Z",
        "message": "Fixes https://github.com/h2oai/h2ogpt/issues/1378#issuecomment-1932658797"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-06T21:46:30Z",
        "message": "Generalize vllm so can pass api_key and url extras too"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-06T21:41:06Z",
        "message": "Fix evaluate use, fix check of img_file if not set, and relax openai model checks a bit"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-06T10:44:41Z",
        "message": "cosmetics"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-06T10:37:07Z",
        "message": "Make Vision Q/A work for CLI/eval"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-06T09:11:37Z",
        "message": "Fix gradio_server setting so plain doesn't trigger both context and chat_conversation to have conversation history"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-06T08:14:58Z",
        "message": "Handle gradio->gradio for Vision Q/A"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-06T04:28:39Z",
        "message": "Use llava prompt so can handle chat history"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-06T02:57:31Z",
        "message": "WIP for langchain vision model passthrough.  Handle more general image files, simple gridnumbers.gif fails, still fails on ChatGPT"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-05T22:23:15Z",
        "message": "pass image from UI/API to Vision models"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-05T21:43:46Z",
        "message": "Add vision models as llms"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-05T00:20:00Z",
        "message": "Fixes #1361 -- just UI/embed_info issue, was still using OpenAI for embeddings"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-04T02:42:57Z",
        "message": "Resolve"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-04T01:31:12Z",
        "message": "protection if user didn't upgrade all packages"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-02-04T01:20:49Z",
        "message": "Faster transfers"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-31T19:40:10Z",
        "message": "Account for beacon memory, don't use for now"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-31T08:44:01Z",
        "message": "Improve conditional"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-31T08:42:09Z",
        "message": "Set namespace-Pt/activation-beacon-llama2-7b-chat as llama2 type and try to clear cace for some cache bug"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-29T02:03:40Z",
        "message": "Resolve conflict"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-26T01:33:24Z",
        "message": "Improve exception handling"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-25T23:20:16Z",
        "message": "Ensure use locked and copied version of stream from gradio server"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-21T11:14:06Z",
        "message": "min_max_new_tokens=256 is too strict for more modern LLMs, default to 512 to avoid truncation"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-21T11:00:26Z",
        "message": "Avoid need for openvllm, just use openai, pass vllm-specific things through extra_body key, i.e. https://github.com/01-ai/Yi/issues/223#issuecomment-1897731789"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-19T08:18:08Z",
        "message": "Only need requests for HF, gradio client already fails fast if system not up."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-19T02:01:07Z",
        "message": "Cleanup of new stream function, but still avoid since still hangs in gradio sync code"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-18T22:53:17Z",
        "message": "Bring back old for now, doesn't hang"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-18T22:11:05Z",
        "message": "Avoid late import, get clients early just don't store if regenerate_clients=True.  Other clean-up"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-18T05:59:58Z",
        "message": "Separate out GradioClient streaming part with h2oGPT related logic"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-17T10:23:28Z",
        "message": "Adjust presence_penalty down a bit"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-17T09:56:18Z",
        "message": "Avoid conversation system prompt if empty system prompt"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-16T06:47:35Z",
        "message": "Conform more to OpenAI API for proxy server for Issue #1217"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-15T06:30:03Z",
        "message": "MistralAI"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-14T05:20:29Z",
        "message": "More for https://github.com/h2oai/h2ogpt/issues/1217#issuecomment-1890474230"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-14T05:08:27Z",
        "message": "Probably Fixes empty openai_chat/vllm_chat messages as https://github.com/h2oai/h2ogpt/issues/1217#issuecomment-1890474230"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-11T23:40:02Z",
        "message": "Avoid plot for string scoring models"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-11T23:24:49Z",
        "message": "Get all models in gen.py, not cli/eval codes duplicated"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-11T18:40:15Z",
        "message": "Merge branch 'main' into verifier"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-11T16:42:21Z",
        "message": "WIP generalize scorer to allow for verifier"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-11T00:33:06Z",
        "message": "For Issue #1217 -- force do_sample=False if temp=0, allow temp=0 from UI."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-10T08:01:28Z",
        "message": "Resolve conflict"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-10T07:43:26Z",
        "message": "Stop clearing torch cache periodically too, and no ping gpu if public.  Add memory debug, disabled by default."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-10T03:03:24Z",
        "message": "Fixes #1048"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-10T02:50:20Z",
        "message": "Merge pull request #1272 from h2oai/windows_jan8\n\nWindows update Jan 8, 2024"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-09T20:56:18Z",
        "message": "make windows installer easier for GPU and exlpain how to tweak install and code at runtime"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-08T17:27:31Z",
        "message": "cleanup for using transformers for attention sinks"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-07T11:13:16Z",
        "message": "For Issue #1248 -- dedup and clean-up a bit model list from disk"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-06T08:28:12Z",
        "message": "Add verifier"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-06T06:14:57Z",
        "message": "Show sources in separate chat by default, so easier to copy non-source text from UI"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-06T00:36:28Z",
        "message": "For Issue #1217 -- decouple prompt_type from instruct or few-shot mode for langchain, so can use h2oGPT as pass through with --prompt_type=plain when using vllm_chat etc. and let inference server handling prompting"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-05T21:39:24Z",
        "message": "Try to work around https://github.com/encode/httpx/discussions/3043"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-05T21:19:16Z",
        "message": "user None removes specific messages already, but parse out if present too"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-05T11:48:34Z",
        "message": "Show exceptions in chat by default inside accordion that is removed when going to LLM, and add more images/video/audio for document view"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-05T10:11:24Z",
        "message": "Remove HYDE accordion outputs if present before giving history to LLM, and remove chat=True/False for prompt generation, hold-over and led to bugs in prompting for gradio->gradio"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-05T06:34:22Z",
        "message": "Add note"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-05T04:57:55Z",
        "message": "Control llava prompt"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-04T23:33:59Z",
        "message": "Stop showing 12b"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-04T23:13:54Z",
        "message": "Prompt template info for Issue #1257"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-03T00:48:48Z",
        "message": "Go back to checking system hash since stored in docker image now, even if takes 0.2s, worth it. Could delay checks to every minute or something, but more risky."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2024-01-01T21:10:06Z",
        "message": "Fixes for vllm_chat for Issue #1217"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2023-12-29T02:59:16Z",
        "message": "Fixes #1245"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2023-12-29T02:17:06Z",
        "message": "Avoid redundant return if not streaming"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2023-12-27T04:51:40Z",
        "message": "Figure out restriction on visible models"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2023-12-27T04:28:36Z",
        "message": "max visible models"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2023-12-24T06:46:17Z",
        "message": "force openai -> openai_chat if using chat models, so simpler code and correct handling of system prompt counting"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2023-12-23T04:22:51Z",
        "message": "Better auto system prompt, and go to auto as default"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2023-12-22T11:17:45Z",
        "message": "Accordion intermediate HYDE results by default.  For https://github.com/h2oai/h2ogpt/issues/1128#issuecomment-1866938571"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2023-12-22T09:51:03Z",
        "message": "Update so gradio4 client works with gradio4 server, but gradio4 can't talk to gradio3"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2023-12-22T01:57:48Z",
        "message": "Improve testing for OpenAI server and fix key issues with auth etc."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2023-12-21T12:41:55Z",
        "message": "OpenAI Proxy Server redirects to Gradio Server"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/gen.py",
        "commit_date": "2023-12-20T08:12:48Z",
        "message": "Update tests, and require max_seq_len if using gradio->gradio so first gradio knows that for docQA handling etc., since can't just get tokenizer for llama/gptj, would have to get model too."
    },
    {
        "repo_url": "github.com/AI4Finance-Foundation/FinGPT",
        "filepath": "fingpt/FinGPT_Forecaster/app.py",
        "commit_date": "2023-11-08T13:08:09Z",
        "message": "add FinGPT_Forecaster"
    },
    {
        "repo_url": "github.com/haotian-liu/LLaVA",
        "filepath": "llava/model/builder.py",
        "commit_date": "2024-02-06T16:39:06Z",
        "message": "Fix lora loading #1075"
    },
    {
        "repo_url": "github.com/haotian-liu/LLaVA",
        "filepath": "llava/model/builder.py",
        "commit_date": "2024-02-03T04:34:33Z",
        "message": "Improve serving."
    },
    {
        "repo_url": "github.com/haotian-liu/LLaVA",
        "filepath": "llava/model/builder.py",
        "commit_date": "2024-02-02T22:08:26Z",
        "message": "Fix multiple GPU inference."
    },
    {
        "repo_url": "github.com/haotian-liu/LLaVA",
        "filepath": "llava/model/builder.py",
        "commit_date": "2024-01-31T06:04:23Z",
        "message": "Release LLaVA-v1.6"
    },
    {
        "repo_url": "github.com/haotian-liu/LLaVA",
        "filepath": "llava/model/builder.py",
        "commit_date": "2023-11-24T13:55:43Z",
        "message": "\ud83e\ude79 make ``load_pretrained_model`` accept kwargs\n\n- this allows it to e.g. take a ``cache_dir`` argument"
    },
    {
        "repo_url": "github.com/haotian-liu/LLaVA",
        "filepath": "llava/model/builder.py",
        "commit_date": "2023-10-31T20:09:27Z",
        "message": "Update macOS support."
    },
    {
        "repo_url": "github.com/haotian-liu/LLaVA",
        "filepath": "llava/model/builder.py",
        "commit_date": "2023-10-06T23:49:36Z",
        "message": "For inference in model_worker, allow the device to be specified via a command line parameter.\n\nRight now it has only been tested with Apple Sillicon devices via the mps device."
    },
    {
        "repo_url": "github.com/haotian-liu/LLaVA",
        "filepath": "llava/model/builder.py",
        "commit_date": "2023-09-01T16:45:00Z",
        "message": "Update instruction for LoRA"
    },
    {
        "repo_url": "github.com/haotian-liu/LLaVA",
        "filepath": "llava/model/builder.py",
        "commit_date": "2023-07-29T23:55:59Z",
        "message": "Update docs"
    },
    {
        "repo_url": "github.com/haotian-liu/LLaVA",
        "filepath": "llava/model/builder.py",
        "commit_date": "2023-07-19T09:08:02Z",
        "message": "Release v1.0.0"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2024-01-05T10:11:24Z",
        "message": "Remove HYDE accordion outputs if present before giving history to LLM, and remove chat=True/False for prompt generation, hold-over and led to bugs in prompting for gradio->gradio"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-10-24T21:46:27Z",
        "message": "Avoid some warnings from langchain and transformers, but one transformers warning left that is their issue: https://github.com/huggingface/transformers/issues/27049"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-08-24T16:06:07Z",
        "message": "Comment out fsdp for now, not yet working."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-08-23T21:55:57Z",
        "message": "Upgrade peft/accelerate/transformers."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-08-23T20:48:21Z",
        "message": "Add comment."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-08-23T20:35:53Z",
        "message": "Fix fine-tuning of Llama2 7B, needs bf16 instead of fp16."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-08-23T20:22:06Z",
        "message": "Update fsdp"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-08-17T17:23:04Z",
        "message": "Fixes #678"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-08-09T14:54:58Z",
        "message": "deprecation fix"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-08-02T03:30:34Z",
        "message": "Token updates"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-07-28T16:41:37Z",
        "message": "Add env var to other scripts that accepts \"Fire\" arguments"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-07-26T02:38:55Z",
        "message": "Stick to 11.7 so conda install makes sense"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-07-21T22:04:22Z",
        "message": "Fixes #514 -- ensure bot and human type strings filled and account for possible system prompt.  Note that FastSYS versions are probably not right, missing space before end of instruction: [/INST]."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-07-14T20:33:05Z",
        "message": "Fix missing space in docs"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-07-11T08:25:19Z",
        "message": "Simpler use of model loader, and fix instructions to avoid CUDA extension not installed issue"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-07-10T21:22:20Z",
        "message": "Fix imports for fine-tune."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-07-07T22:37:35Z",
        "message": "Adjust generate import for new location"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-06-29T06:01:26Z",
        "message": "If have HF model/tokenizer, use that instead of faketokenizer (tiktoken) since see too large differences and failures even with 250 token buffer, still of by another 350."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-06-25T08:58:45Z",
        "message": "Fix generate_prompt use in fine tune"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-06-24T00:08:00Z",
        "message": "Fix prompting"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-06-20T22:33:05Z",
        "message": "Better fix for #307"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-06-20T22:10:48Z",
        "message": "Fix empty final state."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-06-06T18:32:02Z",
        "message": "Use optional dependencies"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-06-06T03:10:57Z",
        "message": "Make API easier, and add prompt_dict for custom control over prompt as example of new API parameter don't need to pass"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-06-01T15:05:46Z",
        "message": "Add support for Falcon 40b"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-25T05:55:01Z",
        "message": "Update generation/fine-tuning to handle 4-bit"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-24T10:33:29Z",
        "message": "Fix issue with 6.9 being in readme but code having 6_9, so 6.9 would not use correct prompt_type"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-23T20:44:36Z",
        "message": "Move loaders out of finetune, which is only for training, while loader used for generation too"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-19T18:30:27Z",
        "message": "Merge pull request #132 from orellavie1212/patch-1\n\nUpdate finetune.py"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-15T18:40:37Z",
        "message": "Update finetune.py\n\nFIXED bug model =  set_peft_model_state_dict, as it only set without return value."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-14T22:52:48Z",
        "message": "Refactor prompt stuff into single fine instead of finetune"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-14T22:13:08Z",
        "message": "Resolve conflict"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-13T18:28:54Z",
        "message": "Add wizard-vicuna"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-12T04:39:39Z",
        "message": "Add LLama and Vicuna models to eval."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-12T01:31:19Z",
        "message": "Add wizardlm prompts."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-11T21:15:10Z",
        "message": "Merge pull request #31 from h2oai/flash-attn\n\nneox Flash attn"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-11T21:12:32Z",
        "message": "Revert more changes."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-11T21:11:14Z",
        "message": "Revert name change."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-11T21:09:54Z",
        "message": "Cleanup."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-11T16:06:36Z",
        "message": "Fix model types map."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-11T06:49:29Z",
        "message": "Add 2 new gm models."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-11T06:06:25Z",
        "message": "Rebase, rename llama_flash_attn -> flash_attn."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-11T04:06:47Z",
        "message": "WIP - nothing working yet. Disable mix_in by default."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-10T23:06:05Z",
        "message": "Allow pipeline to stream, and use for langchain case."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-10T18:20:03Z",
        "message": "Add new models to list of human_bot."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-08T23:32:29Z",
        "message": "Use TrainingArguments, more general."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-08T19:06:38Z",
        "message": "Undo chat_sep"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-08T16:40:31Z",
        "message": "End with <human>:\\n for human_bot"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-07T09:57:59Z",
        "message": "Support OpenAssistant models in basic form, including 30B xor one\n\nFollow: https://huggingface.co/OpenAssistant/oasst-sft-7-llama-30b-xor\n\nsource ~/.bashrc.mamba\nmamba create -n oasstllamahf\nconda activate oasstllamahf\nmamba install python==3.10\nmkdir oasstllamahf\ncd oasstllamahf\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\ngit checkout d04ec99bec8a0b432fc03ed60cea9a1a20ebaf3c\npip install .\npip install torch==1.13.1 accelerate==0.18.0 sentencepiece==0.1.98 protobuf==3.20.1\npip freeze  # check\npython src/transformers/models/llama/convert_llama_weights_to_hf.py --input_dir /data/jon/LLaMA  --output_dir LLaMA30B --model_size 30B\npython\n> from huggingface_hub import snapshot_download\n> snapshot_download(\"OpenAssistant/oasst-sft-7-llama-30b-xor\")\n> exit()\nls -arlt ~/.cache/huggingface/hub/models--OpenAssistant--oasst-sft-7-llama-30b-xor/snapshots/d31b76426385f683c6efb8d4daea58379fb9bb09/oasst-sft-7-llama-30b-xor/  # check\nls -alrt LLaMA30B/  # check\nfind LLaMA30B -type f | xargs md5sum  # check\noadir=~/.cache/huggingface/hub/models--OpenAssistant--oasst-sft-7-llama-30b-xor/snapshots/d31b76426385f683c6efb8d4daea58379fb9bb09\npython $oadir/xor_codec.py oasst-sft-7-llama-30b/ $oadir/oasst-sft-7-llama-30b-xor/ LLaMA30B\nls -alrt oasst-sft-7-llama-30b/ # check\ncd ~/h2ogpt/\nln -s <location of oasst-sft-7-llama-30b> ."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-05T23:03:07Z",
        "message": "Specify chat separator"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-05T19:08:35Z",
        "message": "Default to plain if nothing set in lookup"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-04T06:46:35Z",
        "message": "Fix extra import"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-04T06:36:40Z",
        "message": "Work around suspected bug in peft (empty lora weights at end of training)."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-04T00:19:31Z",
        "message": "Add h2ogpt-gm models, and prompt_answer prompt_type."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-05-01T09:05:42Z",
        "message": "Give default context to help chatbot"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-30T20:40:38Z",
        "message": "improve control of devices"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-27T20:57:19Z",
        "message": "directly control save_total_limit"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-27T20:55:36Z",
        "message": "Flush log"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-27T20:23:05Z",
        "message": "Dup if mixin needs to match training data, and allow prune of token-truncation rows to avoid learning from truncated language"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-27T18:49:03Z",
        "message": "Refactor finetune so some of it can be used to check data and its tokenization"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-26T20:38:51Z",
        "message": "Update peft imports"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-26T08:24:05Z",
        "message": "Raise if asked for flash attn but don't have it."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-26T08:15:34Z",
        "message": "Refactor a bit to avoid early global imports to control when flash attention is done, so early on.\n\nNOTE: flash attention requires installing cuda 11.7 via https://developer.nvidia.com/cuda-11-7-0-download-archive?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=runfile_local and then when running, to avoid installing driver, docs, samples, just install toolkit.  Then when pip installing flash attention do:\n\nCUDA_HOME=/usr/local/cuda-11.7 pip install flash-attn"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-26T07:03:23Z",
        "message": "Add llama flash attention. Add option to disable lora (with lora_r=0)"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-26T06:30:34Z",
        "message": "Go back to 512 cutoff len."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-26T05:41:08Z",
        "message": "Add distilgpt2 lora target, like gpt2"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-26T03:58:13Z",
        "message": "use autocausal for distilgpt2"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-26T03:33:06Z",
        "message": "Merge pull request #85 from h2oai/refactor1\n\nRefactor gradio into separate file and isolate it from torch specific stuff"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-26T03:00:45Z",
        "message": "Refactor gradio into separate file and isolate it from torch specific stuff"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-26T02:59:40Z",
        "message": "Improve error message, pass default dataset."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-26T00:22:30Z",
        "message": "Add option to train with 8-bit, enabled by default."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-24T22:29:29Z",
        "message": "Fix context to have space when using reduce mode, so human-bot consistent in context"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-23T17:38:15Z",
        "message": "Updates to model cards and default model selection."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-22T09:08:28Z",
        "message": "Allow system pre-prompt context in non-chat mode, hide context in chat mode since used for history\n\ne.g. could add to chat=False case a context of:\n\n<bot>: I am a helpful assistant that can generate detailed elaborate question from topics such as biology, physics, mathematics, psychology, or computer science.\n\nAnd then one gives the instruction:\n\nI am a person seeking assistance from a bot.  Please generate an elaborate question.\n\nAnd that will force the bot to behave a certain way because it has already effectively responded that way."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-21T22:00:26Z",
        "message": "Change default model."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-21T19:35:02Z",
        "message": "Add some h2oGPT models to finetune/generate examples."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-21T03:01:45Z",
        "message": "Undo change to PreResponse, makes it no longer speak English by default...?"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-21T02:44:14Z",
        "message": "Merge pull request #69 from h2oai/save-output\n\nAdd option to save prompt and response as .json."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-21T02:01:47Z",
        "message": "Add option to save prompt and response as .json."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-21T00:33:05Z",
        "message": "Notes about llama 65B"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-19T07:16:58Z",
        "message": "Add code to push spaces chatbot"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-18T05:53:54Z",
        "message": "Update model list."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-18T05:51:59Z",
        "message": "Clean up more."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-17T17:36:25Z",
        "message": "Add links to HF."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-17T17:10:21Z",
        "message": "Add h2ogpt-oig-oasst1-256-12b to list."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-17T12:42:22Z",
        "message": "Don't require HF token"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-17T12:07:37Z",
        "message": "Clean-up LORA part with controlled visibility, and tell from_pretrained that can use_auth_token=True but no models online yet (even private which would then have worked)"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-17T08:43:48Z",
        "message": "Terminate vicuna output even if keeps generating, until work-around non-uniqueness of its tokens"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-17T08:36:23Z",
        "message": "Work-around gr.State() deepcopy https://github.com/gradio-app/gradio/issues/3558 of model/tokenizer when load model up front from CLI.  But can't seem to get memory off GPU, even .cpu().  Even if references are in another thread, why shouldn't .cpu() + clear cache + collect work?"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-17T07:21:10Z",
        "message": "Make human_bot not add the Date/Time."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-17T03:22:22Z",
        "message": "Simplify handling of stopping words since not all extra can be removed, just remove pad if in front.  Add dolly and instruct_with_end."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-17T00:41:12Z",
        "message": "update transformers and remove assert, and fix use of special tokens in stopping for cases when object not str"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-16T23:48:35Z",
        "message": "Merge pull request #40 from h2oai/instruct_vicuna\n\nAnother prompt + fix gradio add with state"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-16T21:46:59Z",
        "message": "Add another prompt type, but prompt parsing is prone to mistakes, best to use unique token or stop generation on text if prompts are text-based."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-16T18:19:21Z",
        "message": "Add pythia models."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-15T20:38:04Z",
        "message": "Add score model, defaulting to https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2\n\nStill some problems with model/data on different devices for multi-GPU. Infer helps some for non-lora case, since often puts on 1 GPU, even if 20B could have been on multi-GPU spread."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-15T10:28:06Z",
        "message": "If no prompt_type at start, but know about model, choose reasonable one"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-15T10:12:21Z",
        "message": "Comment-out WIP"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-15T10:10:52Z",
        "message": "Allow fast start without model, show message if user didn't load yet"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "finetune.py",
        "commit_date": "2023-04-15T09:37:05Z",
        "message": "Support model control at run-time, including loading models/loras, adding new HF models/loras.  Ensure GPU memory cleared between switching, including cache."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/ia3/model.py",
        "commit_date": "2024-02-20T14:12:34Z",
        "message": "FIX Correctly unload double wrapped modules (#1490)\n\nResolves #1485, but note that some additional solutions are mentioned in\nthet issue.\n\nThis checks that when unloading a PEFT model, if the\nModulesToSaveWrapper contains a tuner module, it is correctly unloaded.\nThe unloaded model should not have PEFT layers at the end."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/ia3/model.py",
        "commit_date": "2024-02-08T13:39:46Z",
        "message": "DOC How to freeze adapter after set_adapter call (#1447)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/ia3/model.py",
        "commit_date": "2024-02-07T11:52:35Z",
        "message": "MNT Move code quality fully to ruff (#1421)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/ia3/model.py",
        "commit_date": "2024-01-09T11:18:31Z",
        "message": "Refactor dispatching logic of LoRA layers (#1319)\n\nThis PR's goal is to simplify the logic for deciding which LoRA layer\nbackend is being used when LoRA is applied to a target layer.\n\nOriginally, this refactor was done in #1286 which was about adding the\n\"fast\" backend for LoRA, but since that PR was closed, I moved the\nrefactor to this dedicated PR.\n\nMotivation\n\nRight, now, the LoraModel._create_new_module method has become quite\ncomplex and hard to read, spanning >100 lines:\n\nhttps://github.com/huggingface/peft/blob/8665e2b5719faa4e4b91749ddec09442927b53e0/src/peft/tuners/lora/model.py#L235-L339\n\nThe reason for this is that method contains the logic for deciding which\nLoRA layer backend to use for all the different types of LoRA layers\nthat we have, i.e. normal Linear layer, Conv2d layer, bnb layer, gptq,\netc.\n\nDescription\n\nTo remedy this, I moved the logic for deciding which layer to match to\nthe respective implementation of the layers. For example, in\nlora/layer.py, there is now a function called dispatch_default, whose\nresponsibility it is to decide if an Embedding layer, Conv2d layer or\nLinear layer is the right match. Similarly, in lora/bnb.py, there are\nnow the two functions dispatch_bnb_8bit and dispatch_bnb_4bit to decide\nwhat/if any bnb 8bit or 4bit layer should be matched.\n\nThis way, the logic to decide what layer to match now resides next to\nthe respective layers. The only thing that LoraModel now needs to do is\nto collect all the dispatching methods and use the first layer that\nmatches.\n\nNote that only LoRA was modified, the other tuners don't have different\nbackends and thus this approach was not necessary for them. The only\nexception is IA\u00b3, which has the normal and bnb backend. Since those are\nonly 2, it's not as complicated as for LoRA, but if this PR is accepted,\nI can refactor IA\u00b3 in a similar fashion.\n\nOther changes\n\n- Removed the optional_kwargs argument from _create_and_replace, as it\n  was an unnecessary indirection.\n- Removed the bias argument from kwargs, as it was not used.\n\nBackwards compatibility\n\nThis should be fully backwards compatible, as the constructed LoRA model\nis 100% the same. If there are users that override _create_new_module,\ntheir code will probably break, but since this is a private method, we\nshould be fine."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/ia3/model.py",
        "commit_date": "2023-12-18T09:59:17Z",
        "message": "Refactor and a couple of fixes for adapter layer updates (#1268)\n\n* Refactor: Move LoRA update_layer to child classes\n\nFor LoRA, so far, we have update_layer for Linear,\nupdate_layer_embedding for Embedding, and update_layer_conv2d for\nConv2d, all defined on LoraLayer.\n\nWe can simplify the code by always using the name update_layer, and by\nmoving the layer-specific methods to the subclasses. So e.g.\nupdate_layer_embedding is moved to the Embedding class and renamed to\nupdate_layer. This way, the caller does not need to differentiate which\ntype of layer it's calling.\n\nInterestingly, this was already practiced for IA\u00b3, so the same change\nwas not necessary there. But I did find the same method implemented\ntwice, once on IA3Layer and once on Linear, so I removed one of the\nduplicates\n\n* Systematic handling of r (rank) <= 0\n\nAlways raise an error when r <= 0, not only for LoRA. Also, removed\nlater check for r > 0 in LoRA layers, since we already check for r <= 0.\n\n* Fix broken __repr__ method on QuantLinear\n\nWas indented too deep, thus not being applied.\n\n* Fix bug for updating Lora GPTQ and IA3 bnb layers\n\nBefore this fix, when adding a 2nd adapter to a model, we did not\ncorrectly check if there was already an adapter layer in the model when\ndealing with LoRA GPTQ or IA3 bnb layers. As a consequence, instead of\nupdating the existing layers, we would create a new layer and the\nexisting layer would be set as the base_layer of that new layer. Now, we\ncorrectly update the existing layer to add the new adapter.\n\nNote that for this fix to work correctly with LoRA and GPTQ, I had to\nadd a check for qweight, since we only checked for weight before.\n\nTests were added to check this. They fail with the current main but are\nfixed with this PR.\n\n* Don't match AdaLoraLayer when updating LoraLayers\n\nAdaLoraLayer is a subclass of LoraLayer, so just checking for\nisinstance(target, LoraLayer) will match AdaLoraLayer, which we don't\nwant when it comes to updating a LoraLayer. Now, we explicitly check\nthat the layer is *not* an instance of AdaLoraLayer."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/ia3/model.py",
        "commit_date": "2023-12-12T14:34:45Z",
        "message": "Fix: Multiple adapters with bnb layers (#1243)\n\nResolves #1239\n\nFixes a bug that led to an error when loading multiple adapters into a\npeft model that uses bnb layers.\n\nAlso: Fix for loading 2nd adapter with AutoGPTQ"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/ia3/model.py",
        "commit_date": "2023-12-11T11:35:28Z",
        "message": "FIX Use model argument consistently (#1198) (#1205)\n\nSome methods were using model and self.model interchangeably. This was\nfine, as they were referring to the same object, but is also confusing.\nNow model is used consistently."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/ia3/model.py",
        "commit_date": "2023-12-07T15:39:08Z",
        "message": "Lazy import of bitsandbytes (#1230)\n\nPreviously, we imported from bitsandbytes eagerly if the package was\ninstalled. This caused two major issues:\n\n- Slow loading time of PEFT (~4 sec)\n- Errors with multiprocessing because bnb initializes CUDA\n\nThis commit fixes both issues by importing bitsandbytes lazily. PEFT\nimport time is now reduced to ~2sec.\n\nNotes\n\nImplementation-wise, I use a combination of local imports and\nmodule-level __getattr__. The latter was introduced in Python 3.7 and\nshould therefore be safe to use."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/ia3/model.py",
        "commit_date": "2023-12-06T13:44:58Z",
        "message": "Raise error when `modules_to_save` is specified and multiple adapters are being unloaded (#1137)\n\n* handle `modules_to_save` when unloading\n\n* address comments\n\n* Apply suggestions from code review\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* quality\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/ia3/model.py",
        "commit_date": "2023-12-04T11:22:03Z",
        "message": "DOC: Update & improve docstrings and type annotations for common methods and classes (#1201)\n\nThe docstrings of the most user-exposed methods and classes have been\nupdated, or added if not already present. Furthermore, type annotations\nhave been updated or added for those methods and classes.\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/ia3/model.py",
        "commit_date": "2023-11-20T17:22:52Z",
        "message": "ENH Delete IA3 adapters (#1153)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/ia3/model.py",
        "commit_date": "2023-11-16T11:45:12Z",
        "message": "Refactor base layer pattern (#1106)\n\nDescription\n\nRefactor all tuners (where it applies, i.e. not prompt tuning) to use\nthe \"base layer pattern\". This means that the adapter layer will always\nhold a reference to the original layer that it modifies. This pattern is\nalready partly used (e.g. LoRA bnb, gptq layers), now it is consistently\nused everywhere when applicable.\n\nThis PR is a companion PR to #1069, where I first added these changes.\nThey are now extracted to a separate PR to make code review easier and\nto advance more quickly.\n\nImplementation\n\nThe main change is that the adapter layer wraps the original layer and\ncalls forward on that layer, instead of doing stuff like this:\n\nF.linear(input, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n\nwhich completely circumvents the call to the target layer's forward\nmethod. With the base layer pattern, we now call the target layer's\nforward method. Therefore, if the target layer is another adapter\nlayer (which will be crucial for mixed adapters), we call its forward\nmethod correctly. Also, this should allow passing extra arguments, like\nlora_scale to forward.\n\nThis change has the nice side benefit that we no longer need to use\n_init_empty_weights -- in fact, we don't initialize any of the target\nlayer's weights anymore, since we have a reference to it. There is thus\nno risk of having slow but superfluous initialization of layers.\n\nMoreover, I could greatly simplify merge_and_unload by just using the\nbase_layer instead of having to create a completely new layer. For\nOPT-350m, this results in a 15x speedup.\n\nNote that same as for the bnb layers, this should be backwards\nincompatible, since the adapter weights and their state_dicts are not\naffected by this change. I used #1115 for regression testing.\n\nSomewhat unrelated changes\n\nDuring debugging, I got very annoyed with the fact that the reprs of\nadapter layers and normal PyTorch layers are hard to distinguish, e.g.\nthe type is just \"Linear\". Now, for adapter layers, it is prefixed by\nthe adapter type, e.g. \"lora.Linear\". This should have no further\nimplications except for the repr (e.g. state_dict remains unaffected).\n\nFor LoHa and LoKr, I had to change the init of weights when using\ninit_weights=False. This is because of what is discussed in Numerical\ninstabilities with LoHa #1058.\n\nIA\u00b3 now has the unload method too.\n\nLoHa and LoKr now support safe_merge=True when merging layers.\n\nMigration guide\n\nFor 99% of users, the code should continue working as ususal, because\nthe API stays the same. Only low level details have been changed.\n\nCode that relies on isinstance checks on specific PEFT classes may\nbreak. E.g. the LoRA Linear layer no longer inherits from nn.Linear. It\nis, however, still a BaseTunerLayer. The same logic applies for other\nlayer types like Conv2d and for other tuners like IA\u00b3.\n\nTo retrieve the base layer of an adapter layer, you should now call\nmodule.get_base_layer() if you deal with a BaseTunerLayer. Don't rely on\nsomething like module.weight being present (though it might be)."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/ia3/model.py",
        "commit_date": "2023-11-16T11:05:22Z",
        "message": "FEAT: Merging only specified `adapter_names` when calling `merge` (#1132)\n\n* working v1\n\n* add tests\n\n* remove\n\n* add it also for lokr and loha, left a todo\n\n* Update tests/testing_common.py\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>\n\n* better test\n\n* up\n\n* fix tests\n\n* credits contrib and suggestions from disscussions\n\n* credits contrib and suggestions from disscussions\n\n* address last comments\n\n---------\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/ia3/model.py",
        "commit_date": "2023-11-10T12:33:56Z",
        "message": "Refactor adapter deletion (#1105)\n\nDescription\n\nThe job of deleting an adapter is now transferred to the adapter layer,\ninstead of the adapter model. This makes it easier for users or other\nlibraries who don't use the adapter model to delete adapters.\n\nImplementation\n\nThe code should now be more generic, relying less on hard-coded\nattributes.\n\nAs a precaution, I also changed the type of adapter_layer_names from\nlist to tuple, as it should not be mutated.\n\nWhen deleting the active adapter, the logic for choosing the new active\nadapter has been changed slightly to ensure consistency across layers.\nIn practice, this should rarely make a difference. An error is now\nraised if the last remaining adapter is deleted.\n\nTest coverage has been increased:\n\n- Deleting adapters is now also tested for custom models.\n- It is also tested for LoHa, LoKr, not only LoRA.\n- I added a test for deleting the non-active adapter.\n\nNot implemented\n\nI did not add adapter deletion to IA\u00b3, since it is included in #980. LMK\nif it should be added here instead."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/ia3/model.py",
        "commit_date": "2023-11-01T10:39:40Z",
        "message": "TST test coverage for layer matching (#1031)\n\nAdd tests for module name matching using regex and other custom arguments."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/ia3/model.py",
        "commit_date": "2023-10-26T13:51:49Z",
        "message": "FIX Conv1D merge error for IA3 (#1014)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/ia3/model.py",
        "commit_date": "2023-10-09T16:28:00Z",
        "message": "FEAT: Add `safe_merge` option in `merge` (#1001)\n\n* add `safe_merge` option in `merge`\n\n* oops\n\n* Apply suggestions from code review\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* address final comments\n\n* Update src/peft/tuners/lora/layer.py\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* Update src/peft/tuners/lora/layer.py\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* add it for ia3\n\n* add it for adalora\n\n* up\n\n* revert for loha\n\n* style\n\n* fix CI\n\n* adapt from suggestions\n\n* add tests\n\n* up\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/ia3/model.py",
        "commit_date": "2023-10-09T10:20:19Z",
        "message": "ENH Support Conv2d layers for IA\u00b3 (#972)\n\nAdds support for Conv2D layers to the IA\u00b3 tuner. Tests are added to\ncheck that they work.\n\nNotes:\n\nUnfortunately, when unmerging the Conv2d IA\u00b3 layers, there is quite a\nbit of rounding error. I had to increase the tolerances for this\nspecific test case to make the tests pass. I'm not 100% sure why this\nis, but I could imagine that for Conv2d, small errors accumulate because\nof the convolution operation.\n\nI also added tests for IA\u00b3 Linear layers for the custom models, which\nalso pass. However, there is an error when using Conv1D. The reason is\nthat merging fails because there is a shape mismatch when\nfan_in_fan_out=True (which is set automatically for Conv1D). This is\nleft for a future PR."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/ia3/model.py",
        "commit_date": "2023-10-05T07:57:49Z",
        "message": "Fix lora creation (#993)\n\n* reducing the time for inject lora modules\n\n* fix bugs\n\n* fix bug\n\n* fixes\n\n* Revert \"fixes\"\n\nThis reverts commit c7f30627c1798db11be8a5da8f3c801f9469a5e3.\n\n* refactor\n\n* fix failing tests\n\n* fix tests\n\n* fix tests\n\n* fix tests\n\n* fix tests\n\n* Apply suggestions from code review\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* address comments\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/ia3/model.py",
        "commit_date": "2023-09-29T04:14:30Z",
        "message": "[tests] add multiple active adapters tests (#961)\n\n* add tests for multiple active adapters\n\n* add multiple active adapter tests\n\n* fix tests\n\n* fix the device error\n\n* fix typo\n\n* fix the variables\n\n* fix the `adalora` config\n\n* add util function for proper naming of tests\n\n* fix bugs\n\n1. fix `add_weighted_adapter` when working with adapters targeting different layers\n2. fix `ia3` model and layer to handle adapters targeting different layers\n3. fix the multiple active adapter tests\n\n* fix `ia3` issue\n\n* remove debug statements\n\n* fix test\n\n* fix bug\n\n* address comments\n\n* address comments\n\nCo-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* fix tests\n\n* remove unused code\n\n* Update test_custom_models.py\n\n* increasing tolerance for a test\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/ia3/model.py",
        "commit_date": "2023-09-26T12:11:32Z",
        "message": "ENH Add 4-bit support for IA3 (#864)\n\nNotes:\n\n- Add guard to IA\u00b3 Linear8bitLt definition (should have already been there).\n- Merging not supported (yet)."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/ia3/model.py",
        "commit_date": "2023-09-26T07:31:05Z",
        "message": "FIX: setting requires_grad on adapter layers (#905)\n\n* [WIP] Fix setting requires_grad on adapter layers\n\nThis is an alternative to #900, resolves #899.\n\nDescription\n\nCurrently, we don't handle setting requires_grad on adapter layers\nreally well. The main issue is that it can be set to True on adapter\nparameters that are not being used, e.g. the original_module in\nModulesToSaveWrapper or inactive adapters in LoRA.\n\nNormally, this is not a big issue, except maybe if we want to correctly\ncount the number of trainable parameters. However, when training with\nDistributedDataParallel, this results in errors, as PyTorch thinks that\nall parameters with requires_grad=True should participate in the loss\ncomputation, but those mentioned parameters don't. For that reason,\ntraining with DDP currently fails when using modules_to_save or multiple\nadapters.\n\nImplementation\n\nThis turned out to be more complicated than I initially thought. The\nlogic for setting requires_grad is all over the place, it was hard to\nencapsulate the logic and I only succeeded partially. As is, this PR is\nmore complex than the one it tries to supersede, #900, but it is also\n\"more correct\".\n\nTests were added to check whether requires_grad is set correctly. There\nare (so far) no tests for whether DDP indeed works, they could be added\nwith multi-GPU. I did, however, test an early stage of this PR with DDP\nand setting requires_grad correctly will indeed fix the DDP error.\n\nDONE/TODO\n\n- [x] ModulesToSaveWrapper\n- [x] LoRA\n- [ ] IA\u00b3\n- [ ] AdaLora\n\nSince some tuners are not implemented yet, tests are expected to fail.\nCheck the new tests at the bottom of test_custom.py, those should pass.\n\n* Refactor: move more requires_grad machinery to ABC\n\n* [skip ci] [WIP] Add requires_grad logic to IA\u00b3\n\n* Add AdaLora\n\n* Fix some minor issues\n\n* Make style"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/ia3/model.py",
        "commit_date": "2023-09-22T20:03:44Z",
        "message": "support multiple ranks and alphas for LoRA (#873)\n\n* support multiple ranks and alphas\n\n* Update lora.py\n\n* Update lora.py\n\n* commit suggestions\n\nCo-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* address comments\n\nCo-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* Fixed multirank + multialpha for sequential LoRAs, added correct support of LoRA-C3Lier conversion (#937)\n\n* Fixed multirank multialpha for sequential loras, added tests, fixed docs\n\n* Refactored kohya_ss conversion script for proper support of LoRA-C3Lier\n\n* Fixed styling\n\n* Removed old comment from docstring\n\n* shift `scale_layer`/`unscale_layer` to `LoraLayer` class to support all the child classes\n\n* support multiple active adapters\n\n* add `active_adapters` property\n\nCo-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* fix bug related to active adapter of `ModulesToSaveWrapper`\n\n* revert the change wrt active_adapter assignment\n\nCo-Authored-By: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* addressing comments\n\n* address comments\n\n* address comment\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\nCo-authored-by: Alexander Kovalchuk <kovalexal@gmail.com>\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/ia3/model.py",
        "commit_date": "2023-08-29T09:32:29Z",
        "message": "MNT: Move tuners to subpackages (#807)\n\nFor each tuner, created a sub-module that contains at least:\n\n- config.py for config stuff\n- model.py for the actual model/encoder/embedding\n- __init__.py so that imports are preserved\n\nThen, when there was a need, further files were created, like layer.py\nor utils.py.\n\nImports were changed to absolute imports everywhere, except for the\nsub-packages within a tuner directory, as these packages will always \nstay together in the same place.\n\nFor some existing modules, the license comment of the top of the file\nwas missing, I always added it.\n\nThere was a bug in the forward method of 4bit linear lora layers introduced\nin #851, for the case that the model is merged AND adapters are disabled.\nFor that scenario, we need to unmerge first before generating the output,\nsame as we do for the vanilla Linear layer. This step was missing from the\ncode previously and is now implemented correctly. Tests were adjusted to\ncatch that error."
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/utils/llm_utils.py",
        "commit_date": "2024-02-27T21:48:50Z",
        "message": "fix: use eos token in target tensor for instruction-tuning (#3945)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/utils/llm_utils.py",
        "commit_date": "2024-01-23T17:52:25Z",
        "message": "Add default LoRA target modules for Phi-2 (#3911)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/utils/llm_utils.py",
        "commit_date": "2024-01-22T19:13:10Z",
        "message": "Revert \"Revert \"Add support for the official `microsoft/phi-2` model \u2026 (#3901)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/utils/llm_utils.py",
        "commit_date": "2024-01-18T23:02:39Z",
        "message": "Revert \"Add support for the official `microsoft/phi-2` model (#3880)\" (#3898)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/utils/llm_utils.py",
        "commit_date": "2024-01-18T18:03:51Z",
        "message": "Add support for the official `microsoft/phi-2` model (#3880)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/utils/llm_utils.py",
        "commit_date": "2024-01-11T21:51:21Z",
        "message": "Add streaming support for zero shot inference (#3878)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/utils/llm_utils.py",
        "commit_date": "2023-12-18T19:08:56Z",
        "message": "Add support for Phi 2 (#3838)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/utils/llm_utils.py",
        "commit_date": "2023-12-15T00:04:03Z",
        "message": "Add LLM Text Encoder (#3828)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/utils/llm_utils.py",
        "commit_date": "2023-12-14T20:58:06Z",
        "message": "Add support for Phi-1 and Phi 1.5 (#3831)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/utils/llm_utils.py",
        "commit_date": "2023-12-12T04:01:34Z",
        "message": "Move reusable LLM model methods to utility functions (#3821)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/utils/llm_utils.py",
        "commit_date": "2023-11-20T23:13:17Z",
        "message": "Cleanup: Use existing HFTokenizer to consolidate manual pad token setting. (#3774)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/utils/llm_utils.py",
        "commit_date": "2023-10-25T21:00:45Z",
        "message": "Fix: Prevent memory from ballooning during post-training evaluation (#3756)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/utils/llm_utils.py",
        "commit_date": "2023-10-13T08:17:48Z",
        "message": "Dynamically set `max_new_tokens` based on output feature length, GMSL and model window size (#3713)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/utils/llm_utils.py",
        "commit_date": "2023-09-12T21:07:51Z",
        "message": "Add codellama to tokenizer list for set_pad_token (#3598)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/utils/llm_utils.py",
        "commit_date": "2023-09-08T17:36:19Z",
        "message": "Add support for Paged Optimizers (Adam, Adamw), 8-bit optimizers, and new optimizers: LARS, LAMB and LION (#3588)\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/utils/llm_utils.py",
        "commit_date": "2023-08-25T20:18:23Z",
        "message": "Refactor evaluation metrics to support decoded generated text metrics like BLEU and ROUGE. (#3539)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/utils/llm_utils.py",
        "commit_date": "2023-06-29T07:38:49Z",
        "message": "[llm] fix device placement issues when using CPUs and GPUs during LLM fine tuning (#3447)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/utils/llm_utils.py",
        "commit_date": "2023-06-23T18:24:35Z",
        "message": "[LLM] Various fixes for LLM Fine-Tuning issues that caused loss disparity between train and val sets (#3437)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "ludwig/utils/llm_utils.py",
        "commit_date": "2023-06-07T07:42:32Z",
        "message": "[LLM] Skip left padding removal when there is no left padding (#3432)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2024-02-27T11:02:11Z",
        "message": "FEAT Implement DoRA (#1474)\n\nAdd DoRA (Weight-Decomposed Low-Rank Adaptation).\n\nhttps://arxiv.org/abs/2402.09353\n\nTo use this with LoRA, add use_dora=True to the LoraConfig.\n\nCurrently only supports nn.Linear layers, not other types or\nquantized linear layers like bnb."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2024-02-22T01:31:04Z",
        "message": "AQLM support for LoRA (#1476)\n\n* aqlm\n\n* Style and copied tests\n\n* aqlm import guadr\n\n* docs\n\n* correct model in tests\n\n* Update docs/source/developer_guides/quantization.md\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* Update docs/source/developer_guides/quantization.md\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* moved aqlm install and added >=\n\n* Removed `quant_linear_module`\n\n* AqlmLoraLinear\n\n* docs update\n\n* transformers version check\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2024-02-20T14:12:34Z",
        "message": "FIX Correctly unload double wrapped modules (#1490)\n\nResolves #1485, but note that some additional solutions are mentioned in\nthet issue.\n\nThis checks that when unloading a PEFT model, if the\nModulesToSaveWrapper contains a tuner module, it is correctly unloaded.\nThe unloaded model should not have PEFT layers at the end."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2024-02-19T00:31:21Z",
        "message": "FEAT: add awq suppot in PEFT (#1399)\n\n* add awq suppot in PEFT\n\n* fix\n\n* fux\n\n* Update src/peft/tuners/lora/awq.py\n\n* style & fix tests\n\n* forward contrib credits from PR14084\n\n* forward contrib credits from autoawq PR\n\n* change name\n\n* fix\n\n* change to peft internal testing\n\n* fix\n\n* fix\n\n* add multi-GPU tests\n\n* add to dockerfile\n\n* fix todo\n\n* raise error only at the dispatch level\n\n* quality\n\n* fix test\n\n* fix dockerfile\n\n* fix\n\n* fix\n\n* update dockerfile and tests\n\n---------\n\nCo-authored-by: s4rduk4r <s4rduk4r@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2024-02-15T12:29:39Z",
        "message": "add `magnitude_prune` merging method (#1466)\n\n* add `magnitude_prune` merging method\n\n* Update model.py\n\n* \ud83d\ude05"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2024-02-12T17:31:15Z",
        "message": "[docs] Docstring typo (#1455)\n\n* fix typo\n\n* fix"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2024-02-09T06:40:04Z",
        "message": "Add new merging methods (#1364)\n\n* add code\n\n* update docstring\n\n* quality\n\n* fix test\n\n* fix test\n\n* fix svd embedding layer merging\n\n* fixes\n\n* fixes\n\n* Update model.py\n\n* Add test and example\n\n* quality\n\n* fix tests\n\n* update the example\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* address comments\n\n* address comments and add co-authors\n\nCo-Authored-By: Prateek Yadav <15224633+prateeky2806@users.noreply.github.com>\nCo-Authored-By: Yu Le <55241218+yule-buaa@users.noreply.github.com>\nCo-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* quality\n\n* Update merge_utils.py\n\n* revert\n\n* address comments\n\n* address comment\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\nCo-authored-by: Prateek Yadav <15224633+prateeky2806@users.noreply.github.com>\nCo-authored-by: Yu Le <55241218+yule-buaa@users.noreply.github.com>\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2024-02-08T13:39:46Z",
        "message": "DOC How to freeze adapter after set_adapter call (#1447)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2024-02-07T11:52:35Z",
        "message": "MNT Move code quality fully to ruff (#1421)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2024-02-06T00:54:06Z",
        "message": "Fix typos (#1435)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2024-01-09T11:18:31Z",
        "message": "Refactor dispatching logic of LoRA layers (#1319)\n\nThis PR's goal is to simplify the logic for deciding which LoRA layer\nbackend is being used when LoRA is applied to a target layer.\n\nOriginally, this refactor was done in #1286 which was about adding the\n\"fast\" backend for LoRA, but since that PR was closed, I moved the\nrefactor to this dedicated PR.\n\nMotivation\n\nRight, now, the LoraModel._create_new_module method has become quite\ncomplex and hard to read, spanning >100 lines:\n\nhttps://github.com/huggingface/peft/blob/8665e2b5719faa4e4b91749ddec09442927b53e0/src/peft/tuners/lora/model.py#L235-L339\n\nThe reason for this is that method contains the logic for deciding which\nLoRA layer backend to use for all the different types of LoRA layers\nthat we have, i.e. normal Linear layer, Conv2d layer, bnb layer, gptq,\netc.\n\nDescription\n\nTo remedy this, I moved the logic for deciding which layer to match to\nthe respective implementation of the layers. For example, in\nlora/layer.py, there is now a function called dispatch_default, whose\nresponsibility it is to decide if an Embedding layer, Conv2d layer or\nLinear layer is the right match. Similarly, in lora/bnb.py, there are\nnow the two functions dispatch_bnb_8bit and dispatch_bnb_4bit to decide\nwhat/if any bnb 8bit or 4bit layer should be matched.\n\nThis way, the logic to decide what layer to match now resides next to\nthe respective layers. The only thing that LoraModel now needs to do is\nto collect all the dispatching methods and use the first layer that\nmatches.\n\nNote that only LoRA was modified, the other tuners don't have different\nbackends and thus this approach was not necessary for them. The only\nexception is IA\u00b3, which has the normal and bnb backend. Since those are\nonly 2, it's not as complicated as for LoRA, but if this PR is accepted,\nI can refactor IA\u00b3 in a similar fashion.\n\nOther changes\n\n- Removed the optional_kwargs argument from _create_and_replace, as it\n  was an unnecessary indirection.\n- Removed the bias argument from kwargs, as it was not used.\n\nBackwards compatibility\n\nThis should be fully backwards compatible, as the constructed LoRA model\nis 100% the same. If there are users that override _create_new_module,\ntheir code will probably break, but since this is a private method, we\nshould be fine."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2024-01-09T05:31:30Z",
        "message": "Extend merge_and_unload to offloaded models (#1190)\n\n* activated pre-forward\n\n* activated pre-forward hook\n\n* activated pre-forward hook\n\n* activated pre-forward hook\n\n* debugged hook call\n\n* added explicit forwards\n\n* debugged\n\n* debugged\n\n* fixed pre-forward hook call\n\n* fixed pre-forward hook call\n\n* debugged module iteration\n\n* fixed post forward args\n\n* added conditional attr check\n\n* fixed conditional attr check\n\n* memory overflow debug\n\n* memory overflow debug\n\n* added mem trace\n\n* added mem trace\n\n* more memory traces\n\n* debug memory leak\n\n* debug memory leak\n\n* removed replace\n\n* removed device assign during replacement\n\n* no grad during replacement\n\n* new module hook\n\n* to cpu\n\n* to cpu\n\n* removed replace module\n\n* conditional on replace module\n\n* removed traces\n\n* make style\n\n* added back replace_module\n\n* added test and make style\n\n* inline key, module\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* fixed test and make style\n\n* reverted _unload_and_optionally_merge and moved test\n\n* match main\n\n* make style\n\n* reverted model.py\n\n* make style\n\n* reverted merge\n\n* fetched model.py from head\n\n* added onload\n\n* debug\n\n* removed replace module\n\n* removed replace module\n\n* pre forward on target and parent\n\n* removed _replace_module\n\n* reverted\n\n* debugged\n\n* debugged\n\n* traced adapters\n\n* debugged\n\n* added trace on adapter names\n\n* onloaded target\n\n* further traces\n\n* further traces\n\n* further traces\n\n* further traces\n\n* further traces\n\n* onloaded adapters\n\n* onload module\n\n* onload module\n\n* onload module\n\n* debugged\n\n* debugged\n\n* debugged\n\n* removed delta weight onload\n\n* revamped delta weight onload\n\n* revamped delta weight onload\n\n* removed replace module\n\n* added parent and target act\n\n* debugged\n\n* debugged\n\n* added traces\n\n* added traces\n\n* added traces\n\n* init hook\n\n* init hook\n\n* traces\n\n* traces\n\n* specd weights map\n\n* removed traces and offload check\n\n* post forwards on lora\n\n* added post forward for target and parent\n\n* added trace\n\n* removed traces and tp post forwards\n\n* added onloads and offloads to embedding and conv2d\n\n* updated test\n\n* make style\n\n* debugged and make style\n\n* refactored and make style\n\n* cleaned\n\n* refactored and make style\n\n* cleaned\n\n* cleaned\n\n* make style\n\n* make style\n\n* disk offload compatibility\n\n* refactored linear onload via contextmanager\n\n* refactored onloads\n\n* debugged\n\n* tempfile to tempfolder\n\n* changed disk offload to original directory\n\n* refactored for general tuners\n\n* debugged\n\n* explicit base layer\n\n* added base traces\n\n* more traces\n\n* debugged;\n\n* reverted lora layer.py\n\n* removed traces and make style\n\n* cleaned\n\n* removed todo\n\n* fixed test and cleaned\n\n* added suggestions and make style\n\n* onload for unmerge and merge_and_unload\n\n* improved docstring\n\n* onload target only and make style\n\n* Update src/peft/tuners/tuners_utils.py\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* revised descriptions\n\n* make style\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-12-18T09:59:17Z",
        "message": "Refactor and a couple of fixes for adapter layer updates (#1268)\n\n* Refactor: Move LoRA update_layer to child classes\n\nFor LoRA, so far, we have update_layer for Linear,\nupdate_layer_embedding for Embedding, and update_layer_conv2d for\nConv2d, all defined on LoraLayer.\n\nWe can simplify the code by always using the name update_layer, and by\nmoving the layer-specific methods to the subclasses. So e.g.\nupdate_layer_embedding is moved to the Embedding class and renamed to\nupdate_layer. This way, the caller does not need to differentiate which\ntype of layer it's calling.\n\nInterestingly, this was already practiced for IA\u00b3, so the same change\nwas not necessary there. But I did find the same method implemented\ntwice, once on IA3Layer and once on Linear, so I removed one of the\nduplicates\n\n* Systematic handling of r (rank) <= 0\n\nAlways raise an error when r <= 0, not only for LoRA. Also, removed\nlater check for r > 0 in LoRA layers, since we already check for r <= 0.\n\n* Fix broken __repr__ method on QuantLinear\n\nWas indented too deep, thus not being applied.\n\n* Fix bug for updating Lora GPTQ and IA3 bnb layers\n\nBefore this fix, when adding a 2nd adapter to a model, we did not\ncorrectly check if there was already an adapter layer in the model when\ndealing with LoRA GPTQ or IA3 bnb layers. As a consequence, instead of\nupdating the existing layers, we would create a new layer and the\nexisting layer would be set as the base_layer of that new layer. Now, we\ncorrectly update the existing layer to add the new adapter.\n\nNote that for this fix to work correctly with LoRA and GPTQ, I had to\nadd a check for qweight, since we only checked for weight before.\n\nTests were added to check this. They fail with the current main but are\nfixed with this PR.\n\n* Don't match AdaLoraLayer when updating LoraLayers\n\nAdaLoraLayer is a subclass of LoraLayer, so just checking for\nisinstance(target, LoraLayer) will match AdaLoraLayer, which we don't\nwant when it comes to updating a LoraLayer. Now, we explicitly check\nthat the layer is *not* an instance of AdaLoraLayer."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-12-15T11:16:59Z",
        "message": "ENH Rank-stabilized LoRA scaling option (#1244)\n\nAdd option to scale LoRA weights by alpha/sqrt(r) by passing\nLoraConfig(..., use_rslora=True).\n\nhttps://doi.org/10.48550/arXiv.2312.03732"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-12-12T14:34:45Z",
        "message": "Fix: Multiple adapters with bnb layers (#1243)\n\nResolves #1239\n\nFixes a bug that led to an error when loading multiple adapters into a\npeft model that uses bnb layers.\n\nAlso: Fix for loading 2nd adapter with AutoGPTQ"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-12-11T11:35:28Z",
        "message": "FIX Use model argument consistently (#1198) (#1205)\n\nSome methods were using model and self.model interchangeably. This was\nfine, as they were referring to the same object, but is also confusing.\nNow model is used consistently."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-12-07T15:39:08Z",
        "message": "Lazy import of bitsandbytes (#1230)\n\nPreviously, we imported from bitsandbytes eagerly if the package was\ninstalled. This caused two major issues:\n\n- Slow loading time of PEFT (~4 sec)\n- Errors with multiprocessing because bnb initializes CUDA\n\nThis commit fixes both issues by importing bitsandbytes lazily. PEFT\nimport time is now reduced to ~2sec.\n\nNotes\n\nImplementation-wise, I use a combination of local imports and\nmodule-level __getattr__. The latter was introduced in Python 3.7 and\nshould therefore be safe to use."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-12-06T13:44:58Z",
        "message": "Raise error when `modules_to_save` is specified and multiple adapters are being unloaded (#1137)\n\n* handle `modules_to_save` when unloading\n\n* address comments\n\n* Apply suggestions from code review\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* quality\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-12-04T11:22:03Z",
        "message": "DOC: Update & improve docstrings and type annotations for common methods and classes (#1201)\n\nThe docstrings of the most user-exposed methods and classes have been\nupdated, or added if not already present. Furthermore, type annotations\nhave been updated or added for those methods and classes.\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-11-30T15:24:58Z",
        "message": "Megatron distributed parallel linear LoRA (#1092)\n\nAdds option to use Megatron's ColumnParallelLinear and RowParallelLinear\nfor LoRA linear layers, leading to improved performance when using LoRA\nwith Megatron."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-11-29T16:08:17Z",
        "message": "Add LoftQ initialization method for LoRA (#1150)\n\n---------\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-11-22T12:14:21Z",
        "message": "fix `add_weighted_adapter` method (#1169)\n\n* fix `add_weighted_adapter` method\n\nCo-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\nCo-Authored-By: jihuishan <151612440+jihuishan@users.noreply.github.com>\n\n* Update testing_common.py\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\nCo-authored-by: jihuishan <151612440+jihuishan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-11-16T11:45:12Z",
        "message": "Refactor base layer pattern (#1106)\n\nDescription\n\nRefactor all tuners (where it applies, i.e. not prompt tuning) to use\nthe \"base layer pattern\". This means that the adapter layer will always\nhold a reference to the original layer that it modifies. This pattern is\nalready partly used (e.g. LoRA bnb, gptq layers), now it is consistently\nused everywhere when applicable.\n\nThis PR is a companion PR to #1069, where I first added these changes.\nThey are now extracted to a separate PR to make code review easier and\nto advance more quickly.\n\nImplementation\n\nThe main change is that the adapter layer wraps the original layer and\ncalls forward on that layer, instead of doing stuff like this:\n\nF.linear(input, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n\nwhich completely circumvents the call to the target layer's forward\nmethod. With the base layer pattern, we now call the target layer's\nforward method. Therefore, if the target layer is another adapter\nlayer (which will be crucial for mixed adapters), we call its forward\nmethod correctly. Also, this should allow passing extra arguments, like\nlora_scale to forward.\n\nThis change has the nice side benefit that we no longer need to use\n_init_empty_weights -- in fact, we don't initialize any of the target\nlayer's weights anymore, since we have a reference to it. There is thus\nno risk of having slow but superfluous initialization of layers.\n\nMoreover, I could greatly simplify merge_and_unload by just using the\nbase_layer instead of having to create a completely new layer. For\nOPT-350m, this results in a 15x speedup.\n\nNote that same as for the bnb layers, this should be backwards\nincompatible, since the adapter weights and their state_dicts are not\naffected by this change. I used #1115 for regression testing.\n\nSomewhat unrelated changes\n\nDuring debugging, I got very annoyed with the fact that the reprs of\nadapter layers and normal PyTorch layers are hard to distinguish, e.g.\nthe type is just \"Linear\". Now, for adapter layers, it is prefixed by\nthe adapter type, e.g. \"lora.Linear\". This should have no further\nimplications except for the repr (e.g. state_dict remains unaffected).\n\nFor LoHa and LoKr, I had to change the init of weights when using\ninit_weights=False. This is because of what is discussed in Numerical\ninstabilities with LoHa #1058.\n\nIA\u00b3 now has the unload method too.\n\nLoHa and LoKr now support safe_merge=True when merging layers.\n\nMigration guide\n\nFor 99% of users, the code should continue working as ususal, because\nthe API stays the same. Only low level details have been changed.\n\nCode that relies on isinstance checks on specific PEFT classes may\nbreak. E.g. the LoRA Linear layer no longer inherits from nn.Linear. It\nis, however, still a BaseTunerLayer. The same logic applies for other\nlayer types like Conv2d and for other tuners like IA\u00b3.\n\nTo retrieve the base layer of an adapter layer, you should now call\nmodule.get_base_layer() if you deal with a BaseTunerLayer. Don't rely on\nsomething like module.weight being present (though it might be)."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-11-16T11:05:22Z",
        "message": "FEAT: Merging only specified `adapter_names` when calling `merge` (#1132)\n\n* working v1\n\n* add tests\n\n* remove\n\n* add it also for lokr and loha, left a todo\n\n* Update tests/testing_common.py\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>\n\n* better test\n\n* up\n\n* fix tests\n\n* credits contrib and suggestions from disscussions\n\n* credits contrib and suggestions from disscussions\n\n* address last comments\n\n---------\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-11-10T12:33:56Z",
        "message": "Refactor adapter deletion (#1105)\n\nDescription\n\nThe job of deleting an adapter is now transferred to the adapter layer,\ninstead of the adapter model. This makes it easier for users or other\nlibraries who don't use the adapter model to delete adapters.\n\nImplementation\n\nThe code should now be more generic, relying less on hard-coded\nattributes.\n\nAs a precaution, I also changed the type of adapter_layer_names from\nlist to tuple, as it should not be mutated.\n\nWhen deleting the active adapter, the logic for choosing the new active\nadapter has been changed slightly to ensure consistency across layers.\nIn practice, this should rarely make a difference. An error is now\nraised if the last remaining adapter is deleted.\n\nTest coverage has been increased:\n\n- Deleting adapters is now also tested for custom models.\n- It is also tested for LoHa, LoKr, not only LoRA.\n- I added a test for deleting the non-active adapter.\n\nNot implemented\n\nI did not add adapter deletion to IA\u00b3, since it is included in #980. LMK\nif it should be added here instead."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-10-25T12:53:45Z",
        "message": "FIX setting active adapter correctly (#1051)\n\nCurrently, when calling set_adapter, the active adapter is not updated.\nTests have been added to trigger the bug and the method updated to fix\nit.\n\nMoreover, I created an active_adapters property on the PeftModel class\nso that it behaves consistently with the underlying models like\nLoraModel."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-10-10T14:47:35Z",
        "message": "ENH: Refactor LoRA bnb layers for faster initialization (#994)\n\nPartly addresses #896\n\nDescription\n\nAfter speeding up normal LoRA layer initialization, this PR improves\ninitialization speed of bnb LoRA layers.\n\nThe method to achieve this is different from the one used before, namely\nthis time the base layer is stored as a reference on the LoRA layer.\nThis allows us to avoid calling __init__ on the bnb layer, which is what\nis slow.\n\nNotes\n\nWe cannot use the same method as for the normal LoRA layers, (i.e.\ncalling the super class's __init__ with meta device) because the bnb\nlayers have extra logic that still creates unnecessary weights.\n\nHowever, the way used here could also be a solution to the normal\nlayers, so if we want to have consistency, the normal layers could be\nrefactored to use the same approach.\n\nInterestingly, even though we now save the base layer as a reference,\nwhich results in a different state_dict, the existing models can still\nbe loaded successfully. This is because the adapter state_dict is not\naffected by the change, so users can still load their existing adapters.\n\nThe only problem would occur if users dump the whole model, i.e. base\nmodel and adapter, using torch.save and then trying to load with\ntorch.load. For those users, we could theoretically provide a script to\nconvert the state_dict (i.e. renaming some keys).\n\nTo ensure that the old adapters can still be loaded successfully, I'm\nworking at the same time on adding regression tests. I'll create a\nseparate PR for those to avoid blowing up this one.\n\nTests\n\nI ran a test on bloomz-1b1 for how long it takes to create the\nPeftModel, the results are:\n\n8bit: 1108.34 ms > 26.82 ms\n4bit: 1101.96 ms > 23.69 ms"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-10-09T16:28:00Z",
        "message": "FEAT: Add `safe_merge` option in `merge` (#1001)\n\n* add `safe_merge` option in `merge`\n\n* oops\n\n* Apply suggestions from code review\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* address final comments\n\n* Update src/peft/tuners/lora/layer.py\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* Update src/peft/tuners/lora/layer.py\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* add it for ia3\n\n* add it for adalora\n\n* up\n\n* revert for loha\n\n* style\n\n* fix CI\n\n* adapt from suggestions\n\n* add tests\n\n* up\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-10-05T07:57:49Z",
        "message": "Fix lora creation (#993)\n\n* reducing the time for inject lora modules\n\n* fix bugs\n\n* fix bug\n\n* fixes\n\n* Revert \"fixes\"\n\nThis reverts commit c7f30627c1798db11be8a5da8f3c801f9469a5e3.\n\n* refactor\n\n* fix failing tests\n\n* fix tests\n\n* fix tests\n\n* fix tests\n\n* fix tests\n\n* Apply suggestions from code review\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* address comments\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-10-02T08:44:51Z",
        "message": "FEAT Add LyCORIS LoHa for SD&SDXL models (#956)\n\nhttps://arxiv.org/abs/2108.06098"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-09-29T04:14:30Z",
        "message": "[tests] add multiple active adapters tests (#961)\n\n* add tests for multiple active adapters\n\n* add multiple active adapter tests\n\n* fix tests\n\n* fix the device error\n\n* fix typo\n\n* fix the variables\n\n* fix the `adalora` config\n\n* add util function for proper naming of tests\n\n* fix bugs\n\n1. fix `add_weighted_adapter` when working with adapters targeting different layers\n2. fix `ia3` model and layer to handle adapters targeting different layers\n3. fix the multiple active adapter tests\n\n* fix `ia3` issue\n\n* remove debug statements\n\n* fix test\n\n* fix bug\n\n* address comments\n\n* address comments\n\nCo-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* fix tests\n\n* remove unused code\n\n* Update test_custom_models.py\n\n* increasing tolerance for a test\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-09-26T07:31:05Z",
        "message": "FIX: setting requires_grad on adapter layers (#905)\n\n* [WIP] Fix setting requires_grad on adapter layers\n\nThis is an alternative to #900, resolves #899.\n\nDescription\n\nCurrently, we don't handle setting requires_grad on adapter layers\nreally well. The main issue is that it can be set to True on adapter\nparameters that are not being used, e.g. the original_module in\nModulesToSaveWrapper or inactive adapters in LoRA.\n\nNormally, this is not a big issue, except maybe if we want to correctly\ncount the number of trainable parameters. However, when training with\nDistributedDataParallel, this results in errors, as PyTorch thinks that\nall parameters with requires_grad=True should participate in the loss\ncomputation, but those mentioned parameters don't. For that reason,\ntraining with DDP currently fails when using modules_to_save or multiple\nadapters.\n\nImplementation\n\nThis turned out to be more complicated than I initially thought. The\nlogic for setting requires_grad is all over the place, it was hard to\nencapsulate the logic and I only succeeded partially. As is, this PR is\nmore complex than the one it tries to supersede, #900, but it is also\n\"more correct\".\n\nTests were added to check whether requires_grad is set correctly. There\nare (so far) no tests for whether DDP indeed works, they could be added\nwith multi-GPU. I did, however, test an early stage of this PR with DDP\nand setting requires_grad correctly will indeed fix the DDP error.\n\nDONE/TODO\n\n- [x] ModulesToSaveWrapper\n- [x] LoRA\n- [ ] IA\u00b3\n- [ ] AdaLora\n\nSince some tuners are not implemented yet, tests are expected to fail.\nCheck the new tests at the bottom of test_custom.py, those should pass.\n\n* Refactor: move more requires_grad machinery to ABC\n\n* [skip ci] [WIP] Add requires_grad logic to IA\u00b3\n\n* Add AdaLora\n\n* Fix some minor issues\n\n* Make style"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-09-22T20:03:44Z",
        "message": "support multiple ranks and alphas for LoRA (#873)\n\n* support multiple ranks and alphas\n\n* Update lora.py\n\n* Update lora.py\n\n* commit suggestions\n\nCo-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* address comments\n\nCo-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* Fixed multirank + multialpha for sequential LoRAs, added correct support of LoRA-C3Lier conversion (#937)\n\n* Fixed multirank multialpha for sequential loras, added tests, fixed docs\n\n* Refactored kohya_ss conversion script for proper support of LoRA-C3Lier\n\n* Fixed styling\n\n* Removed old comment from docstring\n\n* shift `scale_layer`/`unscale_layer` to `LoraLayer` class to support all the child classes\n\n* support multiple active adapters\n\n* add `active_adapters` property\n\nCo-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* fix bug related to active adapter of `ModulesToSaveWrapper`\n\n* revert the change wrt active_adapter assignment\n\nCo-Authored-By: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* addressing comments\n\n* address comments\n\n* address comment\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\nCo-authored-by: Alexander Kovalchuk <kovalexal@gmail.com>\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-09-20T09:26:35Z",
        "message": "ENH error message when choosing wrong bias (#946)\n\nRaise an error with a helpful error message when the user chooses an incorrect\noption for the bias argument.\n\n---------\n\nCo-authored-by: datta0 <venkatadattasanimmaturi@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-09-07T10:14:37Z",
        "message": "ENH Merge lora module to 8bit model (#875)\n\nAllows merging 8bit weights from bnb.\n\n4bit weight merging was already implemented through the dequantization method\nprovided by bnb but there is no official dequantization method for 8bit weights.\nThis PR works by multiplying the weights to an identity matrix using bnb's\nquantization aware matmul operation. Empirically, this results in a very small\nrounding error."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-09-06T15:31:55Z",
        "message": "ENH Remove redundant initialization layer calls (#887)\n\nThis should lead to a big speedup when initializing LoRA layers.\n\n---------\n\nCo-authored-by: poedator <ruslansv@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-08-29T09:32:29Z",
        "message": "MNT: Move tuners to subpackages (#807)\n\nFor each tuner, created a sub-module that contains at least:\n\n- config.py for config stuff\n- model.py for the actual model/encoder/embedding\n- __init__.py so that imports are preserved\n\nThen, when there was a need, further files were created, like layer.py\nor utils.py.\n\nImports were changed to absolute imports everywhere, except for the\nsub-packages within a tuner directory, as these packages will always \nstay together in the same place.\n\nFor some existing modules, the license comment of the top of the file\nwas missing, I always added it.\n\nThere was a bug in the forward method of 4bit linear lora layers introduced\nin #851, for the case that the model is merged AND adapters are disabled.\nFor that scenario, we need to unmerge first before generating the output,\nsame as we do for the vanilla Linear layer. This step was missing from the\ncode previously and is now implemented correctly. Tests were adjusted to\ncatch that error."
    },
    {
        "repo_url": "github.com/hpcaitech/ColossalAI",
        "filepath": "applications/Chat/examples/community/peft/easy_models.py",
        "commit_date": "2023-09-19T06:20:26Z",
        "message": "[misc] update pre-commit and run all files (#4752)\n\n* [misc] update pre-commit\n\n* [misc] run pre-commit\n\n* [misc] remove useless configuration files\n\n* [misc] ignore cuda for clang-format"
    },
    {
        "repo_url": "github.com/hpcaitech/ColossalAI",
        "filepath": "applications/Chat/examples/community/peft/easy_models.py",
        "commit_date": "2023-04-06T07:04:48Z",
        "message": "add community example dictionary (#3465)"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-07-15T06:17:18Z",
        "message": "fix code style"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-07-15T06:10:54Z",
        "message": "update emperical formula"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-07-13T13:11:14Z",
        "message": "refactoring code"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-07-13T04:19:21Z",
        "message": "fix potential inv_freq issue; add alpha argument"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-07-13T01:56:50Z",
        "message": "update max_position_embeddings"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-07-13T00:48:15Z",
        "message": "add pathces for memory_efficient_attention and NTK scaling"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-07-06T02:01:24Z",
        "message": "update banner path, change default decoding values"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-07-05T09:26:51Z",
        "message": "Merge pull request #705 from ymcui/context_extend\n\nExtend context size (8K+) without fine-tuning"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-07-05T08:39:09Z",
        "message": "replace position interploation with NTK method"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-07-04T01:32:19Z",
        "message": "fix Codacy issues"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-07-04T01:21:34Z",
        "message": "fix Codacy issues"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-07-03T10:13:14Z",
        "message": "fix output speed in gradio demo"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-07-03T01:37:10Z",
        "message": "add Position Interpolation for inference scripts"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-06-28T13:46:37Z",
        "message": "fix: fix English output missing space and tune default option"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-06-26T13:58:28Z",
        "message": "fix: make unused argument max_memory into use"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-06-26T13:31:03Z",
        "message": "refactor: Apply GPT-4 instructions to refactor the code"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-06-18T12:14:59Z",
        "message": "chore: change port"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-06-18T12:12:39Z",
        "message": "feat: support stream output for gradio demo"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-05-31T14:24:03Z",
        "message": "fix banner path"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-05-31T10:05:13Z",
        "message": "Merge remote-tracking branch 'origin/33b' into gradio_notebook"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-05-26T02:24:38Z",
        "message": "reorganize scripts folder structure\n\n\u26a0\ufe0f note that these changes will take effect in next version."
    },
    {
        "repo_url": "github.com/stanfordnlp/dspy",
        "filepath": "dsp/modules/hf.py",
        "commit_date": "2024-03-01T21:18:19Z",
        "message": "Big reformat push"
    },
    {
        "repo_url": "github.com/stanfordnlp/dspy",
        "filepath": "dsp/modules/hf.py",
        "commit_date": "2024-02-20T11:34:56Z",
        "message": "Propagate device_map to hf model"
    },
    {
        "repo_url": "github.com/stanfordnlp/dspy",
        "filepath": "dsp/modules/hf.py",
        "commit_date": "2024-02-13T17:33:28Z",
        "message": "Allow HFModel to use CPU (fix)"
    },
    {
        "repo_url": "github.com/stanfordnlp/dspy",
        "filepath": "dsp/modules/hf.py",
        "commit_date": "2024-02-13T17:31:49Z",
        "message": "Revert the coding style to original"
    },
    {
        "repo_url": "github.com/stanfordnlp/dspy",
        "filepath": "dsp/modules/hf.py",
        "commit_date": "2024-02-07T10:21:14Z",
        "message": "Allow HFModel to use CPU"
    },
    {
        "repo_url": "github.com/stanfordnlp/dspy",
        "filepath": "dsp/modules/hf.py",
        "commit_date": "2024-02-05T10:56:30Z",
        "message": "chore: move torch init to when it is not client in hf module\n\nthis make sure that torch does not need to be installed when hf or vllm model is accessed via api"
    },
    {
        "repo_url": "github.com/stanfordnlp/dspy",
        "filepath": "dsp/modules/hf.py",
        "commit_date": "2023-10-11T15:50:28Z",
        "message": "Add multihop_finetune initial notebook"
    },
    {
        "repo_url": "github.com/stanfordnlp/dspy",
        "filepath": "dsp/modules/hf.py",
        "commit_date": "2023-10-05T01:12:06Z",
        "message": "Update modules. Include generic ReAct, MultiChainComparison, ChainOfThoughtWithHint, improve dspy.majority, tweak teleprompters, update default OpenAI LM to turbo-instruct"
    },
    {
        "repo_url": "github.com/stanfordnlp/dspy",
        "filepath": "dsp/modules/hf.py",
        "commit_date": "2023-08-15T17:37:42Z",
        "message": "Readying for release. Working notebook without cache yet"
    },
    {
        "repo_url": "github.com/stanfordnlp/dspy",
        "filepath": "dsp/modules/hf.py",
        "commit_date": "2023-04-23T15:44:33Z",
        "message": "minor device usage fix"
    },
    {
        "repo_url": "github.com/stanfordnlp/dspy",
        "filepath": "dsp/modules/hf.py",
        "commit_date": "2023-04-23T14:48:19Z",
        "message": "minor fix"
    },
    {
        "repo_url": "github.com/stanfordnlp/dspy",
        "filepath": "dsp/modules/hf.py",
        "commit_date": "2023-04-23T14:45:19Z",
        "message": "export HFModel withtout requiring deps"
    },
    {
        "repo_url": "github.com/stanfordnlp/dspy",
        "filepath": "dsp/modules/hf.py",
        "commit_date": "2023-04-23T14:24:27Z",
        "message": "minor hf usage fix"
    },
    {
        "repo_url": "github.com/stanfordnlp/dspy",
        "filepath": "dsp/modules/hf.py",
        "commit_date": "2023-04-14T19:47:09Z",
        "message": "Drop the prompt from a Causal LM model's output"
    },
    {
        "repo_url": "github.com/stanfordnlp/dspy",
        "filepath": "dsp/modules/hf.py",
        "commit_date": "2023-04-14T19:41:30Z",
        "message": "--cruft"
    },
    {
        "repo_url": "github.com/stanfordnlp/dspy",
        "filepath": "dsp/modules/hf.py",
        "commit_date": "2023-04-11T21:54:31Z",
        "message": "Add support for Causal LMs"
    },
    {
        "repo_url": "github.com/stanfordnlp/dspy",
        "filepath": "dsp/modules/hf.py",
        "commit_date": "2023-04-04T21:19:35Z",
        "message": "Add Support for Multi-GPU Inference"
    },
    {
        "repo_url": "github.com/stanfordnlp/dspy",
        "filepath": "dsp/modules/hf.py",
        "commit_date": "2023-03-27T06:59:08Z",
        "message": "fixing conflicting changes"
    },
    {
        "repo_url": "github.com/stanfordnlp/dspy",
        "filepath": "dsp/modules/hf.py",
        "commit_date": "2023-03-21T03:11:56Z",
        "message": "Add code for supporting arbitrary Hugging Face models"
    },
    {
        "repo_url": "github.com/hpcaitech/ColossalAI",
        "filepath": "applications/ColossalEval/colossal_eval/models/huggingface.py",
        "commit_date": "2024-02-06T02:53:03Z",
        "message": " [eval] update llama npu eval (#5366)"
    },
    {
        "repo_url": "github.com/hpcaitech/ColossalAI",
        "filepath": "applications/ColossalEval/colossal_eval/models/huggingface.py",
        "commit_date": "2023-12-15T07:06:06Z",
        "message": "Fix ColossalEval (#5186)\n\nCo-authored-by: Xu Yuanchen <yuanchen.xu00@gmail.com>"
    },
    {
        "repo_url": "github.com/hpcaitech/ColossalAI",
        "filepath": "applications/ColossalEval/colossal_eval/models/huggingface.py",
        "commit_date": "2023-12-12T06:47:35Z",
        "message": "[ColossalEval] Support GSM, Data Leakage Evaluation and Tensor Parallel (#5169)\n\n* Support GSM, Data Leakage Evaluation and Tensor Parallel\n\n* remove redundant code and update inference.py in examples/gpt_evaluation\n\n---------\n\nCo-authored-by: Xu Yuanchen <yuanchen.xu00@gmail.com>"
    },
    {
        "repo_url": "github.com/hpcaitech/ColossalAI",
        "filepath": "applications/ColossalEval/colossal_eval/models/huggingface.py",
        "commit_date": "2023-11-09T05:41:50Z",
        "message": "Support mtbench (#5025)\n\nCo-authored-by: Xu Yuanchen <yuanchen.xu00@gmail.com>"
    },
    {
        "repo_url": "github.com/hpcaitech/ColossalAI",
        "filepath": "applications/ColossalEval/colossal_eval/models/huggingface.py",
        "commit_date": "2023-10-31T02:30:03Z",
        "message": "fix ColossalEval (#4992)\n\nCo-authored-by: Xu Yuanchen <yuanchen.xu00@gmail.com>"
    },
    {
        "repo_url": "github.com/hpcaitech/ColossalAI",
        "filepath": "applications/ColossalEval/colossal_eval/models/huggingface.py",
        "commit_date": "2023-09-24T15:14:11Z",
        "message": "[feature] ColossalEval: Evaluation Pipeline for LLMs (#4786)\n\n* Add ColossalEval\n\n* Delete evaluate in Chat\n\n---------\n\nCo-authored-by: Xu Yuanchen <yuanchen.xu00@gmail.com>\nCo-authored-by: Tong Li <tong.li352711588@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2024-02-12T13:41:35Z",
        "message": "FIX Honor HF_HUB_OFFLINE mode if set by user (#1454)\n\nResolves #1452\n\nIf users enable offline mode, don't perform checks for files on HF Hub,\nas they would fail."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2024-02-07T11:52:35Z",
        "message": "MNT Move code quality fully to ruff (#1421)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2024-01-31T06:08:13Z",
        "message": "fix: subfolder existence check (#1417)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2024-01-30T09:43:47Z",
        "message": "Fix breaking change  (#1414)\n\n* fix\n\n* Update src/peft/utils/save_and_load.py\n\n* Update src/peft/utils/save_and_load.py"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2024-01-22T14:46:42Z",
        "message": "save the embeddings even when they aren't targetted but resized (#1383)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2024-01-12T16:19:12Z",
        "message": "FEAT Add Poly Adapter (#1129)\n\nImplement the Poly (Polytropon) adapter.\n\nPapers:\n\n- https://arxiv.org/abs/2202.13914\n- https://arxiv.org/abs/2211.03831\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2024-01-03T14:35:06Z",
        "message": "fix diffusers tests (#1317)\n\n* fix diffusers tests\n\n* quality"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2024-01-03T09:52:26Z",
        "message": "fix the embedding saving for adaption prompt (#1314)\n\n* fix the embedding saving for adaption prompt\n\n* fix\n\n* automate setting `save_embedding_layers` when embedding layer is resized during finetuning\n\n* fix\n\n* address comment\n\nCo-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* oops\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-12-15T12:05:06Z",
        "message": "feat: add apple silicon GPU acceleration (#1217)\n\n* feat: add apple silicon GPU acceleration\n\n* Fix device compatibility issue in\nload_peft_weights function\n\n* Update save_and_load.py\n\n* Update save_and_load.py\n\n* Update save_and_load.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update src/peft/utils/save_and_load.py\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* Fix string formatting in image_classification_timm_peft_lora.ipynb and multilayer_perceptron_lora.ipynb\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-11-30T15:58:42Z",
        "message": "[Feature] Support OFT (#1160)\n\n* Support OFT\n\n* add test\n\n* Update README\n\n* fix code quality\n\n* fix test\n\n* Skip 1 test\n\n* fix eps rule and add more test\n\n* feat: added examples to new OFT method\n\n* fix: removed wrong arguments from model example\n\n* fix: changed name of inference file\n\n* fix: changed prompt variable\n\n* fix docs\n\n* fix: dreambooth inference revision based on feedback\n\n* fix: review from BenjaminBossan\n\n* apply safe merge\n\n* del partially\n\n* refactor oft\n\n* refactor oft\n\n* del unused line\n\n* del unused line\n\n* fix skip in windows\n\n* skip test\n\n* Add comments about bias added place\n\n* rename orig_weights to new_weights\n\n* use inverse instead of linalg.inv\n\n* delete alpha and scaling\n\n---------\n\nCo-authored-by: Lukas Kuhn <lukaskuhn.lku@gmail.com>\nCo-authored-by: Lukas Kuhn <lukas.kuhn@deutschebahn.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-11-29T13:58:41Z",
        "message": "Training PEFT models with new tokens being added to the embedding layers and tokenizer (#1147)\n\n* add support for saving base layers weights along with adapter weights\n\n* Update save_and_load.py\n\n* Add an example showing the usage of the added feature\n\n* refactor the functionality\n\n* fix\n\n* refactoring code\n\n1. Add `is_embedding_layer_resized` parameter to `save_pretrained`\n2. Fix the deduplication in README when adding PEFT details.\n3. `save_pretrained` should only save the model when `is_main_process=True` which is one of the parameters of `save_pretrained`.\n\n* update example\n\n* fix the model card\n\n* fix model card\n\n* \ud83d\ude05\n\n* fix model card\n\n* automate setting `is_embedding_layer_resized`\n\n* nits\n\n* Update peft_lora_clm_with_additional_tokens.ipynb\n\n* add test\n\n* fix tests\n\n* maybe fixes the issue?\n\n* address comments\n\nCo-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-11-28T13:17:25Z",
        "message": "FIX Pass HF token when calling PeftModel.from_pretrained (#1076)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-11-17T14:48:02Z",
        "message": "Use `huggingface_hub.file_exists` instead of custom helper (#1145)\n\n* Use 'huggingface_hub.file_exists' instead of custom helper\n\n* make quality"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-10-30T14:36:41Z",
        "message": "Add implementation of LyCORIS LoKr for SD&SDXL models (#978)\n\nKronA-like adapter"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-10-02T08:44:51Z",
        "message": "FEAT Add LyCORIS LoHa for SD&SDXL models (#956)\n\nhttps://arxiv.org/abs/2108.06098"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-09-21T07:46:28Z",
        "message": "Fix some tests that would fail with torch.compile (#949)\n\nSome tests would currently fail with torch.compile, not because there is\nanything wrong with how PEFT works with compiled models, but simply\nbecause of the way the tests are written. This is because when models\nare compiled, the keys of the state dict change. Tests have now been\nadapted to unwrap the compiled model first before getting the state\ndict.\n\nNote that the mentioned issue does not affect saving and loading,\nbecause save_pretrained is already called on the original module, so\nthere is no issue with mismatched keys.\n\nAlso fixed the docstring of get_peft_model_state_dict."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-08-25T06:12:11Z",
        "message": "\ud83c\udf89 Add Multitask Prompt Tuning (#400)\n\n* mpt\n\n* fix save\n\n* fix save\n\n* add jupyter notebook\n\n* add jupyter notebook\n\n* add jupyter notebook\n\n* drop shuffling\n\n* drop classify_dataset\n\n* drop classify_dataset\n\n* fix keys\n\n* fix keys\n\n* add comments\n\n* use EXACT_SOURCE_TASK in the example\n\n* formatting\n\n* Fix dict index in embedding retrieval\n\n* run style and quality\n\n* run style and quality\n\n* run style and quality\n\n* style\n\n* final fix\n\n* style\n\n* comment out failing tests\n\n* fix generation tests\n\n* fix style and save test\n\n* all testcases\n\n* fix import\n\n* add license header\n\n* reformat\n\n* fix encoder-decoder models\n\n* fix tests running multiple times\n\n* fix paper name for IA3 and add MPT paper\n\n* Trigger CI\n\n* address the recommended changes\n\n* reformat\n\n* address suggestions\n\n* address suggestions\n\n* revert reformatting\n\n* revert reformatting\n\n---------\n\nCo-authored-by: Alex-Brooks <Alex.Brooks@ibm.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-08-07T14:34:54Z",
        "message": "[`core`] PEFT refactor + introducing `inject_adapter_in_model` public method (#749)\n\nRefactors a bit the internals of some PEFT models and introduces a new\nmethod inject_adapter_in_model for users that want to pass a bare model\nand a peft config to inject adapters in-place into the model. These\nchanges are totally BC with the previous PEFT versions.\n\nThis PR makes things easier for the PEFT integration in transformers\nhuggingface/transformers#25077\n\nThe main goal of the PR is to expose a new API for advanced users that\nwant to integrate PEFT method without the need to use the PeftModel\nwrapper. A simple use case could be someone that wants to inject adapters\ninto a model and wants to keep the original class of the model without\nhaving to offload that to peft that will create a PeftModel. I have\nfaced this issue in huggingface/transformers#25077 Among other things,\nthis PR refactors some internals of PEFT library, while keeping it fully\nbackward compatible.\n\nTo tackle the main motivation I propose to differentiate things between\ntwo type of adapters\n\n1- adapters that are injectable (LoRA, AdaLoRA, IA3)\n2- adapters that are not injectable (the rest)\n\nAs a first iteration this API would be supported only for the scenario\n1- / therefore I decided to create 2 abstract classes to make things\neasy to be able to determine if the adapter layer (e.g. LoraLayer) /\nadapter module (e.g. LoraModel) does follow the minimal\nrequirement (i.e. needed attributes, etc.)\n\nOther related changes:\n\n1- Creates a new property method is_prompt_learning to avoid importing\n   PromptLearningConfig all the way down\n2- Introduces a new object TUNERS_MAPPING, which is a mapping of\n   supported pluggable adapters\n3- Creates two abstract classes\n3.1- BaseTunerLayer: a mixin to check for minimal required attributes\n     that a tuner layer should have active_adapter / _is_plugable\n3.2- BaseTuner: a higher level module mixin that should be used for any\n     injectable adapters in the future.\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-07-13T07:45:50Z",
        "message": "Add functionality to support IA3 (#578)\n\n* Added initial ia3 code\n\n* Implemented ia3 correctly for feedforward layers; Fixed regex matching\n\n* Fixed module mapping for mt5\n\n* Merged changes from huggingface:main\n\n* Merged changes\n\n* Fixed lora merge conflicts\n\n* Different bloom config\n\n* Added save option for ia3\n\n* Added loading code for ia3\n\n* Added feedforward implementation in utils and seq cls example\n\n* Added feedforward implementation in utils and seq cls example\n\n* Implemented merge, unmerge, enable/disable adapters functionality\n\n* Fixed feedforward during merge\n\n* Debugging Merge\n\n* Removing debug messages\n\n* Cleaned up repo\n\n* Removed non-IA3 changes\n\n* Refactor save and load\n\n* Added support to all models in tests; Added IA3Config for common tests\n\n* Added half-precision support and test for gradient checkpointing; Formatted jupyter notebooks\n\n* Added target modules for new models GPTBigCode and LLama\n\n* Cleaned up code\n\n* Cleaned up code\n\n* Cleaned up example notebook\n\n* Cleaned up  seq2seq notebook\n\n* Corrected function docstrings; refactored find_and_replace\n\n* Corrected function docstrings; refactored find_and_replace\n\n* Added basic docs for IA3\n\n* Added new conceptual guide in source tree for documentation\n\n* Minor fix to documentation\n\n* Minor fixes to docstrings; Added error handling for 4bit quantization; Cleaned unused merge/unmerge methods\n\n* styling changes after merge from main\n\n* Update src/peft/tuners/ia3.py\n\nRemove unused attribute merge_weights\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Abhishek2304 <abhishekgupta2304@gmail.com>\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-06-01T09:16:38Z",
        "message": "return load_result when load_adapter (#481)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-04-25T06:54:18Z",
        "message": "Implement adaption prompt from Llama-Adapter paper (#268)\n\n* Implement adaption prompt from Llama-Adapter paper\n\n* Support multi-adapters\n\n* Refactor adaption prompt to target attn modules instead of layers\n\n* Refactor adaption prompt to be more generic\n\n* Fix adaption prompt not on right device\n\n* Apply suggestions from code review\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>\n\n* Fix style\n\n* Add support for Llama config use_cache=True\n\n* Fix rebase issues\n\n---------\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-04-06T22:38:10Z",
        "message": "fixing adalora saving and loading"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-04-06T14:32:31Z",
        "message": "fix"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-04-06T13:35:31Z",
        "message": "Merge branch 'main' into smangrul/multi-lora-support"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-04-05T20:52:12Z",
        "message": "Run make style and make quality"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-04-04T14:35:59Z",
        "message": "\ud83d\ude05. Fix \ud83d\udc1b"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-04-04T14:14:58Z",
        "message": "fixing \ud83d\udc1b"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-03-30T06:19:45Z",
        "message": "Finish the test for model load and save"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-03-30T05:59:26Z",
        "message": "Implement the save_pretrained for AdaLoRA"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-03-28T14:49:06Z",
        "message": "fix bugs"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-03-28T13:26:24Z",
        "message": "multi adapter for training and inference\n\nMight have breaking changes"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-03-02T01:04:48Z",
        "message": "finish the testing and debugging"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-02-08T06:59:03Z",
        "message": "remove `peft_model_load_and_dispatch` as it is part of `PeftModel.from_pretrained`"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-02-08T03:55:21Z",
        "message": "Update save_and_load.py"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-02-08T03:51:49Z",
        "message": "update `peft_model_load_and_dispatch`"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-01-30T08:01:01Z",
        "message": "fixes"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-01-18T21:53:57Z",
        "message": "fixing doc strings to follow hf doc-builder format"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-01-18T14:13:29Z",
        "message": "addressing comments and bug fixes"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "src/peft/utils/save_and_load.py",
        "commit_date": "2023-01-15T13:39:56Z",
        "message": "addressing comments and renaming `pet` to `peft`"
    },
    {
        "repo_url": "github.com/AI4Finance-Foundation/FinGPT",
        "filepath": "fingpt/FinGPT_Benchmark/benchmarks/benchmarks.py",
        "commit_date": "2023-10-28T03:47:59Z",
        "message": "Update benchmarks.py"
    },
    {
        "repo_url": "github.com/AI4Finance-Foundation/FinGPT",
        "filepath": "fingpt/FinGPT_Benchmark/benchmarks/benchmarks.py",
        "commit_date": "2023-10-28T03:40:39Z",
        "message": "Update benchmarks.py"
    },
    {
        "repo_url": "github.com/AI4Finance-Foundation/FinGPT",
        "filepath": "fingpt/FinGPT_Benchmark/benchmarks/benchmarks.py",
        "commit_date": "2023-10-28T03:35:35Z",
        "message": "Update benchmarks.py"
    },
    {
        "repo_url": "github.com/AI4Finance-Foundation/FinGPT",
        "filepath": "fingpt/FinGPT_Benchmark/benchmarks/benchmarks.py",
        "commit_date": "2023-10-16T04:39:45Z",
        "message": "FinGPT 2023-10-16 code refactoring"
    },
    {
        "repo_url": "github.com/NVIDIA/NeMo",
        "filepath": "nemo/collections/multimodal/parts/utils.py",
        "commit_date": "2024-02-27T17:43:26Z",
        "message": "Add Gemma Support and Conversion Scripts (#8468)\n\n* Add taurus pytorch to nemo\n\nSigned-off-by: yaoyu-33 <yaoyu.094@gmail.com>\n\n* Add a taurus jax to nemo conversion script and few other fixes\n\nSigned-off-by: yaoyu-33 <yaoyu.094@gmail.com>\n\n* Clean up code\n\nSigned-off-by: yaoyu-33 <yaoyu.094@gmail.com>\n\n* bug fix\n\nSigned-off-by: yaoyu-33 <yaoyu.094@gmail.com>\n\n* renaming\n\nSigned-off-by: yaoyu-33 <yaoyu.094@gmail.com>\n\n* Fix arguments\n\nSigned-off-by: yaoyu-33 <yaoyu.094@gmail.com>\n\n* Add HF Gemma converter\n\nSigned-off-by: yaoyu-33 <yaoyu.094@gmail.com>\n\n* Turn off `apply_rope_fusion` during inference\n\nSigned-off-by: yaoyu-33 <yaoyu.094@gmail.com>\n\n* update conversion scripts\n\nSigned-off-by: yaoyu-33 <yaoyu.094@gmail.com>\n\n* Add exporting stuff\n\n* update conversion scripts\n\nSigned-off-by: yaoyu-33 <yaoyu.094@gmail.com>\n\n* Add readme\n\nSigned-off-by: yaoyu-33 <yaoyu.094@gmail.com>\n\n* Save readme\n\nSigned-off-by: yaoyu-33 <yaoyu.094@gmail.com>\n\n* Update jax\n\nSigned-off-by: yaoyu-33 <yaoyu.094@gmail.com>\n\n* Remove Gemma README_Gemma.rst\n\nSigned-off-by: yaoyu-33 <yaoyu.094@gmail.com>\n\n* Update import path\n\nSigned-off-by: yaoyu-33 <yaoyu.094@gmail.com>\n\n* Update docstring\n\nSigned-off-by: yaoyu-33 <yaoyu.094@gmail.com>\n\n* Revert \"Add exporting stuff\"\n\nThis reverts commit 17d00b0f64f074ace8cb2607377e1c1744a314fb.\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Remove neva cyclic imports\n\nSigned-off-by: yaoyu-33 <yaoyu.094@gmail.com>\n\n* Remove not used vars\n\nSigned-off-by: yaoyu-33 <yaoyu.094@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Revert \"Remove neva cyclic imports\"\n\nThis reverts commit 898d9ed5ee4052281aa443e612f8e3dc7c2d63ca.\n\n* Fix cyclic import\n\nSigned-off-by: yaoyu-33 <yaoyu.094@gmail.com>\n\n* remove neva folder\n\nSigned-off-by: yaoyu-33 <yaoyu.094@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove not used vars\n\nSigned-off-by: yaoyu-33 <yaoyu.094@gmail.com>\n\n* update docstrings in converter\n\nSigned-off-by: yaoyu-33 <yaoyu.094@gmail.com>\n\n* Address comments\n\nSigned-off-by: yaoyu-33 <yaoyu.094@gmail.com>\n\n* Add docstring\n\nSigned-off-by: yaoyu-33 <yaoyu.094@gmail.com>\n\n---------\n\nSigned-off-by: yaoyu-33 <yaoyu.094@gmail.com>\nCo-authored-by: Bobby Chen <bobchen@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/NVIDIA/NeMo",
        "filepath": "nemo/collections/multimodal/parts/utils.py",
        "commit_date": "2024-02-16T00:34:36Z",
        "message": "Fix dreambooth data sampler issue (#8400) (#8413)\n\n* Turn on drop last\n\n\n\n* Some neva fixes\n\n\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: yaoyu-33 <yaoyu.094@gmail.com>\nCo-authored-by: yaoyu-33 <54727607+yaoyu-33@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/NVIDIA/NeMo",
        "filepath": "nemo/collections/multimodal/parts/utils.py",
        "commit_date": "2024-01-20T01:59:23Z",
        "message": "Final multimodal PR with our recent developments on MM side (#8127)\n\n* Hotfix (#7501) (#7568)\n\nSigned-off-by: Jan Baczek <jbaczek@nvidia.com>\nCo-authored-by: jbaczek <45043825+jbaczek@users.noreply.github.com>\n\n* Avoid duplicated checkpoint save (#7555) (#7566)\n\nSigned-off-by: Miko\u0142aj B\u0142a\u017c <mblaz@nvidia.com>\nCo-authored-by: mikolajblaz <mikolajblaz@users.noreply.github.com>\n\n* Cache FP8 weight and transpose only at the first micro-batch in each validation and test routine (#7470) (#7483)\n\n* Cache weight and transpose only in the first batch in all training, val, and test runs\n\n\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Add an option to disable manual GC in validation (#7467) (#7476)\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\n\n* Remove PUBLICATIONS.md, point to github.io NeMo page instead (#7694) (#7695)\n\n* update publications section to point to blog website page\n\n\n\n* add hyphen\n\n\n\n* use double backquotes for code formatting\n\n\n\n---------\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\nSigned-off-by: Elena Rastorgueva <80532067+erastorgueva-nv@users.noreply.github.com>\nCo-authored-by: Elena Rastorgueva <80532067+erastorgueva-nv@users.noreply.github.com>\n\n* Fix multi rank finetune for ASR (#7684) (#7699)\n\n* Fix multi rank finetune for ASR\n\n\n\n* Actually add time\n\n\n\n* Actually add time\n\n\n\n---------\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\n\n* Update docs: readme, getting started, ASR intro (#7679)\n\n* [TTS] Add dataset to path of logged artifacts (#7462)\n\n* [TTS] Add dataset to path of logged artifacts\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Revert axis name back to Audio Frames\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* move install info to INSTALLATION.md\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* tidy up links\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix sft dataset truncation (#7464)\n\n* Add fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Automatic Lip Reading Recognition (ALR) - ASR/CV (Visual ASR) (#7330)\n\n* striding_conv1d_k5 and dw_striding_conv1d_k5 subsampling\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* transpose conv1d inputs\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* Update subsampling.py\n\nchange striding_conv1d_k5 to striding_conv1d\n\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\n\n* cv branch\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* video manifest\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* add collection classes\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add test_step_outputs\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* correct manifest bug when having only audio or only videos\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* correct manifest bug when having only audio or only videos\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* clean references\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* freeze unfreeze transcribe cv models\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* correct manifest get_full_path bug\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* update for PR\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* guard torchvision\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update nemo/collections/cv/data/video_to_text_dataset.py\n\nCo-authored-by: Igor Gitman <igor.a.gitman@gmail.com>\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\n\n* _video_speech_collate_fn in cv/data/video_to_text.py\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* add self.out = None to asr subsampling\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* Update nemo/collections/cv/data/video_to_text_dataset.py\n\nCo-authored-by: Igor Gitman <igor.a.gitman@gmail.com>\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\n\n* cv -> multimodal/speech_cv branch\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Igor Gitman <igor.a.gitman@gmail.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* HF StarCoder to NeMo conversion script (#7421)\n\n* Script to convert HF StarCoder checkpoint to NeMo\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* StarCoder conversion test\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Fix test\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Catch up with save_to changes\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Don't abbreviate args for clarity\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Configurable precision: BF16 vs FP32\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* fix bug when loading dist ckpt in peft (#7452)\n\nSigned-off-by: Hongbin Liu <hongbinl@nvidia.com>\nCo-authored-by: Hongbin Liu <hongbinl@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix adding positional embeddings in-place in transformer module (#7440)\n\nSigned-off-by: Tamerlan Tabolov <tktabolov@gmail.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix (#7478)\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* add sleep (#7498) (#7499)\n\n* add sleep\n\n\n\n* add sleep onto config instead\n\n\n\n* add comment\n\n\n\n---------\n\nSigned-off-by: Gerald Shen <geshen@nvidia.com>\nCo-authored-by: Gerald Shen <119401249+gshennvm@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix exp manager check for sleep (#7503) (#7504)\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* bugfix: trainer.accelerator=auto from None. (#7492) (#7493)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [doc] fix broken link (#7481)\n\nSigned-off-by: Stas Bekman <stas00@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [TTS] Read audio as int32 to avoid flac read errors (#7477)\n\n* [TTS] Read audio as int32 to avoid flac read errors\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Add comment about read failures\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Add dataset 'AISHELL-3' from OpenSLR for training mandarin TTS (#7409)\n\n* Add dataset 'AISHELL-3' from OpenSLR for training mandarin TTS\n* Train 'AISHELL-3' dataset with multi-speakers\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update get_data.py\n\nupdate copyright header\n\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* Update get_data.py\n\nadded a disclaimer\n\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add new configuration file for AISHELL3 with multispeaker of fastpitch\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* dllogger - log on rank 0 only (#7513)\n\nSigned-off-by: Stas Bekman <stas00@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix TTS FastPitch tutorial (#7494) (#7516)\n\n* Fix\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix get_dist() tensor dimension (#7506) (#7515)\n\nSigned-off-by: Jocelyn Huang <jocelynh@nvidia.com>\nCo-authored-by: Jocelyn <jocelynh@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* bugfix: specify trainer.strategy=auto when devices=1 (#7509) (#7512)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* fix (#7511)\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [TTS] Fix FastPitch data prep tutorial (#7524)\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* add italian tokenization (#7486)\n\n* add italian tokenization\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add more ipa lexicon it\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix error deletion\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* add test\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Replace None strategy with auto in tutorial notebooks (#7521) (#7527)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* unpin setuptools (#7534) (#7535)\n\nSigned-off-by: fayejf <36722593+fayejf@users.noreply.github.com>\nCo-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* remove auto generated examples (#7510)\n\n* explicitly remove autogenerated examples for data parallel evaluation\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* mark autogenrated and remove it for test\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: arendu <adithyare@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Add the `strategy` argument to `MegatronGPTModel.generate()` (#7264)\n\nIt is passed as an explicit argument rather than through\n`**strategy_args` so as to ensure someone cannot accidentally pass other\narguments that would end up being ignored.\n\nIt is a keyword-only argument to ensure that if in the future we want to\nupdate the signature to `**strategy_args`, we can do it without breaking\ncode.\n\nSigned-off-by: Olivier Delalleau <507137+odelalleau@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix PTL2.0 related ASR bugs in r1.21.0: Val metrics logging, None dataloader issue (#7531) (#7533)\n\n* fix none dataloader issue ptl2\n\n\n\n* ptl2.0 logging fixes for rnnt_models\n\n\n\n---------\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\nCo-authored-by: Kunal Dhawan <kunaldhawan97@gmail.com>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* gpus -> devices (#7542) (#7545)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Update FFMPEG version to fix issue with torchaudio (#7551) (#7553)\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* PEFT GPT & T5 Refactor (#7308)\n\n* initial implementation of add_adapters API\n\n* correct type hint\n\n* Add config in add_adapters for save and load (@author bobchen)\n\n* Remove AdapterConfig to avoid import error\n\n* Add AdaterConfig back and move adaptermixin to sft model\n\n* Add NLPSaveRestoreConnector as default in NLPModel.restore_from\n\n* Add restore_from_nemo_with_adapter and test script\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* rename t5 file and classes to be consistent with GPT\n\n* add t5 sft dataset\n\n* add support for single-file format with T5SFTDataset\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Various small changes to make T5 SFT work like GPT SFT\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add adapter evaluation test script\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add MultiAdaterConfig for ia3 and fix builder issue\n\n* Make ptuning for T5SFTModel work using mixin\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add IA3_Adapter for AdapterName\n\n* Add adapter name for ptuning and attention adapter\n\n* Make test script GPT/T5 agnostic\n\n* Add layer selection feature\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Integrate adapter name and config\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update gpt peft tuning script to new API\n\n* add t5 peft tuning script with new API\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix IA3 layer selection issue\n\n* Override state_dict on SFT model instead of mixin\n\n* Add load adapter by adapter config\n\n* move peft config map away from example script\n\n* auto get config from nemo adapter\n\n* Move PEFTConfig to new file\n\n* fix ckpt save/load for t5\n\n* name change: add_adapters -> add_adapter\n\n* variable name change\n\n* update t5 script\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix t5 issues\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add weight tying\n\n* update gpt tuning script\n\n* PEFT-API proposal\n\n* Fix according to comments\n\n* update tuning scripts\n\n* move merge_cfg_with to mixin class since it applies to both gpt and t5 and requires the model class for restore\n\n* Add mcore_gpt support for NLPAdapterMixin\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix typo\n\n* variable name change to distinguish \"peft\" and \"adapter\"\n\n* override `load_adapters` to support `add_adapter` name change\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update tuning and eval script for adapter save/load\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add Ptuning on first stage only\n\n* add lora tutorial for review\n\n* Fix layer selection for mcore\n\n* add landing page\n\n* fix resume training\n\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add mcore condition in sharded_state_dict to make sft work\n\n* Update lora_tutorial.md\n\nFirst edit of this file for PEFT documentation for NeMO\n\nSigned-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\n\n* rename Adapter to AttentionAdapter to avoid confusion in doc\n\n* Change load_adapters to load .nemo\n\n* add quick start guide\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add load_adapters with .ckpt\n\n* Remove setup_complete changes in load_adapters\n\n* update landing page\n\n* remove typo\n\n* Updated quick_start.md per Chen Cui\n\nSigned-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\n\n* Add inference config merger and tutorial\n\n* Add doc string for NLPAdapterModelMixin and deprecated warning on MegatronGPTPEFTModel\n\n* add supported_methods.md and update other documentations\n\n* Update supported_methods.md\n\nminor updates.\n\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\n\n* Update landing_page.md\n\nminor update.\n\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\n\n* Modify doc string for NLPAdapterModelMixin\n\n* Add doc string add_adapters in NLPAdapterModelMixin\n\n* rename canonical adapters\n\n* remove mcore hard dependency\n\n* [PATCH] move microbatch calculator to nemo from apex\n\n* remove apex dependency in gpt and t5 sft models\n\n* remove apex dependency in gpt model\n\n* render doc strings\n\n* fix\n\n* Add missing virtual_tokens on ptuning\n\n* fix docstrings\n\n* update gpt-style model coverage in docs\n\n* update docstring\n\n* Remove pdb\n\n* add lightning_fabric to make docstring rendering work\n\n* Add Ptuning missing key\n\n* try docstring rendering\n\n* Fix ptuning issue\n\n* update gpt t5 peft tuning and eval scripts\n\n* typos\n\n* update eval config\n\n* fix bug relating to apex dependency removal\n\n* typo\n\n* make predict step behave the same as test step\n\n* make lora tutorial work in notebook\n\n* cosmetics\n\n* update yaml scripts\n\n* mcore_gpt attribute optional\n\n* typo\n\n* update eval scripts and fix T5 eval bugs\n\n* add NLPDDPStrategyNotebook and trainer builder logic to use it\n\n* update lora notebook to use new trainer builder\n\n* fix microbatch calculator bug for inference after training\n\n* Convert markdown files to RST and incorporate with doc\n\n* typo\n\n* revise language\n\n* remove extra cell\n\n* remove unnecessary inheritance\n\n* remove old tests\n\n* move layer selection default so logging messages make sense\n\n* remove `save_adapters` as adapter weights are saved automatically during training\n\n* initialize weights from a checkpoint instead of randomly\n\n* multiple fields can form a context (#7147)\n\n* list of context fields and flexible prompt template\n\nSigned-off-by: arendu <adithya.r@gmail.com>\n\n* list of fields for context\n\nSigned-off-by: arendu <adithya.r@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Add multiple truncation fields and middle truncation\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Compatible to old ckpt\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix tokenize detokenize issue\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Remove detokenization, add truncation augmentation\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Resolve comments\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Remove unused import\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* revert eos\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Add tokenizer space_sensitive attribute\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix error\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Fix erorr and use re\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Change assert logic\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Follow adi suggestion\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Remove merge function\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add example and comment\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Remove context_key and add comment\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Remove random truncation\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix template none\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-off-by: arendu <adithya.r@gmail.com>\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nSigned-off-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\n* revert config changes\n\n* remove accidental breakpoint\n\n* support TP>1 loading\n\n* infer adapter type from checkpoint in during eval\n\n* breakup add adapter\n\n* enable interpolation of train_ds and validation_ds\n\n* update metric calc script to conform to single-file eval format\n\n* remove extraneous print\n\n* update lora notebook for updated merge_inference_cfg\n\n* Update nlp_adapter_mixins.py\n\nvariable name change\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* turn off grad scaler for PP to match old scripts\n\n* remove PEFTSaveRestoreConnector since functionality all covered by the new mixin class\n\n* remove resume_from_checkpoint check since covered in #7335\n\n* revert changes made in eval config interpolation\n\n* more interpolation\n\n* typo\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove dup line\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* code style warnings\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix config mistake\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* add copyright header\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* fix code check warnings\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* revert changes to remove apex dependency (mixed apex+nemo microbatch calculator broke some CI tests)\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add more deprecation notices\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* update deprecation notices\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* update deprecation notices\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* consolidate peft and sft scripts\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* update CI tests\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* notebook branch points to main to prepare for merge\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* fix gpt and t5 validation with any metric other than loss\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* support pre-extracted checkpoints\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n---------\n\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\nSigned-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\nSigned-off-by: arendu <adithya.r@gmail.com>\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nSigned-off-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\nSigned-off-by: Chen Cui <chcui@nvidia.com>\nCo-authored-by: Chen Cui <chcui@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Marc Romeyn <marcromeyn@gmail.com>\nCo-authored-by: jasonwan <jasonwan@nvidia.com>\nCo-authored-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\nCo-authored-by: Adi Renduchintala <adithyare@nvidia.com>\nCo-authored-by: Yuanzhe Dong <yudong@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* fix a typo (#7496)\n\nSigned-off-by: BestJuly <chntaoli@163.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [TTS] remove curly braces from ${BRANCH} in jupyer notebook cell. (#7554) (#7560)\n\n* remove curly braces.\n* remove installation of pynini.\n---------\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* add youtube embed url (#7570)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Remap speakers to continuous range of speaker_id for dataset AISHELL3 (#7536)\n\n* Remap speakers to continuous range of speaker_id for dataset AISHELL3\n* Add new key/value pair to record raw speaker for AISHELL3 dataset\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* fix validation_step_outputs initialization for multi-dataloader (#7546) (#7572)\n\n* added correct validation_step_outputs initialization for mutli-dataloader\n\n\n\n* changed kernel for display\n\n\n\n* Update logic for validation and test step outputs\n\n\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* revert multidataloader changes in multilang ASR notebook\n\n\n\n---------\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Kunal Dhawan <kunaldhawan97@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Append output of val step to self.validation_step_outputs (#7530) (#7532)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [TTS] fixed trainer's accelerator and strategy. (#7569) (#7574)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Append val/test output to instance variable in EncDecSpeakerLabelModel (#7562) (#7573)\n\n* Append val/test output to the instance variable in EncDecSpeakerLabelModel\n\n\n\n* Handle test case in evaluation_step\n\n\n\n* Replace type with isinstance\n\n\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix CustomProgressBar for resume (#7427) (#7522)\n\n* Fix CustomProgress Bar for resume and multiple epochs\n\n\n\n* Edit num_training_batches\n\n\n\n* Use max_steps as total for progress bar for resume\n\n\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* fix typos in nfa and speech enhancement tutorials (#7580) (#7583)\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\nCo-authored-by: Elena Rastorgueva <80532067+erastorgueva-nv@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Add strategy as ddp_find_unused_parameters_true for glue_benchmark.py (#7454) (#7461)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* update strategy (#7577) (#7578)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix typos (#7581)\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Change hifigan finetune strategy to ddp_find_unused_parameters_true (#7579) (#7584)\n\n* Change strategy to auto\n\n\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [BugFix] Add missing quotes for auto strategy in tutorial notebooks (#7541) (#7548)\n\n* Add missing quotes for auto strategy\n\n\n\n* Revert trainer.gpus to trainer.devices in Self_Supervised_Pre_Training.ipynb\n\n\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nSigned-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* add build os key (#7596) (#7599)\n\n* add build os key\n\n\n\n* add tools\n\n\n\n* update to stable version\n\n\n\n---------\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* StarCoder SFT test + bump PyT NGC image to 23.09 (#7540)\n\n* Add SFT StarCoder test\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Remove _modify_config call as it is covered in load_from_nemo just below\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Test with pyt:23.09 container\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n---------\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* defaults changed (#7600)\n\n* defaults changed\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* typo\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* update\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n---------\n\nSigned-off-by: arendu <adithyare@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* add ItalianPhonemesTokenizer (#7587)\n\n* add ItalianPhonemesTokenizer\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix Italian phonemes\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add test\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n---------\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* best ckpt fix (#7564) (#7588)\n\nSigned-off-by: dimapihtar <dpihtar@gmail.com>\nCo-authored-by: Dmytro Pykhtar <37850217+dimapihtar@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Add files via upload (#7598)\n\nspecifies the branch\n\nSigned-off-by: George <37293288+Jorjeous@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix validation in G2PModel and ThutmoseTaggerModel (#7597) (#7606)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain (#7576) (#7586)\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Safeguard nemo_text_processing installation on ARM (#7485)\n\n* safeguard nemo_text_processing installing\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update check\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n---------\n\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Bound transformers version in requirements (#7620)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* fix llama2 70b lora tuning bug (#7622)\n\n* fix llama2 70b lora tuning bug\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* Update peft_config.py\n\nbrackets\n\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\n\n---------\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\nCo-authored-by: Adi Renduchintala <adithyare@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix import error no module name model_utils (#7629)\n\nSigned-off-by: Mehadi Hasan Menon <mehadihasan80@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* add fc large ls models (#7641)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao Koluguri <nithinraok>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* bugfix: trainer.gpus, trainer.strategy, trainer.accelerator (#7621) (#7642)\n\n* [TTS] bugfix for Tacotron2 tutorial due to PTL 2.0\n* trainer.gpus -> trainer.devices\n* fixed related tutorial bugs\n---------\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* fix ssl models ptl monitor val through logging (#7608) (#7614)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix metrics for SE tutorial (#7604) (#7612)\n\nSigned-off-by: Ante Jukic\u0301 <ajukic@nvidia.com>\nCo-authored-by: anteju <108555623+anteju@users.noreply.github.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Add ddp_find_unused_parameters=True and change accelerator to auto (#7623) (#7644)\n\n* Add ddp_find_unused_parameters=True and change acclerator to auto\n\n\n\n* Add ddp_find_unused_parameters True for normalization_as_tagging_train.py\n\n\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix py3.11 dataclasses issue  (#7616)\n\n* Fix py3.11 dataclasses issue  (#7582)\n\n* Update ASR configs to support Python 3.11\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Update TTS configs to support Python 3.11\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Guard MeCab and Ipadic\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix remaining ASR dataclasses\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Fix remaining ASR dataclasses\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Fix scripts\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Update name to ConfidenceMethodConfig\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain (#7576) (#7586)\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Safeguard nemo_text_processing installation on ARM (#7485)\n\n* safeguard nemo_text_processing installing\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update check\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n---------\n\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Fix changes to confidence measure\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Jason <jasoli@nvidia.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Fix issues with Dockerfile (#7650) (#7652)\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [ASR] RNN-T greedy decoding max_frames fix for alignment and confidence (#7635)\n\n* decoding and test fix\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [ASR] Fix type error in jasper (#7636) (#7653)\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nCo-authored-by: Ryan Langman <rlangman@nvidia.com>\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* [TTS] Add STFT and SI-SDR loss to audio codec recipe (#7468)\n\n* [TTS] Add STFT and SI-SDR loss to audio codec recipe\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Fix STFT resolution\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Fix training metric logging\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Add docstring to mel and stft losses\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* add outline of asr quickstart info to asr/intro.rst\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* add CLI, LM and real-time transcription sections\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\n\n* Create per.py (#7538)\n\n* Move model precision copy (#7336)\n\n* move cfg precision set to megatron base model\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* remove copy from other models\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* modify attribute not arg\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* fix gpt model test for ptl 2.0\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* rename function and add docstring\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* replace precision to dtype conditionals with func call\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* unnecessary function and cfg reset\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* set default value\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* fix precision lookup in a few more places\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* rename mapping function\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* ununsed import\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* save torch datatype to model\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* set weights precision wrt amp o2\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* Revert \"set weights precision wrt amp o2\"\n\nThis reverts commit 313a4bfe5eb69d771a6d2433898c0685836aef5c.\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* revert half precision at inference attempt\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* move autocast dtype to base model\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* move params dtype to base model, enable fp16 O2 inf\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* unused imports\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n---------\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix PEFT checkpoint loading (#7388)\n\n* Fix PEFT checkpoint loading\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Use distributed optimizer support for multiple dtypes (#7359)\n\n* Update distopt wrapper with multiple dtype support\n\nRemove manual handling of separate FP32 optimizer.\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Use distopt support for contiguous buffers with multiple dtypes\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Fix typo\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Separate distopt buckets for first GPT layer and non-overlapped params\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Add distopt logic for int dtypes\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Update Apex commit\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Remove unused variables\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Update Apex commit in README and Jenkensfile\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Debug Dockerfile and Jenkinsfile\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n---------\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* minor fix for llama ckpt conversion script (#7387)\n\n* minor fix for llama ckpt conversion script\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* Update Jenkinsfile\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* remove fast_swiglu configuration\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n---------\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix wrong calling of librosa.get_duration() in notebook (#7376)\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [PATCH] PEFT import mcore (#7393)\n\n* [PATCH] PEFT import mcore\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Create per.py\n\nScript for calculation Punctuation Error Rate and related rates (correct rate, deletions rate, etc.)\n\nSigned-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [TTS] Added a callback for logging initial data (#7384)\n\nSigned-off-by: Ante Jukic\u0301 <ajukic@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update Core Commit (#7402)\n\n* Update Core Commit\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n* update commit\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n---------\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Use cfg attribute in bert (#7394)\n\n* use cfg attribute instead of arg\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* use torch_dtype in place of cfg.precision\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* move precision copy before super constructor\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* use trainer arg\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n---------\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add support for bias conversion in Swiglu models (#7386)\n\n* Add support for bias conversion in Swiglu models\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Add support for auto extracting tokenizer model\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add support for auto extracting tokenizer model\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Fix issue with missing tokenizer\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Refactor\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Refactor\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update save_to and restore_from for dist checkpointing (#7343)\n\n* add dist ckpt to save to, in progress\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* move dist ckpt\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* clean up\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update restore from, need to figure out how to initialize distributed\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* launch distrib if needed when restoring dist ckpt\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* when using mcore we can change tp pp on the fly\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* add load_from_checkpoint support for dist ckpt\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update llama convert script to save dist .nemo\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* fix load dist ckpt\n\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* setup TE TP groups if needed\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* setup te tp groups if needed\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* remove import\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n---------\n\nSigned-off-by: eharper <eharper@nvidia.com>\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: jasonwan <jasonwan@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* fix forward for with mcore=false (#7403)\n\nSigned-off-by: Jimmy Zhang <jiemingz@nvidia.com>\nCo-authored-by: Jimmy Zhang <jiemingz@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix logging to remove 's/it' from progress bar in Megatron models and add train_step_timing (#7374)\n\n* Add CustomProgressBar class to exp_manager and trainer callbacks\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix the progress bar to reflect total microbatch cnt\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* Modify CustomProgressBar class\n\n1) Modify CustomProgressBar class to update progress bar per global_step instead of per microbatch\n2) Add the callback to other megatron training/finetuning files that are not using MegatronTrainerBuilder\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* Add CustomProgressBar callback to tuning files\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Set Activation Checkpointing Defaults (#7404)\n\n* Set Activation Checkpointing Defaults\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* check for None\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* make loss mask default to false (#7407)\n\nSigned-off-by: eharper <eharper@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add dummy userbuffer config files (#7408)\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* add missing ubconf files (#7412)\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* New tutorial on Speech Data Explorer (#7405)\n\n* Added Google Colab based tutorial on Speech Data Explorer\n\nSigned-off-by: George Zelenfroynd <gzelenfroind@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update ptl training ckpt conversion script to work with dist ckpt (#7416)\n\n* update ptl convert script\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* don't break legacy\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: eharper <eharper@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Allow disabling sanity checking when num_sanity_val_steps=0 (#7413)\n\n* Allow disabling sanity checking when num_sanity_val_steps=0\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* Update num_sanity_val_steps to be a multiple of num_microbatches\n\nSigned-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nSigned-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add comprehensive error messages (#7261)\n\nSigned-off-by: Anton Peganov <apeganov@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* check NEMO_PATH (#7418)\n\nSigned-off-by: Nikolay Karpov <karpnv@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* layer selection for ia3 (#7417)\n\n* layer selection for ia3\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: arendu <adithyare@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix missing pip package 'einops' (#7397)\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix failure of pyaudio in Google Colab (#7396)\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update README.md: output_path --> output_manifest_filepath (#7442)\n\nSigned-off-by: Samuele Cornell <cornellsamuele@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add rope dynamic linear scaling (#7437)\n\n* Add dynamic linear scaling\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix None dataloader issue in PTL2.0 (#7455)\n\n* Fix None dataloader issue in PTL2.0\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* updating values of self._validation_dl and self._test_dl as well\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* updating values of self._validation_dl and self._test_dl as well\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [ASR] Confidence measure -> method renames (#7434)\n\n* measure -> method\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add steps for document of getting dataset 'SF Bilingual Speech' (#7378)\n\n* Add steps for document of getting dataset 'SF Bilingual Speech'\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update datasets.rst\n\nadded a link from a tutorial demonstrating detailed data prep steps.\n\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* RNN-T confidence and alignment bugfix (#7381)\n\n* new frame_confidence and alignments lists are now always created after the while loop\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* tests added\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix resume from checkpoint in exp_manager (#7424) (#7426)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix checking of cuda/cpu device for inputs of Decoder (#7444)\n\n* Fix checking of cuda/cpu device for inputs of Decoder\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update tacotron2.py\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: Jason <jasoli@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix failure of ljspeech's get_data.py (#7430)\n\n* Fix failure of ljspeech's get_data.py\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [TTS] Fix audio codec type checks (#7373)\n\n* [TTS] Fix audio codec type checks\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Fix audio codec tests\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [\u2026"
    },
    {
        "repo_url": "github.com/NVIDIA/NeMo",
        "filepath": "nemo/collections/multimodal/parts/utils.py",
        "commit_date": "2023-12-13T02:12:55Z",
        "message": "Add All Multimodal Source Code (#7791)\n\n* Add comprehensive error messages (#7261)\n\nSigned-off-by: Anton Peganov <apeganov@nvidia.com>\n\n* check NEMO_PATH (#7418)\n\nSigned-off-by: Nikolay Karpov <karpnv@gmail.com>\n\n* layer selection for ia3 (#7417)\n\n* layer selection for ia3\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: arendu <adithyare@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Fix missing pip package 'einops' (#7397)\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Fix failure of pyaudio in Google Colab (#7396)\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update README.md: output_path --> output_manifest_filepath (#7442)\n\nSigned-off-by: Samuele Cornell <cornellsamuele@gmail.com>\n\n* Updating FlashAttention API to match FlashAttentionV2\n\n* Multiple fixes for mm\n\n* Fix CI inductor issue and update to torch compile\n\n* Remove suppress error\n\n* Fix when conversion config uses fp16 and it complains about precision plugin\n\n* Fixing FAv2 API usage\n\n* Initial release of content filtering model\n\n* Added synthetic dataloader for precached and online mode\n\n* Mingyuanm/dreambooth opt\n\n* Add llama2 support in neva training\n\n* Fix sampler length\n\n* Fix all precision issues in nemo multimodal\n\n* Add rope dynamic linear scaling (#7437)\n\n* Add dynamic linear scaling\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>\n\n* Fix None dataloader issue in PTL2.0 (#7455)\n\n* Fix None dataloader issue in PTL2.0\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* updating values of self._validation_dl and self._test_dl as well\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* updating values of self._validation_dl and self._test_dl as well\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* [ASR] Confidence measure -> method renames (#7434)\n\n* measure -> method\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Add steps for document of getting dataset 'SF Bilingual Speech' (#7378)\n\n* Add steps for document of getting dataset 'SF Bilingual Speech'\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update datasets.rst\n\nadded a link from a tutorial demonstrating detailed data prep steps.\n\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* RNN-T confidence and alignment bugfix (#7381)\n\n* new frame_confidence and alignments lists are now always created after the while loop\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* tests added\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* Fix resume from checkpoint in exp_manager (#7424) (#7426)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\n\n* Fix checking of cuda/cpu device for inputs of Decoder (#7444)\n\n* Fix checking of cuda/cpu device for inputs of Decoder\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update tacotron2.py\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: Jason <jasoli@nvidia.com>\n\n* Fix failure of ljspeech's get_data.py (#7430)\n\n* Fix failure of ljspeech's get_data.py\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* [TTS] Fix audio codec type checks (#7373)\n\n* [TTS] Fix audio codec type checks\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Fix audio codec tests\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Add dataset to path of logged artifacts (#7462)\n\n* [TTS] Add dataset to path of logged artifacts\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Revert axis name back to Audio Frames\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* Fix sft dataset truncation (#7464)\n\n* Add fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Automatic Lip Reading Recognition (ALR) - ASR/CV (Visual ASR) (#7330)\n\n* striding_conv1d_k5 and dw_striding_conv1d_k5 subsampling\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* transpose conv1d inputs\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* Update subsampling.py\n\nchange striding_conv1d_k5 to striding_conv1d\n\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\n\n* cv branch\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* video manifest\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* add collection classes\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add test_step_outputs\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* correct manifest bug when having only audio or only videos\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* correct manifest bug when having only audio or only videos\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* clean references\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* freeze unfreeze transcribe cv models\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* correct manifest get_full_path bug\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* update for PR\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* guard torchvision\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update nemo/collections/cv/data/video_to_text_dataset.py\n\nCo-authored-by: Igor Gitman <igor.a.gitman@gmail.com>\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\n\n* _video_speech_collate_fn in cv/data/video_to_text.py\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* add self.out = None to asr subsampling\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* Update nemo/collections/cv/data/video_to_text_dataset.py\n\nCo-authored-by: Igor Gitman <igor.a.gitman@gmail.com>\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\n\n* cv -> multimodal/speech_cv branch\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: mburchi <maxime.burchi@gmail.com>\nSigned-off-by: Maxime Burchi <60737204+burchim@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Igor Gitman <igor.a.gitman@gmail.com>\n\n* HF StarCoder to NeMo conversion script (#7421)\n\n* Script to convert HF StarCoder checkpoint to NeMo\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* StarCoder conversion test\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Fix test\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Catch up with save_to changes\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Don't abbreviate args for clarity\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Configurable precision: BF16 vs FP32\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* fix bug when loading dist ckpt in peft (#7452)\n\nSigned-off-by: Hongbin Liu <hongbinl@nvidia.com>\nCo-authored-by: Hongbin Liu <hongbinl@nvidia.com>\n\n* Fix adding positional embeddings in-place in transformer module (#7440)\n\nSigned-off-by: Tamerlan Tabolov <tktabolov@gmail.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\n* Fix (#7478)\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* add sleep (#7498) (#7499)\n\n* add sleep\n\n\n\n* add sleep onto config instead\n\n\n\n* add comment\n\n\n\n---------\n\nSigned-off-by: Gerald Shen <geshen@nvidia.com>\nCo-authored-by: Gerald Shen <119401249+gshennvm@users.noreply.github.com>\n\n* Fix exp manager check for sleep (#7503) (#7504)\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\n\n* bugfix: trainer.accelerator=auto from None. (#7492) (#7493)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\n\n* [doc] fix broken link (#7481)\n\nSigned-off-by: Stas Bekman <stas00@users.noreply.github.com>\n\n* [TTS] Read audio as int32 to avoid flac read errors (#7477)\n\n* [TTS] Read audio as int32 to avoid flac read errors\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Add comment about read failures\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* Add dataset 'AISHELL-3' from OpenSLR for training mandarin TTS (#7409)\n\n* Add dataset 'AISHELL-3' from OpenSLR for training mandarin TTS\n* Train 'AISHELL-3' dataset with multi-speakers\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update get_data.py\n\nupdate copyright header\n\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* Update get_data.py\n\nadded a disclaimer\n\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add new configuration file for AISHELL3 with multispeaker of fastpitch\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* dllogger - log on rank 0 only (#7513)\n\nSigned-off-by: Stas Bekman <stas00@users.noreply.github.com>\n\n* Fix TTS FastPitch tutorial (#7494) (#7516)\n\n* Fix\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\n* Fix get_dist() tensor dimension (#7506) (#7515)\n\nSigned-off-by: Jocelyn Huang <jocelynh@nvidia.com>\nCo-authored-by: Jocelyn <jocelynh@nvidia.com>\n\n* bugfix: specify trainer.strategy=auto when devices=1 (#7509) (#7512)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\n\n* fix (#7511)\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n* [TTS] Fix FastPitch data prep tutorial (#7524)\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* add italian tokenization (#7486)\n\n* add italian tokenization\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add more ipa lexicon it\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix error deletion\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* add test\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Replace None strategy with auto in tutorial notebooks (#7521) (#7527)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* unpin setuptools (#7534) (#7535)\n\nSigned-off-by: fayejf <36722593+fayejf@users.noreply.github.com>\nCo-authored-by: fayejf <36722593+fayejf@users.noreply.github.com>\n\n* remove auto generated examples (#7510)\n\n* explicitly remove autogenerated examples for data parallel evaluation\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* mark autogenrated and remove it for test\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: arendu <adithyare@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Add the `strategy` argument to `MegatronGPTModel.generate()` (#7264)\n\nIt is passed as an explicit argument rather than through\n`**strategy_args` so as to ensure someone cannot accidentally pass other\narguments that would end up being ignored.\n\nIt is a keyword-only argument to ensure that if in the future we want to\nupdate the signature to `**strategy_args`, we can do it without breaking\ncode.\n\nSigned-off-by: Olivier Delalleau <507137+odelalleau@users.noreply.github.com>\n\n* Fix PTL2.0 related ASR bugs in r1.21.0: Val metrics logging, None dataloader issue (#7531) (#7533)\n\n* fix none dataloader issue ptl2\n\n\n\n* ptl2.0 logging fixes for rnnt_models\n\n\n\n---------\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\nCo-authored-by: Kunal Dhawan <kunaldhawan97@gmail.com>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\n\n* gpus -> devices (#7542) (#7545)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\n\n* Update FFMPEG version to fix issue with torchaudio (#7551) (#7553)\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\n\n* PEFT GPT & T5 Refactor (#7308)\n\n* initial implementation of add_adapters API\n\n* correct type hint\n\n* Add config in add_adapters for save and load (@author bobchen)\n\n* Remove AdapterConfig to avoid import error\n\n* Add AdaterConfig back and move adaptermixin to sft model\n\n* Add NLPSaveRestoreConnector as default in NLPModel.restore_from\n\n* Add restore_from_nemo_with_adapter and test script\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* rename t5 file and classes to be consistent with GPT\n\n* add t5 sft dataset\n\n* add support for single-file format with T5SFTDataset\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Various small changes to make T5 SFT work like GPT SFT\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add adapter evaluation test script\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add MultiAdaterConfig for ia3 and fix builder issue\n\n* Make ptuning for T5SFTModel work using mixin\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add IA3_Adapter for AdapterName\n\n* Add adapter name for ptuning and attention adapter\n\n* Make test script GPT/T5 agnostic\n\n* Add layer selection feature\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Integrate adapter name and config\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update gpt peft tuning script to new API\n\n* add t5 peft tuning script with new API\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix IA3 layer selection issue\n\n* Override state_dict on SFT model instead of mixin\n\n* Add load adapter by adapter config\n\n* move peft config map away from example script\n\n* auto get config from nemo adapter\n\n* Move PEFTConfig to new file\n\n* fix ckpt save/load for t5\n\n* name change: add_adapters -> add_adapter\n\n* variable name change\n\n* update t5 script\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix t5 issues\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add weight tying\n\n* update gpt tuning script\n\n* PEFT-API proposal\n\n* Fix according to comments\n\n* update tuning scripts\n\n* move merge_cfg_with to mixin class since it applies to both gpt and t5 and requires the model class for restore\n\n* Add mcore_gpt support for NLPAdapterMixin\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix typo\n\n* variable name change to distinguish \"peft\" and \"adapter\"\n\n* override `load_adapters` to support `add_adapter` name change\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update tuning and eval script for adapter save/load\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add Ptuning on first stage only\n\n* add lora tutorial for review\n\n* Fix layer selection for mcore\n\n* add landing page\n\n* fix resume training\n\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add mcore condition in sharded_state_dict to make sft work\n\n* Update lora_tutorial.md\n\nFirst edit of this file for PEFT documentation for NeMO\n\nSigned-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\n\n* rename Adapter to AttentionAdapter to avoid confusion in doc\n\n* Change load_adapters to load .nemo\n\n* add quick start guide\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add load_adapters with .ckpt\n\n* Remove setup_complete changes in load_adapters\n\n* update landing page\n\n* remove typo\n\n* Updated quick_start.md per Chen Cui\n\nSigned-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\n\n* Add inference config merger and tutorial\n\n* Add doc string for NLPAdapterModelMixin and deprecated warning on MegatronGPTPEFTModel\n\n* add supported_methods.md and update other documentations\n\n* Update supported_methods.md\n\nminor updates.\n\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\n\n* Update landing_page.md\n\nminor update.\n\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\n\n* Modify doc string for NLPAdapterModelMixin\n\n* Add doc string add_adapters in NLPAdapterModelMixin\n\n* rename canonical adapters\n\n* remove mcore hard dependency\n\n* [PATCH] move microbatch calculator to nemo from apex\n\n* remove apex dependency in gpt and t5 sft models\n\n* remove apex dependency in gpt model\n\n* render doc strings\n\n* fix\n\n* Add missing virtual_tokens on ptuning\n\n* fix docstrings\n\n* update gpt-style model coverage in docs\n\n* update docstring\n\n* Remove pdb\n\n* add lightning_fabric to make docstring rendering work\n\n* Add Ptuning missing key\n\n* try docstring rendering\n\n* Fix ptuning issue\n\n* update gpt t5 peft tuning and eval scripts\n\n* typos\n\n* update eval config\n\n* fix bug relating to apex dependency removal\n\n* typo\n\n* make predict step behave the same as test step\n\n* make lora tutorial work in notebook\n\n* cosmetics\n\n* update yaml scripts\n\n* mcore_gpt attribute optional\n\n* typo\n\n* update eval scripts and fix T5 eval bugs\n\n* add NLPDDPStrategyNotebook and trainer builder logic to use it\n\n* update lora notebook to use new trainer builder\n\n* fix microbatch calculator bug for inference after training\n\n* Convert markdown files to RST and incorporate with doc\n\n* typo\n\n* revise language\n\n* remove extra cell\n\n* remove unnecessary inheritance\n\n* remove old tests\n\n* move layer selection default so logging messages make sense\n\n* remove `save_adapters` as adapter weights are saved automatically during training\n\n* initialize weights from a checkpoint instead of randomly\n\n* multiple fields can form a context (#7147)\n\n* list of context fields and flexible prompt template\n\nSigned-off-by: arendu <adithya.r@gmail.com>\n\n* list of fields for context\n\nSigned-off-by: arendu <adithya.r@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Add multiple truncation fields and middle truncation\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Compatible to old ckpt\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix tokenize detokenize issue\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Remove detokenization, add truncation augmentation\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Resolve comments\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Remove unused import\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* revert eos\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Add tokenizer space_sensitive attribute\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix error\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Fix erorr and use re\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Change assert logic\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Follow adi suggestion\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Remove merge function\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add example and comment\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Remove context_key and add comment\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* Remove random truncation\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix template none\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-off-by: arendu <adithya.r@gmail.com>\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nSigned-off-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\n* revert config changes\n\n* remove accidental breakpoint\n\n* support TP>1 loading\n\n* infer adapter type from checkpoint in during eval\n\n* breakup add adapter\n\n* enable interpolation of train_ds and validation_ds\n\n* update metric calc script to conform to single-file eval format\n\n* remove extraneous print\n\n* update lora notebook for updated merge_inference_cfg\n\n* Update nlp_adapter_mixins.py\n\nvariable name change\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* turn off grad scaler for PP to match old scripts\n\n* remove PEFTSaveRestoreConnector since functionality all covered by the new mixin class\n\n* remove resume_from_checkpoint check since covered in #7335\n\n* revert changes made in eval config interpolation\n\n* more interpolation\n\n* typo\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove dup line\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* code style warnings\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix config mistake\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* add copyright header\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* fix code check warnings\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* revert changes to remove apex dependency (mixed apex+nemo microbatch calculator broke some CI tests)\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add more deprecation notices\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* update deprecation notices\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* update deprecation notices\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* consolidate peft and sft scripts\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* update CI tests\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* notebook branch points to main to prepare for merge\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* fix gpt and t5 validation with any metric other than loss\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* support pre-extracted checkpoints\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n---------\n\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\nSigned-off-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\nSigned-off-by: arendu <adithya.r@gmail.com>\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nSigned-off-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\nSigned-off-by: Chen Cui <chcui@nvidia.com>\nCo-authored-by: Chen Cui <chcui@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Marc Romeyn <marcromeyn@gmail.com>\nCo-authored-by: jasonwan <jasonwan@nvidia.com>\nCo-authored-by: hkelly33 <58792115+hkelly33@users.noreply.github.com>\nCo-authored-by: Adi Renduchintala <adithyare@nvidia.com>\nCo-authored-by: Yuanzhe Dong <yudong@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\n* fix a typo (#7496)\n\nSigned-off-by: BestJuly <chntaoli@163.com>\n\n* [TTS] remove curly braces from ${BRANCH} in jupyer notebook cell. (#7554) (#7560)\n\n* remove curly braces.\n* remove installation of pynini.\n---------\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\n\n* add youtube embed url (#7570)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\n\n* Remap speakers to continuous range of speaker_id for dataset AISHELL3 (#7536)\n\n* Remap speakers to continuous range of speaker_id for dataset AISHELL3\n* Add new key/value pair to record raw speaker for AISHELL3 dataset\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* fix validation_step_outputs initialization for multi-dataloader (#7546) (#7572)\n\n* added correct validation_step_outputs initialization for mutli-dataloader\n\n\n\n* changed kernel for display\n\n\n\n* Update logic for validation and test step outputs\n\n\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* revert multidataloader changes in multilang ASR notebook\n\n\n\n---------\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Kunal Dhawan <kunaldhawan97@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Append output of val step to self.validation_step_outputs (#7530) (#7532)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* [TTS] fixed trainer's accelerator and strategy. (#7569) (#7574)\n\nSigned-off-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <16880-xueyang@users.noreply.gitlab-master.nvidia.com>\n\n* Append val/test output to instance variable in EncDecSpeakerLabelModel (#7562) (#7573)\n\n* Append val/test output to the instance variable in EncDecSpeakerLabelModel\n\n\n\n* Handle test case in evaluation_step\n\n\n\n* Replace type with isinstance\n\n\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* Fix CustomProgressBar for resume (#7427) (#7522)\n\n* Fix CustomProgress Bar for resume and multiple epochs\n\n\n\n* Edit num_training_batches\n\n\n\n* Use max_steps as total for progress bar for resume\n\n\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* fix typos in nfa and speech enhancement tutorials (#7580) (#7583)\n\nSigned-off-by: Elena Rastorgueva <erastorgueva@nvidia.com>\nCo-authored-by: Elena Rastorgueva <80532067+erastorgueva-nv@users.noreply.github.com>\n\n* Add strategy as ddp_find_unused_parameters_true for glue_benchmark.py (#7454) (#7461)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* update strategy (#7577) (#7578)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\n\n* Fix typos (#7581)\n\n* Change hifigan finetune strategy to ddp_find_unused_parameters_true (#7579) (#7584)\n\n* Change strategy to auto\n\n\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: Cheng-Ping Hsieh <37269846+hsiehjackson@users.noreply.github.com>\n\n* [BugFix] Add missing quotes for auto strategy in tutorial notebooks (#7541) (#7548)\n\n* Add missing quotes for auto strategy\n\n\n\n* Revert trainer.gpus to trainer.devices in Self_Supervised_Pre_Training.ipynb\n\n\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nSigned-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* add build os key (#7596) (#7599)\n\n* add build os key\n\n\n\n* add tools\n\n\n\n* update to stable version\n\n\n\n---------\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\n\n* StarCoder SFT test + bump PyT NGC image to 23.09 (#7540)\n\n* Add SFT StarCoder test\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Remove _modify_config call as it is covered in load_from_nemo just below\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* Test with pyt:23.09 container\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n---------\n\nSigned-off-by: Jan Lasek <janek.lasek@gmail.com>\n\n* defaults changed (#7600)\n\n* defaults changed\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* typo\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* update\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n---------\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* add ItalianPhonemesTokenizer (#7587)\n\n* add ItalianPhonemesTokenizer\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix Italian phonemes\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add test\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\n\n---------\n\nSigned-off-by: GiacomoLeoneMaria <giacomoleonemaria@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* best ckpt fix (#7564) (#7588)\n\nSigned-off-by: dimapihtar <dpihtar@gmail.com>\nCo-authored-by: Dmytro Pykhtar <37850217+dimapihtar@users.noreply.github.com>\n\n* Add files via upload (#7598)\n\nspecifies the branch\n\nSigned-off-by: George <37293288+Jorjeous@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* Fix validation in G2PModel and ThutmoseTaggerModel (#7597) (#7606)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain (#7576) (#7586)\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Safeguard nemo_text_processing installation on ARM (#7485)\n\n* safeguard nemo_text_processing installing\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update check\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n---------\n\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Bound transformers version in requirements (#7620)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* fix llama2 70b lora tuning bug (#7622)\n\n* fix llama2 70b lora tuning bug\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\n\n* Update peft_config.py\n\nbrackets\n\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\n\n---------\n\nSigned-off-by: Chen Cui <chcui@nvidia.com>\nSigned-off-by: Adi Renduchintala <adithyare@nvidia.com>\nCo-authored-by: Adi Renduchintala <adithyare@nvidia.com>\n\n* Fix import error no module name model_utils (#7629)\n\nSigned-off-by: Mehadi Hasan Menon <mehadihasan80@gmail.com>\n\n* add fc large ls models (#7641)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao Koluguri <nithinraok>\n\n* bugfix: trainer.gpus, trainer.strategy, trainer.accelerator (#7621) (#7642)\n\n* [TTS] bugfix for Tacotron2 tutorial due to PTL 2.0\n* trainer.gpus -> trainer.devices\n* fixed related tutorial bugs\n---------\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* fix ssl models ptl monitor val through logging (#7608) (#7614)\n\nSigned-off-by: Nithin Rao Koluguri <nithinraok>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n* Fix metrics for SE tutorial (#7604) (#7612)\n\nSigned-off-by: Ante Jukic\u0301 <ajukic@nvidia.com>\nCo-authored-by: anteju <108555623+anteju@users.noreply.github.com>\n\n* Add ddp_find_unused_parameters=True and change accelerator to auto (#7623) (#7644)\n\n* Add ddp_find_unused_parameters=True and change acclerator to auto\n\n\n\n* Add ddp_find_unused_parameters True for normalization_as_tagging_train.py\n\n\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* Fix py3.11 dataclasses issue  (#7616)\n\n* Fix py3.11 dataclasses issue  (#7582)\n\n* Update ASR configs to support Python 3.11\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Update TTS configs to support Python 3.11\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Guard MeCab and Ipadic\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix remaining ASR dataclasses\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Fix remaining ASR dataclasses\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Fix scripts\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Update name to ConfidenceMethodConfig\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain (#7576) (#7586)\n\n* Broadcast loss only when using pipeline parallelism and within the pipeline parallel domain\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Safeguard nemo_text_processing installation on ARM (#7485)\n\n* safeguard nemo_text_processing installing\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update check\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n---------\n\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Fix changes to confidence measure\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>\nCo-authored-by: Sangkug Lym <slym@nvidia.com>\nCo-authored-by: Jason <jasoli@nvidia.com>\n\n* [Stable Diffusion/ControlNet] Enable O2 training for SD and Fix ControlNet CI failure\n\n* Mingyuanm/dreambooth fix\n\n* Fix NeMo CI Infer Issue\n\n* DreamFusion\n\n* Move neva export changes\n\n* Add Imagen Synthetic Dataloader\n\n* Add VITWrapper and export stuff to wrapper\n\n* Update neva with megatron-core support\n\n* Fix issues with Dockerfile (#7650) (#7652)\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\n\n* [ASR] RNN-T greedy decoding max_frames fix for alignment and confidence (#7635)\n\n* decoding and test fix\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* [ASR] Fix type error in jasper (#7636) (#7653)\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nCo-authored-by: Ryan Langman <rlangman@nvidia.com>\n\n* [TTS] Add STFT and SI-SDR loss to audio codec recipe (#7468)\n\n* [TTS] Add STFT and SI-SDR loss to audio codec recipe\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Fix STFT resolution\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Fix training metric logging\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Add docstring to mel and stft losses\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Create per.py (#7538)\n\n* Move model precision copy (#7336)\n\n* move cfg precision set to megatron base model\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* remove copy from other models\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* modify attribute not arg\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* fix gpt model test for ptl 2.0\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* rename function and add docstring\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* replace precision to dtype conditionals with func call\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* unnecessary function and cfg reset\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* set default value\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* fix precision lookup in a few more places\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* rename mapping function\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* ununsed import\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* save torch datatype to model\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* set weights precision wrt amp o2\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* Revert \"set weights precision wrt amp o2\"\n\nThis reverts commit 313a4bfe5eb69d771a6d2433898c0685836aef5c.\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* revert half precision at inference attempt\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* move autocast dtype to base model\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* move params dtype to base model, enable fp16 O2 inf\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* unused imports\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n---------\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix PEFT checkpoint loading (#7388)\n\n* Fix PEFT checkpoint loading\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Use distributed optimizer support for multiple dtypes (#7359)\n\n* Update distopt wrapper with multiple dtype support\n\nRemove manual handling of separate FP32 optimizer.\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Use distopt support for contiguous buffers with multiple dtypes\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Fix typo\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Separate distopt buckets for first GPT layer and non-overlapped params\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Add distopt logic for int dtypes\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Update Apex commit\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Remove unused variables\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Update Apex commit in README and Jenkensfile\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n* Debug Dockerfile and Jenkinsfile\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\n\n---------\n\nSigned-off-by: Tim Moon <tmoon@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* minor fix for llama ckpt conversion script (#7387)\n\n* minor fix for llama ckpt conversion script\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* Update Jenkinsfile\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* remove fast_swiglu configuration\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n---------\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix wrong calling of librosa.get_duration() in notebook (#7376)\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nCo-authored-by: Somshubra Majumdar <titu1994@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [PATCH] PEFT import mcore (#7393)\n\n* [PATCH] PEFT import mcore\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Jason Wang <jasonwan@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Create per.py\n\nScript for calculation Punctuation Error Rate and related rates (correct rate, deletions rate, etc.)\n\nSigned-off-by: Sasha Meister <117230141+ssh-meister@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [TTS] Added a callback for logging initial data (#7384)\n\nSigned-off-by: Ante Jukic\u0301 <ajukic@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update Core Commit (#7402)\n\n* Update Core Commit\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n* update commit\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n---------\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Use cfg attribute in bert (#7394)\n\n* use cfg attribute instead of arg\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* use torch_dtype in place of cfg.precision\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* move precision copy before super constructor\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n* use trainer arg\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\n\n---------\n\nSigned-off-by: Maanu Grover <maanug@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add support for bias conversion in Swiglu models (#7386)\n\n* Add support for bias conversion in Swiglu models\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Add support for auto extracting tokenizer model\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add support for auto extracting tokenizer model\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Fix issue with missing tokenizer\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Refactor\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* Refactor\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: smajumdar <titu1994@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update save_to and restore_from for dist checkpointing (#7343)\n\n* add dist ckpt to save to, in progress\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* move dist ckpt\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* clean up\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update restore from, need to figure out how to initialize distributed\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* launch distrib if needed when restoring dist ckpt\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* when using mcore we can change tp pp on the fly\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* add load_from_checkpoint support for dist ckpt\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update llama convert script to save dist .nemo\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* fix load dist ckpt\n\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* setup TE TP groups if needed\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* setup te tp groups if needed\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* remove import\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n---------\n\nSigned-off-by: eharper <eharper@nvidia.com>\nSigned-off-by: jasonwan <jasonwan@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: jasonwan <jasonwan@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* fix forward for with mcore=false (#7403)\n\nSigned-off-by: Jimmy Zhang <jiemingz@nvidia.com>\nCo-authored-by: Jimmy Zhang <jiemingz@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix logging to remove 's/it' from progress bar in Megatron models and add train_step_timing (#7374)\n\n* Add CustomProgressBar class to exp_manager and trainer callbacks\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix the progress bar to reflect total microbatch cnt\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* Modify CustomProgressBar class\n\n1) Modify CustomProgressBar class to update progress bar per global_step instead of per microbatch\n2) Add the callback to other megatron training/finetuning files that are not using MegatronTrainerBuilder\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* Add CustomProgressBar callback to tuning files\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Set Activation Checkpointing Defaults (#7404)\n\n* Set Activation Checkpointing Defaults\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* check for None\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* make loss mask default to false (#7407)\n\nSigned-off-by: eharper <eharper@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add dummy userbuffer config files (#7408)\n\nSigned-off-by: Sangkug Lym <slym@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* add missing ubconf files (#7412)\n\nSigned-off-by: Abhinav Khattar <aklife97@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* New tutorial on Speech Data Explorer (#7405)\n\n* Added Google Colab based tutorial on Speech Data Explorer\n\nSigned-off-by: George Zelenfroynd <gzelenfroind@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update ptl training ckpt conversion script to work with dist ckpt (#7416)\n\n* update ptl convert script\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* don't break legacy\n\nSigned-off-by: eharper <eharper@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: eharper <eharper@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Allow disabling sanity checking when num_sanity_val_steps=0 (#7413)\n\n* Allow disabling sanity checking when num_sanity_val_steps=0\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\n\n* Update num_sanity_val_steps to be a multiple of num_microbatches\n\nSigned-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nSigned-off-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add comprehensive error messages (#7261)\n\nSigned-off-by: Anton Peganov <apeganov@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* check NEMO_PATH (#7418)\n\nSigned-off-by: Nikolay Karpov <karpnv@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* layer selection for ia3 (#7417)\n\n* layer selection for ia3\n\nSigned-off-by: arendu <adithyare@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: arendu <adithyare@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix missing pip package 'einops' (#7397)\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix failure of pyaudio in Google Colab (#7396)\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Update README.md: output_path --> output_manifest_filepath (#7442)\n\nSigned-off-by: Samuele Cornell <cornellsamuele@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add rope dynamic linear scaling (#7437)\n\n* Add dynamic linear scaling\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix bug\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\n\n---------\n\nSigned-off-by: Cheng-Ping Hsieh <chsieh@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Yang Zhang <yzhang123@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix None dataloader issue in PTL2.0 (#7455)\n\n* Fix None dataloader issue in PTL2.0\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* updating values of self._validation_dl and self._test_dl as well\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* updating values of self._validation_dl and self._test_dl as well\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: KunalDhawan <kunaldhawan97@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [ASR] Confidence measure -> method renames (#7434)\n\n* measure -> method\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Add steps for document of getting dataset 'SF Bilingual Speech' (#7378)\n\n* Add steps for document of getting dataset 'SF Bilingual Speech'\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update datasets.rst\n\nadded a link from a tutorial demonstrating detailed data prep steps.\n\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nCo-authored-by: Xuesong Yang <1646669+XuesongYang@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* RNN-T confidence and alignment bugfix (#7381)\n\n* new frame_confidence and alignments lists are now always created after the while loop\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n* tests added\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\n\n---------\n\nSigned-off-by: Aleksandr Laptev <alaptev@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix resume from checkpoint in exp_manager (#7424) (#7426)\n\nSigned-off-by: Abhishree <abhishreetm@gmail.com>\nCo-authored-by: Abhishree Thittenamane <47577437+athitten@users.noreply.github.com>\nCo-authored-by: Eric Harper <complex451@gmail.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix checking of cuda/cpu device for inputs of Decoder (#7444)\n\n* Fix checking of cuda/cpu device for inputs of Decoder\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* Update tacotron2.py\n\nSigned-off-by: Jason <jasoli@nvidia.com>\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nSigned-off-by: Jason <jasoli@nvidia.com>\nCo-authored-by: Jason <jasoli@nvidia.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* Fix failure of ljspeech's get_data.py (#7430)\n\n* Fix failure of ljspeech's get_data.py\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Robin Dong <robin.k.dong@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nSigned-off-by: Sasha Meister <sasha.meister.work@gmail.com>\n\n* [TTS] Fix audio codec type checks (#7373)\n\n* [TTS] Fix audio codec type checks\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n* [TTS] Fix audio codec tests\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\n\n---------\n\nSigned-off-by: Ryan <rlangman@nvidia.com>\nSigne\u2026"
    },
    {
        "repo_url": "github.com/nlpxucan/WizardLM",
        "filepath": "training/src/generate.py",
        "commit_date": "2023-07-08T14:21:04Z",
        "message": "training code"
    },
    {
        "repo_url": "github.com/artidoro/qlora",
        "filepath": "examples/guanaco_generate.py",
        "commit_date": "2023-07-23T22:05:52Z",
        "message": "generage example"
    },
    {
        "repo_url": "github.com/ray-project/ray",
        "filepath": "doc/source/templates/04_finetuning_llms_with_deepspeed/merge_lora_weights.py",
        "commit_date": "2023-10-19T03:13:00Z",
        "message": "[Train][Templates] Add LoRA support to Llama-2 finetuning example (#37794)\n\nSigned-off-by: Artur Niederfahrenhorst <attaismyname@googlemail.com>\nCo-authored-by: kourosh hakhamaneshi <31483498+kouroshHakha@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/RUCAIBox/LLMSurvey",
        "filepath": "Experiments/InstructTuning/mmlu/modeling.py",
        "commit_date": "2023-06-29T09:47:51Z",
        "message": "add:prompts"
    },
    {
        "repo_url": "github.com/apple/ml-ferret",
        "filepath": "ferret/model/builder.py",
        "commit_date": "2023-10-31T03:44:41Z",
        "message": "first code commit"
    },
    {
        "repo_url": "github.com/NVIDIA/TensorRT-LLM",
        "filepath": "examples/enc_dec/run.py",
        "commit_date": "2024-02-27T09:37:34Z",
        "message": "Update TensorRT-LLM (#1168)\n\n* Update TensorRT-LLM\n\n---------\n\nCo-authored-by: Bhuvanesh Sridharan <bhuvan.sridharan@gmail.com>\nCo-authored-by: Shixiaowei02 <39303645+Shixiaowei02@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/NVIDIA/TensorRT-LLM",
        "filepath": "examples/enc_dec/run.py",
        "commit_date": "2024-02-21T13:30:55Z",
        "message": "Update TensorRT-LLM (#1122)\n\n* Update TensorRT-LLM\n\n---------\n\nCo-authored-by: Eddie-Wang1120 <wangjinheng1120@163.com>\nCo-authored-by: Shixiaowei02 <39303645+Shixiaowei02@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/NVIDIA/TensorRT-LLM",
        "filepath": "examples/enc_dec/run.py",
        "commit_date": "2024-02-18T07:48:08Z",
        "message": "Update TensorRT-LLM (#1098)\n\n* Update TensorRT-LLM\n\n* update submodule\n\n* Remove unused binaries"
    },
    {
        "repo_url": "github.com/NVIDIA/TensorRT-LLM",
        "filepath": "examples/enc_dec/run.py",
        "commit_date": "2024-01-31T13:55:32Z",
        "message": "Update TensorRT-LLM (#1019)\n\n* Update TensorRT-LLM\n\n---------\n\nCo-authored-by: erenup <ping.nie@pku.edu.cn>\nCo-authored-by: Shixiaowei02 <39303645+Shixiaowei02@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/NVIDIA/TensorRT-LLM",
        "filepath": "examples/enc_dec/run.py",
        "commit_date": "2024-01-23T15:22:35Z",
        "message": "Update TensorRT-LLM (#941)\n\n* Update TensorRT-LLM\n\n---------\n\nCo-authored-by: Shixiaowei02 <39303645+Shixiaowei02@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/NVIDIA/TensorRT-LLM",
        "filepath": "examples/enc_dec/run.py",
        "commit_date": "2024-01-09T13:03:35Z",
        "message": "Update TensorRT-LLM (#846)\n\n* Update TensorRT-LLM\n\n---------\n\nCo-authored-by: Shixiaowei02 <39303645+Shixiaowei02@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/NVIDIA/TensorRT-LLM",
        "filepath": "examples/enc_dec/run.py",
        "commit_date": "2024-01-02T09:54:32Z",
        "message": "Update TensorRT-LLM (#787)\n\n* Update TensorRT-LLM\n\n---------\n\nCo-authored-by: Shixiaowei02 <39303645+Shixiaowei02@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/NVIDIA/TensorRT-LLM",
        "filepath": "examples/enc_dec/run.py",
        "commit_date": "2023-12-27T09:41:24Z",
        "message": "Update TensorRT-LLM main branch (#754)\n\n* Update TensorRT-LLM\n\n---------\n\nCo-authored-by: Shixiaowei02 <39303645+Shixiaowei02@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/NVIDIA/TensorRT-LLM",
        "filepath": "examples/enc_dec/run.py",
        "commit_date": "2023-12-15T14:14:51Z",
        "message": "Update TensorRT-LLM (#667)\n\n* Update TensorRT-LLM\n\n---------\n\nCo-authored-by: 0xymoro <jerrymeng100@gmail.com>\nCo-authored-by: Shixiaowei02 <39303645+Shixiaowei02@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/NVIDIA/TensorRT-LLM",
        "filepath": "examples/enc_dec/run.py",
        "commit_date": "2023-12-08T09:49:24Z",
        "message": "Update TensorRT-LLM (#613)\n\n* Update TensorRT-LLM\n\n---------\n\nCo-authored-by: Shixiaowei02 <39303645+Shixiaowei02@users.noreply.github.com>\nCo-authored-by: zhang-ge-hao <842720660@qq.com>"
    },
    {
        "repo_url": "github.com/NVIDIA/TensorRT-LLM",
        "filepath": "examples/enc_dec/run.py",
        "commit_date": "2023-12-01T14:27:51Z",
        "message": "Update TensorRT-LLM (#524)"
    },
    {
        "repo_url": "github.com/NVIDIA/TensorRT-LLM",
        "filepath": "examples/enc_dec/run.py",
        "commit_date": "2023-11-24T14:12:26Z",
        "message": "Update TensorRT-LLM (#465)\n\n* Update TensorRT-LLM\n\n---------\n\nCo-authored-by: Shixiaowei02 <39303645+Shixiaowei02@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/NVIDIA/TensorRT-LLM",
        "filepath": "examples/enc_dec/run.py",
        "commit_date": "2023-11-17T16:05:54Z",
        "message": "Update TensorRT-LLM (#422)\n\n* Update TensorRT-LLM\n\n---------\n\nCo-authored-by: Tltin <TltinDeng01@gmail.com>\nCo-authored-by: zhaohb <zhaohbcloud@126.com>\nCo-authored-by: Bradley Heilbrun <brad@repl.it>\nCo-authored-by: nqbao11 <nqbao11.01@gmail.com>\nCo-authored-by: Nikhil Varghese <nikhil@bot-it.ai>"
    },
    {
        "repo_url": "github.com/NVIDIA/TensorRT-LLM",
        "filepath": "examples/enc_dec/run.py",
        "commit_date": "2023-10-18T14:38:53Z",
        "message": "Kaiyu/update main (#5)\n\n* Update\n\n* Update"
    },
    {
        "repo_url": "github.com/pengxiao-song/LaWGPT",
        "filepath": "merge.py",
        "commit_date": "2023-05-22T17:11:58Z",
        "message": "[ENH] Add merge.py"
    },
    {
        "repo_url": "github.com/LianjiaTech/BELLE",
        "filepath": "train/src/trainer.py",
        "commit_date": "2023-09-26T09:10:33Z",
        "message": "add ppo (#519)\n\n* add ppo\n\n* update doc\n\n* update readme"
    },
    {
        "repo_url": "github.com/LianjiaTech/BELLE",
        "filepath": "train/src/trainer.py",
        "commit_date": "2023-08-10T10:27:54Z",
        "message": "add zero inference"
    },
    {
        "repo_url": "github.com/LianjiaTech/BELLE",
        "filepath": "train/src/trainer.py",
        "commit_date": "2023-07-18T15:44:57Z",
        "message": "add peft & deepspeed + peft resume"
    },
    {
        "repo_url": "github.com/LianjiaTech/BELLE",
        "filepath": "train/src/trainer.py",
        "commit_date": "2023-06-15T15:58:12Z",
        "message": "remove peft\nadjust source tree\nfix peft + deepspeed save model\nadd client server inference"
    },
    {
        "repo_url": "github.com/pengxiao-song/LaWGPT",
        "filepath": "infer.py",
        "commit_date": "2023-06-06T13:00:30Z",
        "message": "[MNT] Fix infer scripts"
    },
    {
        "repo_url": "github.com/pengxiao-song/LaWGPT",
        "filepath": "infer.py",
        "commit_date": "2023-05-22T13:37:02Z",
        "message": "[ENH] Add infer.py"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_llama.py",
        "commit_date": "2023-06-24T11:02:21Z",
        "message": "\u5c06\u6240\u6709\u8bf7\u6c42\u53c2\u6570\u5168\u90e8\u66b4\u9732\u7ed9\u6a21\u578b"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_llama.py",
        "commit_date": "2023-06-09T12:59:54Z",
        "message": "Update llm_llama.py"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_llama.py",
        "commit_date": "2023-06-09T11:19:33Z",
        "message": "Update llm_llama.py"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_llama.py",
        "commit_date": "2023-06-08T06:24:43Z",
        "message": "Update llm_llama.py"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_llama.py",
        "commit_date": "2023-05-30T17:58:22Z",
        "message": "LLAMA\u7cfb\u5217\u652f\u6301torch\u63a8\u7406\uff0c\u4f7f\u7528\u548cRWKV\u76f8\u540c\u7684strategy\u903b\u8f91\u8fdb\u884c\u63a7\u5236torch\u548cllamacpp\u7684\u5207\u6362"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_llama.py",
        "commit_date": "2023-05-07T10:20:26Z",
        "message": "\u7ed3\u6784\u8c03\u6574"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_glm6b.py",
        "commit_date": "2023-11-03T13:02:23Z",
        "message": "Update llm_glm6b.py"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_glm6b.py",
        "commit_date": "2023-11-03T11:59:39Z",
        "message": "\u652f\u6301chatglm3\u5de5\u5177\u4f7f\u7528\uff0c\u5e76\u589e\u52a0auto\u793a\u4f8b"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_glm6b.py",
        "commit_date": "2023-11-03T05:34:41Z",
        "message": "\u4fee\u590dchatglm3\u5386\u53f2\u5bf9\u8bdd"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_glm6b.py",
        "commit_date": "2023-07-08T15:58:10Z",
        "message": "\u4fee\u6539device_map\u4ee5\u652f\u6301ChatGLM2\u591a\u5361\u8fd0\u884c\uff0c\u7528\u6cd5\u53ca\u53c2\u6570\u8bbe\u7f6e\u4e0d\u53d8"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_glm6b.py",
        "commit_date": "2023-06-30T06:15:54Z",
        "message": "Update llm_glm6b.py"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_glm6b.py",
        "commit_date": "2023-06-29T13:17:39Z",
        "message": "Update llm_glm6b.py"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_glm6b.py",
        "commit_date": "2023-06-29T12:37:32Z",
        "message": "Revert \"Update llm_glm6b.py\"\n\nThis reverts commit 58f01e6823a9c2bf64a98df2da59123d9fa1dd49."
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_glm6b.py",
        "commit_date": "2023-06-27T17:09:56Z",
        "message": "Update llm_glm6b.py"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_glm6b.py",
        "commit_date": "2023-06-26T08:10:11Z",
        "message": "torch1.x\u652f\u6301chatglm2"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_glm6b.py",
        "commit_date": "2023-06-24T11:02:21Z",
        "message": "\u5c06\u6240\u6709\u8bf7\u6c42\u53c2\u6570\u5168\u90e8\u66b4\u9732\u7ed9\u6a21\u578b"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_glm6b.py",
        "commit_date": "2023-06-24T10:52:44Z",
        "message": "\u4f18\u5316Lora\u5728\u7ebf\u5207\u6362\u903b\u8f91\uff0c\u767e\u5ddd\u652f\u6301\u4f18\u5316Lora\u5728\u7ebf\u5207\u6362"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_glm6b.py",
        "commit_date": "2023-06-15T13:15:44Z",
        "message": "Update llm_glm6b.py"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_glm6b.py",
        "commit_date": "2023-06-15T13:13:31Z",
        "message": "-"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_glm6b.py",
        "commit_date": "2023-06-15T13:05:08Z",
        "message": "\u521d\u6b65\u652f\u6301aquila\u7684hf\u7248"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_glm6b.py",
        "commit_date": "2023-06-01T03:21:33Z",
        "message": "\u9759\u6001\u8d44\u6e90\u8def\u7531\u7531fastapi\u5b9e\u73b0"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_glm6b.py",
        "commit_date": "2023-05-30T17:58:22Z",
        "message": "LLAMA\u7cfb\u5217\u652f\u6301torch\u63a8\u7406\uff0c\u4f7f\u7528\u548cRWKV\u76f8\u540c\u7684strategy\u903b\u8f91\u8fdb\u884c\u63a7\u5236torch\u548cllamacpp\u7684\u5207\u6362"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_glm6b.py",
        "commit_date": "2023-05-27T13:17:26Z",
        "message": "\u589e\u52a0GLM-6B Lora\u5728\u7ebf\u5207\u6362\u529f\u80fd"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_glm6b.py",
        "commit_date": "2023-05-07T10:20:26Z",
        "message": "\u7ed3\u6784\u8c03\u6574"
    },
    {
        "repo_url": "github.com/pengxiao-song/LaWGPT",
        "filepath": "webui.py",
        "commit_date": "2023-05-22T09:18:21Z",
        "message": "[MNT] Update webui.py"
    },
    {
        "repo_url": "github.com/pengxiao-song/LaWGPT",
        "filepath": "webui.py",
        "commit_date": "2023-05-22T07:47:47Z",
        "message": "[MNT] Update some file"
    },
    {
        "repo_url": "github.com/pengxiao-song/LaWGPT",
        "filepath": "webui.py",
        "commit_date": "2023-05-21T18:39:04Z",
        "message": "[ENH] Restructure the project."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/export_hf_checkpoint.py",
        "commit_date": "2023-08-19T19:46:34Z",
        "message": "Merge remote-tracking branch 'origin/main' into perf"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/export_hf_checkpoint.py",
        "commit_date": "2023-08-17T17:23:04Z",
        "message": "Fixes #678"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/export_hf_checkpoint.py",
        "commit_date": "2023-08-15T04:24:27Z",
        "message": "WIP for performance benchmarks. Use h2ogpt models."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/export_hf_checkpoint.py",
        "commit_date": "2023-07-31T22:49:26Z",
        "message": "Add 70b"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/export_hf_checkpoint.py",
        "commit_date": "2023-07-31T06:06:48Z",
        "message": "Create 7b/13b merges."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/export_hf_checkpoint.py",
        "commit_date": "2023-07-28T19:57:19Z",
        "message": "Create llama2 export."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/export_hf_checkpoint.py",
        "commit_date": "2023-07-12T20:03:00Z",
        "message": "Refactors/cleanups, still fails the same."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/export_hf_checkpoint.py",
        "commit_date": "2023-07-12T00:27:30Z",
        "message": "Improve naming of vars."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/export_hf_checkpoint.py",
        "commit_date": "2023-07-11T22:50:32Z",
        "message": "Add test code."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/export_hf_checkpoint.py",
        "commit_date": "2023-07-11T19:26:05Z",
        "message": "Fix model loader refactor."
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/export_hf_checkpoint.py",
        "commit_date": "2023-07-11T08:25:19Z",
        "message": "Simpler use of model loader, and fix instructions to avoid CUDA extension not installed issue"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/export_hf_checkpoint.py",
        "commit_date": "2023-07-08T05:10:09Z",
        "message": "Fix test_export_copy"
    },
    {
        "repo_url": "github.com/h2oai/h2ogpt",
        "filepath": "src/export_hf_checkpoint.py",
        "commit_date": "2023-07-07T22:27:22Z",
        "message": "Move files to src"
    },
    {
        "repo_url": "github.com/RUCAIBox/LLMSurvey",
        "filepath": "Experiments/InstructTuning/auto_eval/generate.py",
        "commit_date": "2023-06-29T09:47:51Z",
        "message": "add:prompts"
    },
    {
        "repo_url": "github.com/nlpxucan/WizardLM",
        "filepath": "WizardLM/src/infer_wizardlm13b.py",
        "commit_date": "2023-06-14T08:55:50Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_baichuan.py",
        "commit_date": "2023-07-19T15:14:25Z",
        "message": "\u589e\u52a0generic_transformers\u6a21\u7ec4\uff0c\u7ecf\u6d4b\u8bd5\u53ef\u517c\u5bb9Llama-2-13B-chat-GPTQ"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_baichuan.py",
        "commit_date": "2023-07-19T12:58:15Z",
        "message": "\u767e\u5ddd\u6a21\u7ec4\u652f\u6301\u81ea\u5b9a\u4e49interface"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_baichuan.py",
        "commit_date": "2023-07-19T12:14:18Z",
        "message": "Update llm_baichuan.py"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_baichuan.py",
        "commit_date": "2023-07-11T14:44:34Z",
        "message": "\u767e\u5ddd13b\u81ea\u52a8\u91cf\u5316"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_baichuan.py",
        "commit_date": "2023-07-07T10:45:35Z",
        "message": "\u767e\u5ddd\u589e\u52a0GPTQ\u652f\u6301"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_baichuan.py",
        "commit_date": "2023-06-25T03:59:18Z",
        "message": "-"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_baichuan.py",
        "commit_date": "2023-06-24T16:55:38Z",
        "message": "Update llm_baichuan.py"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_baichuan.py",
        "commit_date": "2023-06-24T16:48:48Z",
        "message": "Update llm_baichuan.py"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_baichuan.py",
        "commit_date": "2023-06-24T16:23:44Z",
        "message": "Update llm_baichuan.py"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_baichuan.py",
        "commit_date": "2023-06-24T11:02:21Z",
        "message": "\u5c06\u6240\u6709\u8bf7\u6c42\u53c2\u6570\u5168\u90e8\u66b4\u9732\u7ed9\u6a21\u578b"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_baichuan.py",
        "commit_date": "2023-06-24T10:52:44Z",
        "message": "\u4f18\u5316Lora\u5728\u7ebf\u5207\u6362\u903b\u8f91\uff0c\u767e\u5ddd\u652f\u6301\u4f18\u5316Lora\u5728\u7ebf\u5207\u6362"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_baichuan.py",
        "commit_date": "2023-06-24T10:02:43Z",
        "message": "\u767e\u5ddd\u652f\u6301\u6d41\u5f0f\u8f93\u51fa\u548c\u591a\u7528\u6237\u5e76\u884c\uff08\u9700\u66f4\u65b0transformers\uff09"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_baichuan.py",
        "commit_date": "2023-06-24T03:26:36Z",
        "message": "\u767e\u5ddd\u66f4\u65b0\uff1alora\u652f\u6301\u3001\u5bf9\u8bddprompt\u3001raw\u6a21\u5f0f\u3001\u591a\u8f6e\u5bf9\u8bdd"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_baichuan.py",
        "commit_date": "2023-06-15T09:24:42Z",
        "message": "Update llm_baichuan.py"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_baichuan.py",
        "commit_date": "2023-06-15T07:25:15Z",
        "message": "\u5b9e\u9a8c\u6027\u652f\u6301aquila\u3001baichuan"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/kto_trainer.py",
        "commit_date": "2024-03-01T11:19:55Z",
        "message": "[KTO] prevent nans from appearing in metrics (#1386)\n\n* add warning for imbalanced data\n\n* update documentation\n\n* update script commands to be same as in dpo\n\n* use batch_size KL examples and batch_size target examples to calculate batch_size losses\n\n* fix deepspeed issue\n\n* speed up forward with no_grad for KL\n\n* add some removed metrics\n\n* Update trl/trainer/kto_trainer.py\n\n* Update trl/trainer/kto_trainer.py\n\n* Update trl/trainer/kto_trainer.py\n\nadd reference to paper\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* add more detailed comments\n\n* convert assert to ValueError\n\n* Update kto_trainer.py\n\n* precommit formatting\n\n* remove nans in metrics by gathering across machines\n\n* fix formatting\n\n---------\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/kto_trainer.py",
        "commit_date": "2024-03-01T11:15:14Z",
        "message": "[KTO] merge eval dataset only if it exists (#1383)\n\n* merge eval dataset if it exists\n\n* add eval dataset test"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/kto_trainer.py",
        "commit_date": "2024-02-29T08:01:52Z",
        "message": "fix bugs in KTO implementation (#1380)\n\n* add warning for imbalanced data\n\n* update documentation\n\n* update script commands to be same as in dpo\n\n* use batch_size KL examples and batch_size target examples to calculate batch_size losses\n\n* fix deepspeed issue\n\n* speed up forward with no_grad for KL\n\n* add some removed metrics\n\n* Update trl/trainer/kto_trainer.py\n\n* Update trl/trainer/kto_trainer.py\n\n* Update trl/trainer/kto_trainer.py\n\nadd reference to paper\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* add more detailed comments\n\n* convert assert to ValueError\n\n* Update kto_trainer.py\n\n* precommit formatting\n\n---------\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/kto_trainer.py",
        "commit_date": "2024-02-19T13:43:17Z",
        "message": "Kto trainer (#1181)\n\n* initial file\n\n* initial tokenizer\n\n* UnpairedPreferenceBatchSampler\n\n* use batch_sampler\n\n* use interleave_datasets\n\n* add loss\n\n* fix imports\n\n* use SequentialSampler when training\n\n* formatting\n\n* add other helpers\n\n* add prediction_step\n\n* fix the kto pair docs\n\n* tests\n\n* compute_reference_log_probs\n\n* add get_eval_dataloader\n\n* fix typo\n\n* kto with is_encoder_decoder true\n\n* Update docs/source/dpo_trainer.mdx\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* fixed typo\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update docs/source/kto_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update docs/source/kto_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* renamed KTO dataset keys\n\n* use DPOTrainer's get_batch_logps\n\n* add get_batch_samples\n\n* typo\n\n* Handle last token in prompt\n\n* Create KTOConfig class that subclasses transformers.TrainingArguments\n\n* Update KTO tests to handle KTOConfig\n\n* Update KTO script to use KTOConfig\n\n* formatting\n\n* Update docs/source/dpo_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update docs/source/kto_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update docs/source/kto_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update trl/trainer/training_configs.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update examples/scripts/kto.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update examples/scripts/kto.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* use max_completion_length\n\n* Update examples/scripts/kto.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* add back get_batch_logps\n\n* use max_completion_length\n\n* move config to its own file\n\n* Check tokenize params on Trainer init\n\n* Clone labels for end-dec model to solve RuntimeError\n\n* formatting\n\n* fix enc-dec later\n\n* completion_decoder_input_ids is optional for enc-dec\n\n* fix breaking test\n\n* add a kl key for KL estimation with shuffled completion\n\n* add loss ad weights\n\n* fix bug in chosen_idx\n\n* add back metrics\n\n* fix typos\n\n* fix kto_loss docs\n\n* typo\n\n* set loss to None when there is no target completions in batch\n\n* use nan tensor instead of none\n\n* fix reference_logps test\n\n* fix logits\n\n* a bit more robust options\n\n* log only the correct prompt-completion during eval\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update examples/scripts/kto.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update examples/scripts/kto.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update docs/source/kto_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update docs/source/dpo_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* add docs for desirable_weight and undesirable_weight args\n\n* dropout is always disabled\n\n* remove DDP hack\n\n* formatting\n\n* move more arguments of trainer to config\n\n* comment out T5 test for now\n\n* Add docstring to KTOTrainer\n\n* moved Config docstrings to the appropriate class\n\n* add autodoc to markdown\n\n* formatting\n\n* updated copyright year\n\n* add model tags\n\n* do not add BOS to start of completion\n\n* Move data_collator to KTOTrainer\n\n* formatting\n\n* data_collator is not in args\n\n* shuffle_completion with specific input_columns\n\n* remove all but the needed columns\n\n* Update docs/source/dpo_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update examples/scripts/kto.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update tests/test_kto_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* moved more args to kto_config\n\n* fjx test\n\n* use all_exhausted strategy and shuffle after\n\n* use KTOConfig in HfArgumentParser\n\n* use ModelConfig\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\nCo-authored-by: Pablo Vicente Juan <p.vicente.juan@gmail.com>"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "chat.py",
        "commit_date": "2023-05-10T12:30:03Z",
        "message": "update code"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "chat.py",
        "commit_date": "2023-05-10T09:08:35Z",
        "message": "update code"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "chat.py",
        "commit_date": "2023-04-18T13:48:23Z",
        "message": "add OutOfMemory exception handler."
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "chat.py",
        "commit_date": "2023-04-04T11:39:35Z",
        "message": "rearrange code; add html code support"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "chat.py",
        "commit_date": "2023-04-01T14:11:04Z",
        "message": "add better support for chat (streamly beam search,sample,greedy and beam-sample; cancel button, more prompt type and so on )"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "chat.py",
        "commit_date": "2023-03-30T06:35:32Z",
        "message": "link you can share"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "chat.py",
        "commit_date": "2023-03-28T16:21:03Z",
        "message": "add support for clear history; fix original reset btn"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2024-02-27T10:19:16Z",
        "message": "feature request add `force_use_ref_model` (#1367)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2024-02-19T13:43:17Z",
        "message": "Kto trainer (#1181)\n\n* initial file\n\n* initial tokenizer\n\n* UnpairedPreferenceBatchSampler\n\n* use batch_sampler\n\n* use interleave_datasets\n\n* add loss\n\n* fix imports\n\n* use SequentialSampler when training\n\n* formatting\n\n* add other helpers\n\n* add prediction_step\n\n* fix the kto pair docs\n\n* tests\n\n* compute_reference_log_probs\n\n* add get_eval_dataloader\n\n* fix typo\n\n* kto with is_encoder_decoder true\n\n* Update docs/source/dpo_trainer.mdx\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* fixed typo\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update docs/source/kto_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update docs/source/kto_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* renamed KTO dataset keys\n\n* use DPOTrainer's get_batch_logps\n\n* add get_batch_samples\n\n* typo\n\n* Handle last token in prompt\n\n* Create KTOConfig class that subclasses transformers.TrainingArguments\n\n* Update KTO tests to handle KTOConfig\n\n* Update KTO script to use KTOConfig\n\n* formatting\n\n* Update docs/source/dpo_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update docs/source/kto_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update docs/source/kto_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update trl/trainer/training_configs.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update examples/scripts/kto.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update examples/scripts/kto.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* use max_completion_length\n\n* Update examples/scripts/kto.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* add back get_batch_logps\n\n* use max_completion_length\n\n* move config to its own file\n\n* Check tokenize params on Trainer init\n\n* Clone labels for end-dec model to solve RuntimeError\n\n* formatting\n\n* fix enc-dec later\n\n* completion_decoder_input_ids is optional for enc-dec\n\n* fix breaking test\n\n* add a kl key for KL estimation with shuffled completion\n\n* add loss ad weights\n\n* fix bug in chosen_idx\n\n* add back metrics\n\n* fix typos\n\n* fix kto_loss docs\n\n* typo\n\n* set loss to None when there is no target completions in batch\n\n* use nan tensor instead of none\n\n* fix reference_logps test\n\n* fix logits\n\n* a bit more robust options\n\n* log only the correct prompt-completion during eval\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update examples/scripts/kto.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update examples/scripts/kto.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update docs/source/kto_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update docs/source/dpo_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* add docs for desirable_weight and undesirable_weight args\n\n* dropout is always disabled\n\n* remove DDP hack\n\n* formatting\n\n* move more arguments of trainer to config\n\n* comment out T5 test for now\n\n* Add docstring to KTOTrainer\n\n* moved Config docstrings to the appropriate class\n\n* add autodoc to markdown\n\n* formatting\n\n* updated copyright year\n\n* add model tags\n\n* do not add BOS to start of completion\n\n* Move data_collator to KTOTrainer\n\n* formatting\n\n* data_collator is not in args\n\n* shuffle_completion with specific input_columns\n\n* remove all but the needed columns\n\n* Update docs/source/dpo_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update examples/scripts/kto.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update tests/test_kto_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* moved more args to kto_config\n\n* fjx test\n\n* use all_exhausted strategy and shuffle after\n\n* use KTOConfig in HfArgumentParser\n\n* use ModelConfig\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\nCo-authored-by: Pablo Vicente Juan <p.vicente.juan@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2024-02-15T13:47:32Z",
        "message": "[`core` /  `xxxTrainer`] Automatic tagging (#1329)\n\n* automatic tagging\n\n* add comments\n\n* fix tests\n\n* fix"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2024-02-02T10:02:40Z",
        "message": "Fix AttributeError in dpo_trainer for reference_free case in dpo_loss function (#1313)\n\n* Update dpo_trainer.py\n\nupdate reference_free parameter for dpo_loss\n\n* Update dpo_trainer for reference_free case\n\nupdated the docstring typo and set device parameter to ref_logratios tensor"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2024-02-01T16:58:00Z",
        "message": "Add num_proc arg to the eval_dataset processing (#1307)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2024-01-31T13:51:58Z",
        "message": "Types: Fix PEP 484 implicit-optional compliance (#1297)\n\nThis was done automatically with hauntsaninja/no_implicit_optional."
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2024-01-31T13:49:41Z",
        "message": "Fix `DPOTrainer` docstrings (#1298)\n\nSome issues were leading the auto-generation of the API reference to fail and the args were overlapped in the documentation page"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2024-01-30T09:14:22Z",
        "message": "load data only on main process + fix dpo example test (#1291)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2024-01-30T07:25:29Z",
        "message": "fix DPO trainer + mistral + FA2 (#1290)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2024-01-30T07:24:48Z",
        "message": "fix padding in dpo trainer (#1284)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2024-01-30T07:06:03Z",
        "message": "raise value error if one passes a ref_model and a peft_config (#1289)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2024-01-30T01:55:07Z",
        "message": "Add multiprocessing in the DPO trainer. (#1286)\n\n* Update dpo_trainer.py\n\nAdded support for num_proc to tokenize the training dataset.\n\n* Update dpo_trainer.py\n\nadded type in the new num_proc variable\n\n* added test case\n\n* add test case\n\n* fix type\n\n---------\n\nCo-authored-by: imraviagrawal <ravi.agrawal@umass.edu>\nCo-authored-by: Ravi Agrawal <raviagrawal@Ravis-MacBook-Pro.local>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2024-01-24T11:18:04Z",
        "message": "[DPO] average_log_prob when loss is IPO (#1265)\n\n* average_log_prob when loss is IPO\n\n* updated docs with the fix"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2024-01-18T16:20:52Z",
        "message": "fix: fix loss_type and some args desc (#1247)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2024-01-15T14:40:44Z",
        "message": "Update dpo_trainer.py (#1160)\n\nLog metrics on all distributed processes"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2024-01-09T13:10:22Z",
        "message": "Check tokenize params on DPOTrainer (#1197)\n\n* Check if tokenizer and max len params are None\n\n* Update warning messages for missing parameters"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2024-01-09T08:35:50Z",
        "message": "[`DPOTrainer`] Fix peft + DPO + bf16 if one uses `generate_during_eval` or pre-computed logits (#1203)\n\n* fix peft + DPO + bf16\n\n* fix\n\n* revert old behaviour\n\n* fix tests\n\n* fix\n\n* fix\n\n* fix\n\n* fix"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2024-01-08T15:12:45Z",
        "message": "Allow swapping PEFT adapters for target/ref model. (#1193)\n\n* Allow swapping PEFT adapters for target/ref model.\n\n* Update DPOTrainer docs.\n\n* python format\n\n* isort\n\n* Update docs/source/dpo_trainer.mdx\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update docs/source/dpo_trainer.mdx\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update docs/source/dpo_trainer.mdx\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update docs/source/dpo_trainer.mdx\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n* Update docs/source/dpo_trainer.mdx\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>\n\n---------\n\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2024-01-08T09:26:40Z",
        "message": "Allow separate devices for target/ref models. (#1190)\n\n* Allow separate devices for target/ref models.\n\n* Remove original/duplicate.\n\n* Cleanup original, black formatting.\n\n---------\n\nCo-authored-by: Jon Durbin <jonathan@convai.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2024-01-08T08:15:53Z",
        "message": "Handle last token from generation prompt (#1153)\n\n* Handle last token from generation prompt\n\n* Remove prints\n\n* Reformat dpo_trainer file"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-12-29T09:55:58Z",
        "message": "change device order of metrics (#1154)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-12-26T15:39:10Z",
        "message": "[`xxxTrainer`] Add unsloth tag (#1130)\n\n* add unsloth tag\n\n* add it on all trainers\n\n* few changes\n\n* add in docs\n\n* revert\n\n* final commit"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-12-26T10:25:53Z",
        "message": "add peft_module_casting_to_bf16 in DPOTrainer (#1143)\n\n* add peft_module_casting_to_bf16 in DPOTrainer\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>\n\n* Update trl/trainer/dpo_trainer.py\n\n---------\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>\nCo-authored-by: Kashif Rasul <kashif.rasul@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-12-22T13:52:16Z",
        "message": "[`xxxTrainer`] multi-tags support for tagging (#1133)\n\n* multi-tags support for tagging\n\n* oops"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-12-22T12:32:16Z",
        "message": "rename kto loss (#1127)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-12-21T16:28:56Z",
        "message": "save eval_dataset for subsequent calls (#1125)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-12-21T16:04:18Z",
        "message": "[`xxxTrainer`] Add tags to all trainers in TRL (#1120)\n\n* add tags to sfttrainer\n\n* extend it to other trainers\n\n* add for ddpo"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-12-12T16:16:46Z",
        "message": "[DPO] use ref model logprobs if it exists in the data (#885)\n\n* use logprobs if it exists in the batch\n\n* add features to tokenized batch if in data\n\n* make get_batch_logps a static method\n\n* add tokenize_batch_element dataset mapper\n\n* Remove tokenize_batch method from DPODataCollator\n\n* Initial sketch to precompute reference_logps\n\n* run ref model via pytorch dataloader\n\n* add a padding helper\n\n* clean up the helper\n\n* use logprob item()\n\n* default behaviour\n\n* clean up collator\n\n* add docstring\n\n* copy data back to cpu if needed\n\n* use get_train_dataloader methods\n\n* fix tests\n\n* rename: more explicit variable name precompute_ref_log_probs\n\n* improve comment\n\n* update comment\n\n* Update trl/trainer/dpo_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* refactor models into setup parameters\n\n* parametrize precompute_ref_log_probs flag\n\n* remove useless test\n\n* Update trl/trainer/dpo_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update tests/test_dpo_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update tests/test_dpo_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update trl/trainer/dpo_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update trl/trainer/dpo_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* update function arg name\n\n* distinguish between pad token_id and mask values\n\n* fix tokenization #932 by @nrailg\n\n* fix test\n\n* undo test refactor\n\n* new line\n\n* undo breaking change\n\n* Update token counter condition to allow Llama tokenizer\n\n* Acount for merged tokens on certain tokenizers such Llama-2 tokenizer\n\n* Update variable name to match list value when truncating response\n\n* map function on multi-gpu and gather\n\n* Add test cases for DPOTrainer tokenization step\n\n* revert since we need the prepeared model\n\n* Use gather_with_metrics on ref_logps precomputation to keep original dataset size\n\n* Add flag to keep track of when ref_logps are precomputed\n\n* make variable names private\n\n* formatting\n\n* if precompute_ref_log_probs is true one can use non-peft to populate log-probs\n\n* Use tokenizer padding token unless padding_value is set\n\n* Move dataset.map(tokenize_batch) outside dataloader to avoid serialization errors\n\n* eval can be none\n\n* move to cpu to avoid gpu oom\n\n* remove unneeded cast to float32\n\n* remove unneeded\n\n* fix merge\n\n* fix merge\n\n* fix merge\n\n* add precompute log-prob status via tqdm\n\n* Truncate answer if too longer once prompt has been truncated\n\n* Add prompt_input_ids to batch to enable generation\n\n* formatting and add lora example\n\n* fix formatting\n\n* Tokenize row now expects sample to have space on chosen/rejected for llama\n\n* Revert \"Tokenize row now expects sample to have space on chosen/rejected for llama\"\n\nThis reverts commit dd07a10fe8c19b6ac6bbcc7b8144189756710d52.\n\n* raise error when using zero-3 with precompute_ref_log_probs\n\n---------\n\nCo-authored-by: Pablo Vicente Juan <p.vicente.juan@gmail.com>\nCo-authored-by: Shoaib Burq <saburq@gmail.com>\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-12-11T10:41:03Z",
        "message": "[DPO] add KTO loss (#1075)\n\n* add KTO loss\n\n* fix docs\n\n* Update trl/trainer/dpo_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* formatting\n\n* add link to papers\n\n---------\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-12-07T07:40:53Z",
        "message": "Add missing `loss_type` in `ValueError` message (#1067)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-12-01T16:03:09Z",
        "message": "Update dpo_trainer.py (#1049)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-12-01T09:33:31Z",
        "message": "Revert \"[DPO] Refactor eval logging of dpo trainer (#954)\" (#1047)\n\nThis reverts commit 6d9ea38ae18c7e266f797b62de4a68a12a13aba4."
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-11-30T11:09:33Z",
        "message": "[DPO] Refactor eval logging of dpo trainer (#954)\n\n* first attempts at refactor of dpo trainer\n\n* removed extra stuff in prediction step\n\n* import fixes\n\n* label names\n\n* all working\n\n---------\n\nCo-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-11-30T10:50:30Z",
        "message": "[DPO] cDPO loss (#1035)\n\n* add cDPO loss\n\n* add comment\n\n* docs\n\n* info about label_smoothing not being used"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-11-24T14:52:40Z",
        "message": "[DPO] IPO Training loss (#1022)\n\n* initial IPO loss\n\n* fix loss\n\n* fixed comments\n\n* added docs\n\n* fix doc-strings\n\n* add tests\n\n* Update trl/trainer/dpo_trainer.py\n\nCo-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>\n\n* fixes for review\n\n* Added doc about beta in the Trainer's docstring\n\n---------\n\nCo-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-11-06T13:37:04Z",
        "message": "fix: dpo trainer ds config (#957)\n\n* fix: dpo trainer ds config\n\nref_model and model shouldn share the same ds config, so we shouldn modify the ds config directly. or else, it will cause sth wrong when init deepspeed engine\n\n* fix: import sort\n\nimport sort by isort"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-11-06T08:48:18Z",
        "message": "Adds model kwargs to SFT and DPO trainers (#951)\n\n* adds model kwargs to SFT and DPO trainers\n\n* adds checks for model_kwarg passing when model is not str\n\n* changed warning to ValueError\n\n* renames model_kwargs to model_init_kwargs\n\n* corrects argument names in"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-11-06T08:45:46Z",
        "message": "[DPO] Merge initial peft model if trainer has a peft_config (#956)\n\n* failing test\nCo-authored-by: Shoaib Burq <saburq@gmail.com>\n\n* merge initial peft model"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-11-02T10:27:49Z",
        "message": "Update dpo_trainer.py (#941)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-10-31T17:50:17Z",
        "message": "[`core` / `DDP`] Fix RM trainer + DDP + quantization + propagate `gradient_checkpointing_kwargs` in SFT & DPO (#912)\n\n* make use of forward hooks\n\n* correctly delete attributes\n\n* fix RM DPP issues\n\n* revert unneeded changes\n\n* more fixes\n\n* fix diff\n\n* fix\n\n* propagate to SFT\n\n* Update examples/scripts/reward_modeling.py\n\n* propagate the fix on DPO trainer\n\n* add to example scripts\n\n* trigger CI"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-10-31T09:58:41Z",
        "message": "hotfix for dpo trainer (#919)\n\naddresses #914"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-10-31T09:55:46Z",
        "message": "fix DPO + GC issues (#927)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-10-16T14:02:57Z",
        "message": "[DPO] add SLiC hinge loss to DPOTrainer (#866)\n\n* add SLiC hinge loss\n\n* fix links\n\n* beta when loss is hinge is reciprocal of margin\n\n* fix tests\n\n* fix docs\n\n* doc strings\n\n* fix method name\n\n* raise error if loss_type is not correct\n\n* Update trl/trainer/dpo_trainer.py\n\nCo-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>\n\n* fix formatting\n\n---------\n\nCo-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-10-03T07:56:00Z",
        "message": "Fix DeepSpeed ZeRO-{1,2} for DPOTrainer (#825)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-09-29T10:33:47Z",
        "message": "add option for compute_metrics in DPOTrainer (#822)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-09-26T15:09:15Z",
        "message": "init custom eval loop for further DPO evals (#766)\n\n* init\n\n* run\n\n* Update custom eval loop to aid DPO debugging (#770)\n\n* sample_during_eval -> generate_during_eval\n\n* Remove unused return_tokens\n\n* Add import utils for W&B, prevent test fails\n\n* Optimize dataloader random batch selection\n\n* Separate prompt and response in logs\n\nMakes it much easier to quickly read the starts of the generations\n\n* Simplify logging\n\n* reset eval steps\n\n* manual merge fixes\n\n* revert merge\n\n* remove self.max_length\n\n* style\n\n* fix max_length\n\n---------\n\nCo-authored-by: Tom Aarsen <37621491+tomaarsen@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-09-12T08:56:10Z",
        "message": "update to `prepare_model_for_kbit_training` (#728)\n\n* update to `prepare_model_for_kbit_training`\n\nfrom deprecated `prepare_model_for_int8_training`\nand add `use_gradient_checkpointing=args.gradient_checkpointing` to\nautomatically follow the gradient checkpointing choice\n\nis also the workaround for #694\n\n* workaround for gradient checkpointing issue\n\ncalling model.gradient_checkpointing_enable() twice causes issues\nthis workaround calls it in prepare_model_for_kbit_training and then\nchanges the arg to false to make sure it isn't called again in\nhuggingface trainer inner loop\n\nalso changes stack_llama_2 sft trainer to use correct device map for ddp\ntraining so that you can test this issue"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-09-08T09:50:06Z",
        "message": "[DPO] self.accelerator._prepare_deepspeed return tuples (#745)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-09-07T16:03:10Z",
        "message": "Seq2Seq model support for DPO (#586)\n\n* dpo_collator for seq2seq models\n\n* dpo trainer support\n\n* refactoring\n\n* update collator\n\n* computes decoder input ids if possible\n\n* decoder input ids for dpo trainer\n\n* added test for seq2seq\n\n* quality\n\n* fixed typo\n\n* fixed string padding for seq2seq\n\n* fixed minor issues in padding\n\n* fixed typo in dpo.py\n\n* add docstring\n\n* run all precommit\n\n* fixed gradient accumulation steps in test\n\n* reformatting\n\n* fixing dpo tests\n\n* update .mdx"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-09-07T16:02:20Z",
        "message": "fixed metrics typo (#743)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-09-01T09:27:54Z",
        "message": "Fix: RuntimeError: 'weight' must be 2-D issue (#687)\n\n* Update dpo_trainer.py\n\n* Fix: self.args.deepspeed > self.is_deepspeed_enabled\n\n* Update trl/trainer/dpo_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-08-29T10:57:10Z",
        "message": "[DPO] fix DPO ref_model=None (#703)\n\n* fix by @tannonk\n\n* Update trl/trainer/dpo_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* add import\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-08-18T08:02:16Z",
        "message": "Allow for ref_model=None in DPOTrainer (#640)\n\n* Update dpo_trainer.py\n\nMake ref_model optional.\n\n* add tests for ref_model=None\n\n* better handling for ref_model=None\n\n* Update dpo_trainer.py\n\nCorrect docstring\n\n* move instantiation of self.ref_model closer to model\n\n* use .disable_adapters instead of .get_base_model\n\n* handle ref_model=None in get_batch_samples\n\n* fix failing test in dpo_trainer due to disable_dropout_in_model\n\n* Update trl/trainer/dpo_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-08-14T12:40:45Z",
        "message": "Disable dropout in DPO Training (#639)\n\n* disable dropout in dpo\n\n* quick fix docs\n\n* precommiot\n\n* add disable_dropout_in_model to DPOTrainer\n\n* disable_dropout -> disable_dropout_in_model\n\n* .\n\n* ."
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-07-26T06:06:25Z",
        "message": "[`DPO`] Resolve logging for DPOTrainer (#570)\n\n* Resolve logging for DPOTrainer\n\n* Ensure the WandB logger correctly prefixes all logs\n\n* Run pre-commit\n\nWhoops, hadn't run `pre-commit install` yet"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-07-24T11:23:33Z",
        "message": "remove unused batch_size arg (#554)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-07-19T12:12:23Z",
        "message": "Relax reward trainer constraint (#539)\n\n* relax reward trainer constraint\n\n* Update trl/trainer/reward_trainer.py\n\nCo-authored-by: Tom Aarsen <37621491+tomaarsen@users.noreply.github.com>\n\n* relax also for DPO\n\n---------\n\nCo-authored-by: Tom Aarsen <37621491+tomaarsen@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-07-18T11:21:17Z",
        "message": "all the concated batches are on same device (#528)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/dpo_trainer.py",
        "commit_date": "2023-07-17T12:52:14Z",
        "message": "DPO Trainer (#416)\n\n* initial DPO Trainer\n\n* typo\n\n* initial dpo from reward trainer\n\n* calc. log_probs from logits\n\n* remove dpo config for now\n\n* fix inits\n\n* add intial DPODataCollatorWithPadding\n\n* use the RewardDataCollatorWithPadding\n\n* initial test\n\n* means of loss\n\n* add assert\n\n* just call the train instead of step\n\n* functional debug example before refactor\n\n* check the params have changed\n\n* initial DPODataCollatorWithPadding\n\n* Data collator with masking\n\n* going through trainer.accelerate to wrap ref_model\n\n* style / imports\n\n* style / imports\n\n* `broadcast_buffers=False` fix to distributed training\n\n* better fix for DDP issues\n\n* arguments and style clean-up\n\n* better doc, some light refactoring\n\n* better imports\n\n* initial dpo doc\n\n* fix test\n\n* fix formatting\n\n* fix\n\n* called models once\n\n* fix tests\n\n* add example\n\n* fix doc string\n\n* intitial example with anthropic hh dataset\n\n* refactored dpo trainer\n\n* revert\n\n* return metrics\n\n* fixed tests\n\n* updated docs\n\n* update test\n\n* fixed typo\n\n* note about the beta\n\n* added dpo authors\n\n* fix docstrings\n\n* add prediction_step\n\n* remove compute_metrics and log metrics manually\n\n* fix typo\n\n* add DPOTrainer doc\n\n* add dpo to toc\n\n* ValueError\n\n* add to index and example\n\n* fix docs\n\n* fix assert\n\n---------\n\nCo-authored-by: TevenLeScao <teven.lescao@gmail.com>\nCo-authored-by: Gaetan LOPEZ <gaetanloplat@gmail.com>\nCo-authored-by: younesbelkada <younesbelkada@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2024-02-27T10:19:06Z",
        "message": "add  `eval_packing` (#1369)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2024-02-19T13:43:17Z",
        "message": "Kto trainer (#1181)\n\n* initial file\n\n* initial tokenizer\n\n* UnpairedPreferenceBatchSampler\n\n* use batch_sampler\n\n* use interleave_datasets\n\n* add loss\n\n* fix imports\n\n* use SequentialSampler when training\n\n* formatting\n\n* add other helpers\n\n* add prediction_step\n\n* fix the kto pair docs\n\n* tests\n\n* compute_reference_log_probs\n\n* add get_eval_dataloader\n\n* fix typo\n\n* kto with is_encoder_decoder true\n\n* Update docs/source/dpo_trainer.mdx\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* fixed typo\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update docs/source/kto_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update docs/source/kto_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* renamed KTO dataset keys\n\n* use DPOTrainer's get_batch_logps\n\n* add get_batch_samples\n\n* typo\n\n* Handle last token in prompt\n\n* Create KTOConfig class that subclasses transformers.TrainingArguments\n\n* Update KTO tests to handle KTOConfig\n\n* Update KTO script to use KTOConfig\n\n* formatting\n\n* Update docs/source/dpo_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update docs/source/kto_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update docs/source/kto_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update trl/trainer/training_configs.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update examples/scripts/kto.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update examples/scripts/kto.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* use max_completion_length\n\n* Update examples/scripts/kto.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* add back get_batch_logps\n\n* use max_completion_length\n\n* move config to its own file\n\n* Check tokenize params on Trainer init\n\n* Clone labels for end-dec model to solve RuntimeError\n\n* formatting\n\n* fix enc-dec later\n\n* completion_decoder_input_ids is optional for enc-dec\n\n* fix breaking test\n\n* add a kl key for KL estimation with shuffled completion\n\n* add loss ad weights\n\n* fix bug in chosen_idx\n\n* add back metrics\n\n* fix typos\n\n* fix kto_loss docs\n\n* typo\n\n* set loss to None when there is no target completions in batch\n\n* use nan tensor instead of none\n\n* fix reference_logps test\n\n* fix logits\n\n* a bit more robust options\n\n* log only the correct prompt-completion during eval\n\n* Update trl/trainer/kto_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update examples/scripts/kto.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update examples/scripts/kto.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update docs/source/kto_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update docs/source/dpo_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* add docs for desirable_weight and undesirable_weight args\n\n* dropout is always disabled\n\n* remove DDP hack\n\n* formatting\n\n* move more arguments of trainer to config\n\n* comment out T5 test for now\n\n* Add docstring to KTOTrainer\n\n* moved Config docstrings to the appropriate class\n\n* add autodoc to markdown\n\n* formatting\n\n* updated copyright year\n\n* add model tags\n\n* do not add BOS to start of completion\n\n* Move data_collator to KTOTrainer\n\n* formatting\n\n* data_collator is not in args\n\n* shuffle_completion with specific input_columns\n\n* remove all but the needed columns\n\n* Update docs/source/dpo_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update examples/scripts/kto.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update tests/test_kto_trainer.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* moved more args to kto_config\n\n* fjx test\n\n* use all_exhausted strategy and shuffle after\n\n* use KTOConfig in HfArgumentParser\n\n* use ModelConfig\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\nCo-authored-by: Pablo Vicente Juan <p.vicente.juan@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2024-02-15T13:47:32Z",
        "message": "[`core` /  `xxxTrainer`] Automatic tagging (#1329)\n\n* automatic tagging\n\n* add comments\n\n* fix tests\n\n* fix"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2024-02-15T03:37:41Z",
        "message": "pre-commit: replace linters + formatters with Ruff; fix some issues (#1300)\n\n* pre-commit: replace linters + formatters with Ruff\n\n* Don't use bare except\n\n* Clean up `noqa`s\n\n* Enable Ruff UP; apply auto-fixes\n\n* Enable Ruff B; apply fixes\n\n* Enable Ruff T with exceptions\n\n* Enable Ruff C (complexity); autofix\n\n* Upgrade Ruff to 0.2.0"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2024-01-31T13:51:58Z",
        "message": "Types: Fix PEP 484 implicit-optional compliance (#1297)\n\nThis was done automatically with hauntsaninja/no_implicit_optional."
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2024-01-31T02:31:53Z",
        "message": "Fix sft trainer when args is None (#1295)\n\n* fix sft trainer when args is None\n\n* add test\n\n* fix"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2024-01-26T12:50:37Z",
        "message": "FIx SFTTrainer bugs on TRL main (#1276)\n\n* Update sft_trainer.py\n\n* Update trl/trainer/sft_trainer.py"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2024-01-26T09:38:07Z",
        "message": "Only load data on main process (#1255)\n\n* fix: only load data on main process\n\n* define is_main_process once\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* avoid re-initializing PartialState on train dataset check\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* avoid re-initializing PartialState on eval dataset check\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* process dataset on main first to take advantage of caching\n\n* fix typo in docs\n\n* use decorator to manage state\n\n* Revert \"fix typo in docs\"\n\nThis reverts commit 0880a188812a698f7106853245ce1ba96a036831.\n\n* Revert \"Revert \"fix typo in docs\"\"\n\nThis reverts commit ff7ee33fbeedcd0032b728d86a17cfcb10e43f9b.\n\n* Revert \"use decorator to manage state\"\n\nThis reverts commit 7ac7a45949f621941fedc522f0d2ca7b29367c3a.\n\n* use is_local_main_process instead of is_main_process\n\n* fix: use context manager instead of attribute\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/sft_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2024-01-23T13:46:13Z",
        "message": "Fix typo in extra_columns variable name (#1269)\n\nCo-authored-by: Otto Laitila <otto.laitila@op.fi>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2024-01-17T14:16:07Z",
        "message": "Update sft_trainer.py (#1241)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2024-01-17T13:45:22Z",
        "message": "[`core` / SFTTrainer] Fix breaking change (#1229)\n\n* fix breaking change\n\n* revert\n\n* fix\n\n* final fix\n\n* fix\n\n* fix tests"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2024-01-12T07:05:32Z",
        "message": "Add support for ChatML dataset format in (#1208)\n\n* Add support for ChatML dataset format in\nSFTTrainer\n\n* fix formatting\n\n* fix tests\n\n* more comment\n\n* fix intent\n\n* fix doc string\n\n* Update dataset_formatting.py\n\n* Update dataset_formatting.py\n\n* add documentation\n\n* Update sft_trainer.mdx\n\n* add leonardos comment and more tests\n\n* added more tests and fixed batching\n\n* style\n\n* comment in"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2024-01-09T09:20:50Z",
        "message": "Revert \"Address issue #1122 (#1174)\" (#1205)\n\nThis reverts commit d57d0f9ca46a63d370b91791352edda0154576f5."
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2024-01-08T05:09:10Z",
        "message": "SFTTrainer: follow args.remove_unused_columns (#1188)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2024-01-08T04:43:34Z",
        "message": "Address issue #1122 (#1174)\n\n* Address issue #1122\n\n    Issue [#1122](https://github.com/huggingface/trl/issues/1122)\n    takes care of an inconsistency between `_prepare_packed_dataloader`\n    and `_prepare_non_packed_dataloader`\n\n* made attention_mask field in ConstantLengthDataset a tensor"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2024-01-04T15:33:53Z",
        "message": "Update sft_trainer.py (#1162)\n\nFix spelling mistakes in argument description for trl -> SFT Trainer"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-12-26T15:39:10Z",
        "message": "[`xxxTrainer`] Add unsloth tag (#1130)\n\n* add unsloth tag\n\n* add it on all trainers\n\n* few changes\n\n* add in docs\n\n* revert\n\n* final commit"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-12-22T13:52:16Z",
        "message": "[`xxxTrainer`] multi-tags support for tagging (#1133)\n\n* multi-tags support for tagging\n\n* oops"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-12-21T16:04:18Z",
        "message": "[`xxxTrainer`] Add tags to all trainers in TRL (#1120)\n\n* add tags to sfttrainer\n\n* extend it to other trainers\n\n* add for ddpo"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-12-20T12:35:56Z",
        "message": "fix gradient checkpointing when using PEFT (#1118)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-12-20T10:28:50Z",
        "message": "Make prepending of bos token configurable. (#1114)\n\n* make prepending of bos token configurable.\n\n* address comments\n\n* fix bug\n\nCo-Authored-By: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/sft_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-12-19T16:43:25Z",
        "message": "`peft_module_casting_to_bf16` util method, `append_concat_token` flag, remove callback `PeftSavingCallback` (#1110)\n\n* SFT Trainer enhancements\n\n* remove the callback `PeftSavingCallback`\n\n* bump the version of transformers to `4.31.0`\n\n* remove `PeftSavingCallback` from all places."
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-12-06T19:26:24Z",
        "message": "enable multiple eval datasets (#1052)\n\n* enable multiple eval datasets\n\n* added test\n\n* try to avoid infinite computation\n\n* make sure eval set is not infinite\n\n* downsizing the test"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-12-06T18:02:09Z",
        "message": "[`SFTTrainer`] Fix Trainer when args is None (#1064)\n\n* fix sfttrainer when args is None\n\n* oops"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-12-06T16:46:36Z",
        "message": "update doc for the computer_metrics argument of SFTTrainer (#1062)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-12-04T12:13:18Z",
        "message": "[SFT Trainer] precompute packed iterable into a dataset (#979)\n\n* precompute packed iterable into a dataset\n\n* add generator function\n\n* fix typo\n\n* fix style\n\n* fix test\n\n* fix style\n\n* add test\n\n* minor refactor\n\n* fix test\n\n* Apply suggestions from code review\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* style\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\nCo-authored-by: younesbelkada <younesbelkada@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-11-20T14:57:46Z",
        "message": "Adds `requires_grad` to input for non-quantized peft models (#1006)\n\n* Update sft_trainer.py\n\n* style\n\n* add tests"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-11-08T10:11:39Z",
        "message": "fix peft config typehint (#967)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-11-06T08:48:18Z",
        "message": "Adds model kwargs to SFT and DPO trainers (#951)\n\n* adds model kwargs to SFT and DPO trainers\n\n* adds checks for model_kwarg passing when model is not str\n\n* changed warning to ValueError\n\n* renames model_kwargs to model_init_kwargs\n\n* corrects argument names in"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-11-05T07:31:47Z",
        "message": "Fix unwrapping peft models (#948)\n\n* First unwrap the model and then process the input embeddings\n\n* Changed base_model to base_model.model to stay consistent with peft model abstractions"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-10-31T17:50:17Z",
        "message": "[`core` / `DDP`] Fix RM trainer + DDP + quantization + propagate `gradient_checkpointing_kwargs` in SFT & DPO (#912)\n\n* make use of forward hooks\n\n* correctly delete attributes\n\n* fix RM DPP issues\n\n* revert unneeded changes\n\n* more fixes\n\n* fix diff\n\n* fix\n\n* propagate to SFT\n\n* Update examples/scripts/reward_modeling.py\n\n* propagate the fix on DPO trainer\n\n* add to example scripts\n\n* trigger CI"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-10-31T15:04:09Z",
        "message": "[`SFTTrainer`] Make sure to not conflict between `transformers` and TRL implementation (#933)\n\n* standardize neftune\n\n* up\n\n* fix again"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-10-30T10:17:14Z",
        "message": "Generalize NEFTune for FSDP, DDP, ... (#924)\n\n* Update sft_trainer.py\n\n* quality"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-10-24T12:18:44Z",
        "message": "[`NEFTune`] Make use of forward hooks instead (#889)\n\n* make use of forward hooks\n\n* correctly delete attributes\n\n* address suggestions"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-10-18T21:45:38Z",
        "message": "fix peft_config type (#883)\n\nCo-authored-by: wanglei.w <wanglei.w@bytedance.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-10-17T04:58:05Z",
        "message": "[`SFTTrainer`] Adds NEFTune into `SFTTrainer` (#871)\n\n* v1 neftune\n\n* docstring\n\n* add doc + fix nit\n\n* add more docs\n\n* Apply suggestions from code review\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n---------\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-09-12T08:56:10Z",
        "message": "update to `prepare_model_for_kbit_training` (#728)\n\n* update to `prepare_model_for_kbit_training`\n\nfrom deprecated `prepare_model_for_int8_training`\nand add `use_gradient_checkpointing=args.gradient_checkpointing` to\nautomatically follow the gradient checkpointing choice\n\nis also the workaround for #694\n\n* workaround for gradient checkpointing issue\n\ncalling model.gradient_checkpointing_enable() twice causes issues\nthis workaround calls it in prepare_model_for_kbit_training and then\nchanges the arg to false to make sure it isn't called again in\nhuggingface trainer inner loop\n\nalso changes stack_llama_2 sft trainer to use correct device map for ddp\ntraining so that you can test this issue"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-09-06T08:24:55Z",
        "message": "check correctly for condition (#668)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-08-09T15:48:25Z",
        "message": "Move repo (#628)\n\n* update actions\n\n* update references"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-07-22T08:53:16Z",
        "message": "[`SFTTrainer`] Add warning for wrong padding_side (#550)\n\n* add warning for wrong padding_side\n\n* add warning\n\n* revert\n\n* oops"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-07-20T13:41:48Z",
        "message": "ADD: num_proc to SFTTrainer (#547)\n\n* ADD: num_proc to SFTTrainer\n\n* make precommit\n\n* Update trl/trainer/sft_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/sft_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/sft_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update trl/trainer/sft_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* add batch_size\n\n* Update trl/trainer/sft_trainer.py\n\nCo-authored-by: Tom Aarsen <37621491+tomaarsen@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\nCo-authored-by: Tom Aarsen <37621491+tomaarsen@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-07-12T13:25:17Z",
        "message": "[`SFTTrainer`] Fix the sequence length check of `SFTTrainer` (#512)\n\n* fix the sequence length check of `SFTTrainer`\n\n* forward contrib credits from initial contribution\n\n* forward contrib credits from initial contribution\n\n* final comments\n\n---------\n\nCo-authored-by: mrm8488 <mrm8488@users.noreply.github.com>\nCo-authored-by: BramVanroy <BramVanroy@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-06-27T22:44:15Z",
        "message": "Update sft_trainer.py (#474)\n\n* Update sft_trainer.py\n\nAllows the user to give their own peft model arg. https://github.com/lvwerra/trl/issues/473\n\n* cleaner"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-06-24T06:43:03Z",
        "message": "Debug the tortuous logic in `_prepare_dataset` function (#464)\n\n* Debug the tortuous logic in `_prepare_dataset` function\n\nThere are two issues with the previous `_prepare_dataset` function.\n\n1. Tortuous and burdensome logic: the `is_already_dataset` variable is confusing and not helpful. So, remove it.\n2. The comments and the logics do not match. \n\nFor instance, in the previous version, the comments said \"check if torch dataset ... and do nothing\". However, when \"dataset\" is a torch.utils.data.Dataset and `packing = True`? It will still move into the _prepare_non_packed_dataloader(...) function call. \n\nThe corrected version will do nothing if the dataset is already a torch dataloader/dataset/ConstantLengthDataset.\n\n* Lint: sft_trainer.py\n\n* Lint empty line"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-06-22T09:19:45Z",
        "message": "Multi adapter RL (MARL) - a single model for RM & Value Head (#373)\n\n* fix doc\n\n* adapt from suggestions\n\n* working v1 multiple adapters\n\n* style\n\n* style && quality\n\n* oops\n\n* docs\n\n* add tests and docs\n\n* add RM script\n\n* Apply suggestions from code review\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update docs/source/0_abstraction_rl.mdx\n\n* Apply suggestions from code review\n\n* Update docs/source/0_abstraction_rl.mdx\n\n* add 4bit\n\n* replace with `reward_adapter`\n\n* explain break\n\n* simple comment\n\n* fix llama tokenizer\n\n* fixes\n\n* fixes\n\n* rename\n\n* quality\n\n* rm unneeded file\n\n* add disclaimer\n\n---------\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-06-22T08:12:55Z",
        "message": "[`ConstantLengthDataset`] Fix packed dataset issue (#452)\n\n* fix packed dataset issue\n\n* Apply suggestions from code review\n\nCo-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>\n\n* address\n\n* more docs\n\n* trigger CI\n\n* fix failing CI\n\n---------\n\nCo-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-06-20T15:51:23Z",
        "message": "[`SFTTrainer`] Introducing `DataCollatorForCompletionOnlyLM` (#445)\n\n* v1 of alpaca datacollator\n\n* make sure to match the response tokens\n\n* add test\n\n* add it in main init\n\n* add check\n\n* adapt test\n\n---------\n\nCo-authored-by: Costa Huang <costa.huang@outlook.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-06-16T16:51:20Z",
        "message": "[`SFTTrainer`] Fix non packed dataset (#444)\n\n* fix non packed dataset\n\n* fixing tests and documentation\n\n* Update docs/source/sft_trainer.mdx"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-06-16T11:55:47Z",
        "message": "fix packing issue (#442)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-06-02T08:49:41Z",
        "message": "fix 4 bit SFT (#396)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-05-15T14:26:27Z",
        "message": "[`docs`] fix SFT doc (#367)\n\n* fix doc\n\n* adapt from suggestions"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-05-03T10:53:32Z",
        "message": "fix sft issues (#336)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "trl/trainer/sft_trainer.py",
        "commit_date": "2023-05-03T08:42:01Z",
        "message": "[`core`]\u00a0officially support SFT (Supervised Finetuning)  (#323)\n\n* add v1\n\n* revert\n\n* correct filename\n\n* add tests and final tweaks\n\n* fix tests\n\n* adapt from offline suggestions\n\n* Update trl/trainer/sft_trainer.py\n\nCo-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>\n\n* fixes\n\n* remove warning\n\n* multiple fixes\n\n* fixes\n\n* fix\n\n* final fixes\n\n* final fix\n\n* more clarification\n\n* Apply suggestions from code review\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* add test\n\n* add arg\n\n* add callback instructions\n\n* add formatting_prompts_func\n\n* try docs\n\n* add CLD\n\n* fix docstrings\n\n* format\n\n* Update docs/source/sft_trainer.mdx\n\nCo-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>\n\n* remove `prepare_in_int8_kwargs`\n\n* change `return_overflowing_tokens`\n\n* add warnings\n\n* address comments\n\n* revert pretrained kwargs\n\n* quality\n\n* fix sft script\n\n---------\n\nCo-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "utils.py",
        "commit_date": "2023-06-01T09:05:42Z",
        "message": "multi GPUs inference"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "utils.py",
        "commit_date": "2023-05-10T08:32:19Z",
        "message": "mv shell script and update utils.py"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "utils.py",
        "commit_date": "2023-04-27T03:42:18Z",
        "message": "Update utils.py"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "utils.py",
        "commit_date": "2023-04-04T11:42:21Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/intel-analytics/BigDL",
        "filepath": "python/llm/src/bigdl/llm/transformers/qlora.py",
        "commit_date": "2024-01-17T07:51:38Z",
        "message": "Support fast rope for training (#9745)\n\n* init\n\n* init\n\n* fix style\n\n* add test and fix\n\n* address comment\n\n* update\n\n* merge upstream main"
    },
    {
        "repo_url": "github.com/intel-analytics/BigDL",
        "filepath": "python/llm/src/bigdl/llm/transformers/qlora.py",
        "commit_date": "2023-12-25T06:49:30Z",
        "message": "LLM: fix `BF16Linear` related training & inference issue (#9755)\n\n* fix bf16 related issue\n\n* fix\n\n* update based on comment & add arc lora script\n\n* update readme\n\n* update based on comment\n\n* update based on comment\n\n* update\n\n* force to bf16\n\n* fix style\n\n* move check input dtype into function\n\n* update convert\n\n* meet code review\n\n* meet code review\n\n* update merged model to support new training_mode api\n\n* fix typo"
    },
    {
        "repo_url": "github.com/intel-analytics/BigDL",
        "filepath": "python/llm/src/bigdl/llm/transformers/qlora.py",
        "commit_date": "2023-12-22T03:05:39Z",
        "message": "LLM: bigdl-llm lora support & lora example (#9740)\n\n* lora support and single card example\n\n* support multi-card, refactor code\n\n* fix model id and style\n\n* remove torch patch, add two new class for bf16, update example\n\n* fix style\n\n* change to training_mode\n\n* small fix\n\n* add more info in help\n\n* fixstyle, update readme\n\n* fix ut\n\n* fix ut\n\n* Handling compatibility issues with default LoraConfig"
    },
    {
        "repo_url": "github.com/intel-analytics/BigDL",
        "filepath": "python/llm/src/bigdl/llm/transformers/qlora.py",
        "commit_date": "2023-12-08T08:13:03Z",
        "message": "Support peft LoraConfig (#9636)\n\n* support peft loraconfig\n\n* use testcase to test\n\n* fix style\n\n* meet comments"
    },
    {
        "repo_url": "github.com/intel-analytics/BigDL",
        "filepath": "python/llm/src/bigdl/llm/transformers/qlora.py",
        "commit_date": "2023-12-07T08:32:02Z",
        "message": "LLM: fix unlora module in qlora finetune (#9621)\n\n* fix unlora module\n\n* split train and inference"
    },
    {
        "repo_url": "github.com/intel-analytics/BigDL",
        "filepath": "python/llm/src/bigdl/llm/transformers/qlora.py",
        "commit_date": "2023-12-06T07:36:21Z",
        "message": "QALora example (#9551)\n\n* Support qa-lora\n\n* init\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update merge\n\n* update\n\n* fix style & update scripts\n\n* update\n\n* address comments\n\n* fix typo\n\n* fix typo\n\n---------\n\nCo-authored-by: Yang Wang <yang3.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel-analytics/BigDL",
        "filepath": "python/llm/src/bigdl/llm/transformers/qlora.py",
        "commit_date": "2023-11-29T07:16:18Z",
        "message": "LLM: fix loss error on Arc (#9550)"
    },
    {
        "repo_url": "github.com/intel-analytics/BigDL",
        "filepath": "python/llm/src/bigdl/llm/transformers/qlora.py",
        "commit_date": "2023-11-21T09:08:36Z",
        "message": "LLM: GPU QLoRA update to bf16 to accelerate gradient checkpointing (#9499)\n\n* update to bf16 to accelerate gradient checkpoint\n\n* add utils and fix ut"
    },
    {
        "repo_url": "github.com/intel-analytics/BigDL",
        "filepath": "python/llm/src/bigdl/llm/transformers/qlora.py",
        "commit_date": "2023-11-08T09:46:49Z",
        "message": "LLM: optimize QLoRA by updating lora convert logic (#9372)\n\n* update convert logic of qlora\n\n* update\n\n* refactor and further improve performance\n\n* fix style\n\n* meet code review"
    },
    {
        "repo_url": "github.com/intel-analytics/BigDL",
        "filepath": "python/llm/src/bigdl/llm/transformers/qlora.py",
        "commit_date": "2023-10-17T03:47:19Z",
        "message": "Support XPU DDP training and autocast for LowBitMatmul (#9167)\n\n* support autocast in low bit matmul\n\n* Support XPU DDP training\n\n* fix  amp"
    },
    {
        "repo_url": "github.com/intel-analytics/BigDL",
        "filepath": "python/llm/src/bigdl/llm/transformers/qlora.py",
        "commit_date": "2023-10-15T04:28:59Z",
        "message": "Fixes for xpu Bf16 training (#9156)\n\n* Support bf16 training\n\n* Use a stable transformer version\n\n* remove env\n\n* fix style"
    },
    {
        "repo_url": "github.com/intel-analytics/BigDL",
        "filepath": "python/llm/src/bigdl/llm/transformers/qlora.py",
        "commit_date": "2023-10-05T04:18:52Z",
        "message": "add export merged model example (#9018)\n\n* add export merged model example\n\n* add sources\n\n* add script\n\n* fix style"
    },
    {
        "repo_url": "github.com/intel-analytics/BigDL",
        "filepath": "python/llm/src/bigdl/llm/transformers/qlora.py",
        "commit_date": "2023-09-19T17:15:44Z",
        "message": "Experiment XPU QLora Finetuning (#8937)\n\n* Support xpu finetuning\n\n* support xpu finetuning\n\n* fix style\n\n* fix style\n\n* fix style\n\n* refine example\n\n* add readme\n\n* refine readme\n\n* refine api\n\n* fix fp16\n\n* fix example\n\n* refactor\n\n* fix style\n\n* fix compute type\n\n* add qlora\n\n* refine training args\n\n* fix example\n\n* fix style\n\n* fast path forinference\n\n* address comments\n\n* refine readme\n\n* revert lint"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2024-03-03T12:35:22Z",
        "message": "Vllm update DP+TP (#1508)\n\n* use `@ray.remote` with distributed vLLM\n\n* update versions\n\n* bugfix\n\n* unpin vllm\n\n* fix pre-commit\n\n* added version assertion error\n\n* Revert \"added version assertion error\"\n\nThis reverts commit 8041e9b78e95eea9f4f4d0dc260115ba8698e9cc.\n\n* added version assertion for DP\n\n* expand DP note\n\n* add warning\n\n* nit\n\n* pin vllm\n\n* fix typos"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2024-03-01T16:04:44Z",
        "message": "always include EOS token in stopsequences if possible (#1480)"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2024-02-27T23:31:10Z",
        "message": "Fix AttributeError in huggingface.py When 'model_type' is Missing (#1489)\n\n* model_type attribute error\n\nGetting attribute error when using a model without a 'model_type'\n\n* fix w/ and w/out the 'model_type' specification\n\n* use getattr(), also fix other config.model_type reference\n\n* Update huggingface.py\n\n---------\n\nCo-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2024-02-27T14:03:56Z",
        "message": "Refactor `evaluater.evaluate` (#1441)\n\n* change `all_gather` to `gather`\n\n* add TaskOutput utility class\n\n* Add FilterResults class and refactor task handling.\n\n* Rename `key` to `filter_key` for clarity\n\n* Add `print_writeout` function in utils.py\n\n* Add function to calculate limit size.\n\n* Add doc_iterator method to Task class\n\n* Refactor `doc_iterator` and cleanup in Task class\n\n* remove superfluous bits\n\n* change `all_gather` to `gather`\n\n* bugfix\n\n* bugfix\n\n* fix `gather`\n\n* Refactor `gather` loop\n\n* Refactor aggregate metrics calculation\n\n* Refactor and simplify aggregate metrics calculation\nRemoved unused code\n\n* Simplify metrics calculation and remove unused code.\n\n* simplify the metrics calculation in `utils.py` and `evaluator.py`.\n\n* Fix group metric\n\n* change evaluate to hf_evaluate\n\n* change evaluate to hf_evaluate\n\n* add docs\n\n* add docs\n\n* nits\n\n* make isslice keyword only\n\n* nit\n\n* add todo\n\n* nit\n\n* nit\n\n* nit: swap order samples_metrics tuple\n\n* move instance sorting outside loop\n\n* nit\n\n* nit\n\n* Add __repr__ for ConfigurableTask\n\n* nit\n\n* nit\n\n* Revert \"nit\"\n\nThis reverts commit dab8d9977a643752a17f840fd8cf7e4b107df28f.\n\n* fix some logging\n\n* nit\n\n* fix `predict_only` bug. thanks to `@LSinev`!\n\n* change `print_tasks` to `prepare_print_tasks`\n\n* nits\n\n* move eval utils\n\n* move eval utils\n\n* nit\n\n* add comment\n\n* added tqdm descriptions\n\n* Update lm_eval/evaluator_utils.py\n\nCo-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>\n\n* fix mgsm bug\n\n* nit\n\n* fix `build_all_requests`\n\n* pre-commit\n\n* add ceil to limit\n\n---------\n\nCo-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2024-02-26T14:21:35Z",
        "message": "Revert \"setting trust_remote_code (#1467)\" (#1474)\n\nThis reverts commit c1145dfdd8f9ddfceec0410d328ac1c590738a5d."
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2024-02-26T14:02:21Z",
        "message": "Add Gemma support (Add flag to control BOS token usage) (#1465)\n\n* add add_bos_token to HFLM\n\n* add BOS token flag to other local model classes\n\n---------\n\nCo-authored-by: Lintang Sutawika <lintang@eleuther.ai>"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2024-02-26T13:05:05Z",
        "message": "setting trust_remote_code (#1467)"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2024-02-22T00:39:42Z",
        "message": "Add TemplateLM boilerplate LM class (#1279)\n\n* loglikelihood refactor using template lm\n\n* linter\n\n* fix whitespace in target + prompt for CoT gsm8k (#1275)\n\n* Make `parallelize=True` vs. `accelerate launch` distinction clearer in docs (#1261)\n\n* Make parallelize=True distinction clearer in documentation.\n\n* run linter\n\n* Allow parameter edits for registered tasks when listed in a benchmark (#1273)\n\n* benchmark yamls allow minor edits of already registered tasks\n\n* add documentation\n\n* removed print\n\n* Fix data-parallel evaluation with quantized models (#1270)\n\n* add WIP device_map overrides\n\n* update handling outside of accelerate launcher\n\n* change .to(device) log to debug level\n\n* run linter\n\n* Rework documentation for explaining local dataset (#1284)\n\n* rewor documentation for explaining local dataset\n\n* fix typo\n\n* Update new_task_guide.md\n\n* Re-add citation\n\nIt looks like Google Scholar has [already noticed](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C9&authuser=2&q=%22A+framework+for+few-shot+language+model+evaluation%2C+12+2023%22&btnG=) the updated citation block so let's add it back in.\n\n* Update CITATION.bib (#1285)\n\nBumping CITATION.bib to match re-adding the citation in readme. \n\ncc @StellaAthena\n\n* Update nq_open.yaml (#1289)\n\n* Update README.md with custom integration doc (#1298)\n\n* Update README.md\n\n* punctuation\n\n---------\n\nCo-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>\n\n* Update nq_open.yaml (#1305)\n\n* Update nq_open.yaml\n\nchange regex\n\n* Bump NQ version\n\n---------\n\nCo-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>\n\n* Update task_guide.md (#1306)\n\n* Update pyproject.toml (#1312)\n\n* Fix polemo2_in.yaml config name (#1313)\n\n* Update pyproject.toml (#1314)\n\n* Fix group register (#1315)\n\n* tuple should be considered as well\n\n* set option to keep callable as callable\n\n* Update task_guide.md (#1316)\n\n* Update polemo2_in.yaml (#1318)\n\n* don't pass extra kwargs to mamba any more (#1328)\n\n* Fix Issue regarding stderr (#1327)\n\n* add fix fordeciding if stderr is N/A or not\n\n* process N/A\n\n* Add `local-completions` support using OpenAI interface (#1277)\n\n* Add `local-completions` support using OpenAI interface\n\n* Refactor oa_completion\n\n* Address tokenizer comments and change request chunks to batch size\n\n* Add warning message for tiktoken backend\n\n* fix formatting\n\n* fix whitespace\n\n* Update README.md\n\n---------\n\nCo-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>\n\n* fallback to classname when LM doesnt have config (#1334)\n\n* fix a trailing whitespace that breaks a lint job (#1335)\n\n* skip \"benchmarks\" in changed_tasks (#1336)\n\n* Update migrated HF dataset paths (#1332)\n\n* Update arc_easy.yaml\n\n* Update flan_cot.yaml\n\n* update HF dataset path\n\n* Update freeform.yaml\n\n* Update flan_cot.yaml\n\n---------\n\nCo-authored-by: Lintang Sutawika <lintang@eleuther.ai>\n\n* Don't use `get_task_dict()` in task registration / initialization (#1331)\n\n* don't use get_task_dict() as a helper, it will download the dataset!\n\n* pre-commit\n\n* Update README.md\n\n---------\n\nCo-authored-by: lintangsutawika <lintang@eleuther.ai>\n\n* manage default (greedy) gen_kwargs in vllm (#1341)\n\n* manage default (greedy) gen_kwargs in vllm better\n\n* mirror HF `do_sample`\n\n* just need to set temp=0 for greedy\n\n* modified default gen_kwargs to work better with CLI; changed prompt_logprobs=1 (#1345)\n\n* update links to task_guide.md (#1348)\n\n* `Filter` docs not offset by `doc_id`  (#1349)\n\n* get `doc` from instance\n\n* acceletate bugfix: get ground doc from instance\n\n* convert filter to `process_result`\n\n* get docs from instances in `FilterEnsemble`\n\n* rename\n\n* nit\n\n* better looping\n\n* fix typehint\n\n* Add FAQ on `lm_eval.tasks.initialize_tasks()` to README (#1330)\n\n* Update README.md\n\n* [!Tip]\n\n* Refix issue regarding stderr (#1357)\n\n* Add causalLM OpenVino models (#1290)\n\n* added intel optimum\n\n* added intel optimum in readme\n\n* modified intel optimum\n\n* modified intel optimum\n\n* modified intel optimum\n\n* modified install optimum\n\n* modified path of IR file\n\n* added openvino_device\n\n* added openvino_device2\n\n* changed optimum-causal to openvino-causal\n\n* Update README.md\n\n* Update README.md\n\n* remove `lm_eval.base` import\n\n* update openvino-causal -> openvino ; pass device through super().__init__()\n\n* Update README.md\n\n* Add optimum to tests dependencies\n\n* apply pre-commit\n\n* fix so tests pass\n\n---------\n\nCo-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>\nCo-authored-by: haileyschoelkopf <hailey@eleuther.ai>\n\n* Apply some best practices and guideline recommendations to code (#1363)\n\n* raise Exception, not a string\n\nAdditional info https://peps.python.org/pep-0352/#exception-hierarchy-changes\nhttps://docs.python.org/3.8/tutorial/errors.html#raising-exceptions\n\n* Apply PEP8 recommendation to prefer isinstance\n\n\"Object type comparisons should always use isinstance() instead of comparing types directly\"\nhttps://peps.python.org/pep-0008/\n\n* Remove dangerous default mutable values in arguments\n\nhttps://pylint.readthedocs.io/en/stable/user_guide/messages/warning/dangerous-default-value.html\n\n* Format logging messages with fstring (not with format)\n\nAdditional info\nhttps://pylint.readthedocs.io/en/stable/user_guide/messages/warning/logging-format-interpolation.html\nThere are also discussions about the speed of formatting while logging or some unintended code executions\nhttps://github.com/pylint-dev/pylint/issues/2395\nhttps://stackoverflow.com/a/54368109\nbut at least one format (fstring one) will be used throughout the project\n\n* Specify utf-8 encoding for `open` explicitly\n\nIf not specified, it may be supposed differently in different environments, OSes, and Python versions. See\nhttps://peps.python.org/pep-0597/\nhttps://docs.python.org/3.11/library/locale.html#locale.getencoding\nhttps://docs.python.org/3.10/library/os.html#utf8-mode\nhttps://pylint.readthedocs.io/en/stable/user_guide/messages/warning/unspecified-encoding.html\n\nHelps also if some code from English language tasks is taken as inspiration for tasks in non-English languages.\n\n* Use inline-ignoring comments to pass pre-commit instead of identity process\n\nhttps://flake8.pycqa.org/en/3.0.1/user/ignoring-errors.html#in-line-ignoring-errors\nhttps://www.flake8rules.com/rules/F841.html\n\nflake8 comments are supported by ruff: https://docs.astral.sh/ruff/linter/#error-suppression\n\n* serialize callable functions in config (#1367)\n\n* delay filter init; remove `*args` (#1369)\n\n* delay filter init; remove `*args`\n\n* bugfix\n\n* optimize\n\n* type hint\n\n* Fix unintuitive `--gen_kwargs` behavior (#1329)\n\n* don't override do_sample if no value for it is passed\n\n* Update gen_kwargs override condition\n\n* Update huggingface.py\n\n* Update huggingface.py\n\n* run linters\n\n* silence an erroneous warning\n\n* Publish to pypi (#1194)\n\n* publish to pypi\n\n* lint\n\n* Update publish.yml\n\n* minor\n\n* Make dependencies compatible with PyPI (#1378)\n\n* make deps not point to github urls\n\n* formatting\n\n* try making PyPI only run on tag pushes\n\n* Add support for RWKV models with World tokenizer (#1374)\n\n* Add support for RWKV models with World tokenizer\n\nThe RWKV line of model with the World tokenizer, does not allow the padding token to be configured, and has its value preset as 0\n\nThis however fails all the \"if set\" checks, and would cause the tokenizer to crash.\n\nA tokenizer class name check was added, in addition to a model type check, as there exists RWKV models which uses the neox tokenizers\n\n* Update huggingface.py\n\nGenericized so that this supports any RWKVWorld tokenizer, and added a fall-back for if the HF implementation name changes.\n\n* Comply with formatting guidelines\n\n* fix format\n\n---------\n\nCo-authored-by: Stella Biderman <stellabiderman@gmail.com>\nCo-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>\n\n* add bypass metric (#1156)\n\n* add bypass metric\n\n* fixed `bypass` metric.\n\n* add task attributes if predict_only\n\n* add `predict_only` checks\n\n* add docs\n\n* added `overide_metric`, `override_config` to `Task`\n\n* nits\n\n* nit\n\n* changed --predict_only to generations; nits\n\n* nits\n\n* nits\n\n* change gen_kwargs warning\n\n* add note about `--predict_only` in README.md\n\n* added `predict_only`\n\n* move table to bottom\n\n* nit\n\n* change null aggregation to bypass (conflict)\n\n* bugfix; default `temp=0.0`\n\n* typo\n\n* loglikelihood refactor using template lm\n\n* lint\n\n* code review\n\n* neuron optimum\n\n* Mention TemplateLM in model_guide.md\n\n* Update lm_eval/api/model.py\n\n* fix linter\n\n* fix format\n\n* fix format\n\n* fix format\n\n---------\n\nCo-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>\nCo-authored-by: Lintang Sutawika <lintang@eleuther.ai>\nCo-authored-by: Stella Biderman <stellabiderman@gmail.com>\nCo-authored-by: Mark Saroufim <marksaroufim@meta.com>\nCo-authored-by: Hannibal046 <38466901+Hannibal046@users.noreply.github.com>\nCo-authored-by: Danielle Pintz <38207072+daniellepintz@users.noreply.github.com>\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\nCo-authored-by: kwrobel.eth <djstrong@gmail.com>\nCo-authored-by: Michael Goin <michael@neuralmagic.com>\nCo-authored-by: Brian Vaughan <nairbv@users.noreply.github.com>\nCo-authored-by: Baber Abbasi <92168766+baberabb@users.noreply.github.com>\nCo-authored-by: thnkinbtfly <70014488+thnkinbtfly@users.noreply.github.com>\nCo-authored-by: NoushNabi <33136068+NoushNabi@users.noreply.github.com>\nCo-authored-by: haileyschoelkopf <hailey@eleuther.ai>\nCo-authored-by: LSinev <LSinev@users.noreply.github.com>\nCo-authored-by: Eugene Cheah <PicoCreator@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2024-02-20T14:57:01Z",
        "message": "Group reqs by context (#1425)\n\n* add key lookup for same contexts\n\n* nit\n\n* appease pre-commit\n\n* nit\n\n* use `expand` (in-place view) rather than `repeat`\n\n* try mixed grouping\n\n* add docs.\n\n* nit\n\n* nit\n\n* nits\n\n* fix tests\n\n* Move greedy_tokens calculation out of cache loop\n\n* nit\n\n* nits\n\n* add test\n\n* nits\n\n* fix name conflict\n\n* fix name conflict\n\n* chunk tensor\n\n* move Collator\n\n* nits/docstring\n\n* fixup\n\n* fixup\n\n* group contexts only for decoders\n\n* pre-commit\n\n* fix `generate_until` test\n\n* fix `generate_until` test\n\n* Update lm_eval/models/huggingface.py\n\nCo-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>\n\n* add docs\n\n* nit\n\n* add docs\n\n* add docs\n\n* add 'logits_cache' arg\n\n* bugfix\n\n---------\n\nCo-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2024-02-14T18:21:03Z",
        "message": "Refactor utilities into a separate model utils file. (#1429)"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2024-02-10T20:54:36Z",
        "message": "Fix watchdog timeout (#1404)\n\n* Fix watchdog timeout\n\n* Pre-commit fix\n\n* Timedelta"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2024-02-10T15:03:50Z",
        "message": "Fixes https://github.com/EleutherAI/lm-evaluation-harness/issues/1416 (#1418)\n\n* Fixes https://github.com/EleutherAI/lm-evaluation-harness/issues/1416\n\nSets `do_sample = False` if `temperature == 0.0` and `do_sample = None`\n\n* Update huggingface.py\n\n* Update huggingface.py\n\nmaking linter happy"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2024-02-07T13:48:47Z",
        "message": "`batch_size` with `auto` defaults to 1 if `No executable batch size found` (#1405)\n\nFixes https://github.com/EleutherAI/lm-evaluation-harness/issues/1323"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2024-02-01T09:13:29Z",
        "message": "Hf: minor egde cases (#1380)\n\n* edge cases where variable might not be assigned.\n\n* type hint"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2024-01-31T22:33:14Z",
        "message": "add bypass metric (#1156)\n\n* add bypass metric\n\n* fixed `bypass` metric.\n\n* add task attributes if predict_only\n\n* add `predict_only` checks\n\n* add docs\n\n* added `overide_metric`, `override_config` to `Task`\n\n* nits\n\n* nit\n\n* changed --predict_only to generations; nits\n\n* nits\n\n* nits\n\n* change gen_kwargs warning\n\n* add note about `--predict_only` in README.md\n\n* added `predict_only`\n\n* move table to bottom\n\n* nit\n\n* change null aggregation to bypass (conflict)\n\n* bugfix; default `temp=0.0`\n\n* typo"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2024-01-31T20:59:31Z",
        "message": "Add support for RWKV models with World tokenizer (#1374)\n\n* Add support for RWKV models with World tokenizer\n\nThe RWKV line of model with the World tokenizer, does not allow the padding token to be configured, and has its value preset as 0\n\nThis however fails all the \"if set\" checks, and would cause the tokenizer to crash.\n\nA tokenizer class name check was added, in addition to a model type check, as there exists RWKV models which uses the neox tokenizers\n\n* Update huggingface.py\n\nGenericized so that this supports any RWKVWorld tokenizer, and added a fall-back for if the HF implementation name changes.\n\n* Comply with formatting guidelines\n\n* fix format\n\n---------\n\nCo-authored-by: Stella Biderman <stellabiderman@gmail.com>\nCo-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2024-01-31T14:19:59Z",
        "message": "Fix unintuitive `--gen_kwargs` behavior (#1329)\n\n* don't override do_sample if no value for it is passed\n\n* Update gen_kwargs override condition\n\n* Update huggingface.py\n\n* Update huggingface.py\n\n* run linters\n\n* silence an erroneous warning"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2024-01-28T20:44:08Z",
        "message": "Apply some best practices and guideline recommendations to code (#1363)\n\n* raise Exception, not a string\n\nAdditional info https://peps.python.org/pep-0352/#exception-hierarchy-changes\nhttps://docs.python.org/3.8/tutorial/errors.html#raising-exceptions\n\n* Apply PEP8 recommendation to prefer isinstance\n\n\"Object type comparisons should always use isinstance() instead of comparing types directly\"\nhttps://peps.python.org/pep-0008/\n\n* Remove dangerous default mutable values in arguments\n\nhttps://pylint.readthedocs.io/en/stable/user_guide/messages/warning/dangerous-default-value.html\n\n* Format logging messages with fstring (not with format)\n\nAdditional info\nhttps://pylint.readthedocs.io/en/stable/user_guide/messages/warning/logging-format-interpolation.html\nThere are also discussions about the speed of formatting while logging or some unintended code executions\nhttps://github.com/pylint-dev/pylint/issues/2395\nhttps://stackoverflow.com/a/54368109\nbut at least one format (fstring one) will be used throughout the project\n\n* Specify utf-8 encoding for `open` explicitly\n\nIf not specified, it may be supposed differently in different environments, OSes, and Python versions. See\nhttps://peps.python.org/pep-0597/\nhttps://docs.python.org/3.11/library/locale.html#locale.getencoding\nhttps://docs.python.org/3.10/library/os.html#utf8-mode\nhttps://pylint.readthedocs.io/en/stable/user_guide/messages/warning/unspecified-encoding.html\n\nHelps also if some code from English language tasks is taken as inspiration for tasks in non-English languages.\n\n* Use inline-ignoring comments to pass pre-commit instead of identity process\n\nhttps://flake8.pycqa.org/en/3.0.1/user/ignoring-errors.html#in-line-ignoring-errors\nhttps://www.flake8rules.com/rules/F841.html\n\nflake8 comments are supported by ruff: https://docs.astral.sh/ruff/linter/#error-suppression"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2024-01-26T20:43:45Z",
        "message": "Add causalLM OpenVino models (#1290)\n\n* added intel optimum\n\n* added intel optimum in readme\n\n* modified intel optimum\n\n* modified intel optimum\n\n* modified intel optimum\n\n* modified install optimum\n\n* modified path of IR file\n\n* added openvino_device\n\n* added openvino_device2\n\n* changed optimum-causal to openvino-causal\n\n* Update README.md\n\n* Update README.md\n\n* remove `lm_eval.base` import\n\n* update openvino-causal -> openvino ; pass device through super().__init__()\n\n* Update README.md\n\n* Add optimum to tests dependencies\n\n* apply pre-commit\n\n* fix so tests pass\n\n---------\n\nCo-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>\nCo-authored-by: haileyschoelkopf <hailey@eleuther.ai>"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2024-01-15T15:02:35Z",
        "message": "Fix data-parallel evaluation with quantized models (#1270)\n\n* add WIP device_map overrides\n\n* update handling outside of accelerate launcher\n\n* change .to(device) log to debug level\n\n* run linter"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2024-01-11T15:20:02Z",
        "message": "Fix bug in multi-token Stop Sequences (#1268)\n\n* fix incorrect lookback protections\n\n* bump generate_until task versions"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-12-27T13:08:03Z",
        "message": "fix unbounded local variable (#1218)"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-12-23T17:58:50Z",
        "message": "Consolidate batching (#1197)\n\n* refactor dataloader\n\n* cleanup + add docs\n\n* change arg\n\n* renamed Collator and added testing\n\n* parametrized test for Collator\n\n* appease pre-commit\n\n* added edge case batch 0 (no batching)\n\n* fix typos\n\n---------\n\nCo-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-12-22T19:37:46Z",
        "message": "Fixes https://github.com/EleutherAI/lm-evaluation-harness/issues/437 (#1180)"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-12-20T14:14:46Z",
        "message": "Switch Linting to `ruff` (#1166)\n\n* add ruff and isort. remove black and flake8\n\n* remove unnecessary dependencies\n\n* remove dependency from table\n\n* change order\n\n* ran ruff\n\n* check 3.9\n\n* exclude evaluator\n\n* update CI workflow\n\n* use ruff config in pyproject.toml\n\n* test\n\n* add isort rules to ruff\n\n* sort imports\n\n* import `make_table`\n\n* try stages for no-commit-to-branch\n\n* turn on mypy for pre-commit\n\n* test\n\n* test\n\n* test\n\n* change no-commit-to-branch to default\n\n* nits\n\n* fixed dependency"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-12-19T16:23:00Z",
        "message": "self.device in huggingface.py line 210 treated as torch.device but might be a string (#1172)\n\n* self.device in huggingface.py line 210\n\nIn huggingface.py line 210, self.device is str and does not have a \"type\" attribute\n\n* Update huggingface.py\n\nThis handles both the case where `self.device` is a `torch.device` and a string\n\n* Update huggingface.py\n\n---------\n\nCo-authored-by: Hailey Schoelkopf <65563625+haileyschoelkopf@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-12-19T13:11:13Z",
        "message": "generalize qwen pad token fix (#1146)"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-12-15T12:08:56Z",
        "message": "place device onto `mps` (#1133)"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-12-14T14:02:35Z",
        "message": "Refactor `hf` modeling code (#1096)\n\n* modularize HFLM code\n\n* pass through extra kwargs to AutoModel.from_pretrained call\n\n* remove explicit model_kwargs\n\n* rename gptq -> autogptq\n\n* fix tokenizer pad token errors\n\n* ensure model always respects device_map and autogptq's selected devices\n\n* add a _get_config helper fn"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-11-29T06:58:31Z",
        "message": "change torch req for mps"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-11-28T14:20:44Z",
        "message": "Merge pull request #1024 from EleutherAI/fix-mbart\n\n[Refactor] Use correct HF model type for MBart-like models"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-11-27T17:11:05Z",
        "message": "Merge pull request #1011 from baberabb/big-refactor_vllm\n\n[Refactor] vllm support"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-11-26T14:59:10Z",
        "message": "use Seq2Seq backend where either can be loaded from HF"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-11-21T16:28:52Z",
        "message": "update multi-token stopsequence handling"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-11-20T15:56:30Z",
        "message": "add typehints"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-11-17T06:44:14Z",
        "message": "edits and format"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-11-10T15:09:28Z",
        "message": "added initialize_task and updated where eval_logger is imported from"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-11-02T18:34:26Z",
        "message": "cleanup hf tqdm"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-11-01T15:43:13Z",
        "message": "fix tqdm total  for hf model"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-10-19T14:42:02Z",
        "message": "fix issue with default metrics and aggregation functions"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-10-19T12:07:31Z",
        "message": "Merge pull request #916 from jasonkrone/big-refactor\n\nFix 'tqdm' object is not subscriptable\" error in huggingface.py when batch size is auto"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-10-17T14:42:37Z",
        "message": "change all mentions of `greedy_until` to `generate_until`"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-10-13T17:46:10Z",
        "message": "Fix \"TypeError: 'tqdm' object is not subscriptable\" error that occurs\nin hugging face model loglikelihood_tokens and greedy_util functions\nwhen batch-size is set to auto"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-10-11T10:20:42Z",
        "message": "check with pre-commit"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-10-11T03:54:55Z",
        "message": "finished test code"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-10-11T03:36:31Z",
        "message": "add _batch_scheduler in greedy_until"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-09-25T14:43:39Z",
        "message": "merged latest"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-09-21T13:09:09Z",
        "message": "Fix positional arguments in HF model generate"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-09-21T11:03:05Z",
        "message": "merged with latest big-refactor"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-09-13T19:37:30Z",
        "message": "Update device list and dtype detection for MPS"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-09-06T15:22:40Z",
        "message": "Merge branch 'big-refactor' into flan-benchmark"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-09-05T16:05:39Z",
        "message": "format for pre-commit"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-09-05T15:44:23Z",
        "message": "Merge pull request #833 from EleutherAI/fix-ppl\n\n[Refactor] Fix wikitext task"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-09-05T15:43:22Z",
        "message": "pre-commit"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-09-05T11:56:57Z",
        "message": "Merge branch 'big-refactor' into mypy"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-09-04T18:50:00Z",
        "message": "Merge pull request #812 from fattorib/bump-accelerate\n\n[Refactor] Bump min accelerate version and update documentation"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-09-04T18:48:05Z",
        "message": "placate precommit"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-09-04T10:41:57Z",
        "message": "modified changes to fix loglikelihood prediction for seq2seq"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-08-26T14:31:31Z",
        "message": "fix FSDP error with .prepare_model()"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-08-25T21:46:34Z",
        "message": "Add suggestions from autotyping\n\nThis adds a bunch of simple annotations suggested by https://github.com/JelleZijlstra/autotyping."
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-08-25T04:27:34Z",
        "message": "reformat"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-08-22T15:22:24Z",
        "message": "added truncation option"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-08-21T16:33:54Z",
        "message": "Merge branch 'big-refactor' of https://github.com/EleutherAI/lm-evaluation-harness into superglue"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-08-11T00:44:27Z",
        "message": "placate pre-commit"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-08-10T21:14:36Z",
        "message": "Use evaluation_mode=True for accelerate to prevent OOM"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-08-10T17:00:24Z",
        "message": "Merge branch 'big-refactor' of https://github.com/EleutherAI/lm-evaluation-harness into superglue"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-08-07T13:58:11Z",
        "message": "making t5 version of superglue prompt"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-08-04T10:04:15Z",
        "message": "pre-commit format"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-08-04T06:42:07Z",
        "message": "Merge pull request #673 from fattorib/big-refactor-autobatching\n\n[Refactor] Port over Autobatching"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-08-03T13:24:38Z",
        "message": "Update huggingface.py\n\n`max_length` was misdefined as a tuple."
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-08-02T15:11:43Z",
        "message": "fix to add a case for if a user add `max_length` to generation_kwargs"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-08-01T02:33:42Z",
        "message": "merge conflicts"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-07-27T17:45:51Z",
        "message": "Merge remote-tracking branch 'upstream/big-refactor' into big-refactor-autobatching"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-07-27T17:45:22Z",
        "message": "skip recomputing batch size if it is maximal"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-07-27T09:27:05Z",
        "message": "autobatching support for enc-dec"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-07-26T17:53:39Z",
        "message": "materialze \"out\" tensor during batch calculation"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-07-24T03:56:02Z",
        "message": "Early stop bug of greedy_until (primary_until should be a list of str)\n\nI discovered that the accuracy of all models (e.g., llama7b, llama13b, starcoder) in the 'gsm8k-cot' task was 0%. After a thorough investigation, I realized that the generated text for each question was causing an early stop, preventing the 'regex_pattern' from finding any answers. This issue was caused by an incorrect assignment of the 'primary_until' variable in the 'greedy_until' function. Specifically, 'primary_until' should be a list of strings instead of a single string, as the 'stop_sequences' parameter in the 'stop_sequences_criteria' function requires a List[str]. Once I assigned 'primary_until' to '[until[0]]', the accuracy of llama7b in the 'gsm8k-cot' task increased to 1.67%."
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-07-22T18:11:19Z",
        "message": "multi-device: take minimum computed autobatch over all ranks"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-07-22T17:46:16Z",
        "message": "Merge remote-tracking branch 'upstream/big-refactor' into big-refactor-autobatching"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-07-21T14:04:46Z",
        "message": "handle trust_remote_code models better"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-07-17T19:05:20Z",
        "message": "edge case for P-Tuning in causal LM"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-07-16T04:02:50Z",
        "message": "set fp32 if device=mps"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-07-15T16:51:05Z",
        "message": "add mps"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-07-15T16:44:05Z",
        "message": "removed rnd from task.fewshot_context in write_out.py; add mps in huggingface.py"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-07-13T21:37:20Z",
        "message": "add fast tokenizer flag"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-07-12T17:44:35Z",
        "message": "remove TODO comment"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-07-10T19:58:55Z",
        "message": "pass override bs to _loglikelihood_tokens"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-07-10T19:49:24Z",
        "message": "initial autobatching commit"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-07-03T16:24:36Z",
        "message": "don't place quantized models .to(device)"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-07-03T16:05:44Z",
        "message": "Merge pull request #647 from EleutherAI/handle-multigpu-errors\n\n[Refactor] Handle `cuda:0` device assignment"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-07-03T16:04:24Z",
        "message": "revert typehint"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-07-03T16:03:41Z",
        "message": "revert gpus > num processes case"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-07-03T15:56:31Z",
        "message": "handle device assignment better"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-07-03T15:43:33Z",
        "message": "auto-gptq -> gptq and an import warning"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-07-02T11:27:47Z",
        "message": "Add PEFT, quantization, remote code, LLaMA fix"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-06-29T01:42:58Z",
        "message": "Merge branch 'big-refactor' into device-mapping"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-06-29T01:40:37Z",
        "message": "remove stray print"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-06-28T20:30:45Z",
        "message": "cleanup + change to name=parallelize"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-06-28T18:49:34Z",
        "message": "add device_map options for HFLM"
    },
    {
        "repo_url": "github.com/EleutherAI/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-06-28T15:02:47Z",
        "message": "Merge branch 'big-refactor' into add-back-cache"
    },
    {
        "repo_url": "github.com/SCIR-HI/Huatuo-Llama-Med-Chinese",
        "filepath": "infer.py",
        "commit_date": "2023-08-07T13:46:05Z",
        "message": "Update Huozi-based model\n\nMajor update. Please try our new Huozi-based model, which is much better."
    },
    {
        "repo_url": "github.com/SCIR-HI/Huatuo-Llama-Med-Chinese",
        "filepath": "infer.py",
        "commit_date": "2023-04-12T12:43:41Z",
        "message": "add test"
    },
    {
        "repo_url": "github.com/SCIR-HI/Huatuo-Llama-Med-Chinese",
        "filepath": "infer.py",
        "commit_date": "2023-04-01T09:37:49Z",
        "message": "init code"
    },
    {
        "repo_url": "github.com/LlamaFamily/Llama-Chinese",
        "filepath": "examples/chat_gradio_no_merge.py",
        "commit_date": "2024-01-30T06:52:49Z",
        "message": "requirements update"
    },
    {
        "repo_url": "github.com/LlamaFamily/Llama-Chinese",
        "filepath": "examples/chat_gradio_no_merge.py",
        "commit_date": "2024-01-23T11:57:23Z",
        "message": "\u8c03\u6574\u6a21\u578b\u52a0\u8f7d\u4ee3\u7801,\u4ee5\u53ca\u652f\u6301tensorrt_llm\u7684\u63a8\u7406"
    },
    {
        "repo_url": "github.com/LlamaFamily/Llama-Chinese",
        "filepath": "examples/chat_gradio_no_merge.py",
        "commit_date": "2023-09-02T05:20:43Z",
        "message": "\u6dfb\u52a0lora \u4e0d\u5408\u5e76\u7684gradio \u6d4b\u8bd5"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2024-02-28T17:59:20Z",
        "message": "Remove \"generation_length\" in favor of \"generation_kwargs\" (#3014)\n\n* kill generation_length\n\n* fix tests\n\n* fix test\n\n* add deprecation warning\n\n* fix test\n\n* add gen_len back into static_keys\n\n* simplify setting variable in forward and add test\n\n* simply test\n\n* trailing comma\n\n* trailing comma\n\n* linting\n\n---------\n\nCo-authored-by: Daniel King <43149077+dakinggg@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2024-02-12T16:29:26Z",
        "message": "Refactor `update_metric` (#2965)\n\n* commit one\n\n* rm unused imports\n\n* Update nlp.py\n\n* Update huggingface.py\n\n* Update nlp.py\n\nMake all args same name\n\n* Update huggingface.py\n\n* Update nlp.py\n\n* Update nlp.py\n\n* Update nlp.py\n\n* fix\n\n* wip\n\n* Update composer/metrics/nlp.py\n\nCo-authored-by: Daniel King <43149077+dakinggg@users.noreply.github.com>\n\n* finish\n\n* Update composer/metrics/nlp.py\n\nCo-authored-by: Daniel King <43149077+dakinggg@users.noreply.github.com>\n\n* add tests, dont return logits, fix linting\n\n* rm incorrect asserts\n\n* del incorrect asserts\n\n* rm pyright ignore\n\n* Update composer/metrics/nlp.py\n\nCo-authored-by: Daniel King <43149077+dakinggg@users.noreply.github.com>\n\n* rm comments\n\n---------\n\nCo-authored-by: Jeremy D <115047575+bmosaicml@users.noreply.github.com>\nCo-authored-by: Jeremy Dohmann <jeremy@mosaicml.com>\nCo-authored-by: Daniel King <43149077+dakinggg@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2024-01-31T15:55:53Z",
        "message": "Remove torch 1.13 (#2941)\n\n* v1\n\n* kill torch 1.13\n\n* more cleanup\n\n* remove local\n\n* lint\n\n* raise value error\n\n* remove import\n\n* lint\n\n* fix\n\n* fix type\n\n* lint\n\n* fix test\n\n* remove type\n\n* fix types\n\n* fix\n\n* fix\n\n* fix\n\n* remove vision\n\n* fix tests"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2024-01-30T06:03:43Z",
        "message": "Fix daily tests for peft on 1.13 (#2923)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2024-01-29T18:59:43Z",
        "message": "Integrate PEFT LoRA with HuggingFaceModel (#2829)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2024-01-12T21:04:53Z",
        "message": "Upgrade pyright to 1.1.310 (#2841)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2024-01-08T17:51:15Z",
        "message": "Add encoding=utf-8 (#2824)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-10-23T21:44:20Z",
        "message": "Upgrade to transformers 4.34.1 (#2635)\n\n* bump transformers version\n\n* add new special casing to tokenizer equivalence check\n\n* try/except for flash v1 issue"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-10-05T21:04:29Z",
        "message": "Change the tokenizer json file to read binary (#2608)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-10-04T19:04:19Z",
        "message": " Add chain of thought eval (#2466)\n\n* implement cot\n\n* fix tests\n\n* debug print statement\n\n* fix max answer lengths for cot\n\n* fix pyright; git push\n\n* Update composer/datasets/in_context_learning_evaluation.py\n\nCo-authored-by: Brian <23239305+b-chu@users.noreply.github.com>\n\n* Update composer/metrics/nlp.py\n\nCo-authored-by: Brian <23239305+b-chu@users.noreply.github.com>\n\n* address comments from daniel\n\n* fix debugging\n\n* address comments\n\n* address comments\n\n* address comments\n\n* Update composer/datasets/in_context_learning_evaluation.py\n\nCo-authored-by: Brian <23239305+b-chu@users.noreply.github.com>\n\n* Update composer/datasets/in_context_learning_evaluation.py\n\nCo-authored-by: Brian <23239305+b-chu@users.noreply.github.com>\n\n* we outtie\n\n* prepend _\n\n---------\n\nCo-authored-by: Brian <23239305+b-chu@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-08-28T18:35:34Z",
        "message": "Fix huggingface tokenizer loading for slow tokenizers (#2483)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-08-22T05:50:49Z",
        "message": "kwarg for input_ids (#2459)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-08-04T23:32:44Z",
        "message": "Add ruff pre-commit  (#2414)\n\n* Add ruff pre-commit hook and apply some sane autofixes\n\n* Reduce number of autofixes\n\n* Fix pyproject\n\n* Update ruff hook\n\n---------\n\nCo-authored-by: Daniel King <43149077+dakinggg@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-07-25T21:35:04Z",
        "message": "Change transformers (#2383)\n\n* fix autoresume with slashed directory\n\n* Revert \"fix autoresume with slashed directory\"\n\nThis reverts commit 3dfb5f5430da5512bbf418820086b4f291d814f6.\n\nrevert\n\n* upgrade transformers to 4.31.0\n\n* add transformers support\n\n* fix precommit\n\n* fix precommit\n\n* fix precommit\n\n* add trust_remote_code\n\n* pre-commit fix"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-07-20T23:09:13Z",
        "message": "Add code eval dataset and metric (#2301)\n\n* fix autoresume with slashed directory\n\n* Revert \"fix autoresume with slashed directory\"\n\nThis reverts commit 3dfb5f5430da5512bbf418820086b4f291d814f6.\n\nrevert\n\n* add dataset file -- testing next\n\n* add dataloader for code eval\n\n* more testing stuff\n\n* add unit tests\n\n* add full human_eval dataset\n\n* push metric and tests with timeout and stopping criteria\n\n* finish code eval\n\n* fix precommit\n\n* fix test bug\n\n* add autouse fixture to clear cache\n\n* try to fix tests\n\n* try to fix tests\n\n* fix tests\n\n* fix distributed tests\n\n* make tests less bulky\n\n* add lambda support for code eval\n\n* finish lambdas\n\n* fix boto3 import\n\n* Update nlp.py\n\n* Update nlp.py\n\n* fix metric stuff\n\n* fix pyright\n\n* delete go targz\n\n* Delete icl_0.jsonl\n\n* fix yapf\n\n* finish PR\n\n* finish tests\n\n* change timeout\n\n* add clients\n\n* add clients\n\n* add all clients\n\n* add clients finished\n\n* add default sampling params\n\n* change sentencepiece tokenization\n\n* fix precommit\n\n* fix sentencepiece\n\n* Update composer/metrics/nlp.py\n\nCo-authored-by: Mihir Patel <mihir.v.patel7@gmail.com>\n\n* Update composer/datasets/in_context_learning_evaluation.py\n\nCo-authored-by: Mihir Patel <mihir.v.patel7@gmail.com>\n\n* Update in_context_learning_evaluation.py\n\n---------\n\nCo-authored-by: brandon <bcui8377@gmail.com>\nCo-authored-by: Mihir Patel <mihir.v.patel7@gmail.com>"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-06-08T17:37:26Z",
        "message": "fixed adding tokenizer to hf (#2290)\n\nFixed the function 'write_huggingface_pretrained_from_composer_checkpoint' to properly write the tokenizer from the composer checkpoint by additionally saving the tokenizer from the composer checkpoint to the HuggingFace checkpoint.\n\nAn additional test was implemented for this issue.\n\n---------\n\nCo-authored-by: Vincent Chen <vincent@mosaicml.com>"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-06-05T19:53:50Z",
        "message": "Patch for tokenizers that have python files in save_pretrained output (#2279)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-06-05T16:51:41Z",
        "message": "Confirming the output variable has two dimensions before confirming the shape of the second element. (#2275)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-05-02T03:04:31Z",
        "message": "Update warning->info for number of tokens (#2192)\n\n* update warning\n\n* fix test\n\n* update warning\n\n---------\n\nCo-authored-by: nik-mosaic <101217697+nik-mosaic@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-04-27T22:29:51Z",
        "message": "Add support for torch 2.0 (#2172)\n\n* fix prefetch default\n* fix pyright issues with torch2\n* xfail onnx test on torch2\n* attempt to add torch2 workflows\n* summon full params for generate\n* fix optimizer monitor\n* fix gradient clipping tests\n* fix autolog hparams\n* backwards compat for optimizer monitor test\n* Upgrade FSDP checkpointing to be compatible with torch 2.0 (#2169)\n* rename using_torch_2\n* add torch2 to daily gpu"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-04-14T21:05:08Z",
        "message": "Better defaults for `get_num_tokens_in_batch` (#2139)\n\n* add better defaults for num tokens counting\n* add tests for max duration in tokens"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-04-06T20:22:50Z",
        "message": "add logic for direct instantiation (#2122)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-03-31T21:14:11Z",
        "message": "add sentencepiece support (#2093)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-03-31T17:00:03Z",
        "message": "adjust decoding for eval forward (#2107)\n\n* adjust decoding for eval forward to the proper length"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-03-28T21:47:07Z",
        "message": "gate the extra forward call specifically for fsdp (#2102)\n\n* Gate extra forward call before generate with FSDP check\n* adjust FSDP check helper to check top level module and early exit"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-03-21T00:47:40Z",
        "message": "Add support for ICL QA tasks and generation during evaluation with `HuggingFaceModel` (#2045)\n\n* Add InContextLearningQADataset and InContextLearningQAAccuracy for generation style tasks\n* Add support for calling generate using HuggingFaceModel\n* Fixes duplicated download of ICL datasets per node"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-03-20T18:14:10Z",
        "message": "fix name_or_path usage in HF save/load usage (#2075)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-03-09T19:01:40Z",
        "message": "Backward Compat with Torchmetrics (#2046)\n\n* backwards compatbility\n\n* fix test\n\n* debug\n\n* safe load\n\n* add logs\n\n* remove print\n\n* add print\n\n* flip check\n\n* fix eval\n\n* add lint"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-03-07T21:41:56Z",
        "message": "Remove deprecated code (#2026)\n\n* cleanup\n\n* cleanup\n\n* fix hf model\n\n* fix test\n\n* fix test\n\n* set to evla metrics\n\n* fix bug in metrics\n\n* switch to warnings\n\n* add warnings\n\n* filter warnings\n\n* retry\n\n* lint\n\n* remove dead comment\n\n* remove test"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-03-06T20:40:38Z",
        "message": "Adjust how HuggingFaceModel handles embedding resizing (#2027)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-02-21T18:03:31Z",
        "message": "add map location cpu to huggingface utils (#1980)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-02-17T18:45:22Z",
        "message": "Util for writing HuggingFace save_pretrained from a composer checkpoint (#1974)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-02-15T21:07:09Z",
        "message": "allow eval metrics to be passed in to HuggingFaceModel directly (#1971)\n\n* allow eval metrics to be passed in to HuggingFaceModel directly\n* add deprecation warning and fix typo"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-02-09T21:41:35Z",
        "message": "add support for enc-dec batches without decoder_input_ids (#1950)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-02-09T18:29:30Z",
        "message": "add return dict false test and bug fix (#1948)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-02-09T18:20:25Z",
        "message": "Change how we check for forwards args in models for hf models (#1955)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-02-01T22:36:49Z",
        "message": "Add few shot and multiple choice to ICL evaluation (#1876)\n\n* new branch\n\n* unittest multi gpu\n\n* add testing for batch padding and idx sampling\n\n* update\n\n* change naming of file\n\n* reimplement lambada\n\n* merge\n\n* add multiple choice eval\n\n* merge multiple choice\n\n* update\n\n* reimplement lambada\n\n* update changes\n\n* finish rebase\n\n* merge mc and lm into same file\n\n* add testing\n\n* add testing for batch padding and idx sampling\n\n* add testing for batch padding and idx sampling\n\n* new branch\n\n* unittest multi gpu\n\n* add testing for batch padding and idx sampling\n\n* update\n\n* change naming of file\n\n* reimplement lambada\n\n* merge\n\n* add multiple choice eval\n\n* update\n\n* reimplement lambada\n\n* update changes\n\n* finish rebase\n\n* merge mc and lm into same file\n\n* add testing\n\n* add testing for batch padding and idx sampling\n\n* add testing for batch padding and idx sampling\n\n* merge updates\n\n* finished merge\n\n* merge\n\n* fix broken merge\n\n* fix broken tests\n\n* yapf\n\n* fix comment\n\n* move label shifting back where it was and dont store labels in the no logits case\n\n* fix duplicated dep in merge;\n\n* fix nits\n\n* fix nits\n\n---------\n\nCo-authored-by: Daniel King <daniel@mosaicml.com>"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-01-27T00:48:37Z",
        "message": "Add logic for shifting labels before computing metrics (#1913)\n\nAdds a `shift_labels` init argument to `HuggingFaceModel` class. This instructs the model whether to shift labels by one token before computing metrics, which mimics the way HF Causal LM classes handle labels when computing loss. This fixes the current implementation, which never does this shifting and produces incorrect metric results for Causal LMs.\n\nIf `shift_labels` is not specified, `HuggingFaceModel` will try to infer the correct behavior based on whether the model is an instance of a registered HF Causal LM class (or a subclass of one)."
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-01-11T20:49:50Z",
        "message": "Fix unprotected import  (#1874)\n\nAs title. Along for the ride, change default recommendation to use .[all] instead of .[dev] for lint (latter is only for docs + tests)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2023-01-09T22:51:44Z",
        "message": "Feature/lambada evaluator (#1845)\n\n* new branch\n\n* unittest multi gpu\n\n* reimplement lambada\n\n* update\n\n* pyright\n\n* change naming of file\n\nCo-authored-by: Daniel King <daniel@mosaicml.com>"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2022-12-03T04:34:55Z",
        "message": "Huggingface pretrain + finetune notebook (#1775)\n\nAdds an example notebook for pretrain + finetune using huggingface"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2022-11-30T21:55:15Z",
        "message": "Autoload HuggingFace model/tokenizer (#1754)\n\nAdd autoload of HF model/tokenizer from composer checkpoint to HuggingFaceModel"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2022-11-29T21:57:47Z",
        "message": "Add huggingface info to state dict (#1744)\n\nAdd huggingface info to the state dict in the new integrations key."
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2022-11-14T21:50:07Z",
        "message": "Simple nlp tests (#1716)\n\nAdds a simple transformer classifier and a simple \"text\" (really just input ids, not using a tokenizer) classification dataset for testing purposes. Adds a test for HuggingfaceModel using the above dataset to train, eval, and predict. Removes the model_inputs from HuggingfaceModel"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2022-10-06T16:28:45Z",
        "message": "fix loss and eval_forward for HF models (#1597)\n\n* fix loss and eval_forward for HF models\n\n* add a comment\n\nCo-authored-by: nik-mosaic <101217697+nik-mosaic@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2022-09-01T14:11:08Z",
        "message": "update model token embeddings according to tokenizer len (#1493)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2022-08-26T00:59:51Z",
        "message": "Metrics Refactor Part 2 (#1419)\n\nThis PR implements the second half of the metrics refactor (#1411) that updates the training loop and all the Composer models.\n\nThe training loop now uses the previously added state.train_metrics and state.eval_metrics from #1411 to perform all training and evaluation on the correct set of metrics.\n\nThe main changes with respect to models is that the models now have split up validate() into eval_forward() and update_metrics() methods, which run an evaluation forward pass and update the metrics with the outputs of the evaluation forward pass respectively. This is mainly to get rid of the double forward pass (#467)."
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2022-06-29T21:30:09Z",
        "message": "Add Debug Log Statements; Fix Pyright (#1218)\n\nThis PR adds debug log statements throughout the Composer trainer and engine. When running with the log level set to debug, it will generate a (verbose) log containing a paper trail of the events, callbacks, and algorithms that fire.\n\nComing along for the ride:\n\n* Pyright version bump and type fixes\n* Some refactoring to make the autoresume logic in the trainer more readable"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2022-06-28T20:10:29Z",
        "message": "refactor bert and gpt (#1130)\n\nConverting the composer GPT2 and BERT models to use a HuggingFaceModel base class compatible with any pretrained model off the HF Hub. Converted the model creations to factory functions and moved all logic out of the hparams classes."
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2022-06-11T00:22:17Z",
        "message": "Switch to single quotes (#1152)\n\nEnable the \"double-quote-string-fixer\" pre-commit hook."
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "composer/models/huggingface.py",
        "commit_date": "2022-05-24T21:18:36Z",
        "message": "Huggingface part1 (#1047)"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2024-02-02T09:25:58Z",
        "message": "fix: proper SSE handling for vllm (#877)\n\nfix: proper SSE handling"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-12-18T17:41:19Z",
        "message": "feat(vllm): support GPTQ with 0.2.6 (#797)\n\n* feat(vllm): GPTQ support passthrough\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: run scripts\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix(install): set order of xformers before vllm\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* feat: support GPTQ with vLLM\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-12-14T22:27:32Z",
        "message": "fix(infra): conform ruff to 150 LL (#781)\n\nGenerally correctly format it with ruff format and manual style\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-30T12:58:35Z",
        "message": "fix(style): setup correct block format\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-30T12:36:34Z",
        "message": "fix(setter): correct item with the same kwargs with stubs\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-29T18:00:08Z",
        "message": "fix(llm): remove unecessary parsing\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-29T06:45:14Z",
        "message": "chore(vllm): add arguments for gpu memory utilization\n\nProbably not going to fix anything, just delaying the problem.\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-26T09:53:36Z",
        "message": "fix(sdk): remove broken sdk\n\ncodespace now around 2.8k lines\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-26T07:49:48Z",
        "message": "fix(gpus): disable slots for now to enable cached_property\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-26T06:44:59Z",
        "message": "fix(style): reduce boilerplate and format to custom logics\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-24T06:11:31Z",
        "message": "refactor: focus (#730)\n\n* perf: remove based images\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: update changelog\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: move dockerifle to run on release only\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: cleanup unused types\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-22T11:49:14Z",
        "message": "feat(openai): chat templates and complete control of prompt generation (#725)\n\n* feat(openai): chat templates and complete control of prompt generation\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: correctly use base chat templates\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: remove symlink\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-22T09:21:13Z",
        "message": "fix(openai): correct stop tokens and finish_reason state (#722)\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-22T06:15:19Z",
        "message": "fix(base-image): update base image to include cuda for now (#720)\n\n* fix(base-image): update base image to include cuda for now\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* fix: build core and client on release images\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: cleanup style changes\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-21T23:54:51Z",
        "message": "chore(logger): fix warnings and streamline style (#717)\n\n\nSorry but there are too much wasted spacing in `_llm.py`, and I'm unhappy and not productive anytime I look or want to do anything with it\n\n---------\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-21T09:39:48Z",
        "message": "refactor: delete unused code (#716)\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-21T07:01:36Z",
        "message": "feat(generation): add support for eos_token_id (#714)\n\nchore: add support for custom eos_token_id\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-21T06:56:51Z",
        "message": "chore: cleanup unused prompt templates (#713)\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-21T01:37:15Z",
        "message": "refactor: update runner helpers and add max_model_len (#712)\n\n* chore(runner): cleanup unecessary checks for runnable backend\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: saving llm reference to runner\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: correct inject item\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: update support for max_seq_len\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* fix: correct max_model_len\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: update and warning backward compatibility\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: remove unused sets\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-20T22:06:25Z",
        "message": "fix(build): only load model when eager is True\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-20T03:42:25Z",
        "message": "fix(backend): correct use variable for backend when initialisation (#702)\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-19T15:25:08Z",
        "message": "feat(engine): CTranslate2 (#698)\n\n* chore: update instruction for dependencies\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* feat(experimental): CTranslate2\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-19T07:52:32Z",
        "message": "feat(vllm): bump to 0.2.2 (#695)\n\n* feat(vllm): bump to 0.2.2\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: update changelog\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: move up to CUDA 12.1\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* fix: remove auto-gptq installation\n\nsince the builder image doesn't have access to GPU\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* fix: update containerization warning\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-19T06:48:33Z",
        "message": "feat(ctranslate): initial infrastructure support (#694)\n\n* perf: compact and improve speed and agility\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* --wip--\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: cleanup infrastructure\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: update styles notes and autogen mypy configuration\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-19T00:26:20Z",
        "message": "feat: heuristics logprobs (#692)\n\n* fix(encoder): bring back T5 support on PyTorch\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* feat: support logprobs and prompt_logprobs\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* docs: update changelog\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-18T07:02:16Z",
        "message": "fix(annotations): check library through find_spec (#691)\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-17T16:23:22Z",
        "message": "fix(llm): remove unnecessary check (#683)\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-17T15:52:05Z",
        "message": "fix(torch_dtype): correctly infer based on options (#682)\n\nUsers should be able to set the dtype during build, as we it doesn't effect start time\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-16T09:45:49Z",
        "message": "perf: reduce footprint (#668)\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-16T08:12:52Z",
        "message": "infra: makes huggingface-hub requirements on fine-tune (#665)\n\ninfra: makes huggingface-hub core deps\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-16T08:05:58Z",
        "message": "feat(llm): respect warnings environment for dtype warning (#664)\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-16T07:58:45Z",
        "message": "feat(type): provide structured annotations stubs (#663)\n\n* feat(type): provide client stubs\n\nseparation of concern for more brevity code base\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* docs: update changelog\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-15T05:18:31Z",
        "message": "perf: improve build logics and cleanup speed (#657)\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-15T02:55:24Z",
        "message": "feat: Yi models (#651)\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-14T01:40:50Z",
        "message": "fix(cpu): more verbose definition for dtype casting (#639)\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-14T01:32:07Z",
        "message": "fix(generation): compatibility dtype with CPU (#638)\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-13T18:48:04Z",
        "message": "fix(torch_dtype): load eagerly (#631)\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-13T10:25:50Z",
        "message": "feat(cli): `--dtype` arguments (#627)\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-13T10:08:33Z",
        "message": "feat(vllm): support passing specific dtype (#626)\n\n* feat(vllm): support passing specific dtype\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: correctly cached the item\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* ci: auto fixes from pre-commit.ci\n\nFor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-13T02:12:50Z",
        "message": "fix(ruff): correct consistency between isort and formatter (#624)\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-12T22:39:06Z",
        "message": "feat(llm): update warning envvar and add embedded mode (#618)\n\n* chore: unify warning envvar and update type inference\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore; update documentation about embedded\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-12T19:55:37Z",
        "message": "chore(llm): expose quantise and lazy load heavy imports (#617)\n\n* chore(llm): expose quantise and lazy load heavy imports\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: move transformers to TYPE_CHECKING block\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-12T03:36:10Z",
        "message": "refactor(config): simplify configuration and update start CLI output (#611)\n\n* chore(config): simplify configuration and update start CLI output\nhandling\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: remove state and message sent after server lifecycle\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: update color stream and refactor reusable logic\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: update documentations and mypy\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-10T07:36:12Z",
        "message": "fix: loading correct local models (#599)\n\n* fix(model): loading local correctly\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* chore: update repr and correct bentomodel processor\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* ci: auto fixes from pre-commit.ci\n\nFor more information, see https://pre-commit.ci\n\n* chore: cleanup transformers implementation\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* fix: ruff to ignore I001 on all stubs\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-09T17:44:05Z",
        "message": "infra: using ruff formatter (#594)\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-09T16:40:17Z",
        "message": "refactor(cli): cleanup API (#592)\n\n* chore: remove unused imports\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* refactor(cli): update to only need model_id\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* feat: `openllm start model-id`\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: add changelog\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: update changelog notice\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: update correct config and running tools\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: update backward compat options and treat JSON outputs\ncorespondingly\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-08T11:57:11Z",
        "message": "fix(awq): correct awq detection for support (#586)\n\n* fix(awq): correct detection for awq\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* chore: update base docker to work\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* chore: disable awq on pytorch for now\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* ci: auto fixes from pre-commit.ci\n\nFor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-08T07:53:08Z",
        "message": "chore(service): cleanup API (#579)\n\n* chore(service): cleanup API\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: running tools\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* fix: tests import\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-08T07:23:08Z",
        "message": "refactor(strategies): move logics into openllm-python (#578)\n\nfix(strategies): move to openllm\n\nStrategies shouldn't be a part of openllm-core\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-08T06:24:03Z",
        "message": "refactor: cleanup typing to expose correct API (#576)\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-08T03:34:11Z",
        "message": "chore(runner): yield the outputs directly (#573)\n\nupdate openai client examples to >1\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-07T21:42:20Z",
        "message": "fix: update build dependencies and format chat prompt (#569)\n\nchore: update correct check and format prompt\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-07T02:34:44Z",
        "message": "infra: update docs on serving fine-tuning layers (#567)\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-07T01:39:43Z",
        "message": "perf: unify LLM interface (#518)\n\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\nSigned-off-by: paperspace <29749331+aarnphm@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-11-03T17:44:25Z",
        "message": "fix: Max new tokens (#550)\n\nBug fix for retrieving user input max_new_tokens"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-10-30T07:20:43Z",
        "message": "fix(openai): Chat templates (#519)\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\nCo-authored-by: Aaron <29749331+aarnphm@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-10-14T20:04:35Z",
        "message": "fix(breaking): remove embeddings and update client implementation (#500)"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-10-12T21:21:54Z",
        "message": "feat(client): simple implementation and streaming (#256)"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-10-10T16:29:20Z",
        "message": "fix: do not reply on env var for built bento/docker (#477)\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-10-07T06:21:31Z",
        "message": "fix(style): remove weird break on split item\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-10-07T04:50:03Z",
        "message": "feat: OpenAI-compatible API (#417)\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-10-03T13:53:37Z",
        "message": "feat: PromptTemplate and system prompt support (#407)\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-09-19T07:04:59Z",
        "message": "feat: support continuous batching on `generate` (#375)\n\n* feat: support continuous batching on `generate`\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: add changelog\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-09-18T06:26:53Z",
        "message": "fix: set default serialisation methods (#355)\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-09-14T07:09:36Z",
        "message": "feat: continuous batching with vLLM (#349)\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* feat: continuous batching\n\nSigned-off-by: paperspace <29749331+aarnphm@users.noreply.github.com>\n\n* chore: add changeloe\n\nSigned-off-by: paperspace <29749331+aarnphm@users.noreply.github.com>\n\n* chore: add one shot generation\n\nSigned-off-by: paperspace <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: paperspace <29749331+aarnphm@users.noreply.github.com>\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-09-12T21:44:01Z",
        "message": "fix(serialisation): vLLM safetensors support (#324)\n\n* fix(serilisation): vllm support for safetensors\n\nSigned-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>\n\n* chore: running tools\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: generalize one shot generation\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: add changelog [skip ci]\n\nSigned-off-by: paperspace <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\nSigned-off-by: paperspace <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-09-07T01:48:45Z",
        "message": "fix(quantize): dyn quant for int8 and int4\n\nonly set tokenizer when it is gptq\n\nSigned-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-09-06T18:08:52Z",
        "message": "fix: synchronize device for inference\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-09-05T01:08:23Z",
        "message": "chore: only add bentomodel branch during generated service with\nOpenLLM\n\nSigned-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-09-04T18:05:50Z",
        "message": "fix(gptq): use upstream integration (#297)\n\n* wip\n\nSigned-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>\n\n* feat: GPTQ transformers integration\n\nSigned-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>\n\n* fix: only load if variable is available and add changelog\n\nSigned-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>\n\n* chore: remove boilerplate check\n\nSigned-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-09-01T17:00:49Z",
        "message": "chore: ignore new lines split [skip ci]\n\nSigned-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-09-01T09:34:22Z",
        "message": "fix(yapf): align weird new lines break [generated] [skip ci] (#284)\n\nfix(yapf): align weird new lines break\n\nSigned-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-09-01T09:15:19Z",
        "message": "refactor(breaking): unify LLM API (#283)\n\nSigned-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-08-30T17:52:35Z",
        "message": "style: google\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-08-30T15:37:41Z",
        "message": "fix: persistent styling between ruff and yapf (#279)"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-08-26T11:36:57Z",
        "message": "chore(style): add one blank line\n\nto conform with Google style\n\nSigned-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-08-26T11:27:32Z",
        "message": "feat(vllm): streaming (#260)"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-08-25T10:38:59Z",
        "message": "cron(style): run formatter [generated] [skip ci] (#257)"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-08-25T08:36:35Z",
        "message": "chore: ignore peft and fix adapter loading issue (#255)\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-08-23T12:46:22Z",
        "message": "chore(style): synchronized style across packages [skip ci]\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-08-22T17:02:00Z",
        "message": "chore(style): reduce line length and truncate compression\n\nSigned-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-08-22T14:03:06Z",
        "message": "chore(style): enable yapf to match with style guidelines\n\nSigned-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-08-22T12:55:46Z",
        "message": "refactor: packages (#249)"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-08-21T12:02:35Z",
        "message": "fix(generate): Correct set batch output for generate from iterator\n\nSigned-off-by: aarnphm-ec2-dev <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-08-20T11:32:49Z",
        "message": "feat: token streaming and SSE support (#240)"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-08-17T19:17:00Z",
        "message": "feat(embedding): Adding generic endpoint (#227)"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm/_llm.py",
        "commit_date": "2023-08-15T06:11:14Z",
        "message": "refactor: monorepo (#203)"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "generate.py",
        "commit_date": "2023-06-29T07:59:57Z",
        "message": "rename"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "generate.py",
        "commit_date": "2023-06-01T09:05:42Z",
        "message": "multi GPUs inference"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "generate.py",
        "commit_date": "2023-05-10T12:30:03Z",
        "message": "update code"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "generate.py",
        "commit_date": "2023-05-10T09:08:35Z",
        "message": "update code"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "generate.py",
        "commit_date": "2023-04-16T07:55:58Z",
        "message": "release continue-finetune-7epoch-cMedQA2"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "generate.py",
        "commit_date": "2023-04-04T11:39:35Z",
        "message": "rearrange code; add html code support"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "generate.py",
        "commit_date": "2023-04-01T03:19:51Z",
        "message": "update chat"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "generate.py",
        "commit_date": "2023-03-28T07:52:13Z",
        "message": "merge origin"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "generate.py",
        "commit_date": "2023-03-28T07:38:15Z",
        "message": "add new parameter: min new tokens"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "generate.py",
        "commit_date": "2023-03-28T03:30:02Z",
        "message": "Merge branch 'master' into generation_visualization"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "generate.py",
        "commit_date": "2023-03-28T03:26:19Z",
        "message": "clean and reformat code"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "generate.py",
        "commit_date": "2023-03-28T03:20:49Z",
        "message": "add support for beam search streamly output"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "generate.py",
        "commit_date": "2023-03-28T01:15:23Z",
        "message": "Merge branch 'master' of https://github.com/Facico/Chinese-Vicuna"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "generate.py",
        "commit_date": "2023-03-28T01:15:11Z",
        "message": "\u6dfb\u52a0\u8fdc\u7a0blora\u52a0\u8f7d"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "generate.py",
        "commit_date": "2023-03-27T14:44:39Z",
        "message": "merge"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "generate.py",
        "commit_date": "2023-03-27T12:53:12Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "generate.py",
        "commit_date": "2023-03-27T12:33:58Z",
        "message": "\u589e\u52a0\u7b80\u5355\u7684\u4ea4\u4e92\u811a\u672c"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "generate.py",
        "commit_date": "2023-03-24T02:15:18Z",
        "message": "\u65b0\u589e8000checkpoint"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "generate.py",
        "commit_date": "2023-03-23T08:56:14Z",
        "message": "\u6dfb\u52a0repetition_penalty"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "generate.py",
        "commit_date": "2023-03-23T06:30:13Z",
        "message": "\u4fee\u6539generate\uff0c\u8ba9\u5176\u80fd\u5206\u4eab\u7f51\u5740"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "generate.py",
        "commit_date": "2023-03-23T03:17:12Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/pengxiao-song/LaWGPT",
        "filepath": "utils/merge.py",
        "commit_date": "2023-05-21T18:39:04Z",
        "message": "[ENH] Restructure the project."
    },
    {
        "repo_url": "github.com/yangjianxin1/Firefly",
        "filepath": "component/utils.py",
        "commit_date": "2023-09-02T10:26:07Z",
        "message": "\u4f7f\u75284bit\u8fdb\u884c\u63a8\u7406 & \u65e0\u9700\u624b\u52a8\u5408\u5e76\u6743\u91cd\u8fdb\u884c\u63a8\u7406"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "examples/loftq_finetuning/train_gsm8k_llama.py",
        "commit_date": "2024-02-07T11:52:35Z",
        "message": "MNT Move code quality fully to ruff (#1421)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "examples/loftq_finetuning/train_gsm8k_llama.py",
        "commit_date": "2023-12-17T14:21:25Z",
        "message": "LoftQ: edit README.md and example files (#1276)\n\n* fix when num_bits == 2 or 8\n\n* try 13b"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "examples/loftq_finetuning/train_gsm8k_llama.py",
        "commit_date": "2023-12-04T14:15:19Z",
        "message": "remove HF tokens (#1207)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "examples/loftq_finetuning/train_gsm8k_llama.py",
        "commit_date": "2023-11-29T16:08:17Z",
        "message": "Add LoftQ initialization method for LoRA (#1150)\n\n---------\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-06-25T13:47:49Z",
        "message": "stylistic update based on Codacy"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-06-08T02:14:02Z",
        "message": "Merge branch 'main' into 33b"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-05-27T08:57:03Z",
        "message": "improve error message"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-05-26T00:40:23Z",
        "message": "add 33b/65b configs"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-05-25T10:06:36Z",
        "message": "improve error message"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-05-08T10:17:13Z",
        "message": "fix saving to mulitple shards"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-04-28T09:17:06Z",
        "message": "improve format"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-04-28T09:16:10Z",
        "message": "fix bug when loading from hub"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-04-28T00:33:43Z",
        "message": "update merge scripts"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-04-27T17:13:09Z",
        "message": "update merge scripts"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-04-27T16:58:52Z",
        "message": "update merge scripts"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-04-11T16:00:10Z",
        "message": "add assertion on merging"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-04-10T08:39:42Z",
        "message": "deprecate --model_size argument"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-04-07T01:11:32Z",
        "message": "update script"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-04-07T00:58:04Z",
        "message": "preparing for 13b release"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-04-06T06:03:32Z",
        "message": "add low-ram support, especially for 13b"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-04-04T12:17:47Z",
        "message": "update print info"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-04-04T12:17:05Z",
        "message": "update print info"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-04-04T09:19:19Z",
        "message": "update merge script to support multiple shards"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-04-03T09:07:55Z",
        "message": "update merge script to support 13B model"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-03-31T01:51:17Z",
        "message": "easier merge"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "interaction.py",
        "commit_date": "2023-06-01T09:05:42Z",
        "message": "multi GPUs inference"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "interaction.py",
        "commit_date": "2023-04-01T14:17:23Z",
        "message": "update readme for chat.py"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "interaction.py",
        "commit_date": "2023-04-01T14:11:04Z",
        "message": "add better support for chat (streamly beam search,sample,greedy and beam-sample; cancel button, more prompt type and so on )"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "interaction.py",
        "commit_date": "2023-04-01T07:01:06Z",
        "message": "remove the last assistant int exemplar"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "interaction.py",
        "commit_date": "2023-03-31T15:46:51Z",
        "message": "fix exemplar"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "interaction.py",
        "commit_date": "2023-03-30T06:35:32Z",
        "message": "link you can share"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "interaction.py",
        "commit_date": "2023-03-29T12:40:37Z",
        "message": "fix instruction"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "interaction.py",
        "commit_date": "2023-03-28T01:15:11Z",
        "message": "\u6dfb\u52a0\u8fdc\u7a0blora\u52a0\u8f7d"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "interaction.py",
        "commit_date": "2023-03-27T12:33:58Z",
        "message": "\u589e\u52a0\u7b80\u5355\u7684\u4ea4\u4e92\u811a\u672c"
    },
    {
        "repo_url": "github.com/SuperDuperDB/superduperdb",
        "filepath": "superduperdb/ext/llm/model.py",
        "commit_date": "2024-03-01T17:39:07Z",
        "message": "Optimize the logic of llm infenrence"
    },
    {
        "repo_url": "github.com/SuperDuperDB/superduperdb",
        "filepath": "superduperdb/ext/llm/model.py",
        "commit_date": "2024-03-01T17:39:07Z",
        "message": "Add Checkpoint component to save checkpoint"
    },
    {
        "repo_url": "github.com/SuperDuperDB/superduperdb",
        "filepath": "superduperdb/ext/llm/model.py",
        "commit_date": "2024-03-01T17:39:07Z",
        "message": "Supports direct use of datasets for fine-tuning."
    },
    {
        "repo_url": "github.com/SuperDuperDB/superduperdb",
        "filepath": "superduperdb/ext/llm/model.py",
        "commit_date": "2024-03-01T17:39:07Z",
        "message": "Fix bugs in LLM under new model code and support new adapter checkpoint saving method"
    },
    {
        "repo_url": "github.com/SuperDuperDB/superduperdb",
        "filepath": "superduperdb/ext/llm/model.py",
        "commit_date": "2024-03-01T08:20:53Z",
        "message": "Lazy model initialization project wide"
    },
    {
        "repo_url": "github.com/SuperDuperDB/superduperdb",
        "filepath": "superduperdb/ext/llm/model.py",
        "commit_date": "2024-02-22T11:43:42Z",
        "message": "Improve _Predictor developer contract"
    },
    {
        "repo_url": "github.com/SuperDuperDB/superduperdb",
        "filepath": "superduperdb/ext/llm/model.py",
        "commit_date": "2024-02-13T14:16:15Z",
        "message": "Merge Serializer, Datatype and Document"
    },
    {
        "repo_url": "github.com/SuperDuperDB/superduperdb",
        "filepath": "superduperdb/ext/llm/model.py",
        "commit_date": "2024-02-06T10:54:43Z",
        "message": "Add integration tests for LLM finetune"
    },
    {
        "repo_url": "github.com/SuperDuperDB/superduperdb",
        "filepath": "superduperdb/ext/llm/model.py",
        "commit_date": "2024-02-02T07:27:29Z",
        "message": "Added llm finetune code comments"
    },
    {
        "repo_url": "github.com/SuperDuperDB/superduperdb",
        "filepath": "superduperdb/ext/llm/model.py",
        "commit_date": "2024-02-02T07:27:29Z",
        "message": "Fixed bug when using ray + lora + gradient_checkpointing"
    },
    {
        "repo_url": "github.com/SuperDuperDB/superduperdb",
        "filepath": "superduperdb/ext/llm/model.py",
        "commit_date": "2024-02-02T07:27:29Z",
        "message": "Support finetuning on remote ray"
    },
    {
        "repo_url": "github.com/SuperDuperDB/superduperdb",
        "filepath": "superduperdb/ext/llm/model.py",
        "commit_date": "2024-02-02T07:27:29Z",
        "message": "Support finetuning on deepspped and ray"
    },
    {
        "repo_url": "github.com/SuperDuperDB/superduperdb",
        "filepath": "superduperdb/ext/llm/model.py",
        "commit_date": "2024-01-23T03:27:24Z",
        "message": "Optimize the logic of multi-LORA loading and predicting"
    },
    {
        "repo_url": "github.com/SuperDuperDB/superduperdb",
        "filepath": "superduperdb/ext/llm/model.py",
        "commit_date": "2024-01-23T03:27:24Z",
        "message": "Delete datasets dependency"
    },
    {
        "repo_url": "github.com/SuperDuperDB/superduperdb",
        "filepath": "superduperdb/ext/llm/model.py",
        "commit_date": "2024-01-23T03:27:24Z",
        "message": "Optimize llm parameter messages"
    },
    {
        "repo_url": "github.com/SuperDuperDB/superduperdb",
        "filepath": "superduperdb/ext/llm/model.py",
        "commit_date": "2024-01-23T03:27:24Z",
        "message": "Optimize llm compatible prompt logic"
    },
    {
        "repo_url": "github.com/SuperDuperDB/superduperdb",
        "filepath": "superduperdb/ext/llm/model.py",
        "commit_date": "2024-01-23T03:27:24Z",
        "message": "Optimize metrics, validation_sets, etc. in llm training"
    },
    {
        "repo_url": "github.com/SuperDuperDB/superduperdb",
        "filepath": "superduperdb/ext/llm/model.py",
        "commit_date": "2024-01-23T03:27:24Z",
        "message": "Revert query_dataset.py"
    },
    {
        "repo_url": "github.com/SuperDuperDB/superduperdb",
        "filepath": "superduperdb/ext/llm/model.py",
        "commit_date": "2024-01-23T03:27:24Z",
        "message": "Optimize some use cases, code and comments of llm model"
    },
    {
        "repo_url": "github.com/SuperDuperDB/superduperdb",
        "filepath": "superduperdb/ext/llm/model.py",
        "commit_date": "2024-01-23T03:27:24Z",
        "message": "Support llm model based on huggingface"
    },
    {
        "repo_url": "github.com/shibing624/pycorrector",
        "filepath": "pycorrector/gpt/gpt_model.py",
        "commit_date": "2024-02-18T10:43:15Z",
        "message": "Set data type to float32"
    },
    {
        "repo_url": "github.com/shibing624/pycorrector",
        "filepath": "pycorrector/gpt/gpt_model.py",
        "commit_date": "2023-11-06T08:42:13Z",
        "message": "update model predict demo."
    },
    {
        "repo_url": "github.com/shibing624/pycorrector",
        "filepath": "pycorrector/gpt/gpt_model.py",
        "commit_date": "2023-11-02T08:38:30Z",
        "message": "update chat result."
    },
    {
        "repo_url": "github.com/shibing624/pycorrector",
        "filepath": "pycorrector/gpt/gpt_model.py",
        "commit_date": "2023-11-02T06:43:18Z",
        "message": "update infer demo."
    },
    {
        "repo_url": "github.com/shibing624/pycorrector",
        "filepath": "pycorrector/gpt/gpt_model.py",
        "commit_date": "2023-11-02T04:12:12Z",
        "message": "update infer demo."
    },
    {
        "repo_url": "github.com/shibing624/pycorrector",
        "filepath": "pycorrector/gpt/gpt_model.py",
        "commit_date": "2023-11-02T03:36:44Z",
        "message": "update test case."
    },
    {
        "repo_url": "github.com/shibing624/pycorrector",
        "filepath": "pycorrector/gpt/gpt_model.py",
        "commit_date": "2023-11-02T02:41:33Z",
        "message": "update gpt training with eval."
    },
    {
        "repo_url": "github.com/shibing624/pycorrector",
        "filepath": "pycorrector/gpt/gpt_model.py",
        "commit_date": "2023-11-01T02:18:01Z",
        "message": "update optim saved."
    },
    {
        "repo_url": "github.com/shibing624/pycorrector",
        "filepath": "pycorrector/gpt/gpt_model.py",
        "commit_date": "2023-10-31T04:51:06Z",
        "message": "update chatglm3 output layer."
    },
    {
        "repo_url": "github.com/shibing624/pycorrector",
        "filepath": "pycorrector/gpt/gpt_model.py",
        "commit_date": "2023-10-31T02:40:26Z",
        "message": "update args."
    },
    {
        "repo_url": "github.com/shibing624/pycorrector",
        "filepath": "pycorrector/gpt/gpt_model.py",
        "commit_date": "2023-10-31T02:26:44Z",
        "message": "update do sample."
    },
    {
        "repo_url": "github.com/shibing624/pycorrector",
        "filepath": "pycorrector/gpt/gpt_model.py",
        "commit_date": "2023-10-30T09:38:49Z",
        "message": "add llm model for correction."
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-04-09T21:07:59Z",
        "message": "Update export_hf_checkpoint.py (#302)\n\n* Update export_hf_checkpoint.py\n\n* Update finetune.py\n\nNew tokenizer base model for the current dev branch of transformers\n\n* Update generate.py\n\n* Update export_state_dict_checkpoint.py\n\n* Update export_hf_checkpoint.py"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-03-28T15:33:47Z",
        "message": "remove asserts"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-03-27T17:31:44Z",
        "message": "Add HF dataset loading, add linters, pyproject.toml (#175)\n\n* add HF dataset loading, add linters, pyproject.toml\n\n- applied markdownlint\n- add black, black[jupyter], isort\n- fix noqa codes\n- add .github workflow linting\n- update README.md\n\n* restore default settings\n\n* resume_from_checkpoint\n\nCo-authored-by: AngainorDev <54739135+AngainorDev@users.noreply.github.com>\n\n* Print warning on checkpoint not found\n\n* add HF dataset loading, add linters, pyproject.toml\n\n- applied markdownlint\n- add black, black[jupyter], isort\n- fix noqa codes\n- add .github workflow linting\n- update README.md\n\n* Default to local copy and update it\n\n* Typo\n\n* Remove duplicate code block\n\n---------\n\nCo-authored-by: Eric Wang <eric.james.wang@gmail.com>\nCo-authored-by: AngainorDev <54739135+AngainorDev@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-03-23T20:54:39Z",
        "message": "Remove LLaMA download code, as a precaution"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-03-18T23:42:58Z",
        "message": "fix HF export script"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-03-18T00:56:10Z",
        "message": "HF export script"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2023-07-05T08:20:51Z",
        "message": "added enable input embeddings for lora\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2023-06-29T02:03:44Z",
        "message": "fixed some changes\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2023-06-29T01:44:47Z",
        "message": "reorganized lora\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2023-06-27T09:02:32Z",
        "message": "saved word\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2023-06-25T13:39:04Z",
        "message": "rechanged autoloader\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2023-06-25T10:37:32Z",
        "message": "added lora inference file\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2023-06-25T06:27:22Z",
        "message": "fixed fp16 and device error\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2023-06-19T09:03:41Z",
        "message": "added license and fix issue416\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2023-06-09T07:42:32Z",
        "message": "removed some parts\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2023-06-08T12:16:08Z",
        "message": "modified docs\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2023-06-08T10:08:34Z",
        "message": "modified docs and files\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2023-06-08T02:30:11Z",
        "message": "updated docs\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2023-06-07T01:46:47Z",
        "message": "renamed and removed some files\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2023-06-06T12:41:46Z",
        "message": "added aquila\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2023-06-06T02:32:37Z",
        "message": "raw add for testing\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2023-03-29T00:39:38Z",
        "message": "local conflicts resolved\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2023-03-28T13:24:36Z",
        "message": "add altclip-m18\n\nSigned-off-by: zhaohu xing <920232796@qq.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2023-03-16T13:41:46Z",
        "message": " fix bug in setting for mp size >1"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2023-03-07T07:50:42Z",
        "message": "Revert \"add llama model\""
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2023-03-07T02:15:50Z",
        "message": "add llama model\n\nSigned-off-by: zhaohu xing <920232796@qq.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2023-01-13T02:19:04Z",
        "message": "enabled fp16 in AltDiffusion\n\nSigned-off-by: Anhforth <yanzaodong2021@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2022-11-26T10:40:46Z",
        "message": " download from server"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2022-11-26T10:20:27Z",
        "message": "merge changes from the simplification of AltDiffusion loading"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2022-11-26T10:15:13Z",
        "message": "simplify the loading of AltDiffusion model"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2022-11-23T10:12:46Z",
        "message": "fix device bug of AltDiffusion (#134)\n\n\nSigned-off-by: zhaohu xing <920232796@qq.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2022-11-11T08:03:16Z",
        "message": "Add AltDiffusion and AltCLIP (#90)\n\n* added diffusion model\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* fixed download path\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* added online version of diffusion\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* removed unused code\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* solved pr issue\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* solve unicode issue\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* solve unicode issue\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* fixed pr issue\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* fixed pr issue\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* fix issue\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* updated requireed packages\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updated requirement.txt\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updated import error\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updatde the import\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* seperate the cn_clip module\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updated the packages\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updatde the package versions\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updated torchvision version\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* fixed tokenizer import\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* recover change\n\n* Delete Bert.py\n\n* reformate the code\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* fix json read warning and reformat code\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updated the changes\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* remove local path\n\n* added readme file\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* fix cpm3 infer bugs\n\n* fix cpm3 model bugs\n\n* Update README.md\n\n* updated the diffusion README and commets\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* removed bmtrain install to avoid error\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* add xlmroberta\n\n* added flagstudio\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* enabled online loading\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* imporved README and changed filename\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* changed the filename\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* add readme file for AltCLIP\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* removed pdb\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* add model_dir\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* updated\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* added more requirements\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* updated pr info\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* modified readme\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* modified readme\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* added the RightBrainAI for our long image generation technology\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* updated the readme\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* fix bugs\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* tokenizer\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* Delete vocab.txt\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\nSigned-off-by: zhaohu xing <920232796@qq.com>\nCo-authored-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\nCo-authored-by: shunxing1234 <xw747777271@gmail.com>\nCo-authored-by: zhaohu xing <920232796@qq.com>\nCo-authored-by: Anhforth <yanzhaodong2021@163.com>\nCo-authored-by: Zac Liu <liuguang@baai.ac.cn>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2022-10-24T09:25:42Z",
        "message": "merged upstream\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2022-09-20T03:02:37Z",
        "message": "Fix issue#85 (#86)\n\n* updated the requirement packages list\n* tried to fix the data directory not found error\n* fixed issues in running glm_seq2seq\n* Update test_glm_seq2seq.py\n* fix glm tokenizer bug\n* add news section in Readme\n* updated docs for tokenizer\n* update required packages\n* fix_issue_85\n* fix bugs in bert forward with different shape of attention mask\n* enable offline loading\n* fixed error in test_files and add robert ch tokenizer\n* fix error in action\n* solved Filenotfounderror\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\nSigned-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>\nSigned-off-by: zhaohu xing <920232796@qq.com>\nSigned-off-by: marscrazy <liuguang@baai.ac.cn>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2022-09-06T07:17:20Z",
        "message": "fix_issue_85"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2022-07-21T13:11:52Z",
        "message": "Develop (#71)\n\n\n\n* add vit and examples\n\n* vit and examples\n\n* Update base_model.py\n\nremove unused glob\n\n* Update vit.py\n\nremove data statis\n\n* modify readme.md\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* modify readme.md\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* delete annotating code\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* Vit xzh (#25)\n\n* add vit and examples\n\n* vit and examples\n\n* Update base_model.py\n\nremove unused glob\n\n* Update vit.py\n\nremove data statis\n\n* modify readme.md\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* modify readme.md\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* delete annotating code\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\nCo-authored-by: Zac Liu <liuguang@baai.ac.cn>\n\n* env trainer\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* Create README.md\n\nfix typo in flagai introduction.\n\n* vit-checkpoint-activations\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* vit-checkpoint-activations\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\nCo-authored-by: zhaohu xing <32668889+920232796@users.noreply.github.com>\nCo-authored-by: Zhaodong Yan <94831503+Anhforth@users.noreply.github.com>\nCo-authored-by: Zac Liu <liuguang@baai.ac.cn>\nCo-authored-by: Anhforth <yanzhaodong2021@163.com>\nCo-authored-by: zhaohu xing <920232796@qq.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2022-07-15T13:59:11Z",
        "message": "Vit xzh (#25)\n\n* add vit and examples\n\n* vit and examples\n\n* Update base_model.py\n\nremove unused glob\n\n* Update vit.py\n\nremove data statis\n\n* modify readme.md\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* modify readme.md\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* delete annotating code\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\nCo-authored-by: Zac Liu <liuguang@baai.ac.cn>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2022-07-06T09:38:48Z",
        "message": "fixed tokenizer issues (#63)\n\n* Opt 30b (#16)\n* clean codes \nCo-authored-by: Zac Liu <liuguang@baai.ac.cn>\n* fix bert tokenizer issue (#18)\n* fix bert tokenizer issue\n* updated t5, opt and roberta tokenizers\n* fixed doc 404 error\nSigned-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>\n* Opt 66b (#19)\n* autoloader for opt\n* opt-66b inference\n* Update train.py\n* Load data from example dir\n* add readme of multi GPU inference\nCo-authored-by: Zac Liu <liuguang@baai.ac.cn>\n* updated release version\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n* fix tokenizer issue\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\nCo-authored-by: zhaohu xing <32668889+920232796@users.noreply.github.com>\nCo-authored-by: Zhaodong Yan <94831503+Anhforth@users.noreply.github.com>\nCo-authored-by: Zac Liu <liuguang@baai.ac.cn>\nCo-authored-by: Anhforth <yanzhaodong2021@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2022-07-06T06:19:29Z",
        "message": "Opt 66b (#19)\n\n* autoloader for opt\n* opt-66b inference\n* Update train.py\n* Load data from example dir\n* add readme of multi GPU inference\n\nCo-authored-by: Zac Liu <liuguang@baai.ac.cn>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2022-06-29T03:07:34Z",
        "message": "opt-30b (#15)"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2022-06-24T05:52:51Z",
        "message": "test merging upstream Develop to master (#5)\n\nfix bugs in distributed training & evaluating\n\nCo-authored-by: Zac Liu <liuguang@baai.ac.cn>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2022-06-23T14:16:24Z",
        "message": "Add OPT models  (#6)\n\n* added opt tokenizer and updated cpm1 tokenizer\n* fixed superglue traininng error\nCo-authored-by: Anhforth <yanzhaodong2021@163.com>\n* fixed auto_loader\n* opt model\n* add opt examples, tests\n* add opt_6.7b\n* opt-13b\n* opt-13b-en\nSigned-off-by: zhaohu xing <920232796@qq.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2022-06-19T18:42:49Z",
        "message": "Release v1.0.4 (#61)\n\n* add gradient accumulation and fix bugs in distributed evaluation\n* fix bugs in accumulate gradients in DDP / Pytorch / DeepSpeed\n* release v1.0.4\n* set epoch for distributed sampler\n* add warnings for DDP with FP16\n* change save_epoch to save_interval\n* code formate with yapf\n* fix typos in utils.py\n* del train_test.py\n* fix bugs while test seq2seq task with  rather than\n* the glm superglue need some works\nSigned-off-by: marscrazy <marscrazy_90@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/base_model.py",
        "commit_date": "2022-06-01T15:23:40Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/microsoft/LMOps",
        "filepath": "minillm/finetune.py",
        "commit_date": "2023-12-27T16:36:34Z",
        "message": "use_bf16_for_qwen"
    },
    {
        "repo_url": "github.com/microsoft/LMOps",
        "filepath": "minillm/finetune.py",
        "commit_date": "2023-11-18T15:45:46Z",
        "message": "Update dev-num to 500"
    },
    {
        "repo_url": "github.com/microsoft/LMOps",
        "filepath": "minillm/finetune.py",
        "commit_date": "2023-11-18T15:30:20Z",
        "message": "llama2 fp16 sft mp"
    },
    {
        "repo_url": "github.com/microsoft/LMOps",
        "filepath": "minillm/finetune.py",
        "commit_date": "2023-11-18T09:23:45Z",
        "message": "upgrade transformers"
    },
    {
        "repo_url": "github.com/microsoft/LMOps",
        "filepath": "minillm/finetune.py",
        "commit_date": "2023-10-21T15:47:28Z",
        "message": "lora"
    },
    {
        "repo_url": "github.com/microsoft/LMOps",
        "filepath": "minillm/finetune.py",
        "commit_date": "2023-08-16T15:50:30Z",
        "message": "fix mp=1 bug"
    },
    {
        "repo_url": "github.com/microsoft/LMOps",
        "filepath": "minillm/finetune.py",
        "commit_date": "2023-08-16T15:44:33Z",
        "message": "faster load"
    },
    {
        "repo_url": "github.com/microsoft/LMOps",
        "filepath": "minillm/finetune.py",
        "commit_date": "2023-06-14T14:05:05Z",
        "message": "fix eval"
    },
    {
        "repo_url": "github.com/microsoft/LMOps",
        "filepath": "minillm/finetune.py",
        "commit_date": "2023-06-14T11:44:35Z",
        "message": "add minillm"
    },
    {
        "repo_url": "github.com/yangjianxin1/Firefly",
        "filepath": "script/merge_lora.py",
        "commit_date": "2024-02-03T13:33:36Z",
        "message": "update train args"
    },
    {
        "repo_url": "github.com/yangjianxin1/Firefly",
        "filepath": "script/merge_lora.py",
        "commit_date": "2023-08-18T07:32:58Z",
        "message": "\u5728cpu\u4e2d\u5408\u5e76\u6743\u91cd\uff0c\u7f13\u89e3\u663e\u5b58\u4e0d\u8db3\u5bfc\u81f4\u7684\u5408\u5e76\u95ee\u9898"
    },
    {
        "repo_url": "github.com/yangjianxin1/Firefly",
        "filepath": "script/merge_lora.py",
        "commit_date": "2023-08-15T08:29:46Z",
        "message": "\u66f4\u65b0\u6743\u91cd\u5408\u5e76\u811a\u672c"
    },
    {
        "repo_url": "github.com/yangjianxin1/Firefly",
        "filepath": "script/merge_lora.py",
        "commit_date": "2023-06-26T12:02:49Z",
        "message": "\u5408\u5e76lora\u4e0ebase model\u7684\u6743\u91cd"
    },
    {
        "repo_url": "github.com/microsoft/LMOps",
        "filepath": "minillm/train_minillm.py",
        "commit_date": "2023-12-27T16:36:34Z",
        "message": "use_bf16_for_qwen"
    },
    {
        "repo_url": "github.com/microsoft/LMOps",
        "filepath": "minillm/train_minillm.py",
        "commit_date": "2023-11-28T14:01:10Z",
        "message": "HP configuration"
    },
    {
        "repo_url": "github.com/microsoft/LMOps",
        "filepath": "minillm/train_minillm.py",
        "commit_date": "2023-11-18T15:30:20Z",
        "message": "llama2 fp16 sft mp"
    },
    {
        "repo_url": "github.com/microsoft/LMOps",
        "filepath": "minillm/train_minillm.py",
        "commit_date": "2023-10-21T15:47:28Z",
        "message": "lora"
    },
    {
        "repo_url": "github.com/microsoft/LMOps",
        "filepath": "minillm/train_minillm.py",
        "commit_date": "2023-08-16T15:50:30Z",
        "message": "fix mp=1 bug"
    },
    {
        "repo_url": "github.com/microsoft/LMOps",
        "filepath": "minillm/train_minillm.py",
        "commit_date": "2023-08-16T15:44:33Z",
        "message": "faster load"
    },
    {
        "repo_url": "github.com/microsoft/LMOps",
        "filepath": "minillm/train_minillm.py",
        "commit_date": "2023-06-14T11:44:35Z",
        "message": "add minillm"
    },
    {
        "repo_url": "github.com/SCIR-HI/Huatuo-Llama-Med-Chinese",
        "filepath": "generate.py",
        "commit_date": "2023-08-07T13:46:05Z",
        "message": "Update Huozi-based model\n\nMajor update. Please try our new Huozi-based model, which is much better."
    },
    {
        "repo_url": "github.com/SCIR-HI/Huatuo-Llama-Med-Chinese",
        "filepath": "generate.py",
        "commit_date": "2023-04-01T09:37:49Z",
        "message": "init code"
    },
    {
        "repo_url": "github.com/facebookresearch/llama-recipes",
        "filepath": "src/llama_recipes/inference/model_utils.py",
        "commit_date": "2024-02-06T03:10:30Z",
        "message": "adding sdpa for flash attn"
    },
    {
        "repo_url": "github.com/facebookresearch/llama-recipes",
        "filepath": "src/llama_recipes/inference/model_utils.py",
        "commit_date": "2024-02-06T02:48:31Z",
        "message": "update the model load with native flash attn"
    },
    {
        "repo_url": "github.com/facebookresearch/llama-recipes",
        "filepath": "src/llama_recipes/inference/model_utils.py",
        "commit_date": "2023-08-30T22:59:15Z",
        "message": "Move modules into separate src folder"
    },
    {
        "repo_url": "github.com/intel-analytics/BigDL",
        "filepath": "python/llm/example/GPU/LLM-Finetuning/common/utils/util.py",
        "commit_date": "2024-01-25T11:02:38Z",
        "message": "LLM: reorganize GPU finetuning examples (#9952)"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "generate_4bit.py",
        "commit_date": "2023-06-01T09:06:31Z",
        "message": "finetune & inference & requirements for 4bit"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "examples/fp4_finetuning/finetune_fp4_opt_bnb_peft.py",
        "commit_date": "2023-11-22T11:23:50Z",
        "message": "FIX Dataset loaded twice in 4-bit finetuning script (#1164)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "examples/fp4_finetuning/finetune_fp4_opt_bnb_peft.py",
        "commit_date": "2023-06-02T08:07:46Z",
        "message": "Remove device_map when training 4,8-bit model. (#534)\n\n* Remove device_map when training 4,8-bit model.\n\n* Fix style"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "examples/fp4_finetuning/finetune_fp4_opt_bnb_peft.py",
        "commit_date": "2023-05-20T15:47:15Z",
        "message": "4-bit QLoRA via bitsandbytes (4-bit base model + LoRA) (#476)\n\n* 4bit lora\n\n* 4bit test\n\n* fixing 4bits bugs\n\n* fp4 pass variables\n\n* fix inference datatype and generation config\n\n* updating prep for int8 function to work for 4-bit\n\n* Added FP4 LoRA and FP4 fine-tuning example.\n\n* LinearFP4 -> Linear4bit\n\n* fixes\n\n* Fixed 4-bit example.\n\n* Style changes.\n\n* final changes\n\n---------\n\nCo-authored-by: Artidoro Pagnoni <pagnoni.artidoro@gmail.com>\nCo-authored-by: younesbelkada <younesbelkada@gmail.com>"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-12-27T03:18:14Z",
        "message": "Add 64k-context models (#478)\n\nFor full changes, see the latest release.\n\n---------\n\nCo-authored-by: iMountTai <2506700016@qq.com>\nCo-authored-by: Xin Yao <35353688+iMountTai@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-10-26T07:56:48Z",
        "message": "Add flash attention support for inference (#367)\n\n* Add flash attention support for inference\n\n* update: output information when using flash-attention/xformers-attention\n\n* Remove trailing whitespace\n\n---------\n\nCo-authored-by: Ziqing Yang <yangziqing@163.com>"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-10-20T06:17:08Z",
        "message": "Add Speculative sampling support (#328)\n\n* update readme\n\n* add speculative sample\n\n* update inference scripts about speculative sampling\n\n* update README.md\n\n* update readme\n\n* update readme\n\n* update readme\n\n* update readme\n\n* update HF links\n\n* Update speculative_sample.py\n\n* Update gradio_demo.py\n\n* Update README.md\n\n* Update README_EN.md\n\n* Update speculative_sample.py\n\n* Update speculative_sample.py\n\n* Update speculative_sample.py\n\n* Update speculative_sample.py\n\n* Update gradio_demo.py\n\n* fix bugs in speculative sampling\n\n---------\n\nCo-authored-by: GoGoJoestar <qazxc59419@163.com>\nCo-authored-by: GoGoJoestar <58219543+GoGoJoestar@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-09-21T03:28:33Z",
        "message": "Fix quantized inference (#302)\n\nFixed possible mismatches caused by high version dependencies."
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-08-17T01:23:31Z",
        "message": "fix inference with lora"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-08-08T01:06:08Z",
        "message": "Update gradio_demo.py"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-08-07T09:51:14Z",
        "message": "Update gradio_demo.py"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-08-07T08:43:09Z",
        "message": "add cfg sampling"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-08-04T00:51:49Z",
        "message": "Merge pull request #69 from ymcui/update-hyperparameters\n\nUpdate default decoding hyperparameters"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-08-03T09:36:00Z",
        "message": "update default hyperparameters"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-08-03T06:48:31Z",
        "message": "update system prompt label"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-08-03T04:21:26Z",
        "message": "style update"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-08-03T04:17:56Z",
        "message": "add system prompt input"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-08-02T11:08:49Z",
        "message": "Merge pull request #47 from iMountTai/main\n\nadd support for 4bit inference"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-08-02T09:30:57Z",
        "message": "Update gradio_demo.py"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-08-02T09:22:47Z",
        "message": "modify message information"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-08-02T08:49:13Z",
        "message": "Reserve load_in_8bit argument"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-08-02T07:47:07Z",
        "message": "fix bug in gradio_demo.py when launch vLLM server"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-08-02T07:03:11Z",
        "message": "add support for 4bit inference"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-08-01T09:22:41Z",
        "message": "fix spelling error: tokenzier_vocab_size -> tokenizer_vocab_size"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-08-01T09:05:14Z",
        "message": "change subprocess.call to subprocess.check_call"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-08-01T08:55:53Z",
        "message": "change subprocess call without shell=True"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-08-01T08:41:22Z",
        "message": "fix bugs"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-08-01T08:01:27Z",
        "message": "merge"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-08-01T07:34:01Z",
        "message": "add vLLM support"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-08-01T07:30:28Z",
        "message": "add arguments for setting system prompts"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/inference/gradio_demo.py",
        "commit_date": "2023-07-31T06:12:17Z",
        "message": "add scripts"
    },
    {
        "repo_url": "github.com/intel-analytics/BigDL",
        "filepath": "python/llm/example/CPU/PyTorch-Models/Model/llava/generate.py",
        "commit_date": "2023-11-02T06:48:29Z",
        "message": "add llava gpu example (#9324)\n\n* add llava gpu example\n\n* use 7b model\n\n* fix typo\n\n* add in README"
    },
    {
        "repo_url": "github.com/intel-analytics/BigDL",
        "filepath": "python/llm/example/CPU/PyTorch-Models/Model/llava/generate.py",
        "commit_date": "2023-10-27T10:59:20Z",
        "message": "add cpu example of LLaVA (#9269)\n\n* add LLaVA cpu example\n\n* Small text updates\n\n* update link\n\n---------\n\nCo-authored-by: Yuwen Hu <yuwen.hu@intel.com>"
    },
    {
        "repo_url": "github.com/intel-analytics/BigDL",
        "filepath": "python/llm/example/GPU/PyTorch-Models/Model/llava/generate.py",
        "commit_date": "2024-01-29T03:25:11Z",
        "message": "LLM: GPU Example Updates for Windows (#9992)\n\n* modify aquila\n\n* modify aquila2\n\n* add baichuan\n\n* modify baichuan2\n\n* modify blue-lm\n\n* modify chatglm3\n\n* modify chinese-llama2\n\n* modiy codellama\n\n* modify distil-whisper\n\n* modify dolly-v1\n\n* modify dolly-v2\n\n* modify falcon\n\n* modify flan-t5\n\n* modify gpt-j\n\n* modify internlm\n\n* modify llama2\n\n* modify mistral\n\n* modify mixtral\n\n* modify mpt\n\n* modify phi-1_5\n\n* modify qwen\n\n* modify qwen-vl\n\n* modify replit\n\n* modify solar\n\n* modify starcoder\n\n* modify vicuna\n\n* modify voiceassistant\n\n* modify whisper\n\n* modify yi\n\n* modify aquila2\n\n* modify baichuan\n\n* modify baichuan2\n\n* modify blue-lm\n\n* modify chatglm2\n\n* modify chatglm3\n\n* modify codellama\n\n* modify distil-whisper\n\n* modify dolly-v1\n\n* modify dolly-v2\n\n* modify flan-t5\n\n* modify llama2\n\n* modify llava\n\n* modify mistral\n\n* modify mixtral\n\n* modify phi-1_5\n\n* modify qwen-vl\n\n* modify replit\n\n* modify solar\n\n* modify starcoder\n\n* modify yi\n\n* correct the comments\n\n* remove cpu_embedding in code for whisper and distil-whisper\n\n* remove comment\n\n* remove cpu_embedding for voice assistant\n\n* revert modify voice assistant\n\n* modify for voice assistant\n\n* add comment for voice assistant\n\n* fix comments\n\n* fix comments"
    },
    {
        "repo_url": "github.com/intel-analytics/BigDL",
        "filepath": "python/llm/example/GPU/PyTorch-Models/Model/llava/generate.py",
        "commit_date": "2024-01-04T05:33:29Z",
        "message": "[LLM] IPEX auto importer set on by default (#9832)\n\n* Set BIGDL_IMPORT_IPEX default to True\n\n* Remove import intel_extension_for_pytorch as ipex from GPU example"
    },
    {
        "repo_url": "github.com/intel-analytics/BigDL",
        "filepath": "python/llm/example/GPU/PyTorch-Models/Model/llava/generate.py",
        "commit_date": "2023-12-22T08:38:24Z",
        "message": "Revert \"[LLM] IPEX auto importer turn on by default for XPU (#9730)\" (#9759)\n\nThis reverts commit aa377035affbfa57100310a1d80e62b01bd58141."
    },
    {
        "repo_url": "github.com/intel-analytics/BigDL",
        "filepath": "python/llm/example/GPU/PyTorch-Models/Model/llava/generate.py",
        "commit_date": "2023-12-22T08:20:32Z",
        "message": "[LLM] IPEX auto importer turn on by default for XPU (#9730)\n\n* Set BIGDL_IMPORT_IPEX default to true, i.e., auto import IPEX for XPU.\n* Remove import intel_extension_for_pytorch as ipex from GPU example.\n* Add support for bigdl-core-xe-21."
    },
    {
        "repo_url": "github.com/intel-analytics/BigDL",
        "filepath": "python/llm/example/GPU/PyTorch-Models/Model/llava/generate.py",
        "commit_date": "2023-11-02T06:48:29Z",
        "message": "add llava gpu example (#9324)\n\n* add llava gpu example\n\n* use 7b model\n\n* fix typo\n\n* add in README"
    },
    {
        "repo_url": "github.com/LlamaFamily/Llama-Chinese",
        "filepath": "train/merge_peft_model/merge_peft_adapter.py",
        "commit_date": "2024-01-23T11:57:23Z",
        "message": "\u8c03\u6574\u6a21\u578b\u52a0\u8f7d\u4ee3\u7801,\u4ee5\u53ca\u652f\u6301tensorrt_llm\u7684\u63a8\u7406"
    },
    {
        "repo_url": "github.com/LlamaFamily/Llama-Chinese",
        "filepath": "train/merge_peft_model/merge_peft_adapter.py",
        "commit_date": "2023-08-16T06:54:49Z",
        "message": "add merge model"
    },
    {
        "repo_url": "github.com/OpenBMB/ToolBench",
        "filepath": "toolbench/model/model_adapter.py",
        "commit_date": "2023-07-27T08:09:51Z",
        "message": "update new version"
    },
    {
        "repo_url": "github.com/OpenBMB/ToolBench",
        "filepath": "toolbench/model/model_adapter.py",
        "commit_date": "2023-06-05T09:32:51Z",
        "message": "add lora inference"
    },
    {
        "repo_url": "github.com/OpenBMB/ToolBench",
        "filepath": "toolbench/model/model_adapter.py",
        "commit_date": "2023-05-28T03:47:24Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-07-15T06:10:54Z",
        "message": "update emperical formula"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-07-13T13:11:14Z",
        "message": "refactoring code"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-07-13T04:19:21Z",
        "message": "fix potential inv_freq issue; add alpha argument"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-07-13T01:56:50Z",
        "message": "update max_position_embeddings"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-07-05T09:26:51Z",
        "message": "Merge pull request #705 from ymcui/context_extend\n\nExtend context size (8K+) without fine-tuning"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-07-05T08:39:09Z",
        "message": "replace position interploation with NTK method"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-07-03T01:37:10Z",
        "message": "add Position Interpolation for inference scripts"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-06-30T06:15:38Z",
        "message": "fix: add do_sample argument for /v1/completions and /v1/chat/completions"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-06-28T10:47:52Z",
        "message": "fix system message attribute error"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-06-25T14:36:35Z",
        "message": "Update openai_api_server.py"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-06-25T14:36:00Z",
        "message": "Update openai_api_server.py"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-06-25T14:19:38Z",
        "message": "Stylistic fixes based on Codacy suggestions"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-06-25T14:18:16Z",
        "message": "Stylistic fixes based on Codacy suggestions"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-06-25T13:48:00Z",
        "message": "Update openai_api_server.py\n\nremove unused import"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-06-25T12:56:20Z",
        "message": "stylistic update based on Codacy"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-06-19T13:28:45Z",
        "message": "chore: delete print"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-06-19T13:27:33Z",
        "message": "fix: fix template of chat completion in api demo"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-06-19T06:36:43Z",
        "message": "fix cpu inference"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-06-16T05:52:36Z",
        "message": "fix inference on cpu"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-06-15T14:14:04Z",
        "message": "fix: fix the bug in the text embedding calculation and add a pad_token if the tokenizer do not already have one"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-06-07T14:57:55Z",
        "message": "feat: add openai api demo"
    },
    {
        "repo_url": "github.com/yangjianxin1/Firefly",
        "filepath": "script/http/start_service.py",
        "commit_date": "2023-09-02T09:34:59Z",
        "message": "\u652f\u6301\u5c06\u6a21\u578b\u5c01\u88c5\u6210http\u63a5\u53e3"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "examples/lora_dreambooth/convert_peft_sd_lora_to_kohya_ss.py",
        "commit_date": "2023-06-27T08:26:54Z",
        "message": "[Bugfix] Inserted adapter_name to get_peft_model_state_dict function (#626)\n\n* Update train_dreambooth.py\n\nAccelerator init updated from logging_dir to project_dir. Newer versions of accelerate uses project_dir. logging_dir is deprecated\n\n* Bugfix: Adapter name variable inserted, when changing LORA_ADAPTER_NAME it causes error\n\n* Adapter name added as kwarg\n\n* Black code formatted\n\n* Style & Quality check"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "examples/lora_dreambooth/convert_peft_sd_lora_to_kohya_ss.py",
        "commit_date": "2023-06-21T14:04:39Z",
        "message": "Added Civitai LoRAs conversion to PEFT, PEFT LoRAs conversion to webui (#596)\n\n* Fixed kohya_ss to peft lora conversion, added script for backward conversion\n\n* Fixed getting alpha from PEFT\n\n---------\n\nCo-authored-by: Alexander Kovalchuk <a.kovalchuk@prequelapp.com>"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-03-03T07:21:44Z",
        "message": "Fix Gemma activation function (#214)\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* save\n\n* trainer\n\n* spaces\n\n* original\n\n* Gemma\n\n* Update pyproject.toml\n\n* Update mapper.py\n\n* Update fast_lora.py\n\n* FastGemmaModel\n\n* model_type\n\n* Update llama.py\n\n* Update llama.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update cross_entropy_loss.py\n\n* Update llama.py\n\n* Update llama.py\n\n* gemma\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Fast CE Loss\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* CE\n\n* Update llama.py\n\n* Update llama.py\n\n* Update cross_entropy_loss.py\n\n* Update geglu.py\n\n* Update cross_entropy_loss.py\n\n* revert\n\n* Update llama.py\n\n* Update llama.py\n\n* norm\n\n* Update gemma.py\n\n* Update gemma.py\n\n* position_ids\n\n* Update gemma.py\n\n* Update gemma.py\n\n* pos\n\n* Update llama.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update cross_entropy_loss.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* revert\n\n* revert\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update cross_entropy_loss.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* rope\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* llama\n\n* Update llama.py\n\n* gemma\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update save.py\n\n* RoPE\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update gemma.py\n\n* correct_dtype\n\n* Update gemma.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Chat Templates\n\n* Update README.md\n\n* Update README.md\n\n* Update llama.py\n\n* DoRA\n\n* Update _utils.py\n\n* Update chat_templates.py\n\n* Update llama.py\n\n* Hotfix - fix DoRA, Gemma prompt template (#202) (#203)\n\n* Update save.py\n\n* saving\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update __init__.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* save\n\n* trainer\n\n* spaces\n\n* original\n\n* Gemma\n\n* Update pyproject.toml\n\n* Update mapper.py\n\n* Update fast_lora.py\n\n* FastGemmaModel\n\n* model_type\n\n* Update llama.py\n\n* Update llama.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update cross_entropy_loss.py\n\n* Update llama.py\n\n* Update llama.py\n\n* gemma\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Fast CE Loss\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* CE\n\n* Update llama.py\n\n* Update llama.py\n\n* Update cross_entropy_loss.py\n\n* Update geglu.py\n\n* Update cross_entropy_loss.py\n\n* revert\n\n* Update llama.py\n\n* Update llama.py\n\n* norm\n\n* Update gemma.py\n\n* Update gemma.py\n\n* position_ids\n\n* Update gemma.py\n\n* Update gemma.py\n\n* pos\n\n* Update llama.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update cross_entropy_loss.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* revert\n\n* revert\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update cross_entropy_loss.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* rope\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* llama\n\n* Update llama.py\n\n* gemma\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update save.py\n\n* RoPE\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update gemma.py\n\n* correct_dtype\n\n* Update gemma.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Chat Templates\n\n* Update README.md\n\n* Update README.md\n\n* Update llama.py\n\n* DoRA\n\n* Update _utils.py\n\n* Update chat_templates.py\n\n* Update pyproject.toml\n\n* Small fixes\n\n* Update pyproject.toml\n\n* Approx gelu\n\n* Update geglu.py\n\n* Approx gelu\n\n* Update llama.py\n\n* Update __init__.py\n\n* Update __init__.py\n\n* Update _utils.py\n\n* Update geglu.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-02-28T13:18:38Z",
        "message": "Nightly (#204)\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update __init__.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* save\n\n* trainer\n\n* spaces\n\n* original\n\n* Gemma\n\n* Update pyproject.toml\n\n* Update mapper.py\n\n* Update fast_lora.py\n\n* FastGemmaModel\n\n* model_type\n\n* Update llama.py\n\n* Update llama.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update cross_entropy_loss.py\n\n* Update llama.py\n\n* Update llama.py\n\n* gemma\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Fast CE Loss\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* CE\n\n* Update llama.py\n\n* Update llama.py\n\n* Update cross_entropy_loss.py\n\n* Update geglu.py\n\n* Update cross_entropy_loss.py\n\n* revert\n\n* Update llama.py\n\n* Update llama.py\n\n* norm\n\n* Update gemma.py\n\n* Update gemma.py\n\n* position_ids\n\n* Update gemma.py\n\n* Update gemma.py\n\n* pos\n\n* Update llama.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update cross_entropy_loss.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* revert\n\n* revert\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update cross_entropy_loss.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* rope\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* llama\n\n* Update llama.py\n\n* gemma\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update save.py\n\n* RoPE\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update gemma.py\n\n* correct_dtype\n\n* Update gemma.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Chat Templates\n\n* Update README.md\n\n* Update README.md\n\n* Update llama.py\n\n* DoRA\n\n* Update _utils.py\n\n* Update chat_templates.py\n\n* Update llama.py\n\n* Hotfix - fix DoRA, Gemma prompt template (#202) (#203)\n\n* Update save.py\n\n* saving\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update __init__.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* save\n\n* trainer\n\n* spaces\n\n* original\n\n* Gemma\n\n* Update pyproject.toml\n\n* Update mapper.py\n\n* Update fast_lora.py\n\n* FastGemmaModel\n\n* model_type\n\n* Update llama.py\n\n* Update llama.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update cross_entropy_loss.py\n\n* Update llama.py\n\n* Update llama.py\n\n* gemma\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Fast CE Loss\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* CE\n\n* Update llama.py\n\n* Update llama.py\n\n* Update cross_entropy_loss.py\n\n* Update geglu.py\n\n* Update cross_entropy_loss.py\n\n* revert\n\n* Update llama.py\n\n* Update llama.py\n\n* norm\n\n* Update gemma.py\n\n* Update gemma.py\n\n* position_ids\n\n* Update gemma.py\n\n* Update gemma.py\n\n* pos\n\n* Update llama.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update cross_entropy_loss.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* revert\n\n* revert\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update cross_entropy_loss.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* rope\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* llama\n\n* Update llama.py\n\n* gemma\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update save.py\n\n* RoPE\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update gemma.py\n\n* correct_dtype\n\n* Update gemma.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Chat Templates\n\n* Update README.md\n\n* Update README.md\n\n* Update llama.py\n\n* DoRA\n\n* Update _utils.py\n\n* Update chat_templates.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-02-26T14:42:10Z",
        "message": "2.4x faster Gemma (#197)\n\n* Update save.py\n\n* Update save.py\n\n* linking\n\n* llama.cpp bugs\n\n* Update save.py\n\n* Update save.py\n\n* saving\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update __init__.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* save\n\n* trainer\n\n* spaces\n\n* original\n\n* Gemma\n\n* Update pyproject.toml\n\n* Update mapper.py\n\n* Update fast_lora.py\n\n* FastGemmaModel\n\n* model_type\n\n* Update llama.py\n\n* Update llama.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update cross_entropy_loss.py\n\n* Update llama.py\n\n* Update llama.py\n\n* gemma\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Fast CE Loss\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* CE\n\n* Update llama.py\n\n* Update llama.py\n\n* Update cross_entropy_loss.py\n\n* Update geglu.py\n\n* Update cross_entropy_loss.py\n\n* revert\n\n* Update llama.py\n\n* Update llama.py\n\n* norm\n\n* Update gemma.py\n\n* Update gemma.py\n\n* position_ids\n\n* Update gemma.py\n\n* Update gemma.py\n\n* pos\n\n* Update llama.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update cross_entropy_loss.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* revert\n\n* revert\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update cross_entropy_loss.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* rope\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* llama\n\n* Update llama.py\n\n* gemma\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update save.py\n\n* RoPE\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update gemma.py\n\n* correct_dtype\n\n* Update gemma.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Chat Templates\n\n* Update README.md\n\n* Update README.md"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-02-20T16:58:59Z",
        "message": "Feb 2024 Release (#187)\n\n* Fast inference repatch\n\n* Update llama.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update mistral.py\n\n* Update __init__.py\n\n* Fix inference\n\n* Update mistral.py\n\n* fast lm_head\n\n* Remove fast path\n\n* Update rope_embedding.py\n\n* Update loader.py\n\n* LlamaAttention_fast_forward_inference\n\n* if past_key_value is not None and q_len == 1:\n\n* revert inference\n\n* Update loader.py\n\n* past_key_value\n\n* Update llama.py\n\n* Update llama.py\n\n* Fix SDPA\n\n* Update llama.py\n\n* padding\n\n* Inference\n\n* Update llama.py\n\n* Revert\n\n* Update mistral.py\n\n* faster inference\n\n* inference\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* inference\n\n* Update llama.py\n\n* Update utils.py\n\n* faster inference\n\n* Update llama.py\n\n* revert\n\n* lm_head\n\n* Update llama.py\n\n* inference\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* faster inference\n\n* Update llama.py\n\n* fast inference\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* torch compile\n\n* past_key_values\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update llama.py\n\n* fast inference + saving config.json\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* fast inference again\n\n* more temp matrices\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* fast inference\n\n* Update mistral.py\n\n* Update llama.py\n\n* SDPA\n\n* attention_mask\n\n* New version\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update save.py\n\n* Update save.py\n\n* Torch 2.2.0\n\n* Update save.py\n\n* mistral swa\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Fix SWA inference\n\n* Fix llm_int8_skip_modules\n\n* SWA inference\n\n* Update save.py\n\n* Update save.py\n\n* Update pyproject.toml\n\n* __version__\n\n* __version__\n\n* Update save.py\n\n* Update save.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Chat Templates\n\n* Update chat_templates.py\n\n* Update chat_templates.py\n\n* Update chat_templates.py\n\n* Update chat_templates.py\n\n* patch tokenizer\n\n* Update chat_templates.py\n\n* Saving, LlamaRotaryEmbedding issues\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update mapper.py\n\n* Fix RoPE precision issues\n\n* Bugs\n\n* saving bugs\n\n* Update llama.py\n\n* readme\n\n* spaces\n\n* spaces\n\n* globals\n\n* slash\n\n* slashes\n\n* spaces\n\n* apache\n\n* Update save.py\n\n* Update save.py\n\n* Update loader.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* trainer\n\n* Update save.py\n\n* Update pyproject.toml\n\n* install\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* PeftModel token + saving\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* linking\n\n* llama.cpp bugs\n\n* Update save.py\n\n* Update save.py\n\n* saving\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update __init__.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* save\n\n* trainer\n\n* spaces\n\n* original"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-02-14T13:07:42Z",
        "message": "Prelim Feb release (#173)\n\n* Works?\n\n* Update pyproject.toml\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Swiglu\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* attention_mask\n\n* Update llama.py\n\n* Update llama.py\n\n* labels\n\n* Update mistral.py\n\n* Update llama.py\n\n* attention mask\n\n* Update save.py\n\n* Update save.py\n\n* Update mistral.py\n\n* attention mask\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update dpo.py\n\n* Patch saving\n\n* Update save.py\n\n* Update save.py\n\n* patch_saving_functions\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* print\n\n* Mistral patch\n\n* Update mistral.py\n\n* Update save.py\n\n* saving\n\n* Update llama.py\n\n* Update llama.py\n\n* Fast inference repatch\n\n* Update llama.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update mistral.py\n\n* Update __init__.py\n\n* Fix inference\n\n* Update mistral.py\n\n* fast lm_head\n\n* Remove fast path\n\n* Update rope_embedding.py\n\n* Update loader.py\n\n* LlamaAttention_fast_forward_inference\n\n* if past_key_value is not None and q_len == 1:\n\n* revert inference\n\n* Update loader.py\n\n* past_key_value\n\n* Update llama.py\n\n* Update llama.py\n\n* Fix SDPA\n\n* Update llama.py\n\n* padding\n\n* Inference\n\n* Update llama.py\n\n* Revert\n\n* Update mistral.py\n\n* faster inference\n\n* inference\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* inference\n\n* Update llama.py\n\n* Update utils.py\n\n* faster inference\n\n* Update llama.py\n\n* revert\n\n* lm_head\n\n* Update llama.py\n\n* inference\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* faster inference\n\n* Update llama.py\n\n* fast inference\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* torch compile\n\n* past_key_values\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update llama.py\n\n* fast inference + saving config.json\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* fast inference again\n\n* more temp matrices\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* fast inference\n\n* Update mistral.py\n\n* Update llama.py\n\n* SDPA\n\n* attention_mask\n\n* New version\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update save.py\n\n* Update save.py\n\n* Torch 2.2.0\n\n* Update save.py\n\n* mistral swa\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Fix SWA inference\n\n* Fix llm_int8_skip_modules\n\n* SWA inference\n\n* Update save.py\n\n* Update save.py\n\n* Update pyproject.toml\n\n* __version__\n\n* __version__\n\n* Update save.py\n\n* Update save.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Chat Templates\n\n* Update chat_templates.py\n\n* Update chat_templates.py\n\n* Update chat_templates.py\n\n* Update chat_templates.py\n\n* patch tokenizer\n\n* Update chat_templates.py\n\n* Saving, LlamaRotaryEmbedding issues\n\n* Update llama.py\n\n* Update mistral.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-02-07T16:40:28Z",
        "message": "Nightly (#161)\n\n* Update fast_lora.py\n\n* Update utils.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update save.py\n\n* Update save.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Revert \"Update llama.py\"\n\nThis reverts commit a208ec46e012cf470ecefe6268a66358215df7b6.\n\n* Update llama.py\n\n* Works?\n\n* Update pyproject.toml\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Swiglu\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* attention_mask\n\n* Update llama.py\n\n* Update llama.py\n\n* labels\n\n* Update mistral.py\n\n* Update llama.py\n\n* attention mask\n\n* Update save.py\n\n* Update save.py\n\n* Update mistral.py\n\n* attention mask\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update dpo.py\n\n* Patch saving\n\n* Update save.py\n\n* Update save.py\n\n* patch_saving_functions\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* print\n\n* Mistral patch\n\n* Update mistral.py\n\n* Update save.py\n\n* saving\n\n* Update llama.py\n\n* Update llama.py\n\n* Fast inference repatch\n\n* Update llama.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update mistral.py\n\n* Update __init__.py\n\n* Fix inference\n\n* Update mistral.py\n\n* fast lm_head\n\n* Remove fast path\n\n* Update rope_embedding.py\n\n* Update loader.py\n\n* LlamaAttention_fast_forward_inference\n\n* if past_key_value is not None and q_len == 1:\n\n* revert inference\n\n* Update loader.py\n\n* past_key_value\n\n* Update llama.py\n\n* Update llama.py\n\n* Fix SDPA\n\n* Update llama.py\n\n* padding\n\n* Inference\n\n* Update llama.py\n\n* Revert\n\n* Update mistral.py\n\n* faster inference\n\n* inference\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* inference\n\n* Update llama.py\n\n* Update utils.py\n\n* faster inference\n\n* Update llama.py\n\n* revert\n\n* lm_head\n\n* Update llama.py\n\n* inference\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* faster inference\n\n* Update llama.py\n\n* fast inference\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* torch compile\n\n* past_key_values\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update llama.py\n\n* fast inference + saving config.json\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* fast inference again\n\n* more temp matrices\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* fast inference\n\n* Update mistral.py\n\n* Update llama.py\n\n* SDPA\n\n* attention_mask\n\n* New version\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update save.py\n\n* Update save.py\n\n* Torch 2.2.0\n\n* Update save.py\n\n* mistral swa\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Fix SWA inference\n\n* Fix llm_int8_skip_modules\n\n* SWA inference\n\n* Update save.py\n\n* Update save.py\n\n* Update pyproject.toml\n\n* __version__\n\n* __version__\n\n* Update save.py\n\n* Update save.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-02-06T17:40:50Z",
        "message": "Torch 2.2 (#157)\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update save.py\n\n* Update fast_lora.py\n\n* Update utils.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update save.py\n\n* Update save.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Revert \"Update llama.py\"\n\nThis reverts commit a208ec46e012cf470ecefe6268a66358215df7b6.\n\n* Update llama.py\n\n* Works?\n\n* Update pyproject.toml\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Swiglu\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* attention_mask\n\n* Update llama.py\n\n* Update llama.py\n\n* labels\n\n* Update mistral.py\n\n* Update llama.py\n\n* attention mask\n\n* Update save.py\n\n* Update save.py\n\n* Update mistral.py\n\n* attention mask\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update dpo.py\n\n* Patch saving\n\n* Update save.py\n\n* Update save.py\n\n* patch_saving_functions\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* print\n\n* Mistral patch\n\n* Update mistral.py\n\n* Update save.py\n\n* saving\n\n* Update llama.py\n\n* Update llama.py\n\n* Fast inference repatch\n\n* Update llama.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update mistral.py\n\n* Update __init__.py\n\n* Fix inference\n\n* Update mistral.py\n\n* fast lm_head\n\n* Remove fast path\n\n* Update rope_embedding.py\n\n* Update loader.py\n\n* LlamaAttention_fast_forward_inference\n\n* if past_key_value is not None and q_len == 1:\n\n* revert inference\n\n* Update loader.py\n\n* past_key_value\n\n* Update llama.py\n\n* Update llama.py\n\n* Fix SDPA\n\n* Update llama.py\n\n* padding\n\n* Inference\n\n* Update llama.py\n\n* Revert\n\n* Update mistral.py\n\n* faster inference\n\n* inference\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* inference\n\n* Update llama.py\n\n* Update utils.py\n\n* faster inference\n\n* Update llama.py\n\n* revert\n\n* lm_head\n\n* Update llama.py\n\n* inference\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* faster inference\n\n* Update llama.py\n\n* fast inference\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* torch compile\n\n* past_key_values\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update llama.py\n\n* fast inference + saving config.json\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* fast inference again\n\n* more temp matrices\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* fast inference\n\n* Update mistral.py\n\n* Update llama.py\n\n* SDPA\n\n* attention_mask\n\n* New version\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update save.py\n\n* Update save.py\n\n* Torch 2.2.0\n\n* Update save.py\n\n* mistral swa\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Fix SWA inference\n\n* Fix llm_int8_skip_modules\n\n* SWA inference\n\n* Update save.py\n\n* Update save.py\n\n* Update pyproject.toml\n\n* __version__\n\n* __version__\n\n* Update save.py\n\n* Update save.py\n\n* Update mistral.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-02-04T06:35:56Z",
        "message": "2x faster inference (#151)\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update save.py\n\n* Update fast_lora.py\n\n* Update utils.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update save.py\n\n* Update save.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Revert \"Update llama.py\"\n\nThis reverts commit a208ec46e012cf470ecefe6268a66358215df7b6.\n\n* Update llama.py\n\n* Works?\n\n* Update pyproject.toml\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Swiglu\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* attention_mask\n\n* Update llama.py\n\n* Update llama.py\n\n* labels\n\n* Update mistral.py\n\n* Update llama.py\n\n* attention mask\n\n* Update save.py\n\n* Update save.py\n\n* Update mistral.py\n\n* attention mask\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update dpo.py\n\n* Patch saving\n\n* Update save.py\n\n* Update save.py\n\n* patch_saving_functions\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* print\n\n* Mistral patch\n\n* Update mistral.py\n\n* Update save.py\n\n* saving\n\n* Update llama.py\n\n* Update llama.py\n\n* Fast inference repatch\n\n* Update llama.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update mistral.py\n\n* Update __init__.py\n\n* Fix inference\n\n* Update mistral.py\n\n* fast lm_head\n\n* Remove fast path\n\n* Update rope_embedding.py\n\n* Update loader.py\n\n* LlamaAttention_fast_forward_inference\n\n* if past_key_value is not None and q_len == 1:\n\n* revert inference\n\n* Update loader.py\n\n* past_key_value\n\n* Update llama.py\n\n* Update llama.py\n\n* Fix SDPA\n\n* Update llama.py\n\n* padding\n\n* Inference\n\n* Update llama.py\n\n* Revert\n\n* Update mistral.py\n\n* faster inference\n\n* inference\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* inference\n\n* Update llama.py\n\n* Update utils.py\n\n* faster inference\n\n* Update llama.py\n\n* revert\n\n* lm_head\n\n* Update llama.py\n\n* inference\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* faster inference\n\n* Update llama.py\n\n* fast inference\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* torch compile\n\n* past_key_values\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update llama.py\n\n* fast inference + saving config.json\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* fast inference again\n\n* more temp matrices\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* fast inference\n\n* Update mistral.py\n\n* Update llama.py\n\n* SDPA\n\n* attention_mask\n\n* New version\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update utils.py\n\n* Update utils.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-01-30T17:03:37Z",
        "message": "Hotfix - fix inference (#146)\n\n* faster saving & inference\n\n* Update llama.py\n\n* Update save.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* fast inference\n\n* Update llama.py\n\n* Update save.py\n\n* Update llama.py\n\n* Mistral correct RoPE scaling\n\n* Max sequence lengths\n\n* Apache 2\n\n* fast_linear_forward\n\n* Update utils.py\n\n* Update utils.py\n\n* No print\n\n* Update utils.py\n\n* Update utils.py\n\n* inference\n\n* Update llama.py\n\n* Fast inference RoPE\n\n* Update llama.py\n\n* Update llama.py\n\n* RoPE\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* LoRA\n\n* Fast LoRA saving\n\n* Update llama.py\n\n* hidden_states\n\n* q_len == 1\n\n* q_len issue\n\n* Update mistral.py\n\n* Update mistral.py\n\n* incorrect inference\n\n* Update to transformers 4.37\n\n* Graceful FA2 error + torch 2.1.1\n\n* Update mapper.py\n\n* Update pyproject.toml\n\n* Fix saving and bnb-4bit\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* remove patching\n\n* Update llama.py\n\n* Update llama.py\n\n* Update swiglu.py\n\n* Repatch\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update save.py\n\n* Update fast_lora.py\n\n* Update utils.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update save.py\n\n* Update save.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Revert \"Update llama.py\"\n\nThis reverts commit a208ec46e012cf470ecefe6268a66358215df7b6.\n\n* Update llama.py\n\n* Works?\n\n* Update pyproject.toml\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Swiglu\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* attention_mask\n\n* Update llama.py\n\n* Update llama.py\n\n* labels\n\n* Update mistral.py\n\n* Update llama.py\n\n* attention mask\n\n* Update save.py\n\n* Update save.py\n\n* Update mistral.py\n\n* attention mask\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update dpo.py\n\n* Patch saving\n\n* Update save.py\n\n* Update save.py\n\n* patch_saving_functions\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* print\n\n* Mistral patch\n\n* Update mistral.py\n\n* Update save.py\n\n* saving\n\n* Update llama.py\n\n* Update llama.py\n\n* Fast inference repatch\n\n* Update llama.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update mistral.py\n\n* Update __init__.py\n\n* Fix inference\n\n* Update mistral.py\n\n* fast lm_head\n\n* Remove fast path\n\n* Update rope_embedding.py\n\n* Update loader.py\n\n* LlamaAttention_fast_forward_inference\n\n* if past_key_value is not None and q_len == 1:\n\n* revert inference\n\n* Update loader.py\n\n* past_key_value"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-01-29T06:49:54Z",
        "message": "Fix inference attention mask (#142)\n\n* faster saving & inference\n\n* Update llama.py\n\n* Update save.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* fast inference\n\n* Update llama.py\n\n* Update save.py\n\n* Update llama.py\n\n* Mistral correct RoPE scaling\n\n* Max sequence lengths\n\n* Apache 2\n\n* fast_linear_forward\n\n* Update utils.py\n\n* Update utils.py\n\n* No print\n\n* Update utils.py\n\n* Update utils.py\n\n* inference\n\n* Update llama.py\n\n* Fast inference RoPE\n\n* Update llama.py\n\n* Update llama.py\n\n* RoPE\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* LoRA\n\n* Fast LoRA saving\n\n* Update llama.py\n\n* hidden_states\n\n* q_len == 1\n\n* q_len issue\n\n* Update mistral.py\n\n* Update mistral.py\n\n* incorrect inference\n\n* Update to transformers 4.37\n\n* Graceful FA2 error + torch 2.1.1\n\n* Update mapper.py\n\n* Update pyproject.toml\n\n* Fix saving and bnb-4bit\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* remove patching\n\n* Update llama.py\n\n* Update llama.py\n\n* Update swiglu.py\n\n* Repatch\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update save.py\n\n* Update fast_lora.py\n\n* Update utils.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update save.py\n\n* Update save.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Revert \"Update llama.py\"\n\nThis reverts commit a208ec46e012cf470ecefe6268a66358215df7b6.\n\n* Update llama.py\n\n* Works?\n\n* Update pyproject.toml\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Swiglu\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* attention_mask\n\n* Update llama.py\n\n* Update llama.py\n\n* labels\n\n* Update mistral.py\n\n* Update llama.py\n\n* attention mask\n\n* Update save.py\n\n* Update save.py\n\n* Update mistral.py\n\n* attention mask\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update dpo.py\n\n* Patch saving\n\n* Update save.py\n\n* Update save.py\n\n* patch_saving_functions\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* print\n\n* Mistral patch\n\n* Update mistral.py\n\n* Update save.py\n\n* saving\n\n* Update llama.py\n\n* Update llama.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-01-28T15:52:39Z",
        "message": "Fix saving issues (#139)\n\n* faster saving & inference\n\n* Update llama.py\n\n* Update save.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* fast inference\n\n* Update llama.py\n\n* Update save.py\n\n* Update llama.py\n\n* Mistral correct RoPE scaling\n\n* Max sequence lengths\n\n* Apache 2\n\n* fast_linear_forward\n\n* Update utils.py\n\n* Update utils.py\n\n* No print\n\n* Update utils.py\n\n* Update utils.py\n\n* inference\n\n* Update llama.py\n\n* Fast inference RoPE\n\n* Update llama.py\n\n* Update llama.py\n\n* RoPE\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* LoRA\n\n* Fast LoRA saving\n\n* Update llama.py\n\n* hidden_states\n\n* q_len == 1\n\n* q_len issue\n\n* Update mistral.py\n\n* Update mistral.py\n\n* incorrect inference\n\n* Update to transformers 4.37\n\n* Graceful FA2 error + torch 2.1.1\n\n* Update mapper.py\n\n* Update pyproject.toml\n\n* Fix saving and bnb-4bit\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* remove patching\n\n* Update llama.py\n\n* Update llama.py\n\n* Update swiglu.py\n\n* Repatch\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update save.py\n\n* Update fast_lora.py\n\n* Update utils.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update save.py\n\n* Update save.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Revert \"Update llama.py\"\n\nThis reverts commit a208ec46e012cf470ecefe6268a66358215df7b6.\n\n* Update llama.py\n\n* Works?\n\n* Update pyproject.toml\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Swiglu\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* attention_mask\n\n* Update llama.py\n\n* Update llama.py\n\n* labels\n\n* Update mistral.py\n\n* Update llama.py\n\n* attention mask\n\n* Update save.py\n\n* Update save.py\n\n* Update mistral.py\n\n* attention mask\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update dpo.py\n\n* Patch saving\n\n* Update save.py\n\n* Update save.py\n\n* patch_saving_functions\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* print"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-01-26T17:50:22Z",
        "message": "Inference bug fix (#134)\n\n* faster saving & inference\n\n* Update llama.py\n\n* Update save.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* fast inference\n\n* Update llama.py\n\n* Update save.py\n\n* Update llama.py\n\n* Mistral correct RoPE scaling\n\n* Max sequence lengths\n\n* Apache 2\n\n* fast_linear_forward\n\n* Update utils.py\n\n* Update utils.py\n\n* No print\n\n* Update utils.py\n\n* Update utils.py\n\n* inference\n\n* Update llama.py\n\n* Fast inference RoPE\n\n* Update llama.py\n\n* Update llama.py\n\n* RoPE\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* LoRA\n\n* Fast LoRA saving\n\n* Update llama.py\n\n* hidden_states\n\n* q_len == 1\n\n* q_len issue\n\n* Update mistral.py\n\n* Update mistral.py\n\n* incorrect inference\n\n* Update to transformers 4.37\n\n* Graceful FA2 error + torch 2.1.1\n\n* Update mapper.py\n\n* Update pyproject.toml\n\n* Fix saving and bnb-4bit\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* remove patching\n\n* Update llama.py\n\n* Update llama.py\n\n* Update swiglu.py\n\n* Repatch\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update save.py\n\n* Update fast_lora.py\n\n* Update utils.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update save.py\n\n* Update save.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Revert \"Update llama.py\"\n\nThis reverts commit a208ec46e012cf470ecefe6268a66358215df7b6.\n\n* Update llama.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-01-26T17:47:54Z",
        "message": "More bug fixes (#133)\n\n* faster saving & inference\n\n* Update llama.py\n\n* Update save.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* fast inference\n\n* Update llama.py\n\n* Update save.py\n\n* Update llama.py\n\n* Mistral correct RoPE scaling\n\n* Max sequence lengths\n\n* Apache 2\n\n* fast_linear_forward\n\n* Update utils.py\n\n* Update utils.py\n\n* No print\n\n* Update utils.py\n\n* Update utils.py\n\n* inference\n\n* Update llama.py\n\n* Fast inference RoPE\n\n* Update llama.py\n\n* Update llama.py\n\n* RoPE\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* LoRA\n\n* Fast LoRA saving\n\n* Update llama.py\n\n* hidden_states\n\n* q_len == 1\n\n* q_len issue\n\n* Update mistral.py\n\n* Update mistral.py\n\n* incorrect inference\n\n* Update to transformers 4.37\n\n* Graceful FA2 error + torch 2.1.1\n\n* Update mapper.py\n\n* Update pyproject.toml\n\n* Fix saving and bnb-4bit\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* remove patching\n\n* Update llama.py\n\n* Update llama.py\n\n* Update swiglu.py\n\n* Repatch\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update save.py\n\n* Update fast_lora.py\n\n* Update utils.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update save.py\n\n* Update save.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-01-25T17:19:17Z",
        "message": "Fix bugs (#129)\n\n* faster saving & inference\n\n* Update llama.py\n\n* Update save.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* fast inference\n\n* Update llama.py\n\n* Update save.py\n\n* Update llama.py\n\n* Mistral correct RoPE scaling\n\n* Max sequence lengths\n\n* Apache 2\n\n* fast_linear_forward\n\n* Update utils.py\n\n* Update utils.py\n\n* No print\n\n* Update utils.py\n\n* Update utils.py\n\n* inference\n\n* Update llama.py\n\n* Fast inference RoPE\n\n* Update llama.py\n\n* Update llama.py\n\n* RoPE\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* LoRA\n\n* Fast LoRA saving\n\n* Update llama.py\n\n* hidden_states\n\n* q_len == 1\n\n* q_len issue\n\n* Update mistral.py\n\n* Update mistral.py\n\n* incorrect inference\n\n* Update to transformers 4.37\n\n* Graceful FA2 error + torch 2.1.1\n\n* Update mapper.py\n\n* Update pyproject.toml\n\n* Fix saving and bnb-4bit\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* remove patching\n\n* Update llama.py\n\n* Update llama.py\n\n* Update swiglu.py\n\n* Repatch\n\n* Update fast_lora.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-01-22T16:55:24Z",
        "message": "2-4x faster native HF inference (#119)\n\n* faster saving & inference\n\n* Update llama.py\n\n* Update save.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* fast inference\n\n* Update llama.py\n\n* Update save.py\n\n* Update llama.py\n\n* Mistral correct RoPE scaling\n\n* Max sequence lengths\n\n* Apache 2\n\n* fast_linear_forward\n\n* Update utils.py\n\n* Update utils.py\n\n* No print\n\n* Update utils.py\n\n* Update utils.py\n\n* inference\n\n* Update llama.py\n\n* Fast inference RoPE\n\n* Update llama.py\n\n* Update llama.py\n\n* RoPE\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* LoRA\n\n* Fast LoRA saving"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-01-21T11:20:22Z",
        "message": "Hotfix (#118)\n\n* faster saving & inference\n\n* Update llama.py\n\n* Update save.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-01-20T12:23:00Z",
        "message": "Hotfix for Jan 2024 Release (#110)\n\n* Fix tokenizer, dropout, bias for LoRA\n\n* Update loader.py\n\n* Fix LoRA downcasting\n\n* Update _utils.py\n\n* Saving to GGUF\n\n* fix\n\n* colab_quantize_to_gguf\n\n* move save modules\n\n* save module\n\n* Update __init__.py\n\n* Update save.py\n\n* Temp downgrade due to TRL issue\n\n* Fix up bugs\n\n* Faster saving + other changes\n\n* Update llama.py\n\n* Saving modules\n\n* spelling\n\n* Update llama.py\n\n* Update save.py\n\n* Update save.py\n\n* Update loader.py\n\n* Update llama.py\n\n* patch saving\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* patch saving\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* original_model\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* saving to RAM leakage?\n\n* Update save.py\n\n* new_save_directory\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update pyproject.toml\n\n* Update pyproject.toml\n\n* Update pyproject.toml\n\n* Quick fixes\n\n* Update llama.py\n\n* Update llama.py\n\n* Update dpo.py\n\n* Update dpo.py\n\n* Update llama.py\n\n* Update save.py\n\n* getattr\n\n* RSLoRA and LoftQ direct support\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Fix DPO + GGUF\n\n* Fix quantization_method\n\n* Fix quantization_config\n\n* patch model\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update save.py\n\n* Update save.py\n\n* tokenizer_save_settings\n\n* Update save.py\n\n* quantization and loftq\n\n* Update save.py\n\n* Update llama.py\n\n* Update save.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-01-19T17:25:06Z",
        "message": "Quick fixes (#106)\n\n* Fix tokenizer, dropout, bias for LoRA\n\n* Update loader.py\n\n* Fix LoRA downcasting\n\n* Update _utils.py\n\n* Saving to GGUF\n\n* fix\n\n* colab_quantize_to_gguf\n\n* move save modules\n\n* save module\n\n* Update __init__.py\n\n* Update save.py\n\n* Temp downgrade due to TRL issue\n\n* Fix up bugs\n\n* Faster saving + other changes\n\n* Update llama.py\n\n* Saving modules\n\n* spelling\n\n* Update llama.py\n\n* Update save.py\n\n* Update save.py\n\n* Update loader.py\n\n* Update llama.py\n\n* patch saving\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* patch saving\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* original_model\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* saving to RAM leakage?\n\n* Update save.py\n\n* new_save_directory\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update pyproject.toml\n\n* Update pyproject.toml\n\n* Update pyproject.toml\n\n* Quick fixes\n\n* Update llama.py\n\n* Update llama.py\n\n* Update dpo.py\n\n* Update dpo.py\n\n* Update llama.py\n\n* Update save.py\n\n* getattr\n\n* RSLoRA and LoftQ direct support\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Fix DPO + GGUF"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-01-19T11:57:22Z",
        "message": "getattr issues (#103)\n\n* Fix tokenizer, dropout, bias for LoRA\n\n* Update loader.py\n\n* Fix LoRA downcasting\n\n* Update _utils.py\n\n* Saving to GGUF\n\n* fix\n\n* colab_quantize_to_gguf\n\n* move save modules\n\n* save module\n\n* Update __init__.py\n\n* Update save.py\n\n* Temp downgrade due to TRL issue\n\n* Fix up bugs\n\n* Faster saving + other changes\n\n* Update llama.py\n\n* Saving modules\n\n* spelling\n\n* Update llama.py\n\n* Update save.py\n\n* Update save.py\n\n* Update loader.py\n\n* Update llama.py\n\n* patch saving\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* patch saving\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* original_model\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* saving to RAM leakage?\n\n* Update save.py\n\n* new_save_directory\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update pyproject.toml\n\n* Update pyproject.toml\n\n* Update pyproject.toml\n\n* Quick fixes\n\n* Update llama.py\n\n* Update llama.py\n\n* Update dpo.py\n\n* Update dpo.py\n\n* Update llama.py\n\n* Update save.py\n\n* getattr"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-01-19T11:52:30Z",
        "message": "Quick fixes (#101)\n\n* Fix tokenizer, dropout, bias for LoRA\n\n* Update loader.py\n\n* Fix LoRA downcasting\n\n* Update _utils.py\n\n* Saving to GGUF\n\n* fix\n\n* colab_quantize_to_gguf\n\n* move save modules\n\n* save module\n\n* Update __init__.py\n\n* Update save.py\n\n* Temp downgrade due to TRL issue\n\n* Fix up bugs\n\n* Faster saving + other changes\n\n* Update llama.py\n\n* Saving modules\n\n* spelling\n\n* Update llama.py\n\n* Update save.py\n\n* Update save.py\n\n* Update loader.py\n\n* Update llama.py\n\n* patch saving\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* patch saving\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* original_model\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* saving to RAM leakage?\n\n* Update save.py\n\n* new_save_directory\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update pyproject.toml\n\n* Update pyproject.toml\n\n* Update pyproject.toml\n\n* Quick fixes\n\n* Update llama.py\n\n* Update llama.py\n\n* Update dpo.py\n\n* Update dpo.py\n\n* Update llama.py\n\n* Update save.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-01-18T17:51:19Z",
        "message": "2024 Release (#96)\n\n* Fix tokenizer, dropout, bias for LoRA\n\n* Update loader.py\n\n* Fix LoRA downcasting\n\n* Update _utils.py\n\n* Saving to GGUF\n\n* fix\n\n* colab_quantize_to_gguf\n\n* move save modules\n\n* save module\n\n* Update __init__.py\n\n* Update save.py\n\n* Temp downgrade due to TRL issue\n\n* Fix up bugs\n\n* Faster saving + other changes\n\n* Update llama.py\n\n* Saving modules\n\n* spelling\n\n* Update llama.py\n\n* Update save.py\n\n* Update save.py\n\n* Update loader.py\n\n* Update llama.py\n\n* patch saving\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* patch saving\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* original_model\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* saving to RAM leakage?\n\n* Update save.py\n\n* new_save_directory\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update pyproject.toml\n\n* Update pyproject.toml\n\n* Update pyproject.toml"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-01-10T17:08:03Z",
        "message": "Fix some bugs (#83)\n\n* Fix tokenizer, dropout, bias for LoRA\n\n* Update loader.py\n\n* Fix LoRA downcasting\n\n* Update _utils.py\n\n* Saving to GGUF\n\n* fix\n\n* colab_quantize_to_gguf\n\n* move save modules\n\n* save module\n\n* Update __init__.py\n\n* Update save.py\n\n* Temp downgrade due to TRL issue\n\n* Fix up bugs"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-01-09T14:02:44Z",
        "message": "fix_tokenizer"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-01-09T12:40:43Z",
        "message": "check_tokenizer"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-01-06T08:13:39Z",
        "message": "Fix tokenizer, bias, dropout supported for LoRA (#69)\n\n* Fix tokenizer, dropout, bias for LoRA\n\n* Update loader.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-01-04T17:08:53Z",
        "message": "Fix tokenizer + docs (#62)\n\n* Patch tokenizer\n\n* Update _utils.py\n\n* Update _utils.py\n\n* Update _utils.py\n\n* Cleanup\n\n* Add comments to functions\n\n* Update rope_embedding.py\n\n* Update rope_embedding.py\n\n* Update llama.py\n\n* New logos!\n\n* Update README.md"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-01-01T16:41:05Z",
        "message": "Update llama.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-01-01T16:37:04Z",
        "message": "Update llama.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2024-01-01T16:20:11Z",
        "message": "Add tokenizer checking + TinyLlama"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2023-12-29T17:35:35Z",
        "message": "Prelim Qwen, Deepseek support (#58)\n\n* Pytorch 2.1.1 install path, 4bit loading\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Update loader.py\n\n* Update loader.py\n\n* Update loader.py\n\n* Update loader.py\n\n* Spelling errors\n\n* Update __init__.py\n\n* DPO loss fix\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Out of bounds tokenization\n\n* Fix Mistral SWA\n\n* Prelim support Qwen, Deepseek etc"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2023-12-28T17:55:01Z",
        "message": "DPO, SWA fixes (#57)\n\n* Pytorch 2.1.1 install path, 4bit loading\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Update loader.py\n\n* Update loader.py\n\n* Update loader.py\n\n* Update loader.py\n\n* Spelling errors\n\n* Update __init__.py\n\n* DPO loss fix\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Out of bounds tokenization\n\n* Fix Mistral SWA"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2023-12-27T17:19:12Z",
        "message": "Nightly (#56)\n\n* Pytorch 2.1.1 install path, 4bit loading\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Update loader.py\n\n* Update loader.py\n\n* Update loader.py\n\n* Update loader.py\n\n* Spelling errors\n\n* Update __init__.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2023-12-26T16:51:31Z",
        "message": "Fix inference"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2023-12-26T16:01:45Z",
        "message": "Fix FastLanguageModel"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2023-12-25T17:32:04Z",
        "message": "Fix RoPE Scaling issues (#52)\n\n* Fix RoPE Scaling\n\n* Update llama.py\n\n* Update llama.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2023-12-25T12:28:05Z",
        "message": "Fix Pytorch 2.1.1"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2023-12-22T17:22:48Z",
        "message": "Small fixes (#48)\n\n* Fix generation for GQA\n\n* Update _utils.py\n\n* flash attn\n\n* Update _utils.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* platform\n\n* Update _utils.py\n\n* Update llama.py\n\n* Logo changed\n\n* Update README.md\n\n* Update README.md"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2023-12-12T02:07:38Z",
        "message": "Update llama.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2023-12-11T15:54:54Z",
        "message": "Update llama.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2023-12-11T15:40:37Z",
        "message": "tokenizer pad fix"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2023-12-11T13:57:06Z",
        "message": "Update llama.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2023-12-11T12:58:36Z",
        "message": "Mistral, GQA support"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2023-12-06T13:13:29Z",
        "message": "Fix generation"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2023-12-05T15:59:22Z",
        "message": "Pre-release 2023 December version (Mistral, Prelim DPO, WSL, bug fixes) (#16)\n\n* Immediate bug fixes\n\n* Update README.md\n\n* Update README.md\n\n* Update llama.py\n\n* Update llama.py\n\n* Rope Scaling and max_seq_len will change\n\n* Update llama.py\n\n* new images\n\n* Update README.md\n\n* Images\n\n* Update README.md\n\n* Update pyproject.toml\n\n* GQA\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/llama.py",
        "commit_date": "2023-11-29T16:51:54Z",
        "message": "First upload of Unsloth code"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora_low_mem.py",
        "commit_date": "2023-07-02T16:16:02Z",
        "message": "fix vocab_size after low-mem-merge"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora_low_mem.py",
        "commit_date": "2023-06-25T13:47:49Z",
        "message": "stylistic update based on Codacy"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora_low_mem.py",
        "commit_date": "2023-06-25T12:56:20Z",
        "message": "stylistic update based on Codacy"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora_low_mem.py",
        "commit_date": "2023-06-16T03:04:20Z",
        "message": "Update merge_llama_with_chinese_lora_low_mem.py"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora_low_mem.py",
        "commit_date": "2023-06-16T03:03:10Z",
        "message": "update help info"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora_low_mem.py",
        "commit_date": "2023-06-16T02:58:08Z",
        "message": "improve naming"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora_low_mem.py",
        "commit_date": "2023-06-16T02:10:02Z",
        "message": "Update merge_llama_with_chinese_lora_low_mem.py\n\n\u62a5\u9519\u4fe1\u606f\u5bb9\u6613\u5f15\u8d77\u6df7\u6dc6\uff0c\u53ef\u5220\u9664"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora_low_mem.py",
        "commit_date": "2023-06-15T16:11:39Z",
        "message": "remove comments"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora_low_mem.py",
        "commit_date": "2023-06-15T16:06:10Z",
        "message": "add assertions"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_chinese_lora_low_mem.py",
        "commit_date": "2023-06-15T09:13:17Z",
        "message": "add low-mem-merge script"
    },
    {
        "repo_url": "github.com/LianjiaTech/BELLE",
        "filepath": "train/src/entry_point/interface.py",
        "commit_date": "2023-09-26T11:39:39Z",
        "message": "update parameter name and bug fix (#523)\n\n* update docker\n\n* update parameter name and bug fix\n\n* delete redundancy file"
    },
    {
        "repo_url": "github.com/LianjiaTech/BELLE",
        "filepath": "train/src/entry_point/interface.py",
        "commit_date": "2023-08-11T08:35:23Z",
        "message": "update readme"
    },
    {
        "repo_url": "github.com/LianjiaTech/BELLE",
        "filepath": "train/src/entry_point/interface.py",
        "commit_date": "2023-08-10T10:27:54Z",
        "message": "add zero inference"
    },
    {
        "repo_url": "github.com/LianjiaTech/BELLE",
        "filepath": "train/src/entry_point/interface.py",
        "commit_date": "2023-06-30T07:38:17Z",
        "message": "add parallelize generation"
    },
    {
        "repo_url": "github.com/LianjiaTech/BELLE",
        "filepath": "train/src/entry_point/interface.py",
        "commit_date": "2023-06-25T14:50:22Z",
        "message": "fix bug in train & add retry in MultiClient"
    },
    {
        "repo_url": "github.com/LianjiaTech/BELLE",
        "filepath": "train/src/entry_point/interface.py",
        "commit_date": "2023-06-15T15:58:12Z",
        "message": "remove peft\nadjust source tree\nfix peft + deepspeed save model\nadd client server inference"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-10-23T03:12:54Z",
        "message": "added low_cpu_mem_usage\n\nSigned-off-by: root <yanzhaodong2021@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-10-16T13:36:31Z",
        "message": "Add all_devices config in AutoLoader.\n\nSigned-off-by: ldwang <ftgreat@gmail.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-10-10T08:43:19Z",
        "message": "updated loader\n\nSigned-off-by: \u4e25\u7167\u4e1c <yanzhaodong2-21@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-10-10T08:30:44Z",
        "message": "enabled aquila2 quantize\n\nSigned-off-by: \u4e25\u7167\u4e1c <yanzhaodong2-21@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-10-09T03:35:57Z",
        "message": "enabled auto torchdtype detecting\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-10-07T08:11:38Z",
        "message": "fixed bug\n\nSigned-off-by: \u4e25\u7167\u4e1c <yanzhaodong2-21@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-10-07T07:13:14Z",
        "message": "reformat autoloader\n\nSigned-off-by: \u4e25\u7167\u4e1c <yanzhaodong2-21@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-10-07T05:37:38Z",
        "message": "added aquila2 finetuning\n\nSigned-off-by: \u4e25\u7167\u4e1c <yanzhaodong2-21@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-09-28T09:51:07Z",
        "message": "enabledaquila2 finetuning\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-09-26T07:39:15Z",
        "message": "remove unused files\n\nSigned-off-by: \u4e25\u7167\u4e1c <yanzhaodong2-21@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-09-26T07:21:02Z",
        "message": "updated model usage method\n\nSigned-off-by: \u4e25\u7167\u4e1c <yanzhaodong2-21@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-09-25T09:58:43Z",
        "message": "flagai fit for aquila2\n\nSigned-off-by: \u4e25\u7167\u4e1c <yanzhaodong2-21@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-07-19T08:23:58Z",
        "message": "docs updated\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-06-29T02:32:49Z",
        "message": "updated lora generation\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-06-29T01:44:47Z",
        "message": "reorganized lora\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-06-25T10:37:32Z",
        "message": "added lora inference file\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-06-25T10:14:37Z",
        "message": "updated lora inference\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-06-25T06:29:06Z",
        "message": "delete a line\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-06-25T06:27:22Z",
        "message": "fixed fp16 and device error\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-06-14T07:34:08Z",
        "message": "released 1,7,3\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-06-08T02:30:11Z",
        "message": "updated docs\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-06-07T01:46:47Z",
        "message": "renamed and removed some files\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-06-06T12:41:46Z",
        "message": "added aquila\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-06-06T02:32:37Z",
        "message": "raw add for testing\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-04-12T08:38:47Z",
        "message": "name changed\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-04-12T08:12:51Z",
        "message": "added AltDiffusion-m18\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-04-11T07:34:10Z",
        "message": "work saved\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-03-29T03:07:21Z",
        "message": "Merge branch 'add_adm18' of github.com:Anhforth/FlagAI into add_adm18"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-03-29T00:39:38Z",
        "message": "local conflicts resolved\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-03-28T13:24:36Z",
        "message": "add altclip-m18\n\nSigned-off-by: zhaohu xing <920232796@qq.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-03-28T10:04:53Z",
        "message": "added altdiffusion_m18\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-03-20T08:40:24Z",
        "message": "updated\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-03-07T07:50:42Z",
        "message": "Revert \"add llama model\""
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-03-07T02:15:50Z",
        "message": "add llama model\n\nSigned-off-by: zhaohu xing <920232796@qq.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-02-28T09:19:16Z",
        "message": "Merge branch 'master' into add_bminf"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-02-27T07:58:46Z",
        "message": "add galactica model"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-02-22T09:23:58Z",
        "message": "changed special token fule\n\nSigned-off-by: Anhforth <yanzaodong2021@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-02-20T02:48:32Z",
        "message": "updated\n\nSigned-off-by: Anhforth <yanzaodong2021@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-02-15T09:38:02Z",
        "message": "Add docs\n\nSigned-off-by: Anhforth <yanzaodong2021@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-01-13T02:19:04Z",
        "message": "enabled fp16 in AltDiffusion\n\nSigned-off-by: Anhforth <yanzaodong2021@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2023-01-03T09:35:20Z",
        "message": "updated\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-12-08T09:41:49Z",
        "message": "fix downfile name err\n\nSigned-off-by: zhaohu xing <920232796@qq.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-12-08T09:40:36Z",
        "message": "fix downfile name err\n\nSigned-off-by: zhaohu xing <920232796@qq.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-12-08T02:28:41Z",
        "message": "load model no network\n\nSigned-off-by: zhaohu xing <920232796@qq.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-12-02T04:11:47Z",
        "message": "add alm module (#160)\n\n* Update setup.py\n\n* add alm model\n\n* add alm\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* add almseq2seq model\n\n* add almseq2seq\n\n* fix\n\n* add new alm\n\nSigned-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>\n\n* add seq2seq alm example\n\n* Update train.py\n\n* add rouge\n\n* add rouge\n\n* add rouge metric\n\n* add bleu\n\n* fix\n\n* delete def\n\n* add generate\n\n* updated tokenizer encode_plus\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* move pdb\n\n* fix bug\n\n* delete pdb\n\n* fix oom\n\n* readjust pad token\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* adjust command token ids\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* fix bugs\n\n* fix error\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* modified according to comments\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updated model dir\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updated readme\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* Added readme for ALM\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* Update README.md\n\n* Update README_zh.md\n\n* Fix generation alm (#156)\n\n* enable normal generated random results for alm\n* enabled nomal generation of alm beamsearch\n* remove sys import\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updated\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* recovered\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* canceled random\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* modified reademe\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* add alm module\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updated alm\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* remove temp path\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updated version number\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updated\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* modified docs\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* changed filename\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* Update predictor.py\n\n* removed unnecessary import\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\nSigned-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>\nCo-authored-by: shunxing1234 <xw747777271@gmail.com>\nCo-authored-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\nCo-authored-by: shunxing1234 <33774367+shunxing1234@users.noreply.github.com>\nCo-authored-by: ZhaodongYan1 <yzdrogeryan@gmail.com>\nCo-authored-by: BAAI-OpenPlatform <107522723+BAAI-OpenPlatform@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-11-28T08:46:14Z",
        "message": "Add alm (#150)\n\n* add alm model\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\nSigned-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>\nCo-authored-by: shunxing1234 <33774367+shunxing1234@users.noreply.github.com>\nCo-authored-by: ZhaodongYan1 <yzdrogeryan@gmail.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-11-26T10:40:46Z",
        "message": " download from server"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-11-26T10:20:27Z",
        "message": "merge changes from the simplification of AltDiffusion loading"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-11-26T10:15:13Z",
        "message": "simplify the loading of AltDiffusion model"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-11-24T01:47:11Z",
        "message": "add eva clip && unit test\n\nSigned-off-by: Quan Sun <quansun84@gmail.com>\nSigned-off-by: quansun <sunquan@baai.ac.cn>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-11-22T10:33:33Z",
        "message": "Revert \"add_eva_clip\""
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-11-22T09:11:11Z",
        "message": "add_eva_clip\n\nSigned-off-by: quansun <sunquan@baai.ac.cn>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-11-19T09:26:19Z",
        "message": "add M9 model to autoloader (#116)\n\n\n\nSigned-off-by: zhaohu xing <920232796@qq.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-11-12T12:47:11Z",
        "message": "update the documents and removed redundant files (#97)\n\n* added diffusion model\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* fixed download path\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* added online version of diffusion\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* removed unused code\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* solved pr issue\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* solve unicode issue\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* solve unicode issue\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* fixed pr issue\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* fixed pr issue\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* fix issue\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* updated requireed packages\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updated requirement.txt\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updated import error\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updatde the import\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* seperate the cn_clip module\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updated the packages\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updatde the package versions\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updated torchvision version\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* fixed tokenizer import\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* recover change\n\n* Delete Bert.py\n\n* reformate the code\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* fix json read warning and reformat code\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updated the changes\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* remove local path\n\n* added readme file\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* fix cpm3 infer bugs\n\n* fix cpm3 model bugs\n\n* Update README.md\n\n* updated the diffusion README and commets\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* removed bmtrain install to avoid error\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* add xlmroberta\n\n* added flagstudio\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* enabled online loading\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* imporved README and changed filename\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* changed the filename\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* add readme file for AltCLIP\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* removed pdb\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* add model_dir\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* updated\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* added more requirements\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* updated pr info\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* modified readme\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* modified readme\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* added the RightBrainAI for our long image generation technology\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* updated the readme\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* fix bugs\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* tokenizer\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* Delete vocab.txt\n\n* fix tokenizer bugs\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* added huggingface open-source address to docs\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* delete some files (#95)\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* modified docs\n\nSigned-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>\n\n* modified laion link and turn parameters descriptions in a table\n\nSigned-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>\n\n* modify file name (#96)\n\n\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* updated the docs\n\nSigned-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>\n\n* modified the docs\n\nSigned-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>\n\n* updated\n\nSigned-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>\n\n* updated the docs\n\nSigned-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>\n\n* solve license linke issye\n\nSigned-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>\n\n* modify classnames\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\nSigned-off-by: zhaohu xing <920232796@qq.com>\nSigned-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>\nCo-authored-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\nCo-authored-by: shunxing1234 <xw747777271@gmail.com>\nCo-authored-by: zhaohu xing <920232796@qq.com>\nCo-authored-by: Anhforth <yanzhaodong2021@163.com>\nCo-authored-by: Zac Liu <liuguang@baai.ac.cn>\nCo-authored-by: zhaohu xing <32668889+920232796@users.noreply.github.com>\nCo-authored-by: ZhaodongYan1 <yzdrogeryan@gmail.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-11-11T08:03:16Z",
        "message": "Add AltDiffusion and AltCLIP (#90)\n\n* added diffusion model\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* fixed download path\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* added online version of diffusion\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* removed unused code\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* solved pr issue\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* solve unicode issue\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* solve unicode issue\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* fixed pr issue\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* fixed pr issue\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* fix issue\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\n\n* updated requireed packages\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updated requirement.txt\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updated import error\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updatde the import\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* seperate the cn_clip module\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updated the packages\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updatde the package versions\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updated torchvision version\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* fixed tokenizer import\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* recover change\n\n* Delete Bert.py\n\n* reformate the code\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* fix json read warning and reformat code\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* updated the changes\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* remove local path\n\n* added readme file\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* fix cpm3 infer bugs\n\n* fix cpm3 model bugs\n\n* Update README.md\n\n* updated the diffusion README and commets\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* removed bmtrain install to avoid error\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* add xlmroberta\n\n* added flagstudio\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* enabled online loading\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* imporved README and changed filename\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* changed the filename\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* add readme file for AltCLIP\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* removed pdb\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* add model_dir\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* updated\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* added more requirements\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* updated pr info\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* modified readme\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* modified readme\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* added the RightBrainAI for our long image generation technology\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* updated the readme\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* fix bugs\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* tokenizer\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* Delete vocab.txt\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\nSigned-off-by: zhaohu xing <920232796@qq.com>\nCo-authored-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>\nCo-authored-by: shunxing1234 <xw747777271@gmail.com>\nCo-authored-by: zhaohu xing <920232796@qq.com>\nCo-authored-by: Anhforth <yanzhaodong2021@163.com>\nCo-authored-by: Zac Liu <liuguang@baai.ac.cn>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-11-02T03:29:23Z",
        "message": "fix tokenizer"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-11-02T02:01:50Z",
        "message": "add cpm3"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-10-24T09:25:42Z",
        "message": "merged upstream\n\nSigned-off-by: root <root@zhadong-4mhn8-8819-worker-0.yanzhaodong.baaishare-sailing.svc.kubebrain.local>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-10-24T06:00:15Z",
        "message": "add cpm3 model"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-09-20T03:02:37Z",
        "message": "Fix issue#85 (#86)\n\n* updated the requirement packages list\n* tried to fix the data directory not found error\n* fixed issues in running glm_seq2seq\n* Update test_glm_seq2seq.py\n* fix glm tokenizer bug\n* add news section in Readme\n* updated docs for tokenizer\n* update required packages\n* fix_issue_85\n* fix bugs in bert forward with different shape of attention mask\n* enable offline loading\n* fixed error in test_files and add robert ch tokenizer\n* fix error in action\n* solved Filenotfounderror\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\nSigned-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>\nSigned-off-by: zhaohu xing <920232796@qq.com>\nSigned-off-by: marscrazy <liuguang@baai.ac.cn>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-09-06T07:17:20Z",
        "message": "fix_issue_85"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-08-29T09:00:37Z",
        "message": "Fixed errors by upper-case of model name, and changed the description (#82)\n\n\n* fix a glm tokenizer bug\nSigned-off-by: zhaohu xing <920232796@qq.com>\n* Update tokenizer.py\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-08-29T07:54:27Z",
        "message": "Added CLIP module and redesigned tokenizer apis (#81)\n\n* merged clip tokenizer\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* Update inference_clip.py\n\n* Update auto_loader.py\n\n* Update glm_10b_en_tokenizer.py\n\n* swinv1v2\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* updated the version\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* updated the requirement packages list\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* fixed some issues\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* fixed some issues\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* tried to fix the data directory not found error\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* fixed issues in running glm_seq2seq\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\n\n* Update test_glm_seq2seq.py\n\n* Update setup.py\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\nSigned-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>\nSigned-off-by: zhaohu xing <920232796@qq.com>\nSigned-off-by: shunxing1234 <xw747777271@gmail.com>\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>\nCo-authored-by: Anhforth <yanzhaodong2021@163.com>\nCo-authored-by: zhaohu xing <32668889+920232796@users.noreply.github.com>\nCo-authored-by: Zhaodong Yan <94831503+Anhforth@users.noreply.github.com>\nCo-authored-by: ZhaodongYan1 <yzdrogeryan@gmail.com>\nCo-authored-by: Zac Liu <liuguang@baai.ac.cn>\nCo-authored-by: zhaohu xing <920232796@qq.com>\nCo-authored-by: jongjyh <jongjyh0221@gmail.com>\nCo-authored-by: wchh-2000 <2371156095@qq.com>\nCo-authored-by: xuanricheng <xuanricheng@hotmail.com>\nCo-authored-by: shunxing1234 <xw747777271@gmail.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-08-25T07:56:50Z",
        "message": "swinv1v2\n\nSigned-off-by: zhaohu xing <920232796@qq.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-08-25T01:37:51Z",
        "message": "Update auto_loader.py"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-08-23T01:29:21Z",
        "message": "merged clip tokenizer\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-08-22T07:57:39Z",
        "message": "merged the clip tokenizer\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-08-19T02:30:04Z",
        "message": "updated according to comments\n\nSigned-off-by: BAAI-OpenPlatform <open.platform@baai.ac.cn>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-08-18T07:35:47Z",
        "message": "Merge branch 'master' into transform_tokenizer"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-08-16T08:35:10Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-08-15T07:23:53Z",
        "message": "add autoloader and example training data"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-08-07T01:23:09Z",
        "message": "add cpm model\n\nSigned-off-by: marscrazy <liuguang@baai.ac.cn>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-07-29T01:32:32Z",
        "message": "inference and train"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-07-26T02:29:35Z",
        "message": "merged the master\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-07-21T13:11:52Z",
        "message": "Develop (#71)\n\n\n\n* add vit and examples\n\n* vit and examples\n\n* Update base_model.py\n\nremove unused glob\n\n* Update vit.py\n\nremove data statis\n\n* modify readme.md\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* modify readme.md\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* delete annotating code\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* Vit xzh (#25)\n\n* add vit and examples\n\n* vit and examples\n\n* Update base_model.py\n\nremove unused glob\n\n* Update vit.py\n\nremove data statis\n\n* modify readme.md\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* modify readme.md\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* delete annotating code\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\nCo-authored-by: Zac Liu <liuguang@baai.ac.cn>\n\n* env trainer\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* Create README.md\n\nfix typo in flagai introduction.\n\n* vit-checkpoint-activations\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n* vit-checkpoint-activations\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\nCo-authored-by: zhaohu xing <32668889+920232796@users.noreply.github.com>\nCo-authored-by: Zhaodong Yan <94831503+Anhforth@users.noreply.github.com>\nCo-authored-by: Zac Liu <liuguang@baai.ac.cn>\nCo-authored-by: Anhforth <yanzhaodong2021@163.com>\nCo-authored-by: zhaohu xing <920232796@qq.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-07-19T03:22:17Z",
        "message": "Merge branch 'develop' into vit_xzh"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-07-15T09:57:37Z",
        "message": "add vit and examples"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-07-15T06:08:41Z",
        "message": "modified encoder_plus\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-07-15T01:39:53Z",
        "message": "updated\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-07-11T06:42:25Z",
        "message": "Develop (#66)\n\n\n\n* fix bug multi_gpu_training\n\n* Update trainer.py\n\nremove comment\n\n* changed the version\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* Update trainer.py\n\n* fix_validation_bug (#24)\n\n* updated the version\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-07-11T05:37:05Z",
        "message": "fix_validation_bug (#24)"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-07-07T10:11:30Z",
        "message": "fixed conflicts\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-07-07T10:05:20Z",
        "message": "test of tokenizer transform\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-07-06T09:38:48Z",
        "message": "fixed tokenizer issues (#63)\n\n* Opt 30b (#16)\n* clean codes \nCo-authored-by: Zac Liu <liuguang@baai.ac.cn>\n* fix bert tokenizer issue (#18)\n* fix bert tokenizer issue\n* updated t5, opt and roberta tokenizers\n* fixed doc 404 error\nSigned-off-by: ZhaodongYan1 <yzdrogeryan@gmail.com>\n* Opt 66b (#19)\n* autoloader for opt\n* opt-66b inference\n* Update train.py\n* Load data from example dir\n* add readme of multi GPU inference\nCo-authored-by: Zac Liu <liuguang@baai.ac.cn>\n* updated release version\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n* fix tokenizer issue\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\nCo-authored-by: zhaohu xing <32668889+920232796@users.noreply.github.com>\nCo-authored-by: Zhaodong Yan <94831503+Anhforth@users.noreply.github.com>\nCo-authored-by: Zac Liu <liuguang@baai.ac.cn>\nCo-authored-by: Anhforth <yanzhaodong2021@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-07-06T06:19:29Z",
        "message": "Opt 66b (#19)\n\n* autoloader for opt\n* opt-66b inference\n* Update train.py\n* Load data from example dir\n* add readme of multi GPU inference\n\nCo-authored-by: Zac Liu <liuguang@baai.ac.cn>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-07-01T05:40:17Z",
        "message": "transformed tokenizer: progressing\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-06-28T09:38:57Z",
        "message": "fixed tokenizer in glm blankfilling (#13)\n\n* fixed tokenizer in glm blankfilling\n\nSigned-off-by: Anhforth <yanzhaodong2021@163.com>\n\n* Update glm_generate_samples_en.py\n\nCo-authored-by: Anhforth <yanzhaodong2021@163.com>\nCo-authored-by: Zac Liu <liuguang@baai.ac.cn>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-06-24T05:52:51Z",
        "message": "test merging upstream Develop to master (#5)\n\nfix bugs in distributed training & evaluating\n\nCo-authored-by: Zac Liu <liuguang@baai.ac.cn>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-06-24T05:03:50Z",
        "message": "Add opt (#9)\n\nfix bug in autoloader\n\nSigned-off-by: zhaohu xing <920232796@qq.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-06-24T02:43:30Z",
        "message": "change the save dirname for autoloader\uff0ckeep the dirname == cased modelname (#8)\n\n\n* fix autoloader\nSigned-off-by: zhaohu xing <920232796@qq.com>\nCo-authored-by: Zac Liu <liuguang@baai.ac.cn>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-06-23T14:16:24Z",
        "message": "Add OPT models  (#6)\n\n* added opt tokenizer and updated cpm1 tokenizer\n* fixed superglue traininng error\nCo-authored-by: Anhforth <yanzhaodong2021@163.com>\n* fixed auto_loader\n* opt model\n* add opt examples, tests\n* add opt_6.7b\n* opt-13b\n* opt-13b-en\nSigned-off-by: zhaohu xing <920232796@qq.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-06-19T18:42:49Z",
        "message": "Release v1.0.4 (#61)\n\n* add gradient accumulation and fix bugs in distributed evaluation\n* fix bugs in accumulate gradients in DDP / Pytorch / DeepSpeed\n* release v1.0.4\n* set epoch for distributed sampler\n* add warnings for DDP with FP16\n* change save_epoch to save_interval\n* code formate with yapf\n* fix typos in utils.py\n* del train_test.py\n* fix bugs while test seq2seq task with  rather than\n* the glm superglue need some works\nSigned-off-by: marscrazy <marscrazy_90@163.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-06-10T04:00:46Z",
        "message": "fix_cpm\n\nSigned-off-by: zhaohu xing <920232796@qq.com>"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/auto_model/auto_loader.py",
        "commit_date": "2022-06-01T15:23:40Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/microsoft/LMOps",
        "filepath": "minillm/transformers/src/transformers/trainer.py",
        "commit_date": "2023-11-16T12:47:19Z",
        "message": "upgrade transformers"
    },
    {
        "repo_url": "github.com/microsoft/LMOps",
        "filepath": "minillm/transformers/src/transformers/trainer.py",
        "commit_date": "2023-08-16T03:11:32Z",
        "message": "upgrade transformers"
    },
    {
        "repo_url": "github.com/microsoft/LMOps",
        "filepath": "minillm/transformers/src/transformers/trainer.py",
        "commit_date": "2023-06-14T11:44:35Z",
        "message": "add minillm"
    },
    {
        "repo_url": "github.com/LianjiaTech/BELLE",
        "filepath": "train/src/merge_llama_with_lora.py",
        "commit_date": "2023-05-09T10:40:42Z",
        "message": "Update train v2"
    },
    {
        "repo_url": "github.com/pengxiao-song/LaWGPT",
        "filepath": "utils/evaluate.py",
        "commit_date": "2023-05-21T18:39:04Z",
        "message": "[ENH] Restructure the project."
    },
    {
        "repo_url": "github.com/liguodongiot/llm-action",
        "filepath": "train/alpaca-lora/generate.py",
        "commit_date": "2023-07-23T11:43:58Z",
        "message": "fix"
    },
    {
        "repo_url": "github.com/huggingface/alignment-handbook",
        "filepath": "scripts/run_dpo.py",
        "commit_date": "2024-03-01T16:29:42Z",
        "message": "\ud83e\ude81 (#129)\n\n* Add Gemma 7B recipe\n\n* Use Gemma template\n\n* Make it work for dolly lol\n\n* Enable cahce\n\n* Clean up\n\n* DPO to the max\n\n* DPO, DPO, DPO\n\n* Add openhermes\n\n* Add custom configs\n\n* Add kwargs\n\n* Fix config\n\n* Bump deps\n\n* Move old recipes\n\n* Add doc\n\n* Add norte\n\n* Renable cache\n\n* Nuke\n\n* Clean\n\n* Apply suggestions from code review\n\nCo-authored-by: Alvaro Bartolome <alvaro@argilla.io>\n\n* Fix isort\n\n* Update README.md\n\n* Update config_full.yaml\n\n---------\n\nCo-authored-by: Alvaro Bartolome <alvaro@argilla.io>\nCo-authored-by: Philipp Schmid <32632186+philschmid@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/alignment-handbook",
        "filepath": "scripts/run_dpo.py",
        "commit_date": "2024-02-28T19:05:44Z",
        "message": "Add `auto_insert_empty_system_msg` config flag (#123)\n\n* Make system messages optional\n\nAlso use the `maybe_insert_system_message` in dpo setting\n\n* add `auto_insert_empty_system_msg` flag\n\n* add `auto_insert_empty_system_msg`\n\n* add auto_insert_empty_system_msg\n\n* Update src/alignment/configs.py\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* make style\n\n---------\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/alignment-handbook",
        "filepath": "scripts/run_dpo.py",
        "commit_date": "2024-02-05T15:50:17Z",
        "message": "Apply quantization during DPO QLoRA (#115)\n\n* Add QLoRA fix\n\n* Update script"
    },
    {
        "repo_url": "github.com/huggingface/alignment-handbook",
        "filepath": "scripts/run_dpo.py",
        "commit_date": "2024-01-10T06:42:24Z",
        "message": "Update Zephyr configs to account for UltraFeedback & TRL fixes (#88)\n\n* Add files\n\n* Add checkpointing\n\n* Add checkpointing to SFT\n\n* Add loss type\n\n* Fix setup|\n\n* Clean SFT\n\n* Add lora config\n\n* Rename config\n\n* Remove max eval samples\n\n* Add kwargs tp push to hub\n\n* Add DPO configs\n\n* Fix dpo configs\n\n* Extend chat template test to multi-turn\n\n* Add warmup\n\n* Refactor\n\n* Fix LoRA -> QLoRA\n\n* Fix configs\n\n* Specify chat template\n\n* Add sample logging\n\n* Fix push to hub hanging\n\n* Add reentrant\n\n* Fix quality\n\n* Add transformer logging\n\n* Tweak grad acc\n\n* Add null type\n\n* Add doc"
    },
    {
        "repo_url": "github.com/huggingface/alignment-handbook",
        "filepath": "scripts/run_dpo.py",
        "commit_date": "2024-01-04T22:06:47Z",
        "message": "Clean deprecated max_samples arguments (#89)"
    },
    {
        "repo_url": "github.com/huggingface/alignment-handbook",
        "filepath": "scripts/run_dpo.py",
        "commit_date": "2023-12-04T08:10:41Z",
        "message": "Add check that parameters are not intended to be offloaded (#51)\n\n* Add check that parameters are not intended to be offloaded\n\n* Only push model to device if quantization config is set."
    },
    {
        "repo_url": "github.com/huggingface/alignment-handbook",
        "filepath": "scripts/run_dpo.py",
        "commit_date": "2023-11-10T13:38:45Z",
        "message": "Refactor imports"
    },
    {
        "repo_url": "github.com/huggingface/alignment-handbook",
        "filepath": "scripts/run_dpo.py",
        "commit_date": "2023-11-10T13:15:44Z",
        "message": "adds auto adapter merge to dpo script"
    },
    {
        "repo_url": "github.com/huggingface/alignment-handbook",
        "filepath": "scripts/run_dpo.py",
        "commit_date": "2023-11-09T13:39:03Z",
        "message": "Add more doc"
    },
    {
        "repo_url": "github.com/huggingface/alignment-handbook",
        "filepath": "scripts/run_dpo.py",
        "commit_date": "2023-11-08T22:58:34Z",
        "message": "Make DPO work!"
    },
    {
        "repo_url": "github.com/huggingface/alignment-handbook",
        "filepath": "scripts/run_dpo.py",
        "commit_date": "2023-11-08T13:21:57Z",
        "message": "Add skeleton"
    },
    {
        "repo_url": "github.com/liguodongiot/llm-action",
        "filepath": "train/qlora/inference_merge.py",
        "commit_date": "2023-07-23T11:43:58Z",
        "message": "fix"
    },
    {
        "repo_url": "github.com/liguodongiot/llm-action",
        "filepath": "train/qlora/inference_qlora.py",
        "commit_date": "2023-07-23T11:43:58Z",
        "message": "fix"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "demo.py",
        "commit_date": "2023-11-05T16:58:41Z",
        "message": "fix starting token repetition"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "demo.py",
        "commit_date": "2023-10-30T15:24:42Z",
        "message": "Update demo.py"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "demo.py",
        "commit_date": "2023-10-26T10:08:36Z",
        "message": "[demo] add ip and port input"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "demo.py",
        "commit_date": "2023-10-09T14:00:53Z",
        "message": "Update demo.py\n\nAdded 'skip_special_tokens=True' to remove special tokens from the output."
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "demo.py",
        "commit_date": "2023-10-09T11:30:35Z",
        "message": "Update demo.py\n\nAdded skip_prompt argument to TextStreamer"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "demo.py",
        "commit_date": "2023-10-09T10:59:19Z",
        "message": "Update demo.py"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "demo.py",
        "commit_date": "2023-10-09T10:20:31Z",
        "message": "Update demo.py\n\n- Generated words are displayed immediately by using TextStreamer.\n\n- Removed examples as the example .txt files are not there."
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "demo.py",
        "commit_date": "2023-10-09T08:17:03Z",
        "message": "Update demo.py"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "demo.py",
        "commit_date": "2023-10-03T03:43:44Z",
        "message": "Update demo.py"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "demo.py",
        "commit_date": "2023-10-03T03:12:47Z",
        "message": "Update demo.py"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "demo.py",
        "commit_date": "2023-09-23T05:35:05Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "demo.py",
        "commit_date": "2023-09-21T18:13:56Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/AutoGPTQ/AutoGPTQ",
        "filepath": "auto_gptq/utils/peft_utils.py",
        "commit_date": "2024-02-13T16:06:07Z",
        "message": "Use ruff for linting (#537)\n\n* lint\n\n* add back init\n\n* remove pyproject.toml that automatically triggers build isolation\n\n* install torch\n\n* maybe use 3.9?\n\n* debug\n\n* upgrade setuptools\n\n* maybe ubuntu 22.04?\n\n* wheel\n\n* working now?\n\n* typog\n\n* indent\n\n* fix indent\n\n* do not use powershell\n\n* free space\n\n* fix cuda path\n\n* prints\n\n* where is conda?\n\n* should finally work\n\n* fix\n\n* final fixN\n\n* arch ist\n\n* typog\n\n* add quality extra\n\n* last fix"
    },
    {
        "repo_url": "github.com/AutoGPTQ/AutoGPTQ",
        "filepath": "auto_gptq/utils/peft_utils.py",
        "commit_date": "2023-11-08T11:41:25Z",
        "message": "Fix windows (no triton) and cpu-only support (#411)\n\n* fix windows (no triton) and cpu-only support\n\nFrom: fxmarty <9808326+fxmarty@users.noreply.github.com>\nFrom: Yang Wang <yang3.wang@intel.com>\n\n* Add co-author\n\nCo-authored-by: Yang Wang <yang3.wang@intel.com>\n\n* fix tests when cuda ext are not installed\n\n---------\n\nCo-authored-by: Yang Wang <yang3.wang@intel.com>"
    },
    {
        "repo_url": "github.com/AutoGPTQ/AutoGPTQ",
        "filepath": "auto_gptq/utils/peft_utils.py",
        "commit_date": "2023-11-07T16:16:52Z",
        "message": "fix windows support (#407)"
    },
    {
        "repo_url": "github.com/AutoGPTQ/AutoGPTQ",
        "filepath": "auto_gptq/utils/peft_utils.py",
        "commit_date": "2023-10-27T07:12:16Z",
        "message": "PEFT initialization fix (#361)\n\n* Initial code for GPTQLoraLinear initialization\n\n* Working AdaLora\n\n* remove unused methods and pin peft\n\n---------\n\nCo-authored-by: Felix Marty <9808326+fxmarty@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/AutoGPTQ/AutoGPTQ",
        "filepath": "auto_gptq/utils/peft_utils.py",
        "commit_date": "2023-09-24T13:11:19Z",
        "message": "Use `adapter_name` for `get_gptq_peft_model` with `train_mode=True`"
    },
    {
        "repo_url": "github.com/AutoGPTQ/AutoGPTQ",
        "filepath": "auto_gptq/utils/peft_utils.py",
        "commit_date": "2023-05-30T15:03:51Z",
        "message": "remove  raise"
    },
    {
        "repo_url": "github.com/AutoGPTQ/AutoGPTQ",
        "filepath": "auto_gptq/utils/peft_utils.py",
        "commit_date": "2023-05-28T14:35:34Z",
        "message": "raise NotImplementedError when model with fused attention injected try to use ADAPTION_PROMPT peft type"
    },
    {
        "repo_url": "github.com/AutoGPTQ/AutoGPTQ",
        "filepath": "auto_gptq/utils/peft_utils.py",
        "commit_date": "2023-05-28T14:11:02Z",
        "message": "reset value of AdaptionPromptConfig.adapter_layers to number of model's hidden layers when exceeds"
    },
    {
        "repo_url": "github.com/AutoGPTQ/AutoGPTQ",
        "filepath": "auto_gptq/utils/peft_utils.py",
        "commit_date": "2023-05-28T13:30:45Z",
        "message": "support AdaLora"
    },
    {
        "repo_url": "github.com/AutoGPTQ/AutoGPTQ",
        "filepath": "auto_gptq/utils/peft_utils.py",
        "commit_date": "2023-05-28T09:36:18Z",
        "message": "make GPTQLoraModel to inherit from LoraModel to simplify code"
    },
    {
        "repo_url": "github.com/AutoGPTQ/AutoGPTQ",
        "filepath": "auto_gptq/utils/peft_utils.py",
        "commit_date": "2023-05-28T09:04:38Z",
        "message": "add 'auto_find_all_linears' argument to get_gptq_peft_model function"
    },
    {
        "repo_url": "github.com/AutoGPTQ/AutoGPTQ",
        "filepath": "auto_gptq/utils/peft_utils.py",
        "commit_date": "2023-05-28T08:57:31Z",
        "message": "add warning to guide users interact with lora properly"
    },
    {
        "repo_url": "github.com/AutoGPTQ/AutoGPTQ",
        "filepath": "auto_gptq/utils/peft_utils.py",
        "commit_date": "2023-05-26T23:49:17Z",
        "message": "add find_all_linear_names help function, make customized lora module more general"
    },
    {
        "repo_url": "github.com/AutoGPTQ/AutoGPTQ",
        "filepath": "auto_gptq/utils/peft_utils.py",
        "commit_date": "2023-05-26T06:06:53Z",
        "message": "set xavier_uniform_ as lora_A's init function"
    },
    {
        "repo_url": "github.com/AutoGPTQ/AutoGPTQ",
        "filepath": "auto_gptq/utils/peft_utils.py",
        "commit_date": "2023-05-25T23:18:16Z",
        "message": "refactor file structure of qlinears"
    },
    {
        "repo_url": "github.com/AutoGPTQ/AutoGPTQ",
        "filepath": "auto_gptq/utils/peft_utils.py",
        "commit_date": "2023-05-25T11:44:53Z",
        "message": "lora compatibility"
    },
    {
        "repo_url": "github.com/AutoGPTQ/AutoGPTQ",
        "filepath": "auto_gptq/utils/peft_utils.py",
        "commit_date": "2023-05-25T07:11:11Z",
        "message": "first upload peft_utils.py"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-04-09T21:07:59Z",
        "message": "Update export_hf_checkpoint.py (#302)\n\n* Update export_hf_checkpoint.py\n\n* Update finetune.py\n\nNew tokenizer base model for the current dev branch of transformers\n\n* Update generate.py\n\n* Update export_state_dict_checkpoint.py\n\n* Update export_hf_checkpoint.py"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-03-28T15:33:47Z",
        "message": "remove asserts"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-03-27T17:31:44Z",
        "message": "Add HF dataset loading, add linters, pyproject.toml (#175)\n\n* add HF dataset loading, add linters, pyproject.toml\n\n- applied markdownlint\n- add black, black[jupyter], isort\n- fix noqa codes\n- add .github workflow linting\n- update README.md\n\n* restore default settings\n\n* resume_from_checkpoint\n\nCo-authored-by: AngainorDev <54739135+AngainorDev@users.noreply.github.com>\n\n* Print warning on checkpoint not found\n\n* add HF dataset loading, add linters, pyproject.toml\n\n- applied markdownlint\n- add black, black[jupyter], isort\n- fix noqa codes\n- add .github workflow linting\n- update README.md\n\n* Default to local copy and update it\n\n* Typo\n\n* Remove duplicate code block\n\n---------\n\nCo-authored-by: Eric Wang <eric.james.wang@gmail.com>\nCo-authored-by: AngainorDev <54739135+AngainorDev@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-03-23T20:54:39Z",
        "message": "Remove LLaMA download code, as a precaution"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-03-16T19:11:47Z",
        "message": "Catch outdated installs"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-03-16T19:11:29Z",
        "message": "Update alpaca-lora to use transformers main branch"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-03-16T07:50:24Z",
        "message": "Fix LoRa weight merging"
    },
    {
        "repo_url": "github.com/tloen/alpaca-lora",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-03-16T00:17:32Z",
        "message": "Add script for converting weights from HF"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "tools/merge_lora.py",
        "commit_date": "2023-04-02T06:36:48Z",
        "message": "path typo"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "tools/merge_lora.py",
        "commit_date": "2023-03-27T14:51:27Z",
        "message": "merge changes for cpp inference"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "tools/merge_lora.py",
        "commit_date": "2023-03-27T14:41:25Z",
        "message": "Revert \"Merge branch 'chatglm' of ../Chinese-Vicuna\"\n\nThis reverts commit ca4f9ae8f1f2534ece5694a73457eda44537dc4b, reversing\nchanges made to 1a788ea8952b6c3df6550f9e69c986794e45a0ca."
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "tools/merge_lora.py",
        "commit_date": "2023-03-27T14:39:43Z",
        "message": "update cpp inference files"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "eval.py",
        "commit_date": "2023-10-31T15:03:45Z",
        "message": "Update eval.py"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "eval.py",
        "commit_date": "2023-10-04T02:57:32Z",
        "message": "Update eval.py"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "eval.py",
        "commit_date": "2023-09-21T18:13:56Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/liguodongiot/llm-action",
        "filepath": "train/chatglm-lora/inference.py",
        "commit_date": "2023-07-23T11:43:58Z",
        "message": "fix"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/byom.py",
        "commit_date": "2023-06-11T13:09:39Z",
        "message": "fix: remove force download option"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/byom.py",
        "commit_date": "2023-06-11T12:54:36Z",
        "message": "fix: byom.py to take modes"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/byom.py",
        "commit_date": "2023-06-08T01:59:11Z",
        "message": "exp w/ camel and sam"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/byom.py",
        "commit_date": "2023-06-04T14:50:44Z",
        "message": "disable force download & example view"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/byom.py",
        "commit_date": "2023-05-31T07:55:49Z",
        "message": "add byom feature"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/loader.py",
        "commit_date": "2024-02-26T14:42:10Z",
        "message": "2.4x faster Gemma (#197)\n\n* Update save.py\n\n* Update save.py\n\n* linking\n\n* llama.cpp bugs\n\n* Update save.py\n\n* Update save.py\n\n* saving\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update __init__.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* save\n\n* trainer\n\n* spaces\n\n* original\n\n* Gemma\n\n* Update pyproject.toml\n\n* Update mapper.py\n\n* Update fast_lora.py\n\n* FastGemmaModel\n\n* model_type\n\n* Update llama.py\n\n* Update llama.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update cross_entropy_loss.py\n\n* Update llama.py\n\n* Update llama.py\n\n* gemma\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Fast CE Loss\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* CE\n\n* Update llama.py\n\n* Update llama.py\n\n* Update cross_entropy_loss.py\n\n* Update geglu.py\n\n* Update cross_entropy_loss.py\n\n* revert\n\n* Update llama.py\n\n* Update llama.py\n\n* norm\n\n* Update gemma.py\n\n* Update gemma.py\n\n* position_ids\n\n* Update gemma.py\n\n* Update gemma.py\n\n* pos\n\n* Update llama.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update cross_entropy_loss.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* revert\n\n* revert\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update cross_entropy_loss.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* rope\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* llama\n\n* Update llama.py\n\n* gemma\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update gemma.py\n\n* Update save.py\n\n* RoPE\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update gemma.py\n\n* correct_dtype\n\n* Update gemma.py\n\n* Update cross_entropy_loss.py\n\n* Update cross_entropy_loss.py\n\n* Chat Templates\n\n* Update README.md\n\n* Update README.md"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/loader.py",
        "commit_date": "2024-02-20T16:58:59Z",
        "message": "Feb 2024 Release (#187)\n\n* Fast inference repatch\n\n* Update llama.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update mistral.py\n\n* Update __init__.py\n\n* Fix inference\n\n* Update mistral.py\n\n* fast lm_head\n\n* Remove fast path\n\n* Update rope_embedding.py\n\n* Update loader.py\n\n* LlamaAttention_fast_forward_inference\n\n* if past_key_value is not None and q_len == 1:\n\n* revert inference\n\n* Update loader.py\n\n* past_key_value\n\n* Update llama.py\n\n* Update llama.py\n\n* Fix SDPA\n\n* Update llama.py\n\n* padding\n\n* Inference\n\n* Update llama.py\n\n* Revert\n\n* Update mistral.py\n\n* faster inference\n\n* inference\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* inference\n\n* Update llama.py\n\n* Update utils.py\n\n* faster inference\n\n* Update llama.py\n\n* revert\n\n* lm_head\n\n* Update llama.py\n\n* inference\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* faster inference\n\n* Update llama.py\n\n* fast inference\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* torch compile\n\n* past_key_values\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update llama.py\n\n* fast inference + saving config.json\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* fast inference again\n\n* more temp matrices\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* fast inference\n\n* Update mistral.py\n\n* Update llama.py\n\n* SDPA\n\n* attention_mask\n\n* New version\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update save.py\n\n* Update save.py\n\n* Torch 2.2.0\n\n* Update save.py\n\n* mistral swa\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Fix SWA inference\n\n* Fix llm_int8_skip_modules\n\n* SWA inference\n\n* Update save.py\n\n* Update save.py\n\n* Update pyproject.toml\n\n* __version__\n\n* __version__\n\n* Update save.py\n\n* Update save.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Chat Templates\n\n* Update chat_templates.py\n\n* Update chat_templates.py\n\n* Update chat_templates.py\n\n* Update chat_templates.py\n\n* patch tokenizer\n\n* Update chat_templates.py\n\n* Saving, LlamaRotaryEmbedding issues\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update mapper.py\n\n* Fix RoPE precision issues\n\n* Bugs\n\n* saving bugs\n\n* Update llama.py\n\n* readme\n\n* spaces\n\n* spaces\n\n* globals\n\n* slash\n\n* slashes\n\n* spaces\n\n* apache\n\n* Update save.py\n\n* Update save.py\n\n* Update loader.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* trainer\n\n* Update save.py\n\n* Update pyproject.toml\n\n* install\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* PeftModel token + saving\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* linking\n\n* llama.cpp bugs\n\n* Update save.py\n\n* Update save.py\n\n* saving\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update __init__.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* save\n\n* trainer\n\n* spaces\n\n* original"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/loader.py",
        "commit_date": "2024-02-13T07:28:42Z",
        "message": "add HF tagging in unsloth (#170)"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/loader.py",
        "commit_date": "2024-02-06T17:40:50Z",
        "message": "Torch 2.2 (#157)\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update save.py\n\n* Update fast_lora.py\n\n* Update utils.py\n\n* Update llama.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update save.py\n\n* Update save.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Revert \"Update llama.py\"\n\nThis reverts commit a208ec46e012cf470ecefe6268a66358215df7b6.\n\n* Update llama.py\n\n* Works?\n\n* Update pyproject.toml\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Swiglu\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update swiglu.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* attention_mask\n\n* Update llama.py\n\n* Update llama.py\n\n* labels\n\n* Update mistral.py\n\n* Update llama.py\n\n* attention mask\n\n* Update save.py\n\n* Update save.py\n\n* Update mistral.py\n\n* attention mask\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update dpo.py\n\n* Patch saving\n\n* Update save.py\n\n* Update save.py\n\n* patch_saving_functions\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* print\n\n* Mistral patch\n\n* Update mistral.py\n\n* Update save.py\n\n* saving\n\n* Update llama.py\n\n* Update llama.py\n\n* Fast inference repatch\n\n* Update llama.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update mistral.py\n\n* Update __init__.py\n\n* Fix inference\n\n* Update mistral.py\n\n* fast lm_head\n\n* Remove fast path\n\n* Update rope_embedding.py\n\n* Update loader.py\n\n* LlamaAttention_fast_forward_inference\n\n* if past_key_value is not None and q_len == 1:\n\n* revert inference\n\n* Update loader.py\n\n* past_key_value\n\n* Update llama.py\n\n* Update llama.py\n\n* Fix SDPA\n\n* Update llama.py\n\n* padding\n\n* Inference\n\n* Update llama.py\n\n* Revert\n\n* Update mistral.py\n\n* faster inference\n\n* inference\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* inference\n\n* Update llama.py\n\n* Update utils.py\n\n* faster inference\n\n* Update llama.py\n\n* revert\n\n* lm_head\n\n* Update llama.py\n\n* inference\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* faster inference\n\n* Update llama.py\n\n* fast inference\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* torch compile\n\n* past_key_values\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update llama.py\n\n* fast inference + saving config.json\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* fast inference again\n\n* more temp matrices\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* fast inference\n\n* Update mistral.py\n\n* Update llama.py\n\n* SDPA\n\n* attention_mask\n\n* New version\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update utils.py\n\n* Update utils.py\n\n* Update save.py\n\n* Update save.py\n\n* Torch 2.2.0\n\n* Update save.py\n\n* mistral swa\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Fix SWA inference\n\n* Fix llm_int8_skip_modules\n\n* SWA inference\n\n* Update save.py\n\n* Update save.py\n\n* Update pyproject.toml\n\n* __version__\n\n* __version__\n\n* Update save.py\n\n* Update save.py\n\n* Update mistral.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/loader.py",
        "commit_date": "2024-01-25T17:19:17Z",
        "message": "Fix bugs (#129)\n\n* faster saving & inference\n\n* Update llama.py\n\n* Update save.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* fast inference\n\n* Update llama.py\n\n* Update save.py\n\n* Update llama.py\n\n* Mistral correct RoPE scaling\n\n* Max sequence lengths\n\n* Apache 2\n\n* fast_linear_forward\n\n* Update utils.py\n\n* Update utils.py\n\n* No print\n\n* Update utils.py\n\n* Update utils.py\n\n* inference\n\n* Update llama.py\n\n* Fast inference RoPE\n\n* Update llama.py\n\n* Update llama.py\n\n* RoPE\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* LoRA\n\n* Fast LoRA saving\n\n* Update llama.py\n\n* hidden_states\n\n* q_len == 1\n\n* q_len issue\n\n* Update mistral.py\n\n* Update mistral.py\n\n* incorrect inference\n\n* Update to transformers 4.37\n\n* Graceful FA2 error + torch 2.1.1\n\n* Update mapper.py\n\n* Update pyproject.toml\n\n* Fix saving and bnb-4bit\n\n* Update fast_lora.py\n\n* Update fast_lora.py\n\n* remove patching\n\n* Update llama.py\n\n* Update llama.py\n\n* Update swiglu.py\n\n* Repatch\n\n* Update fast_lora.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/loader.py",
        "commit_date": "2024-01-20T12:23:00Z",
        "message": "Hotfix for Jan 2024 Release (#110)\n\n* Fix tokenizer, dropout, bias for LoRA\n\n* Update loader.py\n\n* Fix LoRA downcasting\n\n* Update _utils.py\n\n* Saving to GGUF\n\n* fix\n\n* colab_quantize_to_gguf\n\n* move save modules\n\n* save module\n\n* Update __init__.py\n\n* Update save.py\n\n* Temp downgrade due to TRL issue\n\n* Fix up bugs\n\n* Faster saving + other changes\n\n* Update llama.py\n\n* Saving modules\n\n* spelling\n\n* Update llama.py\n\n* Update save.py\n\n* Update save.py\n\n* Update loader.py\n\n* Update llama.py\n\n* patch saving\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* patch saving\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* original_model\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* saving to RAM leakage?\n\n* Update save.py\n\n* new_save_directory\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update pyproject.toml\n\n* Update pyproject.toml\n\n* Update pyproject.toml\n\n* Quick fixes\n\n* Update llama.py\n\n* Update llama.py\n\n* Update dpo.py\n\n* Update dpo.py\n\n* Update llama.py\n\n* Update save.py\n\n* getattr\n\n* RSLoRA and LoftQ direct support\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Fix DPO + GGUF\n\n* Fix quantization_method\n\n* Fix quantization_config\n\n* patch model\n\n* Update llama.py\n\n* Update llama.py\n\n* Update llama.py\n\n* Update save.py\n\n* Update save.py\n\n* tokenizer_save_settings\n\n* Update save.py\n\n* quantization and loftq\n\n* Update save.py\n\n* Update llama.py\n\n* Update save.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/loader.py",
        "commit_date": "2024-01-19T11:52:30Z",
        "message": "Quick fixes (#101)\n\n* Fix tokenizer, dropout, bias for LoRA\n\n* Update loader.py\n\n* Fix LoRA downcasting\n\n* Update _utils.py\n\n* Saving to GGUF\n\n* fix\n\n* colab_quantize_to_gguf\n\n* move save modules\n\n* save module\n\n* Update __init__.py\n\n* Update save.py\n\n* Temp downgrade due to TRL issue\n\n* Fix up bugs\n\n* Faster saving + other changes\n\n* Update llama.py\n\n* Saving modules\n\n* spelling\n\n* Update llama.py\n\n* Update save.py\n\n* Update save.py\n\n* Update loader.py\n\n* Update llama.py\n\n* patch saving\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* patch saving\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* original_model\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* saving to RAM leakage?\n\n* Update save.py\n\n* new_save_directory\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update pyproject.toml\n\n* Update pyproject.toml\n\n* Update pyproject.toml\n\n* Quick fixes\n\n* Update llama.py\n\n* Update llama.py\n\n* Update dpo.py\n\n* Update dpo.py\n\n* Update llama.py\n\n* Update save.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/loader.py",
        "commit_date": "2024-01-18T17:51:19Z",
        "message": "2024 Release (#96)\n\n* Fix tokenizer, dropout, bias for LoRA\n\n* Update loader.py\n\n* Fix LoRA downcasting\n\n* Update _utils.py\n\n* Saving to GGUF\n\n* fix\n\n* colab_quantize_to_gguf\n\n* move save modules\n\n* save module\n\n* Update __init__.py\n\n* Update save.py\n\n* Temp downgrade due to TRL issue\n\n* Fix up bugs\n\n* Faster saving + other changes\n\n* Update llama.py\n\n* Saving modules\n\n* spelling\n\n* Update llama.py\n\n* Update save.py\n\n* Update save.py\n\n* Update loader.py\n\n* Update llama.py\n\n* patch saving\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* patch saving\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* original_model\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* saving to RAM leakage?\n\n* Update save.py\n\n* new_save_directory\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update save.py\n\n* Update pyproject.toml\n\n* Update pyproject.toml\n\n* Update pyproject.toml"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/loader.py",
        "commit_date": "2024-01-09T14:02:44Z",
        "message": "fix_tokenizer"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/loader.py",
        "commit_date": "2024-01-09T12:40:43Z",
        "message": "check_tokenizer"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/loader.py",
        "commit_date": "2024-01-06T08:13:39Z",
        "message": "Fix tokenizer, bias, dropout supported for LoRA (#69)\n\n* Fix tokenizer, dropout, bias for LoRA\n\n* Update loader.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/loader.py",
        "commit_date": "2023-12-31T07:36:43Z",
        "message": "PatchDPOTrainer"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/loader.py",
        "commit_date": "2023-12-31T06:57:45Z",
        "message": "DPO"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/loader.py",
        "commit_date": "2023-12-27T17:19:12Z",
        "message": "Nightly (#56)\n\n* Pytorch 2.1.1 install path, 4bit loading\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Update loader.py\n\n* Update loader.py\n\n* Update loader.py\n\n* Update loader.py\n\n* Spelling errors\n\n* Update __init__.py"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/loader.py",
        "commit_date": "2023-12-26T16:01:45Z",
        "message": "Fix FastLanguageModel"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/loader.py",
        "commit_date": "2023-12-22T17:22:48Z",
        "message": "Small fixes (#48)\n\n* Fix generation for GQA\n\n* Update _utils.py\n\n* flash attn\n\n* Update _utils.py\n\n* Update llama.py\n\n* Update mistral.py\n\n* platform\n\n* Update _utils.py\n\n* Update llama.py\n\n* Logo changed\n\n* Update README.md\n\n* Update README.md"
    },
    {
        "repo_url": "github.com/unslothai/unsloth",
        "filepath": "unsloth/models/loader.py",
        "commit_date": "2023-12-17T17:23:16Z",
        "message": "Torch version, docs, readme, general loader"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_poly.py",
        "commit_date": "2024-02-14T15:43:47Z",
        "message": "TST Use plain asserts in tests (#1448)\n\nUse pytest style asserts instead of unittest methods.\n\nUse `pytest.raises` and `pytest.warns` where suitable."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_poly.py",
        "commit_date": "2024-01-12T16:19:12Z",
        "message": "FEAT Add Poly Adapter (#1129)\n\nImplement the Poly (Polytropon) adapter.\n\nPapers:\n\n- https://arxiv.org/abs/2202.13914\n- https://arxiv.org/abs/2211.03831\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/OpenBMB/BMTools",
        "filepath": "bmtools/models/lora_model.py",
        "commit_date": "2023-06-05T08:47:12Z",
        "message": "add lora model"
    },
    {
        "repo_url": "github.com/OpenBMB/BMTools",
        "filepath": "bmtools/models/lora_model.py",
        "commit_date": "2023-06-05T08:27:01Z",
        "message": "add lora model"
    },
    {
        "repo_url": "github.com/LlamaFamily/Llama-Chinese",
        "filepath": "train/merge_peft_model/merge_muilt_peft_adapter.py",
        "commit_date": "2023-08-16T06:54:49Z",
        "message": "add merge model"
    },
    {
        "repo_url": "github.com/microsoft/LMOps",
        "filepath": "minillm/transformers/src/transformers/modeling_utils.py",
        "commit_date": "2023-11-16T12:47:19Z",
        "message": "upgrade transformers"
    },
    {
        "repo_url": "github.com/microsoft/LMOps",
        "filepath": "minillm/transformers/src/transformers/modeling_utils.py",
        "commit_date": "2023-08-16T03:11:32Z",
        "message": "upgrade transformers"
    },
    {
        "repo_url": "github.com/microsoft/LMOps",
        "filepath": "minillm/transformers/src/transformers/modeling_utils.py",
        "commit_date": "2023-06-14T11:44:35Z",
        "message": "add minillm"
    },
    {
        "repo_url": "github.com/LianjiaTech/BELLE",
        "filepath": "train/src/entry_point/zero_inference.py",
        "commit_date": "2023-09-26T11:39:39Z",
        "message": "update parameter name and bug fix (#523)\n\n* update docker\n\n* update parameter name and bug fix\n\n* delete redundancy file"
    },
    {
        "repo_url": "github.com/LianjiaTech/BELLE",
        "filepath": "train/src/entry_point/zero_inference.py",
        "commit_date": "2023-08-10T10:27:54Z",
        "message": "add zero inference"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_config.py",
        "commit_date": "2024-02-14T15:43:47Z",
        "message": "TST Use plain asserts in tests (#1448)\n\nUse pytest style asserts instead of unittest methods.\n\nUse `pytest.raises` and `pytest.warns` where suitable."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_config.py",
        "commit_date": "2024-02-07T11:52:35Z",
        "message": "MNT Move code quality fully to ruff (#1421)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_config.py",
        "commit_date": "2024-01-29T06:25:01Z",
        "message": "add peft type constructor (#1398)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_config.py",
        "commit_date": "2024-01-12T16:19:12Z",
        "message": "FEAT Add Poly Adapter (#1129)\n\nImplement the Poly (Polytropon) adapter.\n\nPapers:\n\n- https://arxiv.org/abs/2202.13914\n- https://arxiv.org/abs/2211.03831\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_config.py",
        "commit_date": "2023-12-06T14:11:00Z",
        "message": "Release: 0.7.0 (#1214)\n\nIn preparation for the 0.7.0 release. Also remove obsolete TODO\ncomments."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_config.py",
        "commit_date": "2023-11-30T15:58:42Z",
        "message": "[Feature] Support OFT (#1160)\n\n* Support OFT\n\n* add test\n\n* Update README\n\n* fix code quality\n\n* fix test\n\n* Skip 1 test\n\n* fix eps rule and add more test\n\n* feat: added examples to new OFT method\n\n* fix: removed wrong arguments from model example\n\n* fix: changed name of inference file\n\n* fix: changed prompt variable\n\n* fix docs\n\n* fix: dreambooth inference revision based on feedback\n\n* fix: review from BenjaminBossan\n\n* apply safe merge\n\n* del partially\n\n* refactor oft\n\n* refactor oft\n\n* del unused line\n\n* del unused line\n\n* fix skip in windows\n\n* skip test\n\n* Add comments about bias added place\n\n* rename orig_weights to new_weights\n\n* use inverse instead of linalg.inv\n\n* delete alpha and scaling\n\n---------\n\nCo-authored-by: Lukas Kuhn <lukaskuhn.lku@gmail.com>\nCo-authored-by: Lukas Kuhn <lukas.kuhn@deutschebahn.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_config.py",
        "commit_date": "2023-11-07T10:44:27Z",
        "message": "Improve documentation for IA\u00b3 (#984)\n\n- Improve ia3 documentation\n- Raise value error for incorrect feedforward_module list\n- Added tests\n\n---------\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_config.py",
        "commit_date": "2023-11-06T13:04:19Z",
        "message": "FIX: fix adaptation prompt CI and compatibility with latest transformers (4.35.0) (#1084)\n\n* fix adaptation prompt CI\n\n* undo some other changes"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_config.py",
        "commit_date": "2023-11-03T14:52:51Z",
        "message": "FIX: Skip adaption prompt tests with new transformers versions (#1077)\n\nAdaption prompt is failing with transformers v4.35.0. This PR skips the\nadaption prompt tests so that CI is green again. The PR also adds an\nerror when users try to use adaption prompt with that version,\ninstructing them to use an older transformers version instead.\n\nThis should be removed as soon as the issue is fixed in\nPEFT/transformers."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_config.py",
        "commit_date": "2023-11-01T10:39:40Z",
        "message": "TST test coverage for layer matching (#1031)\n\nAdd tests for module name matching using regex and other custom arguments."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_config.py",
        "commit_date": "2023-10-24T10:06:01Z",
        "message": "Fix target_modules type in config.from_pretrained (#1046)\n\nFixes #1045, supersedes #1041\n\nDescription\n\nWhen loading a config from a file, we currently set the loaded\nattributes on the config directly. However, this sidesteps the\n__post_init__ call, which is required to convert the target_modules to a\nset. This PR fixes this by avoiding to set attributes on the config\nclass directly, instead of going through __init__.\n\nOther changes\n\nWhile working on this, I did a slight refactor of the config tests.\n\n1. All config classes are included now (some where missing before).\n2. Use parameterized instead of looping through the classes.\n3. Added a unit test for the aforementioned bug."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_config.py",
        "commit_date": "2023-07-19T14:08:29Z",
        "message": "Fixes warning when initializing prompt encoder (#716)\n\nRight now, when the user initializes a prompt encoder with MLP, they get\na warning that a certain argument is ignored, and there is no possible\nvalue for the argument that would stop the warning. Usually, warnings\nare for issues that something is (probably) going wrong, but here,\neverything is going as expected. Therefore, by default, I would not give\nthis warning, thus avoiding users getting confused.\n\nHowever, I would still give the warning if the user set the argument for\nencoder_num_layers explicitly to a different value. In that case, they\nexpect the change to make a difference, but since the argument is\nignored, their expectation is not met, which warrants a warning."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_config.py",
        "commit_date": "2023-07-13T07:45:50Z",
        "message": "Add functionality to support IA3 (#578)\n\n* Added initial ia3 code\n\n* Implemented ia3 correctly for feedforward layers; Fixed regex matching\n\n* Fixed module mapping for mt5\n\n* Merged changes from huggingface:main\n\n* Merged changes\n\n* Fixed lora merge conflicts\n\n* Different bloom config\n\n* Added save option for ia3\n\n* Added loading code for ia3\n\n* Added feedforward implementation in utils and seq cls example\n\n* Added feedforward implementation in utils and seq cls example\n\n* Implemented merge, unmerge, enable/disable adapters functionality\n\n* Fixed feedforward during merge\n\n* Debugging Merge\n\n* Removing debug messages\n\n* Cleaned up repo\n\n* Removed non-IA3 changes\n\n* Refactor save and load\n\n* Added support to all models in tests; Added IA3Config for common tests\n\n* Added half-precision support and test for gradient checkpointing; Formatted jupyter notebooks\n\n* Added target modules for new models GPTBigCode and LLama\n\n* Cleaned up code\n\n* Cleaned up code\n\n* Cleaned up example notebook\n\n* Cleaned up  seq2seq notebook\n\n* Corrected function docstrings; refactored find_and_replace\n\n* Corrected function docstrings; refactored find_and_replace\n\n* Added basic docs for IA3\n\n* Added new conceptual guide in source tree for documentation\n\n* Minor fix to documentation\n\n* Minor fixes to docstrings; Added error handling for 4bit quantization; Cleaned unused merge/unmerge methods\n\n* styling changes after merge from main\n\n* Update src/peft/tuners/ia3.py\n\nRemove unused attribute merge_weights\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Abhishek2304 <abhishekgupta2304@gmail.com>\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_config.py",
        "commit_date": "2023-07-06T07:06:41Z",
        "message": "Fix bug resulting in config copies not working (#653)\n\nResolves #424\n\nThe bug was caused by __dict__ being overwritten to return a copy of the\ndataclass. This can lead to unpredictable behavior, as shown in the\nissue. This fix removes the __dict__ property and preservers the\noriginal behavior where needed.\n\nAll three added tests would fail without the fix."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_config.py",
        "commit_date": "2023-06-15T10:23:05Z",
        "message": "[`core`] Correctly passing the kwargs all over the place (#575)\n\n* v1 of the fix\n\n* forward contrib credits from discussions\n\n* add tests\n\n---------\n\nCo-authored-by: winglian <winglian@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_config.py",
        "commit_date": "2023-06-01T09:09:54Z",
        "message": "Enable PeftConfig & PeftModel to load from revision (#433)\n\n* Enable PeftConfig to load from revision\n\n* Add revision to PeftModel\n\n* Fix weights download with revision"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_config.py",
        "commit_date": "2023-04-25T06:54:18Z",
        "message": "Implement adaption prompt from Llama-Adapter paper (#268)\n\n* Implement adaption prompt from Llama-Adapter paper\n\n* Support multi-adapters\n\n* Refactor adaption prompt to target attn modules instead of layers\n\n* Refactor adaption prompt to be more generic\n\n* Fix adaption prompt not on right device\n\n* Apply suggestions from code review\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>\n\n* Fix style\n\n* Add support for Llama config use_cache=True\n\n* Fix rebase issues\n\n---------\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_config.py",
        "commit_date": "2023-01-30T08:01:01Z",
        "message": "fixes"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_config.py",
        "commit_date": "2023-01-29T10:49:31Z",
        "message": "adapt from code review\n\n- remove `README`\n- inherit from `dataclass`\n- add new test"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_config.py",
        "commit_date": "2023-01-26T11:17:24Z",
        "message": "working v1\n\n- push to hub method works\n- add tests\n- add config super class\n- add Lora support for `from_pretrained`"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_config.py",
        "commit_date": "2023-01-26T10:12:51Z",
        "message": "add config tests"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "chatpdf.py",
        "commit_date": "2024-03-01T09:30:23Z",
        "message": "update return pt."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "chatpdf.py",
        "commit_date": "2024-03-01T09:10:27Z",
        "message": "fix model has no tokenizer.apply_chat_template."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-15T03:20:37Z",
        "message": "update rag demo."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-15T03:09:08Z",
        "message": "update rag torch type."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-14T13:05:40Z",
        "message": "update rag."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-14T03:45:08Z",
        "message": "update rag."
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "inference.py",
        "commit_date": "2023-11-19T07:33:44Z",
        "message": "Update inference.py"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "inference.py",
        "commit_date": "2023-11-19T06:28:46Z",
        "message": "Update inference.py"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "inference.py",
        "commit_date": "2023-11-05T16:58:41Z",
        "message": "fix starting token repetition"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "inference.py",
        "commit_date": "2023-10-30T15:24:59Z",
        "message": "Update inference.py"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "inference.py",
        "commit_date": "2023-10-30T15:22:42Z",
        "message": "Update inference.py"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "inference.py",
        "commit_date": "2023-10-09T10:22:22Z",
        "message": "Update inference.py\n\n- Generated words are printed immediately by using TextStreamer."
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "inference.py",
        "commit_date": "2023-10-07T12:48:38Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "inference.py",
        "commit_date": "2023-10-03T03:44:07Z",
        "message": "Update inference.py"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "inference.py",
        "commit_date": "2023-10-03T03:12:16Z",
        "message": "Update inference.py"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "inference.py",
        "commit_date": "2023-09-24T15:27:08Z",
        "message": "Update inference.py"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "inference.py",
        "commit_date": "2023-09-24T15:25:25Z",
        "message": "Update inference.py"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "inference.py",
        "commit_date": "2023-09-21T18:13:56Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2024-02-21T18:36:10Z",
        "message": "Token accuracy metrics (#983)"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2024-02-09T17:02:30Z",
        "message": "Fix eval.py with lora (#965)\n\n* just remove it?\n\n* or not\n\n* fix\n\n* fix up\n\n* clean up\n\n* fix example yaml\n\n* precommit\n\n* add test"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2024-02-02T21:52:55Z",
        "message": "Switch to the Composer integration of LoRA (works with FSDP) (#886)"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-12-20T00:57:14Z",
        "message": "Updating the Flash Attention version to fix cross entropy loss (#812)\n\n* ..\n\n* ..\n\n* ..\n\n* ..\n\n* .."
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-12-15T23:42:35Z",
        "message": "Clean up the logs, bump datasets and transformers (#804)"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-11-02T22:38:50Z",
        "message": "Fix HF local module copy contention with a meta init on local rank 0 (#710)"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-10-24T18:25:35Z",
        "message": "Allow flash attention 2 and upgrade to transformers 4.34.1 (#672)\n\n* more special casing in tokenizer equivalence check\n* fix addedtoken -> str\n* add lazy load option\n* add gc collect\n* updates for the patch release\n* add documentation for flash attention options"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-10-03T21:56:02Z",
        "message": "Fix overriding of rope_scaling config (#644)"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-10-03T15:45:13Z",
        "message": "Add flag to disable train metrics (#642)\n\n* free mem\n\n* lint\n\n* lint"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-09-26T23:28:03Z",
        "message": "Add code eval (#587)\n\nFinal commit of code eval support.\n\n---------\n\nCo-authored-by: Michael Carbin <michael.carbin@databricks.com>\nCo-authored-by: Daniel King <43149077+dakinggg@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-09-26T18:07:36Z",
        "message": "Add node rank to signal paths (#629)\n\n* add node rank\n\n* lint"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-09-13T00:08:56Z",
        "message": "Refactor logging (#234)\n\nReplaces most print statements with proper logging. Deprecates the `verbose` argument in favor of using the `python_log_level` argument that is also used by composer."
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-09-12T21:09:34Z",
        "message": "Fix ComposerHFCausalLM instantiation with PeftModel (#593)\n\n* Fix bug in hf_causal_lm, causing errors with evaluating peft models\n\n* Move attention patch\n\n* Fix typing"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-09-07T20:28:42Z",
        "message": "Update to transformers 4.32 (#561)\n\n* monkeypatch config for mpt\n* add error if trying to use mpt without trust remote code"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-08-25T04:11:11Z",
        "message": "Enable eval script for HuggingFace 8bit models (#516)\n\n* Enable eval script for HF 8bit models\n\n* nit\n\n* Address comments\n\n* Add eval yaml and checks\n\n* nit\n\n* pre-commit\n\n---------\n\nCo-authored-by: Daniel King <43149077+dakinggg@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-08-24T22:46:06Z",
        "message": "Revert \"Add Programming to Foundry (#441)\" (#557)\n\nThis reverts commit 52a3500ff54923919d7d8e88d05e3cafa3cf594d."
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-08-24T21:10:29Z",
        "message": "Add Programming to Foundry (#441)\n\n* add programming to the gauntlet\n\n* fix pre-commit\n\n* add to builders\n\n* remove programming from gauntlet yaml before v0.1\n\n* add language to human eval jsonl\n\n* test multilingual\n\n* add data\n\n* fix typo\n\n* revert yamls\n\n* add beams to yaml\n\n* final fixes\n\n* final fixes\n\n* upgrade composer\n\n* change beam #s\n\n* upgrade tensorbaord\n\n* remove c dataset:\n\n---------\n\nCo-authored-by: bcui19 <bcui8377@gmail.com>"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-08-15T16:53:46Z",
        "message": "Monkeypatch flash attention in for llama (#520)"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-08-02T18:20:04Z",
        "message": "Adding pyright to pre-commit (#477)"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-06-29T22:49:03Z",
        "message": "make peft installs a extra_dep (#397)\n\n* make peft installs a extra_dep\n\n* updt import error\n\n* allow peft import to fail\n\n* Update llmfoundry/models/hf/hf_causal_lm.py\n\nCo-authored-by: Sam Havens <samhavens@gmail.com>\n\n* updt\n\n* lint\n\n---------\n\nCo-authored-by: Sam Havens <samhavens@gmail.com>"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-06-27T19:42:54Z",
        "message": "Feature/peft compatible models (#346)\n\n* ignore venv in dir\n\n* adding a lean ComposerHFCausalLMFromPython that converts a loaded hf model into a composer one\n\n* typechecking the composer convertor, import peft and transformers.\n\n* support for input_embeds in forward calls for peft compatibility\n\n* fixing inputs_embeds typos\n\n* precommit fixes docs\n\n* refactored hf causal\n\n* removed python convertor from inits\n\n* wip train.py\n\n* added lora deps\n\n* removed 8 bit defaults\n\n* Update llmfoundry/models/mpt/modeling_mpt.py\n\nCo-authored-by: Sam Havens <samhavens@gmail.com>\n\n* precommit edits models\n\n* Update llmfoundry/models/mpt/modeling_mpt.py\n\nCo-authored-by: Mihir Patel <mihir.v.patel7@gmail.com>\n\n* delete deprecated hf class from init\n\n* removed 8-bit and device map support for now\n\n* formatting the peft builder for precommit\n\n* fixed comments on model ifs\n\n* added a util for printing trainable params\n\n* deps pinned down and sent to gpu\n\n* scipy dep for bitsandbytes\n\n* sent lora deps to regular install_requires\n\n* pinned down scipy\n\n---------\n\nCo-authored-by: danbider <dan@mosaicml.com>\nCo-authored-by: Mihir Patel <mihir.v.patel7@gmail.com>\nCo-authored-by: Cody Blakeney <cjb92@txstate.edu>\nCo-authored-by: Sam Havens <samhavens@gmail.com>\nCo-authored-by: Cody Blakeney <cody@mosaicml.com>"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-06-13T22:57:27Z",
        "message": "Huggingface Mixed Initialization (#303)\n\nAdding code s.t. Huggingface mixed initialization should work.\n---------\n\nCo-authored-by: Karan Jariwala <karankjariwala@gmail.com>\nCo-authored-by: Daniel King <daniel@mosaicml.com>\nCo-authored-by: Daniel King <43149077+dakinggg@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-06-06T17:20:40Z",
        "message": "add shift_labels arg to HF wrappers (#288)"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-05-26T00:59:51Z",
        "message": "Removing deprecated vocabulary size parameter from composer CE metrics (#222)\n\n* Removing deprecated vocab param from composer metric\n\n* Cleaning up formatting\n\n* Removing all instances of passing in vocab size"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-05-24T23:46:55Z",
        "message": "torch2 updt with hf fixes (#193)\n\n* fix and test\n\n* Revert \"Revert \"Torch2 (#177) (#178)\" (#181)\"\n\nThis reverts commit 89f56d2df5f4c0ea10e3209ef0d9bd58dbf05565.\n\n* updt import try except\n\n* updt hf model\n\n* updt imports\n\n* lint\n\n* add mpt hf model init / gen test\n\n* updt for temp testing\n\n* lint\n\n* rerun tests\n\n* Update .github/workflows/release.yaml\n\nCo-authored-by: Daniel King <43149077+dakinggg@users.noreply.github.com>\n\n* Update tests/test_hf_mpt_gen.py\n\nCo-authored-by: Daniel King <43149077+dakinggg@users.noreply.github.com>\n\n* add cpu test\n\n* updt tests / cpu img\n\n* updt cpu test install\n\n* rerun tests\n\n* fix hf import structure\n\n* fix test\n\n* pull_request -> pull_request_target\n\n* make onnx test smaller\n\n---------\n\nCo-authored-by: Daniel King <daniel@mosaicml.com>\nCo-authored-by: Daniel King <43149077+dakinggg@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-05-11T02:31:50Z",
        "message": "hf dict cfg overrides (#90)\n\n* hf dict cfg overrides\n\n* custom dict overrides with checks; set all init defaults\n\n* pr cmt"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-05-10T17:49:47Z",
        "message": "Updates to prefixlm and t5 (#85)\n\n* Overlooked updates to prefixlm and t5\n\n* Tokenizer type check"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-05-04T20:04:38Z",
        "message": "Add LLaMa and MPT HFCausalLM support (#32)"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-05-04T17:50:35Z",
        "message": "Add ICL datasets, plus functionality for subcategories in datasets (#30)"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-05-02T20:24:16Z",
        "message": "Fix `ComposerHFCausalLM` initialization (#6)"
    },
    {
        "repo_url": "github.com/mosaicml/llm-foundry",
        "filepath": "llmfoundry/models/hf/hf_causal_lm.py",
        "commit_date": "2023-04-29T02:39:48Z",
        "message": "Refactor codebase for LLM only code"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/training/peft/peft_model.py",
        "commit_date": "2023-09-05T01:57:07Z",
        "message": "fix style"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/training/peft/peft_model.py",
        "commit_date": "2023-09-05T01:47:50Z",
        "message": "fix style"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/training/peft/peft_model.py",
        "commit_date": "2023-09-05T01:31:38Z",
        "message": "fix style"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/training/peft/peft_model.py",
        "commit_date": "2023-09-04T08:00:57Z",
        "message": "fix style"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/training/peft/peft_model.py",
        "commit_date": "2023-09-04T06:02:25Z",
        "message": "Add support for kbits training"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_hub_features.py",
        "commit_date": "2024-02-14T15:43:47Z",
        "message": "TST Use plain asserts in tests (#1448)\n\nUse pytest style asserts instead of unittest methods.\n\nUse `pytest.raises` and `pytest.warns` where suitable."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_hub_features.py",
        "commit_date": "2024-02-07T11:52:35Z",
        "message": "MNT Move code quality fully to ruff (#1421)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_hub_features.py",
        "commit_date": "2023-07-19T05:47:15Z",
        "message": "Fix subfolder issue (#721)\n\n* fix subfolder issue\n\n* added tests"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "tests/ludwig/encoders/test_llm_encoders.py",
        "commit_date": "2024-01-19T21:03:20Z",
        "message": "Enable IA3 adapters in `LLMEncoder` (#3902)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "tests/ludwig/encoders/test_llm_encoders.py",
        "commit_date": "2024-01-19T18:36:25Z",
        "message": "Cast `LLMEncoder` output to `torch.float32`, freeze final layer at init. (#3900)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "tests/ludwig/encoders/test_llm_encoders.py",
        "commit_date": "2024-01-19T18:06:04Z",
        "message": "Enable AdaLoRA tests for LLM adapter (#3896)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "tests/ludwig/encoders/test_llm_encoders.py",
        "commit_date": "2024-01-11T17:26:09Z",
        "message": "Add custom `prepare_for_trianing` logic to ECD model for LLM encoder adapter initialization (#3874)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "tests/ludwig/encoders/test_llm_encoders.py",
        "commit_date": "2023-12-20T22:07:29Z",
        "message": " fix: Handle missing and unexpected keys during LLMEncoder state dict load, part 2 (#3843)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "tests/ludwig/encoders/test_llm_encoders.py",
        "commit_date": "2023-12-20T00:40:41Z",
        "message": "fix: Handle missing and unexpected keys during LLMEncoder state dict load (#3841)"
    },
    {
        "repo_url": "github.com/ludwig-ai/ludwig",
        "filepath": "tests/ludwig/encoders/test_llm_encoders.py",
        "commit_date": "2023-12-15T00:04:03Z",
        "message": "Add LLM Text Encoder (#3828)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2024-02-27T11:02:11Z",
        "message": "FEAT Implement DoRA (#1474)\n\nAdd DoRA (Weight-Decomposed Low-Rank Adaptation).\n\nhttps://arxiv.org/abs/2402.09353\n\nTo use this with LoRA, add use_dora=True to the LoraConfig.\n\nCurrently only supports nn.Linear layers, not other types or\nquantized linear layers like bnb."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2024-02-26T09:37:36Z",
        "message": "FIX Safe merging with LoHa and LoKr (#1505)\n\nThere was a small bug when merging the LoHa and LoKr tuners with\nsafe_merge=True due to a missing clone call. This is now fixed.\n\nFurthermore, the test coverage for merging with LoHa and LoKr has been\nextended, as there were a few tests where these methods were excluded\nunnecessarily."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2024-02-20T14:12:34Z",
        "message": "FIX Correctly unload double wrapped modules (#1490)\n\nResolves #1485, but note that some additional solutions are mentioned in\nthet issue.\n\nThis checks that when unloading a PEFT model, if the\nModulesToSaveWrapper contains a tuner module, it is correctly unloaded.\nThe unloaded model should not have PEFT layers at the end."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2024-02-14T15:43:47Z",
        "message": "TST Use plain asserts in tests (#1448)\n\nUse pytest style asserts instead of unittest methods.\n\nUse `pytest.raises` and `pytest.warns` where suitable."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2024-02-07T13:59:48Z",
        "message": "TST Improve test coverage by skipping fewer tests (#1445)\n\nMany of the common tests are skipped because of lines such as:\n\nif config_cls not in (LoraConfig, IA3Config):\n    return\n\nThese lines were often added before we had more PEFT methods like OFT,\nLoHa, etc. However, these new methods should also pass the common tests.\nTherefore, I relaxed many of these conditions so that they would not\nskip the new methods.\n\nNote:\n\nThere were a handful of test cases that failed. I added TODO comments\nfor those, as it was unclear to me why they failed. As investigating\nthis could take some time, I chose not to fix those cases in this PR."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2024-02-07T11:52:35Z",
        "message": "MNT Move code quality fully to ruff (#1421)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2024-01-31T06:52:04Z",
        "message": "FIX: Make merging of adapter weights idempotent (#1355)\n\n* FIX Make merging of adapter weights idempotent\n\nRight now, merging of adapters weights such as LoRA and IA\u00b3 is not\nidempotent. This means that if a user calls merge multiple times, the\nresulting weights will be different each time because the delta weights\nare added again and again.\n\nThis fix checks that only those adapters are merged that are not yet\nmerged. Also, gives a more precise warning:\n\n- Say when there is nothing to merge.\n- If there are some adapters to merge, only mention those\n\nThis bug is more subtle than it may seem at first, since we sometimes\nmerge implicitly without the user necessarily being aware of it (e.g.\nwhen calling merge_and_unload). Therefore, this bug can occur quite\neasily, even if the user does not explicitly call merge twice in a row.\n\n* Make style"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-12-12T16:53:36Z",
        "message": "Release: 0.7.1 (#1257)\n\nAlso fix some more seeds to prevent flakiness"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-11-30T15:58:42Z",
        "message": "[Feature] Support OFT (#1160)\n\n* Support OFT\n\n* add test\n\n* Update README\n\n* fix code quality\n\n* fix test\n\n* Skip 1 test\n\n* fix eps rule and add more test\n\n* feat: added examples to new OFT method\n\n* fix: removed wrong arguments from model example\n\n* fix: changed name of inference file\n\n* fix: changed prompt variable\n\n* fix docs\n\n* fix: dreambooth inference revision based on feedback\n\n* fix: review from BenjaminBossan\n\n* apply safe merge\n\n* del partially\n\n* refactor oft\n\n* refactor oft\n\n* del unused line\n\n* del unused line\n\n* fix skip in windows\n\n* skip test\n\n* Add comments about bias added place\n\n* rename orig_weights to new_weights\n\n* use inverse instead of linalg.inv\n\n* delete alpha and scaling\n\n---------\n\nCo-authored-by: Lukas Kuhn <lukaskuhn.lku@gmail.com>\nCo-authored-by: Lukas Kuhn <lukas.kuhn@deutschebahn.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-11-29T13:58:41Z",
        "message": "Training PEFT models with new tokens being added to the embedding layers and tokenizer (#1147)\n\n* add support for saving base layers weights along with adapter weights\n\n* Update save_and_load.py\n\n* Add an example showing the usage of the added feature\n\n* refactor the functionality\n\n* fix\n\n* refactoring code\n\n1. Add `is_embedding_layer_resized` parameter to `save_pretrained`\n2. Fix the deduplication in README when adding PEFT details.\n3. `save_pretrained` should only save the model when `is_main_process=True` which is one of the parameters of `save_pretrained`.\n\n* update example\n\n* fix the model card\n\n* fix model card\n\n* \ud83d\ude05\n\n* fix model card\n\n* automate setting `is_embedding_layer_resized`\n\n* nits\n\n* Update peft_lora_clm_with_additional_tokens.ipynb\n\n* add test\n\n* fix tests\n\n* maybe fixes the issue?\n\n* address comments\n\nCo-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-11-16T11:45:12Z",
        "message": "Refactor base layer pattern (#1106)\n\nDescription\n\nRefactor all tuners (where it applies, i.e. not prompt tuning) to use\nthe \"base layer pattern\". This means that the adapter layer will always\nhold a reference to the original layer that it modifies. This pattern is\nalready partly used (e.g. LoRA bnb, gptq layers), now it is consistently\nused everywhere when applicable.\n\nThis PR is a companion PR to #1069, where I first added these changes.\nThey are now extracted to a separate PR to make code review easier and\nto advance more quickly.\n\nImplementation\n\nThe main change is that the adapter layer wraps the original layer and\ncalls forward on that layer, instead of doing stuff like this:\n\nF.linear(input, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n\nwhich completely circumvents the call to the target layer's forward\nmethod. With the base layer pattern, we now call the target layer's\nforward method. Therefore, if the target layer is another adapter\nlayer (which will be crucial for mixed adapters), we call its forward\nmethod correctly. Also, this should allow passing extra arguments, like\nlora_scale to forward.\n\nThis change has the nice side benefit that we no longer need to use\n_init_empty_weights -- in fact, we don't initialize any of the target\nlayer's weights anymore, since we have a reference to it. There is thus\nno risk of having slow but superfluous initialization of layers.\n\nMoreover, I could greatly simplify merge_and_unload by just using the\nbase_layer instead of having to create a completely new layer. For\nOPT-350m, this results in a 15x speedup.\n\nNote that same as for the bnb layers, this should be backwards\nincompatible, since the adapter weights and their state_dicts are not\naffected by this change. I used #1115 for regression testing.\n\nSomewhat unrelated changes\n\nDuring debugging, I got very annoyed with the fact that the reprs of\nadapter layers and normal PyTorch layers are hard to distinguish, e.g.\nthe type is just \"Linear\". Now, for adapter layers, it is prefixed by\nthe adapter type, e.g. \"lora.Linear\". This should have no further\nimplications except for the repr (e.g. state_dict remains unaffected).\n\nFor LoHa and LoKr, I had to change the init of weights when using\ninit_weights=False. This is because of what is discussed in Numerical\ninstabilities with LoHa #1058.\n\nIA\u00b3 now has the unload method too.\n\nLoHa and LoKr now support safe_merge=True when merging layers.\n\nMigration guide\n\nFor 99% of users, the code should continue working as ususal, because\nthe API stays the same. Only low level details have been changed.\n\nCode that relies on isinstance checks on specific PEFT classes may\nbreak. E.g. the LoRA Linear layer no longer inherits from nn.Linear. It\nis, however, still a BaseTunerLayer. The same logic applies for other\nlayer types like Conv2d and for other tuners like IA\u00b3.\n\nTo retrieve the base layer of an adapter layer, you should now call\nmodule.get_base_layer() if you deal with a BaseTunerLayer. Don't rely on\nsomething like module.weight being present (though it might be)."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-11-16T11:05:22Z",
        "message": "FEAT: Merging only specified `adapter_names` when calling `merge` (#1132)\n\n* working v1\n\n* add tests\n\n* remove\n\n* add it also for lokr and loha, left a todo\n\n* Update tests/testing_common.py\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>\n\n* better test\n\n* up\n\n* fix tests\n\n* credits contrib and suggestions from disscussions\n\n* credits contrib and suggestions from disscussions\n\n* address last comments\n\n---------\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-11-15T10:21:23Z",
        "message": "FEAT: Make safe serialization the default one (#1088)\n\n* make safe serialization the default one\n\n* adapt tests\n\n* fix final tests'\n\n* adapt from suggestion"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-11-14T12:14:49Z",
        "message": "TST Improve requires grad testing: (#1131)\n\nPreviously, the corresponding tests were testing only whether specific\nparameters had requires_grad True or False. Now, all parameters are\nbeing checked. This is more rigorous.\n\nAlso, tests for Embedding, Conv1D, Conv2d were added, thus superseding\nPR #1115.\n\nFinally, tests for LoHa and LoKr were added.\n\nNote\n\nI considered moving the tests to a separate module, as they were getting\nquite big and this would help with readability. For now, I left them in\nthe same module because it leads to a better diff view and is thus\neasier to review. LMK if I should move the tests to a separate file."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-11-10T12:33:56Z",
        "message": "Refactor adapter deletion (#1105)\n\nDescription\n\nThe job of deleting an adapter is now transferred to the adapter layer,\ninstead of the adapter model. This makes it easier for users or other\nlibraries who don't use the adapter model to delete adapters.\n\nImplementation\n\nThe code should now be more generic, relying less on hard-coded\nattributes.\n\nAs a precaution, I also changed the type of adapter_layer_names from\nlist to tuple, as it should not be mutated.\n\nWhen deleting the active adapter, the logic for choosing the new active\nadapter has been changed slightly to ensure consistency across layers.\nIn practice, this should rarely make a difference. An error is now\nraised if the last remaining adapter is deleted.\n\nTest coverage has been increased:\n\n- Deleting adapters is now also tested for custom models.\n- It is also tested for LoHa, LoKr, not only LoRA.\n- I added a test for deleting the non-active adapter.\n\nNot implemented\n\nI did not add adapter deletion to IA\u00b3, since it is included in #980. LMK\nif it should be added here instead."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-10-30T14:36:41Z",
        "message": "Add implementation of LyCORIS LoKr for SD&SDXL models (#978)\n\nKronA-like adapter"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-10-26T13:51:49Z",
        "message": "FIX Conv1D merge error for IA3 (#1014)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-10-24T13:26:42Z",
        "message": "FIX: wrong construction of LoHa weights (#1021)\n\nAlso: Update convert_sd_adapter_to_peft.py to account for a bug in\nLycoris-LoRA. See https://github.com/KohakuBlueleaf/LyCORIS/pull/115"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-10-13T10:23:16Z",
        "message": "FEAT: Add fp16 + cpu merge support (#1017)\n\n* add fp16 + cpu merge support\n\n* fix tests\n\n* add fp16 tests for custom models\n\n* fix tests\n\n* adapt from comments\n\n* more clarifications"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-10-09T10:20:19Z",
        "message": "ENH Support Conv2d layers for IA\u00b3 (#972)\n\nAdds support for Conv2D layers to the IA\u00b3 tuner. Tests are added to\ncheck that they work.\n\nNotes:\n\nUnfortunately, when unmerging the Conv2d IA\u00b3 layers, there is quite a\nbit of rounding error. I had to increase the tolerances for this\nspecific test case to make the tests pass. I'm not 100% sure why this\nis, but I could imagine that for Conv2d, small errors accumulate because\nof the convolution operation.\n\nI also added tests for IA\u00b3 Linear layers for the custom models, which\nalso pass. However, there is an error when using Conv1D. The reason is\nthat merging fails because there is a shape mismatch when\nfan_in_fan_out=True (which is set automatically for Conv1D). This is\nleft for a future PR."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-10-04T07:44:10Z",
        "message": "Add base model metadata to model card (#975)\n\nResolves #938\n\nThis PR adds the base model metadata, if present, to the model card.\n\nOn top of this, the code for creating the model card has been refactored\nto use the huggingface_hub classes instead of doing ad hoc parsing and\nwriting.\n---------\n\nCo-authored-by: Lucain <lucainp@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-10-03T11:23:33Z",
        "message": "FIX: issues with (un)merging multiple LoRA and IA\u00b3 adapters (#976)\n\n* Fix issues with merging multiple adapters\n\nThis should resolve the failing slow test\ntest_4bit_merge_and_disable_lora.\n\nWhile investigating, I also noticed that merging multiple adapters was\nnot correct for IA\u00b3. I added a test that should catch this bug and\nprovided a fix for it too. However, the test does not check IA\u00b3 at the\nmoment because the test parameters do not contain IA\u00b3. For this, #972\nneeds to be merged too, which adds IA\u00b3 to the test parameters.\n\n* Small adjustments to tests\n\nPreviously, tests had some exploding gradients, making them unstable."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-10-02T08:44:51Z",
        "message": "FEAT Add LyCORIS LoHa for SD&SDXL models (#956)\n\nhttps://arxiv.org/abs/2108.06098"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-09-29T04:14:30Z",
        "message": "[tests] add multiple active adapters tests (#961)\n\n* add tests for multiple active adapters\n\n* add multiple active adapter tests\n\n* fix tests\n\n* fix the device error\n\n* fix typo\n\n* fix the variables\n\n* fix the `adalora` config\n\n* add util function for proper naming of tests\n\n* fix bugs\n\n1. fix `add_weighted_adapter` when working with adapters targeting different layers\n2. fix `ia3` model and layer to handle adapters targeting different layers\n3. fix the multiple active adapter tests\n\n* fix `ia3` issue\n\n* remove debug statements\n\n* fix test\n\n* fix bug\n\n* address comments\n\n* address comments\n\nCo-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* fix tests\n\n* remove unused code\n\n* Update test_custom_models.py\n\n* increasing tolerance for a test\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-09-26T07:31:05Z",
        "message": "FIX: setting requires_grad on adapter layers (#905)\n\n* [WIP] Fix setting requires_grad on adapter layers\n\nThis is an alternative to #900, resolves #899.\n\nDescription\n\nCurrently, we don't handle setting requires_grad on adapter layers\nreally well. The main issue is that it can be set to True on adapter\nparameters that are not being used, e.g. the original_module in\nModulesToSaveWrapper or inactive adapters in LoRA.\n\nNormally, this is not a big issue, except maybe if we want to correctly\ncount the number of trainable parameters. However, when training with\nDistributedDataParallel, this results in errors, as PyTorch thinks that\nall parameters with requires_grad=True should participate in the loss\ncomputation, but those mentioned parameters don't. For that reason,\ntraining with DDP currently fails when using modules_to_save or multiple\nadapters.\n\nImplementation\n\nThis turned out to be more complicated than I initially thought. The\nlogic for setting requires_grad is all over the place, it was hard to\nencapsulate the logic and I only succeeded partially. As is, this PR is\nmore complex than the one it tries to supersede, #900, but it is also\n\"more correct\".\n\nTests were added to check whether requires_grad is set correctly. There\nare (so far) no tests for whether DDP indeed works, they could be added\nwith multi-GPU. I did, however, test an early stage of this PR with DDP\nand setting requires_grad correctly will indeed fix the DDP error.\n\nDONE/TODO\n\n- [x] ModulesToSaveWrapper\n- [x] LoRA\n- [ ] IA\u00b3\n- [ ] AdaLora\n\nSince some tuners are not implemented yet, tests are expected to fail.\nCheck the new tests at the bottom of test_custom.py, those should pass.\n\n* Refactor: move more requires_grad machinery to ABC\n\n* [skip ci] [WIP] Add requires_grad logic to IA\u00b3\n\n* Add AdaLora\n\n* Fix some minor issues\n\n* Make style"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-09-22T20:03:44Z",
        "message": "support multiple ranks and alphas for LoRA (#873)\n\n* support multiple ranks and alphas\n\n* Update lora.py\n\n* Update lora.py\n\n* commit suggestions\n\nCo-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* address comments\n\nCo-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* Fixed multirank + multialpha for sequential LoRAs, added correct support of LoRA-C3Lier conversion (#937)\n\n* Fixed multirank multialpha for sequential loras, added tests, fixed docs\n\n* Refactored kohya_ss conversion script for proper support of LoRA-C3Lier\n\n* Fixed styling\n\n* Removed old comment from docstring\n\n* shift `scale_layer`/`unscale_layer` to `LoraLayer` class to support all the child classes\n\n* support multiple active adapters\n\n* add `active_adapters` property\n\nCo-Authored-By: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* fix bug related to active adapter of `ModulesToSaveWrapper`\n\n* revert the change wrt active_adapter assignment\n\nCo-Authored-By: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* addressing comments\n\n* address comments\n\n* address comment\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\nCo-authored-by: Alexander Kovalchuk <kovalexal@gmail.com>\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-09-21T07:46:28Z",
        "message": "Fix some tests that would fail with torch.compile (#949)\n\nSome tests would currently fail with torch.compile, not because there is\nanything wrong with how PEFT works with compiled models, but simply\nbecause of the way the tests are written. This is because when models\nare compiled, the keys of the state dict change. Tests have now been\nadapted to unwrap the compiled model first before getting the state\ndict.\n\nNote that the mentioned issue does not affect saving and loading,\nbecause save_pretrained is already called on the original module, so\nthere is no issue with mismatched keys.\n\nAlso fixed the docstring of get_peft_model_state_dict."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-09-12T09:05:29Z",
        "message": "ENH speed up init emb conv2d (#915)\n\nPartly resolves #872\n\nDescription\n\nAfter getting faster initialization of the LoRA Linear layer,\ninitialization of Conv2D and Embedding is now sped up.\n\nImplementation\n\nThe approach of how to achieve the speed up has slightly changed\ncompared to last time. To refresh memory, in #887, we avoided the\nunnecessary initialization of the full weight matrix by completely\nskipping nn.Linear.__init__.\n\nAlthough it is possible to do the same for Embedding and Conv2d, we run\ninto some trouble here. The issue is that the __init__ methods of these\nclasses have quite a lot more arguments and some custom logic (i.e. not\nonly self.foo = foo but more on top). If we wanted to skip __init__\nentirely, we would have to basically copy all of that into our code.\nAlthough that is possible, it is brittle (e.g. the logic could be\ndifferent for different PyTorch versions or change over time).\n\nFor that reason, I opted to implement this differently, using a\nsuggestion we had discussed earlier. The approach is to call __init__ of\nthe parent class but enforce empty weights (this is what\ntorch.nn.utils.skip_init does, although we cannot use that function\ndirectly). This way, we can avoid having to copy the __init__ code while\nstill avoiding expensive initialization of the weights.\n\nI did not change the code for Linear to also use this approach because\nthe logic inside of Linear.__init__ is quite simple (at least for now),\nso we are good here with the existing approach.\n\nHowever, I was curious how changing the approach for Linear would affect\nthe initialization speed. Therefore, I ran the script from #872 again, 3\ntimes each.\n\nCurrent approach:\n\ntest 1 with model bert-base took 0.021 sec.\ntest 1 with model bert-base took 0.020 sec.\ntest 1 with model bert-base took 0.020 sec.\ntest 2 with model bloomz-1b7 took 0.030 sec.\ntest 2 with model bloomz-1b7 took 0.030 sec.\ntest 2 with model bloomz-1b7 took 0.030 sec.\n\nNew approach if applied to Linear:\n\ntest 1 with model bert-base took 0.038 sec.\ntest 1 with model bert-base took 0.039 sec.\ntest 1 with model bert-base took 0.038 sec.\ntest 2 with model bloomz-1b7 took 0.072 sec.\ntest 2 with model bloomz-1b7 took 0.048 sec.\ntest 2 with model bloomz-1b7 took 0.048 sec.\n\nThis shows that the new approach is indeed a bit slower than the\nexisting one, though still a lot faster than what we had before. IMHO, I\nthink we're safe to leave the code inside of Linear as is and benefit\nfrom the slightly better performance at the cost of slightly more\nfragile code."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-09-06T15:31:55Z",
        "message": "ENH Remove redundant initialization layer calls (#887)\n\nThis should lead to a big speedup when initializing LoRA layers.\n\n---------\n\nCo-authored-by: poedator <ruslansv@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-08-30T12:40:56Z",
        "message": "MNT Run tests that were skipped previously (#884)\n\nSome tests were skipped because of an issue with how LoRA weights were\ninitialized for embeddings. This issue has been fixed for some time now,\nso the tests no longer need to be skipped."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-08-16T08:57:38Z",
        "message": "TST: add test about loading custom models (#827)\n\nPrompted by #808, I added a test that shows that loading a trained\ncustom model works as expected. I only added this to custom models\nbecause it involves a few steps of training and I didn't want to slow\ndown tests too much. LMK if this should be added to all tests.\n\nIn addition, I renamed some custom model tests which had strange\nnames (probably caused by an overeager query-replace)."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-08-08T12:35:19Z",
        "message": "Add adapter error handling (#800)\n\nWhen a user tries to add a 2nd adapter, Lora and AdaLora make some checks to\nensure the new adapter is compatible with existing adapters. Currently, that\ncheck is performed halfway through the method. This means that if the check\nfails, the new adapter is partially applied, leaving the model in a bad state.\nThe main purpose of this PR is to ensure that the model state is correct after\nsuch a failure is encountered.\n\nTests were added to catch this potential bug.\n\nWhile working on this, I also did some related, but not strictly necessary\nchanges to the add_adapter methods:\n\n- Previously, the peft_config from the PeftModel was passed to the base\n  model. This meant that sometimes, the base model would hold a reference\n  to PeftModel.peft_config, but not always, as some base models would\n  create new dicts. This is problematic, because some code would rely on\n  the objects being the same. Now, they are never the same, leading to\n  more consistency.\n- I think that the check if multiple adapters have biases (which is not\n  supported) was accidentally removed by #749. It is added back in.\n- Add some type annotations\n- Extend docstrings to contain adapter_name"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-07-24T11:23:23Z",
        "message": "FIX: Disabling adapter works with modules_to_save (#736)\n\nResolves #493\n\nFor LoRA and IA\u00b3, there was a bug that even even using the\ndisable_adapter context, if the module was listed in modules_to_save,\nthe updated weights would be used instead of the original weights. This\nmeant that disable_adapter would not return the same results as the base\nmodel without adaptation. This PR fixes the issue and provides a test.\n\nNote: I tried to adjust AdaLoRA too, since it seemed that the same\nreasoning should apply there. However, I think that AdaLoRA does not\nreally support disabling adapters at all. E.g. there is no\ndisable_adapter_layers method. Therefore, AdaLoRA was not changed."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-07-24T08:34:39Z",
        "message": "ENH: Warn when disabling adapters and bias != 'none' (#741)\n\nFor LoRA, given that bias='all' or bias='none', when doing inference\nwith a model in the disable_adapter context, the output will not be\nidentical to the output of the base model. This may be surprising to\nusers. Therefore, a warning is given. Furthermore, the docstring has\nbeen extended to reflect this fact."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-07-17T08:02:30Z",
        "message": "FEAT: Make LoRA work with custom models (#676)\n\nEnable custom models to work with LoRA\n\nThis PR enables custom models to work with LoRA in peft by performing a few\nchanges required for non-transformers models. New tests for linear,\ntransformers conv1d, and conv2d layers were added.\n\nNot yet contained in this PR:\n\n- support for AdaLoRA and IA\u00b3\n- documentation\n- examples\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/training/peft/tuners/lora.py",
        "commit_date": "2023-09-05T01:57:07Z",
        "message": "fix style"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/training/peft/tuners/lora.py",
        "commit_date": "2023-09-05T01:47:50Z",
        "message": "fix style"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/training/peft/tuners/lora.py",
        "commit_date": "2023-09-05T01:31:38Z",
        "message": "fix style"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/training/peft/tuners/lora.py",
        "commit_date": "2023-09-04T07:15:55Z",
        "message": "delete unused params"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/training/peft/tuners/lora.py",
        "commit_date": "2023-09-04T06:02:25Z",
        "message": "Add support for kbits training"
    },
    {
        "repo_url": "github.com/liguodongiot/llm-action",
        "filepath": "train/qlora/export_hf_checkpoint.py",
        "commit_date": "2023-07-23T11:43:58Z",
        "message": "fix"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/tools/peft/peft_model.py",
        "commit_date": "2023-07-05T08:20:51Z",
        "message": "added enable input embeddings for lora\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/microsoft/LMOps",
        "filepath": "minillm/transformers/src/transformers/integrations/peft.py",
        "commit_date": "2023-11-16T12:47:19Z",
        "message": "upgrade transformers"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/bloom.py",
        "commit_date": "2023-07-25T10:11:50Z",
        "message": "add more models + gptq mode"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/bloom.py",
        "commit_date": "2023-07-01T17:56:43Z",
        "message": "add local files only option"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/bloom.py",
        "commit_date": "2023-06-10T13:08:36Z",
        "message": "add use_safetensor=False"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/bloom.py",
        "commit_date": "2023-06-08T15:32:11Z",
        "message": "tested gpu and cpu options"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/bloom.py",
        "commit_date": "2023-06-08T01:59:11Z",
        "message": "exp w/ camel and sam"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/bloom.py",
        "commit_date": "2023-06-05T00:25:50Z",
        "message": "update examples"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/bloom.py",
        "commit_date": "2023-06-04T14:50:44Z",
        "message": "disable force download & example view"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/bloom.py",
        "commit_date": "2023-05-18T00:47:42Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2024-02-16T11:16:49Z",
        "message": "TST Make tests more work with MPS (#1463)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2024-02-14T15:43:47Z",
        "message": "TST Use plain asserts in tests (#1448)\n\nUse pytest style asserts instead of unittest methods.\n\nUse `pytest.raises` and `pytest.warns` where suitable."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2024-02-07T11:52:35Z",
        "message": "MNT Move code quality fully to ruff (#1421)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2024-01-30T11:32:39Z",
        "message": "Add positional args to PeftModelForCausalLM.generate (#1393)\n\n* add positional args\n\n* update tests"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-11-16T11:45:12Z",
        "message": "Refactor base layer pattern (#1106)\n\nDescription\n\nRefactor all tuners (where it applies, i.e. not prompt tuning) to use\nthe \"base layer pattern\". This means that the adapter layer will always\nhold a reference to the original layer that it modifies. This pattern is\nalready partly used (e.g. LoRA bnb, gptq layers), now it is consistently\nused everywhere when applicable.\n\nThis PR is a companion PR to #1069, where I first added these changes.\nThey are now extracted to a separate PR to make code review easier and\nto advance more quickly.\n\nImplementation\n\nThe main change is that the adapter layer wraps the original layer and\ncalls forward on that layer, instead of doing stuff like this:\n\nF.linear(input, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n\nwhich completely circumvents the call to the target layer's forward\nmethod. With the base layer pattern, we now call the target layer's\nforward method. Therefore, if the target layer is another adapter\nlayer (which will be crucial for mixed adapters), we call its forward\nmethod correctly. Also, this should allow passing extra arguments, like\nlora_scale to forward.\n\nThis change has the nice side benefit that we no longer need to use\n_init_empty_weights -- in fact, we don't initialize any of the target\nlayer's weights anymore, since we have a reference to it. There is thus\nno risk of having slow but superfluous initialization of layers.\n\nMoreover, I could greatly simplify merge_and_unload by just using the\nbase_layer instead of having to create a completely new layer. For\nOPT-350m, this results in a 15x speedup.\n\nNote that same as for the bnb layers, this should be backwards\nincompatible, since the adapter weights and their state_dicts are not\naffected by this change. I used #1115 for regression testing.\n\nSomewhat unrelated changes\n\nDuring debugging, I got very annoyed with the fact that the reprs of\nadapter layers and normal PyTorch layers are hard to distinguish, e.g.\nthe type is just \"Linear\". Now, for adapter layers, it is prefixed by\nthe adapter type, e.g. \"lora.Linear\". This should have no further\nimplications except for the repr (e.g. state_dict remains unaffected).\n\nFor LoHa and LoKr, I had to change the init of weights when using\ninit_weights=False. This is because of what is discussed in Numerical\ninstabilities with LoHa #1058.\n\nIA\u00b3 now has the unload method too.\n\nLoHa and LoKr now support safe_merge=True when merging layers.\n\nMigration guide\n\nFor 99% of users, the code should continue working as ususal, because\nthe API stays the same. Only low level details have been changed.\n\nCode that relies on isinstance checks on specific PEFT classes may\nbreak. E.g. the LoRA Linear layer no longer inherits from nn.Linear. It\nis, however, still a BaseTunerLayer. The same logic applies for other\nlayer types like Conv2d and for other tuners like IA\u00b3.\n\nTo retrieve the base layer of an adapter layer, you should now call\nmodule.get_base_layer() if you deal with a BaseTunerLayer. Don't rely on\nsomething like module.weight being present (though it might be)."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-11-16T11:05:22Z",
        "message": "FEAT: Merging only specified `adapter_names` when calling `merge` (#1132)\n\n* working v1\n\n* add tests\n\n* remove\n\n* add it also for lokr and loha, left a todo\n\n* Update tests/testing_common.py\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>\n\n* better test\n\n* up\n\n* fix tests\n\n* credits contrib and suggestions from disscussions\n\n* credits contrib and suggestions from disscussions\n\n* address last comments\n\n---------\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-11-15T10:21:23Z",
        "message": "FEAT: Make safe serialization the default one (#1088)\n\n* make safe serialization the default one\n\n* adapt tests\n\n* fix final tests'\n\n* adapt from suggestion"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-11-14T11:28:55Z",
        "message": "Prompt tuning: fix AutoTokenizer.from_pretrained (#1053)\n\nFixes #1032\n\nDescription\n\nCurrently, when using prompt tuning with TEXT, we call\nAutoTokenizer.from_pretrained with only the model id. However, it may be\nnecessary to pass additional arguments, e.g. trust_remote_code=True.\nThis fix allows to pass more arguments by setting the argument\ntokenizer_kwargs in the PromptTuningConfig.\n\nI also added a check that when tokenizer_kwargs is set, the TEXT option\nis actually being used.\n\nMoreover, I noticed that we have no tests for prompt tuning with TEXT,\nso I added those tests for decoder models.\n\nAdditional changes\n\nThere was a bug in PromptEmbedding where the device of the\ninit_token_ids was not set, which resulted in errors when using CUDA.\n\nFinally, I removed an unused constant CONFIG_CLASSES from a test."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-11-10T12:33:56Z",
        "message": "Refactor adapter deletion (#1105)\n\nDescription\n\nThe job of deleting an adapter is now transferred to the adapter layer,\ninstead of the adapter model. This makes it easier for users or other\nlibraries who don't use the adapter model to delete adapters.\n\nImplementation\n\nThe code should now be more generic, relying less on hard-coded\nattributes.\n\nAs a precaution, I also changed the type of adapter_layer_names from\nlist to tuple, as it should not be mutated.\n\nWhen deleting the active adapter, the logic for choosing the new active\nadapter has been changed slightly to ensure consistency across layers.\nIn practice, this should rarely make a difference. An error is now\nraised if the last remaining adapter is deleted.\n\nTest coverage has been increased:\n\n- Deleting adapters is now also tested for custom models.\n- It is also tested for LoHa, LoKr, not only LoRA.\n- I added a test for deleting the non-active adapter.\n\nNot implemented\n\nI did not add adapter deletion to IA\u00b3, since it is included in #980. LMK\nif it should be added here instead."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-10-13T10:23:16Z",
        "message": "FEAT: Add fp16 + cpu merge support (#1017)\n\n* add fp16 + cpu merge support\n\n* fix tests\n\n* add fp16 tests for custom models\n\n* fix tests\n\n* adapt from comments\n\n* more clarifications"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-10-09T16:28:00Z",
        "message": "FEAT: Add `safe_merge` option in `merge` (#1001)\n\n* add `safe_merge` option in `merge`\n\n* oops\n\n* Apply suggestions from code review\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* address final comments\n\n* Update src/peft/tuners/lora/layer.py\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* Update src/peft/tuners/lora/layer.py\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* add it for ia3\n\n* add it for adalora\n\n* up\n\n* revert for loha\n\n* style\n\n* fix CI\n\n* adapt from suggestions\n\n* add tests\n\n* up\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-09-07T09:36:42Z",
        "message": "support prefix tuning for starcoder models (#913)\n\n* support prefix tuning for starcoder models\n\n* remoce the test filter for prefix tuning tests for StarCoder models"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-08-08T12:35:19Z",
        "message": "Add adapter error handling (#800)\n\nWhen a user tries to add a 2nd adapter, Lora and AdaLora make some checks to\nensure the new adapter is compatible with existing adapters. Currently, that\ncheck is performed halfway through the method. This means that if the check\nfails, the new adapter is partially applied, leaving the model in a bad state.\nThe main purpose of this PR is to ensure that the model state is correct after\nsuch a failure is encountered.\n\nTests were added to catch this potential bug.\n\nWhile working on this, I also did some related, but not strictly necessary\nchanges to the add_adapter methods:\n\n- Previously, the peft_config from the PeftModel was passed to the base\n  model. This meant that sometimes, the base model would hold a reference\n  to PeftModel.peft_config, but not always, as some base models would\n  create new dicts. This is problematic, because some code would rely on\n  the objects being the same. Now, they are never the same, leading to\n  more consistency.\n- I think that the check if multiple adapters have biases (which is not\n  supported) was accidentally removed by #749. It is added back in.\n- Add some type annotations\n- Extend docstrings to contain adapter_name"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-08-02T14:59:11Z",
        "message": "Allow passing inputs_embeds instead of input_ids (#757)\n\nResolves #727\n\nRight now, there is an issue with a few PeftModelForXxx classes when\nusers pass only inputs_embeds but not input_ids. First of all, the batch\nsize was being derived on input_ids, now it is derived from\ninputs_embeds instead if input_ids is None. Furthermore, a few forward\ncalls to the base model were not passing the inputs_embeds along, which\nresulted in errors down the line. These issues have been fixed now."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-07-28T11:06:53Z",
        "message": "Add tests for AdaLoRA, fix a few bugs (#734)\n\nSo far, there have been no tests for AdaLoRA. This PR adds tests similar\nto the existing ones. While working on those tests, a few bugs were\nencountered and fixed.\n\nThe changes made to AdaLoRA:\n\n- Linked to paper abstract, not pdf.\n- Don't assume that target modules have a .bias attribute (same as for\n  LoRA).\n- Fixed an issue where it was assumed that if an output object from\n  forward has a .loss attribute, it is a scalar, when it can be None.\n- Fixed an issue that when init_lora_weights=False, the weights were\n  still initialized to be an identity transform.\n- When replacing modules, if a target module is a ModuleList or\n  ModuleDict, they are now skipped instead of raising an error that the\n  module type is not supported. My reasoning was that it is never intended\n  to change those modules, so if their names are matched, it must be a\n  false positive. The issue arose because for some target modules, the\n  names are just k\" etc., and since we match with endswith, this can\n  easily lead to modules like \"block\" to match."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-07-17T08:02:30Z",
        "message": "FEAT: Make LoRA work with custom models (#676)\n\nEnable custom models to work with LoRA\n\nThis PR enables custom models to work with LoRA in peft by performing a few\nchanges required for non-transformers models. New tests for linear,\ntransformers conv1d, and conv2d layers were added.\n\nNot yet contained in this PR:\n\n- support for AdaLoRA and IA\u00b3\n- documentation\n- examples\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-07-14T14:28:03Z",
        "message": "[`Feature`] Save only selected adapters for LoRA (#705)\n\n* v1 working for LoRA\n\n* more checks\n\n* fix prompt learning issues\n\n* fix failing test\n\n* Apply suggestions from code review\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* fixed indentation\n\n* move the check above\n\n* added tests for adaption prompt, enc-dec and feature extraction\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-07-14T14:14:51Z",
        "message": "[Core] Enhancements and refactoring of LoRA method (#695)\n\n* refactor lora and add utils\n\n1. Refactor LoRA code\n2. Add method to delete LoRA adapters\n3. Add method to unload the PEFT LoRA model.\n4. Add `svd` weighted adapter support.\n5. minor fixes\n\n* fixes\n\n* fixes\n\n* Update lora.py\n\n* fixes\n\n* Update lora.py\n\n* docstrings for the added public APIs\n\n* docs\n\n* Update src/peft/tuners/lora.py\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* resolve comments, refactoring and adding tests\n\n* fix the remaining failing tests\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-07-14T12:33:33Z",
        "message": "[WIP] FIX for disabling adapter, adding tests (#683)\n\nThis PR deals with some issues with disabling adapter:\n\n- typo in active.adapter\n- prompt encoder could be on wrong device\n- when using prompt learning + generate, disabling did not work\n\nFor the last point, there is a somewhat ugly fix in place for now,\npending a more comprehensive refactor (a comment was added to that\neffect).\n\nComprehensive tests were added to check that everything works now.\n\nThe following tests still not working:\n\n- adaption prompt\n- seq2seq with prompt tuning/prompt encoding\n- stable diffusion is a little bit flaky but test is hopefully robust enough\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-07-13T09:12:40Z",
        "message": "FIX: base_model_torch_dtype when using model.half() after init (#688)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-06-27T12:57:57Z",
        "message": "feat(model): Allow from_pretrained to accept PeftConfig class (#612)\n\n* feat(model): Allow from_pretrained to accept PeftConfig class\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* tests: add test cases for config construction\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: address comments and run tools\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* fix: style\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-06-21T12:13:40Z",
        "message": "add `adapter_name` in `get_peft_model` (#610)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-06-16T09:06:48Z",
        "message": "add more CI tests (#586)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-06-15T10:23:05Z",
        "message": "[`core`] Correctly passing the kwargs all over the place (#575)\n\n* v1 of the fix\n\n* forward contrib credits from discussions\n\n* add tests\n\n---------\n\nCo-authored-by: winglian <winglian@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-06-09T10:33:13Z",
        "message": "[`core`] Add safetensors integration (#553)\n\n* add v1\n\n* clean up\n\n* more improvements\n\n* add device\n\n* final adjustements\n\n* use `EntryNotFoundError`\n\n* better checks\n\n* add tests and final fixes\n\n* make style && make quality\n\n* remove `push_to_hub` because of the release"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-06-01T07:35:24Z",
        "message": "[`LoRA`] Allow applying LoRA at different stages (#429)\n\n* working v1\n\n- working v1\n- added tests\n- needs some documentation\n\n* more fixes\n\n- stronger tests\n- documentation\n- remove unneeded common layers pattern\n\n* add more docstring\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* quality & style\n\n* style\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-05-31T10:14:27Z",
        "message": "[`core`] Add gradient checkpointing check (#404)\n\n* add automatic input enable gradients when calling `get_peft_model`\n\n* style\n\n* better check\n\n* add 4bit check"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-04-14T12:34:01Z",
        "message": "[`tests`] add CI training tests (#311)\n\n* add training tests\n\n* styling"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-04-07T10:48:22Z",
        "message": "add and fix tests"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-04-06T13:36:33Z",
        "message": "making adalora compatible with multiple adapters"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_decoder_models.py",
        "commit_date": "2023-04-05T08:17:26Z",
        "message": "[`tests`] Adds more tests + fix failing tests (#238)\n\n* adds more tests\n\n- refactor tests\n- add enc-dec tests\n- skips generate tests for non-lora adapters\n\n* rm unneeded file\n\n* fix tests\n\n* fix\n\n* more checks\n\n* fix issue"
    },
    {
        "repo_url": "github.com/OptimalScale/LMFlow",
        "filepath": "scripts/export_llama_state_dict_checkpoint.py",
        "commit_date": "2023-04-16T02:02:06Z",
        "message": "diffusion"
    },
    {
        "repo_url": "github.com/OptimalScale/LMFlow",
        "filepath": "scripts/export_llama_state_dict_checkpoint.py",
        "commit_date": "2023-04-06T04:23:17Z",
        "message": "add readme"
    },
    {
        "repo_url": "github.com/OptimalScale/LMFlow",
        "filepath": "scripts/export_llama_state_dict_checkpoint.py",
        "commit_date": "2023-04-05T14:59:36Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/baize.py",
        "commit_date": "2023-07-25T10:11:50Z",
        "message": "add more models + gptq mode"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/baize.py",
        "commit_date": "2023-07-01T17:56:43Z",
        "message": "add local files only option"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/baize.py",
        "commit_date": "2023-06-10T13:08:36Z",
        "message": "add use_safetensor=False"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/baize.py",
        "commit_date": "2023-06-09T04:33:05Z",
        "message": "update vram requirements records"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/baize.py",
        "commit_date": "2023-06-08T15:32:11Z",
        "message": "tested gpu and cpu options"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/baize.py",
        "commit_date": "2023-06-08T01:59:11Z",
        "message": "exp w/ camel and sam"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/baize.py",
        "commit_date": "2023-06-05T00:25:50Z",
        "message": "update examples"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/baize.py",
        "commit_date": "2023-06-04T14:50:44Z",
        "message": "disable force download & example view"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/baize.py",
        "commit_date": "2023-05-24T05:43:07Z",
        "message": "add baize"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_adaption_prompt.py",
        "commit_date": "2024-02-16T11:16:49Z",
        "message": "TST Make tests more work with MPS (#1463)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_adaption_prompt.py",
        "commit_date": "2024-02-14T15:43:47Z",
        "message": "TST Use plain asserts in tests (#1448)\n\nUse pytest style asserts instead of unittest methods.\n\nUse `pytest.raises` and `pytest.warns` where suitable."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_adaption_prompt.py",
        "commit_date": "2024-02-07T11:52:35Z",
        "message": "MNT Move code quality fully to ruff (#1421)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_adaption_prompt.py",
        "commit_date": "2024-02-06T00:54:06Z",
        "message": "Fix typos (#1435)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_adaption_prompt.py",
        "commit_date": "2024-01-30T11:32:39Z",
        "message": "Add positional args to PeftModelForCausalLM.generate (#1393)\n\n* add positional args\n\n* update tests"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_adaption_prompt.py",
        "commit_date": "2023-12-12T14:16:00Z",
        "message": "FIX Issues with transformers 4.36 (#1252)\n\nAdjust for different type of past_key_values when using caching.\n\nAlso: Fix some seeds for flaky tests.\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_adaption_prompt.py",
        "commit_date": "2023-11-15T10:21:23Z",
        "message": "FEAT: Make safe serialization the default one (#1088)\n\n* make safe serialization the default one\n\n* adapt tests\n\n* fix final tests'\n\n* adapt from suggestion"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_adaption_prompt.py",
        "commit_date": "2023-11-06T13:04:19Z",
        "message": "FIX: fix adaptation prompt CI and compatibility with latest transformers (4.35.0) (#1084)\n\n* fix adaptation prompt CI\n\n* undo some other changes"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_adaption_prompt.py",
        "commit_date": "2023-11-03T14:52:51Z",
        "message": "FIX: Skip adaption prompt tests with new transformers versions (#1077)\n\nAdaption prompt is failing with transformers v4.35.0. This PR skips the\nadaption prompt tests so that CI is green again. The PR also adds an\nerror when users try to use adaption prompt with that version,\ninstructing them to use an older transformers version instead.\n\nThis should be removed as soon as the issue is fixed in\nPEFT/transformers."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_adaption_prompt.py",
        "commit_date": "2023-07-14T14:28:03Z",
        "message": "[`Feature`] Save only selected adapters for LoRA (#705)\n\n* v1 working for LoRA\n\n* more checks\n\n* fix prompt learning issues\n\n* fix failing test\n\n* Apply suggestions from code review\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* fixed indentation\n\n* move the check above\n\n* added tests for adaption prompt, enc-dec and feature extraction\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_adaption_prompt.py",
        "commit_date": "2023-07-14T12:33:33Z",
        "message": "[WIP] FIX for disabling adapter, adding tests (#683)\n\nThis PR deals with some issues with disabling adapter:\n\n- typo in active.adapter\n- prompt encoder could be on wrong device\n- when using prompt learning + generate, disabling did not work\n\nFor the last point, there is a somewhat ugly fix in place for now,\npending a more comprehensive refactor (a comment was added to that\neffect).\n\nComprehensive tests were added to check that everything works now.\n\nThe following tests still not working:\n\n- adaption prompt\n- seq2seq with prompt tuning/prompt encoding\n- stable diffusion is a little bit flaky but test is hopefully robust enough\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_adaption_prompt.py",
        "commit_date": "2023-06-01T09:14:11Z",
        "message": "[`Llama-Adapter`]\u00a0fix half precision inference + add tests (#456)\n\n* fix + add tests\n\n* forward contrib credits from discussions\n\n---------\n\nCo-authored-by: HamidShojanazeri <HamidShojanazeri@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_adaption_prompt.py",
        "commit_date": "2023-04-25T06:54:18Z",
        "message": "Implement adaption prompt from Llama-Adapter paper (#268)\n\n* Implement adaption prompt from Llama-Adapter paper\n\n* Support multi-adapters\n\n* Refactor adaption prompt to target attn modules instead of layers\n\n* Refactor adaption prompt to be more generic\n\n* Fix adaption prompt not on right device\n\n* Apply suggestions from code review\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>\n\n* Fix style\n\n* Add support for Llama config use_cache=True\n\n* Fix rebase issues\n\n---------\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/regression/test_regression.py",
        "commit_date": "2024-02-14T15:43:47Z",
        "message": "TST Use plain asserts in tests (#1448)\n\nUse pytest style asserts instead of unittest methods.\n\nUse `pytest.raises` and `pytest.warns` where suitable."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/regression/test_regression.py",
        "commit_date": "2024-02-07T11:52:35Z",
        "message": "MNT Move code quality fully to ruff (#1421)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/regression/test_regression.py",
        "commit_date": "2023-12-08T10:50:48Z",
        "message": "TST: Add tolerance for regression tests (#1241)\n\nTests currently call torch.allclose without any tolerance, which is\nprobably the cause of the CI failure. Now, tolerance is set to 1e-4."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/regression/test_regression.py",
        "commit_date": "2023-12-06T14:07:05Z",
        "message": "TST: Add regression tests 2 (#1115)\n\nDescription\n\nIn general, for regression tests, we need two steps:\n\n1. Creating the regression artifacts, in this case the adapter\n   checkpoint and the expected output of the model.\n2. Running the regression tests, i.e. loading the adapter and checking\n   that the output of the model is the same as the expected output.\n\nMy approach is to re-use as much code as possible between those two\nsteps. Therefore, the same test script can be used for both, with only\nan environment variable to distinguish between the two. Step 1 is\ninvoked by calling:\n\n`REGRESSION_CREATION_MODE=True pytest tests/regression/test_regression.py`\n\nand to run the second step, we call:\n\n`pytest tests/regression/test_regression.py`\n\nCreating regression artifacts\n\nThe first step will create an adapter checkpoint and an output for the\ngiven PEFT version and test setting in a new directory. E.g. it will\ncreate a directory `tests/regression/lora_opt-125m_bnb_4bit/0.5.0/` that\ncontains adapter_model.bin and output.pt.\n\nBefore this step runs, there is a check that the git repo is clean (no\ndirty worktree) and that the commit is tagged (i.e. corresponds to a\nrelease version of PEFT). Otherwise, we may accidentally create\nregression artifacts that do not correspond to any PEFT release.\n\nThe easiest way to get such a clean state (say, for PEFT v0.5.0) is by\nchecking out a tagged commit, e.g:\n\n`git checkout v0.5.0`\n\nbefore running the first step.\n\nThe first step will also skip the creation of regression artifacts if\nthey already exist.\n\nIt is possible to circumvent all the aforementioned checks by setting\nthe environment variable `REGRESSION_FORCE_MODE` to True like so:\n\n`REGRESSION_FORCE_MODE=True REGRESSION_CREATION_MODE=True pytest tests/regression/test_regression.py`\n\nYou should only do this if you know exactly what you're doing.\n\nRunning regression tests\n\nThe second step is much simpler. It will load the adapters and the\noutput created in the first step, and compare the output to the output\nfrom a new PEFT model using the loaded adapter. The outputs should be\nthe same.\n\nIf more than one version is discovered for a given test setting, all of\nthem are tested.\n\nNotes\n\nRegression artifacts are stored on HF Hub."
    },
    {
        "repo_url": "github.com/facebookresearch/llama-recipes",
        "filepath": "examples/hf_text_generation_inference/merge_lora_weights.py",
        "commit_date": "2023-09-01T20:17:39Z",
        "message": "Move inference scripts into example folder"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_encoder_decoder_models.py",
        "commit_date": "2024-02-07T11:52:35Z",
        "message": "MNT Move code quality fully to ruff (#1421)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_encoder_decoder_models.py",
        "commit_date": "2024-01-30T11:32:39Z",
        "message": "Add positional args to PeftModelForCausalLM.generate (#1393)\n\n* add positional args\n\n* update tests"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_encoder_decoder_models.py",
        "commit_date": "2023-11-15T10:21:23Z",
        "message": "FEAT: Make safe serialization the default one (#1088)\n\n* make safe serialization the default one\n\n* adapt tests\n\n* fix final tests'\n\n* adapt from suggestion"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_encoder_decoder_models.py",
        "commit_date": "2023-11-10T12:33:56Z",
        "message": "Refactor adapter deletion (#1105)\n\nDescription\n\nThe job of deleting an adapter is now transferred to the adapter layer,\ninstead of the adapter model. This makes it easier for users or other\nlibraries who don't use the adapter model to delete adapters.\n\nImplementation\n\nThe code should now be more generic, relying less on hard-coded\nattributes.\n\nAs a precaution, I also changed the type of adapter_layer_names from\nlist to tuple, as it should not be mutated.\n\nWhen deleting the active adapter, the logic for choosing the new active\nadapter has been changed slightly to ensure consistency across layers.\nIn practice, this should rarely make a difference. An error is now\nraised if the last remaining adapter is deleted.\n\nTest coverage has been increased:\n\n- Deleting adapters is now also tested for custom models.\n- It is also tested for LoHa, LoKr, not only LoRA.\n- I added a test for deleting the non-active adapter.\n\nNot implemented\n\nI did not add adapter deletion to IA\u00b3, since it is included in #980. LMK\nif it should be added here instead."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_encoder_decoder_models.py",
        "commit_date": "2023-11-09T13:50:35Z",
        "message": "[`core`] Fix safetensors serialization for shared tensors (#1101)\n\n* fix st serialization\n\n* add test\n\n* add CI test\n\n* add comment"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_encoder_decoder_models.py",
        "commit_date": "2023-08-08T12:35:19Z",
        "message": "Add adapter error handling (#800)\n\nWhen a user tries to add a 2nd adapter, Lora and AdaLora make some checks to\nensure the new adapter is compatible with existing adapters. Currently, that\ncheck is performed halfway through the method. This means that if the check\nfails, the new adapter is partially applied, leaving the model in a bad state.\nThe main purpose of this PR is to ensure that the model state is correct after\nsuch a failure is encountered.\n\nTests were added to catch this potential bug.\n\nWhile working on this, I also did some related, but not strictly necessary\nchanges to the add_adapter methods:\n\n- Previously, the peft_config from the PeftModel was passed to the base\n  model. This meant that sometimes, the base model would hold a reference\n  to PeftModel.peft_config, but not always, as some base models would\n  create new dicts. This is problematic, because some code would rely on\n  the objects being the same. Now, they are never the same, leading to\n  more consistency.\n- I think that the check if multiple adapters have biases (which is not\n  supported) was accidentally removed by #749. It is added back in.\n- Add some type annotations\n- Extend docstrings to contain adapter_name"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_encoder_decoder_models.py",
        "commit_date": "2023-07-28T11:06:53Z",
        "message": "Add tests for AdaLoRA, fix a few bugs (#734)\n\nSo far, there have been no tests for AdaLoRA. This PR adds tests similar\nto the existing ones. While working on those tests, a few bugs were\nencountered and fixed.\n\nThe changes made to AdaLoRA:\n\n- Linked to paper abstract, not pdf.\n- Don't assume that target modules have a .bias attribute (same as for\n  LoRA).\n- Fixed an issue where it was assumed that if an output object from\n  forward has a .loss attribute, it is a scalar, when it can be None.\n- Fixed an issue that when init_lora_weights=False, the weights were\n  still initialized to be an identity transform.\n- When replacing modules, if a target module is a ModuleList or\n  ModuleDict, they are now skipped instead of raising an error that the\n  module type is not supported. My reasoning was that it is never intended\n  to change those modules, so if their names are matched, it must be a\n  false positive. The issue arose because for some target modules, the\n  names are just k\" etc., and since we match with endswith, this can\n  easily lead to modules like \"block\" to match."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_encoder_decoder_models.py",
        "commit_date": "2023-07-17T08:02:30Z",
        "message": "FEAT: Make LoRA work with custom models (#676)\n\nEnable custom models to work with LoRA\n\nThis PR enables custom models to work with LoRA in peft by performing a few\nchanges required for non-transformers models. New tests for linear,\ntransformers conv1d, and conv2d layers were added.\n\nNot yet contained in this PR:\n\n- support for AdaLoRA and IA\u00b3\n- documentation\n- examples\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_encoder_decoder_models.py",
        "commit_date": "2023-07-14T14:28:03Z",
        "message": "[`Feature`] Save only selected adapters for LoRA (#705)\n\n* v1 working for LoRA\n\n* more checks\n\n* fix prompt learning issues\n\n* fix failing test\n\n* Apply suggestions from code review\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* fixed indentation\n\n* move the check above\n\n* added tests for adaption prompt, enc-dec and feature extraction\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_encoder_decoder_models.py",
        "commit_date": "2023-07-14T14:14:51Z",
        "message": "[Core] Enhancements and refactoring of LoRA method (#695)\n\n* refactor lora and add utils\n\n1. Refactor LoRA code\n2. Add method to delete LoRA adapters\n3. Add method to unload the PEFT LoRA model.\n4. Add `svd` weighted adapter support.\n5. minor fixes\n\n* fixes\n\n* fixes\n\n* Update lora.py\n\n* fixes\n\n* Update lora.py\n\n* docstrings for the added public APIs\n\n* docs\n\n* Update src/peft/tuners/lora.py\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* resolve comments, refactoring and adding tests\n\n* fix the remaining failing tests\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_encoder_decoder_models.py",
        "commit_date": "2023-07-14T12:33:33Z",
        "message": "[WIP] FIX for disabling adapter, adding tests (#683)\n\nThis PR deals with some issues with disabling adapter:\n\n- typo in active.adapter\n- prompt encoder could be on wrong device\n- when using prompt learning + generate, disabling did not work\n\nFor the last point, there is a somewhat ugly fix in place for now,\npending a more comprehensive refactor (a comment was added to that\neffect).\n\nComprehensive tests were added to check that everything works now.\n\nThe following tests still not working:\n\n- adaption prompt\n- seq2seq with prompt tuning/prompt encoding\n- stable diffusion is a little bit flaky but test is hopefully robust enough\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_encoder_decoder_models.py",
        "commit_date": "2023-07-13T09:12:40Z",
        "message": "FIX: base_model_torch_dtype when using model.half() after init (#688)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_encoder_decoder_models.py",
        "commit_date": "2023-07-07T12:19:10Z",
        "message": "Remove skipping certain tests (#668)\n\nThe generate tests so far were skipped for non-lora, non-prefix tuning\ncases. However, those cases are now passing, so it is no longer\nnecessary to skip the tests."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_encoder_decoder_models.py",
        "commit_date": "2023-06-27T12:57:57Z",
        "message": "feat(model): Allow from_pretrained to accept PeftConfig class (#612)\n\n* feat(model): Allow from_pretrained to accept PeftConfig class\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* tests: add test cases for config construction\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: address comments and run tools\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* fix: style\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_encoder_decoder_models.py",
        "commit_date": "2023-06-21T12:13:40Z",
        "message": "add `adapter_name` in `get_peft_model` (#610)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_encoder_decoder_models.py",
        "commit_date": "2023-06-15T10:23:05Z",
        "message": "[`core`] Correctly passing the kwargs all over the place (#575)\n\n* v1 of the fix\n\n* forward contrib credits from discussions\n\n* add tests\n\n---------\n\nCo-authored-by: winglian <winglian@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_encoder_decoder_models.py",
        "commit_date": "2023-06-09T10:33:13Z",
        "message": "[`core`] Add safetensors integration (#553)\n\n* add v1\n\n* clean up\n\n* more improvements\n\n* add device\n\n* final adjustements\n\n* use `EntryNotFoundError`\n\n* better checks\n\n* add tests and final fixes\n\n* make style && make quality\n\n* remove `push_to_hub` because of the release"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_encoder_decoder_models.py",
        "commit_date": "2023-06-01T07:35:24Z",
        "message": "[`LoRA`] Allow applying LoRA at different stages (#429)\n\n* working v1\n\n- working v1\n- added tests\n- needs some documentation\n\n* more fixes\n\n- stronger tests\n- documentation\n- remove unneeded common layers pattern\n\n* add more docstring\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* quality & style\n\n* style\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_encoder_decoder_models.py",
        "commit_date": "2023-05-31T10:14:27Z",
        "message": "[`core`] Add gradient checkpointing check (#404)\n\n* add automatic input enable gradients when calling `get_peft_model`\n\n* style\n\n* better check\n\n* add 4bit check"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_encoder_decoder_models.py",
        "commit_date": "2023-04-14T12:34:01Z",
        "message": "[`tests`] add CI training tests (#311)\n\n* add training tests\n\n* styling"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_encoder_decoder_models.py",
        "commit_date": "2023-04-07T10:48:22Z",
        "message": "add and fix tests"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_encoder_decoder_models.py",
        "commit_date": "2023-04-06T13:36:33Z",
        "message": "making adalora compatible with multiple adapters"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_encoder_decoder_models.py",
        "commit_date": "2023-04-05T17:17:01Z",
        "message": "fix test"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_encoder_decoder_models.py",
        "commit_date": "2023-04-05T08:17:26Z",
        "message": "[`tests`] Adds more tests + fix failing tests (#238)\n\n* adds more tests\n\n- refactor tests\n- add enc-dec tests\n- skips generate tests for non-lora adapters\n\n* rm unneeded file\n\n* fix tests\n\n* fix\n\n* more checks\n\n* fix issue"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_multitask_prompt_tuning.py",
        "commit_date": "2024-02-19T12:53:39Z",
        "message": "FIX: Multitask prompt tuning with other tuning init (#1144)\n\nResolves #1082.\n\nAlso, adding tests for prompt_tuning_init != RANDOM.\n\n---------\n\nCo-authored-by: Mayank Mishra <32954280+mayank31398@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_multitask_prompt_tuning.py",
        "commit_date": "2024-02-14T15:43:47Z",
        "message": "TST Use plain asserts in tests (#1448)\n\nUse pytest style asserts instead of unittest methods.\n\nUse `pytest.raises` and `pytest.warns` where suitable."
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_multitask_prompt_tuning.py",
        "commit_date": "2024-02-07T11:52:35Z",
        "message": "MNT Move code quality fully to ruff (#1421)"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_multitask_prompt_tuning.py",
        "commit_date": "2024-01-30T11:32:39Z",
        "message": "Add positional args to PeftModelForCausalLM.generate (#1393)\n\n* add positional args\n\n* update tests"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_multitask_prompt_tuning.py",
        "commit_date": "2023-12-12T14:16:00Z",
        "message": "FIX Issues with transformers 4.36 (#1252)\n\nAdjust for different type of past_key_values when using caching.\n\nAlso: Fix some seeds for flaky tests.\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_multitask_prompt_tuning.py",
        "commit_date": "2023-11-15T10:21:23Z",
        "message": "FEAT: Make safe serialization the default one (#1088)\n\n* make safe serialization the default one\n\n* adapt tests\n\n* fix final tests'\n\n* adapt from suggestion"
    },
    {
        "repo_url": "github.com/huggingface/peft",
        "filepath": "tests/test_multitask_prompt_tuning.py",
        "commit_date": "2023-08-25T06:12:11Z",
        "message": "\ud83c\udf89 Add Multitask Prompt Tuning (#400)\n\n* mpt\n\n* fix save\n\n* fix save\n\n* add jupyter notebook\n\n* add jupyter notebook\n\n* add jupyter notebook\n\n* drop shuffling\n\n* drop classify_dataset\n\n* drop classify_dataset\n\n* fix keys\n\n* fix keys\n\n* add comments\n\n* use EXACT_SOURCE_TASK in the example\n\n* formatting\n\n* Fix dict index in embedding retrieval\n\n* run style and quality\n\n* run style and quality\n\n* run style and quality\n\n* style\n\n* final fix\n\n* style\n\n* comment out failing tests\n\n* fix generation tests\n\n* fix style and save test\n\n* all testcases\n\n* fix import\n\n* add license header\n\n* reformat\n\n* fix encoder-decoder models\n\n* fix tests running multiple times\n\n* fix paper name for IA3 and add MPT paper\n\n* Trigger CI\n\n* address the recommended changes\n\n* reformat\n\n* address suggestions\n\n* address suggestions\n\n* revert reformatting\n\n* revert reformatting\n\n---------\n\nCo-authored-by: Alex-Brooks <Alex.Brooks@ibm.com>"
    },
    {
        "repo_url": "github.com/shibing624/pycorrector",
        "filepath": "pycorrector/gpt/merge_peft_adapter.py",
        "commit_date": "2023-11-05T08:35:41Z",
        "message": "update conv seq2seq."
    },
    {
        "repo_url": "github.com/shibing624/pycorrector",
        "filepath": "pycorrector/gpt/merge_peft_adapter.py",
        "commit_date": "2023-11-03T04:40:56Z",
        "message": "update merge."
    },
    {
        "repo_url": "github.com/shibing624/pycorrector",
        "filepath": "pycorrector/gpt/merge_peft_adapter.py",
        "commit_date": "2023-10-30T09:38:49Z",
        "message": "add llm model for correction."
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/replit.py",
        "commit_date": "2023-07-25T10:11:50Z",
        "message": "add more models + gptq mode"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/replit.py",
        "commit_date": "2023-07-01T17:56:43Z",
        "message": "add local files only option"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/replit.py",
        "commit_date": "2023-06-08T01:59:11Z",
        "message": "exp w/ camel and sam"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/replit.py",
        "commit_date": "2023-06-05T00:25:50Z",
        "message": "update examples"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/replit.py",
        "commit_date": "2023-06-04T14:50:44Z",
        "message": "disable force download & example view"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/replit.py",
        "commit_date": "2023-06-03T14:04:14Z",
        "message": "add 40b wizard falcon"
    },
    {
        "repo_url": "github.com/arcee-ai/mergekit",
        "filepath": "mergekit/common.py",
        "commit_date": "2024-01-30T00:03:09Z",
        "message": "Add mergekit-moe script (#141)\n\nAlso incidentally adds support for merging Mixtral models."
    },
    {
        "repo_url": "github.com/arcee-ai/mergekit",
        "filepath": "mergekit/common.py",
        "commit_date": "2024-01-23T23:16:10Z",
        "message": "Computation Graph Overhaul (Again) (#127)\n\nRefactor the internal computational graph to make writing new tasks easier.\n\nThis time for real."
    },
    {
        "repo_url": "github.com/arcee-ai/mergekit",
        "filepath": "mergekit/common.py",
        "commit_date": "2024-01-22T20:57:46Z",
        "message": "Revert #67"
    },
    {
        "repo_url": "github.com/arcee-ai/mergekit",
        "filepath": "mergekit/common.py",
        "commit_date": "2024-01-22T19:55:04Z",
        "message": "Fix str/bytes issue"
    },
    {
        "repo_url": "github.com/arcee-ai/mergekit",
        "filepath": "mergekit/common.py",
        "commit_date": "2024-01-22T18:46:46Z",
        "message": "Computational Graph Overhaul (#67)\n\nRefactor the internal computational graph to make writing new tasks easier.\n\nHuge, invasive change for minimal immediate benefit, but makes my life\neasier down the road for implementing more complicated merge methods."
    },
    {
        "repo_url": "github.com/arcee-ai/mergekit",
        "filepath": "mergekit/common.py",
        "commit_date": "2024-01-03T05:58:30Z",
        "message": "Phi 2 (#66)\n\nAdd support for Phi 2 models."
    },
    {
        "repo_url": "github.com/arcee-ai/mergekit",
        "filepath": "mergekit/common.py",
        "commit_date": "2024-01-02T20:01:24Z",
        "message": "Bump copyright date to 2024"
    },
    {
        "repo_url": "github.com/arcee-ai/mergekit",
        "filepath": "mergekit/common.py",
        "commit_date": "2023-12-30T05:36:56Z",
        "message": "Propagate trust_remote_code to .config()"
    },
    {
        "repo_url": "github.com/arcee-ai/mergekit",
        "filepath": "mergekit/common.py",
        "commit_date": "2023-12-03T20:35:50Z",
        "message": "Cleanup and add pre-commit hook"
    },
    {
        "repo_url": "github.com/arcee-ai/mergekit",
        "filepath": "mergekit/common.py",
        "commit_date": "2023-11-29T07:02:50Z",
        "message": "Lazy Pytorch Unpickler (#13)\n\nAdds a flag `--lazy-unpickle` to `mergekit-yaml` that lazily loads\nindividual tensors instead of whole shards.\n\nWith this flag enabled I'm able to do a linear merge of two 13b models\nin 16GB of RAM.\n\nIt's a little hacky and has to touch pytorch internals - I can't\nguarantee it'll work right for 100% of models. Do let me know if it\nbreaks anything."
    },
    {
        "repo_url": "github.com/arcee-ai/mergekit",
        "filepath": "mergekit/common.py",
        "commit_date": "2023-11-11T22:34:18Z",
        "message": "Add option for trust_remote_code when merging LoRA"
    },
    {
        "repo_url": "github.com/arcee-ai/mergekit",
        "filepath": "mergekit/common.py",
        "commit_date": "2023-10-06T03:06:09Z",
        "message": "Create mergekit package"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_generic_transformers.py",
        "commit_date": "2023-12-10T14:27:27Z",
        "message": "\u652f\u6301awq"
    },
    {
        "repo_url": "github.com/wenda-LLM/wenda",
        "filepath": "llms/llm_generic_transformers.py",
        "commit_date": "2023-07-19T15:14:25Z",
        "message": "\u589e\u52a0generic_transformers\u6a21\u7ec4\uff0c\u7ecf\u6d4b\u8bd5\u53ef\u517c\u5bb9Llama-2-13B-chat-GPTQ"
    },
    {
        "repo_url": "github.com/netease-youdao/QAnything",
        "filepath": "third_party/FastChat/fastchat/model/apply_lora.py",
        "commit_date": "2024-01-25T04:29:46Z",
        "message": "ADD: add FastChat assets from commit e86e70d0b48c3d6e2223de96b20e377315dbf73e"
    },
    {
        "repo_url": "github.com/open-compass/opencompass",
        "filepath": "opencompass/models/modelscope.py",
        "commit_date": "2023-11-22T07:32:21Z",
        "message": "[Feature] support download from modelscope (#534)\n\n* [Feature] download from modelscope\n\n* [Feature] download from modelscope\n\n* minor fix\n\n---------\n\nCo-authored-by: yingfhu <yingfhu@gmail.com>"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/falcon.py",
        "commit_date": "2023-07-26T11:56:18Z",
        "message": "add gptq vram req. templates"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/falcon.py",
        "commit_date": "2023-07-25T10:11:50Z",
        "message": "add more models + gptq mode"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/falcon.py",
        "commit_date": "2023-07-01T17:56:43Z",
        "message": "add local files only option"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/falcon.py",
        "commit_date": "2023-06-11T13:31:52Z",
        "message": "remove half in falcon"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/falcon.py",
        "commit_date": "2023-06-10T13:08:36Z",
        "message": "add use_safetensor=False"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/falcon.py",
        "commit_date": "2023-06-08T15:32:11Z",
        "message": "tested gpu and cpu options"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/falcon.py",
        "commit_date": "2023-06-08T01:59:11Z",
        "message": "exp w/ camel and sam"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/falcon.py",
        "commit_date": "2023-06-05T00:25:50Z",
        "message": "update examples"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/falcon.py",
        "commit_date": "2023-05-30T02:12:34Z",
        "message": "add falcon families"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/alpaca.py",
        "commit_date": "2023-07-25T10:11:50Z",
        "message": "add more models + gptq mode"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/alpaca.py",
        "commit_date": "2023-07-01T17:56:43Z",
        "message": "add local files only option"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/alpaca.py",
        "commit_date": "2023-06-10T13:08:36Z",
        "message": "add use_safetensor=False"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/alpaca.py",
        "commit_date": "2023-06-09T04:33:05Z",
        "message": "update vram requirements records"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/alpaca.py",
        "commit_date": "2023-06-08T15:32:11Z",
        "message": "tested gpu and cpu options"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/alpaca.py",
        "commit_date": "2023-06-08T01:59:11Z",
        "message": "exp w/ camel and sam"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/alpaca.py",
        "commit_date": "2023-06-04T14:50:44Z",
        "message": "disable force download & example view"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/alpaca.py",
        "commit_date": "2023-06-04T13:37:13Z",
        "message": "add model desc"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/alpaca.py",
        "commit_date": "2023-06-03T13:18:00Z",
        "message": "add nous-hermes model"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/alpaca.py",
        "commit_date": "2023-05-04T22:11:52Z",
        "message": "add optimum for bettertrasformer integration"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/alpaca.py",
        "commit_date": "2023-04-20T16:15:07Z",
        "message": "rebranding"
    },
    {
        "repo_url": "github.com/open-compass/opencompass",
        "filepath": "opencompass/models/huggingface.py",
        "commit_date": "2024-02-05T15:29:10Z",
        "message": "[Sync] Merge branch 'dev' into zfz/update-keyset-demo (#876)"
    },
    {
        "repo_url": "github.com/open-compass/opencompass",
        "filepath": "opencompass/models/huggingface.py",
        "commit_date": "2024-01-17T05:48:12Z",
        "message": "[Sync] Add InternLM2 Keyset Evaluation Demo (#807)\n\nCo-authored-by: zhangyifan1 <zhangyifan1@pjlab.org.cn>"
    },
    {
        "repo_url": "github.com/open-compass/opencompass",
        "filepath": "opencompass/models/huggingface.py",
        "commit_date": "2024-01-08T14:07:24Z",
        "message": "[Sync] Sync with internal codes 2023.01.08 (#777)"
    },
    {
        "repo_url": "github.com/open-compass/opencompass",
        "filepath": "opencompass/models/huggingface.py",
        "commit_date": "2024-01-08T08:40:02Z",
        "message": "[Feature] *_batch_generate* function, add the MultiTokenEOSCriteria (#772)\n\n* jiangjin1999: in the _batch_generate function, add the MultiTokenEOSCriteria feature to speed up inference.\n\n* jiangjin1999: in the _batch_generate function, add the MultiTokenEOSCriteria feature to speed up inference.\n\n---------\n\nCo-authored-by: jiangjin08 <jiangjin08@MBP-2F32S5MD6P-0029.local>\nCo-authored-by: jiangjin08 <jiangjin08@a.sh.vip.dianping.com>"
    },
    {
        "repo_url": "github.com/open-compass/opencompass",
        "filepath": "opencompass/models/huggingface.py",
        "commit_date": "2023-12-29T10:46:34Z",
        "message": "[Feat] update code config (#749)\n\n* [Feat] update code dataset\n\n* [Feat] update code dataset\n\n* [Feat] update code dataset"
    },
    {
        "repo_url": "github.com/open-compass/opencompass",
        "filepath": "opencompass/models/huggingface.py",
        "commit_date": "2023-12-11T09:42:53Z",
        "message": "[Sync] minor test (#683)"
    },
    {
        "repo_url": "github.com/open-compass/opencompass",
        "filepath": "opencompass/models/huggingface.py",
        "commit_date": "2023-12-01T07:08:38Z",
        "message": "[Feat] update gsm8k and math agent config (#652)\n\n* [Feat] update gsm8k and math agent config\n\n* minor fix"
    },
    {
        "repo_url": "github.com/open-compass/opencompass",
        "filepath": "opencompass/models/huggingface.py",
        "commit_date": "2023-11-23T06:05:59Z",
        "message": "[Sync] Fix cmnli, fix vicuna meta template, fix longbench postprocess and other minor fixes (#625)"
    },
    {
        "repo_url": "github.com/open-compass/opencompass",
        "filepath": "opencompass/models/huggingface.py",
        "commit_date": "2023-11-16T13:22:06Z",
        "message": "[Feat] support humaneval and mbpp pass@k (#598)\n\n* [Feat] support pass@ k\n\n* [Feat] support pass@k\n\n* [Feat] support pass@k\n\n* [Feat] support pass@k\n\n* [Feat] support pass@k\n\n* [Feat] support pass@k docs\n\n* update naming\n\n---------\n\nCo-authored-by: Leymore <zfz-960727@163.com>"
    },
    {
        "repo_url": "github.com/open-compass/opencompass",
        "filepath": "opencompass/models/huggingface.py",
        "commit_date": "2023-11-13T07:15:34Z",
        "message": "[Sync] update model configs (#574)"
    },
    {
        "repo_url": "github.com/open-compass/opencompass",
        "filepath": "opencompass/models/huggingface.py",
        "commit_date": "2023-10-27T08:54:19Z",
        "message": "[Fix] Enforce `do_sample=False` in HF model (#506)\n\n* update hf model wrapper\n\n* patch llama\n\n---------\n\nCo-authored-by: bot <bot@bot.com>"
    },
    {
        "repo_url": "github.com/open-compass/opencompass",
        "filepath": "opencompass/models/huggingface.py",
        "commit_date": "2023-10-27T03:45:41Z",
        "message": "[Feat] Add _set_model_kwargs_torch_dtype for HF model (#507)\n\n* add _set_model_kwargs_torch_dtype for hf models\n\n* add logger"
    },
    {
        "repo_url": "github.com/open-compass/opencompass",
        "filepath": "opencompass/models/huggingface.py",
        "commit_date": "2023-09-27T08:32:40Z",
        "message": "[Sync] Update LongEval (#443)"
    },
    {
        "repo_url": "github.com/open-compass/opencompass",
        "filepath": "opencompass/models/huggingface.py",
        "commit_date": "2023-08-31T08:53:39Z",
        "message": "[Fix] Fix when missing both pad and eos token (#287)\n\n* fix when missing both pad and eos token\n\n* update pad_token_id impl"
    },
    {
        "repo_url": "github.com/open-compass/opencompass",
        "filepath": "opencompass/models/huggingface.py",
        "commit_date": "2023-08-25T09:36:30Z",
        "message": "[Feature] Simplify entry script (#204)\n\n* [Feature] Simply entry script\n\n* update"
    },
    {
        "repo_url": "github.com/open-compass/opencompass",
        "filepath": "opencompass/models/huggingface.py",
        "commit_date": "2023-08-24T06:07:33Z",
        "message": "[Fix] Fix bugs for PeftModel generate (#252)\n\n* fix bugs\n\n* fix typo"
    },
    {
        "repo_url": "github.com/open-compass/opencompass",
        "filepath": "opencompass/models/huggingface.py",
        "commit_date": "2023-07-28T09:29:37Z",
        "message": "[Feature] Add SC (#126)\n\n* add self-consistency\n\n* add CoT method Self-Consistency\n\n* fix typo error and update openicl_eval\n\n* add tydiQA-GoldP task\n\n* fix sc\n\n* rename gsm8k_sc\n\n* fix sc\n\n* add self-consistency doc\n\n* refine sc\n\n---------\n\nAuthored-by: liushz <qq1791167085@163.com>"
    },
    {
        "repo_url": "github.com/open-compass/opencompass",
        "filepath": "opencompass/models/huggingface.py",
        "commit_date": "2023-07-18T08:21:43Z",
        "message": "[Feature] Support load PEFT adapter for HuggingFace model (#74)\n\n* support peft for HuggingFace model\n\n* add docstring"
    },
    {
        "repo_url": "github.com/open-compass/opencompass",
        "filepath": "opencompass/models/huggingface.py",
        "commit_date": "2023-07-17T07:59:10Z",
        "message": "[Enhancement] Test linting in CI and fix existing linting errors (#69)\n\n* [Enhancement] Test linting in CI\n\n* fix linting"
    },
    {
        "repo_url": "github.com/open-compass/opencompass",
        "filepath": "opencompass/models/huggingface.py",
        "commit_date": "2023-07-04T13:34:55Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2024-02-15T14:31:17Z",
        "message": "LLMs"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2024-01-31T13:25:21Z",
        "message": "bug fixes"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2023-12-07T20:54:50Z",
        "message": "Adding pretraining support for autoencoder 1/n"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2023-11-16T09:35:03Z",
        "message": "Improving Unified RR"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2023-11-12T13:42:04Z",
        "message": "Fix: Add reranking model to optimizer"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2023-11-11T23:14:27Z",
        "message": "LM updates"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2023-08-23T10:39:01Z",
        "message": "Implemented ANCE + clustered"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2023-02-11T23:37:19Z",
        "message": "Unified RR with separate poolers for RR"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2023-02-11T20:56:42Z",
        "message": "Unified RR works but not too well"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2023-02-01T13:04:12Z",
        "message": "Working clustering and large representations"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2022-09-18T17:04:27Z",
        "message": "Pass betas to adamw"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2022-06-10T10:42:52Z",
        "message": "Updated AdamW for all classes"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2022-05-29T15:39:15Z",
        "message": "Bug fixes"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2022-05-05T22:41:00Z",
        "message": "Added Nystromformer support for language model\nTested 512 and 2048 LM\nAdded Nystromformer support for NER"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2022-05-02T17:27:23Z",
        "message": "Added LayoutLMv2 to ner\nAdded RemBert to classification\nAdded Rembert to multiclass classification\nAdded Rembert to language modeling\nAdded Rembert to ner\nUpdated docs\nUpdated examples"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2022-01-20T12:11:10Z",
        "message": "Fixed bug where wrong predictions weren't returned in Classification"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2022-01-20T11:42:32Z",
        "message": "Fixed merge conflict"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2022-01-06T22:10:05Z",
        "message": "Removed extraneous quotes from special tokens xlmroberta tokenizer"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2022-01-05T20:02:35Z",
        "message": "Default special tokens corrected"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-11-29T19:56:16Z",
        "message": "bigbird training dataset samples were truncated"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-11-10T20:22:17Z",
        "message": "Merge branch 'master' into dpr"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-11-10T20:18:45Z",
        "message": "Resolved conflicts"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-11-04T09:18:07Z",
        "message": "Bugfix for SentencePiece trainger\nCommand line execution requires quotes\nAPI errantly adds the quote to the token"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-10-02T12:57:47Z",
        "message": "tensorboard_fix"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-10-02T11:08:52Z",
        "message": "Merge branch 'whr778-language_modeling_SentencePiece_bugfix'"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-10-02T11:08:44Z",
        "message": "Resolved conflict when merging sentencepiece fix"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-10-02T11:04:31Z",
        "message": "replaced tensorboardx imports with default torch version"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-09-24T12:31:40Z",
        "message": "Merge branch 'Zhylkaaa-cls_loss_refactor'"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-09-24T12:30:19Z",
        "message": "Formatting"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-09-11T18:38:00Z",
        "message": "added run id to model attributes"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-09-07T11:49:23Z",
        "message": "SentencePiece is not returning a '<unk>' token, even though it is set correctly in the tokenizer.\nExplicitly set tokenizer.unk_token.\nStripping and not setting tokenizer.unk_token caused misaligned predictions from original input tokens.  Modified the block to use tokenizer.unk_token if word_tokens is empty."
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-08-30T10:54:29Z",
        "message": "BigBird and XLMRoberta required tokens were not accounted for so that the vocab_size of a pretrained model was off by 2 for XLMRoberta and 3 for BigBird. Subsequently, the embedding size of a pretrained model was different than the size saved in config.json"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-08-25T18:16:33Z",
        "message": "DPR working"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-08-08T17:07:42Z",
        "message": "Started dpr"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-07-24T10:41:55Z",
        "message": "bug with sentencepiece model staging"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-07-24T10:32:00Z",
        "message": "sentencepiece saves the vocabulary and tokenization model on the fly... no need to save"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-07-24T10:29:44Z",
        "message": "Comments"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-07-24T10:26:20Z",
        "message": "updated call to sentencepiece vocab trainer based on xlmroberta or bigbird"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-07-24T10:22:48Z",
        "message": "Updated special tokens"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-07-24T10:21:02Z",
        "message": "Updated special tokens"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-07-24T10:02:18Z",
        "message": "Sentencepiece for xlmroberta... pretrain support"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-07-23T17:05:20Z",
        "message": "Bugfix: Wrong initialization class for xlmroberta"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-07-23T16:59:39Z",
        "message": "Bugfix: Wrong initialization class for xlmroberta"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-07-23T11:35:30Z",
        "message": "Whatever IDE code reformatter was used on this file on 5/18 injected a lot of parenthesis around if blocks and assignments changing a tensor into a single tuple of one tensor... I backed out almost all of the formatting retaining the changes up to 0.6.10"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-07-22T20:21:17Z",
        "message": "Added XLMRoberta to Language Modeling"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-07-19T16:57:22Z",
        "message": "Fixed a bug with loading the tokenizer model"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-07-19T16:41:51Z",
        "message": "Updated paths for moving the sentencepiece models after building\nThere is no way to provide an output path documented"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-07-14T13:06:54Z",
        "message": "Changed sentencepiece to google... transformers/tokenizers does not produce a bigbird compatible model"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-07-14T12:38:11Z",
        "message": "Changed sentencepiece to google... transformers/tokenizers does not produce a bigbird compatible model"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-07-13T14:41:46Z",
        "message": "Added bigbird and sentencepiece to language modeling"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-07-13T14:39:56Z",
        "message": "Added bigbird and sentencepiece to language modeling"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-07-09T03:22:03Z",
        "message": "fix tensorboard error"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-06-20T21:11:46Z",
        "message": "wandb repo label update"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-05-18T19:39:41Z",
        "message": "Formatted with black defaults"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-05-18T19:38:42Z",
        "message": "Added repo to wandb config"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-05-17T14:06:58Z",
        "message": "Huggingface Datasets fixes"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-03-27T10:21:23Z",
        "message": "formatting"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-03-27T09:56:37Z",
        "message": "Merge branch 'fix/1002-fix-load-on-cpu' of https://github.com/iomedhealth/simpletransformers into iomedhealth-fix/1002-fix-load-on-cpu"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-03-19T10:03:33Z",
        "message": "Updated changelog"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-03-07T18:58:40Z",
        "message": "T5 done too. Probably."
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-02-24T10:57:05Z",
        "message": "fix bug when loading pretrained model on machine without GPU"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-02-19T09:58:17Z",
        "message": "HF datasets for LM"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-02-18T16:49:15Z",
        "message": "Fixed polynomial_decay_schedule_power not being used correctly"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2021-01-11T03:14:59Z",
        "message": "Fix evaluate_during_training train mode"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-12-05T14:45:14Z",
        "message": "Added support for adafactor and various schedulers for all models"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-12-01T11:26:02Z",
        "message": "Compatibility fix for transformer v4.0"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-11-23T12:58:09Z",
        "message": "Fixed wandb sweep issues"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-11-09T13:07:55Z",
        "message": "Added layout lm. Electra compatibility"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-10-29T07:35:04Z",
        "message": "Updated arguments names for HF methods"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-10-12T13:19:31Z",
        "message": "Fixed issues with using mixed precision training with LanguageModelingModel"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-10-12T08:52:13Z",
        "message": "Merge branch 'fix-sweep-wandb-compatibility' of https://github.com/jonatanklosko/simpletransformers into jonatanklosko-fix-sweep-wandb-compatibility"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-10-11T13:23:02Z",
        "message": "The train_model method now returns training detail"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-10-10T12:31:08Z",
        "message": "Fix compatibility with different versions of wandb when sweeping."
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-10-07T15:04:24Z",
        "message": "Moved model.train()"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-09-26T07:39:39Z",
        "message": "Only average eval_loss for multigpu"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-09-25T18:45:32Z",
        "message": "Implemented layoutlm model"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-09-02T18:35:51Z",
        "message": "Added dynamic quantization support for more models"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-08-04T21:49:42Z",
        "message": "Removed nullcontext for python 3.6 compatibility"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-08-04T20:44:35Z",
        "message": "Fixed bug where amp import was attempted when fp16 is False"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-07-29T12:48:04Z",
        "message": "Use native PyTorch implementation of AMP instead of Apex"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-07-07T22:05:57Z",
        "message": "Bug fix in ELECTRA"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-07-05T21:53:08Z",
        "message": "Merge branch 'param_groups'"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-07-05T21:50:59Z",
        "message": "Merge branch 'master' into docs"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-07-05T21:50:51Z",
        "message": "Added get_named_parameters() method"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-07-05T20:10:20Z",
        "message": "Electra pre-training no longer replaces with random tokens"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-07-05T19:23:28Z",
        "message": "Added options to set custom param_groups"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-07-05T10:03:11Z",
        "message": "Resolved merge conflict"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-07-05T09:48:53Z",
        "message": "Merge branch 'master' into logging"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-07-04T20:49:22Z",
        "message": "fixed tokenizer saving issue in language modeling"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-07-04T19:11:13Z",
        "message": "Merge branch 'tokenizer-model-args' of https://github.com/taranais/simpletransformers into taranais-tokenizer-model-args"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-07-02T07:23:27Z",
        "message": "tokenizer model args"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-06-26T14:32:35Z",
        "message": "Don't create cache_dir when no_cache is True"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-06-24T17:27:37Z",
        "message": "Moved running loss inside tqdm description"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-06-22T14:29:41Z",
        "message": "Added sweep support in multimodal"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-06-22T14:11:25Z",
        "message": "Added sweep support to convai, language_generation, seq2seq"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-06-18T20:54:31Z",
        "message": "Added sweeps support for T5 and LM training"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-06-14T20:31:55Z",
        "message": "Added sweep support for multilabel classification"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-06-09T12:08:46Z",
        "message": "Formatting"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-06-09T11:59:23Z",
        "message": "Merge branch 'cahya-wirawan-dev'"
    },
    {
        "repo_url": "github.com/ThilinaRajapakse/simpletransformers",
        "filepath": "simpletransformers/language_modeling/language_modeling_model.py",
        "commit_date": "2020-06-09T11:58:40Z",
        "message": "Bug fix for when local_rank is not explicitly specified"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "inference-qlora.py",
        "commit_date": "2023-11-19T06:30:19Z",
        "message": "Update inference-qlora.py"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "inference-qlora.py",
        "commit_date": "2023-11-07T15:51:37Z",
        "message": "uploaded inference script using qlora"
    },
    {
        "repo_url": "github.com/PKU-YuanGroup/Video-LLaVA",
        "filepath": "videollava/model/builder.py",
        "commit_date": "2024-01-16T03:38:37Z",
        "message": "reformat code"
    },
    {
        "repo_url": "github.com/liucongg/ChatGLM-Finetuning",
        "filepath": "merge_lora.py",
        "commit_date": "2023-08-06T08:33:35Z",
        "message": "update code"
    },
    {
        "repo_url": "github.com/liucongg/ChatGLM-Finetuning",
        "filepath": "merge_lora.py",
        "commit_date": "2023-08-06T08:32:47Z",
        "message": "update code"
    },
    {
        "repo_url": "github.com/allenai/OLMo",
        "filepath": "inference/compression/dependencies/AutoGPTQ/auto_gptq/utils/peft_utils.py",
        "commit_date": "2023-09-14T00:10:10Z",
        "message": "style and lib sort changes"
    },
    {
        "repo_url": "github.com/allenai/OLMo",
        "filepath": "inference/compression/dependencies/AutoGPTQ/auto_gptq/utils/peft_utils.py",
        "commit_date": "2023-09-13T20:19:23Z",
        "message": "compression files"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "eval_distributed.py",
        "commit_date": "2023-11-19T00:39:45Z",
        "message": "Added multiple GPUs evaluation."
    },
    {
        "repo_url": "github.com/OpenPipe/OpenPipe",
        "filepath": "examples/classify-recipes/utils.py",
        "commit_date": "2023-08-24T23:49:44Z",
        "message": "more work"
    },
    {
        "repo_url": "github.com/OpenPipe/OpenPipe",
        "filepath": "examples/classify-recipes/utils.py",
        "commit_date": "2023-08-24T18:43:42Z",
        "message": "generate-data and some eval"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "examples/research_projects/stack_llama/scripts/merge_peft_adapter.py",
        "commit_date": "2023-10-06T09:04:58Z",
        "message": "Clarify docstrings, help messages, assert messages in merge_peft_adapter.py (#838)\n\nAn assertion was also corrected to the intended test condition"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "examples/research_projects/stack_llama/scripts/merge_peft_adapter.py",
        "commit_date": "2023-07-14T10:00:56Z",
        "message": "[`examples`] Big refactor of examples and documentation (#509)\n\n* added sfttrainer and rmtrainer example scripts.\n\n* added few lines in the documentation.\n\n* moved notebooks.\n\n* delete `examples/summarization`\n\n* remove from docs as well\n\n* refactor sentiment tuning\n\n* more refactoring.\n\n* updated docs for multi-adapter RL.\n\n* add research projects folder\n\n* more refactor\n\n* refactor docs.\n\n* refactor structure\n\n* add correct scripts all over the place\n\n* final touches\n\n* final touches\n\n* updated documentation from feedback."
    },
    {
        "repo_url": "github.com/netease-youdao/QAnything",
        "filepath": "third_party/FastChat/fastchat/model/model_adapter.py",
        "commit_date": "2024-01-25T04:29:46Z",
        "message": "ADD: add FastChat assets from commit e86e70d0b48c3d6e2223de96b20e377315dbf73e"
    },
    {
        "repo_url": "github.com/bigscience-workshop/petals",
        "filepath": "tests/test_full_model.py",
        "commit_date": "2023-08-30T02:59:33Z",
        "message": "Fix `.generate(input_ids=...)` (#485)"
    },
    {
        "repo_url": "github.com/bigscience-workshop/petals",
        "filepath": "tests/test_full_model.py",
        "commit_date": "2023-08-20T15:18:36Z",
        "message": "Make client compatible with transformers' GenerationMixin (#464)\n\nThis PR drops custom generation codes and introduces compatibility with `transformers.GenerationMixin` instead. This includes support for more sampling options (`top_p`, `top_k`, `repetition_penalty` requested in #460) and beam search - all that is now identical to running model with transformers locally.\n\nMost features (excluding beam search and other rarely used stuff) are also compatible with resuming existing sessions.\n\n### Breaking changes\n\nIf `.generate()` or forward passes are being run inside an `.inference_session()` context, they now use the opened session by default. So, these snippets are now equivalent:\n\n```python\n# Using default session\nwith model.inference_session(max_length=100):\n    output_ids = model.generate(input_ids, max_new_tokens=3)\n\n# Explicitly specifying a session\nwith model.inference_session(max_length=100) as sess:\n    output_ids = model.generate(input_ids, max_new_tokens=3, session=sess)\n```\n\nEarlier, the 1st snippet was creating a new session, which is not what most people expected (= such code was most likely to introduce a bug, which is now fixed)."
    },
    {
        "repo_url": "github.com/bigscience-workshop/petals",
        "filepath": "tests/test_full_model.py",
        "commit_date": "2023-08-08T15:10:27Z",
        "message": "Test Llama, rebalancing, throughput eval, and all CLI scripts (#452)\n\nThis PR extends CI to:\n\n1. Test Llama code using [TinyLlama-v0](https://huggingface.co/Maykeye/TinyLLama-v0).\n2. Test rebalancing (sets up a situation where the 1st server needs to change its original position).\n3. Check if benchmark scripts run (in case someone breaks its code). Note that the benchmark results are meaningless here (since they're measured on a tiny swarm of CPU servers, with low `--n_steps`).\n4. Test `petals.cli.run_dht`.\n5. Increase swap space and watch free RAM (a common issue is that actions are cancelled without explanation if there's not enough RAM - so it's a useful reminder + debug tool).\n6. Fix flapping tests for bloom-560m by increasing tolerance.\n\nOther minor changes: fix `--help` messages to show defaults, fix docs, tune rebalancing constants."
    },
    {
        "repo_url": "github.com/bigscience-workshop/petals",
        "filepath": "tests/test_full_model.py",
        "commit_date": "2023-07-22T19:10:46Z",
        "message": "Split long sequences into chunks (#403)\n\nThis PR is designed to avoid OOMs when processing long sequences that happen due to the huge attention logits matrices.\n\nCo-authored-by: Alexander Borzunov <borzunov.alexander@gmail.com>"
    },
    {
        "repo_url": "github.com/bigscience-workshop/petals",
        "filepath": "tests/test_full_model.py",
        "commit_date": "2023-07-12T12:22:28Z",
        "message": "Support peft LoRA adapters (#335)\n\nImplement an option to deploy PEFT adapters to a server. Clients can set active_adapter=... to use these adapters.\n\n---------\n\nCo-authored-by: Aleksandr Borzunov <borzunov.alexander@gmail.com>\nCo-authored-by: justheuristic <justheuristic@gmail.com>"
    },
    {
        "repo_url": "github.com/bigscience-workshop/petals",
        "filepath": "tests/test_full_model.py",
        "commit_date": "2023-06-23T11:46:10Z",
        "message": "Add LLaMA support (#323)\n\nThis PR:\n\n1. **Abolishes the model conversion procedure.** Now, models are downloaded directly from original repositories like https://huggingface.co/bigscience/bloom. Servers download only shards with blocks to be hosted, and clients download only shards with input/output embeddings and layernorms.\n\n    - BLOOM is loaded from `bigscience/bloom`, but we use the DHT prefix `bigscience/bloom-petals` for backward compatibility. Same with smaller BLOOMs and BLOOMZ.\n    - LLaMA can be loaded from any repo like `username/llama-65b-hf`, but we use the DHT prefix `llama-65b-hf` (without the username) to accomodate blocks from different repos (there're a few of them with minor differences, such as `Llama` vs. `LLaMA` in the class name).\n\n2. **Refactors the client to generalize it for multiple models.** Now, we have `petals.models` packages that contain model-specific code (e.g. `petals.models.bloom`, `petals.models.llama`). General code (e.g. CPU-efficient LM head, p-tuning) is kept in `petals.client`.\n\n3. **Introduces** `WrappedLlamaBlock`, `DistributedLlamaConfig`, `DistributedLlamaForCausalLM`, `DistributedLlamaForSequenceClassification`, and `DistributedLlamaModel` compatible with Petals functionality (p-tuning, adapters, etc.).\n\n4. **Introduces** `AutoDistributedConfig` that automatically chooses the correct config class (`DistributedLlamaConfig` or `DistributedBloomConfig`). The refactored configs contain all model-specific info for both clients and servers.\n\nUpgrade instructions:\n\n- Remove disk caches for blocks in old (converted) format to save disk space. That is, remove `~/.cache/petals/model--bigscience--bloom-petals` and  `~/.cache/petals/model--bigscience--bloomz-petals` directories (if present)."
    },
    {
        "repo_url": "github.com/bigscience-workshop/petals",
        "filepath": "tests/test_full_model.py",
        "commit_date": "2023-03-12T21:49:04Z",
        "message": "Speed up loading blocks using init with meta weights (#285)\n\n* Init WrappedBloomBlock with meta weights\n\n---------\n\nCo-authored-by: Alexander Borzunov <borzunov.alexander@gmail.com>"
    },
    {
        "repo_url": "github.com/bigscience-workshop/petals",
        "filepath": "tests/test_full_model.py",
        "commit_date": "2023-02-19T01:46:17Z",
        "message": "Use get_logger(__name__) instead of get_logger(__file__) (#265)"
    },
    {
        "repo_url": "github.com/bigscience-workshop/petals",
        "filepath": "tests/test_full_model.py",
        "commit_date": "2022-12-15T05:12:18Z",
        "message": "Fix logging: do not duplicate lines, enable colors in Colab (#156)"
    },
    {
        "repo_url": "github.com/bigscience-workshop/petals",
        "filepath": "tests/test_full_model.py",
        "commit_date": "2022-12-13T17:09:15Z",
        "message": "Add missing methods for SamplingAlgorithm, fix docstrings (#107)\n\n* Add missing methods for SamplingAlgorithm, fix docstrings\n\n* Add SamplingAlgorithm to _choose_sample_algorithm\n\n* Add test_sampling\n\n* Add a warning if sampling options were passed, but do_sample=False\n\n* Skip the sampling test for now\n\nCo-authored-by: Alexander Borzunov <borzunov.alexander@gmail.com>"
    },
    {
        "repo_url": "github.com/bigscience-workshop/petals",
        "filepath": "tests/test_full_model.py",
        "commit_date": "2022-12-13T08:03:49Z",
        "message": "Bump transformers to 4.25.1 (#151)\n\n- latest accelerate, transformers, huggingface_hub\n- rearrange attention caches to support https://github.com/huggingface/transformers/pull/18344\n- remove unused code\n- fix edge case where session crashes when receiving seq length 0\n- assert transformer version when importing WrappedBloomBlock\n\nCo-authored-by: Alexander Borzunov <borzunov.alexander@gmail.com>\nCo-authored-by: Max Ryabinin <mryabinin0@gmail.com>"
    },
    {
        "repo_url": "github.com/bigscience-workshop/petals",
        "filepath": "tests/test_full_model.py",
        "commit_date": "2022-11-30T06:41:13Z",
        "message": "Make Petals a pip-installable package (attempt 2) (#102)\n\n1. Petals can be now installed using `pip install git+https://github.com/bigscience-workshop/petals`\n    - In case if you already cloned the repo, you can do `pip install .` or `pip install .[dev]`\n2. Moved `src` => `src/petals`\n    - Replaced `from src.smth import smth` with `from petals.smth import smth`\n3. Moved `cli` => `src/petals/cli`\n    - Replaced `python -m cli.run_smth` with `python -m petals.cli.run_smth` (all utilities are now available right after pip installation)\n4. Moved the `requirements*.txt` contents to `setup.cfg` (`requirements.txt` for packages is not supported well by modern packaging utils)\n5. Increased the package version from `0.2` to `1.0alpha1`"
    },
    {
        "repo_url": "github.com/bigscience-workshop/petals",
        "filepath": "tests/test_full_model.py",
        "commit_date": "2022-11-28T09:02:07Z",
        "message": "Add Beam Search decoding algorithm (#87)\n\nAdd beam_search"
    },
    {
        "repo_url": "github.com/bigscience-workshop/petals",
        "filepath": "tests/test_full_model.py",
        "commit_date": "2022-08-29T18:04:37Z",
        "message": "Let users specify sequence length instead of assuming 2048 (#52)\n\n- Maximum length is now provided in `.inference_session(max_length=100)`\n   - previously, we would always assume max length = 2048\n- added a generic way to forward **kwargs to inference session\n  - for compatibility with #47 \n  - Note to @borzunov : it does *not* pass them arbitrarily, but instead checks for kwarg names at the bottom level\n- run_server can be started with a custom max_length for inference\n- renamed --cache_size_bytes to --attention_cache_bytes (to avoid collision with --cache_dir)\n- --attn_cache_bytes can now support humane file sizes (e.g. 300MB instead of 314572800)\n- made some server-side errors more human-readable to user (e.g. when max length is exceeded)\n\nCo-authored-by: Aleksandr Borzunov <borzunov.alexander@gmail.com>\nCo-authored-by: Alexander Borzunov <hxrussia@gmail.com>"
    },
    {
        "repo_url": "github.com/bigscience-workshop/petals",
        "filepath": "tests/test_full_model.py",
        "commit_date": "2022-08-17T15:50:52Z",
        "message": "Reduce vocabulary size in test model, fix bug in routing when overlapped (#45)\n\nThis PR reduces this vocabulary size to save memory during conversion, keeping only the first 50k tokens\nAs a result, \n\n* tests that load client-side embeddings need significantly less RAM\n* we can now run CI tests with 4 servers instead of 2 - needed to test routing - see bugs uncovered\n* some of the servers now use load balancing\n* CI convert_model now takes 4-5 minutes (was 6-7)"
    },
    {
        "repo_url": "github.com/bigscience-workshop/petals",
        "filepath": "tests/test_full_model.py",
        "commit_date": "2022-07-27T06:19:45Z",
        "message": "Pack of Inference Changes (#37)\n\n* Return multibatch mode\n\n* Add tests\n\n* fixes"
    },
    {
        "repo_url": "github.com/bigscience-workshop/petals",
        "filepath": "tests/test_full_model.py",
        "commit_date": "2022-07-22T19:38:40Z",
        "message": "Miscellaneous fixes to automatic tests (#35)\n\n1. __Reduce memory usage in in test_full_model__ \n     - previously, loading the full model would consistently fail IF github is enforcing memory limit [example](https://github.com/bigscience-workshop/distributed-bloom/runs/7473920049?check_suite_focus=true)\n     - the new version uses accelerate to save 2GB of peak memory, that was previously used when loading both reference model AND its state dict at the same time - only to load that state dict :)\n2. __Safer delays when creating servers__\n    - run-tests will now wait for a few seconds after creating the first server - and before creating the second one, so as to make \nsure that the first server creates a DHT instance that subsequent servers can connect to.\n    - also increased the wait time after creating servers by 30 seconds to make sure we load the model in time even when bumping into slow remotes on HF side\n3. __Fix environment variables in CI to avoid build conflicts__\n    - the previous code was using a wrong environment variable that was always \"main\". The current one will correctly resolve branch name, both in main and on pull request.\n    - For reference, below you can find sample environments when running CI in both cases: on pull request and on push to main.\n\n<details>\n<summary> Environment variables when building this branch (on pull request) </summary>\n\nSELENIUM_JAR_PATH=/usr/share/java/selenium-server.jar GOROOT_1_17_X64=/opt/hostedtoolcache/go/1.17.12/x64 CONDA=/usr/share/miniconda GITHUB_WORKSPACE=/home/runner/work/distributed-bloom/distributed-bloom JAVA_HOME_11_X64=/usr/lib/jvm/temurin-11-jdk-amd64 GITHUB_PATH=/home/runner/work/_temp/_runner_file_commands/add_path_0aba811a-a04b-40a2-ba42-79efb2723e9e GITHUB_ACTION=__run_2 JAVA_HOME=/usr/lib/jvm/temurin-11-jdk-amd64 GITHUB_RUN_NUMBER=98 RUNNER_NAME=GitHub Actions 3 GRADLE_HOME=/usr/share/gradle-7.5 XDG_CONFIG_HOME=/home/runner/.config DOTNET_SKIP_FIRST_TIME_EXPERIENCE=1 ANT_HOME=/usr/share/ant JAVA_HOME_8_X64=/usr/lib/jvm/temurin-8-jdk-amd64 HOMEBREW_PREFIX=/home/linuxbrew/.linuxbrew pythonLocation=/opt/hostedtoolcache/Python/3.9.13/x64 GITHUB_REF_TYPE=branch HOMEBREW_CLEANUP_PERIODIC_FULL_DAYS=3650 BOOTSTRAP_HASKELL_NONINTERACTIVE=1 *** PIPX_BIN_DIR=/opt/pipx_bin DEPLOYMENT_BASEPATH=/opt/runner GITHUB_ACTIONS=true ANDROID_NDK_LATEST_HOME=/usr/local/lib/android/sdk/ndk/24.0.8215888 GITHUB_SHA=3b457e8a14e5ecb0d65d6e4c0e9161f7756a8861 POWERSHELL_DISTRIBUTION_CHANNEL=GitHub-Actions-ubuntu20 DOTNET_MULTILEVEL_LOOKUP=0 GITHUB_REF=refs/pull/35/merge RUNNER_OS=Linux GITHUB_REF_PROTECTED=false HOME=/home/runner GITHUB_API_URL=https://api.github.com/ LANG=C.UTF-8 BLOOM_TESTING_WRITE_TOKEN=*** RUNNER_TRACKING_ID=github_cc9b46e4-56a1-40c5-ba08-5a91e21f0f95 STATS_KEEPALIVE=false RUNNER_ARCH=X64 RUNNER_TEMP=/home/runner/work/_temp EDGEWEBDRIVER=/usr/local/share/edge_driver GITHUB_ENV=/home/runner/work/_temp/_runner_file_commands/set_env_0aba811a-a04b-40a2-ba42-79efb2723e9e GITHUB_EVENT_PATH=/home/runner/work/_temp/_github_workflow/event.json INVOCATION_ID=8f0072e74f2847c0851e7ff9b5e4af7c GITHUB_EVENT_NAME=pull_request GITHUB_RUN_ID=2720198689 JAVA_HOME_17_X64=/usr/lib/jvm/temurin-17-jdk-amd64 ANDROID_NDK_HOME=/usr/local/lib/android/sdk/ndk-bundle GITHUB_STEP_SUMMARY=/home/runner/work/_temp/_runner_file_commands/step_summary_0aba811a-a04b-40a2-ba42-79efb2723e9e HOMEBREW_NO_AUTO_UPDATE=1 GITHUB_ACTOR=justheuristic NVM_DIR=/home/runner/.nvm SGX_AESM_ADDR=1 GITHUB_RUN_ATTEMPT=1 ANDROID_HOME=/usr/local/lib/android/sdk GITHUB_GRAPHQL_URL=https://api.github.com/graphql ACCEPT_EULA=Y RUNNER_USER=runner USER=runner GITHUB_SERVER_URL=https://github.com/ HOMEBREW_CELLAR=/home/linuxbrew/.linuxbrew/Cellar PIPX_HOME=/opt/pipx GECKOWEBDRIVER=/usr/local/share/gecko_driver CHROMEWEBDRIVER=/usr/local/share/chrome_driver SHLVL=0 ANDROID_SDK_ROOT=/usr/local/lib/android/sdk VCPKG_INSTALLATION_ROOT=/usr/local/share/vcpkg HOMEBREW_REPOSITORY=/home/linuxbrew/.linuxbrew/Homebrew RUNNER_TOOL_CACHE=/opt/hostedtoolcache ImageVersion=20220717.1 DOTNET_NOLOGO=1 GITHUB_REF_NAME=35/merge STATS_PFS=true GRAALVM_11_ROOT=/usr/local/graalvm/graalvm-ce-java11-22.1.0 GITHUB_JOB=convert-model LD_LIBRARY_PATH=/opt/hostedtoolcache/Python/3.9.13/x64/lib XDG_RUNTIME_DIR=/run/user/1001 AZURE_EXTENSION_DIR=/opt/az/azcliextensions PERFLOG_LOCATION_SETTING=RUNNER_PERFLOG GITHUB_REPOSITORY=bigscience-workshop/distributed-bloom ANDROID_NDK_ROOT=/usr/local/lib/android/sdk/ndk-bundle CHROME_BIN=/usr/bin/google-chrome GOROOT_1_18_X64=/opt/hostedtoolcache/go/1.18.4/x64 GITHUB_RETENTION_DAYS=90 JOURNAL_STREAM=8:23653 RUNNER_WORKSPACE=/home/runner/work/distributed-bloom LEIN_HOME=/usr/local/lib/lein LEIN_JAR=/usr/local/lib/lein/self-installs/leiningen-2.9.8-standalone.jar GITHUB_ACTION_REPOSITORY= PATH=/opt/hostedtoolcache/Python/3.9.13/x64/bin:/opt/hostedtoolcache/Python/3.9.13/x64:/home/linuxbrew/.linuxbrew/bin:/home/linuxbrew/.linuxbrew/sbin:/home/runner/.local/bin:/opt/pipx_bin:/home/runner/.cargo/bin:/home/runner/.config/composer/vendor/bin:/usr/local/.ghcup/bin:/home/runner/.dotnet/tools:/snap/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin RUNNER_PERFLOG=/home/runner/perflog GITHUB_BASE_REF=main CI=true SWIFT_PATH=/usr/share/swift/usr/bin ImageOS=ubuntu20 GITHUB_REPOSITORY_OWNER=bigscience-workshop GITHUB_HEAD_REF=fix-branch-name GITHUB_ACTION_REF= GITHUB_WORKFLOW=Tests DEBIAN_FRONTEND=noninteractive AGENT_TOOLSDIRECTORY=/opt/hostedtoolcache GOROOT_1_16_X64=/opt/hostedtoolcache/go/1.16.15/x64 _=/usr/bin/env\n</details>\n<details>\n<summary> Environment variables when building in main (on push) </summary>\n\nSELENIUM_JAR_PATH=/usr/share/java/selenium-server.jar GOROOT_1_17_X64=/opt/hostedtoolcache/go/1.17.11/x64 CONDA=/usr/share/miniconda GITHUB_WORKSPACE=/home/runner/work/distributed-bloom/distributed-bloom JAVA_HOME_11_X64=/usr/lib/jvm/temurin-11-jdk-amd64 GITHUB_PATH=/home/runner/work/_temp/_runner_file_commands/add_path_cd6c1ed2-0d0f-496d-b7a6-ffa476dcc144 GITHUB_ACTION=__run_2 JAVA_HOME=/usr/lib/jvm/temurin-11-jdk-amd64 GITHUB_RUN_NUMBER=53 RUNNER_NAME=GitHub Actions 3 GRADLE_HOME=/usr/share/gradle-7.4.2 XDG_CONFIG_HOME=/home/runner/.config DOTNET_SKIP_FIRST_TIME_EXPERIENCE=1 ANT_HOME=/usr/share/ant JAVA_HOME_8_X64=/usr/lib/jvm/temurin-8-jdk-amd64 HOMEBREW_PREFIX=/home/linuxbrew/.linuxbrew pythonLocation=/opt/hostedtoolcache/Python/3.9.13/x64 GITHUB_REF_TYPE=branch HOMEBREW_CLEANUP_PERIODIC_FULL_DAYS=3650 BOOTSTRAP_HASKELL_NONINTERACTIVE=1 *** PIPX_BIN_DIR=/opt/pipx_bin DEPLOYMENT_BASEPATH=/opt/runner GITHUB_ACTIONS=true ANDROID_NDK_LATEST_HOME=/usr/local/lib/android/sdk/ndk/24.0.8215888 GITHUB_SHA=49242d81006454d687ff3293c49f6bf234793627 POWERSHELL_DISTRIBUTION_CHANNEL=GitHub-Actions-ubuntu20 DOTNET_MULTILEVEL_LOOKUP=0 GITHUB_REF=refs/heads/main RUNNER_OS=Linux GITHUB_REF_PROTECTED=true HOME=/home/runner GITHUB_API_URL=https://api.github.com/ LANG=C.UTF-8 BLOOM_TESTING_WRITE_TOKEN=*** RUNNER_TRACKING_ID=github_7668f06a-99e1-4ed1-81e9-46d75fab3f33 STATS_KEEPALIVE=false RUNNER_ARCH=X64 RUNNER_TEMP=/home/runner/work/_temp EDGEWEBDRIVER=/usr/local/share/edge_driver GITHUB_ENV=/home/runner/work/_temp/_runner_file_commands/set_env_cd6c1ed2-0d0f-496d-b7a6-ffa476dcc144 GITHUB_EVENT_PATH=/home/runner/work/_temp/_github_workflow/event.json INVOCATION_ID=3dadac48981b4a679a33224db89be1ed GITHUB_EVENT_NAME=push GITHUB_RUN_ID=2680158280 JAVA_HOME_17_X64=/usr/lib/jvm/temurin-17-jdk-amd64 ANDROID_NDK_HOME=/usr/local/lib/android/sdk/ndk-bundle GITHUB_STEP_SUMMARY=/home/runner/work/_temp/_runner_file_commands/step_summary_cd6c1ed2-0d0f-496d-b7a6-ffa476dcc144 HOMEBREW_NO_AUTO_UPDATE=1 GITHUB_ACTOR=justheuristic NVM_DIR=/home/runner/.nvm SGX_AESM_ADDR=1 GITHUB_RUN_ATTEMPT=1 ANDROID_HOME=/usr/local/lib/android/sdk GITHUB_GRAPHQL_URL=https://api.github.com/graphql ACCEPT_EULA=Y RUNNER_USER=runner USER=runner GITHUB_SERVER_URL=https://github.com/ HOMEBREW_CELLAR=/home/linuxbrew/.linuxbrew/Cellar PIPX_HOME=/opt/pipx GECKOWEBDRIVER=/usr/local/share/gecko_driver CHROMEWEBDRIVER=/usr/local/share/chrome_driver SHLVL=0 ANDROID_SDK_ROOT=/usr/local/lib/android/sdk VCPKG_INSTALLATION_ROOT=/usr/local/share/vcpkg HOMEBREW_REPOSITORY=/home/linuxbrew/.linuxbrew/Homebrew RUNNER_TOOL_CACHE=/opt/hostedtoolcache ImageVersion=20220710.1 DOTNET_NOLOGO=1 GITHUB_REF_NAME=main STATS_PFS=true GRAALVM_11_ROOT=/usr/local/graalvm/graalvm-ce-java11-22.1.0 GITHUB_JOB=convert-model LD_LIBRARY_PATH=/opt/hostedtoolcache/Python/3.9.13/x64/lib XDG_RUNTIME_DIR=/run/user/1001 AZURE_EXTENSION_DIR=/opt/az/azcliextensions PERFLOG_LOCATION_SETTING=RUNNER_PERFLOG GITHUB_REPOSITORY=bigscience-workshop/distributed-bloom CHROME_BIN=/usr/bin/google-chrome ANDROID_NDK_ROOT=/usr/local/lib/android/sdk/ndk-bundle GOROOT_1_18_X64=/opt/hostedtoolcache/go/1.18.3/x64 GITHUB_RETENTION_DAYS=90 JOURNAL_STREAM=8:22000 RUNNER_WORKSPACE=/home/runner/work/distributed-bloom LEIN_HOME=/usr/local/lib/lein LEIN_JAR=/usr/local/lib/lein/self-installs/leiningen-2.9.8-standalone.jar GITHUB_ACTION_REPOSITORY= PATH=/opt/hostedtoolcache/Python/3.9.13/x64/bin:/opt/hostedtoolcache/Python/3.9.13/x64:/home/linuxbrew/.linuxbrew/bin:/home/linuxbrew/.linuxbrew/sbin:/home/runner/.local/bin:/opt/pipx_bin:/home/runner/.cargo/bin:/home/runner/.config/composer/vendor/bin:/usr/local/.ghcup/bin:/home/runner/.dotnet/tools:/snap/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin RUNNER_PERFLOG=/home/runner/perflog GITHUB_BASE_REF= CI=true SWIFT_PATH=/usr/share/swift/usr/bin ImageOS=ubuntu20 GITHUB_REPOSITORY_OWNER=bigscience-workshop GITHUB_HEAD_REF= GITHUB_ACTION_REF= GITHUB_WORKFLOW=Tests DEBIAN_FRONTEND=noninteractive AGENT_TOOLSDIRECTORY=/opt/hostedtoolcache GOROOT_1_16_X64=/opt/hostedtoolcache/go/1.16.15/x64 _=/usr/bin/env\n</details>\n\n\n\nCo-authored-by: Dmitry Baranchuk <dmitrybaranchuk@gmail.com>"
    },
    {
        "repo_url": "github.com/bigscience-workshop/petals",
        "filepath": "tests/test_full_model.py",
        "commit_date": "2022-07-19T01:28:04Z",
        "message": "Implement RemoteSequential slicing and extra repr, add tests (#30)\n\n- finish renaming RemoteSequenceInfo -> RemoteSequenceManager (why: if it was an *Info, user would expect it to be similar - to a dataclass; whereas in actuality, the class is doing heavy network interactions on its own)\n- implement RemoteSequenceManager.make_sequence (from https://pastebin.com/uXgy2U8B )\n- make RemoteSequentialInferenceSession use RemoteSequenceManager.make_sequence\n- make tests pass again\n- make it possible to create inference session without RemoteTransformerBlock\n- make a standalone test for RemoteSequential\n- rollback convert-model\n\nCo-authored-by: Tim Dettmers <tim.dettmers@gmail.com>"
    },
    {
        "repo_url": "github.com/bigscience-workshop/petals",
        "filepath": "tests/test_full_model.py",
        "commit_date": "2022-07-15T22:59:23Z",
        "message": "Add automated tests (#23)\n\nThis PR will run basic tests automatically on each subsequent PR\n\n- convert a small model on every PR\n- run existing tests on every PR\n- enforce black / isort\n- require checks on merge\n- make sure tests are not flappy\n\nCo-authored-by: Alexander Borzunov <hxrussia@gmail.com>\nCo-authored-by: Dmitry Baranchuk <dmitrybaranchuk@gmail.com>"
    },
    {
        "repo_url": "github.com/bigscience-workshop/petals",
        "filepath": "tests/test_full_model.py",
        "commit_date": "2022-07-08T16:11:55Z",
        "message": "rm prefix from tests"
    },
    {
        "repo_url": "github.com/bigscience-workshop/petals",
        "filepath": "tests/test_full_model.py",
        "commit_date": "2022-07-06T22:06:03Z",
        "message": "black-isort"
    },
    {
        "repo_url": "github.com/bigscience-workshop/petals",
        "filepath": "tests/test_full_model.py",
        "commit_date": "2022-07-06T22:04:47Z",
        "message": "black-isort"
    },
    {
        "repo_url": "github.com/bigscience-workshop/petals",
        "filepath": "tests/test_full_model.py",
        "commit_date": "2022-07-04T22:54:47Z",
        "message": "design interface & refactoring"
    },
    {
        "repo_url": "github.com/bigscience-workshop/petals",
        "filepath": "tests/test_full_model.py",
        "commit_date": "2022-07-01T00:48:36Z",
        "message": "compare logits to logits"
    },
    {
        "repo_url": "github.com/bigscience-workshop/petals",
        "filepath": "tests/test_full_model.py",
        "commit_date": "2022-07-01T00:38:38Z",
        "message": "test full model exact match"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2024-01-30T04:26:15Z",
        "message": "update infer."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2024-01-29T07:20:20Z",
        "message": "update eos token."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2024-01-28T13:23:01Z",
        "message": "update model infer."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2024-01-18T10:20:23Z",
        "message": "add single tune."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2024-01-07T15:03:40Z",
        "message": "update group"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-11-16T07:48:21Z",
        "message": "update infer."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-11-15T06:48:12Z",
        "message": "update infer use greedy decoding."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-10-23T09:06:58Z",
        "message": "update left padding."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-10-23T08:51:09Z",
        "message": "update decode with batch infer."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-10-23T07:40:32Z",
        "message": "update readme."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-10-23T07:02:02Z",
        "message": "update batch infer."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-10-23T06:57:54Z",
        "message": "update batch infer."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-10-23T04:30:54Z",
        "message": "update infer for do sample."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-10-23T03:51:37Z",
        "message": "update longlora finetune."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-10-17T11:16:06Z",
        "message": "update template name."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-09-10T14:53:08Z",
        "message": "update inf."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-09-07T06:00:19Z",
        "message": "update multi gpu infer."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-08-29T02:43:46Z",
        "message": "update save to jsonl."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-08-28T07:40:12Z",
        "message": "update stop str."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-08-28T07:21:32Z",
        "message": "inference support qwen."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-08-06T07:47:42Z",
        "message": "update inference file."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-08-06T03:25:18Z",
        "message": "update inference stream."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-08-06T02:39:29Z",
        "message": "update gen"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-08-06T02:16:17Z",
        "message": "update inference with gradio"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-08-06T02:03:31Z",
        "message": "update inference with cli."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-08-04T08:38:50Z",
        "message": "update model max length."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-08-04T04:25:12Z",
        "message": "support special template tokenizer."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-08-03T12:50:14Z",
        "message": "del join."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-08-03T12:37:50Z",
        "message": "update infer with stream."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-08-03T11:18:42Z",
        "message": "update infer."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-08-03T09:14:51Z",
        "message": "update infer."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-08-03T08:52:10Z",
        "message": "update thread."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-08-03T02:02:59Z",
        "message": "update default template name."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-07-28T08:49:23Z",
        "message": "update infer stream."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-07-28T07:07:07Z",
        "message": "update infer"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-07-27T13:54:29Z",
        "message": "update infer"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-07-27T04:53:20Z",
        "message": "update conv for openchat sharegpt gpt4"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-07-26T04:00:03Z",
        "message": "update eos token is none."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-07-23T15:39:35Z",
        "message": "update infer"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-07-23T14:43:29Z",
        "message": "update infer"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-07-23T14:26:26Z",
        "message": "update repetition_penalty"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-07-23T14:22:36Z",
        "message": "update gradio infer."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-07-23T03:08:03Z",
        "message": "update inference."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-07-21T11:42:51Z",
        "message": "adjust alpaca data."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-07-21T11:09:37Z",
        "message": "update multi-round demo."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-07-21T09:50:37Z",
        "message": "support multi round infer."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-07-14T02:55:07Z",
        "message": "update inference."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-07-13T03:14:59Z",
        "message": "update merged model."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-07-12T14:05:33Z",
        "message": "update infer."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-07-12T13:44:51Z",
        "message": "update infer."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-06-16T02:57:11Z",
        "message": "update auto model."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-06-15T12:19:04Z",
        "message": "update gradio demo."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-06-15T10:17:15Z",
        "message": "support baichuan-7b."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference.py",
        "commit_date": "2023-06-14T09:34:37Z",
        "message": "remove folder."
    },
    {
        "repo_url": "github.com/SCIR-HI/Huatuo-Llama-Med-Chinese",
        "filepath": "infer_literature.py",
        "commit_date": "2023-08-07T13:46:05Z",
        "message": "Update Huozi-based model\n\nMajor update. Please try our new Huozi-based model, which is much better."
    },
    {
        "repo_url": "github.com/SCIR-HI/Huatuo-Llama-Med-Chinese",
        "filepath": "infer_literature.py",
        "commit_date": "2023-04-24T02:00:47Z",
        "message": "add literature data\n\nWe tried to use the GPT3.5 API to integrate the conclusion in the medical literature as external information into multiple rounds of dialogue, and based on this, we fine-tuned the instructions of LLaMA."
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm_cli/playground/llama2_qlora.py",
        "commit_date": "2023-11-19T15:25:08Z",
        "message": "feat(engine): CTranslate2 (#698)\n\n* chore: update instruction for dependencies\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* feat(experimental): CTranslate2\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bentoml/OpenLLM",
        "filepath": "openllm-python/src/openllm_cli/playground/llama2_qlora.py",
        "commit_date": "2023-11-15T04:20:50Z",
        "message": "chore(cli): move playground to CLI components (#655)\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "passkey_retrivial.py",
        "commit_date": "2023-11-15T04:59:52Z",
        "message": "Update passkey_retrivial.py"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "passkey_retrivial.py",
        "commit_date": "2023-09-26T13:11:12Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/shibing624/pycorrector",
        "filepath": "examples/gpt/use_origin_transformers_demo.py",
        "commit_date": "2023-11-03T14:25:45Z",
        "message": "update t5 and conv seq2seq model."
    },
    {
        "repo_url": "github.com/shibing624/pycorrector",
        "filepath": "examples/gpt/use_origin_transformers_demo.py",
        "commit_date": "2023-11-02T10:01:53Z",
        "message": "update chat result."
    },
    {
        "repo_url": "github.com/shibing624/pycorrector",
        "filepath": "examples/gpt/use_origin_transformers_demo.py",
        "commit_date": "2023-11-02T08:38:30Z",
        "message": "update chat result."
    },
    {
        "repo_url": "github.com/shibing624/pycorrector",
        "filepath": "examples/gpt/use_origin_transformers_demo.py",
        "commit_date": "2023-11-02T08:13:29Z",
        "message": "update origin model for predict."
    },
    {
        "repo_url": "github.com/shibing624/pycorrector",
        "filepath": "examples/gpt/use_origin_transformers_demo.py",
        "commit_date": "2023-11-02T08:05:24Z",
        "message": "update origin model for predict."
    },
    {
        "repo_url": "github.com/shibing624/pycorrector",
        "filepath": "examples/gpt/use_origin_transformers_demo.py",
        "commit_date": "2023-11-02T08:01:19Z",
        "message": "update origin model."
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2024-02-29T06:36:29Z",
        "message": "Add User input + max tokens requested exceeds model context window error response (#1325)\n\n* added User input + max tokens requested exceeds model context window error response.\n\nSigned-off-by: Ye, Xinyu <xinyu.ye@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2024-02-29T04:57:26Z",
        "message": "[NeuralChat] Support microsoft/biogpt model as per the request (#1327)\n\n* Support microsoft/biogpt model\n\n* add dependency\n\n---------\n\nSigned-off-by: lvliang-intel <liang1.lv@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2024-02-23T13:33:51Z",
        "message": "[API Changed] Dispatch the backend based on model_type (#1298)\n\nCo-authored-by: changwangss <chang1.wang@intel.com>\nCo-authored-by: VincyZhang <wenxin.zhang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2024-02-23T11:50:17Z",
        "message": "[NeuralChat] Fix Qwen chat model output undesirable on HPU issue caused by input padding (#1303)\n\n* Fix Qwen chat model output undesirable on HPU issue caused by input padding.\n\n* change pad_token instead of not padding, fix issue of qwen model ignore attention_mask when batch size is 1.\n\nSigned-off-by: Ye, Xinyu <xinyu.ye@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2024-02-23T10:54:06Z",
        "message": "update requirements_xpu.txt in neural chat to add neural_speed and ip\u2026 (#1294)\n\nCo-authored-by: lvliang-intel <liang1.lv@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2024-02-23T06:17:18Z",
        "message": "[Neuralchat] Notebook retrieval update (#1182)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2024-02-08T09:16:07Z",
        "message": "[NeuralChat] Support GGUF model in NeuralChat (#1200)\n\n* Support GGUF model in NeuralChat\n\nSigned-off-by: lvliang-intel <liang1.lv@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2024-02-08T09:14:07Z",
        "message": "[NeuralChat] Fix response issue of model.predict (#1221)\n\n* fix response issue of model.predict\n\nSigned-off-by: LetongHan <letong.han@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2024-02-08T02:42:32Z",
        "message": "resolve woq quantization error when running neuralchat (#1268)\n\n* fixes quantization error caused by pad_token_id not being set"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2024-02-04T22:45:09Z",
        "message": "[NeuralChat] Support DeciLM-7B and DeciLM-7B-instruct (#1228)\n\n* Support DeciLM-7B and DeciLM-7B-instruct\n\nSigned-off-by: lvliang-intel <liang1.lv@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2024-02-04T00:52:38Z",
        "message": "[NeuralChat] Support deepseek-coder models in NeuralChat (#1251)\n\n* Support deepseek-coder model\n\nSigned-off-by: lvliang-intel <liang1.lv@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2024-02-02T14:47:45Z",
        "message": "[NeuralChat] Add Docker use case for CodeGen (#1236)\n\n* Add Docker use case for CodeGen\n\nSigned-off-by: LetongHan <letong.han@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2024-02-02T14:46:56Z",
        "message": "[NeuralChat] Support SQL generation (#1234)\n\n* Support SQL generation\n\nSigned-off-by: lvliang-intel <liang1.lv@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2024-02-01T05:12:24Z",
        "message": "[NeuralChat] Support GPTQ, AWQ model in NeuralChat (#1206)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2024-01-31T09:18:17Z",
        "message": "Add precommit config (#1216)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2024-01-29T09:25:47Z",
        "message": "add pre-commit-ci codespell check (#1188)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2024-01-24T09:36:10Z",
        "message": "add validated sw (#1168)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2024-01-19T14:19:24Z",
        "message": "[NeuralChat] Support vLLM serving (#1120)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2024-01-19T14:12:24Z",
        "message": "[NeuralChat] Fix GPT-J model name issue (#1157)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2024-01-18T13:32:51Z",
        "message": "[Neuralchat] Improve error code UT coverage (#1132)\n\nCo-authored-by: lvliang-intel <liang1.lv@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2024-01-17T03:46:43Z",
        "message": "[LLM Runtime] neural_speed_replace_graph (#1129)\n\nCo-authored-by: Meng, Hengyu <hengyu.meng@intel.com>\nCo-authored-by: Wenxin Zhang <wenxin.zhang@intel.com>\nCo-authored-by: lvliang-intel <liang1.lv@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2024-01-16T01:24:50Z",
        "message": "[NeuralChat] Support compatible stats format (#1112)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2024-01-13T00:06:19Z",
        "message": "Support Phi-2 model (#1137)\n\nSigned-off-by: lvliang-intel <liang1.lv@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2024-01-02T12:00:54Z",
        "message": "[NeuralChat] Align inference parameters num_beams with HF transformers (#1092)\n\nAlign inference parameters num_beams with HF transformers \n\nSigned-off-by: lvliang-intel <liang1.lv@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2024-01-01T23:35:58Z",
        "message": "[NeuralChat] Support LLM runtime ggml int4 (#1098)\n\n* Support llm runtime ggml int4\n\nSigned-off-by: lvliang-intel <liang1.lv@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-12-27T23:15:09Z",
        "message": "Fix magicoder model tokenizer issue and remove codegen streaming redundant end format (#1086)\n\n* Fix magicoder tokenizer issue and streaming redundant end format\n\nSigned-off-by: lvliang-intel <liang1.lv@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-12-25T23:08:57Z",
        "message": "[NeuralChat] Support Mixtral-8x7B-v0.1 model (#972)\n\n* Support Mixstral-8x7b model\n\nSigned-off-by: lvliang-intel <liang1.lv@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-12-25T23:05:46Z",
        "message": "[NeuralChat] Fix magicoder model tokenizer issue (#1075)\n\n* fix magicoder tokenizer issue\n\nSigned-off-by: lvliang-intel <liang1.lv@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-12-25T12:33:24Z",
        "message": "[NeuralChat] Support magicoder model (#1067)\n\n* Support magicoder model and refine load model\n\nSigned-off-by: lvliang-intel <liang1.lv@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-12-20T11:54:31Z",
        "message": "[NeuralChat] Support Salesforce codegen model (#973)\n\n* Support Salesforce/codegen model\n\nSigned-off-by: lvliang-intel <liang1.lv@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-12-20T11:53:20Z",
        "message": "Fix LLM runtime int4 issues (#959)\n\nSigned-off-by: lvliang-intel <liang1.lv@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-12-20T05:53:50Z",
        "message": "Fix llama model inferene hang issue (#970)\n\nSigned-off-by: lvliang-intel <liang1.lv@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-12-20T05:34:31Z",
        "message": "add deepspeed support for ise-uiuc/Magicoder-S-CL-7B which only has safetensors chkpt (#968)\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-12-14T02:56:50Z",
        "message": "[NeuralChat] Fix PC codegen streaming issue and set 'Intel/neural-chat-7b-v3-1' as default model (#920)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-12-14T02:52:30Z",
        "message": "[NeuralChat] Add Qwen model unit test case (#919)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-12-12T12:03:52Z",
        "message": "[NeuralChat] support return error code (#650)\n\n* support error coed for NeuralChat\n\nSigned-off-by: lvliang-intel <liang1.lv@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-12-09T05:19:50Z",
        "message": "add peft model support in deepspeed sharded mode (#884)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-12-09T02:33:16Z",
        "message": "[Infra] use python logging (#752)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-12-08T13:55:09Z",
        "message": "support assisted generation for neuralchat (#896)\n\nSigned-off-by: LetongHan <letong.han@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-11-30T06:23:43Z",
        "message": "[LLM] Support chatglm/falcon series and unified int8 loading (#814)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-11-28T05:18:57Z",
        "message": "[NeuralChat]  Add optimization pipeline for LLaMa model familly (#783)\n\n* Add optimization pipeline for LLaMa model familly.\n\nSigned-off-by: Mikolaj Zyczynski <mikolaj.zyczynski@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-11-24T09:06:52Z",
        "message": "add neuralchat support ipex.optimize_transformers sq int8 model loading (#764)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-11-24T00:55:42Z",
        "message": "[NeuralChat] Fix tokenizer issue for optimized model (#761)\n\n* Fix tokenizer issue for optimized models\n\nSigned-off-by: lvliang-intel <liang1.lv@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-11-21T14:03:16Z",
        "message": "[NeuralChat] Remove unnecessary model load during optimizing model (#722)\n\n* Remove unnecessary model load during optimizing model\n\nSigned-off-by: lvliang-intel <liang1.lv@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-11-20T06:23:32Z",
        "message": "Support Mistral model in NeuralChat (#710)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-11-20T02:08:06Z",
        "message": "Support CodeLlama model in NeuralChat (#711)\n\n* Support neural-chat-7b-v3 and neural-chat-7b-v3-1\n\nSigned-off-by: lvliang-intel <liang1.lv@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-11-17T12:14:08Z",
        "message": "Support neural-chat-7b-v3 and neural-chat-7b-v3-1 (#698)\n\n* Support neural-chat-7b-v3 and neural-chat-7b-v3-1\n\nSigned-off-by: lvliang-intel <liang1.lv@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-11-17T01:29:01Z",
        "message": "enable mistral in habana, fix issue in generate.py (#683)\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-11-16T00:47:19Z",
        "message": "[Infra] update docker related readme (#690)\n\n* update docker related readme\n\n* Revert \"Fix ChatGLM2 llm runtime int4 issue (#679)\"\n\nThis reverts commit 0e9a29d48c3ae9b5c5e382e57b8b43b92c648de0."
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-11-15T11:42:51Z",
        "message": "Fix ChatGLM2 llm runtime int4 issue (#679)\n\nSigned-off-by: Lv, Liang1 <liang1.lv@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-11-09T02:33:12Z",
        "message": "[NeuralChat] Support optimization for server mode and add ut cases (#543)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-11-08T05:19:01Z",
        "message": "Fix llm runtime int4 multi-turn inference issue (#642)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-10-27T02:14:44Z",
        "message": "Add itrex llm runtime graph int4 notebook (#399)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-10-26T01:46:48Z",
        "message": "Habana 1.12 (#535)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-10-23T03:53:10Z",
        "message": "Remove OneDNN env setint for BF16 inference (#509)\n\nSigned-off-by: lvliang-intel <liang1.lv@intel.com>\nCo-authored-by: VincyZhang <wenxin.zhang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-10-23T03:53:10Z",
        "message": "Fix ChatGLM2 model loading issue (#510)\n\n* Fix ChatGLM2 model loading issue\n\nSigned-off-by: lvliang-intel <liang1.lv@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-10-19T01:40:23Z",
        "message": "Fix NeuralChat starcoder mha fusion issue (#494)\n\nSigned-off-by: Wang, Chang <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-10-17T22:44:32Z",
        "message": "NeuralChat support IPEX int8 model (#486)\n\n* to support ipex int8 model\n\nSigned-off-by: changwangss <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-10-13T03:31:47Z",
        "message": "Enable Qwen-7B-Chat (#432)\n\nCo-authored-by: lvliang-intel <liang1.lv@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-09-25T00:53:19Z",
        "message": "Enable xpu for neuralchat predict (#379)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/models/model_utils.py",
        "commit_date": "2023-09-18T06:02:15Z",
        "message": "Refactored inference API, and extracted prompt template as individual module (#315)\n\n* Refactored inference API, and extracted prompt template as individual module.\n\nSigned-off-by: Ye, Xinyu <xinyu.ye@intel.com>\n\n* restore workflow code.\n\nSigned-off-by: Ye, Xinyu <xinyu.ye@intel.com>\n\n* fixed pylint error.\n\nSigned-off-by: Ye, Xinyu <xinyu.ye@intel.com>\n\n* moved mpt_trace.py to llm.\n\nSigned-off-by: Ye, Xinyu <xinyu.ye@intel.com>\n\n* create prompts folder.\n\nSigned-off-by: Ye, Xinyu <xinyu.ye@intel.com>\n\n* path changes.\n\nSigned-off-by: Ye, Xinyu <xinyu.ye@intel.com>\n\n---------\n\nSigned-off-by: Ye, Xinyu <xinyu.ye@intel.com>"
    },
    {
        "repo_url": "github.com/xusenlinzy/api-for-open-llm",
        "filepath": "api/adapter/model.py",
        "commit_date": "2024-01-19T04:12:30Z",
        "message": "Improve code and add support internlm2"
    },
    {
        "repo_url": "github.com/xusenlinzy/api-for-open-llm",
        "filepath": "api/adapter/model.py",
        "commit_date": "2023-12-14T07:28:16Z",
        "message": "Replace |= with update method"
    },
    {
        "repo_url": "github.com/xusenlinzy/api-for-open-llm",
        "filepath": "api/adapter/model.py",
        "commit_date": "2023-12-07T04:14:14Z",
        "message": "Fix bug for p-tuning"
    },
    {
        "repo_url": "github.com/xusenlinzy/api-for-open-llm",
        "filepath": "api/adapter/model.py",
        "commit_date": "2023-12-04T05:49:14Z",
        "message": "Add docstrings for functions"
    },
    {
        "repo_url": "github.com/xusenlinzy/api-for-open-llm",
        "filepath": "api/adapter/model.py",
        "commit_date": "2023-11-30T11:43:08Z",
        "message": "Fix typo error"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "workflows/chatbot/demo/basic_frontend/fastchat/model/apply_lora.py",
        "commit_date": "2024-01-31T09:18:17Z",
        "message": "Add precommit config (#1216)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "workflows/chatbot/demo/basic_frontend/fastchat/model/apply_lora.py",
        "commit_date": "2023-06-06T03:30:42Z",
        "message": "Add chatbot basic mode frontend (#984)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "tests/test_no_peft.py",
        "commit_date": "2024-02-15T03:37:41Z",
        "message": "pre-commit: replace linters + formatters with Ruff; fix some issues (#1300)\n\n* pre-commit: replace linters + formatters with Ruff\n\n* Don't use bare except\n\n* Clean up `noqa`s\n\n* Enable Ruff UP; apply auto-fixes\n\n* Enable Ruff B; apply fixes\n\n* Enable Ruff T with exceptions\n\n* Enable Ruff C (complexity); autofix\n\n* Upgrade Ruff to 0.2.0"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "tests/test_no_peft.py",
        "commit_date": "2024-02-01T22:49:03Z",
        "message": "Codemod Unittest assertions to bare asserts (#1301)\n\n* Remove stray commas from test data\n\n* Codemod Unittest assertions to bare asserts\n\n* Make `assertAlmostEqual` tests more idiomatic\n\n* DRY some test strings"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "tests/test_no_peft.py",
        "commit_date": "2023-03-07T14:08:21Z",
        "message": "`peft` integration (#163)\n\n* adds a hacky peft example\n\n* fixes bug due to missing \"prepare_model_for_training\"\n\n* Formatting\n\n* adds peft to requirements\n\n* Update trl/trainer/ppo_trainer.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* gpt neo runs\n\n* changes requested on the PR\n\n* style\n\n* updates to prepare_model_for_int8_training PEFT PR https://github.com/huggingface/peft/pull/105\n\n* updates to prepare_model_for_int8_training PEFT PR https://github.com/huggingface/peft/pull/105\n\n* adds missing 8-bit attribute to modeling base\n\n* adds lr to example script\n\n* adds missing train to trainer\n\n* disables caching temporarily while I debug something\n\n* debugging issues with unstable training\n\n* Fix peft + int8 (#170)\n\n* add fix\n\n* another fix\n\n* Auto stash before merge of \"peft-example\" and \"origin/peft-example\"\n\n* adds peft model types to modeling base\n\n* reduces memory usage using adapters and no ref model.\n\n* adds support for EleutherAI/gpt-neox-20b\n\n* example for peft finetune of cm model\n\n* removes hacky research code\n\n* fixing the rebase and some typos\n\n* style\n\n* style2\n\n* adds gradient checkpointing to base model\n\n* cleans up comments\n\n* moves config and other pretrained_model properties to __init__\n\n* make style\n\n* added tests\n\n* change dependency\n\n* Update .github/workflows/tests.yml\n\n* fix test\n\n* fix style and failing tests\n\n* make quality\n\n* revert change\n\n* rm unneeded change\n\n* revert changes\n\n* rm changes\n\n* rm changes\n\n* rm uneeded change\n\n* Update trl/models/modeling_base.py\n\n* revert uneeded changes\n\n* make style\n\n* adapt suggestions\n\n* fix tests\n\n* attempt to fix\n\n* fix\n\n* fix\n\n* add no peft test\n\n* revert\n\n* remove unneded check\n\n* more tests\n\n* fix logic\n\n* add `save_pretrained` support\n\n* fix quality\n\n* clean up\n\n* clean up\n\n* stronger test\n\n* refactor comments\n\n* make style\n\n* attempt to add non-peft tests\n\n* remove test runner\n\n* format\n\n* fix test\n\n* move `train` on top\n\n* fix peft import\n\n* make quality\n\n* fixes typo\n\n* adds peft example to docs\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\nCo-authored-by: younesbelakda <younesbelkada@gmail.com>"
    },
    {
        "repo_url": "github.com/HarderThenHarder/transformers_tasks",
        "filepath": "LLM/chatglm_finetune/train.py",
        "commit_date": "2023-08-02T12:16:49Z",
        "message": "add LLMs Trainer"
    },
    {
        "repo_url": "github.com/LianjiaTech/BELLE",
        "filepath": "train/src/entry_point/zero_inference_backend_without_trainer.py",
        "commit_date": "2023-09-26T11:39:39Z",
        "message": "update parameter name and bug fix (#523)\n\n* update docker\n\n* update parameter name and bug fix\n\n* delete redundancy file"
    },
    {
        "repo_url": "github.com/LianjiaTech/BELLE",
        "filepath": "train/src/entry_point/zero_inference_backend_without_trainer.py",
        "commit_date": "2023-08-10T10:27:54Z",
        "message": "add zero inference"
    },
    {
        "repo_url": "github.com/PixArt-alpha/PixArt-alpha",
        "filepath": "app/app_lcm.py",
        "commit_date": "2024-01-20T06:05:46Z",
        "message": "Refractoring (#61)\n\nApproved"
    },
    {
        "repo_url": "github.com/PixArt-alpha/PixArt-alpha",
        "filepath": "app/app_lcm.py",
        "commit_date": "2024-01-11T05:11:15Z",
        "message": "Feat/pixart ctrlnet public (#70)"
    },
    {
        "repo_url": "github.com/lxe/simple-llm-finetuner",
        "filepath": "trainer.py",
        "commit_date": "2023-04-19T01:27:29Z",
        "message": "Add abort button"
    },
    {
        "repo_url": "github.com/lxe/simple-llm-finetuner",
        "filepath": "trainer.py",
        "commit_date": "2023-04-11T04:14:47Z",
        "message": "resolved adapater not found bug (#45)"
    },
    {
        "repo_url": "github.com/lxe/simple-llm-finetuner",
        "filepath": "trainer.py",
        "commit_date": "2023-04-06T23:43:16Z",
        "message": "Full rework: Version 2 release (#37)"
    },
    {
        "repo_url": "github.com/OpenBMB/ToolBench",
        "filepath": "toolbench/inference/LLM/tool_llama_lora_model.py",
        "commit_date": "2023-09-26T16:38:44Z",
        "message": "add inference args"
    },
    {
        "repo_url": "github.com/OpenBMB/ToolBench",
        "filepath": "toolbench/inference/LLM/tool_llama_lora_model.py",
        "commit_date": "2023-07-27T08:09:51Z",
        "message": "update new version"
    },
    {
        "repo_url": "github.com/OpenPipe/OpenPipe",
        "filepath": "trainer/src/trainer/export_weights.py",
        "commit_date": "2024-02-15T07:58:17Z",
        "message": "Merge on CPU"
    },
    {
        "repo_url": "github.com/OpenPipe/OpenPipe",
        "filepath": "trainer/src/trainer/export_weights.py",
        "commit_date": "2024-02-09T02:09:40Z",
        "message": "Mixtral support\n\nAllow users with beta access to fine-tune Mixtral.\n\nAlso, allow all users to fine-tune Llama 2 and Mistral."
    },
    {
        "repo_url": "github.com/OpenPipe/OpenPipe",
        "filepath": "trainer/src/trainer/export_weights.py",
        "commit_date": "2024-01-23T22:38:40Z",
        "message": "Support exports of merged models\n\nPeople keep asking for this so I've productized it."
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/guanaco.py",
        "commit_date": "2023-07-25T10:11:50Z",
        "message": "add more models + gptq mode"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/guanaco.py",
        "commit_date": "2023-07-01T17:56:43Z",
        "message": "add local files only option"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/guanaco.py",
        "commit_date": "2023-06-10T13:08:36Z",
        "message": "add use_safetensor=False"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/guanaco.py",
        "commit_date": "2023-06-09T04:33:05Z",
        "message": "update vram requirements records"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/guanaco.py",
        "commit_date": "2023-06-08T15:32:11Z",
        "message": "tested gpu and cpu options"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/guanaco.py",
        "commit_date": "2023-06-08T01:59:11Z",
        "message": "exp w/ camel and sam"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/guanaco.py",
        "commit_date": "2023-06-05T00:25:50Z",
        "message": "update examples"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/guanaco.py",
        "commit_date": "2023-06-04T14:50:44Z",
        "message": "disable force download & example view"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/guanaco.py",
        "commit_date": "2023-05-27T00:22:42Z",
        "message": "add guanaco"
    },
    {
        "repo_url": "github.com/zjunlp/DeepKE",
        "filepath": "example/llm/InstructKGC/src/model/adapter.py",
        "commit_date": "2024-02-27T08:23:07Z",
        "message": "update InstructKGC add IEPile"
    },
    {
        "repo_url": "github.com/project-baize/baize-chatbot",
        "filepath": "merge_lora.py",
        "commit_date": "2023-05-25T19:22:35Z",
        "message": "Add v2 collect"
    },
    {
        "repo_url": "github.com/project-baize/baize-chatbot",
        "filepath": "merge_lora.py",
        "commit_date": "2023-04-21T19:34:33Z",
        "message": "Create merge_lora.py"
    },
    {
        "repo_url": "github.com/liguodongiot/llm-action",
        "filepath": "train/alpaca-lora/export_hf_checkpoint.py",
        "commit_date": "2023-07-23T11:43:58Z",
        "message": "fix"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2024-01-09T07:36:48Z",
        "message": "add base model template."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-11-15T06:48:12Z",
        "message": "update infer use greedy decoding."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-10-30T03:59:14Z",
        "message": "update gradio"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-10-26T06:49:18Z",
        "message": "update gradio history"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-10-26T06:19:01Z",
        "message": "update gradio"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-10-26T06:13:06Z",
        "message": "update gradio version."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-10-26T03:49:51Z",
        "message": "update gradio sample."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-10-17T11:16:06Z",
        "message": "update template name."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-09-12T05:31:07Z",
        "message": "update gradio history."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-09-12T05:18:59Z",
        "message": "update gradio ."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-09-12T04:11:11Z",
        "message": "update history."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-09-12T04:07:54Z",
        "message": "update history."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-09-12T03:46:45Z",
        "message": "update special tokens."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-08-28T07:40:12Z",
        "message": "update stop str."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-08-28T07:21:32Z",
        "message": "inference support qwen."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-08-06T03:25:18Z",
        "message": "update inference stream."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-08-06T03:04:38Z",
        "message": "update gradio"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-08-06T02:59:31Z",
        "message": "update gradio"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-08-06T02:50:53Z",
        "message": "update gradio"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-08-06T02:39:29Z",
        "message": "update gen"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-08-06T02:16:17Z",
        "message": "update inference with gradio"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-08-04T08:38:50Z",
        "message": "update model max length."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-08-04T04:25:12Z",
        "message": "support special template tokenizer."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-08-03T02:02:59Z",
        "message": "update default template name."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-07-27T13:54:29Z",
        "message": "update infer"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-07-27T04:53:50Z",
        "message": "update conv for openchat sharegpt gpt4"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-07-26T04:00:03Z",
        "message": "update eos token is none."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-07-23T15:39:35Z",
        "message": "update infer"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-07-23T14:43:29Z",
        "message": "update infer"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-07-23T14:22:36Z",
        "message": "update gradio infer."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-07-21T11:42:51Z",
        "message": "adjust alpaca data."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-07-21T11:09:37Z",
        "message": "update multi-round demo."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-07-16T02:13:41Z",
        "message": "\u4f18\u5316prompt"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-06-16T02:57:11Z",
        "message": "update auto model."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-06-15T12:19:04Z",
        "message": "update gradio demo."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "gradio_demo.py",
        "commit_date": "2023-06-14T09:34:37Z",
        "message": "remove folder."
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/mistral.py",
        "commit_date": "2023-11-19T14:28:03Z",
        "message": "add mistral and zephyr"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/chat.py",
        "commit_date": "2024-02-06T08:16:50Z",
        "message": "[Fix] Fix no space in chat output using InternLM2. (#357) (#404)\n\n* [Fix] Fix no space in chat output using InternLM2. (#357)\n\n* Update chat.py\n\n* Update utils.py\n\n* Update utils.py\n\n* fix pre-commit\n\n---------\n\nCo-authored-by: Zhihao Lin <36994684+LZHgrla@users.noreply.github.com>\nCo-authored-by: LZHgrla <linzhihao@pjlab.org.cn>"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/chat.py",
        "commit_date": "2024-01-24T05:33:46Z",
        "message": "[Improve] Add `--repetition-penalty` for `xtuner chat` (#351)\n\nfix"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/chat.py",
        "commit_date": "2024-01-17T02:47:15Z",
        "message": "[Fix] Add `trust_remote_code=True` for AutoModel  (#328)\n\nupdate"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/chat.py",
        "commit_date": "2024-01-11T15:29:39Z",
        "message": "[Improve] Redesign the `prompt_template` (#294)\n\n* update\n\n* update cfgs\n\n* update\n\n* fix bugs\n\n* upload docs\n\n* rename\n\n* update\n\n* Revert \"update cfgs\"\n\nThis reverts commit 93966aa7578fc3963d7c9c3e25eb1f9489e6e6fa.\n\n* update cfgs\n\n* update\n\n* rename\n\n* rename\n\n* fix bc\n\n* fix stop_word\n\n* fix\n\n* fix\n\n* Update prompt_template.md"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/chat.py",
        "commit_date": "2023-12-26T09:39:01Z",
        "message": "[Feature] Support LLaVA (#196)\n\n* v1\n\n* add load_image\n\n* update cfg image url\n\n* del fig\n\n* update\n\n* temp\n\n* update convert\n\n* update chat_mm\n\n* add exclude_frozen_parameters for deepspeed\n\n* update chat\n\n* update xtuner help msg\n\n* fix bugs\n\n* revert bf16 deepspeed\n\n* fix bugs\n\n* add visual_select_layer for chat\n\n* improve pth_to_hf\n\n* rename projecter_pth to pretrained_pth\n\n* temp\n\n* update requirements\n\n* add cfgs\n\n* update\n\n* fix pre-commit\n\n* optim chat\n\n* optim chat\n\n* Delete xtuner/model/unused.py\n\n* move dispatch to a deeper folder\n\n* add projector\n\n* update\n\n* del model/projector\n\n* fix bugs\n\n* add docs\n\n* update\n\n* update\n\n* update\n\n* update\n\n* enhance resume for map_fn\n\n* update import\n\n* add llava_internlm_chat_7b_clip_vit_large_p14\n\n* update dispatch\n\n* update dispatch\n\n* add link\n\n* update max_length\n\n* update max_length\n\n* update hyp\n\n* align\n\n* move yi flash attn\n\n* fix pre-commit\n\n* update deepspeed requirements\n\n* add mmbench script\n\n* install openpyxl\n\n* add entry_point for mmbench\n\n* save args\n\n* update mmbench\n\n* update max_length\n\n* add llama2 qlora\n\n* update mmbench\n\n* fix mmbench bugs\n\n* use osp instead of os.path\n\n* refactor pth_to_hf\n\n* update chat and mmbench to support --llava\n\n* align to chat\n\n* update entry_point\n\n* add vicuna template\n\n* add vicuna_7b_v15\n\n* fix pre-commit\n\n* add vicuna_7b_v1.5 qlora\n\n* skip_special_tokens for decode text\n\n* remove do_sample\n\n* add warmup\n\n* fix pre-commit\n\n* Update dataset_prepare.md\n\n* Update dataset_prepare.md\n\n* Add KEEP_STSTEM for template\n\n* remove\n\n* fix vicuna template\n\n* clean cfgs\n\n* add cfgs\n\n* fix pre-commit\n\n* add --language for mmbench\n\n* fix bugs\n\n* fix pretrain bug\n\n* support visual_encoder lora\n\n* fix bugs\n\n* add paramwise_cfg\n\n* remove print_peft_model_trainable_parameters\n\n* fix bugs\n\n* add paramwise_cfg for DeepSpeedOptimWrapper\n\n* fix engine deepspeed paramwise_cfg bug\n\n* fix encode_fn bug\n\n* fix\n\n* fix pad_image_to_square bugs\n\n* Add space for system to avoid mismatch of 'USER' token\n\n* revert to adding bos_token at each conv\n\n* revert for paramwise_cfg\n\n* better cfgs?\n\n* fix import bug\n\n* fix import bug\n\n* pretrain align\n\n* update prepare_inputs_labels_for_multimodal\n\n* 1792\n\n* support length_grouped_samplers\n\n* 1792\n\n* remove KEEP_SYSTEM\n\n* remove system in cfg\n\n* update 336 cfg\n\n* add torch_dtype for mmbench and chat\n\n* group 50\n\n* quant for pretrain\n\n* update cfgs\n\n* refactor cfgs\n\n* add length for concat dataset\n\n* update requirements\n\n* fix typo\n\n* add template for internlm pretrain\n\n* no zh\n\n* remove 20b cfgs\n\n* fix pre-commit\n\n* revert invalid input\n\n* rename\n\n* Update README.md\n\n* Update README_zh-CN.md\n\n* fix pre-commit\n\n* remove llava_zh from docs\n\n* qlora 512\n\n* rename llava map_fn\n\n* update cfgs\n\n* update model urls\n\n* add docs link\n\n* add llava docs\n\n* update docs\n\n* update urls\n\n* add citation\n\n* fix README\n\n* move\n\n* update\n\n* vicuna pretrain with prompt\n\n* rename\n\n* add results\n\n* fix pre-commit\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* Update README.md\n\n* Update README_zh-CN.md\n\n* Update README_zh.md\n\n* Update README_zh.md\n\n* Update README.md\n\n* Update README_zh.md\n\n* Update README.md\n\n* Update README.md\n\n* fix typo\n\n* fix\n\n* Update README.md\n\n* Update README_zh-CN.md\n\n* rename\n\n* auto cn_string\n\n* fix pre-commit\n\n* rename\n\n* remove language\n\n* add VLMEvalKit\n\n* rename VLLM to VLM\n\n* add the download links of MMBench\n\n* update\n\n* update readme\n\n* update\n\n* update\n\n* update merge\n\n* fix cfg bug\n\n* Update README.md\n\n* Update README_zh.md\n\n* update\n\n* fix\n\n* update requirements\n\n* Update runtime.txt\n\n* Update runtime.txt\n\n* Update runtime.txt\n\n* Update README.md\n\n* Update README.md\n\n* Update README_zh.md\n\n* fix pre-commit\n\n* fix\n\n* update mmbench prompt\n\n* fix bugs\n\n* fix bugs\n\n* update docs\n\n* update\n\n* update\n\n* Update README.md"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/chat.py",
        "commit_date": "2023-12-18T02:57:37Z",
        "message": "[Bug] Fix bugs when chat with --lagent (#269)\n\nfix_chat_for_model_kwargs"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/chat.py",
        "commit_date": "2023-11-30T06:39:02Z",
        "message": "[Bug] Support auto detect torch_dtype in chat.py (#250)\n\n* fix torch dtype\n\n* fix torch dtype\n\n* Update xtuner/tools/chat.py\n\nCo-authored-by: Zhihao Lin <36994684+LZHgrla@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Zhihao Lin <36994684+LZHgrla@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/chat.py",
        "commit_date": "2023-11-14T12:48:02Z",
        "message": "[Feature] Support ChatGLM3-6B (#222)\n\n* add chatglm cfgs\n\n* add chatglm3 template\n\n* fix bos_token_id bug\n\n* add encode_special_tokens\n\n* update readme"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/chat.py",
        "commit_date": "2023-11-08T09:31:02Z",
        "message": "[Feature] Add mistral pretrain (#204)\n\n* [Feature] Add mistral pretrain\n\n* [feat] rename pretrain_map_fn\n\n* [feat] add custom hook\n\n* [feat] change mistral config name\n\n* Update chat.py\n\n* Update xtuner/utils/templates.py\n\nCo-authored-by: Zhihao Lin <36994684+LZHgrla@users.noreply.github.com>\n\n* Update xtuner/configs/mistral/mistral_7b_qlora_skypile_pretrain_e1.py\n\nCo-authored-by: Zhihao Lin <36994684+LZHgrla@users.noreply.github.com>\n\n* Update xtuner/configs/mistral/mistral_7b_qlora_skypile_pretrain_e1.py\n\nCo-authored-by: Zhihao Lin <36994684+LZHgrla@users.noreply.github.com>\n\n* Update xtuner/configs/mistral/mistral_7b_qlora_skypile_pretrain_e1.py\n\nCo-authored-by: Zhihao Lin <36994684+LZHgrla@users.noreply.github.com>\n\n* fix pre-commit\n\n---------\n\nCo-authored-by: Zhihao Lin <36994684+LZHgrla@users.noreply.github.com>\nCo-authored-by: LZHgrla <linzhihao@pjlab.org.cn>"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/chat.py",
        "commit_date": "2023-10-23T07:21:48Z",
        "message": "[Feature] Support the fine-tuning of MSAgent dataset (#156)\n\n* fix bugs for invalid data\n\n* add modelscope in requirements\n\n* add process_ms_dataset\n\n* add msagent dataset\n\n* fix\n\n* add msagent map_fn\n\n* add cfg\n\n* fix pre-commit\n\n* fix eval generate input\n\n* fix f-string bug\n\n* modify question\n\n* add more cfgs\n\n* rename\n\n* support lagent chat\n\n* msagent uses system text\n\n* update cfgs\n\n* update chat\n\n* update readme"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/chat.py",
        "commit_date": "2023-10-12T10:55:45Z",
        "message": "[Refactor] Refactor the preprocess of dataset (#163)\n\n* support system for template\n\n* fix alpaca map_fn\n\n* refactor map_fns\n\n* refactor internlm-7b cfgs\n\n* fix moss_sft\n\n* fix templates\n\n* fix cfgs\n\n* add system for evaluate_chat_hook\n\n* use template system\n\n* fix bugs\n\n* add task\n\n* fix bugs\n\n* rename\n\n* fix\n\n* update templates\n\n* update cfgs\n\n* Update dataset_format.md\n\n* Update dataset_format.md\n\n* Update dataset_format.md\n\n* Update dataset_format.md\n\n* Update dataset_format.md\n\n* Update dataset_format.md\n\n* Update dataset_format.md\n\n* Update dataset_format.md\n\n* Update single_turn_conversation.md\n\n* update\n\n* fix pre-commit\n\n* update\n\n* add toc\n\n* chat supports system\n\n* Update README.md\n\n* Update README_zh-CN.md\n\n* fix typo\n\n* remove chat docs\n\n* Update README_zh-CN.md\n\n* Update README.md\n\n* Update README.md\n\n* Update README_zh-CN.md\n\n* fix pre-commit\n\n* fix language\n\n* update help msg\n\n* add eos_token for qwen\n\n* fix\n\n* Update single_turn_conversation.md\n\n* Update single_turn_conversation.md\n\n* fix apis\n\n* support system-output text\n\n* fix\n\n* fix\n\n* fix typo"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/chat.py",
        "commit_date": "2023-09-27T06:49:12Z",
        "message": "[Feature] Support to remove history for chat script (#144)\n\n* [Feature] Support to remove history for chat script\n\n* fix typo"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/chat.py",
        "commit_date": "2023-09-27T06:48:07Z",
        "message": "[Fix] Add `--offload-folder` for merge and chat (#140)\n\nadd `--offload-folder` for merge and chat"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/chat.py",
        "commit_date": "2023-09-07T09:56:07Z",
        "message": "[Fix] Use `token_id` instead of `token` for `encode_fn` & Set eval mode before generate (#107)\n\n* set eval mode before generate\n\n* use token_id instead of token"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/chat.py",
        "commit_date": "2023-09-05T14:05:41Z",
        "message": "[Fix] fix generation config (#98)\n\n* fix generation config\n\n* add code llama chat template\n\n* add code llama chat template"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/chat.py",
        "commit_date": "2023-09-05T08:08:28Z",
        "message": "[Improve] Redesign convert tools (#96)\n\n* refactor tools\n\n* modify entry_point\n\n* modify docs\n\n* update docs\n\n* fix\n\n* fix\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Update README_zh-CN.md\n\n* fix pre-commit\n\n* rename converter\n\n* update pth2hf\n\n* rename pth2hf to pth_to_hf\n\n* add fp32 for pth_to_hf\n\n* Update README.md\n\n* Update README_zh-CN.md\n\n* Update README_zh-CN.md\n\n* Update README.md\n\n* Update README_zh-CN.md\n\n* Update README_zh-CN.md\n\n* Update README.md\n\n* Update README.md\n\n* Update README_zh-CN.md\n\n* fix pre-commit"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/chat.py",
        "commit_date": "2023-08-29T10:48:52Z",
        "message": "[Feature] Support ChatGLM (#62)\n\n* no space for ChatGLM\n\n* cast to inference during generate\n\n* support chatglm tokenizer\n\n* support chatglm qlora\n\n* fix pre-commit\n\n* update doc\n\n* update\n\n* Update README.md\n\n* Update README_zh-CN.md\n\n* add chatglm template\n\n* fix chat bugs\n\n* add round for template_map_fn\n\n* add round for EvaluateChatHook"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/chat.py",
        "commit_date": "2023-08-28T12:01:27Z",
        "message": "[Refactor] Decouple map fn (#54)\n\n* decouple map_fn\n\n* decouple map_fn\n\n* decouple map_fn\n\n* modify config\n\n* advance the --prompt-template args\n\n* fix config\n\n* Update sql_map_fn.py\n\n* Update sql_map_fn.py\n\n* Update alpaca_map_fn.py\n\n* rename hooks\n\n* refactor template_map_fn\n\n* add openai_map_fn\n\n* fix config\n\n* fix config\n\n* fix print_log\n\n* fix medical map_fn\n\n* improve\n\n* rename dataset_map_fn to dataset_map_fns\n\n* add alpaca_enzh & oasst1 concat dataset config\n\n---------\n\nCo-authored-by: LZHgrla <36994684+LZHgrla@users.noreply.github.com>\nCo-authored-by: LZHgrla <linzhihao@pjlab.org.cn>"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/chat.py",
        "commit_date": "2023-08-22T05:57:46Z",
        "message": "[Feature] Design HuggingFace framework configs & Add more datasets (#41)\n\n* refactor registry\n\n* close bf16 for deepspeed_zero2\n\n* support huggingface framework for train.py\n\n* fix bugs\n\n* add llama2_70b open_platypus\n\n* update configs\n\n* add shuffle_before_pack for process_hf_dataset\n\n* enhance SampleGenerateHook\n\n* enhance error report\n\n* fix bugs\n\n* fix pre-commit\n\n* add code, colors, laywer, openplatypus datasets\n\n* fix typo\n\n* set default launcher to pytorch\n\n* cli uses torchrun\n\n* remove unused code\n\n* add sql\n\n* set default port to 0 for cli"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/chat.py",
        "commit_date": "2023-08-22T05:57:32Z",
        "message": "[Improve] Use lagent to implement google_search (#42)\n\n* fix bugs for input in chat\n\n* use lagent for google_search\n\n* rename SERPER_KEY to SERPER_API_KEY"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/chat.py",
        "commit_date": "2023-08-18T05:47:17Z",
        "message": "[Improve] Add generate_test_freq for configs & Add note for config argument (#40)\n\n* add generate_test_freq for configs\n\n* add note for configs"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/chat.py",
        "commit_date": "2023-08-17T09:56:47Z",
        "message": "[Improve] Support deepspeed adapter to use merge_adapter (#37)\n\nmerge support deepspeed adatper"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/chat.py",
        "commit_date": "2023-08-17T06:10:43Z",
        "message": "[Improve] Support deepspeed adapter to use chat and adapter_pth2hf  (#36)\n\ntools support deepspeed adapter"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/chat.py",
        "commit_date": "2023-08-16T10:47:12Z",
        "message": "[Improve] Move configs to xtuner; Compatible with QwenTokenizer (#29)\n\n* add oasst1 new template\n\n* move configs to xtuner\n\n* rename to xtuner\n\n* improve\n\n* improve\n\n* add internlm configs\n\n* modify oasst1 map_fn\n\n* fix bugs\n\n* remove data_preprocessor from sft\n\n* fix alpaca bugs\n\n* add param_scheduler url\n\n* rename to pack_to_max_length\n\n* fix bugs in internlm_map_fn\n\n* rename prompt_template plugins to moss_sft\n\n* rename input_with_labels to input_ids_with_output\n\n* add e* for config name\n\n* fix pre-commit\n\n* limit max_new_tokens in SampleGenerateHook\n\n* remove unused noqa\n\n* new configs\n\n* compatible with QwenTokenizer\n\n* fix cli bugs\n\n* add per_device comment for batch_size\n\n* add copyright\n\n* fix typo\n\n---------\n\nCo-authored-by: LZHgrla <linzhihao@pjlab.org.cn>"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/chat.py",
        "commit_date": "2023-08-16T02:34:41Z",
        "message": "[Feature] Add CLI for xtuner (#30)\n\n* add cli\n\n* fix typo\n\n* update readme\n\n* update docs\n\n* update docs\n\n* fix typo\n\n* fix typo"
    },
    {
        "repo_url": "github.com/huggingface/autotrain-advanced",
        "filepath": "src/autotrain/trainers/clm/utils.py",
        "commit_date": "2024-02-29T12:02:37Z",
        "message": "update requirements"
    },
    {
        "repo_url": "github.com/huggingface/autotrain-advanced",
        "filepath": "src/autotrain/trainers/clm/utils.py",
        "commit_date": "2024-02-22T22:55:25Z",
        "message": "clm fixes"
    },
    {
        "repo_url": "github.com/huggingface/autotrain-advanced",
        "filepath": "src/autotrain/trainers/clm/utils.py",
        "commit_date": "2024-02-22T15:42:14Z",
        "message": "support for more chat templates"
    },
    {
        "repo_url": "github.com/huggingface/autotrain-advanced",
        "filepath": "src/autotrain/trainers/clm/utils.py",
        "commit_date": "2024-02-22T14:55:14Z",
        "message": "all-linear + token ui fix"
    },
    {
        "repo_url": "github.com/huggingface/autotrain-advanced",
        "filepath": "src/autotrain/trainers/clm/utils.py",
        "commit_date": "2024-02-08T15:15:00Z",
        "message": "Token classification (#493)"
    },
    {
        "repo_url": "github.com/huggingface/autotrain-advanced",
        "filepath": "src/autotrain/trainers/clm/utils.py",
        "commit_date": "2024-02-07T07:42:09Z",
        "message": "peft model merge"
    },
    {
        "repo_url": "github.com/huggingface/autotrain-advanced",
        "filepath": "src/autotrain/trainers/clm/utils.py",
        "commit_date": "2023-12-22T13:37:21Z",
        "message": "fix apply_chat_template"
    },
    {
        "repo_url": "github.com/huggingface/autotrain-advanced",
        "filepath": "src/autotrain/trainers/clm/utils.py",
        "commit_date": "2023-12-22T10:54:29Z",
        "message": "apply chat template (#433)"
    },
    {
        "repo_url": "github.com/huggingface/autotrain-advanced",
        "filepath": "src/autotrain/trainers/clm/utils.py",
        "commit_date": "2023-12-21T12:35:31Z",
        "message": "clm improvements"
    },
    {
        "repo_url": "github.com/huggingface/autotrain-advanced",
        "filepath": "src/autotrain/trainers/clm/utils.py",
        "commit_date": "2023-11-30T14:15:21Z",
        "message": "fix dpo model_ref / ngc (#368)"
    },
    {
        "repo_url": "github.com/huggingface/autotrain-advanced",
        "filepath": "src/autotrain/trainers/clm/utils.py",
        "commit_date": "2023-11-24T11:31:45Z",
        "message": "update llm model card"
    },
    {
        "repo_url": "github.com/huggingface/autotrain-advanced",
        "filepath": "src/autotrain/trainers/clm/utils.py",
        "commit_date": "2023-11-21T15:17:23Z",
        "message": "Update to the app (#354)"
    },
    {
        "repo_url": "github.com/huggingface/autotrain-advanced",
        "filepath": "src/autotrain/trainers/clm/utils.py",
        "commit_date": "2023-11-15T15:22:25Z",
        "message": "update requirements"
    },
    {
        "repo_url": "github.com/huggingface/autotrain-advanced",
        "filepath": "src/autotrain/trainers/clm/utils.py",
        "commit_date": "2023-11-03T13:51:03Z",
        "message": "fix db + target modules"
    },
    {
        "repo_url": "github.com/huggingface/autotrain-advanced",
        "filepath": "src/autotrain/trainers/clm/utils.py",
        "commit_date": "2023-10-12T12:56:16Z",
        "message": "Reward modelling (#297)"
    },
    {
        "repo_url": "github.com/huggingface/autotrain-advanced",
        "filepath": "src/autotrain/trainers/clm/utils.py",
        "commit_date": "2023-08-21T12:39:14Z",
        "message": "prepare for llm ui"
    },
    {
        "repo_url": "github.com/huggingface/autotrain-advanced",
        "filepath": "src/autotrain/trainers/clm/utils.py",
        "commit_date": "2023-08-16T09:58:29Z",
        "message": "improvements to clm training"
    },
    {
        "repo_url": "github.com/PKU-YuanGroup/MoE-LLaVA",
        "filepath": "moellava/model/builder.py",
        "commit_date": "2024-02-15T09:03:23Z",
        "message": "Update builder.py"
    },
    {
        "repo_url": "github.com/PKU-YuanGroup/MoE-LLaVA",
        "filepath": "moellava/model/builder.py",
        "commit_date": "2024-02-14T03:37:09Z",
        "message": "Update builder.py"
    },
    {
        "repo_url": "github.com/PKU-YuanGroup/MoE-LLaVA",
        "filepath": "moellava/model/builder.py",
        "commit_date": "2024-01-23T13:41:46Z",
        "message": "demo"
    },
    {
        "repo_url": "github.com/PKU-YuanGroup/MoE-LLaVA",
        "filepath": "moellava/model/builder.py",
        "commit_date": "2024-01-21T15:03:14Z",
        "message": "1"
    },
    {
        "repo_url": "github.com/PKU-YuanGroup/MoE-LLaVA",
        "filepath": "moellava/model/builder.py",
        "commit_date": "2023-12-28T10:28:13Z",
        "message": "'1'"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/utils/fileio.py",
        "commit_date": "2024-01-24T04:51:57Z",
        "message": "[Feature]Support CEPH (#266)\n\n* support petrelfs\n\n* fix deepspeed save/load/resume\n\n* add ENV to toggle petrelfs\n\n* support hf save_pretrained\n\n* patch deepspeed engine"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/mmbench.py",
        "commit_date": "2024-01-24T11:37:07Z",
        "message": "[Feature] Support MMBench DDP Evaluate (#300)\n\n* support ddp mmbench evaluate\n\n* Update xtuner/tools/mmbench.py\n\nCo-authored-by: Zhihao Lin <36994684+LZHgrla@users.noreply.github.com>\n\n* Update xtuner/tools/mmbench.py\n\nCo-authored-by: Zhihao Lin <36994684+LZHgrla@users.noreply.github.com>\n\n* update minimum version of mmengine\n\n* Update runtime.txt\n\n---------\n\nCo-authored-by: Zhihao Lin <36994684+LZHgrla@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/mmbench.py",
        "commit_date": "2024-01-17T02:47:15Z",
        "message": "[Fix] Add `trust_remote_code=True` for AutoModel  (#328)\n\nupdate"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/mmbench.py",
        "commit_date": "2024-01-12T07:28:37Z",
        "message": "[Fix] Fix errors about `stop_words`  (#313)\n\n* fix bugs\n\n* Update mmbench.py"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/mmbench.py",
        "commit_date": "2024-01-11T15:29:39Z",
        "message": "[Improve] Redesign the `prompt_template` (#294)\n\n* update\n\n* update cfgs\n\n* update\n\n* fix bugs\n\n* upload docs\n\n* rename\n\n* update\n\n* Revert \"update cfgs\"\n\nThis reverts commit 93966aa7578fc3963d7c9c3e25eb1f9489e6e6fa.\n\n* update cfgs\n\n* update\n\n* rename\n\n* rename\n\n* fix bc\n\n* fix stop_word\n\n* fix\n\n* fix\n\n* Update prompt_template.md"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/mmbench.py",
        "commit_date": "2023-12-26T09:39:01Z",
        "message": "[Feature] Support LLaVA (#196)\n\n* v1\n\n* add load_image\n\n* update cfg image url\n\n* del fig\n\n* update\n\n* temp\n\n* update convert\n\n* update chat_mm\n\n* add exclude_frozen_parameters for deepspeed\n\n* update chat\n\n* update xtuner help msg\n\n* fix bugs\n\n* revert bf16 deepspeed\n\n* fix bugs\n\n* add visual_select_layer for chat\n\n* improve pth_to_hf\n\n* rename projecter_pth to pretrained_pth\n\n* temp\n\n* update requirements\n\n* add cfgs\n\n* update\n\n* fix pre-commit\n\n* optim chat\n\n* optim chat\n\n* Delete xtuner/model/unused.py\n\n* move dispatch to a deeper folder\n\n* add projector\n\n* update\n\n* del model/projector\n\n* fix bugs\n\n* add docs\n\n* update\n\n* update\n\n* update\n\n* update\n\n* enhance resume for map_fn\n\n* update import\n\n* add llava_internlm_chat_7b_clip_vit_large_p14\n\n* update dispatch\n\n* update dispatch\n\n* add link\n\n* update max_length\n\n* update max_length\n\n* update hyp\n\n* align\n\n* move yi flash attn\n\n* fix pre-commit\n\n* update deepspeed requirements\n\n* add mmbench script\n\n* install openpyxl\n\n* add entry_point for mmbench\n\n* save args\n\n* update mmbench\n\n* update max_length\n\n* add llama2 qlora\n\n* update mmbench\n\n* fix mmbench bugs\n\n* use osp instead of os.path\n\n* refactor pth_to_hf\n\n* update chat and mmbench to support --llava\n\n* align to chat\n\n* update entry_point\n\n* add vicuna template\n\n* add vicuna_7b_v15\n\n* fix pre-commit\n\n* add vicuna_7b_v1.5 qlora\n\n* skip_special_tokens for decode text\n\n* remove do_sample\n\n* add warmup\n\n* fix pre-commit\n\n* Update dataset_prepare.md\n\n* Update dataset_prepare.md\n\n* Add KEEP_STSTEM for template\n\n* remove\n\n* fix vicuna template\n\n* clean cfgs\n\n* add cfgs\n\n* fix pre-commit\n\n* add --language for mmbench\n\n* fix bugs\n\n* fix pretrain bug\n\n* support visual_encoder lora\n\n* fix bugs\n\n* add paramwise_cfg\n\n* remove print_peft_model_trainable_parameters\n\n* fix bugs\n\n* add paramwise_cfg for DeepSpeedOptimWrapper\n\n* fix engine deepspeed paramwise_cfg bug\n\n* fix encode_fn bug\n\n* fix\n\n* fix pad_image_to_square bugs\n\n* Add space for system to avoid mismatch of 'USER' token\n\n* revert to adding bos_token at each conv\n\n* revert for paramwise_cfg\n\n* better cfgs?\n\n* fix import bug\n\n* fix import bug\n\n* pretrain align\n\n* update prepare_inputs_labels_for_multimodal\n\n* 1792\n\n* support length_grouped_samplers\n\n* 1792\n\n* remove KEEP_SYSTEM\n\n* remove system in cfg\n\n* update 336 cfg\n\n* add torch_dtype for mmbench and chat\n\n* group 50\n\n* quant for pretrain\n\n* update cfgs\n\n* refactor cfgs\n\n* add length for concat dataset\n\n* update requirements\n\n* fix typo\n\n* add template for internlm pretrain\n\n* no zh\n\n* remove 20b cfgs\n\n* fix pre-commit\n\n* revert invalid input\n\n* rename\n\n* Update README.md\n\n* Update README_zh-CN.md\n\n* fix pre-commit\n\n* remove llava_zh from docs\n\n* qlora 512\n\n* rename llava map_fn\n\n* update cfgs\n\n* update model urls\n\n* add docs link\n\n* add llava docs\n\n* update docs\n\n* update urls\n\n* add citation\n\n* fix README\n\n* move\n\n* update\n\n* vicuna pretrain with prompt\n\n* rename\n\n* add results\n\n* fix pre-commit\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* Update README.md\n\n* Update README_zh-CN.md\n\n* Update README_zh.md\n\n* Update README_zh.md\n\n* Update README.md\n\n* Update README_zh.md\n\n* Update README.md\n\n* Update README.md\n\n* fix typo\n\n* fix\n\n* Update README.md\n\n* Update README_zh-CN.md\n\n* rename\n\n* auto cn_string\n\n* fix pre-commit\n\n* rename\n\n* remove language\n\n* add VLMEvalKit\n\n* rename VLLM to VLM\n\n* add the download links of MMBench\n\n* update\n\n* update readme\n\n* update\n\n* update\n\n* update merge\n\n* fix cfg bug\n\n* Update README.md\n\n* Update README_zh.md\n\n* update\n\n* fix\n\n* update requirements\n\n* Update runtime.txt\n\n* Update runtime.txt\n\n* Update runtime.txt\n\n* Update README.md\n\n* Update README.md\n\n* Update README_zh.md\n\n* fix pre-commit\n\n* fix\n\n* update mmbench prompt\n\n* fix bugs\n\n* fix bugs\n\n* update docs\n\n* update\n\n* update\n\n* Update README.md"
    },
    {
        "repo_url": "github.com/gururise/AlpacaDataCleaned",
        "filepath": "eval/eval.py",
        "commit_date": "2023-04-05T04:37:02Z",
        "message": "fix more issues in piqa scoring"
    },
    {
        "repo_url": "github.com/gururise/AlpacaDataCleaned",
        "filepath": "eval/eval.py",
        "commit_date": "2023-04-05T00:16:26Z",
        "message": "update regex to extract correct answer"
    },
    {
        "repo_url": "github.com/gururise/AlpacaDataCleaned",
        "filepath": "eval/eval.py",
        "commit_date": "2023-04-04T23:20:06Z",
        "message": "fix typo"
    },
    {
        "repo_url": "github.com/gururise/AlpacaDataCleaned",
        "filepath": "eval/eval.py",
        "commit_date": "2023-04-04T23:16:53Z",
        "message": "update piqa prompt/scoring."
    },
    {
        "repo_url": "github.com/gururise/AlpacaDataCleaned",
        "filepath": "eval/eval.py",
        "commit_date": "2023-04-04T21:09:03Z",
        "message": "fix piqa scoring"
    },
    {
        "repo_url": "github.com/gururise/AlpacaDataCleaned",
        "filepath": "eval/eval.py",
        "commit_date": "2023-04-03T16:12:42Z",
        "message": "redo scoring in squad"
    },
    {
        "repo_url": "github.com/gururise/AlpacaDataCleaned",
        "filepath": "eval/eval.py",
        "commit_date": "2023-04-03T03:16:39Z",
        "message": "benchmark fixes"
    },
    {
        "repo_url": "github.com/gururise/AlpacaDataCleaned",
        "filepath": "eval/eval.py",
        "commit_date": "2023-04-03T02:07:01Z",
        "message": "add piqa dataset benchmark"
    },
    {
        "repo_url": "github.com/gururise/AlpacaDataCleaned",
        "filepath": "eval/eval.py",
        "commit_date": "2023-04-02T19:35:00Z",
        "message": "updates to benchmark"
    },
    {
        "repo_url": "github.com/gururise/AlpacaDataCleaned",
        "filepath": "eval/eval.py",
        "commit_date": "2023-04-02T17:38:17Z",
        "message": "fix bug in ppl output"
    },
    {
        "repo_url": "github.com/gururise/AlpacaDataCleaned",
        "filepath": "eval/eval.py",
        "commit_date": "2023-04-02T04:32:47Z",
        "message": "update program arguments"
    },
    {
        "repo_url": "github.com/gururise/AlpacaDataCleaned",
        "filepath": "eval/eval.py",
        "commit_date": "2023-04-02T04:28:53Z",
        "message": "add some clarity in the options"
    },
    {
        "repo_url": "github.com/gururise/AlpacaDataCleaned",
        "filepath": "eval/eval.py",
        "commit_date": "2023-04-02T04:06:36Z",
        "message": "program to evaluate models on Squad and Wikitext"
    },
    {
        "repo_url": "github.com/TigerResearch/TigerBot",
        "filepath": "opencompass/opencompass/models/huggingface.py",
        "commit_date": "2023-09-11T01:04:57Z",
        "message": "fix huggingface.py bug"
    },
    {
        "repo_url": "github.com/TigerResearch/TigerBot",
        "filepath": "opencompass/opencompass/models/huggingface.py",
        "commit_date": "2023-09-01T09:34:30Z",
        "message": "fix bugs"
    },
    {
        "repo_url": "github.com/TigerResearch/TigerBot",
        "filepath": "opencompass/opencompass/models/huggingface.py",
        "commit_date": "2023-08-31T03:37:27Z",
        "message": "fix bugs"
    },
    {
        "repo_url": "github.com/TigerResearch/TigerBot",
        "filepath": "opencompass/opencompass/models/huggingface.py",
        "commit_date": "2023-08-24T13:49:19Z",
        "message": "add opencompass"
    },
    {
        "repo_url": "github.com/MetaGLM/FinGLM",
        "filepath": "code/finglm_all/chat_run/nl2sql.py",
        "commit_date": "2023-09-26T15:27:39Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/togethercomputer/OpenChatKit",
        "filepath": "training/lora/example/redpajama-incite-chat-3b_inference.py",
        "commit_date": "2023-06-06T21:31:11Z",
        "message": "Moved redpajama scripts to example"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2024-03-01T12:27:40Z",
        "message": "Improve WOQ algo autoround (#1330)\n\nSigned-off-by: changwangss <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2024-02-27T06:02:00Z",
        "message": "WOQ support autoround algo on cpu device (#1312)\n\n* woq support autoround algo on cpu device\n\nSigned-off-by: changwangss <chang1.wang@intel.com>\n\n---------\n\nSigned-off-by: changwangss <chang1.wang@intel.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2024-02-23T13:33:51Z",
        "message": "[API Changed] Dispatch the backend based on model_type (#1298)\n\nCo-authored-by: changwangss <chang1.wang@intel.com>\nCo-authored-by: VincyZhang <wenxin.zhang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2024-02-21T12:18:03Z",
        "message": "add GPTQ static_groups (#1291)\n\nSigned-off-by: changwangss <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2024-02-06T11:55:27Z",
        "message": "Fix text-generation example accuracy test (#1262)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2024-02-01T03:33:07Z",
        "message": "[LLM] Support woq model save and load (#1211)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2024-01-25T08:50:12Z",
        "message": "Update run_generation.py (#1169)\n\nSigned-off-by: Wang, Chang <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2024-01-19T16:29:58Z",
        "message": "Support weight-only kernel with IPEX for intel GPU (#1153)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2024-01-19T11:43:33Z",
        "message": "remove GPTQ llama dependence (#1163)\n\nSigned-off-by: Wang, Chang1 <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2024-01-19T02:45:51Z",
        "message": "Add WOQ GPTQ frontend and example (#1107)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-12-28T06:13:48Z",
        "message": "[LLM example] add calib_shuffle args for text-generation example (#1087)\n\nSigned-off-by: Wang, Chang1 <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-12-21T10:38:22Z",
        "message": "[LLM] Fix llm models extension issue (#955)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-12-21T02:37:06Z",
        "message": "parser recipes to dict (#979)\n\nSigned-off-by: Wang, Chang1 <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-12-20T12:34:55Z",
        "message": "[LLM] Support recipes with calibration changes and add example parser args. (#978)\n\n* support recipes for calibration func change\n\nSigned-off-by: Wang, Chang1 <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-12-14T03:19:47Z",
        "message": "[LLM] Support fp8 config and update examples (#915)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-12-08T05:35:25Z",
        "message": "[LLM] Support restoring sq optimized model for text-generation  (#860)\n\n* add ipex restoration evaluate\n\nSigned-off-by: Tang, Kaihui <kaihui.tang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-12-04T13:24:09Z",
        "message": "fix chatglm accuracy regression and support chatglm2 prompt (#859)\n\nSigned-off-by: Wang, Chang1 <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-12-04T12:30:54Z",
        "message": "[LLM] make text-generation example woq default weight dtype to \"nf4\" (#853)\n\n* make woq int4 default weight dtype to nf4\n\nSigned-off-by: Wang, Chang1 <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-12-04T03:39:23Z",
        "message": "[LLM] add an option to enable fallback_add (#849)\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-12-01T12:36:57Z",
        "message": "[LLM] add revision to text-generaion example (#843)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-12-01T06:31:36Z",
        "message": "[LLM] add chatglm and codellama extension test (#837)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-11-30T06:23:43Z",
        "message": "[LLM] Support chatglm/falcon series and unified int8 loading (#814)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-11-28T03:40:59Z",
        "message": "improve text-generation example (#792)\n\nSigned-off-by: Wang, Chang1 <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-11-24T09:06:37Z",
        "message": "Fix nightly CI issue (#765)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-11-24T00:57:46Z",
        "message": "move the class path (#758)\n\nSigned-off-by: Wang, Chang1 <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-11-23T06:05:01Z",
        "message": "SmoothQuantConfig support recipes (#750)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-11-23T02:09:14Z",
        "message": "[LLM] Support gpt_neox ipex.optimize_transformers and update quantization workflow. (#741)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-11-22T07:40:55Z",
        "message": "Move TSModelCausalLMForOPTLLM to lm-eval (#744)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-11-21T05:24:06Z",
        "message": "Smoothquant support ipex.optimize_transformers feature (#695)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-11-15T07:18:09Z",
        "message": "add save for mixedprecision model (#687)\n\nSigned-off-by: changwangss <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-11-13T10:04:12Z",
        "message": "[LLM] text-generation example support peft model quantization (#668)\n\n* text-generation example support peft model\n\nSigned-off-by: changwangss <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-11-13T05:31:56Z",
        "message": "[LLM] text-generation example support chatglm2&3 (#638)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-11-01T05:25:55Z",
        "message": "support op_name_dict (#604)\n\nSigned-off-by: changwangss <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-10-31T09:40:19Z",
        "message": "Fix example wrong name and past_kv shape (#561)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-10-24T11:51:23Z",
        "message": "support sq auto tune (#537)\n\nSigned-off-by: changwangss <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-10-23T14:50:09Z",
        "message": "[Optimization] Text-generation support qwen (#513)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-09-26T04:10:10Z",
        "message": "change mainpage (#340)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-09-21T12:04:57Z",
        "message": "Support load_in_4bit and load_in_8bit on CPU device (#354)\n\n* support load_in_4bit and load_in_8bit, remove gptq parser\n\nSigned-off-by: changwangss <chang1.wang@intel.com>\n\n* fix readme\n\nSigned-off-by: changwangss <chang1.wang@intel.com>\n\n* add ut\n\nSigned-off-by: changwangss <chang1.wang@intel.com>\n\n* add awq/teq\n\nSigned-off-by: changwangss <chang1.wang@intel.com>\n\n* Update modeling_auto.py\n\n* Update pytorch_optimize.json\n\n---------\n\nSigned-off-by: changwangss <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-09-20T07:21:02Z",
        "message": "Fix text-generation logger info (#349)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-09-18T23:05:36Z",
        "message": "Enable text-generation with new API (#318)\n\n* enable text-generation with NeuralChat API\n\nSigned-off-by: changwangss <chang1.wang@intel.com>\n\n* fix wrong typing and hide import\n\nSigned-off-by: changwangss <chang1.wang@intel.com>\n\n* improve import check\n\n* rebase main\n\nSigned-off-by: changwangss <chang1.wang@intel.com>\n\n* remove the outdated code\n\nSigned-off-by: changwangss <chang1.wang@intel.com>\n\n* update order\n\n* improve sqconfig and add ut\n\nSigned-off-by: changwangss <chang1.wang@intel.com>\n\n* refine woq\n\nSigned-off-by: changwangss <chang1.wang@intel.com>\n\n* fix mp name\n\nSigned-off-by: changwangss <chang1.wang@intel.com>\n\n* fix pylint\n\nSigned-off-by: changwangss <chang1.wang@intel.com>\n\n* fix import\n\nSigned-off-by: changwangss <chang1.wang@intel.com>\n\n* Fixed shape error for weight-only quantization op\n\nSigned-off-by: Cheng, Penghui <penghui.cheng@intel.com>\n\n* Fixed UT error for weight-only quantization\n\nSigned-off-by: Cheng, Penghui <penghui.cheng@intel.com>\n\n* improve the example\n\nSigned-off-by: changwangss <chang1.wang@intel.com>\n\n* Update README.md\n\n* fix long line\n\nSigned-off-by: changwangss <chang1.wang@intel.com>\n\n* fix import\n\nSigned-off-by: changwangss <chang1.wang@intel.com>\n\n* Update README.md\n\n* Update test_quantization.py\n\n---------\n\nSigned-off-by: changwangss <chang1.wang@intel.com>\nSigned-off-by: Cheng, Penghui <penghui.cheng@intel.com>\nCo-authored-by: Cheng, Penghui <penghui.cheng@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-09-12T23:34:04Z",
        "message": "Update run_generation.py (#301)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-09-12T05:13:58Z",
        "message": "Improve text-generation example ipex installation (#292)\n\n* improve ipex installation\n\nSigned-off-by: Wang, Chang1 <chang1.wang@intel.com>\n\n* Update README.md\n\n* Update README.md\n\n---------\n\nSigned-off-by: Wang, Chang1 <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-09-08T08:22:19Z",
        "message": "Refine mpt text-generation example (#257)\n\n* refine mpt text-generation example\n\nSigned-off-by: Wang, Chang1 <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-08-29T10:33:08Z",
        "message": "refact folder stracture (#170)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-08-22T02:48:16Z",
        "message": "enhance text-generation example (#155)\n\nSigned-off-by: changwangss <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-07-07T09:43:05Z",
        "message": "Improve MPT model loading folder (#1142)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-07-05T06:00:56Z",
        "message": "fix text-generation perf (#1119)\n\nSigned-off-by: changwangss <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-07-03T08:35:57Z",
        "message": "Add MPT example for text-generation (#1091)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-06-21T10:51:47Z",
        "message": "Fix poor generation issue (#1065)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-06-19T10:07:17Z",
        "message": "fix evaluation init import issue (#1053)\n\nSigned-off-by: changwangss <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-06-14T03:33:28Z",
        "message": "remove exit(0) from examples (#1032)\n\n* remove exit(0) from examples\n\n* fix exit(0)\n\nSigned-off-by: Wang, Chang1 <chang1.wang@intel.com>\n\n---------\n\nSigned-off-by: Wang, Chang1 <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-06-07T08:59:06Z",
        "message": "support lm-eval accuracy metric for generation task torchscript model with past-kv (#973)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-06-02T02:37:26Z",
        "message": "Enable opt models for text-generation (#968)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-05-25T08:48:30Z",
        "message": "support bloom in text generation task (#941)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-05-16T08:30:32Z",
        "message": "fix gpt-neox text-generation (#940)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-05-16T07:15:02Z",
        "message": "update text-generation example to support transformers 4.28.1 (#917)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "examples/huggingface/pytorch/text-generation/quantization/run_generation.py",
        "commit_date": "2023-05-08T10:08:05Z",
        "message": "Refine language modeling (#900)"
    },
    {
        "repo_url": "github.com/Fanghua-Yu/SUPIR",
        "filepath": "llava/model/builder.py",
        "commit_date": "2024-01-25T14:42:59Z",
        "message": "20240125"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "tests/test_reward_trainer.py",
        "commit_date": "2024-02-15T13:47:32Z",
        "message": "[`core` /  `xxxTrainer`] Automatic tagging (#1329)\n\n* automatic tagging\n\n* add comments\n\n* fix tests\n\n* fix"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "tests/test_reward_trainer.py",
        "commit_date": "2024-02-15T03:37:41Z",
        "message": "pre-commit: replace linters + formatters with Ruff; fix some issues (#1300)\n\n* pre-commit: replace linters + formatters with Ruff\n\n* Don't use bare except\n\n* Clean up `noqa`s\n\n* Enable Ruff UP; apply auto-fixes\n\n* Enable Ruff B; apply fixes\n\n* Enable Ruff T with exceptions\n\n* Enable Ruff C (complexity); autofix\n\n* Upgrade Ruff to 0.2.0"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "tests/test_reward_trainer.py",
        "commit_date": "2024-02-01T22:49:03Z",
        "message": "Codemod Unittest assertions to bare asserts (#1301)\n\n* Remove stray commas from test data\n\n* Codemod Unittest assertions to bare asserts\n\n* Make `assertAlmostEqual` tests more idiomatic\n\n* DRY some test strings"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "tests/test_reward_trainer.py",
        "commit_date": "2023-09-20T08:18:38Z",
        "message": "Add margin to RM training (#719)\n\n* Start adding margin to RM training\n\n* Fix typo and cleanup\n\n* Fix incompatibilities when not using margin\n\n* Format using 'make precommit'\n\n* Add documentation and test for reward trainer\n\n* Run 'make precommit'\n\n* Update docs/source/reward_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Fix missed merge conflict in reward trainer docs\n\n---------\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "tests/test_reward_trainer.py",
        "commit_date": "2023-09-05T07:05:42Z",
        "message": "Refactor RewardTrainer hyperparameters into dedicated dataclass (#726)\n\n* Refactor RewardTrainer hyperparameters into dedicated dataclass\n\n* Revert\n\n* Add doc string\n\n* Fix warning\n\n* Handle backwards compat\n\n* Fix tests\n\n* Add docs\n\n* Refactor to RewardConfig\n\n* Fix case conditions\n\n* Fix"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "tests/test_reward_trainer.py",
        "commit_date": "2023-06-26T12:30:06Z",
        "message": "fix CI RM (#468)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "tests/test_reward_trainer.py",
        "commit_date": "2023-06-09T13:52:41Z",
        "message": "Update test_reward_trainer.py (#421)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "tests/test_reward_trainer.py",
        "commit_date": "2023-06-07T10:22:22Z",
        "message": "Update test_reward_trainer.py (#410)"
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "tests/test_reward_trainer.py",
        "commit_date": "2023-06-06T14:49:30Z",
        "message": "Resolve broken evaluation/prediction for RewardTrainer (#404)\n\n* Implement evaluation/prediction for RewardTrainer\n\n* Stick with unittest assertions\n\n* Perform prediction forward calls without gradient\n\n* Remove Literal to preserve Python 3.7 support\n\nI recognize that I can also import from typing_extensions with a try-except,\nbut that is a bit overkill for this I feel.\n\n* Remove eval_steps=1 to prevent flaky test on CI\n\nThe flaky test is caused by a division by zero when dividing by the runtime.\nThis is done on the transformers side, so it's not a TRL issue.\nIn practice, this won't happen - it only happens because both the model\nand dataset are tiny."
    },
    {
        "repo_url": "github.com/huggingface/trl",
        "filepath": "tests/test_reward_trainer.py",
        "commit_date": "2023-04-26T09:51:56Z",
        "message": "[`core`] Officially Support Reward Modeling (#303)\n\n* v1\n\n- add working version\n- add all possible tests\n- add docs\n\n* add some contents\n\n* clean up\n\n* fixes\n\n* patch test for now\n\n* fix test\n\n* clean up\n\n* fix\n\n* this time fix\n\n* Update docs/source/trainer.mdx\n\nCo-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>\n\n* fixe\n\n* update\n\n* final changes\n\n* oops\n\n* Update docs/source/reward_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update docs/source/reward_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* Update docs/source/reward_trainer.mdx\n\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n\n* switch to chosen / rejected\n\n* fixes\n\n* add example\n\n* add accuracy metric\n\n* pass PEFT config\n\n* refactor compute metrics\n\n---------\n\nCo-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2024-02-18T08:06:49Z",
        "message": "\u4fee\u6539check_checkpoint_config_path\u903b\u8f91"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2024-02-16T14:45:01Z",
        "message": "\u589e\u52a0PipelineBase\u57fa\u7c7b"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2024-02-15T15:06:58Z",
        "message": "\u6dfb\u52a0\u53c2\u6570\u5907\u6ce8\uff0c\u5f00\u59cb\u5141\u8bb8\u4e0b\u8f7d"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2024-02-03T16:17:29Z",
        "message": "debug\u751f\u6210\u901f\u5ea6"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2024-02-02T10:45:18Z",
        "message": "\u51cf\u5c11\u8017\u65f6"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2024-01-27T14:48:52Z",
        "message": "modify load_variable"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2024-01-26T10:59:30Z",
        "message": "\u4fee\u6539load_variable"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2024-01-24T10:17:44Z",
        "message": "fix hierarchical"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2024-01-22T16:09:46Z",
        "message": "debug save pretrained"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2024-01-22T10:57:11Z",
        "message": "add copy_tree"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2024-01-21T15:47:50Z",
        "message": "debug t5 use states"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2024-01-16T15:42:35Z",
        "message": "v0.4.6"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2024-01-16T10:46:38Z",
        "message": "modify from_pretrain_single"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2024-01-16T10:26:21Z",
        "message": "test save_pretrained"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2024-01-15T15:33:19Z",
        "message": "test save_pretrained"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2024-01-14T16:18:54Z",
        "message": "test basic"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2024-01-12T09:46:30Z",
        "message": "debug from_pretrained"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2024-01-12T06:46:09Z",
        "message": "debug save_pretrained"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2023-12-20T04:18:20Z",
        "message": "import_utils"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2023-12-19T15:23:10Z",
        "message": "v0.4.2"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2023-12-18T05:27:01Z",
        "message": "add some print"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2023-12-17T15:38:00Z",
        "message": "support safetensors"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2023-12-17T12:17:36Z",
        "message": "debug chat"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2023-12-13T15:45:06Z",
        "message": "fix chat module"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2023-12-13T10:48:55Z",
        "message": "add chat module"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2023-11-06T15:10:06Z",
        "message": "v0.3.7"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2023-11-02T10:18:49Z",
        "message": "rm gamma beta"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2023-11-02T08:20:10Z",
        "message": "modify load_ckpt"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2023-11-02T06:17:33Z",
        "message": "modify load_ckpt"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2023-09-03T16:18:58Z",
        "message": "modify comment"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2023-08-26T16:58:24Z",
        "message": "modify rlhf"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2023-08-12T06:30:07Z",
        "message": "v0.3.3"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2023-08-08T16:06:02Z",
        "message": "fix load_ckpt"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2023-08-08T03:09:58Z",
        "message": "rely accelerate"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2023-08-07T15:53:52Z",
        "message": "fix shards load"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2023-08-06T15:47:40Z",
        "message": "add qwen"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2023-07-16T11:35:07Z",
        "message": "v0.3.0"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2023-07-11T09:52:22Z",
        "message": "modify layernorm"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2023-07-10T12:43:29Z",
        "message": "fix skip init bugs"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2023-07-10T08:27:46Z",
        "message": "add low_cpu_mem_usage"
    },
    {
        "repo_url": "github.com/Tongjilibo/bert4torch",
        "filepath": "bert4torch/models/base.py",
        "commit_date": "2023-07-06T09:28:04Z",
        "message": "modify models and layers to dir"
    },
    {
        "repo_url": "github.com/SCIR-HI/Huatuo-Llama-Med-Chinese",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-04-01T09:37:49Z",
        "message": "init code"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/eval_refcoco.py",
        "commit_date": "2024-02-26T06:55:32Z",
        "message": "add refcoco to llava (#425)\n\n* add base dataset\n\n* update dataset generation\n\n* update refcoco\n\n* add convert refcooc\n\n* add eval_refcoco\n\n* add config\n\n* update dataset\n\n* fix bug\n\n* fix bug\n\n* update data prepare\n\n* fix error\n\n* refactor eval_refcoco\n\n* fix bug\n\n* fix error\n\n* update readme\n\n* add entry_point\n\n* update config\n\n* update config\n\n* update entry point\n\n* update\n\n* update doc\n\n* update\n\n---------\n\nCo-authored-by: jacky <jacky@xx.com>"
    },
    {
        "repo_url": "github.com/xusenlinzy/api-for-open-llm",
        "filepath": "api/utils/apply_lora.py",
        "commit_date": "2023-08-24T02:11:15Z",
        "message": "Make embedding api compatible for openai"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-05-25T16:38:44Z",
        "message": "fixed prefix tuning inference bug"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-05-16T16:02:09Z",
        "message": "upload commonsense evaluate scripts"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-21T09:49:20Z",
        "message": "delete locating root path code"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-21T07:51:57Z",
        "message": "Merge branch 'main' into main"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-21T07:20:57Z",
        "message": "added display progress for evaluation and fixed NoneType error"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-21T07:19:28Z",
        "message": "Merge branch 'main' of github.com:HZQ950419/LLM-Adapters into main"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-21T07:14:05Z",
        "message": "added display process for evaluation and fixed NoneType error"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-14T11:13:05Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-13T19:24:41Z",
        "message": "Update evaluate.py"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-10T12:06:22Z",
        "message": "enable checking progress"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-08T17:55:36Z",
        "message": "enable testing"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-07T13:43:02Z",
        "message": "Update evaluate.py\n\ncreate experiment folder when folder not created"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-07T12:04:58Z",
        "message": "fixed inference error without loading int8 model"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-07T11:58:38Z",
        "message": "fixed inference error without loading int8 model"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-07T11:14:17Z",
        "message": "fixed inference error without loading int8 model"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-07T02:21:16Z",
        "message": "fixed unexpected character in prompt"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-04T11:21:26Z",
        "message": "modified peft/setup.py for installation"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-04T08:31:25Z",
        "message": "update evaluate code"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-04T02:44:45Z",
        "message": "fixed a forward bug with autocast for bottleneck adapters"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-03T15:51:49Z",
        "message": "fix evaluate error and upload README"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-02T19:18:11Z",
        "message": "update test code"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-02T18:44:29Z",
        "message": "update test code"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-02T18:34:07Z",
        "message": "update test code"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-02T18:16:38Z",
        "message": "update test code"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-02T17:59:25Z",
        "message": "update test code"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-02T17:57:29Z",
        "message": "update test code"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-02T17:53:42Z",
        "message": "update test code"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-02T17:51:52Z",
        "message": "update test code"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-02T17:47:18Z",
        "message": "update test code"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-02T16:27:57Z",
        "message": "update dataset"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-02T14:01:25Z",
        "message": "update evaluate.py"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-02T13:43:14Z",
        "message": "update evaluate.py"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-02T08:27:11Z",
        "message": "update test code"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-02T08:23:45Z",
        "message": "update test data and code"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-02T08:00:12Z",
        "message": "update test code running math data"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/llm/evaluation/lm_eval/models/huggingface.py",
        "commit_date": "2024-01-31T09:18:17Z",
        "message": "Add precommit config (#1216)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/llm/evaluation/lm_eval/models/huggingface.py",
        "commit_date": "2024-01-15T06:26:19Z",
        "message": "[Runtime] calculate accuracy of runtime (#1123)\n\n* acc runtime\n\nSigned-off-by: zhenwei-intel <zhenwei.liu@intel.com>\n\n* llama2\n\nSigned-off-by: zhenwei-intel <zhenwei.liu@intel.com>\n\n* update\n\nSigned-off-by: Dong, Bo <bo1.dong@intel.com>\n\n* use evaluate\n\nSigned-off-by: zhenwei-intel <zhenwei.liu@intel.com>\n\n* fix llama\n\nSigned-off-by: zhenwei-intel <zhenwei.liu@intel.com>\n\n* fix rope scale\n\nSigned-off-by: zhenwei-intel <zhenwei.liu@intel.com>\n\n* add args\n\nSigned-off-by: zhenwei-intel <zhenwei.liu@intel.com>\n\n* logits_all\n\nSigned-off-by: zhenwei-intel <zhenwei.liu@intel.com>\n\n* add copyright\n\nSigned-off-by: zhenwei-intel <zhenwei.liu@intel.com>\n\n* fix rope scale\n\nSigned-off-by: zhenwei-intel <zhenwei.liu@intel.com>\n\n* update model format\n\nSigned-off-by: zhenwei-intel <zhenwei.liu@intel.com>\n\n* update convert\n\nSigned-off-by: zhenwei-intel <zhenwei.liu@intel.com>\n\n* add args\n\nSigned-off-by: zhenwei-intel <zhenwei.liu@intel.com>\n\n* update format\n\nSigned-off-by: zhenwei-intel <zhenwei.liu@intel.com>\n\n* update script\n\nSigned-off-by: zhenwei-intel <zhenwei.liu@intel.com>\n\n* enable GLIBCXX_3.4.26\n\nSigned-off-by: Wenxin Zhang <wenxin.zhang@intel.com>\n\n* fix conda env name\n\nSigned-off-by: Wenxin Zhang <wenxin.zhang@intel.com>\n\n* fix pylint\n\nSigned-off-by: zhenwei-intel <zhenwei.liu@intel.com>\n\n* add gptq\n\nSigned-off-by: zhenwei-intel <zhenwei.liu@intel.com>\n\n* move to examples\n\nSigned-off-by: zhenwei-intel <zhenwei.liu@intel.com>\n\n---------\n\nSigned-off-by: zhenwei-intel <zhenwei.liu@intel.com>\nSigned-off-by: Dong, Bo <bo1.dong@intel.com>\nSigned-off-by: Wenxin Zhang <wenxin.zhang@intel.com>\nCo-authored-by: Dong, Bo <bo1.dong@intel.com>\nCo-authored-by: Wenxin Zhang <wenxin.zhang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/llm/evaluation/lm_eval/models/huggingface.py",
        "commit_date": "2024-01-12T09:00:19Z",
        "message": "fix Qwen-72b eval precision issue (#1134)\n\n* fix Qwen-72b eval precision issue\n\nSigned-off-by: Zhang, Weiwei1 <weiwei1.zhang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/llm/evaluation/lm_eval/models/huggingface.py",
        "commit_date": "2023-12-21T10:38:22Z",
        "message": "[LLM] Fix llm models extension issue (#955)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/llm/evaluation/lm_eval/models/huggingface.py",
        "commit_date": "2023-12-14T05:54:58Z",
        "message": "Support evaluation for onnx model exported with optimum >= 1.14.0 (#910)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/llm/evaluation/lm_eval/models/huggingface.py",
        "commit_date": "2023-11-13T05:31:56Z",
        "message": "[LLM] text-generation example support chatglm2&3 (#638)"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/llm/evaluation/lm_eval/models/huggingface.py",
        "commit_date": "2023-11-09T13:22:26Z",
        "message": "Fix ONNXRT session bug with upgraded optimum 1.14.0 (#644)\n\nFix ONNXRT session bug with upgraded optimum 1.14.0 (#644)\n\nSigned-off-by: yuwenzho <yuwen.zhou@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/llm/evaluation/lm_eval/models/huggingface.py",
        "commit_date": "2023-11-06T01:22:23Z",
        "message": "[LLM] Update lm-eval commit id (#617)\n\n* update lm-eval commit\n\nSigned-off-by: Wang, Chang <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/llm/evaluation/lm_eval/models/huggingface.py",
        "commit_date": "2023-09-22T07:50:44Z",
        "message": "Support lm-eval for merged decoder ONNX models (#371)\n\n* update for merged onnx model\n\nSigned-off-by: yuwenzho <yuwen.zhou@intel.com>"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/llm/evaluation/lm_eval/models/huggingface.py",
        "commit_date": "2023-08-29T10:33:08Z",
        "message": "refact folder stracture (#170)"
    },
    {
        "repo_url": "github.com/li-plus/chatglm.cpp",
        "filepath": "chatglm_cpp/convert.py",
        "commit_date": "2023-11-22T01:51:25Z",
        "message": "Support all features for ChatGLM3 (system prompt / function call / code interpreter) (#197)"
    },
    {
        "repo_url": "github.com/li-plus/chatglm.cpp",
        "filepath": "chatglm_cpp/convert.py",
        "commit_date": "2023-10-30T07:39:08Z",
        "message": "Fix conversion error for ChatGLM3 32k (#163)"
    },
    {
        "repo_url": "github.com/li-plus/chatglm.cpp",
        "filepath": "chatglm_cpp/convert.py",
        "commit_date": "2023-10-29T13:42:06Z",
        "message": "Support ChatGLM3 (#158)"
    },
    {
        "repo_url": "github.com/li-plus/chatglm.cpp",
        "filepath": "chatglm_cpp/convert.py",
        "commit_date": "2023-10-22T02:43:25Z",
        "message": "Support InternLM 7B & 20B (#149)"
    },
    {
        "repo_url": "github.com/li-plus/chatglm.cpp",
        "filepath": "chatglm_cpp/convert.py",
        "commit_date": "2023-09-28T11:58:50Z",
        "message": "Support Baichuan-7B architecture (#134)\n\n* Support Baichuan-7B architecture\n\n* Bump version\n\n* Unify model config"
    },
    {
        "repo_url": "github.com/li-plus/chatglm.cpp",
        "filepath": "chatglm_cpp/convert.py",
        "commit_date": "2023-08-31T11:40:14Z",
        "message": "Support Baichuan-13B model on CPU & CUDA (#115)"
    },
    {
        "repo_url": "github.com/li-plus/chatglm.cpp",
        "filepath": "chatglm_cpp/convert.py",
        "commit_date": "2023-08-11T12:09:39Z",
        "message": "Support loading from HF models and convert to GGML on the fly (#90)\n\n* Auto assign cuda buffer\n\n* Support convert-on-load for py binding"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/training/peft/tuners/prompt_tuning.py",
        "commit_date": "2023-09-04T06:02:25Z",
        "message": "Add support for kbits training"
    },
    {
        "repo_url": "github.com/X-PLUG/mPLUG-Owl",
        "filepath": "mPLUG-Owl2/mplug_owl2/model/builder.py",
        "commit_date": "2024-02-02T17:48:32Z",
        "message": "Merge branch 'main' into kwargs-for-model-loading"
    },
    {
        "repo_url": "github.com/X-PLUG/mPLUG-Owl",
        "filepath": "mPLUG-Owl2/mplug_owl2/model/builder.py",
        "commit_date": "2024-01-31T17:04:59Z",
        "message": "owl 2.1"
    },
    {
        "repo_url": "github.com/X-PLUG/mPLUG-Owl",
        "filepath": "mPLUG-Owl2/mplug_owl2/model/builder.py",
        "commit_date": "2023-11-27T18:33:44Z",
        "message": "\ud83e\ude79 make load_pretrained_model accept kwargs\n\n- this allows it to e.g. take a cache_dir argument"
    },
    {
        "repo_url": "github.com/X-PLUG/mPLUG-Owl",
        "filepath": "mPLUG-Owl2/mplug_owl2/model/builder.py",
        "commit_date": "2023-11-08T10:50:07Z",
        "message": "Launched mPLUG-Owl2"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "generate.py",
        "commit_date": "2023-04-13T19:23:52Z",
        "message": "Update generate.py"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "generate.py",
        "commit_date": "2023-04-07T02:21:16Z",
        "message": "fixed unexpected character in prompt"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "generate.py",
        "commit_date": "2023-04-04T11:21:26Z",
        "message": "modified peft/setup.py for installation"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "generate.py",
        "commit_date": "2023-04-02T06:50:08Z",
        "message": "solve the generate bug using bottleneck adapters"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "generate.py",
        "commit_date": "2023-04-01T11:59:26Z",
        "message": "support parallel adapter and gpt-j model"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "generate.py",
        "commit_date": "2023-03-29T13:37:38Z",
        "message": "initial upload"
    },
    {
        "repo_url": "github.com/liguodongiot/llm-action",
        "filepath": "train/peft/multimodal/blip2_lora_inference.py",
        "commit_date": "2023-11-30T14:04:31Z",
        "message": "fix-2023-11-30_22:04:31"
    },
    {
        "repo_url": "github.com/GoogleCloudPlatform/vertex-ai-samples",
        "filepath": "community-content/vertex_model_garden/model_oss/peft/handler.py",
        "commit_date": "2024-02-20T15:17:32Z",
        "message": "Unify the model name as MODEL_ID in related containers and notebooks. (#2720)"
    },
    {
        "repo_url": "github.com/GoogleCloudPlatform/vertex-ai-samples",
        "filepath": "community-content/vertex_model_garden/model_oss/peft/handler.py",
        "commit_date": "2024-01-31T16:12:34Z",
        "message": "Add Peft additional parameters (#2657)"
    },
    {
        "repo_url": "github.com/GoogleCloudPlatform/vertex-ai-samples",
        "filepath": "community-content/vertex_model_garden/model_oss/peft/handler.py",
        "commit_date": "2023-12-04T15:11:07Z",
        "message": "copy vertex_vision_model_garden as vertex_model_garden (#2565)"
    },
    {
        "repo_url": "github.com/THUDM/AgentTuning",
        "filepath": "eval_heldout/rewoo/alpaca/lora.py",
        "commit_date": "2023-10-18T19:04:28Z",
        "message": "First commit"
    },
    {
        "repo_url": "github.com/bigcode-project/starcoder",
        "filepath": "finetune/merge_peft_adapters.py",
        "commit_date": "2023-05-31T17:20:13Z",
        "message": "Removed unused line"
    },
    {
        "repo_url": "github.com/bigcode-project/starcoder",
        "filepath": "finetune/merge_peft_adapters.py",
        "commit_date": "2023-04-26T14:44:42Z",
        "message": "merging script"
    },
    {
        "repo_url": "github.com/tatsu-lab/alpaca_eval",
        "filepath": "src/alpaca_eval/decoders/huggingface_local.py",
        "commit_date": "2024-01-20T23:19:13Z",
        "message": "[TEST]: float preference (#214)\n\n* [TEST]: float preference\n\n* debug\n\n* passing all the tests"
    },
    {
        "repo_url": "github.com/tatsu-lab/alpaca_eval",
        "filepath": "src/alpaca_eval/decoders/huggingface_local.py",
        "commit_date": "2024-01-16T19:06:28Z",
        "message": "[DOC] explain params"
    },
    {
        "repo_url": "github.com/tatsu-lab/alpaca_eval",
        "filepath": "src/alpaca_eval/decoders/huggingface_local.py",
        "commit_date": "2024-01-16T09:34:36Z",
        "message": "[ENH] add internlm2-chat-20b-ppo (#207)\n\n* add model config\n\n* modify `huggingface_local_completion` to remove EOS\n\n* add results & update leaderboard\n\n* delete extra leaderboard csv\n\n* add docstring of `remove_ending`\n\n* remove reference_outputs.json"
    },
    {
        "repo_url": "github.com/tatsu-lab/alpaca_eval",
        "filepath": "src/alpaca_eval/decoders/huggingface_local.py",
        "commit_date": "2023-07-20T02:09:54Z",
        "message": "[ENH] add replicate + llama 70B (#90)\n\n* [ENH] add replicate decoder\n\n* adding replicate llama 70B\n\n* del .env\n\n* add 70B llama results\n\n* add 70B llama results noprompt"
    },
    {
        "repo_url": "github.com/tatsu-lab/alpaca_eval",
        "filepath": "src/alpaca_eval/decoders/huggingface_local.py",
        "commit_date": "2023-07-05T16:58:10Z",
        "message": "[CLEAN] setting up precommit (#61)\n\n* setting up precommit\n\n* applying isort"
    },
    {
        "repo_url": "github.com/tatsu-lab/alpaca_eval",
        "filepath": "src/alpaca_eval/decoders/huggingface_local.py",
        "commit_date": "2023-07-05T11:42:46Z",
        "message": "black (#55)"
    },
    {
        "repo_url": "github.com/tatsu-lab/alpaca_eval",
        "filepath": "src/alpaca_eval/decoders/huggingface_local.py",
        "commit_date": "2023-07-05T11:31:18Z",
        "message": "[TEST] test  (#50)\n\n* [TEST] test_string_to_dict\n\n* [TEST] test analyze\n\n* [TEST] test parisers\n\n* [TEST] test decoders integration\n\n* [TEST] test decoders unit\n\n* [TEST] test utils\n\n* [TEST] rm openai test\n\n* [TEST] test_pairwise_eval\n\n* [TEST] test_main\n\n* minor\n\n* minor\n\n* [BUG] docstring r\"\n\n* minor\n\n* [BUG] allow no OPENAI_API_KEYS\n\n* update evaluator lb\n\n* setup for tests\n\n* [BUG] doctest\n\n* alpaca_eval_fn -> verified\n\n* black\n\n* ensure all test pass\n\n* ensure all test pass"
    },
    {
        "repo_url": "github.com/tatsu-lab/alpaca_eval",
        "filepath": "src/alpaca_eval/decoders/huggingface_local.py",
        "commit_date": "2023-07-05T00:11:06Z",
        "message": "fix falcon decoding"
    },
    {
        "repo_url": "github.com/tatsu-lab/alpaca_eval",
        "filepath": "src/alpaca_eval/decoders/huggingface_local.py",
        "commit_date": "2023-06-13T21:11:20Z",
        "message": "add AttributeError exception"
    },
    {
        "repo_url": "github.com/tatsu-lab/alpaca_eval",
        "filepath": "src/alpaca_eval/decoders/huggingface_local.py",
        "commit_date": "2023-06-13T14:09:39Z",
        "message": "remove comment about progress"
    },
    {
        "repo_url": "github.com/tatsu-lab/alpaca_eval",
        "filepath": "src/alpaca_eval/decoders/huggingface_local.py",
        "commit_date": "2023-06-13T14:03:07Z",
        "message": "add progress bar"
    },
    {
        "repo_url": "github.com/tatsu-lab/alpaca_eval",
        "filepath": "src/alpaca_eval/decoders/huggingface_local.py",
        "commit_date": "2023-06-08T12:24:23Z",
        "message": "gitignore\n\nsetting up\n\nclean utils\n\npairwise lb\n\ntypes\n\ninitial setup\n\ninitial requirements\n\nREADME\n\npairwise annotator done\n\nopenai done\n\nmain\n\nmetrics\n\nsetting up empty\n\nlicense\n\nall prompts\n\nexamples\n\nadd anthropic\n\nadd claude prompts\n\nminor OAI\n\nanthropic installation\n\nget_decoder\n\nget_decoder\n\nmax_instances\n\nadding guanaco\n\noasst\n\nstablelm\n\nhugging face\n\nremove langchain\n\nminor\n\nfinish all decoders\n\nhuggingface_local_completions\n\nhuggingface_api_completions\n\nPACKAGES_ALL\n\nadd opt test\n\nupdate packages\n\ndebugging huggingface_local_completions\n\napi_completions\n\n[ENH] add timer\n\n[ENH] fast hugging face local\n\n[CONF] better default models\n\n[CONF] adding all basic conf\n\ntested all basic configs\n\nadd constatns\n\nadd constatns\n\nadd constatns\n\ndocstrings\n\ngigignore\n\n[ENH] cohere\n\n[CLEAN] use hf datasets\n\ncleaning\n\ncleaning\n\nWIP analyze\n\nfn_completions\n\nmino\n\n[ENH] return price and time per example\n\n[ENH] return price and time per example\n\nadd price and time for turkers\n\nWIP agreement_of_annotations\n\n[ENH] agreement_of_annotations\n\n[ENH] add vicuna parsing\n\nfinish vicuna adding\n\n[SCRIPT] add precompute script\n\n[SCRIPT] add precompute script\n\nadd falcon\n\nadd vicuna with inputs\n\nblack\n\n[ENH] list bias\n\n[ENH] vicuna -> lmsys\n\n[ENH] vicuna -> lmsys\n\nblack\n\nalpaca_farm_ppo_human_7b\n\nsetup\n\nmax_instances\n\nbug vicuna\n\n[ENH] analyze_evaluators\n\nclean prompts\n\nminor\n\nleaderboards\n\nmake_evaluator_leaderboard\n\nrm make_evaluator_leaderboard\n\nchange gpt3 to text-davinci-003\n\n[ENH] max_instances to precompute\n\nsolve merging\n\nevaluator leaderboard\n\nminor\n\nadd plotting\n\nadd plotting\n\nrename all and finish leaderboard\n\nrm json\n\nadd local models to lb\n\nadd local models to lb\n\nadd local models to lb\n\nadd local models to lb\n\nREADME\n\nupdate the readme\n\nupdate the readme\n\ninitial adding of constants\n\nignore\n\nclaude lb\n\nformatting\n\nadd make_model_leaderboard\n\nupdate lb\n\nadd constants\n\nminor\n\nis_return_instead_of_print\n\nsave main outputs\n\nMODELS_TO_BENCHMARK\n\nupdate claude leaderboard\n\nleaderbaords\n\nrename\n\nminor\n\nminor\n\nminor\n\n[NOTEBOOK] compare annotators\n\nrm i.dea\n\nupdate readme\n\ncaching\n\nprices\n\nprices\n\ngpt\n\nleadeboards\n\ninstruction-following prompt\n\nminor\n\nminor\n\nrm caches\n\nleaderboard claude drop\n\naviary\n\naviary\n\nREADME\n\naviary\n\nreadme\n\nAPI constants\n\nAPI constants\n\nmaking new evaluator\n\nformatting readme\n\nminor\n\nMaking a new evaluator\n\nminor\n\ninstallation\n\ndeveloping notebooks\n\nrm unecessary\n\nranking\n\nbetter error\n\nreadme\n\nminor\n\nis_single_annotator\n\nleaderboard\n\nANTHROPIC_MAX_CONCURRENCY\n\n[enh] is_save_to_leaderboard\n\n[enh] is_save_to_leaderboard\n\nimports\n\nranking_parser\n\nranking_parser\n\nminor rename\n\ncheck imports\n\ncaching leaderboard\n\ncaching leaderboard\n\nrename completion kwargs\n\nrohan benchmarking\n\nrm example\n\nmoving to evaluators_configs\n\nsingle prompt\n\nremove all unecessary prompts\n\nmodel_configs\n\nrm all input field\n\nupdate readme\n\nupdate readme\n\nadding strip\n\ndocumentation\n\n[CONF] add improved configs\n\nprompts\n\nleaderboards\n\ngitignore\n\nanthropic n_retries\n\nnames of models to keep\n\nhugging face inference_helper\n\nsave to results\n\nconstants\n\nupdate readme\n\nallow globing\n\nleaderboards\n\ncleaning leaderboards\n\ncleaning leaderboards\n\npackage_data\n\ndelete example\n\nadd manifest\n\nadd outputs example\n\nAlpacaEval\n\nfinish developing evalset\n\nleaderboards\n\nleaderboards\n\naviary\n\nbug alpaca farm prompt\n\nleaderboards\n\nleaderboards\n\nbias 1\n\ncompare annotators\n\nnotebook anntoators\n\nconstants\n\nprecompute\n\nallow additional columns\n\nleaderboard\n\nupdate lb\n\nadd table of content\n\nadd TOC\n\nadding more dropdowns\n\nupdate leaderboard\n\nupdate leaderboards\n\nboilerplate for website\n\nmove boilerplate\n\nCreate CNAME\n\nDelete CNAME\n\nAlpacaFarm -> AlpacaEval\n\nadding doc\n\nupdate html\n\nadding helper\n\nadding all helper to README\n\nupdate all leaderboards\n\nupdate all leaderboards\n\nsmaller example of outputs\n\nadd leaderboard modes\n\nudpate readmes\n\nevaluators leaderboard\n\nprint_leaderboard\n\nudpate precompute\n\nconstants\n\nleaderboard_mode_to_print to analyze eval\n\nupdate html\n\nadd radio buttons\n\nudpate differences with alpacafarm\n\nupdate all notebooks\n\nerror out\n\n003 leaderboard\n\nnotebooks analyzing all\n\nanalyzing_annotators\n\nfinish plotting of analyzs\n\nadd figures\n\nadd figures\n\ndding first plot\n\nfinish readme\n\nfinish readme\n\nfix typos in readme.\n\nfix citation issues.\n\nfix readme.\n\nfix setup.\n\nminor.\n\nadd outputs.json example\n\nfix small issues with first headline cmd.\n\ntitle aesthetics.\n\ntitle.\n\nadd filters button\n\nadd all model configs\n\nadd results export file\n\nminor diffs\n\nprettify website\n\nudpate leaderboards\n\nfinish website\n\nscoping intro\n\nscoping intro\n\nscoping intro\n\nbug fix\n\nadd gpt4 full leaderboard\n\nudpate gpt4 leaderboard website\n\nadd interpretation of leaderboards\n\nfinish explanation of main eval metrics\n\nfinish explanation of all eval metrics\n\nfinish explanation of all eval metrics\n\nfinish explanation of all eval metrics\n\nfinish up to evaluator\n\ntest\n\ntest\n\nrun on claude instead of gpt4\n\nadd related work\n\nshorter section\n\nadd limitation section\n\nadd to related work\n\nadd to related work\n\nfinish readme\n\nupdate website:\n\nformat dividers\n\nupdate readme\n\nmake image bigger\n\nmake image bigger\n\nadd contribution guidelines\n\ntypo\n\nupdate readmes\n\nrunning notebook\n\nadd wizard lm\n\nchange subtitle webiste\n\nadd link\n\nadd github\n\nupdate leaderboards\n\nlast\n\nupdate\n\nfinished through tatsu PR\n\nfinished through tatsu PR\n\npass through tatsu PR\n\npass through tatsu PR\n\nadd github"
    },
    {
        "repo_url": "github.com/microsoft/Olive",
        "filepath": "olive/passes/pytorch/lora.py",
        "commit_date": "2024-02-29T10:04:27Z",
        "message": "enable lint f-string check (#973)\n\n## Describe your changes\n* Enable linter check for f-string logging\n\n## Checklist before requesting a review\n- [ ] Add unit tests for this change.\n- [ ] Make sure all tests can pass.\n- [ ] Update documents if necessary.\n- [ ] Lint and apply fixes to your code by running `lintrunner -a`\n- [ ] Is this a user-facing change? If yes, give a description of this\nchange to be included in the release notes.\n\n## (Optional) Issue link"
    },
    {
        "repo_url": "github.com/microsoft/Olive",
        "filepath": "olive/passes/pytorch/lora.py",
        "commit_date": "2024-01-26T07:39:32Z",
        "message": "fix pylint failure caused by wrong ignore-paths pattern (#902)\n\n## Describe your changes\n\n## Checklist before requesting a review\n- [ ] Add unit tests for this change.\n- [ ] Make sure all tests can pass.\n- [ ] Update documents if necessary.\n- [x] Lint and apply fixes to your code by running `lintrunner -a`\n- [ ] Is this a user-facing change? If yes, give a description of this\nchange to be included in the release notes.\n\n## (Optional) Issue link"
    },
    {
        "repo_url": "github.com/microsoft/Olive",
        "filepath": "olive/passes/pytorch/lora.py",
        "commit_date": "2024-01-09T23:10:18Z",
        "message": "Add `LoftQ` pass (#861)\n\n## Describe your changes\nThis PR adds a new pass `LoftQ` which is based on the paper\nhttps://arxiv.org/abs/2310.08659.\nIt adds on the qlora fine tuning approach by initializing the\nquantization and lora adapters together. This \"alleviates the\ndiscrepancy between the quantized and full-precision model and\nsignificantly improves generalization in downstream tasks\".\n\nSince most of the fine-tuning steps are the same as `QLoRA`, the common\ncode has been abstracted into a new base class called `QLoRABase`.\n`LoftQ` is a separate pass and not an option for `QLoRA` so that we\ndon't conflate it too much and make the configuration confusing for the\nuser. They can just switch the pass type instead.\n\n\u00a0\n## Checklist before requesting a review\n- [x] Add unit tests for this change.\n- [x] Make sure all tests can pass.\n- [x] Update documents if necessary.\n- [x] Lint and apply fixes to your code by running `lintrunner -a`\n- [x] Is this a user-facing change? If yes, give a description of this\nchange to be included in the release notes.\n\nIntroduced a new pass `LoftQ` which performs model fine-tuning using the\nLoftQ initialization proposed in https://arxiv.org/abs/2310.08659.\n\n## (Optional) Issue link"
    },
    {
        "repo_url": "github.com/microsoft/Olive",
        "filepath": "olive/passes/pytorch/lora.py",
        "commit_date": "2023-12-12T02:47:52Z",
        "message": "Fix onnx conversion of bnb quantized model, Remove static methods from `HfConfigMixin`  (#807)\n\n## Describe your changes\nThis PR fixes some issues with the conversion of 4bit quantized models\nto onnx:\n- The adapters are unloaded first so that the correct quantized moduels\ncan be found\n- A new pytorch handler is created. Previously, we were ignoring the\nadapters\n\nChanges the static method in `HfConfigMixin` to regular methods since\nthese were made static to support the above use case. But they are no\nlonger required to be static. The code is now more consistent.\n\nTest requirements updated to for transformers since lora unit tests fail\notherwise.\n\u00a0\n## Checklist before requesting a review\n- [x] Add unit tests for this change.\n- [x] Make sure all tests can pass.\n- [ ] Update documents if necessary.\n- [x] Lint and apply fixes to your code by running `lintrunner -a`\n- [ ] Is this a user-facing change? If yes, give a description of this\nchange to be included in the release notes.\n\n## (Optional) Issue link"
    },
    {
        "repo_url": "github.com/microsoft/Olive",
        "filepath": "olive/passes/pytorch/lora.py",
        "commit_date": "2023-12-08T07:51:33Z",
        "message": "refactoring OliveModel and change its name to OliveModelHandler. (#786)\n\n## Describe your changes\n* Rename all OliveModel by adding suffix \"Handler\". For example\nPyTorchModel -> PyTorchModelHandler.\n* Split the logic doesn't belong to model handler into separated mixins.\nHfConfigMixin, IoConfigMixin, DummyInputsMixin, etc.\n* Merge the CompositeOnnxModel and CompositePyTorchModel into one\nCompositeModelHandler.\n* Keep using the old \"OliveModel\" types\n\nBasically, all the logic is keeping same except some logic simplify\nchanges:\n* Merge the Composite ONNX and PyTorch model. \n* Simplify the composite model enumerate logic by using a single method.\n* Same for original PyTorch model to get the hugging face model\ncomponents. In the new implementation, only one get_hf_components is\nused to enumerate both name and model.\n* As a side effect the merging composite onnx model and pytorch model,\nthe original CompositePyTorch model conversion logic is in\nconversion.py, but with the new changes, the child model get and run for\neach child model will be done by the run of olive_pass.py\n* As a side effect of providing only single entry point to enumerate the\nmodel components, the insert_beam_search pass is updated by using the\nnew get model component logic, so does the olive_pass.run.\n\nFrom the time being, the following logic is not tested\n* Distributed models\n* Composite PyTorch model\n\n## Checklist before requesting a review\n- [x] Add unit tests for this change.\n- [ ] Make sure all tests can pass.\n- [x] Update documents if necessary.\n- [x] Lint and apply fixes to your code by running `lintrunner -a`\n- [ ] Is this a user-facing change? If yes, give a description of this\nchange to be included in the release notes.\n\n## (Optional) Issue link"
    },
    {
        "repo_url": "github.com/microsoft/Olive",
        "filepath": "olive/passes/pytorch/lora.py",
        "commit_date": "2023-12-07T02:48:35Z",
        "message": "Add `pydantic_v1` module to make Olive `pydantic` version agnostic (#788)\n\n## Describe your changes\nPydantic v2 had significant breaking changes which blocks migration to\nit. So, we pinned the version to v1.\n\nPydantic v2 has a `v1` namespace inside it. \n\nThis PR introduces `olive.common.pydantic_v1` module that handles the\nimport of `v1` api so that Olive can be version agnostic wrt pydantic.\nThis way, we won't have version conflict with client applications such\nas that in https://github.com/microsoft/Olive/issues/709.\n\nNote:\nThe doc builder still needs pydantic v1 and autodoc_pydantic v1 since we\nare using the v1 api.\n\n## Checklist before requesting a review\n- [ ] Add unit tests for this change.\n- [ ] Make sure all tests can pass.\n- [ ] Update documents if necessary.\n- [ ] Lint and apply fixes to your code by running `lintrunner -a`\n- [ ] Is this a user-facing change? If yes, give a description of this\nchange to be included in the release notes.\n\n## (Optional) Issue link"
    },
    {
        "repo_url": "github.com/microsoft/Olive",
        "filepath": "olive/passes/pytorch/lora.py",
        "commit_date": "2023-12-06T10:43:39Z",
        "message": "Rename HFModelLoadingArgs to HFFromPretrainedArgs (#785)\n\n## Describe your changes\n\nRename `HFModelLoadingArgs` to `HFFromPretrainedArgs`.\n\n###\nUser interface change:\nIn model config, the `model_loading_args` should be\n`from_pretrained_args` instead:\n```\n\"input_model\":{\n    \"type\": \"PyTorchModel\",\n    \"config\": {\n        \"hf_config\": {\n            \"model_name\": \"microsoft/phi-1_5\",\n            \"task\": \"text-generation\",\n            \"model_loading_args\" -> \"from_pretrained_args\": {\n                \"trust_remote_code\": true\n            }\n        }\n    }\n}\n```\n\n\n## Checklist before requesting a review\n- [ ] Add unit tests for this change.\n- [ ] Make sure all tests can pass.\n- [ ] Update documents if necessary.\n- [ ] Lint and apply fixes to your code by running `lintrunner -a`\n- [x] Is this a user-facing change? If yes, give a description of this\nchange to be included in the release notes.\n\n## (Optional) Issue link"
    },
    {
        "repo_url": "github.com/microsoft/Olive",
        "filepath": "olive/passes/pytorch/lora.py",
        "commit_date": "2023-12-01T08:48:06Z",
        "message": "LoRA/QLoRA: Add `bfloat16` support for ort-training (#769)\n\n## Describe your changes\nONNX Runtime training now supports `bfloat16` for lora and qlora.\n\nThis PR\n- Enables ort-training + bfloat16 by setting\n`ORTMODULE_ONNX_OPSET_VERSION` environment variable. Default value is 16\nwhich is required for models with operators such as Where.\n- Adds unit test for the environment variable.\n- Improves dependency checks in the lora passes.\n- Better organizes the extras for lora/qlora. Workflow setup allows\nassigning multiple extras to a single pass.\n\n## Checklist before requesting a review\n- [x] Add unit tests for this change.\n- [x] Make sure all tests can pass.\n- [x] Update documents if necessary.\n- [x] Lint and apply fixes to your code by running `lintrunner -a`\n- [x] Is this a user-facing change? If yes, give a description of this\nchange to be included in the release notes.\nLoRA/QLoRA support `bfloat16` with ort-training.\n## (Optional) Issue link"
    },
    {
        "repo_url": "github.com/microsoft/Olive",
        "filepath": "olive/passes/pytorch/lora.py",
        "commit_date": "2023-11-17T01:06:03Z",
        "message": "`trust_remote_code` for hf data components, Default text-gen corpus dataset_type (#728)\n\n## Describe your changes\nThis PR \n- exposes `trust_remote_code` arguments to huggingface data components\nthat require user permission to run remote code. For input models with\nhf config model_loading_args, the value for `trust_remote_code` is auto\ninserted into the data configs without these specified.\n- makes the default value for `dataset_type` in\n`test_generation_pre_process` component to `corpus`. Previously, the\ndata config set it to `None` if not given and this results in using\n`pair` strategy.\n- Casts attention mask to input_id's data type to ensure it has the\nexpected data type.\n- Removes `\"dataset_type\":\"corpus\"` from the examples since it is the\ndefault and the most common type.\n- Corrects the dataset config in falcon example. Dataset is also changed\nto `timdettmers/openassistant-guanaco` since the previous dataset is\nmassive (>1TB) and we only need it for latency measurement.\n\u00a0\n## Checklist before requesting a review\n- [x] Add unit tests for this change.\n- [x] Make sure all tests can pass.\n- [ ] Update documents if necessary.\n- [x] Lint and apply fixes to your code by running `lintrunner -a`\n- [ ] Is this a user-facing change? If yes, give a description of this\nchange to be included in the release notes.\n\n## (Optional) Issue link"
    },
    {
        "repo_url": "github.com/microsoft/Olive",
        "filepath": "olive/passes/pytorch/lora.py",
        "commit_date": "2023-11-14T02:34:45Z",
        "message": "LoRA/QLoRA: Mixed precision for `torch_dtype=float16`, Support onnxruntime-training (#722)\n\n## Describe your changes\nChanges in this PR:\n- Full `float16` training is unstable. Now the lora passes use\nmixed-precision training (`fp16=True` in the trainer) when `torch_dtype`\nis `float16`.\n- `compute_dtype` for the quantized modules is a separate config\nparameter to allow for flexibility. If not provided, use the same dtype\nas `torch_dtype`\n- Support for `onnxruntime-training`. \n\nAlso adds an example for qlora with onnxruntime-training. \n\n## Checklist before requesting a review\n- [ ] Add unit tests for this change.\n- [ ] Make sure all tests can pass.\n- [ ] Update documents if necessary.\n- [ ] Lint and apply fixes to your code by running `lintrunner -a`\n- [ ] Is this a user-facing change? If yes, give a description of this\nchange to be included in the release notes.\n\n## (Optional) Issue link"
    },
    {
        "repo_url": "github.com/microsoft/Olive",
        "filepath": "olive/passes/pytorch/lora.py",
        "commit_date": "2023-11-08T05:35:56Z",
        "message": "Accept device and dtype in `OnnxConversion`, Add `OnnxBnb4Quantization`, llama2 e2e qlora example (#703)\n\n## Describe your changes\n**OnnxConversion**\n- The code has been refactored a bit so that the methods\n`_convert_model_on_device` and `_convert_distributed_model_on_device`\nhave similar signature and behavior\n- Doc strings for the methods\n- The above also makes the logic for composite models simpler\n- New config parameters added to make the pass more flexible:\n- `use_device`: users can specify what device to run conversion on. For\nexample, there is enough gpu memory to speed up conversion or model is\nfloat16.\n- `torch_dtype`: torch data type to cast the model to before conversion.\nFor example, model is originally float16 but want to convert on cpu and\nrun another pass later to convert to float16. \u00a0\n- It is more aware about loading models using `hf_config`.\n  - Check and update `torch_dtype`\n  - Check if model is quantized using bitsandbytes. \n  \n**OnnxBnb4Quantization**\n- New quantization pass to quantize a model using nf4/fp4. Uses the\n`MatMulBnb4` quantizer and contrib op\n- It can handle both pass config and model attributes such as\n`quantized_modules` and `quantization_config`\n- Using this pass, we usually don't want to quantize all `MaMul` nodes,\nonly the ones that were originally quantized in the source model to\nretain accuracy\n\n**llama2**\n- Add E2E qlora + ort optimization workflow\n- Update readme\n\n\n![qlora-e2e](https://github.com/microsoft/Olive/assets/94929125/05600ba9-a862-4a98-8258-62a52bbfb713)\n\n## Checklist before requesting a review\n- [ ] Add unit tests for this change.\n- [ ] Make sure all tests can pass.\n- [x] Update documents if necessary.\n- [x] Format your code by running `pre-commit run --all-files`\n- [x] Is this a user-facing change? If yes, give a description of this\nchange to be included in the release notes.\n`OnnxConversion` pass has new parameters `use_device` and `torch_dtype`\nfor more flexible conversion.\nNew `OnnxBnb4Quantization` to quantize an ONNX model using `FP4/NF4`\ndata types.\n## (Optional) Issue link"
    },
    {
        "repo_url": "github.com/microsoft/Olive",
        "filepath": "olive/passes/pytorch/lora.py",
        "commit_date": "2023-11-06T18:33:07Z",
        "message": "[LoRA/QLoRA] Check for gradient checkpointing support before enabling it (#695)\n\n## Describe your changes\nThese passes enabled gradient checkpointing without checking if the\nmodel supports it.\n\nNow, it not supported, they log a warning and force\n`gradient_checkpointing` to `False`.\nAlso set `gradient_checkpointing` in `phi` example to `False` since it\ndoesn't actually do gradient checkpointing and the flag might be set to\nFalse in a future update.\n\u00a0\n## Checklist before requesting a review\n- [ ] Add unit tests for this change.\n- [ ] Make sure all tests can pass.\n- [ ] Update documents if necessary.\n- [ ] Format your code by running `pre-commit run --all-files`\n- [ ] Is this a user-facing change? If yes, give a description of this\nchange to be included in the release notes.\n\n## (Optional) Issue link"
    },
    {
        "repo_url": "github.com/microsoft/Olive",
        "filepath": "olive/passes/pytorch/lora.py",
        "commit_date": "2023-10-31T03:00:30Z",
        "message": "Add LoRA pass (#638)\n\n## Describe your changes\n\nAdd LoRA pass\n\n## Checklist before requesting a review\n- [x] Add unit tests for this change.\n- [x] Make sure all tests can pass.\n- [x] Update documents if necessary.\n- [x] Format your code by running `pre-commit run --all-files`\n- [ ] Is this a user-facing change? If yes, give a description of this\nchange to be included in the release notes.\n\n## (Optional) Issue link\n\n---------\n\nCo-authored-by: Jambay Kinley <jambaykinley@microsoft.com>"
    },
    {
        "repo_url": "github.com/Clouditera/secgpt",
        "filepath": "train.py",
        "commit_date": "2023-12-07T02:43:41Z",
        "message": "\u8bad\u7ec3\u811a\u672c\u5c0f\u4f18\u5316"
    },
    {
        "repo_url": "github.com/Clouditera/secgpt",
        "filepath": "train.py",
        "commit_date": "2023-11-20T10:16:26Z",
        "message": "\u4ee3\u7801\u4f18\u5316"
    },
    {
        "repo_url": "github.com/Clouditera/secgpt",
        "filepath": "train.py",
        "commit_date": "2023-11-20T07:33:28Z",
        "message": "update readme"
    },
    {
        "repo_url": "github.com/Clouditera/secgpt",
        "filepath": "train.py",
        "commit_date": "2023-11-20T06:00:38Z",
        "message": "first blood"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-12-27T03:18:14Z",
        "message": "Add 64k-context models (#478)\n\nFor full changes, see the latest release.\n\n---------\n\nCo-authored-by: iMountTai <2506700016@qq.com>\nCo-authored-by: Xin Yao <35353688+iMountTai@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-09-21T03:28:33Z",
        "message": "Fix quantized inference (#302)\n\nFixed possible mismatches caused by high version dependencies."
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-08-09T03:29:18Z",
        "message": "Merge pull request #43 from lealaxy/stream-openai-api\n\nStreaming OpenAI API support"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-08-07T10:24:54Z",
        "message": "fix api first toekn missed"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-08-04T13:25:46Z",
        "message": "fix openai api  repetition of the last word"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-08-04T03:31:17Z",
        "message": "fix api extra token"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-08-03T17:09:16Z",
        "message": "fix stream api bug"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-08-03T16:58:34Z",
        "message": "fix stream api split"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-08-03T09:36:00Z",
        "message": "update default hyperparameters"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-08-03T06:29:31Z",
        "message": "Add openai stream api docs."
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-08-03T01:05:52Z",
        "message": "fix spelling error"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-08-02T12:36:06Z",
        "message": "Merge branch 'main' into stream-openai-api"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-08-02T09:31:57Z",
        "message": "Update openai_api_server.py"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-08-02T09:22:47Z",
        "message": "modify message information"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-08-02T08:49:13Z",
        "message": "Reserve load_in_8bit argument"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-08-02T03:04:09Z",
        "message": "Merge pull request #35 from GoGoJoestar/main\n\nadd vLLM surpport for gradio demo, inference script and openai api demo"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-08-01T16:06:17Z",
        "message": "add stream openai api support"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-08-01T12:21:05Z",
        "message": "fix system prompt"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-08-01T09:22:41Z",
        "message": "fix spelling error: tokenzier_vocab_size -> tokenizer_vocab_size"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-07-31T06:12:17Z",
        "message": "add scripts"
    },
    {
        "repo_url": "github.com/lyhue1991/torchkeras",
        "filepath": "torchkeras/hugmodel.py",
        "commit_date": "2024-03-01T06:08:50Z",
        "message": "update 3.9.7"
    },
    {
        "repo_url": "github.com/lyhue1991/torchkeras",
        "filepath": "torchkeras/hugmodel.py",
        "commit_date": "2023-12-04T07:24:11Z",
        "message": "master: Add comments according to Google Code Rules"
    },
    {
        "repo_url": "github.com/lyhue1991/torchkeras",
        "filepath": "torchkeras/hugmodel.py",
        "commit_date": "2023-07-15T08:09:08Z",
        "message": "update 3.9.1"
    },
    {
        "repo_url": "github.com/lyhue1991/torchkeras",
        "filepath": "torchkeras/hugmodel.py",
        "commit_date": "2023-06-25T16:06:26Z",
        "message": "update 3.8.9"
    },
    {
        "repo_url": "github.com/lyhue1991/torchkeras",
        "filepath": "torchkeras/hugmodel.py",
        "commit_date": "2023-06-11T15:23:16Z",
        "message": "update 3.8.8"
    },
    {
        "repo_url": "github.com/lyhue1991/torchkeras",
        "filepath": "torchkeras/hugmodel.py",
        "commit_date": "2023-05-29T14:32:11Z",
        "message": "update 3.8.6"
    },
    {
        "repo_url": "github.com/lyhue1991/torchkeras",
        "filepath": "torchkeras/hugmodel.py",
        "commit_date": "2023-05-27T06:00:49Z",
        "message": "update 3.8.6"
    },
    {
        "repo_url": "github.com/FlagAI-Open/FlagAI",
        "filepath": "flagai/model/tools/peft/tuners/prompt_tuning.py",
        "commit_date": "2023-07-05T08:20:51Z",
        "message": "added enable input embeddings for lora\n\nSigned-off-by: ftgreat <ftgreat@163.com>"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2024-02-28T17:59:20Z",
        "message": "Remove \"generation_length\" in favor of \"generation_kwargs\" (#3014)\n\n* kill generation_length\n\n* fix tests\n\n* fix test\n\n* add deprecation warning\n\n* fix test\n\n* add gen_len back into static_keys\n\n* simplify setting variable in forward and add test\n\n* simply test\n\n* trailing comma\n\n* trailing comma\n\n* linting\n\n---------\n\nCo-authored-by: Daniel King <43149077+dakinggg@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2024-01-31T15:55:53Z",
        "message": "Remove torch 1.13 (#2941)\n\n* v1\n\n* kill torch 1.13\n\n* more cleanup\n\n* remove local\n\n* lint\n\n* raise value error\n\n* remove import\n\n* lint\n\n* fix\n\n* fix type\n\n* lint\n\n* fix test\n\n* remove type\n\n* fix types\n\n* fix\n\n* fix\n\n* fix\n\n* remove vision\n\n* fix tests"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2024-01-30T06:03:43Z",
        "message": "Fix daily tests for peft on 1.13 (#2923)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2024-01-30T00:20:14Z",
        "message": "Fix daily tests for peft + fsdp (#2920)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2024-01-29T18:59:43Z",
        "message": "Integrate PEFT LoRA with HuggingFaceModel (#2829)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2024-01-23T01:43:51Z",
        "message": "Bump transformers to 4.37 (#2894)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2024-01-12T21:04:53Z",
        "message": "Upgrade pyright to 1.1.310 (#2841)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-10-23T21:44:20Z",
        "message": "Upgrade to transformers 4.34.1 (#2635)\n\n* bump transformers version\n\n* add new special casing to tokenizer equivalence check\n\n* try/except for flash v1 issue"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-10-05T21:04:29Z",
        "message": "Change the tokenizer json file to read binary (#2608)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-09-26T00:48:04Z",
        "message": "Add log_model to MLFlowLogger (#2541)\n\n* tie module run name to client run name"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-08-30T23:19:46Z",
        "message": "Removing min_params (#2494)\n\n* Removing min_params\n\n* formatting?\n\n* removing overlap with another commit"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-08-28T18:35:34Z",
        "message": "Fix huggingface tokenizer loading for slow tokenizers (#2483)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-07-25T21:35:04Z",
        "message": "Change transformers (#2383)\n\n* fix autoresume with slashed directory\n\n* Revert \"fix autoresume with slashed directory\"\n\nThis reverts commit 3dfb5f5430da5512bbf418820086b4f291d814f6.\n\nrevert\n\n* upgrade transformers to 4.31.0\n\n* add transformers support\n\n* fix precommit\n\n* fix precommit\n\n* fix precommit\n\n* add trust_remote_code\n\n* pre-commit fix"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-06-08T17:37:26Z",
        "message": "fixed adding tokenizer to hf (#2290)\n\nFixed the function 'write_huggingface_pretrained_from_composer_checkpoint' to properly write the tokenizer from the composer checkpoint by additionally saving the tokenizer from the composer checkpoint to the HuggingFace checkpoint.\n\nAn additional test was implemented for this issue.\n\n---------\n\nCo-authored-by: Vincent Chen <vincent@mosaicml.com>"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-06-05T19:53:50Z",
        "message": "Patch for tokenizers that have python files in save_pretrained output (#2279)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-06-05T16:51:41Z",
        "message": "Confirming the output variable has two dimensions before confirming the shape of the second element. (#2275)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-05-22T18:05:00Z",
        "message": "CE loss vs CE metric equivalence (#2241)\n\n* add equivalence test\n* add the xfail test"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-05-06T00:54:11Z",
        "message": "Add support for saving HF info in state dict when using DDP (#2206)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-05-02T03:04:31Z",
        "message": "Update warning->info for number of tokens (#2192)\n\n* update warning\n\n* fix test\n\n* update warning\n\n---------\n\nCo-authored-by: nik-mosaic <101217697+nik-mosaic@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-04-20T21:43:04Z",
        "message": "Upgrade to transformers 4.28 (#2152)\n\n* upgrade transformers version\n* fix tokenizer equivalence test"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-04-06T20:22:50Z",
        "message": "add logic for direct instantiation (#2122)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-04-03T18:07:46Z",
        "message": "update transformers to latest version (#2109)\n\n* update transformers to latest version\n* xfail gpt + ddp"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-03-31T21:14:11Z",
        "message": "add sentencepiece support (#2093)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-03-24T00:37:58Z",
        "message": "skip fsdp tests for <1.13 (#2090)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-03-21T00:47:40Z",
        "message": "Add support for ICL QA tasks and generation during evaluation with `HuggingFaceModel` (#2045)\n\n* Add InContextLearningQADataset and InContextLearningQAAccuracy for generation style tasks\n* Add support for calling generate using HuggingFaceModel\n* Fixes duplicated download of ICL datasets per node"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-03-20T18:14:10Z",
        "message": "fix name_or_path usage in HF save/load usage (#2075)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-03-07T21:41:56Z",
        "message": "Remove deprecated code (#2026)\n\n* cleanup\n\n* cleanup\n\n* fix hf model\n\n* fix test\n\n* fix test\n\n* set to evla metrics\n\n* fix bug in metrics\n\n* switch to warnings\n\n* add warnings\n\n* filter warnings\n\n* retry\n\n* lint\n\n* remove dead comment\n\n* remove test"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-03-06T20:40:38Z",
        "message": "Adjust how HuggingFaceModel handles embedding resizing (#2027)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-03-01T22:53:40Z",
        "message": "Upgrade torchmetrics (#2017)\n\nChange default Accuracy to MulticlassAccuracy\n\n---------\n\nCo-authored-by: nik-mosaic <None>"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-02-25T00:18:26Z",
        "message": "Test hf fsdp (#1972)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-02-17T18:45:22Z",
        "message": "Util for writing HuggingFace save_pretrained from a composer checkpoint (#1974)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-02-15T21:07:09Z",
        "message": "allow eval metrics to be passed in to HuggingFaceModel directly (#1971)\n\n* allow eval metrics to be passed in to HuggingFaceModel directly\n* add deprecation warning and fix typo"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-02-09T21:41:35Z",
        "message": "add support for enc-dec batches without decoder_input_ids (#1950)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-02-09T18:29:30Z",
        "message": "add return dict false test and bug fix (#1948)"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-02-06T18:59:27Z",
        "message": "Deprecate HFCrossEntropy and Perplexity (#1857)\n\n* add languageperplexity and deprecate hfperplexity"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-01-30T22:31:26Z",
        "message": "Remove synthetic testing infrastructure for HF/NLP (#1895)\n\nThis PR removes the \"synthetic\" testing infrastructure setup for HF models, instead using a set of simple fixtures and pytest configuration to avoid downloading things from the HF hub multiple times. These fixtures should be easier to use (they are just the expected HF objects), and do away with a lot of extra code written for setting up the synthetic infrastructure."
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2023-01-27T00:48:37Z",
        "message": "Add logic for shifting labels before computing metrics (#1913)\n\nAdds a `shift_labels` init argument to `HuggingFaceModel` class. This instructs the model whether to shift labels by one token before computing metrics, which mimics the way HF Causal LM classes handle labels when computing loss. This fixes the current implementation, which never does this shifting and produces incorrect metric results for Causal LMs.\n\nIf `shift_labels` is not specified, `HuggingFaceModel` will try to infer the correct behavior based on whether the model is an instance of a registered HF Causal LM class (or a subclass of one)."
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2022-12-19T22:29:58Z",
        "message": "HF factory function tests (#1832)\n\nAdd tests for the HF -> ComposerModel factory functions"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2022-12-06T21:09:05Z",
        "message": "Speeding up tests (#1779)\n\nReducing time spent in unit tests"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2022-12-02T00:57:30Z",
        "message": "Fix hf tokenizer test for new hf version (#1772)\n\nFix hf tokenizer test for new hf version"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2022-11-30T21:55:15Z",
        "message": "Autoload HuggingFace model/tokenizer (#1754)\n\nAdd autoload of HF model/tokenizer from composer checkpoint to HuggingFaceModel"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2022-11-29T21:57:47Z",
        "message": "Add huggingface info to state dict (#1744)\n\nAdd huggingface info to the state dict in the new integrations key."
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2022-11-14T21:50:07Z",
        "message": "Simple nlp tests (#1716)\n\nAdds a simple transformer classifier and a simple \"text\" (really just input ids, not using a tokenizer) classification dataset for testing purposes. Adds a test for HuggingfaceModel using the above dataset to train, eval, and predict. Removes the model_inputs from HuggingfaceModel"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2022-07-27T16:45:58Z",
        "message": "Remove `pytest-timeout` (#1317)\n\nThis PR removes pytest-timeout, all timeout markers from tests, and the --duration flag from running pytest.\n\nThe original intent of adding pytest-timeout was to a) incentivise tests to be fast, b) prevent stalled tests from hogging up system resources, and c) provide a heuristic that could be used to split in parallelize tests in the future. However, it has accomplished none of these goals.\n\na) Slow-running tests have just been giving longer timeouts to get it to pass ci/cd. Instead, slow-running tests should either be moved to run only on the daily build (i.e. @pytest.mark.daily), or be made faster.\n\nb) If a test is stalling due to a distributed error, pytest-timeout doesn't help at all, since Composer will hang on a distributed barrier. Because of inconsistent CI/CD hardware that caused tests to sporadically exceed the timeout, we probably have used more CI/CD minutes re-running failed builds due to timeout failures compared to the amount of CI/CD time the timeout has saved. Instead, we should rely on the pytestTimeout variable in the Jenkinsfile -- this controls how long the k8s pod will run before dying, and it is not affected by torch.dist calls that could freeze the python process. It is set to 30 minutes right now, which is relatively short.\n\nc) For when we want to split and parallelize tests, the junitxml reports already include the amount of time each test case took. So, we can split tests based on their historical execution time, rather than requiring an upper-bound be specified in the codebase.\n\nCloses https://mosaicml.atlassian.net/browse/CO-769"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2022-06-11T00:22:17Z",
        "message": "Switch to single quotes (#1152)\n\nEnable the \"double-quote-string-fixer\" pre-commit hook."
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2022-05-26T12:46:43Z",
        "message": "AutoYAHP Part 2: Cleanup the Callbacks and Loggers for AutoYAHP (#1071)\n\nSimilar to #1056, this PR cleans up the callbacks and loggers for AutoYAHP. It does not depend on AutoYAHP itself (a future PR will remove the underlying hparam classes).\n\n- Refactored callback and logger tests to not depend on hparams\n- Reformatted the docstrings so they would be correctly parsed by auto-yahp\n- Added `callback_settings.py`, similar to `algorithm_settings.py`, to return a list of pytest param objects for parameterization across callback tests. These param objects include appropriate markers (e.g. conditional skipping for wandb and mlperf; requiring that the memory monitor runs on GPU, ...)\n- Moved the `TestTrainerAssets` into `tests/callbacks/test_callbacks.py`, since it tests the individual callbacks and loggers, not the trainer, and thus should live in `tests/callbacks`.\n- Cleaned up the `MemoryMonitor` warnings when cuda is not available. Now, it warns when the model is not on cuda, to catch the edge case where one does CPU training when GPUs are available."
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2022-05-25T05:03:15Z",
        "message": "AutoYAHP Part 1: Cleanup the Algorithms for AutoYAHP (#1056)\n\nThis PR refactors the algorithms and tests as will be required by AutoYAHP. It does not depend on AutoYAHP itself (a future PR will remove the underlying hparam classes).\n\n- Refactored algorithm tests to not depend on hparams\n- Reformatted the factorize and selective backprop docstrings so they would be correctly parsed by auto-yahp\n- Refactor algorithm_settings.py to not depend on hparams and to return a list of pytest.param objects for a pytest.mark.parametrize. This change makes it more re-usable since it now includes information about markers required for each algorithm.\n- Moved the TestTrainerAlgorithms into tests/algorithms/test_algorithms_train.py, since it tests the individual algorithms, not the trainer, and thus should live in tests/algorithms.\n- Add helper methods for scanning a module to discover subclass implementations, check that the registry contains an entry, and test that a class is constructible from yaml"
    },
    {
        "repo_url": "github.com/mosaicml/composer",
        "filepath": "tests/models/test_hf_model.py",
        "commit_date": "2022-05-24T21:18:36Z",
        "message": "Huggingface part1 (#1047)"
    },
    {
        "repo_url": "github.com/liguodongiot/llm-action",
        "filepath": "train/alpaca-lora/export_state_dict_checkpoint.py",
        "commit_date": "2023-07-23T11:43:58Z",
        "message": "fix"
    },
    {
        "repo_url": "github.com/InternLM/lmdeploy",
        "filepath": "lmdeploy/pytorch/adapter/adapter.py",
        "commit_date": "2024-02-23T03:12:05Z",
        "message": "Support mistral and sliding window attention (#1075)\n\n* support yi\n\n* update docs\n\n* add kernel\n\n* fix seq\n\n* add win block manager\n\n* finish mistral\n\n* fix drop block\n\n* update docs\n\n* fix ut\n\n* fix for transformers 4.37.1\n\n* update docs\n\n* mistral template\n\n* readme-zh-cn\n\n* remove print\n\n* remove div up\n\n---------\n\nCo-authored-by: grimoire <yaoqian@pjlab.org.cn>"
    },
    {
        "repo_url": "github.com/InternLM/lmdeploy",
        "filepath": "lmdeploy/pytorch/adapter/adapter.py",
        "commit_date": "2024-02-19T03:32:30Z",
        "message": "fix pytorch engine with peft==0.8.2 (#1122)"
    },
    {
        "repo_url": "github.com/InternLM/lmdeploy",
        "filepath": "lmdeploy/pytorch/adapter/adapter.py",
        "commit_date": "2024-01-26T08:51:13Z",
        "message": "Fix baichuan2 lora (#1042)\n\n* update alibi attention\n\n* fix baichuan tp\n\n* remove func usage\n\n* fix lora\n\n* hide local scaling\n\n* fix"
    },
    {
        "repo_url": "github.com/InternLM/lmdeploy",
        "filepath": "lmdeploy/pytorch/adapter/adapter.py",
        "commit_date": "2024-01-16T10:22:55Z",
        "message": "support mlp s-lora (#957)"
    },
    {
        "repo_url": "github.com/InternLM/lmdeploy",
        "filepath": "lmdeploy/pytorch/adapter/adapter.py",
        "commit_date": "2024-01-09T07:33:41Z",
        "message": "S-LoRA support (#894)\n\n* WIP\n\n* cache engine wip\n\n* finish cache engine\n\n* fix cache and scheduler\n\n* add paged attention\n\n* step and stop\n\n* add infer\n\n* add request process\n\n* fix end\n\n* request without schedulersession\n\n* add logits processor\n\n* better context\n\n* update patch\n\n* [Improve] Use 4d input in pytorch poc (#371)\n\n* 4D input, model.eval and llama config\n\n* use auto dtype\n\n* tp wip\n\n* almost\n\n* update logger\n\n* run_check=false\n\n* little optimize\n\ncurrent best\n\nredist w/o dtensor\n\nhost mem in que\n\nless rewrite\n\nless code\n\nupdate model weight\n\n* share attention forward\n\n* fix end\n\n* Support Baichuan (#382)\n\n* add baichuan WIP\n\n* support baichuan\n\n* support baichuan-13b\n\n* fix\n\n* add chat template\n\n* lint\n\n* comments\n\n* fix\n\n* Move `q_seq_info` into `context` (#398)\n\n* move q seq info into context\n\n* remove debugs\n\n* remove debugs\n\n* alibi wip\n\n* add alibi\n\n* reduce logic block (#435)\n\n* add docstring\n\n* add baichuan lint (#445)\n\n* add fill cache back\n\n* support internlm\n\n* fix path of weight index\n\n* Support chatglm2 in pytorch_poc (#360)\n\n* draft support for chatglm2\n\n* debug llama\n\n* gitignore\n\n* update input_id\n\n* better patching\n\n* patch chatglm2 model\n\n* fix after merge\n\n* remove inits\n\n* q_seq_info & remove some debug & orig_self\n\n* remove old unqeuzze inputid\n\n* update patch and model config\n\n* remove debugs and clean codes\n\n* clean codes\n\n* add credit\n\n* add update id / fix dependency\n\n* rename modules (#504)\n\nCo-authored-by: grimoire <yaoqian@pjlab.org.cn>\n\n* optimize fill kv cache (#523)\n\n* optimize fill kv cache\n\n* update internlm\n\n* faster embedding\n\n* fix bias tp\n\n* fix baichuan2\n\n* fix fill kv cache\n\n* fix lint\n\n---------\n\n* Make trust_remote_code as cli argument (#434)\n\n* trust_remote_code_argument\n\n* format\n\n* update tokenizer\n\n* optimize rotary\n\n* wtf\n\n* Support Falcon models (#406)\n\n* move q seq info into context\n\n* falcon aligned\n\n* trust_remote_code_argument\n\n* fix for falcon\n\n* comment out debugs\n\n* comment out debugs\n\n* use position id in context\n\n* remove codes in falcon model\n\n* Revert \"comment out debugs\"\n\nThis reverts commit ee26a25c36f11c7afdad315b8101ebb5c2c637fd.\n\n* 7b correct\n\n* 1b aligned\n\n* remove debugs\n\n* patch to ignore position ids\n\n* remove debug in alibi, avoid empty inputs\n\n* fix\n\n* rename dir to replace to \"models\"\n\n* use position_id and new fill kernel\n\n* remove useless get_prompt func\n\n* fix batch>2\n\n* Refactor scheduler (#551)\n\n* optimize block manager\n\n* scheduler wip\n\n* finish scheduler\n\n* update engine\n\n* profile pytorch poc (#455)\n\n* profile pytorch poc\n\n* update doc and import if need\n\n* arg\n\n* support profile_throughput.py\n\n* reuse pytorch session\n\n* end session\n\n* Support Tensor parallel on Falcon models (#582)\n\n* tp falcon 1b and 7b works\n\n* remove debugs\n\n* update copyright\n\n* add some comments\n\n* remove a debug\n\n* support new hub models\n\n* support 40b\n\n* support 40b model config\n\n* try\n\n* recover\n\n* fix remain len\n\n* Apply rotary kernel (#572)\n\n* apply rotary kernel\n\n* format\n\n* update rmsnorm\n\n* update rms norm\n\n* better unittest\n\n* add docstring\n\n---------\n\nCo-authored-by: grimoire <yaoqian@pjlab.org.cn>\n\n* fix(pytorch_poc): memory cal (#606)\n\n* fix(pytorch_poc): memory cal\n\n* Optimize attention (#597)\n\n* add unittest\n\n* add split k\n\n* add docstring\n\n* fast split k\n\n* optimize load\n\n* manually setup device and stream\n\n* lint\n\n---------\n\nCo-authored-by: grimoire <yaoqian@pjlab.org.cn>\n\n* feat(pytorch_poc): implement ReRoPE (#625)\n\n* fix(pytorch_poc): memory cal\n\n* style(pytorch_poc): lint\n\n* style(.pre-commit-config.yaml): update\n\n* style(pytorch_poc): remove useless\n\n* feat(pytorch_poc): llama2 support rerope\n\n* feat(pytorch_poc): fix long input generate\n\n* feat(lmdeploy): add kernel\n\n* feat(lmdeploy): update\n\n* feat(lmdeploy): add rerope implementation\n\n* fix(lmdeploy/pytorch_poc): apply rotary_emb\n\n* fix(lmdeploy): update\n\n* style(pytorch_poc): format\n\n* style(lmdeploy): fix lint\n\n* style(lmdeploy): typo\n\n* style(pytorch_poc): format\n\n* style(pytorch_poc): format\n\n* fix(pytorch_poc): rms_norm add mask\n\n* style(pytorch_poc/kernels): format rerope\n\n* style(pytorch_poc): format rerope attn function description\n\n* style(lmdeploy/pytorch_poc): format\n\n* style(pytorch_poc): add code ref\n\n* style(pytorch_poc): format rerope attn\n\n* Refactor engine (#623)\n\n* add agent\n\n* optimize postprocess\n\n* optimize decoding fill cache\n\n* add docstring\n\n* logit to cuda\n\n* blocksize 128\n\n* optimize pre/post process\n\n* fix postprocess\n\n* cpu pre/post process\n\n* manually setup stream and device\n\n* remove context\n\n* update model agent\n\n* update max session len\n\n* remove tqdm\n\n* update pre/post process\n\n* inplace kernel\n\n* avoid kv_len computation\n\n* flash decoding with one cache\n\n* remove comment\n\n* add warning when no enough resources\n\n* step if has unfinish\n\n* add request manager\n\n* better fill kv cache\n\n* fix fill kv cache\n\n* optimize prefill attention\n\n* refractor\n\n* refactoring...\n\n* add custom output\n\n* use cache\n\n---------\n\nCo-authored-by: grimoire <yaoqian@pjlab.org.cn>\n\n* [Feature] w8a8 based on pytorch poc (#595)\n\n* refactor smoothquant and support load w8a8 model by from_pretrained\n\n* add w8a8 docs\n\n* add w8a8 en docs\n\n* add convert_to_qmodules function\n\n---------\n\nCo-authored-by: grimoire <streetyao@live.com>\n\n* feat(lmdeploy): add rerope quantization (#718)\n\n* feat(lmdeploy): add rerope quantization\n\n* feat(lmdeploy): fix review\n\n* [Refactor & Doc] Improve w8a8 and add docstring (#768)\n\n* WIP\n\n* improve w8a8 and add doc string\n\n* add docstring\n\n* add docstring\n\n* fix lint\n\n* rename pytorch poc (#764)\n\n* rename pytorch poc\n\n* fix lint\n\n* add docstring\n\n* add docstring\n\n* refactor patch\n\n* add recompute eviction support\n\n* recovery modeling\n\n* add docstring\n\n* Unified paging (#860)\n\n* change 'model_format' to 'qwen' when 'model_name' starts with 'qwen' (#575)\n\n* avoid split chinese characters during decoding (#566)\n\n* add solar chat template (#576)\n\n* robust incremental decode for leading space (#581)\n\n* robust incremental decode for leading space\n\n* speed up lookup as prefix_space_tokens is shorter than no_prefix_space_tokens\n\n* add UT and fix qwen stuff\n\n* update solar chat template (#587)\n\n* Revert \"[Docs] Simplify `build.md` (#370)\" (#586)\n\nThis reverts commit 4b5c2bda074eb4ac2e70c3c793fb5ef48f87d9c8.\n\n* Fix crash and remove `sys_instruct` from `chat.py` and `client.py`(#591)\n\n* fix crash\n\n* update profile_generation.py\n\n* format\n\n* use self.bos_id\n\n* remove sys_instruct\n\n* bump version to v0.0.12 (#604)\n\n* Add \"build from docker\" section (#602)\n\n* add build from docker section\n\n* update\n\n* install python package\n\n* update\n\n* update\n\n* update\n\n* Add more user-friendly CLI  (#541)\n\n* add\n\n* import fire in main\n\n* wrap to speed up fire cli\n\n* update\n\n* update docs\n\n* update docs\n\n* fix\n\n* resolve commennts\n\n* resolve confict and add test for cli\n\n* support inference a batch of prompts (#467)\n\n* support inference a batch of prompts\n\n* docstring and assert\n\n* bump version to v0.0.13 (#620)\n\n* Improve api_server and webui usage (#544)\n\n* make IPv6 compatible, safe run for coroutine interrupting\n\n* instance_id -> session_id and fix api_client.py\n\n* update doc\n\n* remove useless faq\n\n* safe ip mapping\n\n* update app.py\n\n* WIP completion\n\n* completion\n\n* update doc\n\n* disable interactive mode for /v1/chat/completions\n\n* docstring\n\n* docstring\n\n* refactor gradio\n\n* update gradio\n\n* udpate\n\n* update doc\n\n* rename\n\n* session_id default -1\n\n* missed two files\n\n* add a APIClient\n\n* add chat func for APIClient\n\n* refine\n\n* add concurrent function\n\n* sequence_start, sequence_end --> interactive_mode\n\n* update doc\n\n* comments\n\n* doc\n\n* better text completion\n\n* remove /v1/embeddings\n\n* comments\n\n* deprecate generate and use /v1/interactive/completions\n\n* /v1/interactive/completion -> /v1/chat/interactive\n\n* embeddings\n\n* rename\n\n* remove wrong arg description\n\n* docstring\n\n* fix\n\n* update cli\n\n* update doc\n\n* strict session_len limit condition\n\n* pass model args to api_server\n\n* fix: gradio gr.Button.update deprecated after 4.0.0 (#637)\n\n* add cli to list the supported model names (#639)\n\n* update\n\n* resolve comment\n\n* Refactor model conversion (#296)\n\n* split deploy.py\n\n* fix get_cuda_tensor\n\n* deploy qwen_awq\n\n* fix lint\n\n* add docstring\n\n* fix\n\n* support baichuan/baichuan-awq\n\n* parameterizing size_per_head\n\n* remove try/except\n\n* limit input model_format\n\n* add quant_path param\n\n* remove old deploy.py\n\n* fix path\n\n* fix transformer layer range when load bins\n\n* fix qwen init\n\n* split & save log\n\n* relative import\n\n* update get_config\n\n* WeightFileMgr -> Reader\n\n* rename\n\n* update\n\n* fix init_layer_id\n\n* rename llama.py -> meta_llama.py, hf.py -> llama.py\n\n* reduce code\n\n* update arg description\n\n* fix meta llama\n\n* manually cleanup meta model params\n\n* [Enchance] internlm message to prompt (#499)\n\n* update turbomind session_len with model.session_len (#634)\n\n* [Fix] Qwen's quantization results are abnormal & Baichuan cannot be quantized (#605)\n\n* fix awq\n\n* adapt new qwen code\n\n* adapt qwen 14b and baichuan2 7b\n\n* add docstring\n\n* add runtime error for qwen\n\n* FIX: fix stop_session func bug (#578)\n\n* FIX: fix stop_session func bug\n\n* keep sequence_end = False\n\n---------\n\nCo-authored-by: honglei.yan <honglei.yan@nio.com>\nCo-authored-by: AllentDan <AllentDan@yeah.net>\n\n* Manage session id using random int for gradio local mode (#553)\n\n* Use session id from gradio state\n\n* use a new session id after reset\n\n* rename session id like a state\n\n* update comments\n\n* reformat files\n\n* init session id on block loaded\n\n* use auto increased session id\n\n* remove session id textbox\n\n* apply to api_server and tritonserver\n\n* update docstring\n\n* add lock for safety\n\n---------\n\nCo-authored-by: AllentDan <AllentDan@yeah.net>\n\n* fix benchmark serving computation mistake (#630)\n\n* fix benchmark serving computation mistake\n\n* fix timestamps computations\n\n* remove speed up\n\n* no mp\n\n* mp seems faster?\n\n* remove\n\n* update\n\n* remove\n\n* fix\n\n* update\n\n* update print log\n\n* typo\n\n* print fist token latency only stream==True\n\n* remove renew_session\n\n* update AsyncEngine\n\n* fix tokenizer_info when convert the model (#661)\n\n* Add check env sub command (#654)\n\n* add check env\n\n* update issue template'\n\n* remove some reqs from check env\n\n* resolve comment\n\n* fix Tokenizer load error when the path of the being-converted  model is not writable (#669)\n\n* Add UltraCM and WizardLM chat templates (#599)\n\n* add ultracm eval chat template\n\n* add WizardLM chat template\n\n* use ultrachat template instead of ultracm usecase\n\n* bump version to v0.0.14 (#663)\n\n* Add extra_requires to reduce dependencies (#580)\n\n* update reqs\n\n* update docs\n\n* resolve comments\n\n* upgrade pydantic\n\n* fix rebase\n\n* update doc\n\n* update\n\n* update\n\n* update readme\n\n* update\n\n* add flash-attn\n\n* TurboMind 2 (#590)\n\n* refresh decoder attention kernel\n\n* block-level kv cache\n\n* `BlockManager` & `SequenceManager`\n\n* update\n\n* update\n\n* update\n\n* update\n\n* rename\n\n* GQA support\n\n* fix context length\n\n* GQA dispatch\n\n* kv8\n\n* tune\n\n* async stream cb\n\n* nvtx\n\n* config parsing\n\n* debug\n\n* optimize output cost\n\n* split-k decoding\n\n* minor\n\n* truncate `session_len` by available blocks\n\n* minor\n\n* license\n\n* fix\n\n* dispatch `cp.async`\n\n* fix linking\n\n* fix\n\n* fix deadlock\n\n* guard input length\n\n* correct start offset\n\n* fix prefill chunking\n\n* fix `cache_block_seq_len` param passing\n\n* fix `block_size` fmtstr\n\n* fix output tokens\n\n* fix batch resizing\n\n* fix masking of finished sequences\n\n* add debug util\n\n* free unused block early\n\n* add ntk scaling and logn scaling\n\n* cmake flags\n\n* fix typo\n\n* w4a16 for sm75\n\n* fix msvc build\n\n* fix msvc build\n\n* fix block verification\n\n* fix msvc build\n\n* use `std::shuffle`\n\n* fix lint\n\n* fix lint\n\n* fix lint\n\n* clear incoming buffer\n\n* clear finished requests\n\n* fix batch initialization\n\n* fix typo\n\n* fix typo\n\n* fix comparison\n\n* [Docs] Update Supported Matrix (#679)\n\n* update supported matrix\n\n* change the default shard size when saving quantized weights\n\n* baichuan2 kv8\n\n* update kv8 docs (#681)\n\n* Fix init of batch state (#682)\n\n* fix init of finished buf\n\n* fix `finished_count`\n\n* fix turbomind stream canceling (#686)\n\n* fix\n\n* instance for each forward\n\n* [Fix] Fix load_checkpoint_in_model bug (#690)\n\n* fix load_checkpoint_in_model bug\n\n* fix comments\n\n* fix comments\n\n* fix bugs\n\n* [Doc] Update restful api doc (#662)\n\n* update restful_api.md\n\n* add a hint\n\n* repeat 3 time\n\n* Fix Tokenizer encode (#645)\n\n* same encode with HF\n\n* sequence_start -> add_bos\n\n* complement\n\n* Fix wrong eos_id and bos_id obtained through grpc api (#644)\n\n* Fix wrong eos_id and bos_id obtained through grpc api\n\n* fix according to review comments\n\n* update\n\n* Optimize for throughput (#701)\n\n* tmp\n\n* update\n\n* update\n\n* optimize for throughput\n\n* update\n\n* fix eos\n\n* clean up\n\n* fix serving\n\n* fix indexed copy\n\n* minor\n\n* minor\n\n---------\n\nCo-authored-by: lvhan028 <lvhan_028@163.com>\n\n* Check-in user guide about turbomind config (#680)\n\n* update\n\n* update config guide\n\n* update guide\n\n* upate user guide according to review comments\n\n* Replace mmengine with mmengine-lite (#715)\n\n* Support loading hf model directly (#685)\n\n* turbomind support export model params\n\n* fix overflow\n\n* support turbomind.from_pretrained\n\n* fix tp\n\n* support AutoModel\n\n* support load kv qparams\n\n* update auto_awq\n\n* udpate docstring\n\n* export lmdeploy version\n\n* update doc\n\n* remove download_hf_repo\n\n* LmdeployForCausalLM -> LmdeployForCausalLM\n\n* refactor turbomind.py\n\n* update comment\n\n* add bfloat16 convert back\n\n* support gradio run_locl load hf\n\n* support resuful api server load hf\n\n* add docs\n\n* support loading previous quantized model\n\n* adapt pr 690\n\n* udpate docs\n\n* not export turbomind config when quantize a model\n\n* check model_name when can not get it from config.json\n\n* update readme\n\n* remove model_name in auto_awq\n\n* update\n\n* update\n\n* udpate\n\n* fix build\n\n* absolute import\n\n* Fix cache/output length calculation (#738)\n\n* bump version to v0.1.0a0 (#709)\n\n* [Fix] Skip empty batch (#747)\n\n* [Fix] build docker image failed since `packaging` is missing (#753)\n\n* [Fix] Rollback the data type of input_ids to TYPE_UINT32 in preprocessor's proto (#758)\n\n* Set the default value of `max_context_token_num` 1 (#761)\n\n* rename pytorch poc\n\n* fix lint\n\n* add docstring\n\n* add docstring\n\n* refactor patch\n\n* add recompute eviction support\n\n* fix typo (#769)\n\n* add triton server test and workflow yml (#760)\n\n* add triton server test and workflow yml\n\n* update\n\n* revert changes in dockerfile\n\n* update prompts\n\n* recovery modeling\n\n* fix turbomind build on sm<80 (#754)\n\n* fix\n\n* fix lint\n\n* improvement(build): enable ninja and gold linker (#767)\n\n* feat(build): enable ninja and lld\n\n* fix(.github): add ninja installation\n\n* fix(CI): remove dimsize=256\n\n* fix(CI): add option for generate.sh\n\n* fix(docs): update\n\n* Report first-token-latency and token-latency percentiles (#736)\n\n* update profile scripts\n\n* add top_p, top_k and temperature as input arguments\n\n* fix input_ids\n\n* update profile_throughput\n\n* update profile_restful_api\n\n* update profile_serving\n\n* update\n\n* update\n\n* add progress bar\n\n* remove TODO comments\n\n* update\n\n* remove useless profile_* argument\n\n* remove log level\n\n* change concurrency default value to 64\n\n* update restful_api.md\n\n* update according to review comments\n\n* fix docstring\n\n* convert model with hf repo_id (#774)\n\n* bump version to 0.1.0a1 (#776)\n\n* Update benchmark user guide (#763)\n\n* user guide of benchmark generation\n\n* update benchmark generation guide\n\n* update profiling throughput guide\n\n* update profiling api_server guide\n\n* rename file names\n\n* update profile tis user guide\n\n* update\n\n* fix according to review comments\n\n* update\n\n* update according to review comments\n\n* updaste\n\n* add an example\n\n* update\n\n* add docstring\n\n* add unified paging attention support\n\n* refactor block manager\n\n* do not alloc zero\n\n* Fix early exit condition in attention kernel (#788)\n\n* add chat template for Yi (#779)\n\n* Fix missed arguments when benchmark static inference performance (#787)\n\n* minor fix in the profile scripts and docs\n\n* miss arguments\n\n* typo\n\n* fix lint\n\n* update\n\n* Unify prefill & decode passes (#775)\n\n* Unify prefill and decode passes\n\n* dynamic split-fuse\n\n* refactor\n\n* correct input count calculation\n\n* remove unused\n\n* lint\n\n* lint\n\n* fix msvc build\n\n* fix msvc build\n\n* fix msvc build\n\n* fix msvc build\n\n* fix msvc build\n\n* fix msvc build\n\n* fix msvc build\n\n* fix msvc build\n\n* fix msvc build\n\n* add cuda12.1 build check ci (#782)\n\n* update cuda12.1 build check ci\n\n* use matrix\n\n* auto upload cuda12.1 python pkg to release when create new tag (#784)\n\n* add cuda12-whl-release ci\n\n* enable environment\n\n* test py310-311 windows wheel\n\n* fix py310, py311 setup.py error on windows\n\n* fix lint\n\n* fix extra colon in InternLMChat7B (#796)\n\n* fix local kv head num (#806)\n\n* Report the inference benchmark of models with different size (#794)\n\n* update test scripts for models with different sizes\n\n* update\n\n* only test after tunning gemm\n\n* chmod +x\n\n* fix typo\n\n* benchmark on a100\n\n* fix typo\n\n* fix typo\n\n* per-token latency percentile in profile_throughput\n\n* fix\n\n* fix\n\n* rename\n\n* make the script accept parameters\n\n* minor fix\n\n* indent\n\n* reformat table\n\n* change to 3000\n\n* minor fix\n\n* bump version to v0.1.0a2 (#807)\n\n* fix out of bounds access (#809)\n\n* update scheduler\n\n* optimize request\n\n* Simplify block manager (#812)\n\n* simplify block manager\n\n* fix lint\n\n* set smem size for repetition penalty kernel (#818)\n\n* add mbgemm&mbgemv\n\n* fix recompute, fix mbgmm\n\n---------\n\nCo-authored-by: Lyu Han <lvhan_028@163.com>\nCo-authored-by: AllentDan <41138331+AllentDan@users.noreply.github.com>\nCo-authored-by: pppppM <67539920+pppppM@users.noreply.github.com>\nCo-authored-by: Chen Xin <irexyc@gmail.com>\nCo-authored-by: RunningLeon <mnsheng@yeah.net>\nCo-authored-by: Yam(\u957f\u7434) <haoshaochun@gmail.com>\nCo-authored-by: liukuikun <24622904+Harold-lkk@users.noreply.github.com>\nCo-authored-by: yunzhongyan0 <549713537@qq.com>\nCo-authored-by: honglei.yan <honglei.yan@nio.com>\nCo-authored-by: AllentDan <AllentDan@yeah.net>\nCo-authored-by: aisensiy <aisensiy@163.com>\nCo-authored-by: Li Zhang <lzhang329@gmail.com>\nCo-authored-by: whcao <41630003+HIT-cwh@users.noreply.github.com>\nCo-authored-by: Zaida Zhou <58739961+zhouzaida@users.noreply.github.com>\nCo-authored-by: tpoisonooo <khj.application@aliyun.com>\nCo-authored-by: Qian Zhao <112053249+C1rN09@users.noreply.github.com>\n\n* [Fix] Adapt to the pyTorch poc branch (#863)\n\n* Adapt to the pyTorch poc branch\n\n* Adapt to the pyTorch poc branch\n\n* fix comments\n\n* update model\n\n* wip\n\n* wrong implementation\n\n* s-lora single gpu\n\n* refactor tp patch\n\n* add tp support\n\n* add tp gather\n\n* recover profile generation\n\n* daemon process\n\n* inplace gather\n\n* hf style\n\n* add assert when input nothing\n\n* find available port\n\n---------\n\nCo-authored-by: grimoire <yaoqian@pjlab.org.cn>\nCo-authored-by: WRH <12756472+wangruohui@users.noreply.github.com>\nCo-authored-by: AllentDan <AllentDan@yeah.net>\nCo-authored-by: AllentDan <41138331+AllentDan@users.noreply.github.com>\nCo-authored-by: tpoisonooo <khj.application@aliyun.com>\nCo-authored-by: whcao <41630003+HIT-cwh@users.noreply.github.com>\nCo-authored-by: Lyu Han <lvhan_028@163.com>\nCo-authored-by: pppppM <67539920+pppppM@users.noreply.github.com>\nCo-authored-by: Chen Xin <irexyc@gmail.com>\nCo-authored-by: RunningLeon <mnsheng@yeah.net>\nCo-authored-by: Yam(\u957f\u7434) <haoshaochun@gmail.com>\nCo-authored-by: liukuikun <24622904+Harold-lkk@users.noreply.github.com>\nCo-authored-by: yunzhongyan0 <549713537@qq.com>\nCo-authored-by: honglei.yan <honglei.yan@nio.com>\nCo-authored-by: aisensiy <aisensiy@163.com>\nCo-authored-by: Li Zhang <lzhang329@gmail.com>\nCo-authored-by: Zaida Zhou <58739961+zhouzaida@users.noreply.github.com>\nCo-authored-by: Qian Zhao <112053249+C1rN09@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2024-01-18T16:05:54Z",
        "message": "support left padding and prefix post-processing for models like chatglm"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2024-01-03T18:22:30Z",
        "message": "merge"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-12-24T14:04:00Z",
        "message": "Add --save_references_path"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-11-16T14:29:41Z",
        "message": "better naming convention"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-11-15T18:46:22Z",
        "message": "add defaults to parallel_generations"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-11-14T20:24:24Z",
        "message": "pass intermediate_generations as kwarg"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-11-09T18:44:20Z",
        "message": "add intermediate generations to continue generating from"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-11-09T18:01:58Z",
        "message": "save intermediate code generations"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-11-09T16:06:27Z",
        "message": "save gen and ref per task"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-11-09T14:54:03Z",
        "message": "save intermediate res"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-10-24T15:49:06Z",
        "message": "rename lm_eval => bigcode_eval"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-09-12T12:41:35Z",
        "message": "Merge pull request #133 from bigcode-project/wizardcoder_codellama_based\n\nAdd  WizardCoder models (that are CodeLLama based) evaluation"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-09-11T08:01:58Z",
        "message": "allow auto for max_memory_per_gpu"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-09-01T08:39:53Z",
        "message": "fix bos token"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-08-24T09:38:39Z",
        "message": "remove parentheses"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-08-24T09:37:35Z",
        "message": "print precision in correct setting"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-08-16T14:05:24Z",
        "message": "Fix str concat"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-08-08T07:42:53Z",
        "message": "Add import & use acc.processes"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-08-08T07:39:25Z",
        "message": "Use warnings\n\nCo-authored-by: Loubna Ben Allal <44069155+loubnabnl@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-08-07T17:01:38Z",
        "message": "Add warnings"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-08-07T16:09:10Z",
        "message": "Merge main"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-08-07T16:02:27Z",
        "message": "Apply suggestions from code review\n\nCo-authored-by: Loubna Ben Allal <44069155+loubnabnl@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-07-29T17:48:27Z",
        "message": "Move to try except"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-07-28T20:07:46Z",
        "message": "Update main.py\n\nCo-authored-by: Loubna Ben Allal <44069155+loubnabnl@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-07-27T12:43:42Z",
        "message": "Allow no pad token"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-07-26T19:54:24Z",
        "message": "Rename mutate_method to prompt"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-07-26T18:30:31Z",
        "message": "Merge main"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-07-10T18:39:45Z",
        "message": "merghe"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-07-10T18:36:13Z",
        "message": "attempt at loading peft model"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-06-23T12:35:55Z",
        "message": "Add limit start"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-06-18T13:05:55Z",
        "message": "Simplify"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-06-18T10:51:50Z",
        "message": "Add"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-06-14T15:28:29Z",
        "message": "add support for 4bit"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-06-14T12:42:48Z",
        "message": "add load_in_8bit arg"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-06-13T08:57:09Z",
        "message": "reformatting"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-06-12T09:29:41Z",
        "message": "further modification"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-06-12T08:54:40Z",
        "message": "Fix encdec; Fix carper none"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-06-09T07:17:45Z",
        "message": "Add max mem"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-06-06T12:15:47Z",
        "message": "Add modeltype"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-06-05T19:48:43Z",
        "message": "Merge"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-05-31T01:35:35Z",
        "message": "enforce tokenizer padding_side=\"right\""
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-05-07T07:03:59Z",
        "message": "Fix kwarg"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-05-07T06:55:51Z",
        "message": "Merge main"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-05-03T07:16:15Z",
        "message": "add trust_remote_code to tokenizer loading\n\nsome tokenizers require running local code hence the need for trust_remote_code argument"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-04-22T16:34:23Z",
        "message": "reformat code"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-04-22T16:33:03Z",
        "message": "fix save generation path arg"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-04-21T17:14:53Z",
        "message": "Merge branch 'main' into rename-path-arguments"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-04-21T17:11:55Z",
        "message": "add temp and nsamples to metrics file"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-04-21T15:01:50Z",
        "message": "use torch dtype to set precision instead of converting model"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-04-21T11:30:25Z",
        "message": "Update main.py\n\nCo-authored-by: Loubna Ben Allal <44069155+loubnabnl@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-04-21T11:25:20Z",
        "message": "Reword\n\nCo-authored-by: Loubna Ben Allal <44069155+loubnabnl@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-04-21T09:08:44Z",
        "message": "update help message in  precision arg"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-04-20T20:37:00Z",
        "message": "remove model from accelerate prepare and add precision"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-04-06T07:29:08Z",
        "message": "Fix dtype"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-04-06T06:53:57Z",
        "message": "Add import"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-04-06T06:49:09Z",
        "message": "A"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-04-06T06:48:05Z",
        "message": "Enable multiple GPUs"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-04-05T21:00:36Z",
        "message": "Enforce right padding"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-04-03T22:05:01Z",
        "message": "Fix race condition"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-04-03T16:22:11Z",
        "message": "Revert the commit."
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-04-03T13:25:16Z",
        "message": "add custom save generations path"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-04-03T13:14:12Z",
        "message": "rename output path for metrics"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-04-03T13:00:02Z",
        "message": "rename the generations_path arg for loading generations"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-04-02T15:05:07Z",
        "message": "Merge branch 'parity' of https://github.com/bigcode-project/bigcode-evaluation-harness into parity"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-04-02T12:28:04Z",
        "message": "Refactor main.py\n\nAssign the default file location to args.generations_path instead of generations_path."
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-04-02T11:39:39Z",
        "message": "Update main.py\n\nFix a little bug in generations_path. Handle the case where the user would like to generate only when the generationed file is already here."
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-04-02T11:32:53Z",
        "message": "Handle the existing file case when add --generation-only.\nFix a little issue on generation path."
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-03-31T14:36:10Z",
        "message": "Add ref checking"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-03-25T15:16:52Z",
        "message": "Add args to config"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-03-24T22:07:01Z",
        "message": "Fix path"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-03-24T21:50:40Z",
        "message": "Gen path compat"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-03-24T21:14:43Z",
        "message": "Allow gen path"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-03-24T19:18:31Z",
        "message": "Edit compat for HE"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-01-30T11:05:27Z",
        "message": "change format of boolean arguments"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-01-29T16:26:05Z",
        "message": "add trust_remote_code and use_auth_token args"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2023-01-05T11:34:24Z",
        "message": "Merge pull request #20 from infinitylogesh/few_shot_codexglue_t2t\n\n[WIP] Add CodeXGLUE-text-to-text benchmark for documentation translation"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2022-12-20T17:08:27Z",
        "message": "fix typos"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2022-12-17T19:51:03Z",
        "message": "Removed redundant condition after refactoring"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2022-12-17T19:40:37Z",
        "message": "Merge branch 'main' into few_shot_codexglue_t2t"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2022-12-13T21:46:50Z",
        "message": "Add revision kwarg"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2022-12-13T11:06:19Z",
        "message": "put truncation=left in tokenizer call instead of encoding"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2022-12-05T01:47:02Z",
        "message": "add citations and reformat code"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2022-12-04T23:24:44Z",
        "message": "change default values of bs and nsamples"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2022-11-28T21:17:36Z",
        "message": "rename num_tasks argument"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2022-11-28T20:54:42Z",
        "message": "remove evaluation_only arg"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2022-11-28T20:50:44Z",
        "message": "import all_tasks firctly and swap args in docstring"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2022-11-27T06:08:09Z",
        "message": "Added task to evaluation\n\nAdded CodexGlue Text - to - text task to evaluation code. Summary of changes\n- Added task in `main.py`\n- Added task-specifc arguments in `arguments.py`\n- Added prompt formatting function in `lm_eval/prompts.py`\n- Added generation settings and post-processing in `lm_eval/utils.py`"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2022-11-02T08:48:06Z",
        "message": "remove unecessary args"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2022-10-28T09:09:42Z",
        "message": "change default model name and max_generation_length"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2022-10-24T13:27:59Z",
        "message": "add postprocess option"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2022-10-20T21:01:44Z",
        "message": "add model name to results of evaluation_only mode"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2022-10-20T16:21:34Z",
        "message": "skip saving results on generation only mode"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2022-10-20T13:03:59Z",
        "message": "update evaluation and generation separation"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2022-10-19T21:45:25Z",
        "message": "separate evaluation and generation"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2022-09-26T08:34:10Z",
        "message": "handle an edge case of number_tasks wrt num_devices"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2022-09-22T10:06:23Z",
        "message": " move max_length_generation arg"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2022-09-18T23:17:48Z",
        "message": "update code-to-text prompts"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2022-09-16T15:51:21Z",
        "message": "add concode benchmark"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2022-09-16T13:06:47Z",
        "message": "add spider benchmark"
    },
    {
        "repo_url": "github.com/bigcode-project/bigcode-evaluation-harness",
        "filepath": "main.py",
        "commit_date": "2022-09-15T15:23:47Z",
        "message": "add conala 2-shot eval"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/merge_llama2_with_chinese_lora_low_mem.py",
        "commit_date": "2023-12-27T03:18:14Z",
        "message": "Add 64k-context models (#478)\n\nFor full changes, see the latest release.\n\n---------\n\nCo-authored-by: iMountTai <2506700016@qq.com>\nCo-authored-by: Xin Yao <35353688+iMountTai@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/merge_llama2_with_chinese_lora_low_mem.py",
        "commit_date": "2023-12-11T03:49:37Z",
        "message": "Update merge_lora script (#457)\n\nFix merging bug for 1.3B models."
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/merge_llama2_with_chinese_lora_low_mem.py",
        "commit_date": "2023-08-24T11:16:09Z",
        "message": "update config after  merge llama-2-16k"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/merge_llama2_with_chinese_lora_low_mem.py",
        "commit_date": "2023-08-04T09:31:06Z",
        "message": "Update merge_llama2_with_chinese_lora_low_mem.py"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/merge_llama2_with_chinese_lora_low_mem.py",
        "commit_date": "2023-08-04T09:22:04Z",
        "message": "improve the error message"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-LLaMA-Alpaca-2",
        "filepath": "scripts/merge_llama2_with_chinese_lora_low_mem.py",
        "commit_date": "2023-07-31T06:12:17Z",
        "message": "add scripts"
    },
    {
        "repo_url": "github.com/jondurbin/airoboros",
        "filepath": "airoboros/lmoe/api.py",
        "commit_date": "2023-12-04T14:30:17Z",
        "message": "Fix misc randomization errors, typos, etc. (#33)\n\n* Fix misc randomization errors, typos\n\n* Fix typos\n\n* Remove redundant spaces\n\n* Fix OpenAI API defaults"
    },
    {
        "repo_url": "github.com/jondurbin/airoboros",
        "filepath": "airoboros/lmoe/api.py",
        "commit_date": "2023-08-29T12:03:40Z",
        "message": "LMoE updates."
    },
    {
        "repo_url": "github.com/jondurbin/airoboros",
        "filepath": "airoboros/lmoe/api.py",
        "commit_date": "2023-08-29T11:39:26Z",
        "message": "LMoE quant/dequant updates."
    },
    {
        "repo_url": "github.com/jondurbin/airoboros",
        "filepath": "airoboros/lmoe/api.py",
        "commit_date": "2023-08-28T15:34:43Z",
        "message": "Quantize -> dequantize -> merge adapters."
    },
    {
        "repo_url": "github.com/jondurbin/airoboros",
        "filepath": "airoboros/lmoe/api.py",
        "commit_date": "2023-08-27T11:50:12Z",
        "message": "embedding router by default"
    },
    {
        "repo_url": "github.com/jondurbin/airoboros",
        "filepath": "airoboros/lmoe/api.py",
        "commit_date": "2023-08-26T17:56:17Z",
        "message": "Remove early stopping."
    },
    {
        "repo_url": "github.com/jondurbin/airoboros",
        "filepath": "airoboros/lmoe/api.py",
        "commit_date": "2023-08-26T17:10:07Z",
        "message": "Split change."
    },
    {
        "repo_url": "github.com/jondurbin/airoboros",
        "filepath": "airoboros/lmoe/api.py",
        "commit_date": "2023-08-26T17:07:59Z",
        "message": "Agent-based routing."
    },
    {
        "repo_url": "github.com/jondurbin/airoboros",
        "filepath": "airoboros/lmoe/api.py",
        "commit_date": "2023-08-26T12:15:46Z",
        "message": "More performance updates, configurable embedding model."
    },
    {
        "repo_url": "github.com/jondurbin/airoboros",
        "filepath": "airoboros/lmoe/api.py",
        "commit_date": "2023-08-24T17:25:43Z",
        "message": "Flash attn server, more stopping_criteria changes, etc."
    },
    {
        "repo_url": "github.com/jondurbin/airoboros",
        "filepath": "airoboros/lmoe/api.py",
        "commit_date": "2023-08-23T18:08:57Z",
        "message": "CORS"
    },
    {
        "repo_url": "github.com/jondurbin/airoboros",
        "filepath": "airoboros/lmoe/api.py",
        "commit_date": "2023-08-23T17:09:38Z",
        "message": "Fix split."
    },
    {
        "repo_url": "github.com/jondurbin/airoboros",
        "filepath": "airoboros/lmoe/api.py",
        "commit_date": "2023-08-23T17:03:10Z",
        "message": "Fix = vs =="
    },
    {
        "repo_url": "github.com/jondurbin/airoboros",
        "filepath": "airoboros/lmoe/api.py",
        "commit_date": "2023-08-23T16:44:47Z",
        "message": "Cleanup from stopping criteria."
    },
    {
        "repo_url": "github.com/jondurbin/airoboros",
        "filepath": "airoboros/lmoe/api.py",
        "commit_date": "2023-08-23T16:38:36Z",
        "message": "More stop testing..."
    },
    {
        "repo_url": "github.com/jondurbin/airoboros",
        "filepath": "airoboros/lmoe/api.py",
        "commit_date": "2023-08-23T16:33:43Z",
        "message": "offset stop"
    },
    {
        "repo_url": "github.com/jondurbin/airoboros",
        "filepath": "airoboros/lmoe/api.py",
        "commit_date": "2023-08-23T16:32:43Z",
        "message": "Fixes."
    },
    {
        "repo_url": "github.com/jondurbin/airoboros",
        "filepath": "airoboros/lmoe/api.py",
        "commit_date": "2023-08-23T16:01:44Z",
        "message": "More lmoe fixes."
    },
    {
        "repo_url": "github.com/jondurbin/airoboros",
        "filepath": "airoboros/lmoe/api.py",
        "commit_date": "2023-08-23T15:58:47Z",
        "message": "Fix accessor."
    },
    {
        "repo_url": "github.com/jondurbin/airoboros",
        "filepath": "airoboros/lmoe/api.py",
        "commit_date": "2023-08-23T13:56:52Z",
        "message": "More lmoe updates."
    },
    {
        "repo_url": "github.com/jondurbin/airoboros",
        "filepath": "airoboros/lmoe/api.py",
        "commit_date": "2023-08-23T12:58:48Z",
        "message": "remove flash-attn, for now"
    },
    {
        "repo_url": "github.com/jondurbin/airoboros",
        "filepath": "airoboros/lmoe/api.py",
        "commit_date": "2023-08-23T12:26:18Z",
        "message": "LMoE - HF server, since vllm isn't working"
    },
    {
        "repo_url": "github.com/jondurbin/airoboros",
        "filepath": "airoboros/lmoe/api.py",
        "commit_date": "2023-08-22T12:25:31Z",
        "message": "LMoE"
    },
    {
        "repo_url": "github.com/arielnlee/Platypus",
        "filepath": "merge.py",
        "commit_date": "2023-08-14T20:33:37Z",
        "message": "update pipeline"
    },
    {
        "repo_url": "github.com/arielnlee/Platypus",
        "filepath": "merge.py",
        "commit_date": "2023-08-05T20:16:02Z",
        "message": "update scripts"
    },
    {
        "repo_url": "github.com/lyuchenyang/Macaw-LLM",
        "filepath": "run_clm_llms.py",
        "commit_date": "2023-07-01T03:46:41Z",
        "message": "Update run_clm_llms.py"
    },
    {
        "repo_url": "github.com/lyuchenyang/Macaw-LLM",
        "filepath": "run_clm_llms.py",
        "commit_date": "2023-07-01T03:44:06Z",
        "message": "Update run_clm_llms.py"
    },
    {
        "repo_url": "github.com/lyuchenyang/Macaw-LLM",
        "filepath": "run_clm_llms.py",
        "commit_date": "2023-05-31T01:57:01Z",
        "message": "Update run_clm_llms.py"
    },
    {
        "repo_url": "github.com/lyuchenyang/Macaw-LLM",
        "filepath": "run_clm_llms.py",
        "commit_date": "2023-05-31T01:37:22Z",
        "message": "Update run_clm_llms.py"
    },
    {
        "repo_url": "github.com/lyuchenyang/Macaw-LLM",
        "filepath": "run_clm_llms.py",
        "commit_date": "2023-05-31T01:25:09Z",
        "message": "Update run_clm_llms.py"
    },
    {
        "repo_url": "github.com/lyuchenyang/Macaw-LLM",
        "filepath": "run_clm_llms.py",
        "commit_date": "2023-05-31T01:21:42Z",
        "message": "Update run_clm_llms.py"
    },
    {
        "repo_url": "github.com/lyuchenyang/Macaw-LLM",
        "filepath": "run_clm_llms.py",
        "commit_date": "2023-05-30T11:43:18Z",
        "message": "Update run_clm_llms.py"
    },
    {
        "repo_url": "github.com/lyuchenyang/Macaw-LLM",
        "filepath": "run_clm_llms.py",
        "commit_date": "2023-05-30T07:45:43Z",
        "message": "Update run_clm_llms.py"
    },
    {
        "repo_url": "github.com/lyuchenyang/Macaw-LLM",
        "filepath": "run_clm_llms.py",
        "commit_date": "2023-05-26T05:39:45Z",
        "message": "Create run_clm_llms.py"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "swift/tuners/base.py",
        "commit_date": "2024-02-29T04:14:08Z",
        "message": "Support LLaMAPRO and LoRA+ (#472)"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "swift/tuners/base.py",
        "commit_date": "2024-02-22T13:26:16Z",
        "message": "support peft format (#438)"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "swift/tuners/base.py",
        "commit_date": "2024-02-19T05:34:37Z",
        "message": "fix zero3 & swift lora (#416)"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "swift/tuners/base.py",
        "commit_date": "2024-01-23T13:08:27Z",
        "message": "Update orion 14b (#341)"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "swift/tuners/base.py",
        "commit_date": "2024-01-11T04:33:27Z",
        "message": "Fix the saving behaviour of modules without state dict (#309)"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "swift/tuners/base.py",
        "commit_date": "2024-01-10T12:57:55Z",
        "message": "Fix bugs (#305)"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "swift/tuners/base.py",
        "commit_date": "2024-01-09T10:35:20Z",
        "message": "Support studio (#300)"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "swift/tuners/base.py",
        "commit_date": "2024-01-06T10:03:42Z",
        "message": "fix scedit bug (#290)"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "swift/tuners/base.py",
        "commit_date": "2024-01-06T08:15:17Z",
        "message": "Fix offload (#288)"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "swift/tuners/base.py",
        "commit_date": "2024-01-06T06:23:36Z",
        "message": "support ModuleToSave original module offloading (#282)"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "swift/tuners/base.py",
        "commit_date": "2024-01-05T07:35:50Z",
        "message": "Support offload (#281)"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "swift/tuners/base.py",
        "commit_date": "2023-12-30T07:53:08Z",
        "message": "update perf (#264)"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "swift/tuners/base.py",
        "commit_date": "2023-12-27T13:31:27Z",
        "message": "Feat/scedit (#253)\n\nCo-authored-by: zeyinzi.jzyz <zeyinzi.jzyz@alibaba-inc.com>"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "swift/tuners/base.py",
        "commit_date": "2023-12-16T09:13:46Z",
        "message": "Compatible with peft>=0.7.0 (#220)"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "swift/tuners/base.py",
        "commit_date": "2023-12-06T07:44:13Z",
        "message": "Fix compatible error (#201)"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "swift/tuners/base.py",
        "commit_date": "2023-12-06T05:41:23Z",
        "message": "Fix model saving in new format (#198)"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "swift/tuners/base.py",
        "commit_date": "2023-11-29T10:08:45Z",
        "message": "Refine LoRA to peft (#176)"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "swift/tuners/base.py",
        "commit_date": "2023-11-13T02:21:09Z",
        "message": "Add neftune (#145)"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "swift/tuners/base.py",
        "commit_date": "2023-09-25T13:54:32Z",
        "message": "Fix dtype error (#92)"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "swift/tuners/base.py",
        "commit_date": "2023-09-21T01:43:37Z",
        "message": "Replace with loralib and add unload lora interface  (#83)"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "swift/tuners/base.py",
        "commit_date": "2023-09-20T09:26:22Z",
        "message": "Add sphinx doc builder (#77)"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "swift/tuners/base.py",
        "commit_date": "2023-09-15T06:43:59Z",
        "message": "Support restuning/side/lora-embedding/qlora, etc (#48)\n\n## New algorithms:\n\n* Add bnb 4bit & 8bit and autogptq lora support\n* Add lora support for torch.nn.Embedding\n* Add sidetuner\n* Add restuner-bypass\n* Fix some bugs\n\n## New features:\n\n* llm_sft support cross-validation with model.generate\n* llm_sft support perf recording\n* All tuners support activate and deactivate\n* Add more unit tests\n* Fix some bugs"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "swift/tuners/base.py",
        "commit_date": "2023-08-02T02:16:26Z",
        "message": "First commit"
    },
    {
        "repo_url": "github.com/microsoft/Olive",
        "filepath": "olive/model/handler/pytorch.py",
        "commit_date": "2024-02-29T10:04:27Z",
        "message": "enable lint f-string check (#973)\n\n## Describe your changes\n* Enable linter check for f-string logging\n\n## Checklist before requesting a review\n- [ ] Add unit tests for this change.\n- [ ] Make sure all tests can pass.\n- [ ] Update documents if necessary.\n- [ ] Lint and apply fixes to your code by running `lintrunner -a`\n- [ ] Is this a user-facing change? If yes, give a description of this\nchange to be included in the release notes.\n\n## (Optional) Issue link"
    },
    {
        "repo_url": "github.com/microsoft/Olive",
        "filepath": "olive/model/handler/pytorch.py",
        "commit_date": "2024-02-26T09:28:31Z",
        "message": "ignore ruff PTH123 rule (#958)\n\n## Describe your changes\n\n## Checklist before requesting a review\n- [ ] Add unit tests for this change.\n- [ ] Make sure all tests can pass.\n- [ ] Update documents if necessary.\n- [ ] Lint and apply fixes to your code by running `lintrunner -a`\n- [ ] Is this a user-facing change? If yes, give a description of this\nchange to be included in the release notes.\n\n## (Optional) Issue link"
    },
    {
        "repo_url": "github.com/microsoft/Olive",
        "filepath": "olive/model/handler/pytorch.py",
        "commit_date": "2024-01-26T07:39:32Z",
        "message": "fix pylint failure caused by wrong ignore-paths pattern (#902)\n\n## Describe your changes\n\n## Checklist before requesting a review\n- [ ] Add unit tests for this change.\n- [ ] Make sure all tests can pass.\n- [ ] Update documents if necessary.\n- [x] Lint and apply fixes to your code by running `lintrunner -a`\n- [ ] Is this a user-facing change? If yes, give a description of this\nchange to be included in the release notes.\n\n## (Optional) Issue link"
    },
    {
        "repo_url": "github.com/microsoft/Olive",
        "filepath": "olive/model/handler/pytorch.py",
        "commit_date": "2023-12-20T05:53:16Z",
        "message": "Implement ONNX decoder merge (#828)\n\n## Describe your changes\n\nImplement ONNX decoder merge\n\n* Introducing logic to merge onnnx decoders\n* OnnxConversion now returns a merged ONNX model conditionally.\n\n## Checklist before requesting a review\n- [ ] Add unit tests for this change.\n- [x] Make sure all tests can pass.\n- [ ] Update documents if necessary.\n- [x] Lint and apply fixes to your code by running `lintrunner -a`\n- [ ] Is this a user-facing change? If yes, give a description of this\nchange to be included in the release notes.\n\n## (Optional) Issue link"
    },
    {
        "repo_url": "github.com/microsoft/Olive",
        "filepath": "olive/model/handler/pytorch.py",
        "commit_date": "2023-12-12T02:47:52Z",
        "message": "Fix onnx conversion of bnb quantized model, Remove static methods from `HfConfigMixin`  (#807)\n\n## Describe your changes\nThis PR fixes some issues with the conversion of 4bit quantized models\nto onnx:\n- The adapters are unloaded first so that the correct quantized moduels\ncan be found\n- A new pytorch handler is created. Previously, we were ignoring the\nadapters\n\nChanges the static method in `HfConfigMixin` to regular methods since\nthese were made static to support the above use case. But they are no\nlonger required to be static. The code is now more consistent.\n\nTest requirements updated to for transformers since lora unit tests fail\notherwise.\n\u00a0\n## Checklist before requesting a review\n- [x] Add unit tests for this change.\n- [x] Make sure all tests can pass.\n- [ ] Update documents if necessary.\n- [x] Lint and apply fixes to your code by running `lintrunner -a`\n- [ ] Is this a user-facing change? If yes, give a description of this\nchange to be included in the release notes.\n\n## (Optional) Issue link"
    },
    {
        "repo_url": "github.com/microsoft/Olive",
        "filepath": "olive/model/handler/pytorch.py",
        "commit_date": "2023-12-08T10:50:03Z",
        "message": "Add `jsonify_config_keys` to `JsonMixin` to abstract model handler json serialization (#796)"
    },
    {
        "repo_url": "github.com/microsoft/Olive",
        "filepath": "olive/model/handler/pytorch.py",
        "commit_date": "2023-12-08T07:51:33Z",
        "message": "refactoring OliveModel and change its name to OliveModelHandler. (#786)\n\n## Describe your changes\n* Rename all OliveModel by adding suffix \"Handler\". For example\nPyTorchModel -> PyTorchModelHandler.\n* Split the logic doesn't belong to model handler into separated mixins.\nHfConfigMixin, IoConfigMixin, DummyInputsMixin, etc.\n* Merge the CompositeOnnxModel and CompositePyTorchModel into one\nCompositeModelHandler.\n* Keep using the old \"OliveModel\" types\n\nBasically, all the logic is keeping same except some logic simplify\nchanges:\n* Merge the Composite ONNX and PyTorch model. \n* Simplify the composite model enumerate logic by using a single method.\n* Same for original PyTorch model to get the hugging face model\ncomponents. In the new implementation, only one get_hf_components is\nused to enumerate both name and model.\n* As a side effect the merging composite onnx model and pytorch model,\nthe original CompositePyTorch model conversion logic is in\nconversion.py, but with the new changes, the child model get and run for\neach child model will be done by the run of olive_pass.py\n* As a side effect of providing only single entry point to enumerate the\nmodel components, the insert_beam_search pass is updated by using the\nnew get model component logic, so does the olive_pass.run.\n\nFrom the time being, the following logic is not tested\n* Distributed models\n* Composite PyTorch model\n\n## Checklist before requesting a review\n- [x] Add unit tests for this change.\n- [ ] Make sure all tests can pass.\n- [x] Update documents if necessary.\n- [x] Lint and apply fixes to your code by running `lintrunner -a`\n- [ ] Is this a user-facing change? If yes, give a description of this\nchange to be included in the release notes.\n\n## (Optional) Issue link"
    },
    {
        "repo_url": "github.com/beyondguo/LLM-Tuning",
        "filepath": "predict.py",
        "commit_date": "2023-10-06T05:28:56Z",
        "message": "baichuan2, llama2"
    },
    {
        "repo_url": "github.com/beyondguo/LLM-Tuning",
        "filepath": "predict.py",
        "commit_date": "2023-09-08T06:12:44Z",
        "message": "internlm no_prompt_loss"
    },
    {
        "repo_url": "github.com/beyondguo/LLM-Tuning",
        "filepath": "predict.py",
        "commit_date": "2023-08-14T07:46:54Z",
        "message": "predict"
    },
    {
        "repo_url": "github.com/HarderThenHarder/transformers_tasks",
        "filepath": "LLM/chatglm_finetune/train_multi_gpu.py",
        "commit_date": "2023-08-02T12:16:49Z",
        "message": "add LLMs Trainer"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/freewilly.py",
        "commit_date": "2023-07-25T10:11:50Z",
        "message": "add more models + gptq mode"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/freewilly.py",
        "commit_date": "2023-07-23T14:03:48Z",
        "message": "add freewilly2"
    },
    {
        "repo_url": "github.com/Ber666/llm-reasoners",
        "filepath": "reasoners/lm/hf_model.py",
        "commit_date": "2023-09-07T11:03:32Z",
        "message": "clean code and example"
    },
    {
        "repo_url": "github.com/Ber666/llm-reasoners",
        "filepath": "reasoners/lm/hf_model.py",
        "commit_date": "2023-09-07T09:42:52Z",
        "message": "make awq optional ; update exllama submodule info"
    },
    {
        "repo_url": "github.com/Ber666/llm-reasoners",
        "filepath": "reasoners/lm/hf_model.py",
        "commit_date": "2023-08-31T20:29:51Z",
        "message": "Merge branch 'dev-hf' of https://github.com/Ber666/llm-reasoners into dev-hf"
    },
    {
        "repo_url": "github.com/Ber666/llm-reasoners",
        "filepath": "reasoners/lm/hf_model.py",
        "commit_date": "2023-08-31T20:29:45Z",
        "message": "revert awq error"
    },
    {
        "repo_url": "github.com/Ber666/llm-reasoners",
        "filepath": "reasoners/lm/hf_model.py",
        "commit_date": "2023-08-31T20:27:55Z",
        "message": "Merge branch 'main' into dev-hf"
    },
    {
        "repo_url": "github.com/Ber666/llm-reasoners",
        "filepath": "reasoners/lm/hf_model.py",
        "commit_date": "2023-08-31T03:26:21Z",
        "message": "awq"
    },
    {
        "repo_url": "github.com/Ber666/llm-reasoners",
        "filepath": "reasoners/lm/hf_model.py",
        "commit_date": "2023-08-31T03:24:08Z",
        "message": "awq revise"
    },
    {
        "repo_url": "github.com/Ber666/llm-reasoners",
        "filepath": "reasoners/lm/hf_model.py",
        "commit_date": "2023-08-30T11:22:52Z",
        "message": "update exllama and README"
    },
    {
        "repo_url": "github.com/Ber666/llm-reasoners",
        "filepath": "reasoners/lm/hf_model.py",
        "commit_date": "2023-08-23T19:33:24Z",
        "message": "fix the dependecy and setup and req.txt"
    },
    {
        "repo_url": "github.com/Ber666/llm-reasoners",
        "filepath": "reasoners/lm/hf_model.py",
        "commit_date": "2023-08-18T23:27:48Z",
        "message": "add docs"
    },
    {
        "repo_url": "github.com/Ber666/llm-reasoners",
        "filepath": "reasoners/lm/hf_model.py",
        "commit_date": "2023-08-18T23:20:57Z",
        "message": "works well for hf(bnb,awq),exllama on 30b and 70b"
    },
    {
        "repo_url": "github.com/Ber666/llm-reasoners",
        "filepath": "reasoners/lm/hf_model.py",
        "commit_date": "2023-08-16T19:51:42Z",
        "message": "blocksworld run"
    },
    {
        "repo_url": "github.com/Ber666/llm-reasoners",
        "filepath": "reasoners/lm/hf_model.py",
        "commit_date": "2023-07-14T11:25:44Z",
        "message": "update package name"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/model_converters/merge.py",
        "commit_date": "2024-01-17T02:47:15Z",
        "message": "[Fix] Add `trust_remote_code=True` for AutoModel  (#328)\n\nupdate"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/model_converters/merge.py",
        "commit_date": "2023-12-26T09:39:01Z",
        "message": "[Feature] Support LLaVA (#196)\n\n* v1\n\n* add load_image\n\n* update cfg image url\n\n* del fig\n\n* update\n\n* temp\n\n* update convert\n\n* update chat_mm\n\n* add exclude_frozen_parameters for deepspeed\n\n* update chat\n\n* update xtuner help msg\n\n* fix bugs\n\n* revert bf16 deepspeed\n\n* fix bugs\n\n* add visual_select_layer for chat\n\n* improve pth_to_hf\n\n* rename projecter_pth to pretrained_pth\n\n* temp\n\n* update requirements\n\n* add cfgs\n\n* update\n\n* fix pre-commit\n\n* optim chat\n\n* optim chat\n\n* Delete xtuner/model/unused.py\n\n* move dispatch to a deeper folder\n\n* add projector\n\n* update\n\n* del model/projector\n\n* fix bugs\n\n* add docs\n\n* update\n\n* update\n\n* update\n\n* update\n\n* enhance resume for map_fn\n\n* update import\n\n* add llava_internlm_chat_7b_clip_vit_large_p14\n\n* update dispatch\n\n* update dispatch\n\n* add link\n\n* update max_length\n\n* update max_length\n\n* update hyp\n\n* align\n\n* move yi flash attn\n\n* fix pre-commit\n\n* update deepspeed requirements\n\n* add mmbench script\n\n* install openpyxl\n\n* add entry_point for mmbench\n\n* save args\n\n* update mmbench\n\n* update max_length\n\n* add llama2 qlora\n\n* update mmbench\n\n* fix mmbench bugs\n\n* use osp instead of os.path\n\n* refactor pth_to_hf\n\n* update chat and mmbench to support --llava\n\n* align to chat\n\n* update entry_point\n\n* add vicuna template\n\n* add vicuna_7b_v15\n\n* fix pre-commit\n\n* add vicuna_7b_v1.5 qlora\n\n* skip_special_tokens for decode text\n\n* remove do_sample\n\n* add warmup\n\n* fix pre-commit\n\n* Update dataset_prepare.md\n\n* Update dataset_prepare.md\n\n* Add KEEP_STSTEM for template\n\n* remove\n\n* fix vicuna template\n\n* clean cfgs\n\n* add cfgs\n\n* fix pre-commit\n\n* add --language for mmbench\n\n* fix bugs\n\n* fix pretrain bug\n\n* support visual_encoder lora\n\n* fix bugs\n\n* add paramwise_cfg\n\n* remove print_peft_model_trainable_parameters\n\n* fix bugs\n\n* add paramwise_cfg for DeepSpeedOptimWrapper\n\n* fix engine deepspeed paramwise_cfg bug\n\n* fix encode_fn bug\n\n* fix\n\n* fix pad_image_to_square bugs\n\n* Add space for system to avoid mismatch of 'USER' token\n\n* revert to adding bos_token at each conv\n\n* revert for paramwise_cfg\n\n* better cfgs?\n\n* fix import bug\n\n* fix import bug\n\n* pretrain align\n\n* update prepare_inputs_labels_for_multimodal\n\n* 1792\n\n* support length_grouped_samplers\n\n* 1792\n\n* remove KEEP_SYSTEM\n\n* remove system in cfg\n\n* update 336 cfg\n\n* add torch_dtype for mmbench and chat\n\n* group 50\n\n* quant for pretrain\n\n* update cfgs\n\n* refactor cfgs\n\n* add length for concat dataset\n\n* update requirements\n\n* fix typo\n\n* add template for internlm pretrain\n\n* no zh\n\n* remove 20b cfgs\n\n* fix pre-commit\n\n* revert invalid input\n\n* rename\n\n* Update README.md\n\n* Update README_zh-CN.md\n\n* fix pre-commit\n\n* remove llava_zh from docs\n\n* qlora 512\n\n* rename llava map_fn\n\n* update cfgs\n\n* update model urls\n\n* add docs link\n\n* add llava docs\n\n* update docs\n\n* update urls\n\n* add citation\n\n* fix README\n\n* move\n\n* update\n\n* vicuna pretrain with prompt\n\n* rename\n\n* add results\n\n* fix pre-commit\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* Update README.md\n\n* Update README_zh-CN.md\n\n* Update README_zh.md\n\n* Update README_zh.md\n\n* Update README.md\n\n* Update README_zh.md\n\n* Update README.md\n\n* Update README.md\n\n* fix typo\n\n* fix\n\n* Update README.md\n\n* Update README_zh-CN.md\n\n* rename\n\n* auto cn_string\n\n* fix pre-commit\n\n* rename\n\n* remove language\n\n* add VLMEvalKit\n\n* rename VLLM to VLM\n\n* add the download links of MMBench\n\n* update\n\n* update readme\n\n* update\n\n* update\n\n* update merge\n\n* fix cfg bug\n\n* Update README.md\n\n* Update README_zh.md\n\n* update\n\n* fix\n\n* update requirements\n\n* Update runtime.txt\n\n* Update runtime.txt\n\n* Update runtime.txt\n\n* Update README.md\n\n* Update README.md\n\n* Update README_zh.md\n\n* fix pre-commit\n\n* fix\n\n* update mmbench prompt\n\n* fix bugs\n\n* fix bugs\n\n* update docs\n\n* update\n\n* update\n\n* Update README.md"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/model_converters/merge.py",
        "commit_date": "2023-11-14T12:48:02Z",
        "message": "[Feature] Support ChatGLM3-6B (#222)\n\n* add chatglm cfgs\n\n* add chatglm3 template\n\n* fix bos_token_id bug\n\n* add encode_special_tokens\n\n* update readme"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/model_converters/merge.py",
        "commit_date": "2023-09-27T06:48:07Z",
        "message": "[Fix] Add `--offload-folder` for merge and chat (#140)\n\nadd `--offload-folder` for merge and chat"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/model_converters/merge.py",
        "commit_date": "2023-09-22T05:04:53Z",
        "message": "[Fix] Fix CPU OOM during the merge step (#133)\n\nfix"
    },
    {
        "repo_url": "github.com/InternLM/xtuner",
        "filepath": "xtuner/tools/model_converters/merge.py",
        "commit_date": "2023-09-05T08:08:28Z",
        "message": "[Improve] Redesign convert tools (#96)\n\n* refactor tools\n\n* modify entry_point\n\n* modify docs\n\n* update docs\n\n* fix\n\n* fix\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Update README_zh-CN.md\n\n* fix pre-commit\n\n* rename converter\n\n* update pth2hf\n\n* rename pth2hf to pth_to_hf\n\n* add fp32 for pth_to_hf\n\n* Update README.md\n\n* Update README_zh-CN.md\n\n* Update README_zh-CN.md\n\n* Update README.md\n\n* Update README_zh-CN.md\n\n* Update README_zh-CN.md\n\n* Update README.md\n\n* Update README.md\n\n* Update README_zh-CN.md\n\n* fix pre-commit"
    },
    {
        "repo_url": "github.com/haonan-li/CMMLU",
        "filepath": "src/glm.py",
        "commit_date": "2023-06-16T12:48:28Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/predibase/lorax",
        "filepath": "server/lorax_server/server.py",
        "commit_date": "2024-02-28T01:00:29Z",
        "message": "Generate to `max_total_tokens` during warmup (#286)"
    },
    {
        "repo_url": "github.com/predibase/lorax",
        "filepath": "server/lorax_server/server.py",
        "commit_date": "2024-02-17T06:15:54Z",
        "message": "Fix concatenate for flash batch (#254)"
    },
    {
        "repo_url": "github.com/predibase/lorax",
        "filepath": "server/lorax_server/server.py",
        "commit_date": "2024-02-12T23:38:08Z",
        "message": "Added Outlines logits processor for JSON schema validation (#224)\n\nCo-authored-by: Jeffrey Tang <jeff@predibase.com>"
    },
    {
        "repo_url": "github.com/predibase/lorax",
        "filepath": "server/lorax_server/server.py",
        "commit_date": "2024-02-01T05:39:34Z",
        "message": "Merge multiple LoRA adapters per request (linear, TIES, DARE) (#212)"
    },
    {
        "repo_url": "github.com/predibase/lorax",
        "filepath": "server/lorax_server/server.py",
        "commit_date": "2024-01-17T20:45:09Z",
        "message": "Added pbase adapter_source and expose api_token in client (#181)"
    },
    {
        "repo_url": "github.com/predibase/lorax",
        "filepath": "server/lorax_server/server.py",
        "commit_date": "2024-01-10T18:35:46Z",
        "message": "OpenAI v1 Chat Completions API (#171)"
    },
    {
        "repo_url": "github.com/predibase/lorax",
        "filepath": "server/lorax_server/server.py",
        "commit_date": "2024-01-04T16:40:08Z",
        "message": "CUDA graph compilation (#154)"
    },
    {
        "repo_url": "github.com/predibase/lorax",
        "filepath": "server/lorax_server/server.py",
        "commit_date": "2023-12-14T23:55:51Z",
        "message": "Add predibase as a source for adapters (#125)"
    },
    {
        "repo_url": "github.com/predibase/lorax",
        "filepath": "server/lorax_server/server.py",
        "commit_date": "2023-12-05T20:48:43Z",
        "message": "fix gptq fp16 inference (#104)"
    },
    {
        "repo_url": "github.com/predibase/lorax",
        "filepath": "server/lorax_server/server.py",
        "commit_date": "2023-12-01T17:26:29Z",
        "message": "doing some documentation stuff (#91)"
    },
    {
        "repo_url": "github.com/predibase/lorax",
        "filepath": "server/lorax_server/server.py",
        "commit_date": "2023-11-16T01:20:58Z",
        "message": "Rename tgi and text-generation to lorax in rust (#19)"
    },
    {
        "repo_url": "github.com/predibase/lorax",
        "filepath": "server/lorax_server/server.py",
        "commit_date": "2023-11-15T22:41:32Z",
        "message": "Rename to lorax (#16)"
    },
    {
        "repo_url": "github.com/AetherCortex/Llama-X",
        "filepath": "src/generate.py",
        "commit_date": "2023-03-30T11:58:42Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/llama_rlhf.py",
        "commit_date": "2023-07-25T10:11:50Z",
        "message": "add more models + gptq mode"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/llama_rlhf.py",
        "commit_date": "2023-07-01T17:56:43Z",
        "message": "add local files only option"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/llama_rlhf.py",
        "commit_date": "2023-06-08T01:59:11Z",
        "message": "exp w/ camel and sam"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/llama_rlhf.py",
        "commit_date": "2023-06-04T14:50:44Z",
        "message": "disable force download & example view"
    },
    {
        "repo_url": "github.com/deep-diver/LLM-As-Chatbot",
        "filepath": "models/llama_rlhf.py",
        "commit_date": "2023-04-07T05:57:29Z",
        "message": "remaining works flushed"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "merge_peft_adapter.py",
        "commit_date": "2023-11-16T06:16:27Z",
        "message": "update merge for reward."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "merge_peft_adapter.py",
        "commit_date": "2023-11-13T02:32:40Z",
        "message": "update merge peft adapter."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "merge_peft_adapter.py",
        "commit_date": "2023-11-10T15:38:29Z",
        "message": "update merge peft adapter."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "merge_peft_adapter.py",
        "commit_date": "2023-11-10T15:37:47Z",
        "message": "update merge peft adapter."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "merge_peft_adapter.py",
        "commit_date": "2023-11-10T15:36:30Z",
        "message": "update merge peft adapter."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "merge_peft_adapter.py",
        "commit_date": "2023-11-10T15:30:14Z",
        "message": "update merge peft adapter."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "merge_peft_adapter.py",
        "commit_date": "2023-11-05T06:27:51Z",
        "message": "update merge save model."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "merge_peft_adapter.py",
        "commit_date": "2023-11-05T06:24:43Z",
        "message": "update merge tokenizer."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "merge_peft_adapter.py",
        "commit_date": "2023-11-03T04:40:10Z",
        "message": "update merge adapter."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "merge_peft_adapter.py",
        "commit_date": "2023-08-01T10:22:31Z",
        "message": "update reward num labels."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "merge_peft_adapter.py",
        "commit_date": "2023-07-21T11:09:37Z",
        "message": "update multi-round demo."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "merge_peft_adapter.py",
        "commit_date": "2023-07-14T02:08:07Z",
        "message": "update merged model"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "merge_peft_adapter.py",
        "commit_date": "2023-07-12T13:03:36Z",
        "message": "update merge script"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "merge_peft_adapter.py",
        "commit_date": "2023-06-16T02:57:11Z",
        "message": "update auto model."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "merge_peft_adapter.py",
        "commit_date": "2023-06-15T10:17:15Z",
        "message": "support baichuan-7b."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "merge_peft_adapter.py",
        "commit_date": "2023-06-14T09:34:37Z",
        "message": "remove folder."
    },
    {
        "repo_url": "github.com/michael-wzhu/Chinese-LlaMA2",
        "filepath": "peft/peft_model.py",
        "commit_date": "2023-07-19T06:57:59Z",
        "message": "2023/07/19 add further pretraining code"
    },
    {
        "repo_url": "github.com/haonan-li/CMMLU",
        "filepath": "src/qwen.py",
        "commit_date": "2023-08-13T11:55:12Z",
        "message": "add Qwen-7B script and results"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "fastapi_server_demo.py",
        "commit_date": "2024-01-10T02:49:34Z",
        "message": "update fastapi demo."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "fastapi_server_demo.py",
        "commit_date": "2023-11-23T13:31:46Z",
        "message": "update fastapi."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "fastapi_server_demo.py",
        "commit_date": "2023-11-20T08:36:52Z",
        "message": "update fastapi."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "fastapi_server_demo.py",
        "commit_date": "2023-11-15T07:05:20Z",
        "message": "update infer use greedy decoding."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "fastapi_server_demo.py",
        "commit_date": "2023-10-23T06:57:54Z",
        "message": "update batch infer."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "fastapi_server_demo.py",
        "commit_date": "2023-10-19T06:37:37Z",
        "message": "add fastapi server api demo."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "fastapi_server_demo.py",
        "commit_date": "2023-10-19T06:36:33Z",
        "message": "add fastapi server api demo."
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "commonsense_evaluate.py",
        "commit_date": "2023-05-16T16:26:36Z",
        "message": "update commonsense_evaluate.py"
    },
    {
        "repo_url": "github.com/27182812/ChatGLM-LLaMA-chinese-insturct",
        "filepath": "generate_llama1.py",
        "commit_date": "2023-03-28T03:36:48Z",
        "message": "add llama"
    },
    {
        "repo_url": "github.com/Clouditera/secgpt",
        "filepath": "train_dpo.py",
        "commit_date": "2023-12-07T04:08:07Z",
        "message": "dpo ref\u6a21\u578b\u52a0\u5165"
    },
    {
        "repo_url": "github.com/Clouditera/secgpt",
        "filepath": "train_dpo.py",
        "commit_date": "2023-12-07T02:43:41Z",
        "message": "\u8bad\u7ec3\u811a\u672c\u5c0f\u4f18\u5316"
    },
    {
        "repo_url": "github.com/Clouditera/secgpt",
        "filepath": "train_dpo.py",
        "commit_date": "2023-12-07T02:33:25Z",
        "message": "\u5f00\u6e90DPO\u8bad\u7ec3\u811a\u672c"
    },
    {
        "repo_url": "github.com/Clouditera/secgpt",
        "filepath": "train_dpo.py",
        "commit_date": "2023-12-06T07:18:45Z",
        "message": "\u5f00\u6e90DPO\u8bad\u7ec3\u811a\u672c"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-03-29T13:37:38Z",
        "message": "initial upload"
    },
    {
        "repo_url": "github.com/jquesnelle/yarn",
        "filepath": "eval/model_loader.py",
        "commit_date": "2023-11-02T15:33:05Z",
        "message": "mistral and v2 of paper"
    },
    {
        "repo_url": "github.com/jquesnelle/yarn",
        "filepath": "eval/model_loader.py",
        "commit_date": "2023-08-31T17:53:20Z",
        "message": "publish YaRN"
    },
    {
        "repo_url": "github.com/haonan-li/CMMLU",
        "filepath": "src/chatglm.py",
        "commit_date": "2023-07-28T16:24:52Z",
        "message": "update ABCD token mapping"
    },
    {
        "repo_url": "github.com/haonan-li/CMMLU",
        "filepath": "src/chatglm.py",
        "commit_date": "2023-06-28T11:13:12Z",
        "message": "Update ChatGLM2 results"
    },
    {
        "repo_url": "github.com/haonan-li/CMMLU",
        "filepath": "src/chatglm.py",
        "commit_date": "2023-06-16T12:48:28Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/zjunlp/KnowLM",
        "filepath": "tools/weight_diff.py",
        "commit_date": "2023-08-06T03:17:08Z",
        "message": "Update weight_diff.py"
    },
    {
        "repo_url": "github.com/zjunlp/KnowLM",
        "filepath": "tools/weight_diff.py",
        "commit_date": "2023-07-04T09:39:49Z",
        "message": "Update weight_diff.py"
    },
    {
        "repo_url": "github.com/zjunlp/KnowLM",
        "filepath": "tools/weight_diff.py",
        "commit_date": "2023-06-19T08:28:04Z",
        "message": "Adding merge function on weight_diff.py\n\nProviding a merge lora function. This will output a hf format model which can be used for further lora stf or full-finetuning"
    },
    {
        "repo_url": "github.com/zjunlp/KnowLM",
        "filepath": "tools/weight_diff.py",
        "commit_date": "2023-06-17T11:38:03Z",
        "message": "Update weight_diff.py"
    },
    {
        "repo_url": "github.com/zjunlp/KnowLM",
        "filepath": "tools/weight_diff.py",
        "commit_date": "2023-06-17T11:30:47Z",
        "message": "Update weight_diff.py"
    },
    {
        "repo_url": "github.com/zjunlp/KnowLM",
        "filepath": "tools/weight_diff.py",
        "commit_date": "2023-05-24T15:16:00Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/dvlab-research/LISA",
        "filepath": "model/llava/model/builder.py",
        "commit_date": "2023-08-23T13:49:06Z",
        "message": "Refactor the code to support hf model module format & support grefcoco dataset"
    },
    {
        "repo_url": "github.com/SCIR-HI/Huatuo-Llama-Med-Chinese",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-04-01T09:37:49Z",
        "message": "init code"
    },
    {
        "repo_url": "github.com/yeyupiaoling/Whisper-Finetune",
        "filepath": "merge_lora.py",
        "commit_date": "2023-11-16T11:36:55Z",
        "message": "\u4f18\u5316hf\u63a8\u7406"
    },
    {
        "repo_url": "github.com/yeyupiaoling/Whisper-Finetune",
        "filepath": "merge_lora.py",
        "commit_date": "2023-09-28T14:51:22Z",
        "message": "\u66f4\u6587\u6863\u548c\u4fee\u590d\u8bc4\u4f30"
    },
    {
        "repo_url": "github.com/yeyupiaoling/Whisper-Finetune",
        "filepath": "merge_lora.py",
        "commit_date": "2023-07-02T05:14:38Z",
        "message": "\u66f4\u65b0\u6587\u6863"
    },
    {
        "repo_url": "github.com/yeyupiaoling/Whisper-Finetune",
        "filepath": "merge_lora.py",
        "commit_date": "2023-06-01T15:06:29Z",
        "message": "\u4fee\u6539\u9ed8\u8ba4\u53c2\u6570"
    },
    {
        "repo_url": "github.com/yeyupiaoling/Whisper-Finetune",
        "filepath": "merge_lora.py",
        "commit_date": "2023-05-29T14:41:39Z",
        "message": "\u8bad\u7ec3\u65f6\u4e0d\u9002\u5408\u9884\u6d4b\u8bc4\u4f30"
    },
    {
        "repo_url": "github.com/yeyupiaoling/Whisper-Finetune",
        "filepath": "merge_lora.py",
        "commit_date": "2023-05-20T05:36:50Z",
        "message": "\u589e\u52a0\u6307\u5b9a\u591a\u8bed\u8a00"
    },
    {
        "repo_url": "github.com/yeyupiaoling/Whisper-Finetune",
        "filepath": "merge_lora.py",
        "commit_date": "2023-05-12T09:58:24Z",
        "message": "\u4fee\u590dbool\u7c7b\u578b\u53c2\u6570\u65e0\u6cd5\u8f93\u5165\uff0c\u589e\u52a0\u8bad\u7ec3\u6570\u636e\u65f6\u957f\u63a7\u5236"
    },
    {
        "repo_url": "github.com/yeyupiaoling/Whisper-Finetune",
        "filepath": "merge_lora.py",
        "commit_date": "2023-05-11T14:12:32Z",
        "message": "\u4fee\u590d\u591a\u5361\u8bad\u7ec3\u548c\u6ca1\u6709\u4fdd\u5b58tokenizer.json\u95ee\u9898"
    },
    {
        "repo_url": "github.com/yeyupiaoling/Whisper-Finetune",
        "filepath": "merge_lora.py",
        "commit_date": "2023-05-11T13:09:22Z",
        "message": "\u4fee\u590d8bit\u8bad\u7ec3\uff0c\u4f18\u5316\u8bc4\u4f30\u65b9\u5f0f\u548c\u5176\u4ed6\u4ee3\u7801"
    },
    {
        "repo_url": "github.com/yeyupiaoling/Whisper-Finetune",
        "filepath": "merge_lora.py",
        "commit_date": "2023-04-26T10:13:52Z",
        "message": "\u589e\u52a0\u6307\u5b9a\u8bed\u8a00"
    },
    {
        "repo_url": "github.com/yeyupiaoling/Whisper-Finetune",
        "filepath": "merge_lora.py",
        "commit_date": "2023-04-22T07:08:40Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/billxbf/ReWOO",
        "filepath": "alpaca/lora.py",
        "commit_date": "2023-05-19T20:52:16Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/InternLM/lmdeploy",
        "filepath": "lmdeploy/pytorch/engine/model_agent.py",
        "commit_date": "2024-03-03T14:42:41Z",
        "message": "Async torch engine (#1206)\n\n* add multinomial sampling kernel\n\n* add multinomial ut\n\n* update sampling\n\n* fix block offsets\n\n* solve conflict\n\n* not copy bw\n\n* async engine\n\n* add threadsafe\n\n* fix ut\n\n* recovery doc\n\n* fix run_until_complete\n\n* fix chat\n\n* fix chat\n\n* fix async-engine"
    },
    {
        "repo_url": "github.com/InternLM/lmdeploy",
        "filepath": "lmdeploy/pytorch/engine/model_agent.py",
        "commit_date": "2024-03-01T10:50:15Z",
        "message": "optmize chatglm3 in pytorch engine(#1215)\n\n* optmize chatglm3\n\n* update docs"
    },
    {
        "repo_url": "github.com/InternLM/lmdeploy",
        "filepath": "lmdeploy/pytorch/engine/model_agent.py",
        "commit_date": "2024-02-28T11:15:44Z",
        "message": "Fix all devices occupation when applying tp to torch engine by updating device map (#1172)"
    },
    {
        "repo_url": "github.com/InternLM/lmdeploy",
        "filepath": "lmdeploy/pytorch/engine/model_agent.py",
        "commit_date": "2024-02-23T03:12:05Z",
        "message": "Support mistral and sliding window attention (#1075)\n\n* support yi\n\n* update docs\n\n* add kernel\n\n* fix seq\n\n* add win block manager\n\n* finish mistral\n\n* fix drop block\n\n* update docs\n\n* fix ut\n\n* fix for transformers 4.37.1\n\n* update docs\n\n* mistral template\n\n* readme-zh-cn\n\n* remove print\n\n* remove div up\n\n---------\n\nCo-authored-by: grimoire <yaoqian@pjlab.org.cn>"
    },
    {
        "repo_url": "github.com/InternLM/lmdeploy",
        "filepath": "lmdeploy/pytorch/engine/model_agent.py",
        "commit_date": "2024-02-20T14:01:00Z",
        "message": "Support torch cache_max_entry_count (#1166)\n\n* max entry only\n\n* update cli\n\n---------\nCo-authored-by: RunningLeon <mnsheng@yeah.net>"
    },
    {
        "repo_url": "github.com/InternLM/lmdeploy",
        "filepath": "lmdeploy/pytorch/engine/model_agent.py",
        "commit_date": "2024-01-29T02:26:28Z",
        "message": "Fix modelconfig in pytorch engine, support YI. (#1052)\n\n* support yi\n\n* update docs\n\n---------"
    },
    {
        "repo_url": "github.com/InternLM/lmdeploy",
        "filepath": "lmdeploy/pytorch/engine/model_agent.py",
        "commit_date": "2024-01-25T06:50:03Z",
        "message": "add alignment tools (#1004)\n\n* add align tools\n\n* fix doc"
    },
    {
        "repo_url": "github.com/InternLM/lmdeploy",
        "filepath": "lmdeploy/pytorch/engine/model_agent.py",
        "commit_date": "2024-01-25T05:10:31Z",
        "message": "fix tp mem usage of PytorchEngine (#987)\n\n* fix tp mem usage\n\n* fix\n\n* check arch and auto_map"
    },
    {
        "repo_url": "github.com/InternLM/lmdeploy",
        "filepath": "lmdeploy/pytorch/engine/model_agent.py",
        "commit_date": "2024-01-22T04:07:43Z",
        "message": "fix TorchEngine stuck when benchmarking with `tp>1` (#942)\n\n* fix benchmark tp\n\n* slient warning\n\n* fix profile batch\n\n* support torch2.0\n\n* fix dtensor\n\n* fix get type dtype"
    },
    {
        "repo_url": "github.com/InternLM/lmdeploy",
        "filepath": "lmdeploy/pytorch/engine/model_agent.py",
        "commit_date": "2024-01-16T16:51:45Z",
        "message": "Support internlm2 (#963)\n\n* support internlm2\n\n* update\n\n* update chat template\n\n* add internlm2-chat-7b chat template\n\n* pass model_cfg to model reader\n\n* use config.ini as default if not pass engine config\n\n* Fix auto-awq and smooth quant\n\n* support internlm2-4bit\n\n* rollback cmakelist\n\n* Add long context docs\n\n* support internlm2 pytorch\n\n* rename args\n\n* add doc links\n\n* fix doc\n\n* update doc\n\n* update long context guide (#970)\n\n* update long context inference guide\n\n---------\n\nCo-authored-by: irexyc <irexyc@gmail.com>\nCo-authored-by: \u66f9\u5dcd\u701a <caoweihan@pjlab.org.cn>\nCo-authored-by: grimoire <streetyao@live.com>"
    },
    {
        "repo_url": "github.com/InternLM/lmdeploy",
        "filepath": "lmdeploy/pytorch/engine/model_agent.py",
        "commit_date": "2024-01-16T10:22:55Z",
        "message": "support mlp s-lora (#957)"
    },
    {
        "repo_url": "github.com/InternLM/lmdeploy",
        "filepath": "lmdeploy/pytorch/engine/model_agent.py",
        "commit_date": "2024-01-09T07:33:41Z",
        "message": "S-LoRA support (#894)\n\n* WIP\n\n* cache engine wip\n\n* finish cache engine\n\n* fix cache and scheduler\n\n* add paged attention\n\n* step and stop\n\n* add infer\n\n* add request process\n\n* fix end\n\n* request without schedulersession\n\n* add logits processor\n\n* better context\n\n* update patch\n\n* [Improve] Use 4d input in pytorch poc (#371)\n\n* 4D input, model.eval and llama config\n\n* use auto dtype\n\n* tp wip\n\n* almost\n\n* update logger\n\n* run_check=false\n\n* little optimize\n\ncurrent best\n\nredist w/o dtensor\n\nhost mem in que\n\nless rewrite\n\nless code\n\nupdate model weight\n\n* share attention forward\n\n* fix end\n\n* Support Baichuan (#382)\n\n* add baichuan WIP\n\n* support baichuan\n\n* support baichuan-13b\n\n* fix\n\n* add chat template\n\n* lint\n\n* comments\n\n* fix\n\n* Move `q_seq_info` into `context` (#398)\n\n* move q seq info into context\n\n* remove debugs\n\n* remove debugs\n\n* alibi wip\n\n* add alibi\n\n* reduce logic block (#435)\n\n* add docstring\n\n* add baichuan lint (#445)\n\n* add fill cache back\n\n* support internlm\n\n* fix path of weight index\n\n* Support chatglm2 in pytorch_poc (#360)\n\n* draft support for chatglm2\n\n* debug llama\n\n* gitignore\n\n* update input_id\n\n* better patching\n\n* patch chatglm2 model\n\n* fix after merge\n\n* remove inits\n\n* q_seq_info & remove some debug & orig_self\n\n* remove old unqeuzze inputid\n\n* update patch and model config\n\n* remove debugs and clean codes\n\n* clean codes\n\n* add credit\n\n* add update id / fix dependency\n\n* rename modules (#504)\n\nCo-authored-by: grimoire <yaoqian@pjlab.org.cn>\n\n* optimize fill kv cache (#523)\n\n* optimize fill kv cache\n\n* update internlm\n\n* faster embedding\n\n* fix bias tp\n\n* fix baichuan2\n\n* fix fill kv cache\n\n* fix lint\n\n---------\n\n* Make trust_remote_code as cli argument (#434)\n\n* trust_remote_code_argument\n\n* format\n\n* update tokenizer\n\n* optimize rotary\n\n* wtf\n\n* Support Falcon models (#406)\n\n* move q seq info into context\n\n* falcon aligned\n\n* trust_remote_code_argument\n\n* fix for falcon\n\n* comment out debugs\n\n* comment out debugs\n\n* use position id in context\n\n* remove codes in falcon model\n\n* Revert \"comment out debugs\"\n\nThis reverts commit ee26a25c36f11c7afdad315b8101ebb5c2c637fd.\n\n* 7b correct\n\n* 1b aligned\n\n* remove debugs\n\n* patch to ignore position ids\n\n* remove debug in alibi, avoid empty inputs\n\n* fix\n\n* rename dir to replace to \"models\"\n\n* use position_id and new fill kernel\n\n* remove useless get_prompt func\n\n* fix batch>2\n\n* Refactor scheduler (#551)\n\n* optimize block manager\n\n* scheduler wip\n\n* finish scheduler\n\n* update engine\n\n* profile pytorch poc (#455)\n\n* profile pytorch poc\n\n* update doc and import if need\n\n* arg\n\n* support profile_throughput.py\n\n* reuse pytorch session\n\n* end session\n\n* Support Tensor parallel on Falcon models (#582)\n\n* tp falcon 1b and 7b works\n\n* remove debugs\n\n* update copyright\n\n* add some comments\n\n* remove a debug\n\n* support new hub models\n\n* support 40b\n\n* support 40b model config\n\n* try\n\n* recover\n\n* fix remain len\n\n* Apply rotary kernel (#572)\n\n* apply rotary kernel\n\n* format\n\n* update rmsnorm\n\n* update rms norm\n\n* better unittest\n\n* add docstring\n\n---------\n\nCo-authored-by: grimoire <yaoqian@pjlab.org.cn>\n\n* fix(pytorch_poc): memory cal (#606)\n\n* fix(pytorch_poc): memory cal\n\n* Optimize attention (#597)\n\n* add unittest\n\n* add split k\n\n* add docstring\n\n* fast split k\n\n* optimize load\n\n* manually setup device and stream\n\n* lint\n\n---------\n\nCo-authored-by: grimoire <yaoqian@pjlab.org.cn>\n\n* feat(pytorch_poc): implement ReRoPE (#625)\n\n* fix(pytorch_poc): memory cal\n\n* style(pytorch_poc): lint\n\n* style(.pre-commit-config.yaml): update\n\n* style(pytorch_poc): remove useless\n\n* feat(pytorch_poc): llama2 support rerope\n\n* feat(pytorch_poc): fix long input generate\n\n* feat(lmdeploy): add kernel\n\n* feat(lmdeploy): update\n\n* feat(lmdeploy): add rerope implementation\n\n* fix(lmdeploy/pytorch_poc): apply rotary_emb\n\n* fix(lmdeploy): update\n\n* style(pytorch_poc): format\n\n* style(lmdeploy): fix lint\n\n* style(lmdeploy): typo\n\n* style(pytorch_poc): format\n\n* style(pytorch_poc): format\n\n* fix(pytorch_poc): rms_norm add mask\n\n* style(pytorch_poc/kernels): format rerope\n\n* style(pytorch_poc): format rerope attn function description\n\n* style(lmdeploy/pytorch_poc): format\n\n* style(pytorch_poc): add code ref\n\n* style(pytorch_poc): format rerope attn\n\n* Refactor engine (#623)\n\n* add agent\n\n* optimize postprocess\n\n* optimize decoding fill cache\n\n* add docstring\n\n* logit to cuda\n\n* blocksize 128\n\n* optimize pre/post process\n\n* fix postprocess\n\n* cpu pre/post process\n\n* manually setup stream and device\n\n* remove context\n\n* update model agent\n\n* update max session len\n\n* remove tqdm\n\n* update pre/post process\n\n* inplace kernel\n\n* avoid kv_len computation\n\n* flash decoding with one cache\n\n* remove comment\n\n* add warning when no enough resources\n\n* step if has unfinish\n\n* add request manager\n\n* better fill kv cache\n\n* fix fill kv cache\n\n* optimize prefill attention\n\n* refractor\n\n* refactoring...\n\n* add custom output\n\n* use cache\n\n---------\n\nCo-authored-by: grimoire <yaoqian@pjlab.org.cn>\n\n* [Feature] w8a8 based on pytorch poc (#595)\n\n* refactor smoothquant and support load w8a8 model by from_pretrained\n\n* add w8a8 docs\n\n* add w8a8 en docs\n\n* add convert_to_qmodules function\n\n---------\n\nCo-authored-by: grimoire <streetyao@live.com>\n\n* feat(lmdeploy): add rerope quantization (#718)\n\n* feat(lmdeploy): add rerope quantization\n\n* feat(lmdeploy): fix review\n\n* [Refactor & Doc] Improve w8a8 and add docstring (#768)\n\n* WIP\n\n* improve w8a8 and add doc string\n\n* add docstring\n\n* add docstring\n\n* fix lint\n\n* rename pytorch poc (#764)\n\n* rename pytorch poc\n\n* fix lint\n\n* add docstring\n\n* add docstring\n\n* refactor patch\n\n* add recompute eviction support\n\n* recovery modeling\n\n* add docstring\n\n* Unified paging (#860)\n\n* change 'model_format' to 'qwen' when 'model_name' starts with 'qwen' (#575)\n\n* avoid split chinese characters during decoding (#566)\n\n* add solar chat template (#576)\n\n* robust incremental decode for leading space (#581)\n\n* robust incremental decode for leading space\n\n* speed up lookup as prefix_space_tokens is shorter than no_prefix_space_tokens\n\n* add UT and fix qwen stuff\n\n* update solar chat template (#587)\n\n* Revert \"[Docs] Simplify `build.md` (#370)\" (#586)\n\nThis reverts commit 4b5c2bda074eb4ac2e70c3c793fb5ef48f87d9c8.\n\n* Fix crash and remove `sys_instruct` from `chat.py` and `client.py`(#591)\n\n* fix crash\n\n* update profile_generation.py\n\n* format\n\n* use self.bos_id\n\n* remove sys_instruct\n\n* bump version to v0.0.12 (#604)\n\n* Add \"build from docker\" section (#602)\n\n* add build from docker section\n\n* update\n\n* install python package\n\n* update\n\n* update\n\n* update\n\n* Add more user-friendly CLI  (#541)\n\n* add\n\n* import fire in main\n\n* wrap to speed up fire cli\n\n* update\n\n* update docs\n\n* update docs\n\n* fix\n\n* resolve commennts\n\n* resolve confict and add test for cli\n\n* support inference a batch of prompts (#467)\n\n* support inference a batch of prompts\n\n* docstring and assert\n\n* bump version to v0.0.13 (#620)\n\n* Improve api_server and webui usage (#544)\n\n* make IPv6 compatible, safe run for coroutine interrupting\n\n* instance_id -> session_id and fix api_client.py\n\n* update doc\n\n* remove useless faq\n\n* safe ip mapping\n\n* update app.py\n\n* WIP completion\n\n* completion\n\n* update doc\n\n* disable interactive mode for /v1/chat/completions\n\n* docstring\n\n* docstring\n\n* refactor gradio\n\n* update gradio\n\n* udpate\n\n* update doc\n\n* rename\n\n* session_id default -1\n\n* missed two files\n\n* add a APIClient\n\n* add chat func for APIClient\n\n* refine\n\n* add concurrent function\n\n* sequence_start, sequence_end --> interactive_mode\n\n* update doc\n\n* comments\n\n* doc\n\n* better text completion\n\n* remove /v1/embeddings\n\n* comments\n\n* deprecate generate and use /v1/interactive/completions\n\n* /v1/interactive/completion -> /v1/chat/interactive\n\n* embeddings\n\n* rename\n\n* remove wrong arg description\n\n* docstring\n\n* fix\n\n* update cli\n\n* update doc\n\n* strict session_len limit condition\n\n* pass model args to api_server\n\n* fix: gradio gr.Button.update deprecated after 4.0.0 (#637)\n\n* add cli to list the supported model names (#639)\n\n* update\n\n* resolve comment\n\n* Refactor model conversion (#296)\n\n* split deploy.py\n\n* fix get_cuda_tensor\n\n* deploy qwen_awq\n\n* fix lint\n\n* add docstring\n\n* fix\n\n* support baichuan/baichuan-awq\n\n* parameterizing size_per_head\n\n* remove try/except\n\n* limit input model_format\n\n* add quant_path param\n\n* remove old deploy.py\n\n* fix path\n\n* fix transformer layer range when load bins\n\n* fix qwen init\n\n* split & save log\n\n* relative import\n\n* update get_config\n\n* WeightFileMgr -> Reader\n\n* rename\n\n* update\n\n* fix init_layer_id\n\n* rename llama.py -> meta_llama.py, hf.py -> llama.py\n\n* reduce code\n\n* update arg description\n\n* fix meta llama\n\n* manually cleanup meta model params\n\n* [Enchance] internlm message to prompt (#499)\n\n* update turbomind session_len with model.session_len (#634)\n\n* [Fix] Qwen's quantization results are abnormal & Baichuan cannot be quantized (#605)\n\n* fix awq\n\n* adapt new qwen code\n\n* adapt qwen 14b and baichuan2 7b\n\n* add docstring\n\n* add runtime error for qwen\n\n* FIX: fix stop_session func bug (#578)\n\n* FIX: fix stop_session func bug\n\n* keep sequence_end = False\n\n---------\n\nCo-authored-by: honglei.yan <honglei.yan@nio.com>\nCo-authored-by: AllentDan <AllentDan@yeah.net>\n\n* Manage session id using random int for gradio local mode (#553)\n\n* Use session id from gradio state\n\n* use a new session id after reset\n\n* rename session id like a state\n\n* update comments\n\n* reformat files\n\n* init session id on block loaded\n\n* use auto increased session id\n\n* remove session id textbox\n\n* apply to api_server and tritonserver\n\n* update docstring\n\n* add lock for safety\n\n---------\n\nCo-authored-by: AllentDan <AllentDan@yeah.net>\n\n* fix benchmark serving computation mistake (#630)\n\n* fix benchmark serving computation mistake\n\n* fix timestamps computations\n\n* remove speed up\n\n* no mp\n\n* mp seems faster?\n\n* remove\n\n* update\n\n* remove\n\n* fix\n\n* update\n\n* update print log\n\n* typo\n\n* print fist token latency only stream==True\n\n* remove renew_session\n\n* update AsyncEngine\n\n* fix tokenizer_info when convert the model (#661)\n\n* Add check env sub command (#654)\n\n* add check env\n\n* update issue template'\n\n* remove some reqs from check env\n\n* resolve comment\n\n* fix Tokenizer load error when the path of the being-converted  model is not writable (#669)\n\n* Add UltraCM and WizardLM chat templates (#599)\n\n* add ultracm eval chat template\n\n* add WizardLM chat template\n\n* use ultrachat template instead of ultracm usecase\n\n* bump version to v0.0.14 (#663)\n\n* Add extra_requires to reduce dependencies (#580)\n\n* update reqs\n\n* update docs\n\n* resolve comments\n\n* upgrade pydantic\n\n* fix rebase\n\n* update doc\n\n* update\n\n* update\n\n* update readme\n\n* update\n\n* add flash-attn\n\n* TurboMind 2 (#590)\n\n* refresh decoder attention kernel\n\n* block-level kv cache\n\n* `BlockManager` & `SequenceManager`\n\n* update\n\n* update\n\n* update\n\n* update\n\n* rename\n\n* GQA support\n\n* fix context length\n\n* GQA dispatch\n\n* kv8\n\n* tune\n\n* async stream cb\n\n* nvtx\n\n* config parsing\n\n* debug\n\n* optimize output cost\n\n* split-k decoding\n\n* minor\n\n* truncate `session_len` by available blocks\n\n* minor\n\n* license\n\n* fix\n\n* dispatch `cp.async`\n\n* fix linking\n\n* fix\n\n* fix deadlock\n\n* guard input length\n\n* correct start offset\n\n* fix prefill chunking\n\n* fix `cache_block_seq_len` param passing\n\n* fix `block_size` fmtstr\n\n* fix output tokens\n\n* fix batch resizing\n\n* fix masking of finished sequences\n\n* add debug util\n\n* free unused block early\n\n* add ntk scaling and logn scaling\n\n* cmake flags\n\n* fix typo\n\n* w4a16 for sm75\n\n* fix msvc build\n\n* fix msvc build\n\n* fix block verification\n\n* fix msvc build\n\n* use `std::shuffle`\n\n* fix lint\n\n* fix lint\n\n* fix lint\n\n* clear incoming buffer\n\n* clear finished requests\n\n* fix batch initialization\n\n* fix typo\n\n* fix typo\n\n* fix comparison\n\n* [Docs] Update Supported Matrix (#679)\n\n* update supported matrix\n\n* change the default shard size when saving quantized weights\n\n* baichuan2 kv8\n\n* update kv8 docs (#681)\n\n* Fix init of batch state (#682)\n\n* fix init of finished buf\n\n* fix `finished_count`\n\n* fix turbomind stream canceling (#686)\n\n* fix\n\n* instance for each forward\n\n* [Fix] Fix load_checkpoint_in_model bug (#690)\n\n* fix load_checkpoint_in_model bug\n\n* fix comments\n\n* fix comments\n\n* fix bugs\n\n* [Doc] Update restful api doc (#662)\n\n* update restful_api.md\n\n* add a hint\n\n* repeat 3 time\n\n* Fix Tokenizer encode (#645)\n\n* same encode with HF\n\n* sequence_start -> add_bos\n\n* complement\n\n* Fix wrong eos_id and bos_id obtained through grpc api (#644)\n\n* Fix wrong eos_id and bos_id obtained through grpc api\n\n* fix according to review comments\n\n* update\n\n* Optimize for throughput (#701)\n\n* tmp\n\n* update\n\n* update\n\n* optimize for throughput\n\n* update\n\n* fix eos\n\n* clean up\n\n* fix serving\n\n* fix indexed copy\n\n* minor\n\n* minor\n\n---------\n\nCo-authored-by: lvhan028 <lvhan_028@163.com>\n\n* Check-in user guide about turbomind config (#680)\n\n* update\n\n* update config guide\n\n* update guide\n\n* upate user guide according to review comments\n\n* Replace mmengine with mmengine-lite (#715)\n\n* Support loading hf model directly (#685)\n\n* turbomind support export model params\n\n* fix overflow\n\n* support turbomind.from_pretrained\n\n* fix tp\n\n* support AutoModel\n\n* support load kv qparams\n\n* update auto_awq\n\n* udpate docstring\n\n* export lmdeploy version\n\n* update doc\n\n* remove download_hf_repo\n\n* LmdeployForCausalLM -> LmdeployForCausalLM\n\n* refactor turbomind.py\n\n* update comment\n\n* add bfloat16 convert back\n\n* support gradio run_locl load hf\n\n* support resuful api server load hf\n\n* add docs\n\n* support loading previous quantized model\n\n* adapt pr 690\n\n* udpate docs\n\n* not export turbomind config when quantize a model\n\n* check model_name when can not get it from config.json\n\n* update readme\n\n* remove model_name in auto_awq\n\n* update\n\n* update\n\n* udpate\n\n* fix build\n\n* absolute import\n\n* Fix cache/output length calculation (#738)\n\n* bump version to v0.1.0a0 (#709)\n\n* [Fix] Skip empty batch (#747)\n\n* [Fix] build docker image failed since `packaging` is missing (#753)\n\n* [Fix] Rollback the data type of input_ids to TYPE_UINT32 in preprocessor's proto (#758)\n\n* Set the default value of `max_context_token_num` 1 (#761)\n\n* rename pytorch poc\n\n* fix lint\n\n* add docstring\n\n* add docstring\n\n* refactor patch\n\n* add recompute eviction support\n\n* fix typo (#769)\n\n* add triton server test and workflow yml (#760)\n\n* add triton server test and workflow yml\n\n* update\n\n* revert changes in dockerfile\n\n* update prompts\n\n* recovery modeling\n\n* fix turbomind build on sm<80 (#754)\n\n* fix\n\n* fix lint\n\n* improvement(build): enable ninja and gold linker (#767)\n\n* feat(build): enable ninja and lld\n\n* fix(.github): add ninja installation\n\n* fix(CI): remove dimsize=256\n\n* fix(CI): add option for generate.sh\n\n* fix(docs): update\n\n* Report first-token-latency and token-latency percentiles (#736)\n\n* update profile scripts\n\n* add top_p, top_k and temperature as input arguments\n\n* fix input_ids\n\n* update profile_throughput\n\n* update profile_restful_api\n\n* update profile_serving\n\n* update\n\n* update\n\n* add progress bar\n\n* remove TODO comments\n\n* update\n\n* remove useless profile_* argument\n\n* remove log level\n\n* change concurrency default value to 64\n\n* update restful_api.md\n\n* update according to review comments\n\n* fix docstring\n\n* convert model with hf repo_id (#774)\n\n* bump version to 0.1.0a1 (#776)\n\n* Update benchmark user guide (#763)\n\n* user guide of benchmark generation\n\n* update benchmark generation guide\n\n* update profiling throughput guide\n\n* update profiling api_server guide\n\n* rename file names\n\n* update profile tis user guide\n\n* update\n\n* fix according to review comments\n\n* update\n\n* update according to review comments\n\n* updaste\n\n* add an example\n\n* update\n\n* add docstring\n\n* add unified paging attention support\n\n* refactor block manager\n\n* do not alloc zero\n\n* Fix early exit condition in attention kernel (#788)\n\n* add chat template for Yi (#779)\n\n* Fix missed arguments when benchmark static inference performance (#787)\n\n* minor fix in the profile scripts and docs\n\n* miss arguments\n\n* typo\n\n* fix lint\n\n* update\n\n* Unify prefill & decode passes (#775)\n\n* Unify prefill and decode passes\n\n* dynamic split-fuse\n\n* refactor\n\n* correct input count calculation\n\n* remove unused\n\n* lint\n\n* lint\n\n* fix msvc build\n\n* fix msvc build\n\n* fix msvc build\n\n* fix msvc build\n\n* fix msvc build\n\n* fix msvc build\n\n* fix msvc build\n\n* fix msvc build\n\n* fix msvc build\n\n* add cuda12.1 build check ci (#782)\n\n* update cuda12.1 build check ci\n\n* use matrix\n\n* auto upload cuda12.1 python pkg to release when create new tag (#784)\n\n* add cuda12-whl-release ci\n\n* enable environment\n\n* test py310-311 windows wheel\n\n* fix py310, py311 setup.py error on windows\n\n* fix lint\n\n* fix extra colon in InternLMChat7B (#796)\n\n* fix local kv head num (#806)\n\n* Report the inference benchmark of models with different size (#794)\n\n* update test scripts for models with different sizes\n\n* update\n\n* only test after tunning gemm\n\n* chmod +x\n\n* fix typo\n\n* benchmark on a100\n\n* fix typo\n\n* fix typo\n\n* per-token latency percentile in profile_throughput\n\n* fix\n\n* fix\n\n* rename\n\n* make the script accept parameters\n\n* minor fix\n\n* indent\n\n* reformat table\n\n* change to 3000\n\n* minor fix\n\n* bump version to v0.1.0a2 (#807)\n\n* fix out of bounds access (#809)\n\n* update scheduler\n\n* optimize request\n\n* Simplify block manager (#812)\n\n* simplify block manager\n\n* fix lint\n\n* set smem size for repetition penalty kernel (#818)\n\n* add mbgemm&mbgemv\n\n* fix recompute, fix mbgmm\n\n---------\n\nCo-authored-by: Lyu Han <lvhan_028@163.com>\nCo-authored-by: AllentDan <41138331+AllentDan@users.noreply.github.com>\nCo-authored-by: pppppM <67539920+pppppM@users.noreply.github.com>\nCo-authored-by: Chen Xin <irexyc@gmail.com>\nCo-authored-by: RunningLeon <mnsheng@yeah.net>\nCo-authored-by: Yam(\u957f\u7434) <haoshaochun@gmail.com>\nCo-authored-by: liukuikun <24622904+Harold-lkk@users.noreply.github.com>\nCo-authored-by: yunzhongyan0 <549713537@qq.com>\nCo-authored-by: honglei.yan <honglei.yan@nio.com>\nCo-authored-by: AllentDan <AllentDan@yeah.net>\nCo-authored-by: aisensiy <aisensiy@163.com>\nCo-authored-by: Li Zhang <lzhang329@gmail.com>\nCo-authored-by: whcao <41630003+HIT-cwh@users.noreply.github.com>\nCo-authored-by: Zaida Zhou <58739961+zhouzaida@users.noreply.github.com>\nCo-authored-by: tpoisonooo <khj.application@aliyun.com>\nCo-authored-by: Qian Zhao <112053249+C1rN09@users.noreply.github.com>\n\n* [Fix] Adapt to the pyTorch poc branch (#863)\n\n* Adapt to the pyTorch poc branch\n\n* Adapt to the pyTorch poc branch\n\n* fix comments\n\n* update model\n\n* wip\n\n* wrong implementation\n\n* s-lora single gpu\n\n* refactor tp patch\n\n* add tp support\n\n* add tp gather\n\n* recover profile generation\n\n* daemon process\n\n* inplace gather\n\n* hf style\n\n* add assert when input nothing\n\n* find available port\n\n---------\n\nCo-authored-by: grimoire <yaoqian@pjlab.org.cn>\nCo-authored-by: WRH <12756472+wangruohui@users.noreply.github.com>\nCo-authored-by: AllentDan <AllentDan@yeah.net>\nCo-authored-by: AllentDan <41138331+AllentDan@users.noreply.github.com>\nCo-authored-by: tpoisonooo <khj.application@aliyun.com>\nCo-authored-by: whcao <41630003+HIT-cwh@users.noreply.github.com>\nCo-authored-by: Lyu Han <lvhan_028@163.com>\nCo-authored-by: pppppM <67539920+pppppM@users.noreply.github.com>\nCo-authored-by: Chen Xin <irexyc@gmail.com>\nCo-authored-by: RunningLeon <mnsheng@yeah.net>\nCo-authored-by: Yam(\u957f\u7434) <haoshaochun@gmail.com>\nCo-authored-by: liukuikun <24622904+Harold-lkk@users.noreply.github.com>\nCo-authored-by: yunzhongyan0 <549713537@qq.com>\nCo-authored-by: honglei.yan <honglei.yan@nio.com>\nCo-authored-by: aisensiy <aisensiy@163.com>\nCo-authored-by: Li Zhang <lzhang329@gmail.com>\nCo-authored-by: Zaida Zhou <58739961+zhouzaida@users.noreply.github.com>\nCo-authored-by: Qian Zhao <112053249+C1rN09@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/InternLM/lmdeploy",
        "filepath": "lmdeploy/pytorch/engine/model_agent.py",
        "commit_date": "2023-12-28T03:48:58Z",
        "message": "Refactor torch inference engine (#871)\n\n* WIP\n\n* cache engine wip\n\n* finish cache engine\n\n* fix cache and scheduler\n\n* add paged attention\n\n* step and stop\n\n* add infer\n\n* add request process\n\n* fix end\n\n* request without schedulersession\n\n* add logits processor\n\n* better context\n\n* update patch\n\n* [Improve] Use 4d input in pytorch poc (#371)\n\n* 4D input, model.eval and llama config\n\n* use auto dtype\n\n* tp wip\n\n* almost\n\n* update logger\n\n* run_check=false\n\n* little optimize\n\ncurrent best\n\nredist w/o dtensor\n\nhost mem in que\n\nless rewrite\n\nless code\n\nupdate model weight\n\n* share attention forward\n\n* fix end\n\n* Support Baichuan (#382)\n\n* add baichuan WIP\n\n* support baichuan\n\n* support baichuan-13b\n\n* fix\n\n* add chat template\n\n* lint\n\n* comments\n\n* fix\n\n* Move `q_seq_info` into `context` (#398)\n\n* move q seq info into context\n\n* remove debugs\n\n* remove debugs\n\n* alibi wip\n\n* add alibi\n\n* reduce logic block (#435)\n\n* add docstring\n\n* add baichuan lint (#445)\n\n* add fill cache back\n\n* support internlm\n\n* fix path of weight index\n\n* Support chatglm2 in pytorch_poc (#360)\n\n* draft support for chatglm2\n\n* debug llama\n\n* gitignore\n\n* update input_id\n\n* better patching\n\n* patch chatglm2 model\n\n* fix after merge\n\n* remove inits\n\n* q_seq_info & remove some debug & orig_self\n\n* remove old unqeuzze inputid\n\n* update patch and model config\n\n* remove debugs and clean codes\n\n* clean codes\n\n* add credit\n\n* add update id / fix dependency\n\n* rename modules (#504)\n\nCo-authored-by: grimoire <yaoqian@pjlab.org.cn>\n\n* optimize fill kv cache (#523)\n\n* optimize fill kv cache\n\n* update internlm\n\n* faster embedding\n\n* fix bias tp\n\n* fix baichuan2\n\n* fix fill kv cache\n\n* fix lint\n\n---------\n\n* Make trust_remote_code as cli argument (#434)\n\n* trust_remote_code_argument\n\n* format\n\n* update tokenizer\n\n* optimize rotary\n\n* wtf\n\n* Support Falcon models (#406)\n\n* move q seq info into context\n\n* falcon aligned\n\n* trust_remote_code_argument\n\n* fix for falcon\n\n* comment out debugs\n\n* comment out debugs\n\n* use position id in context\n\n* remove codes in falcon model\n\n* Revert \"comment out debugs\"\n\nThis reverts commit ee26a25c36f11c7afdad315b8101ebb5c2c637fd.\n\n* 7b correct\n\n* 1b aligned\n\n* remove debugs\n\n* patch to ignore position ids\n\n* remove debug in alibi, avoid empty inputs\n\n* fix\n\n* rename dir to replace to \"models\"\n\n* use position_id and new fill kernel\n\n* remove useless get_prompt func\n\n* fix batch>2\n\n* Refactor scheduler (#551)\n\n* optimize block manager\n\n* scheduler wip\n\n* finish scheduler\n\n* update engine\n\n* profile pytorch poc (#455)\n\n* profile pytorch poc\n\n* update doc and import if need\n\n* arg\n\n* support profile_throughput.py\n\n* reuse pytorch session\n\n* end session\n\n* Support Tensor parallel on Falcon models (#582)\n\n* tp falcon 1b and 7b works\n\n* remove debugs\n\n* update copyright\n\n* add some comments\n\n* remove a debug\n\n* support new hub models\n\n* support 40b\n\n* support 40b model config\n\n* try\n\n* recover\n\n* fix remain len\n\n* Apply rotary kernel (#572)\n\n* apply rotary kernel\n\n* format\n\n* update rmsnorm\n\n* update rms norm\n\n* better unittest\n\n* add docstring\n\n---------\n\nCo-authored-by: grimoire <yaoqian@pjlab.org.cn>\n\n* fix(pytorch_poc): memory cal (#606)\n\n* fix(pytorch_poc): memory cal\n\n* Optimize attention (#597)\n\n* add unittest\n\n* add split k\n\n* add docstring\n\n* fast split k\n\n* optimize load\n\n* manually setup device and stream\n\n* lint\n\n---------\n\nCo-authored-by: grimoire <yaoqian@pjlab.org.cn>\n\n* feat(pytorch_poc): implement ReRoPE (#625)\n\n* fix(pytorch_poc): memory cal\n\n* style(pytorch_poc): lint\n\n* style(.pre-commit-config.yaml): update\n\n* style(pytorch_poc): remove useless\n\n* feat(pytorch_poc): llama2 support rerope\n\n* feat(pytorch_poc): fix long input generate\n\n* feat(lmdeploy): add kernel\n\n* feat(lmdeploy): update\n\n* feat(lmdeploy): add rerope implementation\n\n* fix(lmdeploy/pytorch_poc): apply rotary_emb\n\n* fix(lmdeploy): update\n\n* style(pytorch_poc): format\n\n* style(lmdeploy): fix lint\n\n* style(lmdeploy): typo\n\n* style(pytorch_poc): format\n\n* style(pytorch_poc): format\n\n* fix(pytorch_poc): rms_norm add mask\n\n* style(pytorch_poc/kernels): format rerope\n\n* style(pytorch_poc): format rerope attn function description\n\n* style(lmdeploy/pytorch_poc): format\n\n* style(pytorch_poc): add code ref\n\n* style(pytorch_poc): format rerope attn\n\n* Refactor engine (#623)\n\n* add agent\n\n* optimize postprocess\n\n* optimize decoding fill cache\n\n* add docstring\n\n* logit to cuda\n\n* blocksize 128\n\n* optimize pre/post process\n\n* fix postprocess\n\n* cpu pre/post process\n\n* manually setup stream and device\n\n* remove context\n\n* update model agent\n\n* update max session len\n\n* remove tqdm\n\n* update pre/post process\n\n* inplace kernel\n\n* avoid kv_len computation\n\n* flash decoding with one cache\n\n* remove comment\n\n* add warning when no enough resources\n\n* step if has unfinish\n\n* add request manager\n\n* better fill kv cache\n\n* fix fill kv cache\n\n* optimize prefill attention\n\n* refractor\n\n* refactoring...\n\n* add custom output\n\n* use cache\n\n---------\n\nCo-authored-by: grimoire <yaoqian@pjlab.org.cn>\n\n* [Feature] w8a8 based on pytorch poc (#595)\n\n* refactor smoothquant and support load w8a8 model by from_pretrained\n\n* add w8a8 docs\n\n* add w8a8 en docs\n\n* add convert_to_qmodules function\n\n---------\n\nCo-authored-by: grimoire <streetyao@live.com>\n\n* feat(lmdeploy): add rerope quantization (#718)\n\n* feat(lmdeploy): add rerope quantization\n\n* feat(lmdeploy): fix review\n\n* [Refactor & Doc] Improve w8a8 and add docstring (#768)\n\n* WIP\n\n* improve w8a8 and add doc string\n\n* add docstring\n\n* add docstring\n\n* fix lint\n\n* rename pytorch poc (#764)\n\n* rename pytorch poc\n\n* fix lint\n\n* add docstring\n\n* add docstring\n\n* refactor patch\n\n* add recompute eviction support\n\n* recovery modeling\n\n* add docstring\n\n* Unified paging (#860)\n\n* change 'model_format' to 'qwen' when 'model_name' starts with 'qwen' (#575)\n\n* avoid split chinese characters during decoding (#566)\n\n* add solar chat template (#576)\n\n* robust incremental decode for leading space (#581)\n\n* robust incremental decode for leading space\n\n* speed up lookup as prefix_space_tokens is shorter than no_prefix_space_tokens\n\n* add UT and fix qwen stuff\n\n* update solar chat template (#587)\n\n* Revert \"[Docs] Simplify `build.md` (#370)\" (#586)\n\nThis reverts commit 4b5c2bda074eb4ac2e70c3c793fb5ef48f87d9c8.\n\n* Fix crash and remove `sys_instruct` from `chat.py` and `client.py`(#591)\n\n* fix crash\n\n* update profile_generation.py\n\n* format\n\n* use self.bos_id\n\n* remove sys_instruct\n\n* bump version to v0.0.12 (#604)\n\n* Add \"build from docker\" section (#602)\n\n* add build from docker section\n\n* update\n\n* install python package\n\n* update\n\n* update\n\n* update\n\n* Add more user-friendly CLI  (#541)\n\n* add\n\n* import fire in main\n\n* wrap to speed up fire cli\n\n* update\n\n* update docs\n\n* update docs\n\n* fix\n\n* resolve commennts\n\n* resolve confict and add test for cli\n\n* support inference a batch of prompts (#467)\n\n* support inference a batch of prompts\n\n* docstring and assert\n\n* bump version to v0.0.13 (#620)\n\n* Improve api_server and webui usage (#544)\n\n* make IPv6 compatible, safe run for coroutine interrupting\n\n* instance_id -> session_id and fix api_client.py\n\n* update doc\n\n* remove useless faq\n\n* safe ip mapping\n\n* update app.py\n\n* WIP completion\n\n* completion\n\n* update doc\n\n* disable interactive mode for /v1/chat/completions\n\n* docstring\n\n* docstring\n\n* refactor gradio\n\n* update gradio\n\n* udpate\n\n* update doc\n\n* rename\n\n* session_id default -1\n\n* missed two files\n\n* add a APIClient\n\n* add chat func for APIClient\n\n* refine\n\n* add concurrent function\n\n* sequence_start, sequence_end --> interactive_mode\n\n* update doc\n\n* comments\n\n* doc\n\n* better text completion\n\n* remove /v1/embeddings\n\n* comments\n\n* deprecate generate and use /v1/interactive/completions\n\n* /v1/interactive/completion -> /v1/chat/interactive\n\n* embeddings\n\n* rename\n\n* remove wrong arg description\n\n* docstring\n\n* fix\n\n* update cli\n\n* update doc\n\n* strict session_len limit condition\n\n* pass model args to api_server\n\n* fix: gradio gr.Button.update deprecated after 4.0.0 (#637)\n\n* add cli to list the supported model names (#639)\n\n* update\n\n* resolve comment\n\n* Refactor model conversion (#296)\n\n* split deploy.py\n\n* fix get_cuda_tensor\n\n* deploy qwen_awq\n\n* fix lint\n\n* add docstring\n\n* fix\n\n* support baichuan/baichuan-awq\n\n* parameterizing size_per_head\n\n* remove try/except\n\n* limit input model_format\n\n* add quant_path param\n\n* remove old deploy.py\n\n* fix path\n\n* fix transformer layer range when load bins\n\n* fix qwen init\n\n* split & save log\n\n* relative import\n\n* update get_config\n\n* WeightFileMgr -> Reader\n\n* rename\n\n* update\n\n* fix init_layer_id\n\n* rename llama.py -> meta_llama.py, hf.py -> llama.py\n\n* reduce code\n\n* update arg description\n\n* fix meta llama\n\n* manually cleanup meta model params\n\n* [Enchance] internlm message to prompt (#499)\n\n* update turbomind session_len with model.session_len (#634)\n\n* [Fix] Qwen's quantization results are abnormal & Baichuan cannot be quantized (#605)\n\n* fix awq\n\n* adapt new qwen code\n\n* adapt qwen 14b and baichuan2 7b\n\n* add docstring\n\n* add runtime error for qwen\n\n* FIX: fix stop_session func bug (#578)\n\n* FIX: fix stop_session func bug\n\n* keep sequence_end = False\n\n---------\n\nCo-authored-by: honglei.yan <honglei.yan@nio.com>\nCo-authored-by: AllentDan <AllentDan@yeah.net>\n\n* Manage session id using random int for gradio local mode (#553)\n\n* Use session id from gradio state\n\n* use a new session id after reset\n\n* rename session id like a state\n\n* update comments\n\n* reformat files\n\n* init session id on block loaded\n\n* use auto increased session id\n\n* remove session id textbox\n\n* apply to api_server and tritonserver\n\n* update docstring\n\n* add lock for safety\n\n---------\n\nCo-authored-by: AllentDan <AllentDan@yeah.net>\n\n* fix benchmark serving computation mistake (#630)\n\n* fix benchmark serving computation mistake\n\n* fix timestamps computations\n\n* remove speed up\n\n* no mp\n\n* mp seems faster?\n\n* remove\n\n* update\n\n* remove\n\n* fix\n\n* update\n\n* update print log\n\n* typo\n\n* print fist token latency only stream==True\n\n* remove renew_session\n\n* update AsyncEngine\n\n* fix tokenizer_info when convert the model (#661)\n\n* Add check env sub command (#654)\n\n* add check env\n\n* update issue template'\n\n* remove some reqs from check env\n\n* resolve comment\n\n* fix Tokenizer load error when the path of the being-converted  model is not writable (#669)\n\n* Add UltraCM and WizardLM chat templates (#599)\n\n* add ultracm eval chat template\n\n* add WizardLM chat template\n\n* use ultrachat template instead of ultracm usecase\n\n* bump version to v0.0.14 (#663)\n\n* Add extra_requires to reduce dependencies (#580)\n\n* update reqs\n\n* update docs\n\n* resolve comments\n\n* upgrade pydantic\n\n* fix rebase\n\n* update doc\n\n* update\n\n* update\n\n* update readme\n\n* update\n\n* add flash-attn\n\n* TurboMind 2 (#590)\n\n* refresh decoder attention kernel\n\n* block-level kv cache\n\n* `BlockManager` & `SequenceManager`\n\n* update\n\n* update\n\n* update\n\n* update\n\n* rename\n\n* GQA support\n\n* fix context length\n\n* GQA dispatch\n\n* kv8\n\n* tune\n\n* async stream cb\n\n* nvtx\n\n* config parsing\n\n* debug\n\n* optimize output cost\n\n* split-k decoding\n\n* minor\n\n* truncate `session_len` by available blocks\n\n* minor\n\n* license\n\n* fix\n\n* dispatch `cp.async`\n\n* fix linking\n\n* fix\n\n* fix deadlock\n\n* guard input length\n\n* correct start offset\n\n* fix prefill chunking\n\n* fix `cache_block_seq_len` param passing\n\n* fix `block_size` fmtstr\n\n* fix output tokens\n\n* fix batch resizing\n\n* fix masking of finished sequences\n\n* add debug util\n\n* free unused block early\n\n* add ntk scaling and logn scaling\n\n* cmake flags\n\n* fix typo\n\n* w4a16 for sm75\n\n* fix msvc build\n\n* fix msvc build\n\n* fix block verification\n\n* fix msvc build\n\n* use `std::shuffle`\n\n* fix lint\n\n* fix lint\n\n* fix lint\n\n* clear incoming buffer\n\n* clear finished requests\n\n* fix batch initialization\n\n* fix typo\n\n* fix typo\n\n* fix comparison\n\n* [Docs] Update Supported Matrix (#679)\n\n* update supported matrix\n\n* change the default shard size when saving quantized weights\n\n* baichuan2 kv8\n\n* update kv8 docs (#681)\n\n* Fix init of batch state (#682)\n\n* fix init of finished buf\n\n* fix `finished_count`\n\n* fix turbomind stream canceling (#686)\n\n* fix\n\n* instance for each forward\n\n* [Fix] Fix load_checkpoint_in_model bug (#690)\n\n* fix load_checkpoint_in_model bug\n\n* fix comments\n\n* fix comments\n\n* fix bugs\n\n* [Doc] Update restful api doc (#662)\n\n* update restful_api.md\n\n* add a hint\n\n* repeat 3 time\n\n* Fix Tokenizer encode (#645)\n\n* same encode with HF\n\n* sequence_start -> add_bos\n\n* complement\n\n* Fix wrong eos_id and bos_id obtained through grpc api (#644)\n\n* Fix wrong eos_id and bos_id obtained through grpc api\n\n* fix according to review comments\n\n* update\n\n* Optimize for throughput (#701)\n\n* tmp\n\n* update\n\n* update\n\n* optimize for throughput\n\n* update\n\n* fix eos\n\n* clean up\n\n* fix serving\n\n* fix indexed copy\n\n* minor\n\n* minor\n\n---------\n\nCo-authored-by: lvhan028 <lvhan_028@163.com>\n\n* Check-in user guide about turbomind config (#680)\n\n* update\n\n* update config guide\n\n* update guide\n\n* upate user guide according to review comments\n\n* Replace mmengine with mmengine-lite (#715)\n\n* Support loading hf model directly (#685)\n\n* turbomind support export model params\n\n* fix overflow\n\n* support turbomind.from_pretrained\n\n* fix tp\n\n* support AutoModel\n\n* support load kv qparams\n\n* update auto_awq\n\n* udpate docstring\n\n* export lmdeploy version\n\n* update doc\n\n* remove download_hf_repo\n\n* LmdeployForCausalLM -> LmdeployForCausalLM\n\n* refactor turbomind.py\n\n* update comment\n\n* add bfloat16 convert back\n\n* support gradio run_locl load hf\n\n* support resuful api server load hf\n\n* add docs\n\n* support loading previous quantized model\n\n* adapt pr 690\n\n* udpate docs\n\n* not export turbomind config when quantize a model\n\n* check model_name when can not get it from config.json\n\n* update readme\n\n* remove model_name in auto_awq\n\n* update\n\n* update\n\n* udpate\n\n* fix build\n\n* absolute import\n\n* Fix cache/output length calculation (#738)\n\n* bump version to v0.1.0a0 (#709)\n\n* [Fix] Skip empty batch (#747)\n\n* [Fix] build docker image failed since `packaging` is missing (#753)\n\n* [Fix] Rollback the data type of input_ids to TYPE_UINT32 in preprocessor's proto (#758)\n\n* Set the default value of `max_context_token_num` 1 (#761)\n\n* rename pytorch poc\n\n* fix lint\n\n* add docstring\n\n* add docstring\n\n* refactor patch\n\n* add recompute eviction support\n\n* fix typo (#769)\n\n* add triton server test and workflow yml (#760)\n\n* add triton server test and workflow yml\n\n* update\n\n* revert changes in dockerfile\n\n* update prompts\n\n* recovery modeling\n\n* fix turbomind build on sm<80 (#754)\n\n* fix\n\n* fix lint\n\n* improvement(build): enable ninja and gold linker (#767)\n\n* feat(build): enable ninja and lld\n\n* fix(.github): add ninja installation\n\n* fix(CI): remove dimsize=256\n\n* fix(CI): add option for generate.sh\n\n* fix(docs): update\n\n* Report first-token-latency and token-latency percentiles (#736)\n\n* update profile scripts\n\n* add top_p, top_k and temperature as input arguments\n\n* fix input_ids\n\n* update profile_throughput\n\n* update profile_restful_api\n\n* update profile_serving\n\n* update\n\n* update\n\n* add progress bar\n\n* remove TODO comments\n\n* update\n\n* remove useless profile_* argument\n\n* remove log level\n\n* change concurrency default value to 64\n\n* update restful_api.md\n\n* update according to review comments\n\n* fix docstring\n\n* convert model with hf repo_id (#774)\n\n* bump version to 0.1.0a1 (#776)\n\n* Update benchmark user guide (#763)\n\n* user guide of benchmark generation\n\n* update benchmark generation guide\n\n* update profiling throughput guide\n\n* update profiling api_server guide\n\n* rename file names\n\n* update profile tis user guide\n\n* update\n\n* fix according to review comments\n\n* update\n\n* update according to review comments\n\n* updaste\n\n* add an example\n\n* update\n\n* add docstring\n\n* add unified paging attention support\n\n* refactor block manager\n\n* do not alloc zero\n\n* Fix early exit condition in attention kernel (#788)\n\n* add chat template for Yi (#779)\n\n* Fix missed arguments when benchmark static inference performance (#787)\n\n* minor fix in the profile scripts and docs\n\n* miss arguments\n\n* typo\n\n* fix lint\n\n* update\n\n* Unify prefill & decode passes (#775)\n\n* Unify prefill and decode passes\n\n* dynamic split-fuse\n\n* refactor\n\n* correct input count calculation\n\n* remove unused\n\n* lint\n\n* lint\n\n* fix msvc build\n\n* fix msvc build\n\n* fix msvc build\n\n* fix msvc build\n\n* fix msvc build\n\n* fix msvc build\n\n* fix msvc build\n\n* fix msvc build\n\n* fix msvc build\n\n* add cuda12.1 build check ci (#782)\n\n* update cuda12.1 build check ci\n\n* use matrix\n\n* auto upload cuda12.1 python pkg to release when create new tag (#784)\n\n* add cuda12-whl-release ci\n\n* enable environment\n\n* test py310-311 windows wheel\n\n* fix py310, py311 setup.py error on windows\n\n* fix lint\n\n* fix extra colon in InternLMChat7B (#796)\n\n* fix local kv head num (#806)\n\n* Report the inference benchmark of models with different size (#794)\n\n* update test scripts for models with different sizes\n\n* update\n\n* only test after tunning gemm\n\n* chmod +x\n\n* fix typo\n\n* benchmark on a100\n\n* fix typo\n\n* fix typo\n\n* per-token latency percentile in profile_throughput\n\n* fix\n\n* fix\n\n* rename\n\n* make the script accept parameters\n\n* minor fix\n\n* indent\n\n* reformat table\n\n* change to 3000\n\n* minor fix\n\n* bump version to v0.1.0a2 (#807)\n\n* fix out of bounds access (#809)\n\n* update scheduler\n\n* optimize request\n\n* Simplify block manager (#812)\n\n* simplify block manager\n\n* fix lint\n\n* set smem size for repetition penalty kernel (#818)\n\n* add mbgemm&mbgemv\n\n* fix recompute, fix mbgmm\n\n---------\n\nCo-authored-by: Lyu Han <lvhan_028@163.com>\nCo-authored-by: AllentDan <41138331+AllentDan@users.noreply.github.com>\nCo-authored-by: pppppM <67539920+pppppM@users.noreply.github.com>\nCo-authored-by: Chen Xin <irexyc@gmail.com>\nCo-authored-by: RunningLeon <mnsheng@yeah.net>\nCo-authored-by: Yam(\u957f\u7434) <haoshaochun@gmail.com>\nCo-authored-by: liukuikun <24622904+Harold-lkk@users.noreply.github.com>\nCo-authored-by: yunzhongyan0 <549713537@qq.com>\nCo-authored-by: honglei.yan <honglei.yan@nio.com>\nCo-authored-by: AllentDan <AllentDan@yeah.net>\nCo-authored-by: aisensiy <aisensiy@163.com>\nCo-authored-by: Li Zhang <lzhang329@gmail.com>\nCo-authored-by: whcao <41630003+HIT-cwh@users.noreply.github.com>\nCo-authored-by: Zaida Zhou <58739961+zhouzaida@users.noreply.github.com>\nCo-authored-by: tpoisonooo <khj.application@aliyun.com>\nCo-authored-by: Qian Zhao <112053249+C1rN09@users.noreply.github.com>\n\n* [Fix] Adapt to the pyTorch poc branch (#863)\n\n* Adapt to the pyTorch poc branch\n\n* Adapt to the pyTorch poc branch\n\n* fix comments\n\n* update model\n\n* update benchmark\n\n* [Fix] Fix conflicts in `lite` (#878)\n\n* cherry-pick Fix meta tensor error commits\n\n* fix smooth quant\n\n---------\n\nCo-authored-by: pppppM <67539920+pppppM@users.noreply.github.com>\n\n* [Feature] Support w8a8 tp (#888)\n\n* fix smooth quant save_pretrained\n\n* support w8a8 tp\n\n* change weight and bias in QLinear back to buffer\n\n* remove debug codes and add comments\n\n* fix message step update\n\n* update docs\n\n---------\nCo-authored-by: grimoire <streetyao@live.com>\nCo-authored-by: WRH <12756472+wangruohui@users.noreply.github.com>\nCo-authored-by: AllentDan <AllentDan@yeah.net>\nCo-authored-by: AllentDan <41138331+AllentDan@users.noreply.github.com>\nCo-authored-by: tpoisonooo <khj.application@aliyun.com>\nCo-authored-by: whcao <41630003+HIT-cwh@users.noreply.github.com>\nCo-authored-by: pppppM <67539920+pppppM@users.noreply.github.com>\nCo-authored-by: Chen Xin <irexyc@gmail.com>\nCo-authored-by: RunningLeon <mnsheng@yeah.net>\nCo-authored-by: Yam(\u957f\u7434) <haoshaochun@gmail.com>\nCo-authored-by: liukuikun <24622904+Harold-lkk@users.noreply.github.com>\nCo-authored-by: yunzhongyan0 <549713537@qq.com>\nCo-authored-by: honglei.yan <honglei.yan@nio.com>\nCo-authored-by: aisensiy <aisensiy@163.com>\nCo-authored-by: Li Zhang <lzhang329@gmail.com>\nCo-authored-by: Zaida Zhou <58739961+zhouzaida@users.noreply.github.com>\nCo-authored-by: Qian Zhao <112053249+C1rN09@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/AILab-CVC/GPT4Tools",
        "filepath": "gpt4tools/llm.py",
        "commit_date": "2023-12-18T17:47:26Z",
        "message": "update vicuna-v1.5"
    },
    {
        "repo_url": "github.com/project-baize/baize-chatbot",
        "filepath": "demo/app_modules/utils.py",
        "commit_date": "2023-05-24T21:05:33Z",
        "message": "Update utils.py"
    },
    {
        "repo_url": "github.com/project-baize/baize-chatbot",
        "filepath": "demo/app_modules/utils.py",
        "commit_date": "2023-04-19T03:39:10Z",
        "message": "Update utils.py\n\nIterator is missing"
    },
    {
        "repo_url": "github.com/project-baize/baize-chatbot",
        "filepath": "demo/app_modules/utils.py",
        "commit_date": "2023-04-05T15:53:42Z",
        "message": "Rename function"
    },
    {
        "repo_url": "github.com/project-baize/baize-chatbot",
        "filepath": "demo/app_modules/utils.py",
        "commit_date": "2023-04-05T05:53:56Z",
        "message": "Fix style"
    },
    {
        "repo_url": "github.com/project-baize/baize-chatbot",
        "filepath": "demo/app_modules/utils.py",
        "commit_date": "2023-04-05T04:05:18Z",
        "message": "Update utils.py"
    },
    {
        "repo_url": "github.com/project-baize/baize-chatbot",
        "filepath": "demo/app_modules/utils.py",
        "commit_date": "2023-04-04T16:23:06Z",
        "message": "Reformat code"
    },
    {
        "repo_url": "github.com/project-baize/baize-chatbot",
        "filepath": "demo/app_modules/utils.py",
        "commit_date": "2023-04-04T16:03:54Z",
        "message": "Update utils.py"
    },
    {
        "repo_url": "github.com/project-baize/baize-chatbot",
        "filepath": "demo/app_modules/utils.py",
        "commit_date": "2023-04-03T07:29:22Z",
        "message": "update code and readme"
    },
    {
        "repo_url": "github.com/project-baize/baize-chatbot",
        "filepath": "demo/app_modules/utils.py",
        "commit_date": "2023-04-03T06:14:07Z",
        "message": "update code"
    },
    {
        "repo_url": "github.com/project-baize/baize-chatbot",
        "filepath": "demo/app_modules/utils.py",
        "commit_date": "2023-04-01T14:18:21Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/liguodongiot/llm-action",
        "filepath": "train/chinese-llama-alpaca/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-07-23T11:43:58Z",
        "message": "fix"
    },
    {
        "repo_url": "github.com/nlpai-lab/KULLM",
        "filepath": "merge_lora.py",
        "commit_date": "2023-08-13T10:40:06Z",
        "message": "fix: truncate embedding size"
    },
    {
        "repo_url": "github.com/nlpai-lab/KULLM",
        "filepath": "merge_lora.py",
        "commit_date": "2023-06-16T08:07:15Z",
        "message": "add: merge lora weight to model"
    },
    {
        "repo_url": "github.com/InternLM/InternLM-XComposer",
        "filepath": "projects/ShareGPT4V/share4v/train/train.py",
        "commit_date": "2023-12-13T17:41:19Z",
        "message": "full update"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference_multigpu_demo.py",
        "commit_date": "2024-01-30T04:26:15Z",
        "message": "update infer."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference_multigpu_demo.py",
        "commit_date": "2023-11-15T06:48:12Z",
        "message": "update infer use greedy decoding."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference_multigpu_demo.py",
        "commit_date": "2023-11-13T12:39:54Z",
        "message": "update multi gpu."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference_multigpu_demo.py",
        "commit_date": "2023-11-10T02:56:59Z",
        "message": "update fp32 for lm_head."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference_multigpu_demo.py",
        "commit_date": "2023-10-25T05:05:42Z",
        "message": "update peft logger."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference_multigpu_demo.py",
        "commit_date": "2023-10-23T09:06:58Z",
        "message": "update left padding."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference_multigpu_demo.py",
        "commit_date": "2023-10-23T04:30:54Z",
        "message": "update infer for do sample."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference_multigpu_demo.py",
        "commit_date": "2023-10-17T11:16:06Z",
        "message": "update template name."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference_multigpu_demo.py",
        "commit_date": "2023-09-08T11:10:34Z",
        "message": "update file exists."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference_multigpu_demo.py",
        "commit_date": "2023-09-07T07:32:04Z",
        "message": "drop truncation."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference_multigpu_demo.py",
        "commit_date": "2023-09-07T07:08:51Z",
        "message": "update DDP"
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference_multigpu_demo.py",
        "commit_date": "2023-09-07T06:13:13Z",
        "message": "update show case."
    },
    {
        "repo_url": "github.com/shibing624/MedicalGPT",
        "filepath": "inference_multigpu_demo.py",
        "commit_date": "2023-09-07T06:00:19Z",
        "message": "update multi gpu infer."
    },
    {
        "repo_url": "github.com/arielnlee/Platypus",
        "filepath": "inference.py",
        "commit_date": "2023-08-14T20:33:37Z",
        "message": "update pipeline"
    },
    {
        "repo_url": "github.com/arielnlee/Platypus",
        "filepath": "inference.py",
        "commit_date": "2023-06-26T04:26:47Z",
        "message": "initial files"
    },
    {
        "repo_url": "github.com/EvilPsyCHo/Play-with-LLMs",
        "filepath": "webui.py",
        "commit_date": "2023-07-05T12:39:28Z",
        "message": "webui support baichuan"
    },
    {
        "repo_url": "github.com/EvilPsyCHo/Play-with-LLMs",
        "filepath": "webui.py",
        "commit_date": "2023-06-28T04:02:48Z",
        "message": "add model inference webui"
    },
    {
        "repo_url": "github.com/nlpai-lab/KULLM",
        "filepath": "merge_model.py",
        "commit_date": "2023-08-13T10:36:33Z",
        "message": "fix: resize embedding considering vocab size of tokenizer"
    },
    {
        "repo_url": "github.com/eosphoros-ai/DB-GPT-Hub",
        "filepath": "dbgpt_hub/llm_base/adapter.py",
        "commit_date": "2023-10-29T03:27:55Z",
        "message": "black style un regular code"
    },
    {
        "repo_url": "github.com/eosphoros-ai/DB-GPT-Hub",
        "filepath": "dbgpt_hub/llm_base/adapter.py",
        "commit_date": "2023-09-22T17:03:10Z",
        "message": "try solve predict import"
    },
    {
        "repo_url": "github.com/eosphoros-ai/DB-GPT-Hub",
        "filepath": "dbgpt_hub/llm_base/adapter.py",
        "commit_date": "2023-09-19T09:51:03Z",
        "message": "change class about llm_base dir,add model_trainer,reduce config_parser"
    },
    {
        "repo_url": "github.com/eosphoros-ai/DB-GPT-Hub",
        "filepath": "dbgpt_hub/llm_base/adapter.py",
        "commit_date": "2023-09-18T14:23:11Z",
        "message": "add llm_base adapter,shared modules needed when loading models for different fine-tuning training methods and inference stages, mainly for the shared processing of Peft related to methods like LoRA, QLoRA, not using adapter, etc."
    },
    {
        "repo_url": "github.com/star-whale/starwhale",
        "filepath": "example/LLM/guanaco/sw.py",
        "commit_date": "2023-06-09T02:22:16Z",
        "message": "example(LLM): add guanaco evaluation example (#2315)"
    },
    {
        "repo_url": "github.com/FudanDISC/DISC-LawLLM",
        "filepath": "eval/src/models.py",
        "commit_date": "2024-01-24T09:22:38Z",
        "message": "Update models.py"
    },
    {
        "repo_url": "github.com/FudanDISC/DISC-LawLLM",
        "filepath": "eval/src/models.py",
        "commit_date": "2023-10-27T05:10:17Z",
        "message": "prepare release of DISC-Law-Eval benchmark scripts"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "peft/src/peft/peft_model.py",
        "commit_date": "2023-05-25T16:38:44Z",
        "message": "fixed prefix tuning inference bug"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "peft/src/peft/peft_model.py",
        "commit_date": "2023-04-07T11:23:59Z",
        "message": "fixed inference error without loading int8 model"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "peft/src/peft/peft_model.py",
        "commit_date": "2023-03-31T12:49:40Z",
        "message": "support bottleneck adapters"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-09-04T15:07:04Z",
        "message": "Clean load_model function and add quantization_inference config"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-08-29T10:50:23Z",
        "message": "CodeModels + TrainingArguments mutable"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-08-09T14:16:22Z",
        "message": "Revert \"Attempt to fix deespeed issues with learning rate being and make style\"\n\nThis reverts commit 62f9c6a14b373410d1abb1eaad01c64c82201073."
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-08-09T11:06:02Z",
        "message": "Attempt to fix deespeed issues with learning rate being and make style"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-07-27T17:45:44Z",
        "message": "Ensure that lr is a float"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-07-07T20:28:07Z",
        "message": "Ensure use_cache is set to False"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-06-27T10:49:57Z",
        "message": "Full model fp16"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-06-27T09:58:50Z",
        "message": "Force dtype to match the fp16/bf16 arguments for model quantification"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-05-17T15:58:46Z",
        "message": "Fix print when labels include -100 value"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-05-17T13:02:46Z",
        "message": "Fix debug print"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-05-17T12:54:07Z",
        "message": "Print first batch of data"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-05-03T17:06:38Z",
        "message": "Fix bug in loss function"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-05-02T14:21:36Z",
        "message": "Custom loss"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-24T14:12:17Z",
        "message": "Oversample to ensure same length in datasets"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-24T13:45:50Z",
        "message": "Rotate datasets during training"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-16T16:50:57Z",
        "message": " line-length to 119 and ruff conf to match HF"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-16T16:32:01Z",
        "message": "Add ruff and code lint improvements"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-14T17:15:38Z",
        "message": "Disable rich until fixed"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-14T16:17:57Z",
        "message": "Revert rich console"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-14T16:15:25Z",
        "message": "Attempt to fix rich and fix predictions"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-14T15:49:45Z",
        "message": "Add refresh=True to rich"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-13T23:30:34Z",
        "message": "more bug fixing"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-13T23:07:31Z",
        "message": "fix bug"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-13T22:59:46Z",
        "message": "add rich progress bar to CollieTrainer"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-13T12:26:05Z",
        "message": "Name change for `trainer.py` and `run.py`"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-12T15:27:22Z",
        "message": "Use trainer.save_model instead of save_pretrained"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-12T15:17:23Z",
        "message": "Modify the Seq2SeqTrainer to only save LoRA weights"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-12T10:15:09Z",
        "message": "Evaluate loss zero-shot models in dev"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-11T13:41:58Z",
        "message": "Rename target_modules as lora_target_modules and update find_hparams"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-11T13:25:46Z",
        "message": "Fix bug in dataset and add target modules as a hparam"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-11T10:03:01Z",
        "message": "Fix model and predictions saving"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-10T12:51:18Z",
        "message": "Fix minor bug"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-10T10:04:32Z",
        "message": "Refactor"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-10T10:03:54Z",
        "message": "Merge remote-tracking branch 'refs/remotes/origin/Train-experiments' into Train-experiments\nMerge local changes with git changes"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-10T10:03:25Z",
        "message": "fix trainer"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-08T14:55:10Z",
        "message": "Fix model save"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-05T20:37:52Z",
        "message": "Fix bug and slurm file"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-05T14:37:19Z",
        "message": "Remove duplicated model saving"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-05T14:25:03Z",
        "message": "Add verbosity"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-05T14:22:05Z",
        "message": "Set padding size to the left. Fix model loading at inference"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-04T19:47:13Z",
        "message": "Save config and eval config"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-04T18:00:06Z",
        "message": "Set logging level INFO"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-04T17:48:44Z",
        "message": "Fix path"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-04T17:40:43Z",
        "message": "Fix bug"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-04T17:25:21Z",
        "message": "Small change in prompt and fixes"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-04T15:51:51Z",
        "message": "Prepare config for debug"
    },
    {
        "repo_url": "github.com/hitz-zentroa/GoLLIE",
        "filepath": "src/trainer.py",
        "commit_date": "2023-04-04T13:50:52Z",
        "message": "Implement dataset and trainer"
    },
    {
        "repo_url": "github.com/dvlab-research/LLaMA-VID",
        "filepath": "llamavid/model/builder.py",
        "commit_date": "2023-11-28T17:03:38Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/Clouditera/secgpt",
        "filepath": "webdemo/webdemo.py",
        "commit_date": "2023-11-20T09:52:20Z",
        "message": "update README.md"
    },
    {
        "repo_url": "github.com/Clouditera/secgpt",
        "filepath": "webdemo/webdemo.py",
        "commit_date": "2023-11-20T07:33:28Z",
        "message": "update readme"
    },
    {
        "repo_url": "github.com/Clouditera/secgpt",
        "filepath": "webdemo/webdemo.py",
        "commit_date": "2023-11-20T06:00:38Z",
        "message": "first blood"
    },
    {
        "repo_url": "github.com/jerry1993-tech/Cornucopia-LLaMA-Fin-Chinese",
        "filepath": "infer.py",
        "commit_date": "2023-05-21T14:15:45Z",
        "message": "first version"
    },
    {
        "repo_url": "github.com/jerry1993-tech/Cornucopia-LLaMA-Fin-Chinese",
        "filepath": "infer.py",
        "commit_date": "2023-05-06T03:26:02Z",
        "message": "init code"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-12-01T07:03:58Z",
        "message": "make adaption to latest peft version"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-07-18T07:23:03Z",
        "message": "fix checkpoint saving issue. work with peft 0.3.0 and transformers 4.30.2"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-05-30T01:34:54Z",
        "message": "Update finetune.py"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-05-10T08:54:57Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-05-10T08:54:34Z",
        "message": "Update finetune.py\n\nAdd eval strategy"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-05-10T08:52:11Z",
        "message": "Add Weights & Biases integrations\n\nWandb is a module that helps to examine the training process, it would be helpful to add it. The usage of wandb is optional so it will not affect the whole process."
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-04-22T07:25:05Z",
        "message": "add assert"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-04-17T16:35:05Z",
        "message": "fix checks"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-04-17T16:35:05Z",
        "message": "fix circular import and add monkeypatch submodule in setup"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-04-17T16:35:01Z",
        "message": "make things installable, refactor things"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-04-17T06:16:05Z",
        "message": "fix continue training for this version"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-04-17T05:42:50Z",
        "message": "fix bug"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-04-17T04:16:21Z",
        "message": "fix bug when loading old lora model"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-04-13T02:25:05Z",
        "message": "Merge pull request #77 from winglian/upstream-peft\n\nuse monkey patch instead of forked peft"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-04-12T10:54:23Z",
        "message": "addtional fix"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-04-12T04:59:44Z",
        "message": "add xformers support"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-04-11T11:15:56Z",
        "message": "fix bug on v1 finetune"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-04-09T15:40:58Z",
        "message": "use monkey patch instead of forked peft"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-04-09T04:26:22Z",
        "message": "add g_idx support on cuda backend"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-04-07T07:34:06Z",
        "message": "add triton backend support for v2 model"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-04-07T02:40:24Z",
        "message": "merge pull request in new branch"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-04-06T22:43:36Z",
        "message": "Update finetune.py"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-04-06T11:49:12Z",
        "message": "Use flash attention monkeypatch"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-04-05T23:29:36Z",
        "message": "GPTQv2 support\n\nGPTQv2 support.\n1. Adds dependency on `triton`\n2. Refactors autograd_4bit to include both GPTQv1 and GPTQv2\n3. Introduces new environment variable GPTQ_VERSION to select autograd_4bit version\n4. Fixes triton kernels\n5. Matrix multiplications are in fp16"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-04-03T15:55:58Z",
        "message": "update multi gpu support in finetune.py"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-03-31T11:44:36Z",
        "message": "fix device_map bug when using lora_apply_dir"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-03-31T02:40:40Z",
        "message": "fix gpt4all training to more closely match the released logic, other small fixes and optimizations"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-03-29T15:21:47Z",
        "message": "better multi-gpu support, support gpt4all training data"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-03-29T06:35:39Z",
        "message": "add resume checkpoint to continue a training"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-03-29T03:20:16Z",
        "message": "add padding support as an option"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-03-28T13:45:33Z",
        "message": "update finetune data format"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-03-28T13:12:51Z",
        "message": "fix bug"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-03-28T12:33:55Z",
        "message": "add v2 model support"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-03-27T20:08:20Z",
        "message": "backwards support for pre-py3.10, add datasets requirement used in train"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-03-26T03:52:38Z",
        "message": "Tested and should be ready!"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-03-25T04:56:06Z",
        "message": "distributed data parallelism with torchrun"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-03-25T04:03:43Z",
        "message": "model parallelism"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-03-25T02:29:02Z",
        "message": "Merge branch 'main' into finetune-refactor"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-03-24T12:46:03Z",
        "message": "Reflect last changes in main\n\nReflect commits:\nhttps://github.com/johnsmith0031/alpaca_lora_4bit/commit/4906961bf1fd22a5e44b27f926bf3b12775ef384\nhttps://github.com/johnsmith0031/alpaca_lora_4bit/commit/60b227d0ba9ff78378afc8703028448be5bee2f3"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-03-24T11:15:07Z",
        "message": "Refactor finetune.py\n\n1. Add command line arguments support\n2. Add Stanford Alpaca-like dataset support. Used code from - https://github.com/tloen/alpaca-lora\n3. Fix LoRA pre-train application"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-03-23T08:43:18Z",
        "message": "fix minor bug"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-03-23T08:25:29Z",
        "message": "Add gradient checkpointing"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "finetune.py",
        "commit_date": "2023-03-22T04:09:04Z",
        "message": "add more scripts and adjust code for transformer branch"
    },
    {
        "repo_url": "github.com/opendilab/LMDrive",
        "filepath": "LAVIS/lavis/models/drive_models/drive.py",
        "commit_date": "2023-12-13T08:53:56Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/ise-uiuc/magicoder",
        "filepath": "src/magicoder/llm_wrapper.py",
        "commit_date": "2023-12-02T12:41:51Z",
        "message": "feat: dscoder 1.3B and 33B"
    },
    {
        "repo_url": "github.com/ise-uiuc/magicoder",
        "filepath": "src/magicoder/llm_wrapper.py",
        "commit_date": "2023-11-29T05:31:57Z",
        "message": "feat: flash attention and language ablation"
    },
    {
        "repo_url": "github.com/ise-uiuc/magicoder",
        "filepath": "src/magicoder/llm_wrapper.py",
        "commit_date": "2023-11-26T15:17:05Z",
        "message": "fix: ds1000 postprocessing"
    },
    {
        "repo_url": "github.com/ise-uiuc/magicoder",
        "filepath": "src/magicoder/llm_wrapper.py",
        "commit_date": "2023-11-26T09:45:43Z",
        "message": "feat: deepseek-coder based model and ds-1000"
    },
    {
        "repo_url": "github.com/ise-uiuc/magicoder",
        "filepath": "src/magicoder/llm_wrapper.py",
        "commit_date": "2023-11-10T08:23:52Z",
        "message": "refactor: rename from magicode to magicoder"
    },
    {
        "repo_url": "github.com/dvlab-research/LongLoRA",
        "filepath": "merge_lora_weights_and_save_hf_model.py",
        "commit_date": "2023-09-21T18:13:56Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/InternLM/InternLM-XComposer",
        "filepath": "projects/ShareGPT4V/share4v/model/builder.py",
        "commit_date": "2024-01-03T04:39:51Z",
        "message": "fix load model bug"
    },
    {
        "repo_url": "github.com/InternLM/InternLM-XComposer",
        "filepath": "projects/ShareGPT4V/share4v/model/builder.py",
        "commit_date": "2023-12-13T17:41:19Z",
        "message": "full update"
    },
    {
        "repo_url": "github.com/xusenlinzy/api-for-open-llm",
        "filepath": "libs/langchain_llm/langchain_llm/utils.py",
        "commit_date": "2024-01-14T11:01:04Z",
        "message": "Update langchain_llm package"
    },
    {
        "repo_url": "github.com/xusenlinzy/api-for-open-llm",
        "filepath": "libs/langchain_llm/langchain_llm/utils.py",
        "commit_date": "2024-01-13T01:20:56Z",
        "message": "Add apply lora"
    },
    {
        "repo_url": "github.com/lich99/ChatGLM-finetune-LoRA",
        "filepath": "old/train.py",
        "commit_date": "2023-04-13T11:39:43Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/AILab-CVC/GPT4Tools",
        "filepath": "eval/inference.py",
        "commit_date": "2023-12-18T17:47:26Z",
        "message": "update vicuna-v1.5"
    },
    {
        "repo_url": "github.com/AILab-CVC/SEED",
        "filepath": "MultiModalLLM/src/model/peft_model.py",
        "commit_date": "2024-02-24T04:23:21Z",
        "message": "Upload training code"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "peft/src/peft/tuners/lora.py",
        "commit_date": "2023-04-07T11:43:28Z",
        "message": "fixed inference error without loading int8 model"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "peft/src/peft/tuners/lora.py",
        "commit_date": "2023-04-07T11:14:17Z",
        "message": "fixed inference error without loading int8 model"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "peft/src/peft/tuners/lora.py",
        "commit_date": "2023-03-31T12:49:40Z",
        "message": "support bottleneck adapters"
    },
    {
        "repo_url": "github.com/star-whale/starwhale",
        "filepath": "example/LLM/llama/finetune.py",
        "commit_date": "2023-11-22T02:28:00Z",
        "message": "enhance(SDK): tune finetune decorator for Dataset type arguments (#3007)"
    },
    {
        "repo_url": "github.com/star-whale/starwhale",
        "filepath": "example/LLM/llama/finetune.py",
        "commit_date": "2023-11-13T08:01:22Z",
        "message": "feat(sdk): finetune decorator supports auto build and validation datasets (#2962)"
    },
    {
        "repo_url": "github.com/star-whale/starwhale",
        "filepath": "example/LLM/llama/finetune.py",
        "commit_date": "2023-10-25T04:27:41Z",
        "message": "chore(client): enable flake8 bugbear for python code (#2897)"
    },
    {
        "repo_url": "github.com/star-whale/starwhale",
        "filepath": "example/LLM/llama/finetune.py",
        "commit_date": "2023-08-04T02:04:03Z",
        "message": "example(LLM): fix llama finetune build model modules typo (#2587)"
    },
    {
        "repo_url": "github.com/star-whale/starwhale",
        "filepath": "example/LLM/llama/finetune.py",
        "commit_date": "2023-08-02T11:05:32Z",
        "message": "example(LLM): disable save checkpoint for llama finetune (#2585)"
    },
    {
        "repo_url": "github.com/star-whale/starwhale",
        "filepath": "example/LLM/llama/finetune.py",
        "commit_date": "2023-06-14T09:46:16Z",
        "message": "example(LLM): add llama model evaluation and finetune example (#2341)"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "tools/application/chatglm_lora_finetune.py",
        "commit_date": "2023-03-30T09:42:50Z",
        "message": "fix cpp ctx-len"
    },
    {
        "repo_url": "github.com/InternLM/InternLM-XComposer",
        "filepath": "projects/DualFocus/dualfocus/model/builder.py",
        "commit_date": "2024-02-22T11:28:54Z",
        "message": "add DualFocus"
    },
    {
        "repo_url": "github.com/0nutation/SpeechGPT",
        "filepath": "speechgpt/src/infer/cli_infer.py",
        "commit_date": "2023-09-15T11:25:27Z",
        "message": "Update cli_infer.py"
    },
    {
        "repo_url": "github.com/0nutation/SpeechGPT",
        "filepath": "speechgpt/src/infer/cli_infer.py",
        "commit_date": "2023-09-15T11:18:33Z",
        "message": "open-source"
    },
    {
        "repo_url": "github.com/michael-wzhu/PromptCBLUE",
        "filepath": "peft/peft_model.py",
        "commit_date": "2023-07-18T09:37:52Z",
        "message": "2023-07-18: add LlaMA + lora code; add vllm model serving"
    },
    {
        "repo_url": "github.com/HarderThenHarder/transformers_tasks",
        "filepath": "LLM/chatglm_finetune/peft-chatglm/src/peft/peft_model.py",
        "commit_date": "2023-08-02T12:16:49Z",
        "message": "add LLMs Trainer"
    },
    {
        "repo_url": "github.com/star-whale/starwhale",
        "filepath": "example/LLM/llama/evaluation.py",
        "commit_date": "2023-06-14T09:46:16Z",
        "message": "example(LLM): add llama model evaluation and finetune example (#2341)"
    },
    {
        "repo_url": "github.com/visual-openllm/visual-openllm",
        "filepath": "visual_openllm/llm.py",
        "commit_date": "2024-02-22T19:37:47Z",
        "message": "\u589e\u52a0\u5bf9Chatglm3\u7684\u652f\u6301\uff0c\u65b0\u589evqa\u548cpix2pix\u529f\u80fd"
    },
    {
        "repo_url": "github.com/visual-openllm/visual-openllm",
        "filepath": "visual_openllm/llm.py",
        "commit_date": "2024-02-19T19:42:53Z",
        "message": "\u589e\u52a0\u5bf9Chatglm3\u7684\u652f\u6301\uff0c\u65b0\u589evqa\u548cpix2pix\u529f\u80fd"
    },
    {
        "repo_url": "github.com/visual-openllm/visual-openllm",
        "filepath": "visual_openllm/llm.py",
        "commit_date": "2023-03-29T10:25:11Z",
        "message": "Insist on putting model to cuda"
    },
    {
        "repo_url": "github.com/visual-openllm/visual-openllm",
        "filepath": "visual_openllm/llm.py",
        "commit_date": "2023-03-26T14:57:55Z",
        "message": "Add PromptParser to parse model inputs and outputs"
    },
    {
        "repo_url": "github.com/visual-openllm/visual-openllm",
        "filepath": "visual_openllm/llm.py",
        "commit_date": "2023-03-26T13:42:19Z",
        "message": "Set maximum token length to 2048"
    },
    {
        "repo_url": "github.com/visual-openllm/visual-openllm",
        "filepath": "visual_openllm/llm.py",
        "commit_date": "2023-03-26T12:11:52Z",
        "message": "Remove instruction prompt"
    },
    {
        "repo_url": "github.com/visual-openllm/visual-openllm",
        "filepath": "visual_openllm/llm.py",
        "commit_date": "2023-03-26T09:43:26Z",
        "message": "Format code"
    },
    {
        "repo_url": "github.com/visual-openllm/visual-openllm",
        "filepath": "visual_openllm/llm.py",
        "commit_date": "2023-03-26T08:22:57Z",
        "message": "add chatglm\n\nSigned-off-by: mymusise <mymusise1@gmail.com>"
    },
    {
        "repo_url": "github.com/bigcode-project/octopack",
        "filepath": "finetuning/starcoder/merge-peft-adapters.py",
        "commit_date": "2023-08-15T16:19:37Z",
        "message": "add santacoder fine-tuning"
    },
    {
        "repo_url": "github.com/intelligent-machine-learning/dlrover",
        "filepath": "atorch/atorch/trainer/atorch_trainer.py",
        "commit_date": "2024-01-17T09:34:01Z",
        "message": "Merge atorch code (202401a) (#957)\n\n* update atorch 202401a\n\n* ignore E266\n\n* add te in dockerfile\n\n* fix ut\n\n* fix ut"
    },
    {
        "repo_url": "github.com/intelligent-machine-learning/dlrover",
        "filepath": "atorch/atorch/trainer/atorch_trainer.py",
        "commit_date": "2023-12-21T02:48:35Z",
        "message": "sync atorch (#904)\n\n* sync atorch\n\n* update req\n\n* format\n\n* remove file\n\n* fix format\n\n* update\n\n* fix format\n\n* add datasets\n\n* fix\n\n* uplodat\n\n* fix tests\n\n* format"
    },
    {
        "repo_url": "github.com/CarperAI/trlx",
        "filepath": "tests/test_peft.py",
        "commit_date": "2023-06-23T21:55:36Z",
        "message": "peft to opendelta migration (#434) + memory optimization (#320) (#486)\n\n* Migrate to peft from opendelta  for parameter efficient tuning methods (#434) +  Collapse reference+learner hydra heads when using LoRa (#320)\n\n* fix from_config\n\n* Review corrections\n\n* ILQL generate when temperature is 0.\n\n* revert: guard against experimental 8-bit loading support\n\n* format: run `black`\n\n---------\n\nCo-authored-by: jon-tow <jonathantow1@gmail.com>\nCo-authored-by: maxreciprocate <56548574+maxreciprocate@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/star-whale/starwhale",
        "filepath": "example/LLM/vicuna/evaluation.py",
        "commit_date": "2023-06-14T09:43:14Z",
        "message": "example(LLM): add vicuna 7B/13B examples (#2349)"
    },
    {
        "repo_url": "github.com/Meituan-AutoML/MobileVLM",
        "filepath": "scripts/mergelora.py",
        "commit_date": "2024-01-11T12:27:42Z",
        "message": "support finetune with lora"
    },
    {
        "repo_url": "github.com/FlagOpen/FlagEmbedding",
        "filepath": "Long_LLM/activation_beacon/src/activation_beacon_llama/__init__.py",
        "commit_date": "2024-01-28T13:56:39Z",
        "message": "update activation beacon"
    },
    {
        "repo_url": "github.com/huggingface/notebooks",
        "filepath": "sagemaker/24_train_bloom_peft_lora/scripts/run_clm.py",
        "commit_date": "2023-06-02T05:37:19Z",
        "message": "upgrade to peft 0.3.0 and reduced context to train on (#383)"
    },
    {
        "repo_url": "github.com/huggingface/notebooks",
        "filepath": "sagemaker/24_train_bloom_peft_lora/scripts/run_clm.py",
        "commit_date": "2023-04-13T13:59:41Z",
        "message": "Add peft example (#348)\n\n* added peft test\n\n* test\n\n* working example\n\n* finish example"
    },
    {
        "repo_url": "github.com/IlyaGusev/rulm",
        "filepath": "self_instruct/src/util/load.py",
        "commit_date": "2023-10-09T11:28:27Z",
        "message": "Mistral fixes"
    },
    {
        "repo_url": "github.com/IlyaGusev/rulm",
        "filepath": "self_instruct/src/util/load.py",
        "commit_date": "2023-09-17T11:28:13Z",
        "message": "is_lora in load"
    },
    {
        "repo_url": "github.com/IlyaGusev/rulm",
        "filepath": "self_instruct/src/util/load.py",
        "commit_date": "2023-09-14T08:47:06Z",
        "message": "Better chat templates"
    },
    {
        "repo_url": "github.com/IlyaGusev/rulm",
        "filepath": "self_instruct/src/util/load.py",
        "commit_date": "2023-09-02T10:24:21Z",
        "message": "Rm seq2seq mode"
    },
    {
        "repo_url": "github.com/IlyaGusev/rulm",
        "filepath": "self_instruct/src/util/load.py",
        "commit_date": "2023-07-21T12:03:24Z",
        "message": "eval rsg fixes"
    },
    {
        "repo_url": "github.com/IlyaGusev/rulm",
        "filepath": "self_instruct/src/util/load.py",
        "commit_date": "2023-07-03T00:11:17Z",
        "message": "fixes"
    },
    {
        "repo_url": "github.com/IlyaGusev/rulm",
        "filepath": "self_instruct/src/util/load.py",
        "commit_date": "2023-06-07T09:33:20Z",
        "message": "New scripts"
    },
    {
        "repo_url": "github.com/huggingface/notebooks",
        "filepath": "sagemaker/28_train_llms_with_qlora/scripts/run_clm.py",
        "commit_date": "2023-07-18T11:29:07Z",
        "message": "add remote code again."
    },
    {
        "repo_url": "github.com/huggingface/notebooks",
        "filepath": "sagemaker/28_train_llms_with_qlora/scripts/run_clm.py",
        "commit_date": "2023-07-18T08:48:25Z",
        "message": "Fix QLoRA SageMaker Script with saving directly to safetensors.  (#414)\n\n* Update run_clm.py\n\n* Update requirements.txt\n\n* Update requirements.txt\n\n* Update run_clm.py\n\n* Update requirements.txt"
    },
    {
        "repo_url": "github.com/huggingface/notebooks",
        "filepath": "sagemaker/28_train_llms_with_qlora/scripts/run_clm.py",
        "commit_date": "2023-07-13T14:05:44Z",
        "message": "added working example (#410)"
    },
    {
        "repo_url": "github.com/yangjianxin1/Firefly-LLaMA2-Chinese",
        "filepath": "component/utils.py",
        "commit_date": "2023-09-18T16:55:49Z",
        "message": "\u5355\u8f6e\u5bf9\u8bdd & \u591a\u8f6e\u5bf9\u8bdd"
    },
    {
        "repo_url": "github.com/BobaZooba/xllm",
        "filepath": "src/xllm/utils/nn.py",
        "commit_date": "2023-11-15T12:03:41Z",
        "message": "Release: 0.1.1"
    },
    {
        "repo_url": "github.com/BobaZooba/xllm",
        "filepath": "src/xllm/utils/nn.py",
        "commit_date": "2023-11-14T12:57:06Z",
        "message": "more docs"
    },
    {
        "repo_url": "github.com/BobaZooba/xllm",
        "filepath": "src/xllm/utils/nn.py",
        "commit_date": "2023-11-13T09:40:43Z",
        "message": "Init"
    },
    {
        "repo_url": "github.com/haonan-li/CMMLU",
        "filepath": "src/hf_causal_model.py",
        "commit_date": "2023-07-28T16:24:52Z",
        "message": "update ABCD token mapping"
    },
    {
        "repo_url": "github.com/zjunlp/KnowLM",
        "filepath": "examples/generate_lora.py",
        "commit_date": "2023-12-07T01:26:39Z",
        "message": "Update generate_lora.py"
    },
    {
        "repo_url": "github.com/zjunlp/KnowLM",
        "filepath": "examples/generate_lora.py",
        "commit_date": "2023-11-08T04:52:35Z",
        "message": "fix bugs in #96 and support multi-gpu inference"
    },
    {
        "repo_url": "github.com/zjunlp/KnowLM",
        "filepath": "examples/generate_lora.py",
        "commit_date": "2023-10-24T13:26:28Z",
        "message": "fix error about file path"
    },
    {
        "repo_url": "github.com/zjunlp/KnowLM",
        "filepath": "examples/generate_lora.py",
        "commit_date": "2023-10-24T13:13:52Z",
        "message": "fix bug #82 when run in cpu"
    },
    {
        "repo_url": "github.com/zjunlp/KnowLM",
        "filepath": "examples/generate_lora.py",
        "commit_date": "2023-08-13T07:44:19Z",
        "message": "remove lora"
    },
    {
        "repo_url": "github.com/zjunlp/KnowLM",
        "filepath": "examples/generate_lora.py",
        "commit_date": "2023-05-25T06:19:18Z",
        "message": "Update generate_lora.py"
    },
    {
        "repo_url": "github.com/zjunlp/KnowLM",
        "filepath": "examples/generate_lora.py",
        "commit_date": "2023-05-25T04:22:49Z",
        "message": "Update generate_lora.py"
    },
    {
        "repo_url": "github.com/zjunlp/KnowLM",
        "filepath": "examples/generate_lora.py",
        "commit_date": "2023-05-24T15:16:00Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/penghao-wu/vstar",
        "filepath": "LLaVA/llava/model/builder.py",
        "commit_date": "2024-01-07T09:38:33Z",
        "message": "int8 support for vqa llm"
    },
    {
        "repo_url": "github.com/penghao-wu/vstar",
        "filepath": "LLaVA/llava/model/builder.py",
        "commit_date": "2023-12-18T04:32:06Z",
        "message": "init code"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-03-29T13:37:38Z",
        "message": "initial upload"
    },
    {
        "repo_url": "github.com/allenai/open-instruct",
        "filepath": "open_instruct/merge_lora.py",
        "commit_date": "2024-02-06T01:56:24Z",
        "message": "fix for newer bitsandbytes"
    },
    {
        "repo_url": "github.com/allenai/open-instruct",
        "filepath": "open_instruct/merge_lora.py",
        "commit_date": "2023-11-17T10:39:48Z",
        "message": "Fix several issues in the evals. (#79)\n\n* Merging lora on CPU to avoid GPU memory OOM.\n\n* Update requirements.\n\n* Change the alpaca_eval annotator to `alpaca_eval_gpt4`\n\n* Make TyDiQA random example selection deterministic.\n\n* Cache openai request for truthfulqa.\n\n* Add templates for other models.\n\n* More evaluation experiments.\n\n* Avoid loading the entire leaderboard."
    },
    {
        "repo_url": "github.com/allenai/open-instruct",
        "filepath": "open_instruct/merge_lora.py",
        "commit_date": "2023-10-01T20:21:20Z",
        "message": "Refine the lora merging code. (#67)"
    },
    {
        "repo_url": "github.com/allenai/open-instruct",
        "filepath": "open_instruct/merge_lora.py",
        "commit_date": "2023-09-15T22:52:16Z",
        "message": "tokenizer update"
    },
    {
        "repo_url": "github.com/allenai/open-instruct",
        "filepath": "open_instruct/merge_lora.py",
        "commit_date": "2023-09-07T22:30:29Z",
        "message": "Added newer qlora code"
    },
    {
        "repo_url": "github.com/allenai/open-instruct",
        "filepath": "open_instruct/merge_lora.py",
        "commit_date": "2023-08-30T06:12:38Z",
        "message": "first pass on qlora code."
    },
    {
        "repo_url": "github.com/allenai/open-instruct",
        "filepath": "open_instruct/merge_lora.py",
        "commit_date": "2023-06-09T16:25:00Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/examples/finetuning/ppo_pipeline/merge_peft_adapter.py",
        "commit_date": "2023-11-30T12:32:45Z",
        "message": "Update ppo readme (#771)"
    },
    {
        "repo_url": "github.com/Facico/Chinese-Vicuna",
        "filepath": "tools/application/chatglm_lora_test.py",
        "commit_date": "2023-03-30T09:42:50Z",
        "message": "fix cpp ctx-len"
    },
    {
        "repo_url": "github.com/shalfun/DrivingDiffusion",
        "filepath": "diffusers_custom/models/modeling_utils.py",
        "commit_date": "2023-12-15T03:20:13Z",
        "message": "update lib"
    },
    {
        "repo_url": "github.com/The-FinAI/PIXIU",
        "filepath": "src/interface.py",
        "commit_date": "2023-08-21T07:34:37Z",
        "message": "fix bug of self hosted evaluation"
    },
    {
        "repo_url": "github.com/The-FinAI/PIXIU",
        "filepath": "src/interface.py",
        "commit_date": "2023-08-08T01:19:08Z",
        "message": "add bertscore"
    },
    {
        "repo_url": "github.com/The-FinAI/PIXIU",
        "filepath": "src/interface.py",
        "commit_date": "2023-06-11T17:06:07Z",
        "message": "add evaluate and preprocess"
    },
    {
        "repo_url": "github.com/SunzeY/AlphaCLIP",
        "filepath": "demo/with_llm/llava/model/builder.py",
        "commit_date": "2023-12-27T10:10:55Z",
        "message": "add llava deom"
    },
    {
        "repo_url": "github.com/LiuHC0428/LAW-GPT",
        "filepath": "src/peft/src/peft/peft_model.py",
        "commit_date": "2023-04-22T04:46:57Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/CSHaitao/LexiLaw",
        "filepath": "src/infer_lora.py",
        "commit_date": "2023-05-19T07:55:28Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/examples/finetuning/finetune_neuralchat_v3/apply_lora.py",
        "commit_date": "2023-11-22T03:15:37Z",
        "message": "update code for training neuralchat-7b-v3. (#713)\n\n* update code for train neuralchat-7b-v3.\n\n* update README.\n---------\n\nCo-authored-by: VincyZhang <wenxin.zhang@intel.com>"
    },
    {
        "repo_url": "github.com/LiuHC0428/LAW-GPT",
        "filepath": "src/peft/src/peft/tuners/lora.py",
        "commit_date": "2023-04-22T04:46:57Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/OpenGVLab/InternVL",
        "filepath": "internvl_chat_llava/llava/model/builder.py",
        "commit_date": "2024-02-23T07:40:24Z",
        "message": "Fix OOM error during training when using transformers==4.36.2"
    },
    {
        "repo_url": "github.com/OpenGVLab/InternVL",
        "filepath": "internvl_chat_llava/llava/model/builder.py",
        "commit_date": "2024-01-15T13:14:52Z",
        "message": "Rename llava to internvl_chat_llava"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-02-26T03:17:55Z",
        "message": "inverse option added"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-02-12T01:35:15Z",
        "message": "adding Torch Truncated SVD methods"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-02-10T07:24:33Z",
        "message": "adding Torch Truncated SVD methods"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-02-10T07:22:57Z",
        "message": "adding Torch Truncated SVD methods"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-02-10T07:21:33Z",
        "message": "adding Torch Truncated SVD methods"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-02-10T07:17:41Z",
        "message": "adding Torch Truncated SVD methods"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-02-10T07:13:13Z",
        "message": "adding Torch Truncated SVD methods"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-02-10T07:11:15Z",
        "message": "adding Torch Truncated SVD methods"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-02-10T07:07:22Z",
        "message": "adding Torch Truncated SVD methods"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-02-10T06:59:23Z",
        "message": "adding Torch Truncated SVD methods"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-02-10T06:54:08Z",
        "message": "adding Torch Truncated SVD methods"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-02-10T06:41:12Z",
        "message": "adding Torch Truncated SVD methods"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-02-10T06:39:58Z",
        "message": "adding Torch Truncated SVD methods"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-02-10T06:38:07Z",
        "message": "adding Torch Truncated SVD methods"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-02-10T05:47:37Z",
        "message": "adding Torch Truncated SVD methods"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-02-10T04:58:55Z",
        "message": "adding Torch Truncated SVD methods"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-02-10T00:08:17Z",
        "message": "adding Torch Truncated SVD methods"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-02-05T07:45:00Z",
        "message": "LASER release 0.7.4.6"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-02-05T07:44:09Z",
        "message": "LASER release 0.7.4.6"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-01-28T03:11:47Z",
        "message": "prepping for 0.7.4.3 release"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-01-25T23:40:03Z",
        "message": "pushed 0.7.4.2 bug fix"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-01-19T19:12:50Z",
        "message": "updating delta iterator"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-01-19T07:55:46Z",
        "message": "updating delta iterator"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-01-19T07:07:09Z",
        "message": "updating delta iterator"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-01-18T21:55:18Z",
        "message": "updating delta iterator"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-01-18T21:17:03Z",
        "message": "updating delta iterator"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2024-01-13T01:55:14Z",
        "message": "unit tests added for PEFT release coming"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-08-22T22:52:46Z",
        "message": "0.7.3.1 bug in SVD Smoothing for Keras layers"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-07-09T19:50:21Z",
        "message": "first pass for 0.7.3"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-07-06T23:17:37Z",
        "message": "first push for 0.7.3"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-06-16T16:31:16Z",
        "message": "added layer_map file for safetensors"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-06-15T06:01:58Z",
        "message": "debugging 0.7.2"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-06-14T15:43:35Z",
        "message": "minor bug fix in tests"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-06-14T00:03:38Z",
        "message": "added WWDeltaLayerIteator, stil testing"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-06-13T05:18:54Z",
        "message": "working on 0.7.2 release"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-06-12T22:48:25Z",
        "message": "changelog for 0.7.1.8"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-06-11T15:46:23Z",
        "message": "added supported for list of safetensors; still debugging"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-06-05T04:06:59Z",
        "message": "0.7.1.7"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-05-24T21:46:08Z",
        "message": "issue 255"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-04-11T05:19:50Z",
        "message": "@43 debugging lazy loading"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-04-10T23:19:26Z",
        "message": "@43 debugging lazy loading"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-04-10T18:51:53Z",
        "message": "Debugging #243 on Google Colab"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-04-03T05:21:24Z",
        "message": "deugging #238"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-23T20:58:49Z",
        "message": "issues #230 & #231"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-23T01:26:06Z",
        "message": "issue #229"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-22T22:26:41Z",
        "message": "issue #229"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-22T21:56:31Z",
        "message": "issue #229"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-22T21:34:07Z",
        "message": "issue #229"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-18T16:44:47Z",
        "message": "removed best fit for issue 224"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-18T06:35:27Z",
        "message": "changed DEFAULT_MIB_EVALS = 10"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-18T04:56:53Z",
        "message": "changed DEFAULT_MIB_EVALS = 10"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-18T04:22:55Z",
        "message": "issue 223"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-18T04:08:06Z",
        "message": "issue 223"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-18T02:47:39Z",
        "message": "cleaning up, fixing error introduceds"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-18T00:44:26Z",
        "message": "trying to reduce memory on PyTorch Layers"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-15T20:57:36Z",
        "message": "added Mac M1/M2 swtich to numpy"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-11T04:32:21Z",
        "message": "addressing issue 217"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-10T16:42:25Z",
        "message": "fixed Test_PyStateDictFileLayers.test_temp"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-09T23:54:59Z",
        "message": "Tests for PyStateDictFile added, mostly working"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-09T23:25:29Z",
        "message": "Tests for PyStateDictFile added, mostly working"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-09T21:49:47Z",
        "message": "Tests for PyStateDictFile added, mostly working"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-09T19:05:50Z",
        "message": "working on Test_PyStateDictFileLayers; more bug fixes"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-09T18:58:03Z",
        "message": "working on Test_PyStateDictFileLayers; more bug fixes"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-09T16:23:17Z",
        "message": "working on Test_PyStateDictFileLayers; bug fix"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-09T07:17:59Z",
        "message": "working on Test_PyStateDictFileLayers"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-09T00:08:50Z",
        "message": "added unit tests for PyStateDictFileLayers"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-08T17:20:13Z",
        "message": "reverted conv2d_fft"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-04T02:28:18Z",
        "message": "fixed 2 unit tests:test_get_framework_layer, test_density_fit_on_randomized_W"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-03T19:22:29Z",
        "message": "fixed 2 unit tests:test_get_framework_layer, test_density_fit_on_randomized_W"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-03T07:01:15Z",
        "message": "Fixed fc_layers member in SVD test"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-03T07:01:15Z",
        "message": "Typo fix. (Test still fails)"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-03T07:01:15Z",
        "message": "typo fix"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-03T07:01:15Z",
        "message": "Relaxed Albert tests and set loglevel to WARNING where it had been INFO"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-03T07:01:15Z",
        "message": "Renamed test classes to conform to convention, make tests distinguishable"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-03T07:01:15Z",
        "message": "self.assertEquals is deprecated and generates a warning. Use assertEqual"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-03T07:01:15Z",
        "message": "Added 'detX_val_unrescaled' column, and a test for detX columns"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-03T05:48:36Z",
        "message": "added test_pl_directly, 213"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-03T01:38:20Z",
        "message": "closing various issues"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-02T23:31:10Z",
        "message": "fixed issue #159"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-02T17:57:55Z",
        "message": "ww2x flag depcreated, changed to pool"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-02T06:18:48Z",
        "message": "debugging fix_fingers"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-02T00:24:56Z",
        "message": "debugging fix_fingers"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-01T22:09:18Z",
        "message": "debugging fix_fingers"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-01T18:23:27Z",
        "message": "bug in main code"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-01T18:00:16Z",
        "message": "fixed xmax not being passed from fit_clipped_powerlaw"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-03-01T02:08:12Z",
        "message": "apply_powerlaw() got renamed by accident to _powerlaw"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-02-28T08:10:54Z",
        "message": "user can now specify xmax=-1, useful for single layers"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-02-28T08:06:38Z",
        "message": "debugging conv2d fft"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-02-28T07:11:06Z",
        "message": "user can now specify xmax=-1, useful for single layers"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-02-28T05:32:47Z",
        "message": "debugging fix fingers for 0.7 release"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-02-27T21:52:22Z",
        "message": "debugging 0.6.6"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-02-25T00:26:09Z",
        "message": "0.6.5 alpha tests not moved correctly"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-02-24T05:31:59Z",
        "message": "added test_keras_model_with_no_bias to TestKeras for issue #201"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-02-24T04:01:21Z",
        "message": "going to merge into 0.6.5 as preview for 0.6"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-02-23T23:10:16Z",
        "message": "cleaning up unit tests"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-02-23T20:31:23Z",
        "message": "moved alpha calcualtions to seperate VGG test"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-02-23T19:58:43Z",
        "message": "moved alpha calcualtions to seperate VGG test"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-02-23T18:42:21Z",
        "message": "changed VGG tests to use older version tests"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-02-23T07:22:09Z",
        "message": "changed VGG tests to use older version tests"
    },
    {
        "repo_url": "github.com/CalculatedContent/WeightWatcher",
        "filepath": "tests/test.py",
        "commit_date": "2023-02-23T07:16:16Z",
        "message": "changed VGG tests to use older version tests"
    },
    {
        "repo_url": "github.com/CSHaitao/LexiLaw",
        "filepath": "inference_lora.py",
        "commit_date": "2023-05-20T11:37:08Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/CSHaitao/LexiLaw",
        "filepath": "inference_lora.py",
        "commit_date": "2023-05-20T07:57:18Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/danielgross/LlamaAcademy",
        "filepath": "export_hf.py",
        "commit_date": "2023-04-19T16:28:29Z",
        "message": "inital"
    },
    {
        "repo_url": "github.com/ztxz16/fastllm",
        "filepath": "test/cmmlu/qwen.py",
        "commit_date": "2023-08-16T02:38:30Z",
        "message": "\u4fee\u590dQWenBase\u6a21\u578bgenerate\u65f6\u505c\u4e0d\u4e0b\u6765\u7684\u95ee\u9898\uff1b\n\u589e\u52a0QWen-7B\u6a21\u578b\u7684cmmlu\u6d4b\u8bd5\u7ed3\u679c"
    },
    {
        "repo_url": "github.com/agiresearch/OpenAGI",
        "filepath": "benchmark_tasks/generate_model_seq_llama.py",
        "commit_date": "2024-02-20T05:53:17Z",
        "message": " fix: decoder-only models' constraint generation score calculation"
    },
    {
        "repo_url": "github.com/agiresearch/OpenAGI",
        "filepath": "benchmark_tasks/generate_model_seq_llama.py",
        "commit_date": "2023-05-26T18:46:24Z",
        "message": "add llama/vicuna finetune"
    },
    {
        "repo_url": "github.com/HarderThenHarder/transformers_tasks",
        "filepath": "LLM/chatglm_finetune/peft-chatglm/src/peft/tuners/prompt_tuning.py",
        "commit_date": "2023-08-02T12:16:49Z",
        "message": "add LLMs Trainer"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "peft/src/peft/tuners/bottleneck.py",
        "commit_date": "2023-04-10T07:57:10Z",
        "message": "support ChatGLM"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "peft/src/peft/tuners/bottleneck.py",
        "commit_date": "2023-04-07T11:14:17Z",
        "message": "fixed inference error without loading int8 model"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "peft/src/peft/tuners/bottleneck.py",
        "commit_date": "2023-04-05T11:37:06Z",
        "message": "for easier using of AdapterP"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "peft/src/peft/tuners/bottleneck.py",
        "commit_date": "2023-04-05T11:18:41Z",
        "message": "support gpt-neo for bottlenck and parallel adapters"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "peft/src/peft/tuners/bottleneck.py",
        "commit_date": "2023-04-04T02:44:45Z",
        "message": "fixed a forward bug with autocast for bottleneck adapters"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "peft/src/peft/tuners/bottleneck.py",
        "commit_date": "2023-04-01T19:27:16Z",
        "message": "support opt and solve autocast bug"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "peft/src/peft/tuners/bottleneck.py",
        "commit_date": "2023-04-01T12:47:14Z",
        "message": "support bottleneck adapters for BLOOM"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "peft/src/peft/tuners/bottleneck.py",
        "commit_date": "2023-04-01T11:59:26Z",
        "message": "support parallel adapter and gpt-j model"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "peft/src/peft/tuners/bottleneck.py",
        "commit_date": "2023-03-31T12:49:40Z",
        "message": "support bottleneck adapters"
    },
    {
        "repo_url": "github.com/beyondguo/LLM-Tuning",
        "filepath": "RLHF/rl_training.py",
        "commit_date": "2023-07-20T06:57:45Z",
        "message": "baichuan-rlhf!"
    },
    {
        "repo_url": "github.com/beyondguo/LLM-Tuning",
        "filepath": "RLHF/rl_training.py",
        "commit_date": "2023-07-13T07:51:03Z",
        "message": "rlhf with baichuan"
    },
    {
        "repo_url": "github.com/beyondguo/LLM-Tuning",
        "filepath": "RLHF/rl_training.py",
        "commit_date": "2023-07-12T09:58:16Z",
        "message": "fix a sample-related bug"
    },
    {
        "repo_url": "github.com/beyondguo/LLM-Tuning",
        "filepath": "RLHF/rl_training.py",
        "commit_date": "2023-07-10T14:36:27Z",
        "message": "fix"
    },
    {
        "repo_url": "github.com/beyondguo/LLM-Tuning",
        "filepath": "RLHF/rl_training.py",
        "commit_date": "2023-07-09T02:36:13Z",
        "message": "make it runable"
    },
    {
        "repo_url": "github.com/beyondguo/LLM-Tuning",
        "filepath": "RLHF/rl_training.py",
        "commit_date": "2023-07-08T15:22:16Z",
        "message": "custom for baichuan"
    },
    {
        "repo_url": "github.com/beyondguo/LLM-Tuning",
        "filepath": "RLHF/rl_training.py",
        "commit_date": "2023-07-06T11:46:07Z",
        "message": "code reading with some comments"
    },
    {
        "repo_url": "github.com/intel/intel-extension-for-transformers",
        "filepath": "intel_extension_for_transformers/neural_chat/examples/finetuning/multi_modal/eval/mmmu_eval/run_llava.py",
        "commit_date": "2024-02-06T11:56:39Z",
        "message": "[NeuralChat] Enable llava mmmu evaluation on Gaudi2. (#1259)"
    },
    {
        "repo_url": "github.com/shalfun/DrivingDiffusion",
        "filepath": "diffusers_custom/pipelines/pipeline_utils.py",
        "commit_date": "2023-12-15T03:20:13Z",
        "message": "update lib"
    },
    {
        "repo_url": "github.com/ztxz16/fastllm",
        "filepath": "test/cmmlu/baichuan.py",
        "commit_date": "2023-08-13T16:48:01Z",
        "message": "update test for baichuan"
    },
    {
        "repo_url": "github.com/SeungyounShin/Llama2-Code-Interpreter",
        "filepath": "eval/inference.py",
        "commit_date": "2023-08-21T18:58:29Z",
        "message": "Feat (Finetunning&Eval) add human-eval"
    },
    {
        "repo_url": "github.com/SeungyounShin/Llama2-Code-Interpreter",
        "filepath": "eval/inference.py",
        "commit_date": "2023-08-21T18:39:43Z",
        "message": "Feat (Eval) add human-eval"
    },
    {
        "repo_url": "github.com/SeungyounShin/Llama2-Code-Interpreter",
        "filepath": "eval/inference.py",
        "commit_date": "2023-08-12T14:28:59Z",
        "message": "Feat (Llama2 Finetuning) Add inference code for test"
    },
    {
        "repo_url": "github.com/SeungyounShin/Llama2-Code-Interpreter",
        "filepath": "eval/inference.py",
        "commit_date": "2023-08-10T18:27:07Z",
        "message": "Feat (Llama2 Finetuning) Add Fully working Finetunning and Inference code"
    },
    {
        "repo_url": "github.com/codefuse-ai/codefuse-devops-eval",
        "filepath": "src/models/base_model.py",
        "commit_date": "2023-12-13T06:24:20Z",
        "message": "add funccall evalution features"
    },
    {
        "repo_url": "github.com/SkunkworksAI/BakLLaVA",
        "filepath": "llava/model/builder.py",
        "commit_date": "2023-10-10T23:07:33Z",
        "message": "Mistral"
    },
    {
        "repo_url": "github.com/SkunkworksAI/BakLLaVA",
        "filepath": "llava/model/builder.py",
        "commit_date": "2023-10-10T20:48:22Z",
        "message": "a"
    },
    {
        "repo_url": "github.com/codefuse-ai/codefuse-devops-eval",
        "filepath": "src/models/qwen_model.py",
        "commit_date": "2023-12-13T06:24:20Z",
        "message": "add funccall evalution features"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "peft/src/peft/tuners/prompt_tuning.py",
        "commit_date": "2023-03-31T12:49:40Z",
        "message": "support bottleneck adapters"
    },
    {
        "repo_url": "github.com/CSHaitao/LexiLaw",
        "filepath": "inference_ptuning.py",
        "commit_date": "2023-05-20T11:37:08Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/CSHaitao/LexiLaw",
        "filepath": "inference_ptuning.py",
        "commit_date": "2023-05-20T07:57:18Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/AutoGPTQ/AutoGPTQ",
        "filepath": "tests/test_peft_conversion.py",
        "commit_date": "2024-02-13T16:06:07Z",
        "message": "Use ruff for linting (#537)\n\n* lint\n\n* add back init\n\n* remove pyproject.toml that automatically triggers build isolation\n\n* install torch\n\n* maybe use 3.9?\n\n* debug\n\n* upgrade setuptools\n\n* maybe ubuntu 22.04?\n\n* wheel\n\n* working now?\n\n* typog\n\n* indent\n\n* fix indent\n\n* do not use powershell\n\n* free space\n\n* fix cuda path\n\n* prints\n\n* where is conda?\n\n* should finally work\n\n* fix\n\n* final fixN\n\n* arch ist\n\n* typog\n\n* add quality extra\n\n* last fix"
    },
    {
        "repo_url": "github.com/AutoGPTQ/AutoGPTQ",
        "filepath": "tests/test_peft_conversion.py",
        "commit_date": "2023-10-27T07:12:16Z",
        "message": "PEFT initialization fix (#361)\n\n* Initial code for GPTQLoraLinear initialization\n\n* Working AdaLora\n\n* remove unused methods and pin peft\n\n---------\n\nCo-authored-by: Felix Marty <9808326+fxmarty@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/abacusai/Long-Context",
        "filepath": "python/eval/longeval/utils.py",
        "commit_date": "2023-07-27T21:16:33Z",
        "message": "Add eval and benchmark files for Lines and WikiQA"
    },
    {
        "repo_url": "github.com/star-whale/starwhale",
        "filepath": "example/llm-leaderboard/src/llm/base.py",
        "commit_date": "2024-02-05T07:30:03Z",
        "message": "example(LLM): add mistral 7b+vllm example for llm-leaderboard (#3133)"
    },
    {
        "repo_url": "github.com/star-whale/starwhale",
        "filepath": "example/llm-leaderboard/src/llm/base.py",
        "commit_date": "2023-08-30T02:22:10Z",
        "message": "example(LLM): tune llm leaderboard for explanaion and readme (#2670)"
    },
    {
        "repo_url": "github.com/star-whale/starwhale",
        "filepath": "example/llm-leaderboard/src/llm/base.py",
        "commit_date": "2023-08-25T06:08:05Z",
        "message": "example(LLM): support llama2 chinese models (#2648)"
    },
    {
        "repo_url": "github.com/star-whale/starwhale",
        "filepath": "example/llm-leaderboard/src/llm/base.py",
        "commit_date": "2023-08-23T02:52:01Z",
        "message": "example(LLM): add llm leaderboard examples (#2610)"
    },
    {
        "repo_url": "github.com/shibing624/textgen",
        "filepath": "textgen/gpt/merge_peft_adapter.py",
        "commit_date": "2023-11-05T06:51:05Z",
        "message": "update merge."
    },
    {
        "repo_url": "github.com/shibing624/textgen",
        "filepath": "textgen/gpt/merge_peft_adapter.py",
        "commit_date": "2023-11-03T04:40:29Z",
        "message": "update merge."
    },
    {
        "repo_url": "github.com/shibing624/textgen",
        "filepath": "textgen/gpt/merge_peft_adapter.py",
        "commit_date": "2023-09-05T14:32:26Z",
        "message": "update ddp env."
    },
    {
        "repo_url": "github.com/shibing624/textgen",
        "filepath": "textgen/gpt/merge_peft_adapter.py",
        "commit_date": "2023-09-04T12:34:17Z",
        "message": "update gpt merge adapter."
    },
    {
        "repo_url": "github.com/shibing624/textgen",
        "filepath": "textgen/gpt/merge_peft_adapter.py",
        "commit_date": "2023-06-19T08:05:33Z",
        "message": "fixed https://github.com/shibing624/textgen/issues/46"
    },
    {
        "repo_url": "github.com/shibing624/textgen",
        "filepath": "textgen/gpt/merge_peft_adapter.py",
        "commit_date": "2023-06-16T11:54:21Z",
        "message": "update gpt model."
    },
    {
        "repo_url": "github.com/MeetKai/functionary",
        "filepath": "functionary/train/merge_lora_weight.py",
        "commit_date": "2023-11-27T04:31:25Z",
        "message": "fix issues based on comments from Jeffrey"
    },
    {
        "repo_url": "github.com/MeetKai/functionary",
        "filepath": "functionary/train/merge_lora_weight.py",
        "commit_date": "2023-10-31T06:30:19Z",
        "message": "clean code"
    },
    {
        "repo_url": "github.com/MeetKai/functionary",
        "filepath": "functionary/train/merge_lora_weight.py",
        "commit_date": "2023-10-20T07:42:28Z",
        "message": "update inference code"
    },
    {
        "repo_url": "github.com/MeetKai/functionary",
        "filepath": "functionary/train/merge_lora_weight.py",
        "commit_date": "2023-10-16T06:53:06Z",
        "message": "update train_lora"
    },
    {
        "repo_url": "github.com/haonan-li/CMMLU",
        "filepath": "src/chinese_llama_alpaca.py",
        "commit_date": "2023-07-28T16:24:52Z",
        "message": "update ABCD token mapping"
    },
    {
        "repo_url": "github.com/haonan-li/CMMLU",
        "filepath": "src/chinese_llama_alpaca.py",
        "commit_date": "2023-06-16T12:48:28Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/horseee/DeepCache",
        "filepath": "DeepCache/svd/pipeline_utils.py",
        "commit_date": "2023-12-20T21:05:12Z",
        "message": "[ADD] code for SVD"
    },
    {
        "repo_url": "github.com/Duxiaoman-DI/XuanYuan",
        "filepath": "FinanceIQ/src/hf_causal_model.py",
        "commit_date": "2023-09-22T13:24:58Z",
        "message": "add FinanceIQ code"
    },
    {
        "repo_url": "github.com/pjlab-sys4nlp/llama-moe",
        "filepath": "smoe/utils/merge_llama_with_lora.py",
        "commit_date": "2023-08-01T02:51:06Z",
        "message": "update format"
    },
    {
        "repo_url": "github.com/pjlab-sys4nlp/llama-moe",
        "filepath": "smoe/utils/merge_llama_with_lora.py",
        "commit_date": "2023-07-27T06:53:19Z",
        "message": "update fpt scripts"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "examples/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py",
        "commit_date": "2024-02-28T14:11:40Z",
        "message": "align the parameters between 2x and 3x example (#1636)\n\nSigned-off-by: chensuyue <suyue.chen@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "examples/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py",
        "commit_date": "2024-01-28T12:55:46Z",
        "message": "Remove gptq_debug options in examples (#1569)\n\nSigned-off-by: YIYANGCAI <yiyang.cai@intel.com>\nSigned-off-by: chensuyue <suyue.chen@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "examples/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py",
        "commit_date": "2024-01-23T12:45:27Z",
        "message": "Support static_groups options in GPTQ API (#1478)\n\nSigned-off-by: YIYANGCAI <yiyang.cai@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "examples/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py",
        "commit_date": "2024-01-17T01:55:31Z",
        "message": "Modify gptq example logic (#1545)\n\nSigned-off-by: YIYANGCAI <yiyang.cai@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "examples/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py",
        "commit_date": "2024-01-12T05:12:38Z",
        "message": "fix trust remote for llm examples (#1537)"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "examples/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py",
        "commit_date": "2023-12-28T02:31:16Z",
        "message": "add code-generaion evaluation for woq gptq (#1475)\n\nSigned-off-by: changwangss <chang1.wang@intel.com>\nSigned-off-by: YIYANGCAI <yiyang.cai@intel.com>\nSigned-off-by: chensuyue <suyue.chen@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "examples/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py",
        "commit_date": "2023-12-15T07:05:37Z",
        "message": "format fix for llm example scripts (#1474)\n\nSigned-off-by: chensuyue <suyue.chen@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "examples/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py",
        "commit_date": "2023-12-06T13:00:27Z",
        "message": "add llm evaluate for language modeling (#1350)\n\nSigned-off-by: Xin He <xin3.he@intel.com>\nSigned-off-by: YIYANGCAI <yiyang.cai@intel.com>\nSigned-off-by: chensuyue <suyue.chen@intel.com>"
    },
    {
        "repo_url": "github.com/datadreamer-dev/DataDreamer",
        "filepath": "src/llms/hf_transformers.py",
        "commit_date": "2024-02-02T23:53:40Z",
        "message": "Update docs [release]"
    },
    {
        "repo_url": "github.com/datadreamer-dev/DataDreamer",
        "filepath": "src/llms/hf_transformers.py",
        "commit_date": "2024-02-02T04:00:55Z",
        "message": "Update docs [release]"
    },
    {
        "repo_url": "github.com/datadreamer-dev/DataDreamer",
        "filepath": "src/llms/hf_transformers.py",
        "commit_date": "2024-02-01T03:26:02Z",
        "message": "Release commit [release]"
    },
    {
        "repo_url": "github.com/michael-wzhu/PromptCBLUE",
        "filepath": "src/ft_chatglm_lora/main.py",
        "commit_date": "2023-07-12T09:55:23Z",
        "message": "2023-07-12: change generation config for batched generation"
    },
    {
        "repo_url": "github.com/michael-wzhu/PromptCBLUE",
        "filepath": "src/ft_chatglm_lora/main.py",
        "commit_date": "2023-07-12T06:41:53Z",
        "message": "2023-07-12: change generation config for batched generation"
    },
    {
        "repo_url": "github.com/michael-wzhu/PromptCBLUE",
        "filepath": "src/ft_chatglm_lora/main.py",
        "commit_date": "2023-07-03T02:56:20Z",
        "message": "2023-07-03: update src/data/CBLUE\u4efb\u52a1\u6539\u9020\u8bf4\u660e\u4e0e\u4e3e\u4f8b.md"
    },
    {
        "repo_url": "github.com/michael-wzhu/PromptCBLUE",
        "filepath": "src/ft_chatglm_lora/main.py",
        "commit_date": "2023-06-26T10:20:19Z",
        "message": "2023-06-26: update train & eval for p-tuning and lora"
    },
    {
        "repo_url": "github.com/michael-wzhu/PromptCBLUE",
        "filepath": "src/ft_chatglm_lora/main.py",
        "commit_date": "2023-06-26T10:19:53Z",
        "message": "2023-06-26: update train & eval for p-tuning"
    },
    {
        "repo_url": "github.com/michael-wzhu/PromptCBLUE",
        "filepath": "src/ft_chatglm_lora/main.py",
        "commit_date": "2023-06-26T10:06:03Z",
        "message": "2023-06-26: update train & eval for p-tuning"
    },
    {
        "repo_url": "github.com/michael-wzhu/PromptCBLUE",
        "filepath": "src/ft_chatglm_lora/main.py",
        "commit_date": "2023-05-27T03:27:07Z",
        "message": "2023/05/27: update main.py; add main_offline.py"
    },
    {
        "repo_url": "github.com/michael-wzhu/PromptCBLUE",
        "filepath": "src/ft_chatglm_lora/main.py",
        "commit_date": "2023-05-27T03:23:46Z",
        "message": "2023/05/27: update main.py; add main_offline.py"
    },
    {
        "repo_url": "github.com/michael-wzhu/PromptCBLUE",
        "filepath": "src/ft_chatglm_lora/main.py",
        "commit_date": "2023-05-27T03:22:03Z",
        "message": "2023/05/21: update wechat qr code"
    },
    {
        "repo_url": "github.com/michael-wzhu/PromptCBLUE",
        "filepath": "src/ft_chatglm_lora/main.py",
        "commit_date": "2023-05-19T07:57:08Z",
        "message": "2023-05-19: modify data proc for eval; change dingding group qrcode;"
    },
    {
        "repo_url": "github.com/michael-wzhu/PromptCBLUE",
        "filepath": "src/ft_chatglm_lora/main.py",
        "commit_date": "2023-05-10T05:37:48Z",
        "message": "2023-05-10: add code for chatglm+lora ft"
    },
    {
        "repo_url": "github.com/michael-wzhu/PromptCBLUE",
        "filepath": "src/ft_chatglm_lora/main.py",
        "commit_date": "2023-05-10T01:45:41Z",
        "message": "2023-05-10: add qr code"
    },
    {
        "repo_url": "github.com/zjunlp/KnowLM",
        "filepath": "examples/generate_lora_web.py",
        "commit_date": "2023-12-06T09:10:52Z",
        "message": "fix issue #99,"
    },
    {
        "repo_url": "github.com/zjunlp/KnowLM",
        "filepath": "examples/generate_lora_web.py",
        "commit_date": "2023-11-08T04:52:35Z",
        "message": "fix bugs in #96 and support multi-gpu inference"
    },
    {
        "repo_url": "github.com/zjunlp/KnowLM",
        "filepath": "examples/generate_lora_web.py",
        "commit_date": "2023-11-03T08:45:53Z",
        "message": "fix bug #96"
    },
    {
        "repo_url": "github.com/zjunlp/KnowLM",
        "filepath": "examples/generate_lora_web.py",
        "commit_date": "2023-10-13T12:32:27Z",
        "message": "fix bugs #81 when run in cpu"
    },
    {
        "repo_url": "github.com/zjunlp/KnowLM",
        "filepath": "examples/generate_lora_web.py",
        "commit_date": "2023-08-13T07:44:53Z",
        "message": "Update generate_lora_web.py"
    },
    {
        "repo_url": "github.com/zjunlp/KnowLM",
        "filepath": "examples/generate_lora_web.py",
        "commit_date": "2023-06-12T14:32:16Z",
        "message": "Update generate_lora_web.py"
    },
    {
        "repo_url": "github.com/zjunlp/KnowLM",
        "filepath": "examples/generate_lora_web.py",
        "commit_date": "2023-05-30T08:02:19Z",
        "message": "Update generate_lora_web.py"
    },
    {
        "repo_url": "github.com/zjunlp/KnowLM",
        "filepath": "examples/generate_lora_web.py",
        "commit_date": "2023-05-29T14:38:51Z",
        "message": "Update generate_lora_web.py"
    },
    {
        "repo_url": "github.com/zjunlp/KnowLM",
        "filepath": "examples/generate_lora_web.py",
        "commit_date": "2023-05-25T04:19:23Z",
        "message": "Create generate_lora_web.py"
    },
    {
        "repo_url": "github.com/LiuHC0428/LAW-GPT",
        "filepath": "src/peft/src/peft/tuners/bottleneck.py",
        "commit_date": "2023-04-22T04:46:57Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/LC1332/Chat-Haruhi-Suzumiya",
        "filepath": "ChatHaruhi2.0/ChatHaruhi/ChatGLM2GPT.py",
        "commit_date": "2023-11-01T04:54:05Z",
        "message": "fix initializing bug"
    },
    {
        "repo_url": "github.com/LC1332/Chat-Haruhi-Suzumiya",
        "filepath": "ChatHaruhi2.0/ChatHaruhi/ChatGLM2GPT.py",
        "commit_date": "2023-10-24T06:23:28Z",
        "message": "remove all jsonl that used in Haruhi 1.0, update into 2.0 , the jsonl will still kept at Legacy-Haruhi-1.0 project"
    },
    {
        "repo_url": "github.com/IlyaGusev/rulm",
        "filepath": "self_instruct/src/tools/merge_lora.py",
        "commit_date": "2023-10-30T10:28:14Z",
        "message": "Update merge_lora.py"
    },
    {
        "repo_url": "github.com/IlyaGusev/rulm",
        "filepath": "self_instruct/src/tools/merge_lora.py",
        "commit_date": "2023-09-09T14:23:42Z",
        "message": "Flake8 refactoring"
    },
    {
        "repo_url": "github.com/IlyaGusev/rulm",
        "filepath": "self_instruct/src/tools/merge_lora.py",
        "commit_date": "2023-09-09T13:55:42Z",
        "message": "Removing unused scripts"
    },
    {
        "repo_url": "github.com/zjunlp/KnowLM",
        "filepath": "tools/export_hf_checkpoint.py",
        "commit_date": "2023-07-01T08:40:46Z",
        "message": "doc: add vLLM serving doc"
    },
    {
        "repo_url": "github.com/michael-wzhu/Chinese-LlaMA2",
        "filepath": "peft/tuners/prompt_tuning.py",
        "commit_date": "2023-07-19T06:57:59Z",
        "message": "2023/07/19 add further pretraining code"
    },
    {
        "repo_url": "github.com/yangjianxin1/Firefly-LLaMA2-Chinese",
        "filepath": "script/merge_lora.py",
        "commit_date": "2023-09-18T17:00:07Z",
        "message": "\u5408\u5e76\u6a21\u578b\u6743\u91cd"
    },
    {
        "repo_url": "github.com/datadreamer-dev/DataDreamer",
        "filepath": "src/trainers/train_hf_ppo.py",
        "commit_date": "2024-02-01T03:26:02Z",
        "message": "Release commit [release]"
    },
    {
        "repo_url": "github.com/basetenlabs/alpaca-7b-truss",
        "filepath": "model/model.py",
        "commit_date": "2023-05-15T15:18:11Z",
        "message": "Update model.py to use 2 beams instead of 4"
    },
    {
        "repo_url": "github.com/basetenlabs/alpaca-7b-truss",
        "filepath": "model/model.py",
        "commit_date": "2023-04-27T04:36:46Z",
        "message": "Use docal data"
    },
    {
        "repo_url": "github.com/basetenlabs/alpaca-7b-truss",
        "filepath": "model/model.py",
        "commit_date": "2023-03-22T04:50:21Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/beyondguo/LLM-Tuning",
        "filepath": "web_demo/src/toolkit.py",
        "commit_date": "2023-07-25T05:02:02Z",
        "message": "web_demo"
    },
    {
        "repo_url": "github.com/airaria/Visual-Chinese-LLaMA-Alpaca",
        "filepath": "scripts/inference/inference.py",
        "commit_date": "2023-07-14T02:24:35Z",
        "message": "support pure text instruction inference"
    },
    {
        "repo_url": "github.com/airaria/Visual-Chinese-LLaMA-Alpaca",
        "filepath": "scripts/inference/inference.py",
        "commit_date": "2023-07-07T03:34:16Z",
        "message": "update README.md and inference.py"
    },
    {
        "repo_url": "github.com/airaria/Visual-Chinese-LLaMA-Alpaca",
        "filepath": "scripts/inference/inference.py",
        "commit_date": "2023-07-07T02:27:55Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/michael-wzhu/PromptCBLUE",
        "filepath": "peft/tuners/prompt_tuning.py",
        "commit_date": "2023-07-18T09:37:52Z",
        "message": "2023-07-18: add LlaMA + lora code; add vllm model serving"
    },
    {
        "repo_url": "github.com/CSHaitao/LexiLaw",
        "filepath": "inference_finetune.py",
        "commit_date": "2023-05-20T11:37:08Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/CSHaitao/LexiLaw",
        "filepath": "inference_finetune.py",
        "commit_date": "2023-05-20T07:57:18Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "src/alpaca_lora_4bit/autograd_4bit.py",
        "commit_date": "2023-08-02T10:08:48Z",
        "message": "fix mat size issue"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "src/alpaca_lora_4bit/autograd_4bit.py",
        "commit_date": "2023-06-26T08:36:00Z",
        "message": "Use zero initializer for bias weights"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "src/alpaca_lora_4bit/autograd_4bit.py",
        "commit_date": "2023-06-05T09:26:14Z",
        "message": "fix bug"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "src/alpaca_lora_4bit/autograd_4bit.py",
        "commit_date": "2023-06-05T09:21:43Z",
        "message": "add auto apply monkeypatch when loading lora"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "src/alpaca_lora_4bit/autograd_4bit.py",
        "commit_date": "2023-06-02T07:47:08Z",
        "message": "switch to older version of faster kernel as default"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "src/alpaca_lora_4bit/autograd_4bit.py",
        "commit_date": "2023-06-02T02:19:22Z",
        "message": "set disable_bias=False as default"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "src/alpaca_lora_4bit/autograd_4bit.py",
        "commit_date": "2023-05-30T02:29:52Z",
        "message": "add 2bit support"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "src/alpaca_lora_4bit/autograd_4bit.py",
        "commit_date": "2023-04-22T15:01:39Z",
        "message": "optimized matmul for v2 model"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "src/alpaca_lora_4bit/autograd_4bit.py",
        "commit_date": "2023-04-22T12:53:52Z",
        "message": "fix bug"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "src/alpaca_lora_4bit/autograd_4bit.py",
        "commit_date": "2023-04-22T07:23:43Z",
        "message": "fix bug and remove bnb"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "src/alpaca_lora_4bit/autograd_4bit.py",
        "commit_date": "2023-04-19T10:49:58Z",
        "message": "fix bug"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "src/alpaca_lora_4bit/autograd_4bit.py",
        "commit_date": "2023-04-17T16:47:53Z",
        "message": "fix gptq install"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "src/alpaca_lora_4bit/autograd_4bit.py",
        "commit_date": "2023-04-17T16:35:05Z",
        "message": "fix imports on cuda/triton since they aren't in __init__"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "src/alpaca_lora_4bit/autograd_4bit.py",
        "commit_date": "2023-04-17T16:35:05Z",
        "message": "fix circular import and add monkeypatch submodule in setup"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "src/alpaca_lora_4bit/autograd_4bit.py",
        "commit_date": "2023-04-17T16:35:05Z",
        "message": "fix conditional"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "src/alpaca_lora_4bit/autograd_4bit.py",
        "commit_date": "2023-04-17T16:35:01Z",
        "message": "make things installable, refactor things"
    },
    {
        "repo_url": "github.com/IBM/Dromedary",
        "filepath": "utils/convert_hf_weights_to_llama_ckpt.py",
        "commit_date": "2023-10-05T11:47:11Z",
        "message": "upgrade to Dromedary-2"
    },
    {
        "repo_url": "github.com/IBM/Dromedary",
        "filepath": "utils/convert_hf_weights_to_llama_ckpt.py",
        "commit_date": "2023-05-04T09:42:43Z",
        "message": "update README"
    },
    {
        "repo_url": "github.com/IBM/Dromedary",
        "filepath": "utils/convert_hf_weights_to_llama_ckpt.py",
        "commit_date": "2023-05-04T05:30:20Z",
        "message": "clean step4 code"
    },
    {
        "repo_url": "github.com/locuslab/wanda",
        "filepath": "dense_ft/trainer.py",
        "commit_date": "2023-11-03T05:37:59Z",
        "message": "add dense ft"
    },
    {
        "repo_url": "github.com/jianzhnie/Efficient-Tuning-LLMs",
        "filepath": "chatllms/utils/apply_lora.py",
        "commit_date": "2023-07-30T02:46:47Z",
        "message": "update model server"
    },
    {
        "repo_url": "github.com/jianzhnie/Efficient-Tuning-LLMs",
        "filepath": "chatllms/utils/apply_lora.py",
        "commit_date": "2023-07-28T10:57:53Z",
        "message": "update server"
    },
    {
        "repo_url": "github.com/jianzhnie/Efficient-Tuning-LLMs",
        "filepath": "chatllms/utils/apply_lora.py",
        "commit_date": "2023-07-28T09:29:48Z",
        "message": "update apply_lora"
    },
    {
        "repo_url": "github.com/jianzhnie/Efficient-Tuning-LLMs",
        "filepath": "chatllms/utils/apply_lora.py",
        "commit_date": "2023-07-12T08:09:42Z",
        "message": "update data utils"
    },
    {
        "repo_url": "github.com/jianzhnie/Efficient-Tuning-LLMs",
        "filepath": "chatllms/utils/apply_lora.py",
        "commit_date": "2023-07-11T07:22:56Z",
        "message": "update apply_lora"
    },
    {
        "repo_url": "github.com/jianzhnie/Efficient-Tuning-LLMs",
        "filepath": "chatllms/utils/apply_lora.py",
        "commit_date": "2023-06-20T03:46:21Z",
        "message": "update apply_lora"
    },
    {
        "repo_url": "github.com/jianzhnie/Efficient-Tuning-LLMs",
        "filepath": "chatllms/utils/apply_lora.py",
        "commit_date": "2023-06-17T14:58:00Z",
        "message": "Formatting the code"
    },
    {
        "repo_url": "github.com/jianzhnie/Efficient-Tuning-LLMs",
        "filepath": "chatllms/utils/apply_lora.py",
        "commit_date": "2023-06-17T10:23:15Z",
        "message": "git commit -m \"Reorganized the code\""
    },
    {
        "repo_url": "github.com/LiuHC0428/LAW-GPT",
        "filepath": "src/peft/src/peft/tuners/prompt_tuning.py",
        "commit_date": "2023-04-22T04:46:57Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "src/alpaca_lora_4bit/server/server.py",
        "commit_date": "2023-04-26T06:38:57Z",
        "message": "fix _SentinelTokenStoppingCriteria"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "src/alpaca_lora_4bit/server/server.py",
        "commit_date": "2023-04-26T05:13:54Z",
        "message": "fix bug"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "src/alpaca_lora_4bit/server/server.py",
        "commit_date": "2023-04-26T04:57:14Z",
        "message": "add server"
    },
    {
        "repo_url": "github.com/OpenGVLab/Multi-Modality-Arena",
        "filepath": "tiny_lvlm_evaluation/models/bliva/models/bliva_vicuna7b_lora.py",
        "commit_date": "2023-10-16T07:03:03Z",
        "message": "Add Ability-level Benchmark"
    },
    {
        "repo_url": "github.com/CMKRG/QiZhenGPT",
        "filepath": "gradio_cama-demo.py",
        "commit_date": "2023-06-02T09:57:18Z",
        "message": "release instruction tuning on CaMA"
    },
    {
        "repo_url": "github.com/datadreamer-dev/DataDreamer",
        "filepath": "src/trainers/_train_hf_base.py",
        "commit_date": "2024-02-13T06:50:43Z",
        "message": "v1 HF tags"
    },
    {
        "repo_url": "github.com/datadreamer-dev/DataDreamer",
        "filepath": "src/trainers/_train_hf_base.py",
        "commit_date": "2024-02-02T04:00:55Z",
        "message": "Update docs [release]"
    },
    {
        "repo_url": "github.com/datadreamer-dev/DataDreamer",
        "filepath": "src/trainers/_train_hf_base.py",
        "commit_date": "2024-02-01T03:26:02Z",
        "message": "Release commit [release]"
    },
    {
        "repo_url": "github.com/SkunkworksAI/hydra-moe",
        "filepath": "hydra_moe/utils.py",
        "commit_date": "2023-09-22T07:20:28Z",
        "message": "Merge branch 'v1_alpha' into alpha"
    },
    {
        "repo_url": "github.com/SkunkworksAI/hydra-moe",
        "filepath": "hydra_moe/utils.py",
        "commit_date": "2023-09-21T22:16:39Z",
        "message": "(artem) refactor part 3"
    },
    {
        "repo_url": "github.com/SkunkworksAI/hydra-moe",
        "filepath": "hydra_moe/utils.py",
        "commit_date": "2023-09-21T22:02:11Z",
        "message": "refactor pt 2"
    },
    {
        "repo_url": "github.com/shawroad/NLP_pytorch_project",
        "filepath": "LLM/peft_example/sequence_cls/bert/run_infer_bert_lora.py",
        "commit_date": "2023-04-21T03:21:49Z",
        "message": "add accelerate"
    },
    {
        "repo_url": "github.com/BYU-PRISM/GEKKO",
        "filepath": "docs/llm/phi2-gekko-eval.py",
        "commit_date": "2024-02-04T15:16:21Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/LC1332/Chat-Haruhi-Suzumiya",
        "filepath": "ChatHaruhi2.0/ChatHaruhi/Qwen118k2GPT.py",
        "commit_date": "2023-12-21T07:22:47Z",
        "message": "move stable Haruhi 2.0 version into main repo"
    },
    {
        "repo_url": "github.com/LC1332/Chat-Haruhi-Suzumiya",
        "filepath": "ChatHaruhi2.0/ChatHaruhi/BaiChuan2GPT.py",
        "commit_date": "2023-10-24T06:23:28Z",
        "message": "remove all jsonl that used in Haruhi 1.0, update into 2.0 , the jsonl will still kept at Legacy-Haruhi-1.0 project"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "examples/3.x_api/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py",
        "commit_date": "2024-02-28T14:11:40Z",
        "message": "align the parameters between 2x and 3x example (#1636)\n\nSigned-off-by: chensuyue <suyue.chen@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "examples/3.x_api/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py",
        "commit_date": "2024-02-21T05:03:23Z",
        "message": "Fix GPTQ/RTN 3.x example & fix asym quantize  (#1611)\n\nSigned-off-by: Kaihui-intel <kaihui.tang@intel.com>\nSigned-off-by: Tang, Kaihui <kaihui.tang@intel.com>\nCo-authored-by: chen, suyue <suyue.chen@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "examples/3.x_api/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py",
        "commit_date": "2024-02-05T15:53:35Z",
        "message": "remove 3.x_api example (#1607)\n\nSigned-off-by: Kaihui-intel <kaihui.tang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "examples/3.x_api/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py",
        "commit_date": "2024-02-05T02:30:58Z",
        "message": "update GPTQConfig and UTs (#1587)\n\nSigned-off-by: xin3he <xin3.he@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "examples/3.x_api/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py",
        "commit_date": "2024-01-19T02:54:50Z",
        "message": "Rename RTNWeightOnlyConfig to RTNConfig (#1551)\n\n* Rename RTNWeightOnlyConfig to RTNConfig\n\nSigned-off-by: xin3he <xin3.he@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "examples/3.x_api/pytorch/nlp/huggingface_models/language-modeling/quantization/llm/run_clm_no_trainer.py",
        "commit_date": "2024-01-02T02:02:04Z",
        "message": "Add double quant example (#1498)\n\nSigned-off-by: Kaihui-intel <kaihui.tang@intel.com>\nSigned-off-by: Tang, Kaihui <kaihui.tang@intel.com>\nSigned-off-by: chensuyue <suyue.chen@intel.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: chensuyue <suyue.chen@intel.com>"
    },
    {
        "repo_url": "github.com/lyuchenyang/Macaw-LLM",
        "filepath": "run_clm_llms_inference.py",
        "commit_date": "2023-07-01T03:46:24Z",
        "message": "Update run_clm_llms_inference.py"
    },
    {
        "repo_url": "github.com/lyuchenyang/Macaw-LLM",
        "filepath": "run_clm_llms_inference.py",
        "commit_date": "2023-07-01T03:45:32Z",
        "message": "Update run_clm_llms_inference.py"
    },
    {
        "repo_url": "github.com/lyuchenyang/Macaw-LLM",
        "filepath": "run_clm_llms_inference.py",
        "commit_date": "2023-07-01T03:41:28Z",
        "message": "Update run_clm_llms_inference.py"
    },
    {
        "repo_url": "github.com/lyuchenyang/Macaw-LLM",
        "filepath": "run_clm_llms_inference.py",
        "commit_date": "2023-06-14T10:31:48Z",
        "message": "Update run_clm_llms_inference.py"
    },
    {
        "repo_url": "github.com/lyuchenyang/Macaw-LLM",
        "filepath": "run_clm_llms_inference.py",
        "commit_date": "2023-05-31T01:58:08Z",
        "message": "Update run_clm_llms_inference.py"
    },
    {
        "repo_url": "github.com/lyuchenyang/Macaw-LLM",
        "filepath": "run_clm_llms_inference.py",
        "commit_date": "2023-05-31T01:21:20Z",
        "message": "Update run_clm_llms_inference.py"
    },
    {
        "repo_url": "github.com/lyuchenyang/Macaw-LLM",
        "filepath": "run_clm_llms_inference.py",
        "commit_date": "2023-05-30T11:43:53Z",
        "message": "Update run_clm_llms_inference.py"
    },
    {
        "repo_url": "github.com/lyuchenyang/Macaw-LLM",
        "filepath": "run_clm_llms_inference.py",
        "commit_date": "2023-05-30T07:45:07Z",
        "message": "Update run_clm_llms_inference.py"
    },
    {
        "repo_url": "github.com/lyuchenyang/Macaw-LLM",
        "filepath": "run_clm_llms_inference.py",
        "commit_date": "2023-05-26T05:39:09Z",
        "message": "Create run_clm_llms_inference.py"
    },
    {
        "repo_url": "github.com/penghao-wu/vstar",
        "filepath": "VisualSearch/model/llava/model/builder.py",
        "commit_date": "2023-12-18T04:32:06Z",
        "message": "init code"
    },
    {
        "repo_url": "github.com/kakaobrain/honeybee",
        "filepath": "pipeline/interface.py",
        "commit_date": "2023-12-20T10:08:51Z",
        "message": "Add inference code"
    },
    {
        "repo_url": "github.com/michael-wzhu/Chinese-LlaMA2",
        "filepath": "src/sft/web_service_with_lora.py",
        "commit_date": "2023-07-24T14:53:56Z",
        "message": "2023/07/24 update readme"
    },
    {
        "repo_url": "github.com/michael-wzhu/Chinese-LlaMA2",
        "filepath": "src/sft/web_service_with_lora.py",
        "commit_date": "2023-07-22T07:04:40Z",
        "message": "2023/07/22 add sft and serving code"
    },
    {
        "repo_url": "github.com/sail-sg/lorahub",
        "filepath": "train_model.py",
        "commit_date": "2023-10-17T01:10:10Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/shawroad/NLP_pytorch_project",
        "filepath": "LLM/peft_example/sequence_cls/bert/run_infer_bert_p_tuning.py",
        "commit_date": "2023-04-21T03:21:49Z",
        "message": "add accelerate"
    },
    {
        "repo_url": "github.com/shibing624/textgen",
        "filepath": "examples/gpt/inference_multigpu_demo.py",
        "commit_date": "2023-09-08T11:14:52Z",
        "message": "update remove file."
    },
    {
        "repo_url": "github.com/shibing624/textgen",
        "filepath": "examples/gpt/inference_multigpu_demo.py",
        "commit_date": "2023-09-07T07:31:29Z",
        "message": "del truncation."
    },
    {
        "repo_url": "github.com/shibing624/textgen",
        "filepath": "examples/gpt/inference_multigpu_demo.py",
        "commit_date": "2023-09-07T07:05:55Z",
        "message": "to DDP"
    },
    {
        "repo_url": "github.com/shibing624/textgen",
        "filepath": "examples/gpt/inference_multigpu_demo.py",
        "commit_date": "2023-09-07T06:12:41Z",
        "message": "update show case."
    },
    {
        "repo_url": "github.com/shibing624/textgen",
        "filepath": "examples/gpt/inference_multigpu_demo.py",
        "commit_date": "2023-09-07T05:46:42Z",
        "message": "update save to jsonl."
    },
    {
        "repo_url": "github.com/shibing624/textgen",
        "filepath": "examples/gpt/inference_multigpu_demo.py",
        "commit_date": "2023-09-07T05:10:44Z",
        "message": "update inference multi gpu token."
    },
    {
        "repo_url": "github.com/shibing624/textgen",
        "filepath": "examples/gpt/inference_multigpu_demo.py",
        "commit_date": "2023-09-07T05:04:38Z",
        "message": "update inference multi gpu."
    },
    {
        "repo_url": "github.com/shibing624/textgen",
        "filepath": "examples/gpt/inference_multigpu_demo.py",
        "commit_date": "2023-09-07T05:04:13Z",
        "message": "update inference multi gpu."
    },
    {
        "repo_url": "github.com/shibing624/textgen",
        "filepath": "examples/gpt/inference_multigpu_demo.py",
        "commit_date": "2023-09-07T05:02:45Z",
        "message": "update inference multi gpu."
    },
    {
        "repo_url": "github.com/shibing624/textgen",
        "filepath": "examples/gpt/inference_multigpu_demo.py",
        "commit_date": "2023-09-07T04:50:34Z",
        "message": "update inference multi gpu."
    },
    {
        "repo_url": "github.com/shibing624/textgen",
        "filepath": "examples/gpt/inference_multigpu_demo.py",
        "commit_date": "2023-09-07T03:44:55Z",
        "message": "update inference multi."
    },
    {
        "repo_url": "github.com/shibing624/textgen",
        "filepath": "examples/gpt/inference_multigpu_demo.py",
        "commit_date": "2023-09-07T03:16:25Z",
        "message": "update inference"
    },
    {
        "repo_url": "github.com/shibing624/textgen",
        "filepath": "examples/gpt/inference_multigpu_demo.py",
        "commit_date": "2023-09-07T03:07:32Z",
        "message": "update inference with multi gpu."
    },
    {
        "repo_url": "github.com/codefuse-ai/codefuse-devops-eval",
        "filepath": "src/models/internlm_model.py",
        "commit_date": "2023-12-13T06:24:20Z",
        "message": "add funccall evalution features"
    },
    {
        "repo_url": "github.com/CambioML/pykoi-rlhf-finetuned-transformers",
        "filepath": "pykoi/chat/llm/peft_huggingface.py",
        "commit_date": "2023-12-23T01:54:59Z",
        "message": "add TODO and run black and isort"
    },
    {
        "repo_url": "github.com/CambioML/pykoi-rlhf-finetuned-transformers",
        "filepath": "pykoi/chat/llm/peft_huggingface.py",
        "commit_date": "2023-11-07T09:15:13Z",
        "message": "update peft sft training, demo default model, and inference result"
    },
    {
        "repo_url": "github.com/CambioML/pykoi-rlhf-finetuned-transformers",
        "filepath": "pykoi/chat/llm/peft_huggingface.py",
        "commit_date": "2023-09-13T05:50:52Z",
        "message": "refactor pykoi into base, huggingface, rag, and rlhf"
    },
    {
        "repo_url": "github.com/CambioML/pykoi-rlhf-finetuned-transformers",
        "filepath": "pykoi/chat/llm/peft_huggingface.py",
        "commit_date": "2023-08-07T06:25:05Z",
        "message": "refactor llm and db into chat folder, refactor their tests into chat folder, and update docs rst"
    },
    {
        "repo_url": "github.com/star-whale/starwhale",
        "filepath": "example/llm-finetune/models/baichuan2/evaluation.py",
        "commit_date": "2024-01-16T02:58:20Z",
        "message": "example(LLM): use `@starwhale.argument` feature to refactor llm finetune examples (#3125)"
    },
    {
        "repo_url": "github.com/star-whale/starwhale",
        "filepath": "example/llm-finetune/models/baichuan2/evaluation.py",
        "commit_date": "2023-11-21T05:41:04Z",
        "message": " example: add llm-finetune example: baichuan2 (#2973)"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/LLM-Adapters",
        "filepath": "peft/tests/test_peft_model.py",
        "commit_date": "2023-03-31T12:49:40Z",
        "message": "support bottleneck adapters"
    },
    {
        "repo_url": "github.com/codefuse-ai/codefuse-devops-eval",
        "filepath": "src/models/baichuan_model.py",
        "commit_date": "2023-12-13T06:24:20Z",
        "message": "add funccall evalution features"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2024-02-27T14:03:21Z",
        "message": "SQ refactor (#1633)\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2024-02-27T06:22:06Z",
        "message": "Fixed smooth quant's UTs (#1631)\n\nSigned-off-by: yiliu30 <yi4.liu@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2024-01-22T07:47:27Z",
        "message": "fix ipex stats bug (#1555)\n\nSigned-off-by: xin3he <xin3.he@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2024-01-18T03:03:59Z",
        "message": "Stop the tuning process early when enabling smooth quant (#1542)\n\nSigned-off-by: yiliu30 <yi4.liu@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-12-22T14:03:35Z",
        "message": "Narrow the tuning space of sq auto-tune (#1489)\n\nSigned-off-by: yiliu30 <yi4.liu@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-12-13T01:17:50Z",
        "message": "Lyt/blockwise (#1441)\n\n* [Algo] blockwise tuning\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\n\n* [Algo] code update\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\n\n* [Algo] sq argument update\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\n\n* [Algo] log update\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* [Algo] code update\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* [Algo] fix bugs\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* [Algo] log update\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* [Algo] enable blockwise on Llama models\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\n\n* [Algo] enable blockwise on Llama models\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* [Algo] code update\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* [Algo] format code\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\n\n* [Algo] fix bug\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\n\n* [Algo] add ut\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* [Algo] fix format issue\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\n\n* [Algo] log update\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* [Algo] move do_blockwise arg\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* [Algo] fix bug\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* [Algo] fix bug\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* [Algo] fix bug\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* [Algo] fix bug\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\n\n* [Algo] fix bug\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\n\n---------\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-12-08T07:13:48Z",
        "message": "fix peft issue in ut (#1450)\n\nSigned-off-by: Xin He <xin3.he@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-11-22T05:40:44Z",
        "message": "support for restoring ipex model from json (#1405)\n\nSigned-off-by: Kaihui-intel <kaihui.tang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-11-16T12:42:23Z",
        "message": "[SmoothQuant] make alpha search space a config argument (#1392)\n\nEnhance SmoothQuant with tunable alpha search space\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-11-16T06:43:47Z",
        "message": "[SmoothQuant] make weight_clipping a default_on option (#1386)\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-11-06T05:35:17Z",
        "message": "support peft model quantization with SmoothQuant (#1282)\n\nPeft model will use below arch: Linears in Linear. This pull request supports this arch with smoothquant.\n```\n(v): Linear(                                                                                                                          \n  in_features=32, out_features=32, bias=False                                                                                         \n  (lora_dropout): ModuleDict(                                                                                                         \n    (default): Dropout(p=0.1, inplace=False)                                                                                          \n  )                                                                                                                                   \n  (lora_A): ModuleDict(                                                                                                               \n    (default): Linear(in_features=32, out_features=8, bias=False)                                                                     \n  )                                                                                                                                   \n  (lora_B): ModuleDict(                                                                                                               \n    (default): Linear(in_features=8, out_features=32, bias=False)                                                                     \n  )                                                                                                                                   \n  (lora_embedding_A): ParameterDict()                                                                                                 \n  (lora_embedding_B): ParameterDict()  \n```\nBTW,\nwhen IPEX version<=1.13, HistogramObserver doesn't support asym scheme, the zero_point is 0 for asym uint8, while the MinMaxObserver works well.\nAlso,\nIPEX SmoothQuant Observer can only use save/load_qconf_summary once. The save_qconf_summary API will freeze the scale used in model and calibration won't work anymore. The load_qconf_summary will overwrite the scales used in model but only work in the first call. Here we implement normal observer to workaround this issue.\n---------\n\nSigned-off-by: changwangss <chang1.wang@intel.com>\nSigned-off-by: Xin He <xin3.he@intel.com>\nSigned-off-by: y <xin3.he@intel.com>\nSigned-off-by: chensuyue <suyue.chen@intel.com>\nCo-authored-by: changwangss <chang1.wang@intel.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: chen, suyue <suyue.chen@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-09-27T11:23:55Z",
        "message": "fix bug in smoothquant for auto alpha (#1287)"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-09-26T13:59:44Z",
        "message": "[regression fix] sq enhance calibration part  (#1276)\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: xinhe <xin3.he@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-09-12T01:31:18Z",
        "message": "fix bug in nf4/fp4 (#1241)\n\nSigned-off-by: Xin He <xin3.he@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-09-08T07:54:53Z",
        "message": "add pre-commit-ci code-spell check (#1206)\n\nSigned-off-by: Sun, Xuehao <xuehao.sun@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-09-07T07:49:34Z",
        "message": "fix device issue of sq (#1218)\n\nSigned-off-by: Xin He <xin3.he@intel.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-09-06T03:00:49Z",
        "message": "fix bug when trace failed (#1214)\n\nSigned-off-by: Xin He <xin3.he@intel.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-08-31T03:15:59Z",
        "message": "sq move_input_to_device tuple issue (#1186)\n\n* fix conflicts\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix for ut\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* code enhance for coverage decrease\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-08-28T08:59:33Z",
        "message": "Add automatic Pre-CI code-scan bug fix tool (#1134)\n\nSigned-off-by: Sun, Xuehao <xuehao.sun@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-08-24T05:42:38Z",
        "message": "Improve sq device mapping and enhance ut (#1168)\n\nSigned-off-by: changwangss <chang1.wang@intel.com>\nCo-authored-by: chensuyue <suyue.chen@intel.com>\nCo-authored-by: xinhe <xin3.he@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-08-21T06:03:52Z",
        "message": "add accuracy test for smoothquant (#1158)\n\n* add accuracy test for smoothquant to avoid accuracy issue in pre-ci\n\nSigned-off-by: Xin He <xin3.he@intel.com>\n\n---------\n\nSigned-off-by: Xin He <xin3.he@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-08-17T05:09:11Z",
        "message": "fix bug in TorchSmoothQuant (#1149)\n\n* [bug fix] when folding=False and QKV is not fully converted to SQLinear.\n\nSigned-off-by: Xin He <xin3.he@intel.com>\n\n---------\n\nSigned-off-by: Xin He <xin3.he@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-08-15T00:39:59Z",
        "message": "a new method to perform alpha auto-tuning in smooth_quant (#1065)\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-08-07T02:05:19Z",
        "message": "[SQ bug] unifiy weight_amax for modules same input (#1139)\n\nunifiy weight_amax for modules same input, or qkv will get different scale and got wrong accuracy."
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-07-28T08:40:24Z",
        "message": "Enhance SmoothQuant tuning structure. (#1109)\n\n* enhance sq tuning\n\nSigned-off-by: Xin He <xin3.he@intel.com>\n\n* Support the tuning of smooth quant' alpha in strategy layer (#1112)\n\nSigned-off-by: yiliu30 <yi4.liu@intel.com>\n\n* added more UTs\n\nSigned-off-by: yiliu30 <yi4.liu@intel.com>\n\n* fixed ut\n\nSigned-off-by: yiliu30 <yi4.liu@intel.com>\n\n* fixed ut\n\nSigned-off-by: yiliu30 <yi4.liu@intel.com>\n\n* enable sq tuning for both quant_level is auto or 1\n\nSigned-off-by: yiliu30 <yi4.liu@intel.com>\n\n* fix accuracy issue\n\nSigned-off-by: Xin He <xin3.he@intel.com>\n\n* fix UT\n\nSigned-off-by: Xin He <xin3.he@intel.com>\n\n* fix alpha=auto\n\nSigned-off-by: Xin He <xin3.he@intel.com>\n\n* support sq tuning for both auto and O1\n\nSigned-off-by: yiliu30 <yi4.liu@intel.com>\n\n* fixed the typo\n\nSigned-off-by: yiliu30 <yi4.liu@intel.com>\n\n* rename func name in ut\n\nSigned-off-by: Xin He <xin3.he@intel.com>\n\n* remove duplicate Linear if Linear is wrapped by Linear\n\nSigned-off-by: Xin He <xin3.he@intel.com>\n\n* refactor tensorflow interface\n\n* adjust the pre-optimization and sq order for ort\n\n* updated ort ut\n\nSigned-off-by: yiliu30 <yi4.liu@intel.com>\n\n* fix pylint and docstyle\n\nSigned-off-by: Xin He <xin3.he@intel.com>\n\n* add sketch for ort tune sq alpha\n\nSigned-off-by: yiliu30 <yi4.liu@intel.com>\n\n* correct the calib_iter\n\nSigned-off-by: yiliu30 <yi4.liu@intel.com>\n\n* fix tensorflow UT and int8 acc issue\n\n* fix ut\n\nSigned-off-by: Xin He <xin3.he@intel.com>\n\n---------\n\nSigned-off-by: Xin He <xin3.he@intel.com>\nSigned-off-by: yiliu30 <yi4.liu@intel.com>\nCo-authored-by: Yi30 <106061964+yiliu30@users.noreply.github.com>\nCo-authored-by: yiliu30 <yi4.liu@intel.com>\nCo-authored-by: spycsh <sihan.chen@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-07-18T07:56:48Z",
        "message": "fix sq skip_connection bugs (#1011)\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-06-15T00:52:26Z",
        "message": "output check added after sq transform (#954)\n\nSigned-off-by: chensuyue <suyue.chen@intel.com>\nCo-authored-by: wenhuach21 <wenhua.cheng@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-06-13T05:04:16Z",
        "message": "Fix onnxrt smooth quant (#951)\n\nSigned-off-by: Mengni Wang <mengni.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-05-30T02:15:13Z",
        "message": "Support sq auto tune for ort (#847)\n\nSigned-off-by: Mengni Wang <mengni.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-05-30T02:06:31Z",
        "message": "Add weight attribute to SQLinearWrapper class (#900)\n\nSigned-off-by: changwangss <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-05-26T04:10:03Z",
        "message": "resolve memory issue of SmoothQuant (#902)\n\nSigned-off-by: Xin He <xin3.he@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-05-05T03:30:40Z",
        "message": "update qconfig to meet ipex2.1 requirement and fix ipex bugs (#823)\n\nSigned-off-by: Xin He <xin3.he@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-04-19T05:23:49Z",
        "message": "enable smoothquant with calibration func (#812)\n\nSigned-off-by: changwa1 <chang1.wang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-04-18T06:22:00Z",
        "message": "Enhance smoothquant and ipex op_type capability. (#808)\n\nSigned-off-by: Xin He <xin3.he@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-04-13T13:08:10Z",
        "message": "update SmoothQuant algorithm with folding choice (#799)\n\nSigned-off-by: Xin He <xin3.he@intel.com>\nCo-authored-by: wenhuach21 <wenhua.cheng@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-04-11T06:15:54Z",
        "message": "fix smooth quant depthwise conv issue (#774)\n\nSigned-off-by: wenhuach21 <wenhua.cheng@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-03-29T01:45:31Z",
        "message": "smoothquant alpha auto-tuning (#747)\n\nSigned-off-by: Lu, Yintong <yintong.lu@intel.com>\nSigned-off-by: wenhuach21 <wenhua.cheng@intel.com>\nCo-authored-by: Wang, Mengni <mengni.wang@intel.com>\nCo-authored-by: yiliu30 <yi4.liu@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-03-24T15:11:20Z",
        "message": "quantization/sq support more patterns (#729)\n\nSigned-off-by: wenhuach21 <wenhua.cheng@intel.com>"
    },
    {
        "repo_url": "github.com/intel/neural-compressor",
        "filepath": "test/algorithm/test_smooth_quant.py",
        "commit_date": "2023-03-01T09:39:06Z",
        "message": "Quantization/smooth quant torch backend update (#594)\n\nSigned-off-by: wenhuach21 <wenhua.cheng@intel.com>"
    },
    {
        "repo_url": "github.com/mallorbc/Finetune_LLMs",
        "filepath": "finetuning_repo/lora_merge.py",
        "commit_date": "2023-10-24T05:48:39Z",
        "message": "updated merging program"
    },
    {
        "repo_url": "github.com/mallorbc/Finetune_LLMs",
        "filepath": "finetuning_repo/lora_merge.py",
        "commit_date": "2023-09-09T19:12:02Z",
        "message": "fixed flash attention for 70B, fixed lora merge for larger models, change default model for analyzing datasets, removed unneeeded code"
    },
    {
        "repo_url": "github.com/mallorbc/Finetune_LLMs",
        "filepath": "finetuning_repo/lora_merge.py",
        "commit_date": "2023-09-08T01:41:11Z",
        "message": "added int8 support"
    },
    {
        "repo_url": "github.com/mallorbc/Finetune_LLMs",
        "filepath": "finetuning_repo/lora_merge.py",
        "commit_date": "2023-09-07T21:38:34Z",
        "message": "updated lora merge and updated logger"
    },
    {
        "repo_url": "github.com/mallorbc/Finetune_LLMs",
        "filepath": "finetuning_repo/lora_merge.py",
        "commit_date": "2023-07-22T21:18:15Z",
        "message": "added lora merge file"
    },
    {
        "repo_url": "github.com/CambioML/pykoi-rlhf-finetuned-transformers",
        "filepath": "pykoi/rlhf/supervised_finetuning.py",
        "commit_date": "2023-12-23T01:54:59Z",
        "message": "add TODO and run black and isort"
    },
    {
        "repo_url": "github.com/CambioML/pykoi-rlhf-finetuned-transformers",
        "filepath": "pykoi/rlhf/supervised_finetuning.py",
        "commit_date": "2023-12-22T22:57:29Z",
        "message": "update openai, datasets, transformers, peft, and trl packages"
    },
    {
        "repo_url": "github.com/CambioML/pykoi-rlhf-finetuned-transformers",
        "filepath": "pykoi/rlhf/supervised_finetuning.py",
        "commit_date": "2023-09-01T07:53:12Z",
        "message": "update telemetry for RLHF"
    },
    {
        "repo_url": "github.com/CambioML/pykoi-rlhf-finetuned-transformers",
        "filepath": "pykoi/rlhf/supervised_finetuning.py",
        "commit_date": "2023-08-07T06:25:05Z",
        "message": "refactor llm and db into chat folder, refactor their tests into chat folder, and update docs rst"
    },
    {
        "repo_url": "github.com/CambioML/pykoi-rlhf-finetuned-transformers",
        "filepath": "pykoi/rlhf/supervised_finetuning.py",
        "commit_date": "2023-08-04T04:52:28Z",
        "message": "reformat pykoi with black"
    },
    {
        "repo_url": "github.com/CambioML/pykoi-rlhf-finetuned-transformers",
        "filepath": "pykoi/rlhf/supervised_finetuning.py",
        "commit_date": "2023-07-31T00:12:53Z",
        "message": "supervised finetuning refactor"
    },
    {
        "repo_url": "github.com/CambioML/pykoi-rlhf-finetuned-transformers",
        "filepath": "pykoi/rlhf/supervised_finetuning.py",
        "commit_date": "2023-07-25T08:54:57Z",
        "message": "rlhf refactor"
    },
    {
        "repo_url": "github.com/nlpai-lab/KULLM",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-05-30T11:26:03Z",
        "message": "add: train and dataset file\n\nSigned-off-by: metterian <dzzy6505@gmail.com>"
    },
    {
        "repo_url": "github.com/datadreamer-dev/DataDreamer",
        "filepath": "src/trainers/train_hf_finetune.py",
        "commit_date": "2024-02-01T03:26:02Z",
        "message": "Release commit [release]"
    },
    {
        "repo_url": "github.com/SkunkworksAI/hydra-moe",
        "filepath": "hydra_moe/router.py",
        "commit_date": "2023-09-22T04:21:17Z",
        "message": "fix roter:"
    },
    {
        "repo_url": "github.com/SkunkworksAI/hydra-moe",
        "filepath": "hydra_moe/router.py",
        "commit_date": "2023-09-21T22:16:39Z",
        "message": "(artem) refactor part 3"
    },
    {
        "repo_url": "github.com/SkunkworksAI/hydra-moe",
        "filepath": "hydra_moe/router.py",
        "commit_date": "2023-09-21T22:02:11Z",
        "message": "refactor pt 2"
    },
    {
        "repo_url": "github.com/IlyaGusev/rulm",
        "filepath": "self_instruct/src/tools/convert_to_native.py",
        "commit_date": "2023-09-16T17:02:24Z",
        "message": "7b v2"
    },
    {
        "repo_url": "github.com/IlyaGusev/rulm",
        "filepath": "self_instruct/src/tools/convert_to_native.py",
        "commit_date": "2023-09-09T14:23:42Z",
        "message": "Flake8 refactoring"
    },
    {
        "repo_url": "github.com/IlyaGusev/rulm",
        "filepath": "self_instruct/src/tools/convert_to_native.py",
        "commit_date": "2023-09-09T13:55:42Z",
        "message": "Removing unused scripts"
    },
    {
        "repo_url": "github.com/kbressem/medAlpaca",
        "filepath": "medalpaca/inferer.py",
        "commit_date": "2023-06-13T06:50:17Z",
        "message": "Allow loading any LlamaForCausalLM model"
    },
    {
        "repo_url": "github.com/kbressem/medAlpaca",
        "filepath": "medalpaca/inferer.py",
        "commit_date": "2023-04-07T00:21:25Z",
        "message": "Redesign of the inference class. It now prompts the models the same way, they were prompted during training.\nThis came with some minor changes in the datahandler and train script"
    },
    {
        "repo_url": "github.com/kbressem/medAlpaca",
        "filepath": "medalpaca/inferer.py",
        "commit_date": "2023-04-03T13:19:37Z",
        "message": "restructure main train function\n\nThis function aimes to combine the alpaca-lora and stanford alpaca script, by allowing to toggle between int8/LoRA and fp16 training"
    },
    {
        "repo_url": "github.com/kbressem/medAlpaca",
        "filepath": "medalpaca/inferer.py",
        "commit_date": "2023-04-02T23:19:00Z",
        "message": "move scripts to libs folder"
    },
    {
        "repo_url": "github.com/BobaZooba/xllm",
        "filepath": "src/xllm/utils/post_training.py",
        "commit_date": "2023-11-16T14:53:10Z",
        "message": "Release: 0.1.6"
    },
    {
        "repo_url": "github.com/BobaZooba/xllm",
        "filepath": "src/xllm/utils/post_training.py",
        "commit_date": "2023-11-16T12:01:00Z",
        "message": "int8 fusing"
    },
    {
        "repo_url": "github.com/BobaZooba/xllm",
        "filepath": "src/xllm/utils/post_training.py",
        "commit_date": "2023-11-14T12:57:06Z",
        "message": "more docs"
    },
    {
        "repo_url": "github.com/BobaZooba/xllm",
        "filepath": "src/xllm/utils/post_training.py",
        "commit_date": "2023-11-13T11:12:41Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/BobaZooba/xllm",
        "filepath": "src/xllm/utils/post_training.py",
        "commit_date": "2023-11-13T09:40:43Z",
        "message": "Init"
    },
    {
        "repo_url": "github.com/IBM/Dromedary",
        "filepath": "utils/convert_hf_weights_to_llama_ckpt_expanded.py",
        "commit_date": "2023-10-05T11:47:11Z",
        "message": "upgrade to Dromedary-2"
    },
    {
        "repo_url": "github.com/IBM/Dromedary",
        "filepath": "utils/convert_hf_weights_to_llama_ckpt_expanded.py",
        "commit_date": "2023-05-04T11:55:28Z",
        "message": "update README"
    },
    {
        "repo_url": "github.com/IBM/Dromedary",
        "filepath": "utils/convert_hf_weights_to_llama_ckpt_expanded.py",
        "commit_date": "2023-05-04T11:26:16Z",
        "message": "update sharding config"
    },
    {
        "repo_url": "github.com/IBM/Dromedary",
        "filepath": "utils/convert_hf_weights_to_llama_ckpt_expanded.py",
        "commit_date": "2023-05-04T09:59:43Z",
        "message": "add step1 instructions"
    },
    {
        "repo_url": "github.com/IBM/Dromedary",
        "filepath": "utils/convert_hf_weights_to_llama_ckpt_expanded.py",
        "commit_date": "2023-05-04T09:42:43Z",
        "message": "update README"
    },
    {
        "repo_url": "github.com/IBM/Dromedary",
        "filepath": "utils/convert_hf_weights_to_llama_ckpt_expanded.py",
        "commit_date": "2023-05-04T08:08:43Z",
        "message": "add inference code"
    },
    {
        "repo_url": "github.com/shawroad/NLP_pytorch_project",
        "filepath": "LLM/peft_example/sequence_cls/bert/run_infer_bert_prompt_tuning.py",
        "commit_date": "2023-04-21T03:21:49Z",
        "message": "add accelerate"
    },
    {
        "repo_url": "github.com/shawroad/NLP_pytorch_project",
        "filepath": "LLM/peft_example/sequence_cls/bert/run_infer_bert_prefix_tuning.py",
        "commit_date": "2023-04-21T03:21:49Z",
        "message": "add accelerate"
    },
    {
        "repo_url": "github.com/zjunlp/EasyEdit",
        "filepath": "easyeditor/models/melo/peft_egg/src/peft/peft_model.py",
        "commit_date": "2024-02-14T09:26:46Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/CMKRG/QiZhenGPT",
        "filepath": "gradio_chatglm_demo.py",
        "commit_date": "2023-05-30T06:43:15Z",
        "message": "release fine tuning chatglm-6b"
    },
    {
        "repo_url": "github.com/yangjianxin1/Firefly-LLaMA2-Chinese",
        "filepath": "script/http/start_service.py",
        "commit_date": "2023-09-18T16:57:31Z",
        "message": "http\u670d\u52a1\u90e8\u7f72"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "text-generation-webui/custom_monkey_patch.py",
        "commit_date": "2023-08-22T12:00:13Z",
        "message": "fix custom_monkey_patch for pip version"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "text-generation-webui/custom_monkey_patch.py",
        "commit_date": "2023-04-18T12:24:23Z",
        "message": "first pass at fixing the dockerfile"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "text-generation-webui/custom_monkey_patch.py",
        "commit_date": "2023-04-17T16:35:05Z",
        "message": "fix circular import and add monkeypatch submodule in setup"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "text-generation-webui/custom_monkey_patch.py",
        "commit_date": "2023-04-13T02:35:10Z",
        "message": "update reference"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "text-generation-webui/custom_monkey_patch.py",
        "commit_date": "2023-04-10T10:41:16Z",
        "message": "Bugfix in custom_monkey_patch for v1 models\n\nPreviously generation would fail with:\n\n    File \"/alpaca_lora_4bit/text-generation-webui/matmul_utils_4bit.py\", line 79, in _matmul4bit_v1_recons\n      quant_cuda.vecquant4recons_v1(qweight, buffer, scales, zeros)\n  RuntimeError: expected scalar type Half but found Float\n\nSee #71"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "text-generation-webui/custom_monkey_patch.py",
        "commit_date": "2023-04-10T01:33:41Z",
        "message": "add v1 model as default in custom monkey patch"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "text-generation-webui/custom_monkey_patch.py",
        "commit_date": "2023-04-06T04:56:27Z",
        "message": "add patch for encode function to remove eos token at the beginning of left side"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "text-generation-webui/custom_monkey_patch.py",
        "commit_date": "2023-03-28T12:33:55Z",
        "message": "add v2 model support"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "text-generation-webui/custom_monkey_patch.py",
        "commit_date": "2023-03-26T03:52:38Z",
        "message": "Tested and should be ready!"
    },
    {
        "repo_url": "github.com/johnsmith0031/alpaca_lora_4bit",
        "filepath": "text-generation-webui/custom_monkey_patch.py",
        "commit_date": "2023-03-22T07:58:51Z",
        "message": "add monkey patch for webui"
    },
    {
        "repo_url": "github.com/locuslab/wanda",
        "filepath": "lora_ft/evaluate_ppl.py",
        "commit_date": "2023-10-23T07:56:08Z",
        "message": "add lora fine_tuning"
    },
    {
        "repo_url": "github.com/datadreamer-dev/DataDreamer",
        "filepath": "src/trainers/train_hf_classifier.py",
        "commit_date": "2024-02-01T03:26:02Z",
        "message": "Release commit [release]"
    },
    {
        "repo_url": "github.com/uclaml/SPIN",
        "filepath": "spin/alignment/trainer.py",
        "commit_date": "2024-02-11T17:24:52Z",
        "message": "Update trainer.py\n\na -> an"
    },
    {
        "repo_url": "github.com/uclaml/SPIN",
        "filepath": "spin/alignment/trainer.py",
        "commit_date": "2024-02-09T11:03:30Z",
        "message": "code cleaning"
    },
    {
        "repo_url": "github.com/uclaml/SPIN",
        "filepath": "spin/alignment/trainer.py",
        "commit_date": "2024-02-09T10:50:15Z",
        "message": "solve compatibility issues"
    },
    {
        "repo_url": "github.com/uclaml/SPIN",
        "filepath": "spin/alignment/trainer.py",
        "commit_date": "2024-02-09T09:49:54Z",
        "message": "updating shell scripts and codes"
    },
    {
        "repo_url": "github.com/uclaml/SPIN",
        "filepath": "spin/alignment/trainer.py",
        "commit_date": "2024-02-09T09:39:26Z",
        "message": "Updated config, finetune.sh, convert_data.py and restructure"
    },
    {
        "repo_url": "github.com/uclaml/SPIN",
        "filepath": "spin/alignment/trainer.py",
        "commit_date": "2024-02-09T09:10:00Z",
        "message": "restructure codes"
    },
    {
        "repo_url": "github.com/uclaml/SPIN",
        "filepath": "spin/alignment/trainer.py",
        "commit_date": "2024-02-09T08:10:23Z",
        "message": "change back to original trainer"
    },
    {
        "repo_url": "github.com/uclaml/SPIN",
        "filepath": "spin/alignment/trainer.py",
        "commit_date": "2024-02-09T00:14:33Z",
        "message": "restructured spin"
    },
    {
        "repo_url": "github.com/sabetAI/BLoRA",
        "filepath": "blora_utils.py",
        "commit_date": "2023-09-01T17:26:15Z",
        "message": "Removed unused code."
    },
    {
        "repo_url": "github.com/sabetAI/BLoRA",
        "filepath": "blora_utils.py",
        "commit_date": "2023-09-01T16:52:30Z",
        "message": "Added requirements.txt and cleaned extraneous files."
    },
    {
        "repo_url": "github.com/sabetAI/BLoRA",
        "filepath": "blora_utils.py",
        "commit_date": "2023-08-29T02:40:49Z",
        "message": "Moved lora batch logic into utils."
    },
    {
        "repo_url": "github.com/sabetAI/BLoRA",
        "filepath": "blora_utils.py",
        "commit_date": "2023-08-27T16:34:26Z",
        "message": "Replaced peft class methods in inference call stack with blora methods."
    },
    {
        "repo_url": "github.com/sabetAI/BLoRA",
        "filepath": "blora_utils.py",
        "commit_date": "2023-08-25T21:29:16Z",
        "message": "Added blora classes."
    },
    {
        "repo_url": "github.com/sabetAI/BLoRA",
        "filepath": "blora_utils.py",
        "commit_date": "2023-08-25T04:41:58Z",
        "message": "Added placeholder README and delete irrelevant files."
    },
    {
        "repo_url": "github.com/sabetAI/BLoRA",
        "filepath": "blora_utils.py",
        "commit_date": "2023-08-23T23:52:13Z",
        "message": "Fixed streaming, tested."
    },
    {
        "repo_url": "github.com/sabetAI/BLoRA",
        "filepath": "blora_utils.py",
        "commit_date": "2023-08-23T15:03:54Z",
        "message": "Wrote method overrides for streaming peft."
    },
    {
        "repo_url": "github.com/airaria/Visual-Chinese-LLaMA-Alpaca",
        "filepath": "scripts/merge_llama_with_visualcla_lora.py",
        "commit_date": "2023-07-07T02:27:55Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/SeungyounShin/Llama2-Code-Interpreter",
        "filepath": "code_interpreter/llama_hf.py",
        "commit_date": "2023-08-21T18:58:29Z",
        "message": "Feat (Finetunning&Eval) add human-eval"
    },
    {
        "repo_url": "github.com/zjunlp/EasyEdit",
        "filepath": "easyeditor/models/melo/peft_egg/src/peft/tuners/prompt_tuning.py",
        "commit_date": "2024-02-14T09:26:46Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/datadreamer-dev/DataDreamer",
        "filepath": "src/trainers/_vendored/dpo_trainer.py",
        "commit_date": "2024-02-01T03:26:02Z",
        "message": "Release commit [release]"
    },
    {
        "repo_url": "github.com/SkunkworksAI/hydra-moe",
        "filepath": "hydra_moe/inference.py",
        "commit_date": "2023-09-21T22:16:39Z",
        "message": "(artem) refactor part 3"
    },
    {
        "repo_url": "github.com/SkunkworksAI/hydra-moe",
        "filepath": "hydra_moe/inference.py",
        "commit_date": "2023-09-21T22:02:11Z",
        "message": "refactor pt 2"
    },
    {
        "repo_url": "github.com/SkunkworksAI/hydra-moe",
        "filepath": "hydra_moe/peft_model.py",
        "commit_date": "2023-09-21T22:16:39Z",
        "message": "(artem) refactor part 3"
    },
    {
        "repo_url": "github.com/SkunkworksAI/hydra-moe",
        "filepath": "hydra_moe/peft_model.py",
        "commit_date": "2023-09-21T22:02:11Z",
        "message": "refactor pt 2"
    },
    {
        "repo_url": "github.com/datadreamer-dev/DataDreamer",
        "filepath": "src/trainers/train_setfit_classifier.py",
        "commit_date": "2024-02-01T03:26:02Z",
        "message": "Release commit [release]"
    },
    {
        "repo_url": "github.com/datadreamer-dev/DataDreamer",
        "filepath": "src/trainers/_vendored/_setfit_helper.py",
        "commit_date": "2024-02-01T03:26:02Z",
        "message": "Release commit [release]"
    },
    {
        "repo_url": "github.com/longyuewangdcu/Chinese-Llama-2",
        "filepath": "test/inference_lora.py",
        "commit_date": "2023-07-20T15:10:13Z",
        "message": "\tnew file:   test/inference_lora.py"
    },
    {
        "repo_url": "github.com/FudanDISC/DISC-FinLLM",
        "filepath": "eval/evaluator/finllm.py",
        "commit_date": "2023-10-24T11:01:18Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/datadreamer-dev/DataDreamer",
        "filepath": "src/trainers/train_sentence_transformer.py",
        "commit_date": "2024-02-27T00:09:58Z",
        "message": "Bump version [release]"
    },
    {
        "repo_url": "github.com/datadreamer-dev/DataDreamer",
        "filepath": "src/trainers/train_sentence_transformer.py",
        "commit_date": "2024-02-01T03:26:02Z",
        "message": "Release commit [release]"
    },
    {
        "repo_url": "github.com/zjunlp/EasyEdit",
        "filepath": "easyeditor/models/melo/peft_egg/tests/test_adaption_prompt.py",
        "commit_date": "2024-02-14T09:26:46Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/BobaZooba/xllm",
        "filepath": "tests/helpers/patches.py",
        "commit_date": "2023-11-14T16:41:46Z",
        "message": "Update docs"
    },
    {
        "repo_url": "github.com/BobaZooba/xllm",
        "filepath": "tests/helpers/patches.py",
        "commit_date": "2023-11-13T09:40:43Z",
        "message": "Init"
    },
    {
        "repo_url": "github.com/X-D-Lab/Sunsimiao",
        "filepath": "scripts/inference_4_bits.py",
        "commit_date": "2023-06-21T12:15:52Z",
        "message": "add inference"
    },
    {
        "repo_url": "github.com/OpenLMLab/LOMO",
        "filepath": "lomo/src/merge_llama_with_lora.py",
        "commit_date": "2023-10-17T03:15:38Z",
        "message": "add lomo folder and move"
    },
    {
        "repo_url": "github.com/nlpai-lab/KULLM",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-05-30T11:26:03Z",
        "message": "add: train and dataset file\n\nSigned-off-by: metterian <dzzy6505@gmail.com>"
    },
    {
        "repo_url": "github.com/open-compass/VLMEvalKit",
        "filepath": "vlmeval/vlm/llava_xtuner.py",
        "commit_date": "2024-01-17T05:40:59Z",
        "message": "[Model] Add `LLaVA-InternLM2` (#53)\n\n* support\n\n* add trust_remote_code"
    },
    {
        "repo_url": "github.com/open-compass/VLMEvalKit",
        "filepath": "vlmeval/vlm/llava_xtuner.py",
        "commit_date": "2024-01-02T12:29:54Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/open-compass/VLMEvalKit",
        "filepath": "vlmeval/vlm/llava_xtuner.py",
        "commit_date": "2024-01-02T07:45:47Z",
        "message": "[Dataset] HallusionBench (#38)\n\n* support HallusionBench\n\n* add GPT4V_SHORT\n\n* update dataset config\n\n* update smp.py\n\n* bug fix\n\n* BUG FIX\n\n* update MD5\n\n* update rating\n\n* fix bug"
    },
    {
        "repo_url": "github.com/open-compass/VLMEvalKit",
        "filepath": "vlmeval/vlm/llava_xtuner.py",
        "commit_date": "2023-12-27T13:04:39Z",
        "message": "[Feature] Support `LLaVA_XTuner` models (#17)\n\n* support\n\n* fix bugs\n\n* fix bugs\n\n* fix prompt bugs\n\n* Update README.md\n\n* Update README.md\n\n* Update llava.py\n\n* update llava_xtuner\n\n* update .gitignore\n\n* update hyp\n\n* use 1024 token\n\n* modify the gen_config\n\n* update"
    },
    {
        "repo_url": "github.com/datadreamer-dev/DataDreamer",
        "filepath": "src/task_models/hf_classification_task_model.py",
        "commit_date": "2024-02-02T23:20:22Z",
        "message": "Update docs"
    },
    {
        "repo_url": "github.com/datadreamer-dev/DataDreamer",
        "filepath": "src/task_models/hf_classification_task_model.py",
        "commit_date": "2024-02-02T04:00:55Z",
        "message": "Update docs [release]"
    },
    {
        "repo_url": "github.com/datadreamer-dev/DataDreamer",
        "filepath": "src/task_models/hf_classification_task_model.py",
        "commit_date": "2024-02-01T03:26:02Z",
        "message": "Release commit [release]"
    },
    {
        "repo_url": "github.com/stanford-crfm/levanter",
        "filepath": "examples/alpaca-lora/hf_lora_inference.py",
        "commit_date": "2023-10-24T06:07:35Z",
        "message": "Improvements to Alpaca tutorial wip (#349)\n\n* aspirational alpaca tutorial\n\n* rename ShardedDataSource to ShardedDataset\n\n* remove targets\n\n* remove targets\n\n* tweak\n\n* wip\n\n* rename ShardedDataSource to ShardedDataset\n\n* somehow missed this\n\n* create a fancier data api so we can make alpaca look nicer\n\n* fix alpaca_lora.py\n\n* remove old batchpreprocessor from alpaca\n\n* load prompts if provided\n\n* oops\n\n* rename\n\n* refactor the attentionmask stuff w/ hopes of supporting sequence packing\n\n* move tutorials\n\n* add bias and precision to FA\n\n* expose top level attn for all models to use\n\n* tweaks to new attention wrapper\n\n* make flash attention not slice dims the bias doesn't have\n\n* switch mpt to new attention wrapper\n\n* move gpt2 over\n\n* move llama over\n\n* move backpack over\n\n* del todo\n\n* refactor the attentionmask stuff w/ hopes of supporting sequence packing\n\n* add bias and precision to FA\n\n* expose top level attn for all models to use\n\n* tweaks to new attention wrapper\n\n* make flash attention not slice dims the bias doesn't have\n\n* switch mpt to new attention wrapper\n\n* move gpt2 over\n\n* move llama over\n\n* move backpack over\n\n* remove causalmask from alpaca\n\n* fix non-flash test\n\n* dumb prints\n\n* make models work with different length input than max input length\n\n* allow for \"max_tune_length\" in alpaca to speed up training\n\n* fix path imports for new alpaca_lora paths\n\n* missed an import\n\n* save tokenizer in peft checkpoints\n\n* save modified tokenizer in alpaca checkpoints\n\n* don't add special tokens for lora, just use unk\n\n* update docs\n\n* fix up tutorial\n\n* wtf\n\n* nmajlkfnmajlkf hf\n\n* hates\n\n* simple script for lora inference"
    },
    {
        "repo_url": "github.com/CMKRG/QiZhenGPT",
        "filepath": "gradio_chinese-llama_demo.py",
        "commit_date": "2023-06-02T09:57:18Z",
        "message": "release instruction tuning on CaMA"
    },
    {
        "repo_url": "github.com/sail-sg/lorahub",
        "filepath": "lorahub/algorithm.py",
        "commit_date": "2023-11-01T03:17:53Z",
        "message": "use copy.deepcopy to prevent modifying the peft state dict"
    },
    {
        "repo_url": "github.com/sail-sg/lorahub",
        "filepath": "lorahub/algorithm.py",
        "commit_date": "2023-08-29T15:41:07Z",
        "message": "update reproducing script for bbh results of lorahub"
    },
    {
        "repo_url": "github.com/sail-sg/lorahub",
        "filepath": "lorahub/algorithm.py",
        "commit_date": "2023-08-15T09:36:42Z",
        "message": "update model to device; update reproduce script for bbh dataset"
    },
    {
        "repo_url": "github.com/sail-sg/lorahub",
        "filepath": "lorahub/algorithm.py",
        "commit_date": "2023-07-27T07:55:07Z",
        "message": "fix minor typo"
    },
    {
        "repo_url": "github.com/sail-sg/lorahub",
        "filepath": "lorahub/algorithm.py",
        "commit_date": "2023-07-27T07:51:33Z",
        "message": "update code"
    },
    {
        "repo_url": "github.com/stanford-crfm/levanter",
        "filepath": "tests/test_lora.py",
        "commit_date": "2024-02-29T07:23:13Z",
        "message": "DoReMi (#416)"
    },
    {
        "repo_url": "github.com/stanford-crfm/levanter",
        "filepath": "tests/test_lora.py",
        "commit_date": "2024-02-28T22:12:27Z",
        "message": "make TrainerState a bit beefier, more standalone method for load_or_init (#496)\n\n* bring over load_checkpoint_or_initialize\n\n* extra test\n\n* is this better? I don't know\n\n* wip\n\n* wip\n\n* minor cleanup\n\n* wip\n\n* wip promoting TrainerState\n\n* almost done promoting TrainerState\n\n* make TrainerState know about mp and optimizers, other tweaks"
    },
    {
        "repo_url": "github.com/stanford-crfm/levanter",
        "filepath": "tests/test_lora.py",
        "commit_date": "2024-02-14T20:20:16Z",
        "message": "Causal flash (#469)\n\n* take advantage of block causal structure in flash attention?\n\n* default to FA=on, adjust default block size for tpu\n\n* fix tests\n\n* missed a spot\n\n* ok actually fix tests maybe\n\n* dumb"
    },
    {
        "repo_url": "github.com/stanford-crfm/levanter",
        "filepath": "tests/test_lora.py",
        "commit_date": "2024-02-14T04:24:24Z",
        "message": "Tweaks/Fixes to Lora_LM, add a basic config for lora_lm (#466)\n\n* require a new enough optax\n\n(didn't i do that?)\n\n* add a lora_llama2.yaml\n\n* forgot to set up the data stuff\n\n* support multiple evaluation sets in lora_lm\n\n* try not failing if the ray node is already up\n\n* don't raise on hf datasets with no validation set\n\n* fix trainable_param_count invocations\n\n* reduce batch size for lora_llama2\n\n* fix serialization of lora, which i had broken"
    },
    {
        "repo_url": "github.com/stanford-crfm/levanter",
        "filepath": "tests/test_lora.py",
        "commit_date": "2023-09-27T17:51:06Z",
        "message": "Script/Tutorial for Alpaca LoRA (#326)\n\n* wip\n\n* factor out and test merged model saving in lora\n\n* finish the alpaca-lora doc\n\n* please pre-commit\n\n* Update Alpaca-LoRA.md"
    },
    {
        "repo_url": "github.com/stanford-crfm/levanter",
        "filepath": "tests/test_lora.py",
        "commit_date": "2023-09-09T05:23:49Z",
        "message": "move inference_mode to tree_utils"
    },
    {
        "repo_url": "github.com/stanford-crfm/levanter",
        "filepath": "tests/test_lora.py",
        "commit_date": "2023-09-05T17:57:22Z",
        "message": "Support dropout in LoRA (#297)"
    },
    {
        "repo_url": "github.com/stanford-crfm/levanter",
        "filepath": "tests/test_lora.py",
        "commit_date": "2023-08-31T17:52:19Z",
        "message": "Support `inference` as a non-static field in Dropout and other fields. (#296)\n\n* support using `inference` field in models instead of passing an inference param around everywhere"
    },
    {
        "repo_url": "github.com/stanford-crfm/levanter",
        "filepath": "tests/test_lora.py",
        "commit_date": "2023-08-10T06:22:39Z",
        "message": "Preliminary LoRA support (#269)"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "tests/tuners/test_peft.py",
        "commit_date": "2024-02-22T13:26:16Z",
        "message": "support peft format (#438)"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "tests/tuners/test_peft.py",
        "commit_date": "2023-12-21T11:47:14Z",
        "message": "Support more peft tuners (#245)"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "tests/tuners/test_peft.py",
        "commit_date": "2023-12-16T09:13:46Z",
        "message": "Compatible with peft>=0.7.0 (#220)"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "tests/tuners/test_peft.py",
        "commit_date": "2023-10-11T09:04:36Z",
        "message": "fix ci (#109)"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "tests/tuners/test_peft.py",
        "commit_date": "2023-08-02T02:16:26Z",
        "message": "First commit"
    },
    {
        "repo_url": "github.com/FlagAI-Open/Aquila2",
        "filepath": "examples/rag_pipe/models/aquila/aquila_model_local.py",
        "commit_date": "2024-01-30T07:38:56Z",
        "message": "update rag"
    },
    {
        "repo_url": "github.com/airaria/Visual-Chinese-LLaMA-Alpaca",
        "filepath": "scripts/inference/text_generation_webui/visualcla/visualcla.py",
        "commit_date": "2023-07-07T02:27:55Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/modelscope/swift",
        "filepath": "tests/hub/test_check_model.py",
        "commit_date": "2023-08-02T02:16:26Z",
        "message": "First commit"
    },
    {
        "repo_url": "github.com/michael-wzhu/Chinese-LlaMA2",
        "filepath": "src/serving/vllm_serving/merge_llama_with_lora.py",
        "commit_date": "2023-07-28T06:45:40Z",
        "message": "2023/07/26 update model; uodate readme"
    },
    {
        "repo_url": "github.com/CMKRG/QiZhenGPT",
        "filepath": "scripts/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-05-23T06:40:06Z",
        "message": "\ud83c\udf89 init: \u521d\u59cb\u5316\n\n\u5f00\u6e90\u4f53\u9a8c\u7248"
    },
    {
        "repo_url": "github.com/beyondguo/LLM-Tuning",
        "filepath": "web_demo/src/test.py",
        "commit_date": "2023-07-25T05:02:02Z",
        "message": "web_demo"
    },
    {
        "repo_url": "github.com/SeungyounShin/Llama2-Code-Interpreter",
        "filepath": "code_interpreter/LlamaCodeInterpreter.py",
        "commit_date": "2023-08-28T17:37:32Z",
        "message": "Rebase :)"
    },
    {
        "repo_url": "github.com/SeungyounShin/Llama2-Code-Interpreter",
        "filepath": "code_interpreter/LlamaCodeInterpreter.py",
        "commit_date": "2023-08-25T10:35:37Z",
        "message": "Feat (CodeLlama) Update CodeLlama support and Some Eval"
    },
    {
        "repo_url": "github.com/SeungyounShin/Llama2-Code-Interpreter",
        "filepath": "code_interpreter/LlamaCodeInterpreter.py",
        "commit_date": "2023-08-21T18:58:29Z",
        "message": "Feat (Finetunning&Eval) add human-eval"
    },
    {
        "repo_url": "github.com/SeungyounShin/Llama2-Code-Interpreter",
        "filepath": "code_interpreter/LlamaCodeInterpreter.py",
        "commit_date": "2023-08-21T18:39:43Z",
        "message": "Feat (Eval) add human-eval"
    },
    {
        "repo_url": "github.com/SeungyounShin/Llama2-Code-Interpreter",
        "filepath": "code_interpreter/LlamaCodeInterpreter.py",
        "commit_date": "2023-08-18T08:32:33Z",
        "message": "Feat (GSM8K) eval"
    },
    {
        "repo_url": "github.com/SeungyounShin/Llama2-Code-Interpreter",
        "filepath": "code_interpreter/LlamaCodeInterpreter.py",
        "commit_date": "2023-08-13T18:50:28Z",
        "message": "Feat (Llama2 Finetunning) Modify Finetuning Model Wrapper Class"
    },
    {
        "repo_url": "github.com/SeungyounShin/Llama2-Code-Interpreter",
        "filepath": "code_interpreter/LlamaCodeInterpreter.py",
        "commit_date": "2023-08-12T14:28:59Z",
        "message": "Feat (Llama2 Finetuning) Add inference code for test"
    },
    {
        "repo_url": "github.com/SeungyounShin/Llama2-Code-Interpreter",
        "filepath": "code_interpreter/LlamaCodeInterpreter.py",
        "commit_date": "2023-08-09T08:06:42Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/michael-wzhu/PromptCBLUE",
        "filepath": "src/ft_llama_lora/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-07-18T09:37:52Z",
        "message": "2023-07-18: add LlaMA + lora code; add vllm model serving"
    },
    {
        "repo_url": "github.com/LiuHC0428/LAW-GPT",
        "filepath": "src/peft/tests/test_peft_model.py",
        "commit_date": "2023-04-22T04:46:57Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/michael-wzhu/PromptCBLUE",
        "filepath": "src/ft_llama_lora/vllm_serving/merge_llama_with_lora.py",
        "commit_date": "2023-07-18T09:37:52Z",
        "message": "2023-07-18: add LlaMA + lora code; add vllm model serving"
    },
    {
        "repo_url": "github.com/PotatoSpudowski/fastLLaMa",
        "filepath": "scripts/export-from-huggingface.py",
        "commit_date": "2023-04-24T13:22:48Z",
        "message": "Sync llama (#63)\n\n* sync and refactor part one\n\n* sync and refactor part two\n\n* sync and refactor part three\n\n* fix percentage callback\n\n* Fix for CMake 11\n\n* fix shadow of variables\n\n* updated the python api and examples\n\n* Updated export to support alpaca 13B and 30B\n\n* add support for mmap, mlock and progress\n\n* update python interface\n\n* refactor code\n\n* fix lora adapters\n\n* remove debug prints\n\n* refactors and fixes\n\n* fix adapter version\n\n* stop users from attach/detach lora to a mmaped model\n\n* use existing function to load tensor\n\n* fix lora adapter for f16 cached matrices\n\n* refactor code\n\n* refactor code\n\n* remove unused struct member\n\n* move scaling to python script\n\n* remove 'constexpr' because gcc 11 complains\n\n* refactor code\n\n* remove redundant variable\n\n* remove unused namespace\n\n* cap progress value to max value\n\n* fix recycling of tokens\n\n* refactor token recycler\n\n* fix token recycler\n\n* fix lora adapter\n\n* add lora adapter support for mmap\n\n* force related tensors to be consecutive\n\n* updated the comment\n\n* set lora adapter path in early exit\n\n* fix colon issue\n\n* clear token buffer after stop token found\n\n* restore state after model is stopped.\n\n* remove unused enum\n\n* remove model kind\n\n* Cleaned up code and updated docs\n\n---------\n\nCo-authored-by: Amit Singh <amitsingh19975@gmail.com>"
    },
    {
        "repo_url": "github.com/PotatoSpudowski/fastLLaMa",
        "filepath": "scripts/export-from-huggingface.py",
        "commit_date": "2023-04-15T21:11:00Z",
        "message": "Big Update! \ud83e\udda3 (#44)\n\n* rearrange files\n\n* add model load\n\n* add macros\n\n* add model eval\n\n* add ring buffer, bridge\n\n* rearrange files\n\n* add model load\n\n* add macros\n\n* add model eval\n\n* add ring buffer, bridge\n\n* update ggml and add generate with ingest\n\n* refactor code\n\n* fix example.py\n\n* fix conflict issue\n\n* fix wrong mapping of tensor using string\n\n* remove old utils files\n\n* disable quantize for now\n\n* add utils.hpp\n\n* unload the model\n\n* add config structure inside the ModelId\n\n* sync with the `llama.cpp`\n\n* add constant and move `n_batch` to model\n\n* remove constant and move `n_batch` to model\n\n* add batch overhead\n\n* add args\n\n* add logger to builder and rename function\n\n* update cmake\n\n* rename `Builder` to `builder`\n\n* add python support\n\n* add fastllama wrapper for python\n\n* show interface folders that we detected\n\n* updated setup.py\n\n* fix to cmake with the correct python version\n\n* update quantize.cpp\n\n* refactor quantize\n\n* remove old 'bridge.cpp'\n\n* make compiler more strict\n\n* remove compiler specific flag\n\n* updated scripts\n\n* fix model parsing\n\n* refactor to make c++17 complient\n\n* add pad token\n\n* fix header\n\n* fix shell invoke\n\n* update 'setup.py'\n\n* compile c++ in strict mode\n\n* add mode for old quantized models\n\n* add alpaca model test source file\n\n* persist system prompt after token recycle\n\n* fix alpaca example\n\n* set thread count dynamically\n\n* add option to use enum instead of string for model id\n\n* add option to use enum in python instead of string for model id\n\n* change python module name and add c interface\n\n* fix for unused variables\n\n* change cmake vars\n\n* allow for language selection using gui and cmd line args\n\n* disable warnings for now\n\n* move away pybind11 to make it version indepenedent\n\n* uncomment setup.py\n\n* comment the find python version\n\n* add signal to quit the app\n\n* add cross compile mode for android\n\n* enable build\n\n* fix cmake file\n\n* improve c example\n\n* use c malloc and free\n\n* build lib first then examples\n\n* remove copy constructor\n\n* default initialize context struct\n\n* add global compiler flags\n\n* remove pybind11\n\n* add embeddings and perplexity\n\n* update `ggml.c` and `ggml.h`\n\n* Add a array view `span`\n\n* add a array view struct for c interface\n\n* add new fields and bug fixes\n\n* refactor code\n\n* fix alpaca example\n\n* change `set stop words` interface\n\n* fix ctype issue\n\n* fix call to set stop words\n\n* fix call to set stop words\n\n* warn the user about getting embeddings before not setting the flag.\n\n* add type to struct\n\n* add function comments\n\n* fix example and add a new one\n\n* add c perplexity example\n\n* Fixes for gcc\n\n* fix python type issue on windows\n\n* fix msvc errors\n\n* fix windows compile errors\n\n* Added examples and logger fix\n\n* Updated readme\n\n---------\n\nCo-authored-by: Amit Singh <amitsingh19975@gmail.com>"
    },
    {
        "repo_url": "github.com/jianzhnie/open-chatgpt",
        "filepath": "chatgpt/models/apply_lora.py",
        "commit_date": "2023-05-22T10:32:13Z",
        "message": "update alpaca_inference"
    },
    {
        "repo_url": "github.com/jianzhnie/open-chatgpt",
        "filepath": "chatgpt/models/apply_lora.py",
        "commit_date": "2023-05-22T08:24:47Z",
        "message": "Create apply_lora.py"
    },
    {
        "repo_url": "github.com/jianzhnie/open-chatgpt",
        "filepath": "examples/chatllama/merge_peft_adapter.py",
        "commit_date": "2023-04-27T07:41:00Z",
        "message": "format code"
    },
    {
        "repo_url": "github.com/jianzhnie/open-chatgpt",
        "filepath": "examples/chatllama/merge_peft_adapter.py",
        "commit_date": "2023-04-27T02:34:50Z",
        "message": "add train chatllama"
    },
    {
        "repo_url": "github.com/LLaVA-VL/LLaVA-Plus-Codebase",
        "filepath": "llava/model/builder.py",
        "commit_date": "2023-10-31T20:09:27Z",
        "message": "Update macOS support."
    },
    {
        "repo_url": "github.com/LLaVA-VL/LLaVA-Plus-Codebase",
        "filepath": "llava/model/builder.py",
        "commit_date": "2023-10-06T23:49:36Z",
        "message": "For inference in model_worker, allow the device to be specified via a command line parameter.\n\nRight now it has only been tested with Apple Sillicon devices via the mps device."
    },
    {
        "repo_url": "github.com/LLaVA-VL/LLaVA-Plus-Codebase",
        "filepath": "llava/model/builder.py",
        "commit_date": "2023-09-01T16:45:00Z",
        "message": "Update instruction for LoRA"
    },
    {
        "repo_url": "github.com/LLaVA-VL/LLaVA-Plus-Codebase",
        "filepath": "llava/model/builder.py",
        "commit_date": "2023-07-29T23:55:59Z",
        "message": "Update docs"
    },
    {
        "repo_url": "github.com/LLaVA-VL/LLaVA-Plus-Codebase",
        "filepath": "llava/model/builder.py",
        "commit_date": "2023-07-19T09:08:02Z",
        "message": "Release v1.0.0"
    },
    {
        "repo_url": "github.com/Zeqiang-Lai/Mini-DALLE3",
        "filepath": "minidalle3/llm/llama.py",
        "commit_date": "2023-12-28T13:52:43Z",
        "message": "add doc for custom llm"
    },
    {
        "repo_url": "github.com/Zeqiang-Lai/Mini-DALLE3",
        "filepath": "minidalle3/llm/llama.py",
        "commit_date": "2023-10-04T02:47:24Z",
        "message": "refactor"
    },
    {
        "repo_url": "github.com/Joyce94/LLM-RLHF-Tuning",
        "filepath": "script/rm/run_rm_with_peft.py",
        "commit_date": "2023-08-31T02:54:51Z",
        "message": "fix process data"
    },
    {
        "repo_url": "github.com/Joyce94/LLM-RLHF-Tuning",
        "filepath": "script/rm/run_rm_with_peft.py",
        "commit_date": "2023-08-23T08:07:52Z",
        "message": "add dpo and ppo_co code"
    },
    {
        "repo_url": "github.com/Joyce94/LLM-RLHF-Tuning",
        "filepath": "script/rm/run_rm_with_peft.py",
        "commit_date": "2023-08-13T14:21:30Z",
        "message": "update code"
    },
    {
        "repo_url": "github.com/Joyce94/LLM-RLHF-Tuning",
        "filepath": "script/rm/run_rm_with_peft.py",
        "commit_date": "2023-08-06T10:21:26Z",
        "message": "add code"
    },
    {
        "repo_url": "github.com/Joyce94/LLM-RLHF-Tuning",
        "filepath": "script/sft/run_sft_with_peft.py",
        "commit_date": "2023-08-31T02:54:51Z",
        "message": "fix process data"
    },
    {
        "repo_url": "github.com/Joyce94/LLM-RLHF-Tuning",
        "filepath": "script/sft/run_sft_with_peft.py",
        "commit_date": "2023-08-26T07:00:37Z",
        "message": "update ppo"
    },
    {
        "repo_url": "github.com/Joyce94/LLM-RLHF-Tuning",
        "filepath": "script/sft/run_sft_with_peft.py",
        "commit_date": "2023-08-23T08:07:52Z",
        "message": "add dpo and ppo_co code"
    },
    {
        "repo_url": "github.com/Joyce94/LLM-RLHF-Tuning",
        "filepath": "script/sft/run_sft_with_peft.py",
        "commit_date": "2023-08-17T09:45:33Z",
        "message": "update code"
    },
    {
        "repo_url": "github.com/Joyce94/LLM-RLHF-Tuning",
        "filepath": "script/sft/run_sft_with_peft.py",
        "commit_date": "2023-08-13T14:21:30Z",
        "message": "update code"
    },
    {
        "repo_url": "github.com/Joyce94/LLM-RLHF-Tuning",
        "filepath": "script/sft/run_sft_with_peft.py",
        "commit_date": "2023-08-06T10:21:26Z",
        "message": "add code"
    },
    {
        "repo_url": "github.com/megvii-research/Sparsebit",
        "filepath": "large_language_models/alpaca-qlora/qlora.py",
        "commit_date": "2023-04-15T09:34:36Z",
        "message": "add alpaca-qlora (#133)"
    },
    {
        "repo_url": "github.com/megvii-research/Sparsebit",
        "filepath": "large_language_models/alpaca-qlora/generate.py",
        "commit_date": "2023-04-30T04:13:58Z",
        "message": "update the usage of generate.py with a cli (#141)"
    },
    {
        "repo_url": "github.com/megvii-research/Sparsebit",
        "filepath": "large_language_models/alpaca-qlora/generate.py",
        "commit_date": "2023-04-30T03:01:47Z",
        "message": "add assert in generate.py to force set CHECKPOINT_PATH (#140)"
    },
    {
        "repo_url": "github.com/megvii-research/Sparsebit",
        "filepath": "large_language_models/alpaca-qlora/generate.py",
        "commit_date": "2023-04-15T13:39:00Z",
        "message": "Update generate.py"
    },
    {
        "repo_url": "github.com/megvii-research/Sparsebit",
        "filepath": "large_language_models/alpaca-qlora/generate.py",
        "commit_date": "2023-04-15T09:34:36Z",
        "message": "add alpaca-qlora (#133)"
    },
    {
        "repo_url": "github.com/zjunlp/EasyInstruct",
        "filepath": "experiments/lora/generate.py",
        "commit_date": "2024-01-11T15:12:15Z",
        "message": "update lora experiments"
    },
    {
        "repo_url": "github.com/zjunlp/EasyInstruct",
        "filepath": "experiments/lora/generate.py",
        "commit_date": "2024-01-07T12:10:23Z",
        "message": "update experiments"
    },
    {
        "repo_url": "github.com/huggingface/api-inference-community",
        "filepath": "docker_images/peft/app/pipelines/text_generation.py",
        "commit_date": "2024-03-01T21:41:32Z",
        "message": "`model.config` handle breaking change in hub (#416)"
    },
    {
        "repo_url": "github.com/huggingface/api-inference-community",
        "filepath": "docker_images/peft/app/pipelines/text_generation.py",
        "commit_date": "2023-07-12T09:46:29Z",
        "message": "Minor fix for PEFT (#300)\n\n* minor fix for widget\n\n* listed dict\n\n* Fixing pydantic (temporarily).\n\n---------\n\nCo-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/api-inference-community",
        "filepath": "docker_images/peft/app/pipelines/text_generation.py",
        "commit_date": "2023-06-20T09:57:35Z",
        "message": "PEFT integration for inference API (#294)\n\n* initial commit\n\n* fixes\n\n* Update docker_images/peft/app/pipelines/text_generation.py\n\nCo-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>\n\n* fix in pipeline\n\n* added test cases\n\n* style fixes\n\n* added peft tests to github actions workflows\n\n* added test case\n\n* make style\n\n* added timeout\n\n* removed ds_store and refactored pipeline init\n\n* Delete .DS_Store\n\n* Update docker_images/peft/app/pipelines/text_generation.py\n\nCo-authored-by: Omar Sanseviero <osanseviero@gmail.com>\n\n* updated api-inference-community version\n\n---------\n\nCo-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>\nCo-authored-by: Omar Sanseviero <osanseviero@gmail.com>"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2024-02-23T05:25:10Z",
        "message": "New prompts (#88)\n\n* new prompts\n\n* add prev_invalid_sql and prev_error_msg to prompt\n\n* - add prev_invalid_sql and prev_error_msg to prompt\n- shift generate_prompt to utils\n\n* add prev_invalid_sql and prev_error_msg to prompt\n\n* - add prev_invalid_sql and prev_error_msg to prompt\n- link to prompt templates in prompt folder\n\n* add more types\n\n* change prompts, add dependencies\n\n* remove print statements\n\n* add instructions for mistral and gemini\n\n* linted"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2024-02-16T02:59:27Z",
        "message": "Added support for Mistral models (#86)\n\n* changed end result printed to correct + error_db_exec, instead of the less useful exact_match and correct\n\n* added mistral runner, though with a hard-coded prompt for now\n\n* linting"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2024-02-13T13:12:37Z",
        "message": "modify pre-processing of generated_query"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2024-02-06T15:46:45Z",
        "message": "linted"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2024-02-06T15:46:45Z",
        "message": "add table_metadata_string"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2024-02-06T15:46:45Z",
        "message": "add glossary"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2024-02-05T08:21:42Z",
        "message": "Added ability to report results to a URL, and sample server code for saving it to a database (#79)\n\n* added utility for reporting results to a webservice\n\n* added webserver for receiving results\n\n* added DDL statements for database tables for storing results\n\n* Add upload_results function to runners\n\n* added upload_url param to args and updated README\n\n* converted uuid4 to hex\n\n* fixed typo in table name\n\n* fixed column name typo\n\n* minor fixes\n\n* converted two columns to string for better compatibility\n\n* formatting\n\n* Shift cloud function for receiving reports into separate folders\nSplit into separate folders fo Bigquery vs Postgres\nAdded instructions for deploying cloud functions\nUpdate pandas_gbq calls\n\n* removed bq_table arg\n\n* Remove bq_table flag from command line options\n\n---------\n\nCo-authored-by: jp <wongjingping@gmail.com>"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2024-01-17T05:28:58Z",
        "message": "Caps `max_new_tokens` in the beam search check (#63)\n\n* Update hf_runner.py\n\n* formatting"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2024-01-15T13:56:03Z",
        "message": "new features (#64)"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2024-01-10T12:24:33Z",
        "message": "force num_beams to 1 if beam search is not supported"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2024-01-08T07:22:36Z",
        "message": "Add support to MPS backend"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2023-11-29T10:13:38Z",
        "message": "lint"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2023-11-29T09:27:32Z",
        "message": "remove flag for public data and automatically infer whether db is public based on the name's membership in defog_data.metadata.dbs"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2023-11-07T10:07:07Z",
        "message": "Multiple Prompts\n- allows multiple prompts and output files in a single run. this saves the model loading time especially when testing multiple prompts for hf and vllm runners\n- we ensure that the number of prompt and output files match early in main.py since it applies to all runners"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2023-10-18T08:06:34Z",
        "message": "Use dynamic number of beams depending on the prompt's token length. We scale it down approximately quadratically due to the quadratic nature of attention, and allow users to specify max_beams\nWe now no longer need the statements to explicitly deal with torch memory before the generate statement.\nUpdate prompt to be the same as sql-coder.\nAdd tests.\nUpdated requirements.txt to support peft."
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2023-10-17T06:38:40Z",
        "message": "pass args to run_hf_eval (standardize)\n-d as a boolean flag to opt in for private data\nfix openai runner to able to use private data\nfix hf runner to use adapter"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2023-10-16T04:26:01Z",
        "message": "add support for private data"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2023-09-28T08:56:58Z",
        "message": "formatting chnages"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2023-09-28T08:56:58Z",
        "message": "minor syntactic updates"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2023-08-28T03:16:43Z",
        "message": "fixed GROUP BY bug, added better Llama 2 support, refactored code"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2023-08-16T07:13:33Z",
        "message": "reformatting"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2023-08-16T07:13:14Z",
        "message": "refactoring and code cleanup"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2023-08-15T17:38:09Z",
        "message": "added check to get more resilience for starcoder and wizardcoder implementations"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2023-08-15T08:20:07Z",
        "message": "minor compatibility changes to make new eval work with hf_runner"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2023-08-14T09:44:19Z",
        "message": "refactoring and small bugfixes"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2023-08-14T09:11:53Z",
        "message": "removed unnecessary print statements"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2023-08-14T09:02:12Z",
        "message": "slight modifications to eval functions"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2023-08-13T13:04:21Z",
        "message": "black reformatting"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2023-08-13T13:03:52Z",
        "message": "made openai prompt and result parsing  more consistent with open-source model"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2023-08-11T22:19:26Z",
        "message": "formatting changes"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2023-08-11T22:14:44Z",
        "message": "added latency of hf evaluator"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2023-08-11T22:09:57Z",
        "message": "added instructions for running HF models"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2023-08-11T20:48:06Z",
        "message": "added padding token to tokenizer"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2023-08-11T20:45:06Z",
        "message": "added helpful status print statements to hf_runner"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2023-08-11T20:30:51Z",
        "message": "minor refactoring nad updates to main.py"
    },
    {
        "repo_url": "github.com/defog-ai/sql-eval",
        "filepath": "eval/hf_runner.py",
        "commit_date": "2023-08-11T20:20:31Z",
        "message": "created hf runner to improve runtime efficiency"
    },
    {
        "repo_url": "github.com/liucongg/ChatGPTBook",
        "filepath": "LLMFTProj/predict_lora.py",
        "commit_date": "2023-06-26T07:14:08Z",
        "message": "update code"
    },
    {
        "repo_url": "github.com/BAAI-DCAI/Bunny",
        "filepath": "bunny/model/builder.py",
        "commit_date": "2024-02-28T14:12:52Z",
        "message": "update README & disable some warnings"
    },
    {
        "repo_url": "github.com/BAAI-DCAI/Bunny",
        "filepath": "bunny/model/builder.py",
        "commit_date": "2024-02-12T15:03:18Z",
        "message": "migrating to latest phi weights"
    },
    {
        "repo_url": "github.com/BAAI-DCAI/Bunny",
        "filepath": "bunny/model/builder.py",
        "commit_date": "2024-02-10T16:18:57Z",
        "message": "Update phi-2 link used in Bunny"
    },
    {
        "repo_url": "github.com/BAAI-DCAI/Bunny",
        "filepath": "bunny/model/builder.py",
        "commit_date": "2024-02-07T15:41:23Z",
        "message": "Initial Commit"
    },
    {
        "repo_url": "github.com/paolorechia/learn-langchain",
        "filepath": "servers/hf_loader.py",
        "commit_date": "2023-05-13T12:18:41Z",
        "message": "LoRA load example"
    },
    {
        "repo_url": "github.com/paolorechia/learn-langchain",
        "filepath": "servers/hf_loader.py",
        "commit_date": "2023-05-05T18:20:46Z",
        "message": "Starcoder (#19)\n\n* Fix merge conflict\n\n* Starcoder example HF 8bit\n\n* Solve conflicts for requirements"
    },
    {
        "repo_url": "github.com/paolorechia/learn-langchain",
        "filepath": "servers/hf_loader.py",
        "commit_date": "2023-04-28T12:23:07Z",
        "message": "Adds 'bitsandbytes' dependency"
    },
    {
        "repo_url": "github.com/paolorechia/learn-langchain",
        "filepath": "servers/hf_loader.py",
        "commit_date": "2023-04-28T12:15:45Z",
        "message": "Fix 16bit HF local model load"
    },
    {
        "repo_url": "github.com/paolorechia/learn-langchain",
        "filepath": "servers/hf_loader.py",
        "commit_date": "2023-04-25T06:31:33Z",
        "message": "Load 4 bit (#5)\n\n* Fix\n\n* Embedding Example\n\n* Add experimental support to GPQT 4bit group size 128 model\n\n* Add gpqt repository\n\n* Switch to safetensors instead\n\n* Reorganize repo\n\n* Fix imports\n\n* git ignore update\n\n* Support 13b\n\n* Applies black to servers\n\n* Applies black to langchain_app\n\n* Fix config\n\n* Fix checkpoint paths\n\n* Update README.md"
    },
    {
        "repo_url": "github.com/paolorechia/learn-langchain",
        "filepath": "langchain_app/models/alpaca_llm.py",
        "commit_date": "2023-04-30T21:11:25Z",
        "message": "Support oogabooga web server (#14)\n\n* Refactor code to support different HTTP servers\n\n* Fix langchain passed stop token with http llm\n\n* Add observation to stopping strings; add port to url\n\n* Apply same building logic\n\n* Test web generation\n\n* Add response extractor\n\n* Update readme\n\n* Extend README\n\n* Force stop token removal for ReAct on text generation web UI\n\n* Updates readme, applies black"
    },
    {
        "repo_url": "github.com/paolorechia/learn-langchain",
        "filepath": "langchain_app/models/alpaca_llm.py",
        "commit_date": "2023-04-25T06:31:33Z",
        "message": "Load 4 bit (#5)\n\n* Fix\n\n* Embedding Example\n\n* Add experimental support to GPQT 4bit group size 128 model\n\n* Add gpqt repository\n\n* Switch to safetensors instead\n\n* Reorganize repo\n\n* Fix imports\n\n* git ignore update\n\n* Support 13b\n\n* Applies black to servers\n\n* Applies black to langchain_app\n\n* Fix config\n\n* Fix checkpoint paths\n\n* Update README.md"
    },
    {
        "repo_url": "github.com/paolorechia/learn-langchain",
        "filepath": "langchain_app/models/alpaca_llm.py",
        "commit_date": "2023-04-22T23:11:07Z",
        "message": "React lora (#2)\n\n* Split into modules\n\n* Add prompt human in the loop\n\n* Exp. Embeddings endpoint"
    },
    {
        "repo_url": "github.com/feizc/MLE-LLaMA",
        "filepath": "peft/peft_model.py",
        "commit_date": "2023-03-20T08:14:22Z",
        "message": "add package dependency"
    },
    {
        "repo_url": "github.com/feizc/MLE-LLaMA",
        "filepath": "peft/tuners/lora.py",
        "commit_date": "2023-03-20T08:14:22Z",
        "message": "add package dependency"
    },
    {
        "repo_url": "github.com/feizc/MLE-LLaMA",
        "filepath": "llama/peft/peft_model.py",
        "commit_date": "2023-03-19T12:17:31Z",
        "message": "add lora"
    },
    {
        "repo_url": "github.com/feizc/MLE-LLaMA",
        "filepath": "llama/peft/tuners/lora.py",
        "commit_date": "2023-03-19T12:17:31Z",
        "message": "add lora"
    },
    {
        "repo_url": "github.com/feizc/MLE-LLaMA",
        "filepath": "peft/tuners/prompt_tuning.py",
        "commit_date": "2023-03-20T08:14:22Z",
        "message": "add package dependency"
    },
    {
        "repo_url": "github.com/feizc/MLE-LLaMA",
        "filepath": "llama/peft/tuners/prompt_tuning.py",
        "commit_date": "2023-03-19T12:17:31Z",
        "message": "add lora"
    },
    {
        "repo_url": "github.com/nbasyl/LLM-FP4",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-11-22T07:11:19Z",
        "message": "release the source code"
    },
    {
        "repo_url": "github.com/georgian-io/LLM-Finetuning-Hub",
        "filepath": "inference/fastapi_naive/predictor.py",
        "commit_date": "2023-09-20T20:48:44Z",
        "message": "polished code, add inference readme"
    },
    {
        "repo_url": "github.com/georgian-io/LLM-Finetuning-Hub",
        "filepath": "inference/fastapi_naive/predictor.py",
        "commit_date": "2023-07-31T17:23:14Z",
        "message": "Updated readme"
    },
    {
        "repo_url": "github.com/georgian-io/LLM-Finetuning-Hub",
        "filepath": "flan-t5/flan_summarization_inference.py",
        "commit_date": "2023-07-26T17:24:46Z",
        "message": "code formatted using black"
    },
    {
        "repo_url": "github.com/georgian-io/LLM-Finetuning-Hub",
        "filepath": "flan-t5/flan_summarization_inference.py",
        "commit_date": "2023-07-26T17:20:24Z",
        "message": "save metrics dictionary to experiment folder"
    },
    {
        "repo_url": "github.com/georgian-io/LLM-Finetuning-Hub",
        "filepath": "flan-t5/flan_summarization_inference.py",
        "commit_date": "2023-07-24T14:29:07Z",
        "message": "flan files"
    },
    {
        "repo_url": "github.com/georgian-io/LLM-Finetuning-Hub",
        "filepath": "flan-t5/flan_classification_inference.py",
        "commit_date": "2023-07-26T17:24:46Z",
        "message": "code formatted using black"
    },
    {
        "repo_url": "github.com/georgian-io/LLM-Finetuning-Hub",
        "filepath": "flan-t5/flan_classification_inference.py",
        "commit_date": "2023-07-24T14:29:07Z",
        "message": "flan files"
    },
    {
        "repo_url": "github.com/georgian-io/LLM-Finetuning-Hub",
        "filepath": "inference/text_generation/merge_script.py",
        "commit_date": "2023-09-20T20:48:44Z",
        "message": "polished code, add inference readme"
    },
    {
        "repo_url": "github.com/georgian-io/LLM-Finetuning-Hub",
        "filepath": "inference/text_generation/merge_script.py",
        "commit_date": "2023-08-11T19:31:02Z",
        "message": "added merge scripts"
    },
    {
        "repo_url": "github.com/georgian-io/LLM-Finetuning-Hub",
        "filepath": "inference/text_generation/merge_script.py",
        "commit_date": "2023-08-04T18:33:49Z",
        "message": "fixed comments"
    },
    {
        "repo_url": "github.com/georgian-io/LLM-Finetuning-Hub",
        "filepath": "inference/text_generation/merge_script.py",
        "commit_date": "2023-08-04T14:01:25Z",
        "message": "added text generation"
    },
    {
        "repo_url": "github.com/ghimiresunil/LLM-PowerHouse-A-Curated-Guide-for-Large-Language-Models-with-Custom-Training-and-Inferencing",
        "filepath": "example_codebase/train_inference_peft_lora/inference_peft.py",
        "commit_date": "2023-12-01T02:06:59Z",
        "message": "Update: Branch Cleaning"
    },
    {
        "repo_url": "github.com/horseee/LLM-Pruner",
        "filepath": "generate.py",
        "commit_date": "2023-08-03T05:20:55Z",
        "message": "add an evaluation script"
    },
    {
        "repo_url": "github.com/horseee/LLM-Pruner",
        "filepath": "generate.py",
        "commit_date": "2023-05-21T17:34:08Z",
        "message": "update generate.py. Add script for testing MACs, #param and memory"
    },
    {
        "repo_url": "github.com/horseee/LLM-Pruner",
        "filepath": "generate.py",
        "commit_date": "2023-05-21T17:22:59Z",
        "message": "Update readme. Add code for generation"
    },
    {
        "repo_url": "github.com/horseee/LLM-Pruner",
        "filepath": "LLMPruner/peft/peft_model.py",
        "commit_date": "2023-07-24T07:51:35Z",
        "message": "upload code for evaluation"
    },
    {
        "repo_url": "github.com/horseee/LLM-Pruner",
        "filepath": "LLMPruner/peft/peft_model.py",
        "commit_date": "2023-05-17T17:15:09Z",
        "message": "add code"
    },
    {
        "repo_url": "github.com/horseee/LLM-Pruner",
        "filepath": "LLMPruner/peft/tuners/prompt_tuning.py",
        "commit_date": "2023-05-17T17:15:09Z",
        "message": "add code"
    },
    {
        "repo_url": "github.com/horseee/LLM-Pruner",
        "filepath": "lm-evaluation-harness/lm_eval/models/huggingface.py",
        "commit_date": "2023-07-24T07:51:35Z",
        "message": "upload code for evaluation"
    },
    {
        "repo_url": "github.com/horseee/LLM-Pruner",
        "filepath": "test_speedup.py",
        "commit_date": "2023-09-27T20:20:21Z",
        "message": "modify linear_dim -> head_dim * num_head"
    },
    {
        "repo_url": "github.com/horseee/LLM-Pruner",
        "filepath": "test_speedup.py",
        "commit_date": "2023-07-11T08:47:24Z",
        "message": "Add input constructor for testing MACs on GPU"
    },
    {
        "repo_url": "github.com/horseee/LLM-Pruner",
        "filepath": "test_speedup.py",
        "commit_date": "2023-07-07T07:32:49Z",
        "message": "Support test_speedup.py on CPU"
    },
    {
        "repo_url": "github.com/horseee/LLM-Pruner",
        "filepath": "test_speedup.py",
        "commit_date": "2023-05-21T17:34:08Z",
        "message": "update generate.py. Add script for testing MACs, #param and memory"
    },
    {
        "repo_url": "github.com/zhuyiche/llava-phi",
        "filepath": "llava_phi/model/builder.py",
        "commit_date": "2024-01-15T05:08:37Z",
        "message": "main"
    },
    {
        "repo_url": "github.com/devsapp/fc-langchain-chatglm6b",
        "filepath": "src/code/chatglm6b-resource/models/moss_llm.py",
        "commit_date": "2023-06-25T11:01:44Z",
        "message": "feat: initialize fc-langchain-chatglm6b project\n\nThis commit initializes the fc-langchain-chatglm6b project with  dependencies."
    },
    {
        "repo_url": "github.com/devsapp/fc-langchain-chatglm6b",
        "filepath": "src/code/chatglm6b-resource/models/chatglm_llm.py",
        "commit_date": "2023-06-25T11:01:44Z",
        "message": "feat: initialize fc-langchain-chatglm6b project\n\nThis commit initializes the fc-langchain-chatglm6b project with  dependencies."
    },
    {
        "repo_url": "github.com/maszhongming/ReactionMiner",
        "filepath": "extraction/extractor.py",
        "commit_date": "2024-01-23T15:56:41Z",
        "message": "init upload"
    },
    {
        "repo_url": "github.com/replicate/cog-llama-template",
        "filepath": "llama_recipes/llama_finetuning.py",
        "commit_date": "2023-11-09T18:59:52Z",
        "message": "apply ruff autofix"
    },
    {
        "repo_url": "github.com/replicate/cog-llama-template",
        "filepath": "llama_recipes/llama_finetuning.py",
        "commit_date": "2023-11-09T18:59:42Z",
        "message": "format!"
    },
    {
        "repo_url": "github.com/replicate/cog-llama-template",
        "filepath": "llama_recipes/llama_finetuning.py",
        "commit_date": "2023-09-13T15:07:50Z",
        "message": "now with gradient checkpointing instead of micro batch size (#33)\n\n* now with gradient checkpointing instead of micro batch size\n\n* words words words"
    },
    {
        "repo_url": "github.com/replicate/cog-llama-template",
        "filepath": "llama_recipes/llama_finetuning.py",
        "commit_date": "2023-09-06T15:06:11Z",
        "message": "Faster, smaller cold starts (#27)\n\n* use slim torch and merge pip installs\n* entrypoint scripts aren't currently copied in multistage\n* fake training\n* Working hotswap loras with transformers\n* use exllama for lora\n* remove non-exllama code paths\n* more fixes - filenames and envvar names\n* add schema to makefile\n* no system prompt\n* 3.11\n* configurable fused attn\n* factor fused_attn option into config.py\n* update all cog.yaml\n* hostname logging\n* try adding system_prompt in predict.py always, and removing it for non-chat models by setting __signature__\n* update makefile to be aware there are two different schemas, and to use symlinks instead of copying when selecting a model\n* don't reload lora if it hasn't changed\n* fix BUILD_STAGE_DEPS and verify chat\n* script to load secrets\n* update schemas\n* add ./.dockerignore to gitignore, remove all of the shared parts of models/*/.dockerignore, and have make select cat it with the template\n* unset lora\n\n---------\n\nCo-authored-by: joe <joe@replicate.com>\nCo-authored-by: technillogue <technillogue@gmail.com>"
    },
    {
        "repo_url": "github.com/replicate/cog-llama-template",
        "filepath": "llama_recipes/llama_finetuning.py",
        "commit_date": "2023-08-25T18:08:01Z",
        "message": "Add support for qlora"
    },
    {
        "repo_url": "github.com/replicate/cog-llama-template",
        "filepath": "llama_recipes/llama_finetuning.py",
        "commit_date": "2023-08-24T20:54:09Z",
        "message": "catch empty dataloader and raise error"
    },
    {
        "repo_url": "github.com/replicate/cog-llama-template",
        "filepath": "llama_recipes/llama_finetuning.py",
        "commit_date": "2023-08-22T18:07:31Z",
        "message": "Expand unit tests and packing support in API"
    },
    {
        "repo_url": "github.com/replicate/cog-llama-template",
        "filepath": "llama_recipes/llama_finetuning.py",
        "commit_date": "2023-08-21T21:36:35Z",
        "message": "Add packing to train.py"
    },
    {
        "repo_url": "github.com/replicate/cog-llama-template",
        "filepath": "llama_recipes/llama_finetuning.py",
        "commit_date": "2023-08-21T21:15:01Z",
        "message": "Switch collators, resize vocab"
    },
    {
        "repo_url": "github.com/replicate/cog-llama-template",
        "filepath": "llama_recipes/llama_finetuning.py",
        "commit_date": "2023-08-18T04:13:10Z",
        "message": "adding configurable lora"
    },
    {
        "repo_url": "github.com/replicate/cog-llama-template",
        "filepath": "llama_recipes/llama_finetuning.py",
        "commit_date": "2023-08-17T21:14:09Z",
        "message": "Add support for validation to train.py"
    },
    {
        "repo_url": "github.com/replicate/cog-llama-template",
        "filepath": "llama_recipes/llama_finetuning.py",
        "commit_date": "2023-08-14T20:23:08Z",
        "message": "minimally working training"
    },
    {
        "repo_url": "github.com/replicate/cog-llama-template",
        "filepath": "src/inference_engines/transformers_engine.py",
        "commit_date": "2023-11-09T18:59:42Z",
        "message": "format!"
    },
    {
        "repo_url": "github.com/replicate/cog-llama-template",
        "filepath": "src/inference_engines/transformers_engine.py",
        "commit_date": "2023-10-24T05:27:03Z",
        "message": "Engines (#47)\n\n* choo choo\n\nadding vLLM engine\n\n---------\n\nCo-authored-by: Moin Nadeem <moin@replicate.com>\n\n* trivial fix\n\n* Dan/vllm exllama engine (#46)\n\nvllm + exllama, together at last\n\nCo-authored-by: Moin Nadeem <moin@replicate.com>\nCo-authored-by: technillogue <technillogue@gmail.com>\n\n---------\n\nCo-authored-by: technillogue <technillogue@gmail.com>\nCo-authored-by: Moin Nadeem <Moinnadeem@moinnadeem.com>\nCo-authored-by: Moin Nadeem <moin@replicate.com>"
    },
    {
        "repo_url": "github.com/princeton-nlp/SWE-bench",
        "filepath": "inference/run_llama.py",
        "commit_date": "2023-11-20T08:28:14Z",
        "message": "Add run_llama.py to inference"
    },
    {
        "repo_url": "github.com/HKUDS/GraphGPT",
        "filepath": "graphgpt/model/builder.py",
        "commit_date": "2023-10-19T12:39:38Z",
        "message": "upload all code 19"
    },
    {
        "repo_url": "github.com/HKUDS/GraphGPT",
        "filepath": "graphgpt/model/apply_lora.py",
        "commit_date": "2023-10-19T12:39:38Z",
        "message": "upload all code 19"
    },
    {
        "repo_url": "github.com/DAMO-NLP-SG/chain-of-knowledge",
        "filepath": "utils/retrieval/wikidata.py",
        "commit_date": "2023-10-03T11:35:39Z",
        "message": "push code"
    },
    {
        "repo_url": "github.com/DAMO-NLP-SG/chain-of-knowledge",
        "filepath": "utils/retrieval/scienceqa_bio.py",
        "commit_date": "2023-10-03T11:35:39Z",
        "message": "push code"
    },
    {
        "repo_url": "github.com/DAMO-NLP-SG/chain-of-knowledge",
        "filepath": "utils/retrieval/scienceqa_phy.py",
        "commit_date": "2023-10-03T11:35:39Z",
        "message": "push code"
    },
    {
        "repo_url": "github.com/togethercomputer/StableDiffusion",
        "filepath": "app/llava/model/builder.py",
        "commit_date": "2023-11-06T14:47:21Z",
        "message": "Dockerfile will load llava dependences and requirements"
    },
    {
        "repo_url": "github.com/DataCanvasIO/LMS",
        "filepath": "lms/runtime/prune/llm_pruner/LLMPruner/peft/peft_model.py",
        "commit_date": "2023-11-01T10:43:45Z",
        "message": "lms v1.0.0"
    },
    {
        "repo_url": "github.com/DataCanvasIO/LMS",
        "filepath": "lms/runtime/prune/llm_pruner/LLMPruner/peft/tuners/prompt_tuning.py",
        "commit_date": "2023-11-01T10:43:45Z",
        "message": "lms v1.0.0"
    },
    {
        "repo_url": "github.com/wjn1996/InstructGraph",
        "filepath": "examples/inference/llama.py",
        "commit_date": "2024-01-17T22:42:41Z",
        "message": "add InstructGraph repo."
    },
    {
        "repo_url": "github.com/wjn1996/InstructGraph",
        "filepath": "src/inference/model_utils.py",
        "commit_date": "2024-01-17T22:42:41Z",
        "message": "add InstructGraph repo."
    },
    {
        "repo_url": "github.com/wjn1996/InstructGraph",
        "filepath": "examples/demo/hf_text_generation_inference/merge_lora_weights.py",
        "commit_date": "2024-01-17T22:42:41Z",
        "message": "add InstructGraph repo."
    },
    {
        "repo_url": "github.com/wjn1996/InstructGraph",
        "filepath": "examples/inference/preference_test.py",
        "commit_date": "2024-02-14T17:57:33Z",
        "message": "update project"
    },
    {
        "repo_url": "github.com/Miraclemarvel55/LLaMA-MOSS-RLHF-LoRA",
        "filepath": "merge_llama_with_chinese_lora_to_hf.py",
        "commit_date": "2023-05-16T09:06:23Z",
        "message": "llama moss rlhf lora implemented"
    },
    {
        "repo_url": "github.com/OpenSPG/openspg",
        "filepath": "python/nn4k/nn4k/executor/huggingface/hf_decode_only_executor.py",
        "commit_date": "2024-02-22T06:08:21Z",
        "message": "feat(nn4k): add huggingface decode only model local sft feature (#1) (#109)\n\nCo-authored-by: xionghuaidong <huaidong.xhd@antgroup.com>"
    },
    {
        "repo_url": "github.com/modal-labs/modal-examples",
        "filepath": "06_gpu_and_ml/alpaca/alpaca_lora.py",
        "commit_date": "2024-02-21T15:36:23Z",
        "message": "Convert examples to all use lifecycle method decorators (#586)\n\n* Convert examples to all use lifecycle method decorators\n\n* Reinstante exc_info arguments on exit methods"
    },
    {
        "repo_url": "github.com/modal-labs/modal-examples",
        "filepath": "06_gpu_and_ml/alpaca/alpaca_lora.py",
        "commit_date": "2024-02-12T19:32:23Z",
        "message": "Update Ruff and remove Black (#578)\n\n* Update Ruff and remove Black\n\n* Format with ruff\n\n* Fix typo\n\n* Edit pyproject.toml and run formatting\n\n* Add ruff-format to pre-commit"
    },
    {
        "repo_url": "github.com/modal-labs/modal-examples",
        "filepath": "06_gpu_and_ml/alpaca/alpaca_lora.py",
        "commit_date": "2024-01-07T23:48:32Z",
        "message": "Example simplification using `@build` (#507)\n\n* Example simplification using __build__\n\n* merge __build__ and __enter__\n\n* add back type annotation that disappeared\n\n* rewrite SDXL\n\n* Rewrite it to use decorators\n\n* Update SDXL Turbo\n\n* Rewrite alpaca-lora\n\n* embeddings/instructor.py \u2013 use @build\n\n* Update webcam example"
    },
    {
        "repo_url": "github.com/modal-labs/modal-examples",
        "filepath": "06_gpu_and_ml/alpaca/alpaca_lora.py",
        "commit_date": "2023-11-07T16:44:53Z",
        "message": "Fix examples now that we test against Python 3.11 (#490)"
    },
    {
        "repo_url": "github.com/modal-labs/modal-examples",
        "filepath": "06_gpu_and_ml/alpaca/alpaca_lora.py",
        "commit_date": "2023-08-19T21:06:56Z",
        "message": "Update examples to use .remote (#408)"
    },
    {
        "repo_url": "github.com/modal-labs/modal-examples",
        "filepath": "06_gpu_and_ml/alpaca/alpaca_lora.py",
        "commit_date": "2023-05-01T18:26:09Z",
        "message": "Add model caching & fix params for Alpaca LoRA (#299)"
    },
    {
        "repo_url": "github.com/modal-labs/modal-examples",
        "filepath": "06_gpu_and_ml/alpaca/alpaca_lora.py",
        "commit_date": "2023-04-18T18:49:08Z",
        "message": "Use cls for lifecycle classes (#280)"
    },
    {
        "repo_url": "github.com/modal-labs/modal-examples",
        "filepath": "06_gpu_and_ml/alpaca/alpaca_lora.py",
        "commit_date": "2023-04-06T08:26:34Z",
        "message": "Update deprecated decorators without args to always use () in examples (#269)"
    },
    {
        "repo_url": "github.com/modal-labs/modal-examples",
        "filepath": "06_gpu_and_ml/alpaca/alpaca_lora.py",
        "commit_date": "2023-04-04T14:48:24Z",
        "message": "ML/GPU subdirectory cleanup (#264)"
    },
    {
        "repo_url": "github.com/modal-labs/modal-examples",
        "filepath": "06_gpu_and_ml/alpaca/alpaca_lora.py",
        "commit_date": "2023-03-29T18:32:05Z",
        "message": "basic Alpaca LoRA example (#259)"
    },
    {
        "repo_url": "github.com/catid/supercharger",
        "filepath": "server/model_baize.py",
        "commit_date": "2023-04-11T22:30:27Z",
        "message": "Benchmark and switch to galpaca"
    },
    {
        "repo_url": "github.com/dongyh20/Octopus",
        "filepath": "octopus/LLaVA/llava/model/builder.py",
        "commit_date": "2024-02-28T08:24:27Z",
        "message": "Modified LLaVA and Otter within Octopus"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-08-24T02:10:48Z",
        "message": "Adding support for upstage/solar 0 70b 8bit (#60)\n\n* adding support for upstage/SOLAR-0-70b-8bit\n\n* typo fix"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-08-24T01:56:23Z",
        "message": "adding support for upstage/SOLAR-0-70b-8bit (#59)"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-07-26T22:29:52Z",
        "message": "Fix for loading local models when GPU map is specified"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-07-20T03:22:27Z",
        "message": "Quantize fix for 4bit (#48)"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-07-18T18:13:51Z",
        "message": "Fix stop words"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-07-14T19:28:42Z",
        "message": "OOM fix by forcing device_map (#44)\n\n* OOM fix by forcing device_map\n\n* minor mention fix"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-07-14T08:29:03Z",
        "message": "logic fix"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-07-13T23:43:01Z",
        "message": "Add instructcodet5p-16B (#41)"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-07-13T22:41:24Z",
        "message": "Fix for MPT and CodeGen OOM issues (#40)"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-07-13T15:11:46Z",
        "message": "Add support for LoRA and 4-bit quantization (#36)\n\n* Add support for LoRA and 4-bit quantization\n\n* Fix base model name to be the lora adapter weights\n\n* Added scipy as a dependency for peft\n\n* Install accelerate and peft from source - required for flora\n\n* Update model_utils.py\n\nCo-authored-by: Charles Srisuwananukorn <1967608+csris@users.noreply.github.com>\n\n* Update model_utils.py\n\nCo-authored-by: Charles Srisuwananukorn <1967608+csris@users.noreply.github.com>\n\n* Update model_utils.py\n\nCo-authored-by: Charles Srisuwananukorn <1967608+csris@users.noreply.github.com>\n\n* Update model_utils.py\n\nCo-authored-by: Charles Srisuwananukorn <1967608+csris@users.noreply.github.com>\n\n* Update model_utils.py\n\nCo-authored-by: Charles Srisuwananukorn <1967608+csris@users.noreply.github.com>\n\n* Add specific commits for source installs\n\n---------\n\nCo-authored-by: Charles Srisuwananukorn <1967608+csris@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-07-13T14:07:19Z",
        "message": "Add LLaMA no split module"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-07-13T12:23:59Z",
        "message": "support llama65b and falcon40b (#38)\n\n* support llama65b and falcon40b\n\n* typo\n\n* update falcon models to use the together fork\n\n---------\n\nCo-authored-by: Jared Baldridge <jrb@together.xyz>"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-07-13T03:00:32Z",
        "message": "using together fork of T5 model"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-07-13T02:46:28Z",
        "message": "Add more T5-based models"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-07-11T23:28:56Z",
        "message": "remove return_token_type_ids mentions"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-06-29T20:51:35Z",
        "message": "Fix trust_remote_code for tokenizers"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-06-07T23:55:18Z",
        "message": "Sets fp16 as default dtype, adds dtype support to HF models"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-06-07T23:21:23Z",
        "message": "Moved max_memory to model_utils and cleaned up model_path\n\nMoved max_memory inside get_local_huggingface_tokenizer_model to simplify code structure\nSet model_path to None by default, thus eliminating the if-else block in HuggingFaceLocalNLPModelInference"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-06-07T22:12:37Z",
        "message": "Adds no-return-token-type-ids arg and DecoderLayer\n\nFalcon tokenizer requires return_token_type_ids=False. This can be set by adding the --no-return-token-type-ids flag\nDecoderLayer is the no_split_layer used by the Falcon models. Used for multi-card support."
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-06-07T19:02:03Z",
        "message": "Enable trust_remote_code for Falcon models\n\nThe Falcon models require trust_remote_code=True to be set to get config and models"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-06-07T18:25:24Z",
        "message": "Add support for multi-card inference"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-06-05T21:03:01Z",
        "message": "Fix auth_token (#26)"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-06-05T03:06:55Z",
        "message": "Remove unnecessary code\n\nCleans up auth_token code to eliminate possible type collision"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-06-05T02:15:25Z",
        "message": "Update model_utils.py"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-06-05T02:09:07Z",
        "message": "Update model_utils.py to use auth_token"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-05-26T02:42:52Z",
        "message": "force togethercomputer/GPT-NeoXT-Chat-Base-20B to load in fp16"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-05-09T23:32:48Z",
        "message": "instead of bailing at the bottom of the ifelse tree, just try loading the model name from hf"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-05-09T22:26:38Z",
        "message": "adding additional models to model_utils instead of properly refactoring it"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-05-03T08:40:50Z",
        "message": "add openllama"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-05-03T08:31:53Z",
        "message": "correct togethercomputer/Pythia-Chat-Base-7B-v0.16 name"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-05-03T06:52:49Z",
        "message": "add togethercomputer/Pythia-Chat-Base-7B support"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-05-03T05:18:09Z",
        "message": "adding additional hf models. why refactor this when we can just keep adding to the ladder"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-04-20T03:37:02Z",
        "message": "add stabilityai models\n\nstablelm-base-alpha-7b and stablelm-base-alpha-3b"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-03-24T02:28:44Z",
        "message": "llm.8bit"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-03-08T09:58:05Z",
        "message": "WIP: add top_k/repetition_penalty  (#3)\n\n* minor\n\n* add top_k/repetition_penalty"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-03-07T23:35:50Z",
        "message": "Add dtype to run opt 175m on mac laptop"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-02-27T14:43:04Z",
        "message": "support optiml175"
    },
    {
        "repo_url": "github.com/togethercomputer/Quick_Deployment_HELM",
        "filepath": "model_utils.py",
        "commit_date": "2023-02-07T11:00:54Z",
        "message": "debug glm"
    },
    {
        "repo_url": "github.com/manyoso/haltt4llm",
        "filepath": "generate_trivia.py",
        "commit_date": "2023-03-31T20:55:04Z",
        "message": "Add new TruthfulQA trivia set and update results."
    },
    {
        "repo_url": "github.com/manyoso/haltt4llm",
        "filepath": "generate_trivia.py",
        "commit_date": "2023-03-30T22:38:00Z",
        "message": "And we have a new winner: GPT4All!"
    },
    {
        "repo_url": "github.com/manyoso/haltt4llm",
        "filepath": "generate_trivia.py",
        "commit_date": "2023-03-30T17:17:30Z",
        "message": "Minor prompt handling."
    },
    {
        "repo_url": "github.com/manyoso/haltt4llm",
        "filepath": "generate_trivia.py",
        "commit_date": "2023-03-30T14:48:45Z",
        "message": "Add ability to generate trivia from local model"
    },
    {
        "repo_url": "github.com/manyoso/haltt4llm",
        "filepath": "generate_trivia.py",
        "commit_date": "2023-03-29T22:44:46Z",
        "message": "Add the initial work."
    },
    {
        "repo_url": "github.com/manyoso/haltt4llm",
        "filepath": "take_test.py",
        "commit_date": "2023-03-30T22:38:00Z",
        "message": "And we have a new winner: GPT4All!"
    },
    {
        "repo_url": "github.com/manyoso/haltt4llm",
        "filepath": "take_test.py",
        "commit_date": "2023-03-30T17:17:30Z",
        "message": "Minor prompt handling."
    },
    {
        "repo_url": "github.com/manyoso/haltt4llm",
        "filepath": "take_test.py",
        "commit_date": "2023-03-30T14:48:45Z",
        "message": "Add ability to generate trivia from local model"
    },
    {
        "repo_url": "github.com/manyoso/haltt4llm",
        "filepath": "take_test.py",
        "commit_date": "2023-03-29T22:44:46Z",
        "message": "Add the initial work."
    },
    {
        "repo_url": "github.com/Hritikbansal/sparse_feedback",
        "filepath": "inference/reranking.py",
        "commit_date": "2023-11-16T19:41:23Z",
        "message": "Merge branch 'main' of https://github.com/Hritikbansal/sparse_feedback"
    },
    {
        "repo_url": "github.com/Hritikbansal/sparse_feedback",
        "filepath": "inference/reranking.py",
        "commit_date": "2023-11-16T19:41:00Z",
        "message": "add support for roberta (faster/lighter than alpaca)"
    },
    {
        "repo_url": "github.com/Hritikbansal/sparse_feedback",
        "filepath": "inference/reranking.py",
        "commit_date": "2023-11-13T04:59:14Z",
        "message": "Update reranking.py"
    },
    {
        "repo_url": "github.com/Hritikbansal/sparse_feedback",
        "filepath": "inference/reranking.py",
        "commit_date": "2023-11-13T01:19:20Z",
        "message": "Update reranking.py"
    },
    {
        "repo_url": "github.com/Hritikbansal/sparse_feedback",
        "filepath": "inference/reranking.py",
        "commit_date": "2023-08-29T20:09:21Z",
        "message": "Initial commit message"
    },
    {
        "repo_url": "github.com/thcheung/FactLLaMA",
        "filepath": "generate.py",
        "commit_date": "2023-06-07T08:52:05Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/yeyupiaoling/Chinese-LLM-Chat",
        "filepath": "ChatGLM/merge_lora.py",
        "commit_date": "2023-06-25T08:37:32Z",
        "message": "Guanaco\u652f\u6301\u76f4\u63a5\u52a0\u8f7dLora\u6a21\u578b\u63a8\u7406\uff0c\u652f\u6301\u4f7f\u7528\u5e26\u6709history\u6570\u636e\u5fae\u8c03\uff0cChatGLM\u652f\u6301\u91cf\u5316\u63a8\u7406"
    },
    {
        "repo_url": "github.com/yeyupiaoling/Chinese-LLM-Chat",
        "filepath": "ChatGLM/merge_lora.py",
        "commit_date": "2023-05-14T06:25:39Z",
        "message": "\u4fee\u590d\u53c2\u6570\u8f93\u5165\u548c\u589e\u52a0\u6587\u6863\u8bf4\u660e"
    },
    {
        "repo_url": "github.com/yeyupiaoling/Chinese-LLM-Chat",
        "filepath": "ChatGLM/merge_lora.py",
        "commit_date": "2023-05-11T12:12:35Z",
        "message": "\u4f18\u5316\u4ee3\u7801\u548c\u589e\u52a0\u672c\u5730\u77e5\u8bc6\u68c0\u7d22"
    },
    {
        "repo_url": "github.com/yeyupiaoling/Chinese-LLM-Chat",
        "filepath": "ChatGLM/merge_lora.py",
        "commit_date": "2023-05-10T15:24:30Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/yeyupiaoling/Chinese-LLM-Chat",
        "filepath": "Guanaco/merge_lora.py",
        "commit_date": "2023-06-25T09:28:20Z",
        "message": "Guanaco\u652f\u6301baichuan-7B"
    },
    {
        "repo_url": "github.com/yeyupiaoling/Chinese-LLM-Chat",
        "filepath": "Guanaco/merge_lora.py",
        "commit_date": "2023-06-25T08:37:32Z",
        "message": "Guanaco\u652f\u6301\u76f4\u63a5\u52a0\u8f7dLora\u6a21\u578b\u63a8\u7406\uff0c\u652f\u6301\u4f7f\u7528\u5e26\u6709history\u6570\u636e\u5fae\u8c03\uff0cChatGLM\u652f\u6301\u91cf\u5316\u63a8\u7406"
    },
    {
        "repo_url": "github.com/yeyupiaoling/Chinese-LLM-Chat",
        "filepath": "Guanaco/merge_lora.py",
        "commit_date": "2023-06-21T15:05:56Z",
        "message": "\u4f18\u5316\u8bad\u7ec3\u548c\u9884\u6d4b"
    },
    {
        "repo_url": "github.com/yeyupiaoling/Chinese-LLM-Chat",
        "filepath": "Guanaco/merge_lora.py",
        "commit_date": "2023-06-17T03:08:46Z",
        "message": "\u4fee\u6539\u4e3aQLora\u8bad\u7ec3"
    },
    {
        "repo_url": "github.com/yeyupiaoling/Chinese-LLM-Chat",
        "filepath": "Guanaco/utils/guanaco_predictor.py",
        "commit_date": "2023-06-26T03:55:13Z",
        "message": "\u652f\u6301chatglm2\u63a8\u7406"
    },
    {
        "repo_url": "github.com/yeyupiaoling/Chinese-LLM-Chat",
        "filepath": "Guanaco/utils/guanaco_predictor.py",
        "commit_date": "2023-06-25T09:28:20Z",
        "message": "Guanaco\u652f\u6301baichuan-7B"
    },
    {
        "repo_url": "github.com/yeyupiaoling/Chinese-LLM-Chat",
        "filepath": "Guanaco/utils/guanaco_predictor.py",
        "commit_date": "2023-06-25T08:37:32Z",
        "message": "Guanaco\u652f\u6301\u76f4\u63a5\u52a0\u8f7dLora\u6a21\u578b\u63a8\u7406\uff0c\u652f\u6301\u4f7f\u7528\u5e26\u6709history\u6570\u636e\u5fae\u8c03\uff0cChatGLM\u652f\u6301\u91cf\u5316\u63a8\u7406"
    },
    {
        "repo_url": "github.com/yeyupiaoling/Chinese-LLM-Chat",
        "filepath": "Guanaco/utils/guanaco_predictor.py",
        "commit_date": "2023-06-21T15:05:56Z",
        "message": "\u4f18\u5316\u8bad\u7ec3\u548c\u9884\u6d4b"
    },
    {
        "repo_url": "github.com/yeyupiaoling/Chinese-LLM-Chat",
        "filepath": "Guanaco/utils/guanaco_predictor.py",
        "commit_date": "2023-06-17T03:08:46Z",
        "message": "\u4fee\u6539\u4e3aQLora\u8bad\u7ec3"
    },
    {
        "repo_url": "github.com/abhinand5/tamil-llama",
        "filepath": "scripts/train/utils/merge_adapter.py",
        "commit_date": "2023-11-09T12:51:23Z",
        "message": "Adding Tamil-LLaMA pretrain, finetune and sentencepiece scripts"
    },
    {
        "repo_url": "github.com/run-llama/modal_finetune_sql",
        "filepath": "src/inference_utils.py",
        "commit_date": "2023-08-17T05:41:44Z",
        "message": "cr"
    },
    {
        "repo_url": "github.com/DAMO-NLP-SG/VCD",
        "filepath": "experiments/llava/model/builder.py",
        "commit_date": "2023-11-28T09:36:39Z",
        "message": "merge"
    },
    {
        "repo_url": "github.com/codefuse-ai/MFTCoder",
        "filepath": "mftcoder_atorch/model/build_model.py",
        "commit_date": "2024-01-04T07:59:42Z",
        "message": "MFTCoder v0.3.0 refactor"
    },
    {
        "repo_url": "github.com/codefuse-ai/MFTCoder",
        "filepath": "mftcoder_atorch/model/peft/tuner/adalora.py",
        "commit_date": "2024-01-04T07:59:42Z",
        "message": "MFTCoder v0.3.0 refactor"
    },
    {
        "repo_url": "github.com/codefuse-ai/MFTCoder",
        "filepath": "mftcoder_atorch/model/peft/tuner/unipelt.py",
        "commit_date": "2024-01-04T07:59:42Z",
        "message": "MFTCoder v0.3.0 refactor"
    },
    {
        "repo_url": "github.com/codefuse-ai/MFTCoder",
        "filepath": "mftcoder_atorch/model/peft/modeling_peft.py",
        "commit_date": "2024-01-04T07:59:42Z",
        "message": "MFTCoder v0.3.0 refactor"
    },
    {
        "repo_url": "github.com/codefuse-ai/MFTCoder",
        "filepath": "mftcoder_atorch/model/peft/tuner/routelora.py",
        "commit_date": "2024-01-04T07:59:42Z",
        "message": "MFTCoder v0.3.0 refactor"
    },
    {
        "repo_url": "github.com/codefuse-ai/MFTCoder",
        "filepath": "mftcoder_accelerate/inference/hf_inference.py",
        "commit_date": "2024-01-04T07:59:42Z",
        "message": "MFTCoder v0.3.0 refactor"
    },
    {
        "repo_url": "github.com/codefuse-ai/MFTCoder",
        "filepath": "mftcoder_atorch/train/trainer/atorch_trainer.py",
        "commit_date": "2024-01-04T07:59:42Z",
        "message": "MFTCoder v0.3.0 refactor"
    },
    {
        "repo_url": "github.com/codefuse-ai/MFTCoder",
        "filepath": "mftcoder_atorch/utils/merge_base_and_lora_to_hf.py",
        "commit_date": "2024-01-04T07:59:42Z",
        "message": "MFTCoder v0.3.0 refactor"
    },
    {
        "repo_url": "github.com/codefuse-ai/MFTCoder",
        "filepath": "mftcoder_accelerate/src/pefts/merge_base_and_lora_to_hf.py",
        "commit_date": "2024-01-08T12:29:12Z",
        "message": "bugfix, fsdp for lora"
    },
    {
        "repo_url": "github.com/codefuse-ai/MFTCoder",
        "filepath": "mftcoder_accelerate/src/pefts/merge_base_and_lora_to_hf.py",
        "commit_date": "2024-01-04T07:59:42Z",
        "message": "MFTCoder v0.3.0 refactor"
    },
    {
        "repo_url": "github.com/ssundaram21/dreamsim",
        "filepath": "dreamsim/model.py",
        "commit_date": "2023-07-27T13:59:50Z",
        "message": "fixed caching bug"
    },
    {
        "repo_url": "github.com/ssundaram21/dreamsim",
        "filepath": "dreamsim/model.py",
        "commit_date": "2023-07-14T17:09:27Z",
        "message": "Added finetuned single-branch models"
    },
    {
        "repo_url": "github.com/ssundaram21/dreamsim",
        "filepath": "dreamsim/model.py",
        "commit_date": "2023-06-16T00:07:26Z",
        "message": "Release"
    },
    {
        "repo_url": "github.com/myshell-ai/AIlice",
        "filepath": "ailice/core/llm/AModelLLAMA.py",
        "commit_date": "2024-02-27T09:09:05Z",
        "message": "1. reduce warnings."
    },
    {
        "repo_url": "github.com/myshell-ai/AIlice",
        "filepath": "ailice/core/llm/AModelLLAMA.py",
        "commit_date": "2024-02-23T03:04:58Z",
        "message": "1. removed the peft dependency in the default installation."
    },
    {
        "repo_url": "github.com/myshell-ai/AIlice",
        "filepath": "ailice/core/llm/AModelLLAMA.py",
        "commit_date": "2024-01-26T11:24:32Z",
        "message": "1. config huggingface models in config.json."
    },
    {
        "repo_url": "github.com/myshell-ai/AIlice",
        "filepath": "ailice/core/llm/AModelLLAMA.py",
        "commit_date": "2024-01-26T09:32:30Z",
        "message": "1. LLM configuration has been moved to config.json.\n2. Support for inference services compatible with openai interface."
    },
    {
        "repo_url": "github.com/myshell-ai/AIlice",
        "filepath": "ailice/core/llm/AModelLLAMA.py",
        "commit_date": "2024-01-08T09:27:36Z",
        "message": "1. systemAsUser is model-related, so we will set it separately for each model in ALLMMeta."
    },
    {
        "repo_url": "github.com/myshell-ai/AIlice",
        "filepath": "ailice/core/llm/AModelLLAMA.py",
        "commit_date": "2024-01-08T09:10:03Z",
        "message": "1. set all the systemAsUser to False, including the one in finetuning. it works better in a finetuned model.\nAn unexpected thing is that although systemAsUser was set to the default True before, it was actually running as False because the coder-proxy modified the value when it was initialized. Ideally, we want to run as False, but It is not known whether it makes some models unfit."
    },
    {
        "repo_url": "github.com/myshell-ai/AIlice",
        "filepath": "ailice/core/llm/AModelLLAMA.py",
        "commit_date": "2023-12-29T15:04:00Z",
        "message": "1. use absolute import."
    },
    {
        "repo_url": "github.com/myshell-ai/AIlice",
        "filepath": "ailice/core/llm/AModelLLAMA.py",
        "commit_date": "2023-12-29T12:59:25Z",
        "message": "1. Directory structure adjustment."
    },
    {
        "repo_url": "github.com/generative-ai-on-aws/generative-ai-on-aws",
        "filepath": "07_rlhf/wip/trl_neuron.py",
        "commit_date": "2023-12-20T04:16:14Z",
        "message": "[wip] added trn/inf examples"
    },
    {
        "repo_url": "github.com/michael-wzhu/ChatMed",
        "filepath": "peft/peft_model.py",
        "commit_date": "2023-05-05T14:24:04Z",
        "message": "2023/05/05: update codes & scripts"
    },
    {
        "repo_url": "github.com/michael-wzhu/ChatMed",
        "filepath": "peft/tuners/prompt_tuning.py",
        "commit_date": "2023-05-05T14:24:04Z",
        "message": "2023/05/05: update codes & scripts"
    },
    {
        "repo_url": "github.com/michael-wzhu/ChatMed",
        "filepath": "src/chatmed_llama_peft/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-05-05T10:12:05Z",
        "message": "2023/05/05: update model and readme, and examples"
    },
    {
        "repo_url": "github.com/rmihaylov/falcontune",
        "filepath": "falcontune/model/lora.py",
        "commit_date": "2023-06-09T05:24:24Z",
        "message": "Use quant class instead of instance"
    },
    {
        "repo_url": "github.com/rmihaylov/falcontune",
        "filepath": "falcontune/model/lora.py",
        "commit_date": "2023-05-31T14:39:11Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/rmihaylov/falcontune",
        "filepath": "falcontune/model/falcon/model.py",
        "commit_date": "2023-06-10T05:54:30Z",
        "message": "Revert fix"
    },
    {
        "repo_url": "github.com/rmihaylov/falcontune",
        "filepath": "falcontune/model/falcon/model.py",
        "commit_date": "2023-06-10T05:23:00Z",
        "message": "Fix multi-gpu"
    },
    {
        "repo_url": "github.com/rmihaylov/falcontune",
        "filepath": "falcontune/model/falcon/model.py",
        "commit_date": "2023-06-02T02:47:30Z",
        "message": "Add function to get decoder layer"
    },
    {
        "repo_url": "github.com/rmihaylov/falcontune",
        "filepath": "falcontune/model/falcon/model.py",
        "commit_date": "2023-06-02T02:47:30Z",
        "message": "Fix bos/eos/pad tokens"
    },
    {
        "repo_url": "github.com/rmihaylov/falcontune",
        "filepath": "falcontune/model/falcon/model.py",
        "commit_date": "2023-06-02T02:47:30Z",
        "message": "Update offloading"
    },
    {
        "repo_url": "github.com/rmihaylov/falcontune",
        "filepath": "falcontune/model/falcon/model.py",
        "commit_date": "2023-06-02T02:47:30Z",
        "message": "Remove inv_freq"
    },
    {
        "repo_url": "github.com/rmihaylov/falcontune",
        "filepath": "falcontune/model/falcon/model.py",
        "commit_date": "2023-06-02T02:47:30Z",
        "message": "Add cpu offloading"
    },
    {
        "repo_url": "github.com/rmihaylov/falcontune",
        "filepath": "falcontune/model/falcon/model.py",
        "commit_date": "2023-05-31T14:39:11Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/IceBear-CreditEase-LLM/aigc-admin",
        "filepath": "docker/vicuna/fastchat/model/apply_lora.py",
        "commit_date": "2024-01-27T17:11:09Z",
        "message": "\u521d\u59cb\u5316\u9879\u76ee"
    },
    {
        "repo_url": "github.com/IceBear-CreditEase-LLM/aigc-admin",
        "filepath": "docker/vicuna/fastchat/model/model_adapter.py",
        "commit_date": "2024-01-27T17:11:09Z",
        "message": "\u521d\u59cb\u5316\u9879\u76ee"
    },
    {
        "repo_url": "github.com/X-jun-0130/LLM-Pretrain-FineTune",
        "filepath": "Instructions_FineTune/Bloom_Lora_Sft.py",
        "commit_date": "2023-04-22T12:12:42Z",
        "message": "Lora\u5fae\u8c03"
    },
    {
        "repo_url": "github.com/trestad/mitigating-reversal-curse",
        "filepath": "generate.py",
        "commit_date": "2023-11-14T06:14:26Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/davendw49/k2",
        "filepath": "generation/generate.py",
        "commit_date": "2023-06-20T12:35:51Z",
        "message": "Update generate.py"
    },
    {
        "repo_url": "github.com/davendw49/k2",
        "filepath": "generation/generate.py",
        "commit_date": "2023-06-06T05:11:09Z",
        "message": "update readme"
    },
    {
        "repo_url": "github.com/davendw49/k2",
        "filepath": "evaluation/run_eval.py",
        "commit_date": "2023-10-13T01:34:53Z",
        "message": "eval"
    },
    {
        "repo_url": "github.com/davendw49/k2",
        "filepath": "evaluation/run_eval.py",
        "commit_date": "2023-10-12T16:43:08Z",
        "message": "add eval"
    },
    {
        "repo_url": "github.com/davendw49/k2",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-06-05T19:16:37Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/davendw49/k2",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-06-05T19:16:37Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/pminervini/llm-service",
        "filepath": "chainer/base.py",
        "commit_date": "2023-08-23T08:59:26Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/pminervini/llm-service",
        "filepath": "chainer/base.py",
        "commit_date": "2023-08-17T12:55:32Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/pminervini/llm-service",
        "filepath": "chainer/base.py",
        "commit_date": "2023-08-03T09:47:17Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/pminervini/llm-service",
        "filepath": "chainer/base.py",
        "commit_date": "2023-07-29T08:02:46Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/pminervini/llm-service",
        "filepath": "chainer/base.py",
        "commit_date": "2023-07-24T10:48:33Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/hppRC/llm-translator",
        "filepath": "src/demo.py",
        "commit_date": "2023-12-30T15:47:57Z",
        "message": "add scripts"
    },
    {
        "repo_url": "github.com/hppRC/llm-translator",
        "filepath": "src/misc/upload.py",
        "commit_date": "2023-12-30T15:47:57Z",
        "message": "add scripts"
    },
    {
        "repo_url": "github.com/hppRC/llm-translator",
        "filepath": "src/run_trained.py",
        "commit_date": "2023-12-30T15:47:57Z",
        "message": "add scripts"
    },
    {
        "repo_url": "github.com/dvlab-research/LLMGA",
        "filepath": "llmga/llava/model/builder.py",
        "commit_date": "2023-12-07T08:20:07Z",
        "message": "Update builder.py"
    },
    {
        "repo_url": "github.com/dvlab-research/LLMGA",
        "filepath": "llmga/llava/model/builder.py",
        "commit_date": "2023-11-29T03:28:28Z",
        "message": "LLMGA"
    },
    {
        "repo_url": "github.com/dvlab-research/LLMGA",
        "filepath": "llmga/diffusers/src/diffusers/models/modeling_utils.py",
        "commit_date": "2023-11-29T03:28:28Z",
        "message": "LLMGA"
    },
    {
        "repo_url": "github.com/Strong-AI-Lab/Logical-and-abstract-reasoning",
        "filepath": "src/models/hf.py",
        "commit_date": "2024-01-17T10:43:40Z",
        "message": "updated configs and added validation sets"
    },
    {
        "repo_url": "github.com/Strong-AI-Lab/Logical-and-abstract-reasoning",
        "filepath": "src/models/hf.py",
        "commit_date": "2023-11-23T00:13:42Z",
        "message": "modified tokenizer configuration for compatibility with Transformers v4.35.2"
    },
    {
        "repo_url": "github.com/Strong-AI-Lab/Logical-and-abstract-reasoning",
        "filepath": "src/models/hf.py",
        "commit_date": "2023-11-22T22:43:34Z",
        "message": "added LLaMA2 and Zephyr models"
    },
    {
        "repo_url": "github.com/Strong-AI-Lab/Logical-and-abstract-reasoning",
        "filepath": "src/models/hf.py",
        "commit_date": "2023-09-26T08:47:03Z",
        "message": "added LoRA fine-tuning"
    },
    {
        "repo_url": "github.com/Strong-AI-Lab/Logical-and-abstract-reasoning",
        "filepath": "src/models/hf.py",
        "commit_date": "2023-09-14T09:29:18Z",
        "message": "LoRA fine-tuning"
    },
    {
        "repo_url": "github.com/Strong-AI-Lab/Logical-and-abstract-reasoning",
        "filepath": "src/models/hf.py",
        "commit_date": "2023-05-31T03:16:28Z",
        "message": "correction label formatting for fine-tuning"
    },
    {
        "repo_url": "github.com/Strong-AI-Lab/Logical-and-abstract-reasoning",
        "filepath": "src/models/hf.py",
        "commit_date": "2023-05-23T07:18:01Z",
        "message": "added llama and merit configs"
    },
    {
        "repo_url": "github.com/Strong-AI-Lab/Logical-and-abstract-reasoning",
        "filepath": "src/models/hf.py",
        "commit_date": "2023-05-15T04:58:44Z",
        "message": "added roberta fine-tuning and fixed bug"
    },
    {
        "repo_url": "github.com/Strong-AI-Lab/Logical-and-abstract-reasoning",
        "filepath": "src/models/hf.py",
        "commit_date": "2023-05-13T09:44:34Z",
        "message": "added Vicuna model and config"
    },
    {
        "repo_url": "github.com/Strong-AI-Lab/Logical-and-abstract-reasoning",
        "filepath": "src/models/hf.py",
        "commit_date": "2023-05-11T00:02:10Z",
        "message": "Added LLama and Alpca models"
    },
    {
        "repo_url": "github.com/Strong-AI-Lab/Logical-and-abstract-reasoning",
        "filepath": "src/models/hf.py",
        "commit_date": "2023-05-08T06:05:31Z",
        "message": "fix fine-tuning batch and token issues for MCQA"
    },
    {
        "repo_url": "github.com/Strong-AI-Lab/Logical-and-abstract-reasoning",
        "filepath": "src/models/hf.py",
        "commit_date": "2023-05-04T02:40:02Z",
        "message": "added fine-tuning script"
    },
    {
        "repo_url": "github.com/Strong-AI-Lab/Logical-and-abstract-reasoning",
        "filepath": "src/models/hf.py",
        "commit_date": "2023-05-03T04:05:09Z",
        "message": "fix HF generation split between answer and context"
    },
    {
        "repo_url": "github.com/Strong-AI-Lab/Logical-and-abstract-reasoning",
        "filepath": "src/models/hf.py",
        "commit_date": "2023-05-01T06:35:15Z",
        "message": "Merge branch 'dev' of https://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning into dev"
    },
    {
        "repo_url": "github.com/Strong-AI-Lab/Logical-and-abstract-reasoning",
        "filepath": "src/models/hf.py",
        "commit_date": "2023-05-01T06:26:58Z",
        "message": "fix truncation in HF completion models"
    },
    {
        "repo_url": "github.com/Strong-AI-Lab/Logical-and-abstract-reasoning",
        "filepath": "src/models/hf.py",
        "commit_date": "2023-05-01T05:03:18Z",
        "message": "update config for gpt-4 and new dataset"
    },
    {
        "repo_url": "github.com/Strong-AI-Lab/Logical-and-abstract-reasoning",
        "filepath": "src/models/hf.py",
        "commit_date": "2023-05-01T03:19:10Z",
        "message": "update model config"
    },
    {
        "repo_url": "github.com/Strong-AI-Lab/Logical-and-abstract-reasoning",
        "filepath": "src/models/hf.py",
        "commit_date": "2023-04-30T07:58:03Z",
        "message": "fix gpu use for MCQA models"
    },
    {
        "repo_url": "github.com/Strong-AI-Lab/Logical-and-abstract-reasoning",
        "filepath": "src/models/hf.py",
        "commit_date": "2023-04-28T05:55:57Z",
        "message": "better handling of huggingface generation"
    },
    {
        "repo_url": "github.com/Strong-AI-Lab/Logical-and-abstract-reasoning",
        "filepath": "src/models/hf.py",
        "commit_date": "2023-04-27T23:26:32Z",
        "message": "gpu handling for HF models"
    },
    {
        "repo_url": "github.com/Strong-AI-Lab/Logical-and-abstract-reasoning",
        "filepath": "src/models/hf.py",
        "commit_date": "2023-04-24T02:11:28Z",
        "message": "configuration for alpaca model"
    },
    {
        "repo_url": "github.com/Strong-AI-Lab/Logical-and-abstract-reasoning",
        "filepath": "src/models/hf.py",
        "commit_date": "2023-04-23T23:20:19Z",
        "message": "handling of batch inputs"
    },
    {
        "repo_url": "github.com/Strong-AI-Lab/Logical-and-abstract-reasoning",
        "filepath": "src/models/hf.py",
        "commit_date": "2023-04-20T22:04:34Z",
        "message": "logit correction and simple logging"
    },
    {
        "repo_url": "github.com/Strong-AI-Lab/Logical-and-abstract-reasoning",
        "filepath": "src/models/hf.py",
        "commit_date": "2023-04-20T11:32:41Z",
        "message": "generic evaluation script and models"
    },
    {
        "repo_url": "github.com/alohachen/Hide-and-Seek",
        "filepath": "demo_model.py",
        "commit_date": "2023-09-05T09:25:38Z",
        "message": "Initial commit\n\nUpload the hide and seek model's weight file and the inference demonstration code."
    },
    {
        "repo_url": "github.com/alohachen/Hide-and-Seek",
        "filepath": "demo_label.py",
        "commit_date": "2023-09-05T09:25:38Z",
        "message": "Initial commit\n\nUpload the hide and seek model's weight file and the inference demonstration code."
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-12-08T04:07:40Z",
        "message": "\u652f\u6301Yi\u6a21\u578b"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-11-23T10:06:03Z",
        "message": "1.\u7b80\u5316\u63a8\u65ad\u4ee3\u7801\uff1b2.internlm\u652f\u6301rope"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-11-02T04:14:15Z",
        "message": "\u652f\u6301aquila2\uff0c\u4ee5\u53caaquilachat2-7b-16k\u7684NTK"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-10-30T08:31:54Z",
        "message": "the Flash Attention method of the HF official currently only supports a few models."
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-10-30T08:17:12Z",
        "message": "\u901a\u8fc7attention_sinks\u652f\u6301StreamingLLM"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-10-30T06:03:04Z",
        "message": "modify name"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-10-30T03:42:37Z",
        "message": "chatglm3-6b-32k\u4e5f\u652f\u6301\u901a\u8fc7rope_ratio\u6765\u4f7f\u7528Rope"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-10-30T03:35:00Z",
        "message": "1.\u4fee\u590d\u5224\u65ad\u4f7f\u7528qlora\u9519\u8bef\u7684bug\uff1b2.\u5c06\u641c\u627e\u6240\u6709\u5168\u8fde\u63a5\u5c42\u653e\u5230model.py\u6587\u4ef6\uff1b3.bnb\u60c5\u51b5\u4e0b\u4e3achatglm\u7684\u91cf\u5316\u4e4b\u540e\u518d\u8fdb\u884cGPU\u5206\u914d"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-10-26T08:21:02Z",
        "message": "\u6307\u5b9a\u52a0\u8f7d\u6a21\u578b\u65f6\u7684\u7cbe\u5ea6"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-10-25T02:58:24Z",
        "message": "Update models.py"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-10-25T02:28:34Z",
        "message": "\u4fee\u6539\u4ee3\u7801\u4f7f\u5176\u7b26\u5408PEP8\u89c4\u8303"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-10-24T16:24:14Z",
        "message": "\u652f\u6301\u8bcd\u8868\u6269\u5145\u529f\u80fd"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-10-23T09:43:13Z",
        "message": "delete a line which is non sense"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-10-23T09:19:11Z",
        "message": "1.\u4fee\u6b63\u4e00\u5904qlore\u62fc\u5199\u7b14\u8bef\n2.\u8bcd\u8868\u6269\u5c55resize\u5c31\u53ef\u4ee5\u4e86\uff0clm_header\u4f1a\u81ea\u52a8\u53d8\u5316\u7684\uff0c\u800c\u4e14\u8fd9\u91cc\u7684\u64cd\u4f5c\u5f88\u591a\u60c5\u51b5\u90fd\u6ca1\u6709\u8003\u8651\u5230\uff0c\u6bd4\u5982_hf_hook\u548cdeep speed zero stage3\u521d\u59cb\u5316\u7684\u652f\u6301\uff0c\u53ef\u4ee5\u53c2\u8003modeling_utils.PreTrainedModel._get_resized_lm_head\u65b9\u6cd5"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-10-12T11:02:01Z",
        "message": "support Mistral model"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-10-09T03:36:21Z",
        "message": "\u589e\u52a0\u6269\u5145\u8bcd\u8868\u540eEmbedding\u521d\u59cb\u5316\u65b9\u5f0f"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-10-08T07:07:58Z",
        "message": "LLama\u548cFalcon\u4e24\u7c7b\u6a21\u578b\u652f\u6301Flash Attention2"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-09-27T02:34:36Z",
        "message": "Update models.py"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-09-25T11:29:17Z",
        "message": "LLAMA\u7684Flash Attention\u652f\u6301(\u672a\u6d4b\u8bd5)"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-09-22T15:16:26Z",
        "message": "tigerbot\u901a\u8fc7template prompt\u652f\u6301\uff0c\u66f4\u65b0\u6587\u6863"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-09-21T14:40:05Z",
        "message": "\u63a8\u65ad\u663e\u5b58\u518d\u5206\u914d\u6a21\u578b"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-09-18T07:03:44Z",
        "message": "\u652f\u6301tigerbot\u5fae\u8c03"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-09-07T10:03:15Z",
        "message": "\u652f\u6301\u4f7f\u7528Rope\u7f16\u7801\u65b9\u5f0f\u7684Falcon\u4f7f\u7528ntk\u65b9\u5f0f"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-09-04T10:04:07Z",
        "message": "\u652f\u6301\u90e8\u5206\u6a21\u578b\u5728\u63a8\u65ad\u7684\u65f6\u5019\u4f7f\u7528ntk\u65b9\u6cd5"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-08-28T02:55:52Z",
        "message": "\u53ea\u652f\u6301lora\u548cadalora\u8bad\u7ec3\u540e\u7684\u6743\u91cd\u5408\u5e76"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-08-28T02:40:08Z",
        "message": "\u9664\u4e86lora\u548cadalora\u4ee5\u5916\u6a21\u578b\u4e0d\u652f\u6301merge_and_unload\u65b9\u6cd5"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-08-24T09:25:41Z",
        "message": "fix"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-08-24T09:23:44Z",
        "message": "\u652f\u6301deepspeed\u8bad\u7ec3"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-08-23T08:19:35Z",
        "message": "\u652f\u6301DPO\u7684\u8bad\u7ec3"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-08-23T02:39:16Z",
        "message": "\u589e\u52a0\u5bf9xverse\u6a21\u578b\u7684\u652f\u6301"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-08-22T03:30:19Z",
        "message": "\u89e3\u51b3PPO\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6a21\u578b\u751f\u6210\u7684\u95ee\u9898"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-08-21T10:07:12Z",
        "message": "\u652f\u6301RLHF\u4e4bPPO\u8bad\u7ec3"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-08-17T03:39:03Z",
        "message": "\u7ee7\u7eed\u5b8c\u6210PPO"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-08-16T15:37:13Z",
        "message": "fix"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-08-16T14:29:17Z",
        "message": "fix"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-08-16T12:09:25Z",
        "message": "\u5c06\u57fa\u7c7b\u4e2d\u7684model\u52a0\u8f7d\u65b9\u6cd5\u5206\u79bb\u4ee5\u9002\u5e94PPO\u6a21\u578b\u5f3a\u5316\u8bad\u7ec3"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-08-15T11:48:05Z",
        "message": "\u52a0\u8f7d\u5956\u52b1\u6a21\u578b\u8fdb\u884c\u6279\u91cf\u8bad\u7ec3"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-08-15T09:59:30Z",
        "message": "\u5956\u52b1\u6a21\u578b\u6279\u91cf\u6d4b\u8bd5"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-08-10T03:51:29Z",
        "message": "\u4fdd\u5b58\u5956\u52b1\u6a21\u578b"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-08-09T12:11:51Z",
        "message": "bug-fix"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-08-09T10:14:21Z",
        "message": "simplify code"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-08-09T05:12:49Z",
        "message": "Update models.py"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-08-09T05:07:24Z",
        "message": "bug-fix"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-08-09T02:35:56Z",
        "message": "Update models.py"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-08-08T16:14:34Z",
        "message": "bug-fix"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-08-08T15:53:05Z",
        "message": "cpm\u91cf\u5316\u4e0d\u7528\u81ea\u5df1\u5b9a\u4e49\u7ebf\u6027\u5c42"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-08-06T15:32:54Z",
        "message": "\u5956\u52b1\u6a21\u578b\u8bad\u7ec3"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-08-03T11:51:49Z",
        "message": "\u66f4\u65b0\u901a\u4e49\u5343\u95ee\u7684\u652f\u6301"
    },
    {
        "repo_url": "github.com/StanleyLsx/llms_tool",
        "filepath": "engines/models.py",
        "commit_date": "2023-07-25T13:53:26Z",
        "message": "\u521d\u59cb\u4ed3\u5e93"
    },
    {
        "repo_url": "github.com/thunlp/ChatEval",
        "filepath": "FastChat/fastchat/model/apply_lora.py",
        "commit_date": "2023-08-18T12:57:58Z",
        "message": "vicuna demo executable"
    },
    {
        "repo_url": "github.com/thunlp/ChatEval",
        "filepath": "FastChat/fastchat/model/model_adapter.py",
        "commit_date": "2023-08-18T12:57:58Z",
        "message": "vicuna demo executable"
    },
    {
        "repo_url": "github.com/zhengbw0324/LC-Rec",
        "filepath": "test.py",
        "commit_date": "2023-11-15T12:36:46Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/zhengbw0324/LC-Rec",
        "filepath": "test_ddp.py",
        "commit_date": "2023-11-15T12:36:46Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/icoz69/StableLLAVA",
        "filepath": "llava/model/builder.py",
        "commit_date": "2023-12-16T12:10:39Z",
        "message": "Add eval models"
    },
    {
        "repo_url": "github.com/PKU-YuanGroup/Chat-UniVi",
        "filepath": "ChatUniVi/model/builder.py",
        "commit_date": "2023-11-15T03:07:24Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/FSoft-AI4Code/CodeCapybara",
        "filepath": "main/train.py",
        "commit_date": "2023-04-26T06:48:20Z",
        "message": "update hf model"
    },
    {
        "repo_url": "github.com/FSoft-AI4Code/CodeCapybara",
        "filepath": "main/train.py",
        "commit_date": "2023-04-24T13:49:20Z",
        "message": "add docs for training code"
    },
    {
        "repo_url": "github.com/FSoft-AI4Code/CodeCapybara",
        "filepath": "main/train.py",
        "commit_date": "2023-04-24T10:26:59Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/FSoft-AI4Code/CodeCapybara",
        "filepath": "main/generate.py",
        "commit_date": "2023-04-24T13:02:47Z",
        "message": "Updated README.md"
    },
    {
        "repo_url": "github.com/codefuse-ai/Test-Agent",
        "filepath": "chat/model/apply_lora.py",
        "commit_date": "2023-10-20T14:14:57Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/codefuse-ai/Test-Agent",
        "filepath": "chat/model/model_adapter.py",
        "commit_date": "2023-10-20T14:14:57Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-09-26T17:53:51Z",
        "message": "Fix multiple do_sample arguments"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-07-03T05:23:00Z",
        "message": "Update modeling.py OpenAIModel to use normal API"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-06-30T15:43:15Z",
        "message": "Code formatting with black"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-06-30T15:22:42Z",
        "message": "Support sampling during generation"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-06-30T15:22:42Z",
        "message": "More robust querying for OpenAIModel"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-06-05T09:05:46Z",
        "message": "Rename"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-06-03T09:10:13Z",
        "message": "Fixed error in modeling"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-06-02T20:59:26Z",
        "message": "Update modeling seq_to_seq get_choices"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-06-02T10:19:50Z",
        "message": "Refactored hhh.py"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-05-11T07:16:46Z",
        "message": "Add GPTQModel for LLaMA-65B"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-05-07T15:19:07Z",
        "message": "RWKVModel bugfix"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-05-07T14:27:16Z",
        "message": "Support RWKV"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-05-06T09:52:58Z",
        "message": "Support causal models with remote code (eg Mosaic, StarCoder)"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-05-04T17:57:19Z",
        "message": "Support OpenAI models"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-04-17T15:13:09Z",
        "message": "Support lora for seq_to_seq models (eg declare-lab/flan-alpaca-xl-lora)"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-04-17T15:13:09Z",
        "message": "Support llama-lora models with peft"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-04-16T09:01:27Z",
        "message": "Refactor for humaneval"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-04-11T16:25:22Z",
        "message": "Merge pull request #1 from declare-lab/humaneval\n\nHumaneval"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-04-09T18:43:40Z",
        "message": "Add Vicuna-13B, OpenChatKit-20B"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-04-09T16:01:03Z",
        "message": "Add inference function for human eval"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-04-09T05:13:54Z",
        "message": "Add Koala-13B results for MMLU and BBH"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-04-08T18:28:05Z",
        "message": "Support load_8bit and add 13B llama/alpaca results"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-04-03T15:53:01Z",
        "message": "Refactor select_model for easier debugging"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-03-31T06:14:53Z",
        "message": "Add ChatGLM-6B"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-03-28T18:01:46Z",
        "message": "Add llama and alpaca results (no template works better)"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-03-28T13:50:10Z",
        "message": "Support CausalModel and LlamaModel in modeling.py"
    },
    {
        "repo_url": "github.com/declare-lab/instruct-eval",
        "filepath": "modeling.py",
        "commit_date": "2023-03-28T09:09:44Z",
        "message": "Refactor model generation to modeling.py"
    },
    {
        "repo_url": "github.com/princeton-nlp/AutoCompressors",
        "filepath": "train.py",
        "commit_date": "2024-02-19T06:36:54Z",
        "message": "Fix BF16 issue"
    },
    {
        "repo_url": "github.com/princeton-nlp/AutoCompressors",
        "filepath": "train.py",
        "commit_date": "2023-12-18T20:16:01Z",
        "message": "chore: rm dbg find unused parameters"
    },
    {
        "repo_url": "github.com/princeton-nlp/AutoCompressors",
        "filepath": "train.py",
        "commit_date": "2023-12-18T20:12:42Z",
        "message": "chore: rm dbg code"
    },
    {
        "repo_url": "github.com/princeton-nlp/AutoCompressors",
        "filepath": "train.py",
        "commit_date": "2023-12-18T19:56:08Z",
        "message": "fix: wrong llama import module name"
    },
    {
        "repo_url": "github.com/princeton-nlp/AutoCompressors",
        "filepath": "train.py",
        "commit_date": "2023-10-27T00:08:09Z",
        "message": "Rename Llama architecture and simplify args"
    },
    {
        "repo_url": "github.com/princeton-nlp/AutoCompressors",
        "filepath": "train.py",
        "commit_date": "2023-10-26T16:09:37Z",
        "message": "committing local changes"
    },
    {
        "repo_url": "github.com/princeton-nlp/AutoCompressors",
        "filepath": "train.py",
        "commit_date": "2023-10-11T15:44:42Z",
        "message": "Flash Llama + LoRA"
    },
    {
        "repo_url": "github.com/princeton-nlp/AutoCompressors",
        "filepath": "train.py",
        "commit_date": "2023-07-20T17:39:03Z",
        "message": "Support for training with fast attention"
    },
    {
        "repo_url": "github.com/princeton-nlp/AutoCompressors",
        "filepath": "train.py",
        "commit_date": "2023-06-27T06:28:24Z",
        "message": "Initial code release"
    },
    {
        "repo_url": "github.com/blcuicall/taoli",
        "filepath": "src/generate.py",
        "commit_date": "2023-06-09T01:10:04Z",
        "message": "Taoli llama"
    },
    {
        "repo_url": "github.com/blcuicall/taoli",
        "filepath": "src/generate.py",
        "commit_date": "2023-06-07T10:06:26Z",
        "message": "Chinese llama base"
    },
    {
        "repo_url": "github.com/blcuicall/taoli",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-06-09T01:11:15Z",
        "message": "Taoli llama"
    },
    {
        "repo_url": "github.com/blcuicall/taoli",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-06-07T10:06:26Z",
        "message": "Chinese llama base"
    },
    {
        "repo_url": "github.com/blcuicall/taoli",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-06-09T01:11:15Z",
        "message": "Taoli llama"
    },
    {
        "repo_url": "github.com/blcuicall/taoli",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-06-07T10:06:26Z",
        "message": "Chinese llama base"
    },
    {
        "repo_url": "github.com/kaistAI/LangBridge",
        "filepath": "evaluation-harness/lm_eval/models/huggingface.py",
        "commit_date": "2024-01-23T12:41:26Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/agiresearch/OpenP5",
        "filepath": "src/src_llama/generate_llama.py",
        "commit_date": "2024-02-04T23:11:01Z",
        "message": "update the framework"
    },
    {
        "repo_url": "github.com/agiresearch/OpenP5",
        "filepath": "src/src_llama/.ipynb_checkpoints/recommendation-checkpoint.py",
        "commit_date": "2024-02-04T23:11:01Z",
        "message": "update the framework"
    },
    {
        "repo_url": "github.com/agiresearch/OpenP5",
        "filepath": "src/src_llama/.ipynb_checkpoints/generate_llama-checkpoint.py",
        "commit_date": "2024-02-04T23:11:01Z",
        "message": "update the framework"
    },
    {
        "repo_url": "github.com/maziarraissi/PetGPT",
        "filepath": "alpaca-lora/generate.py",
        "commit_date": "2023-04-02T23:18:57Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/maziarraissi/PetGPT",
        "filepath": "alpaca-lora/export_hf_checkpoint.py",
        "commit_date": "2023-04-02T23:18:57Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/maziarraissi/PetGPT",
        "filepath": "alpaca-lora/export_state_dict_checkpoint.py",
        "commit_date": "2023-04-02T23:18:57Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/maziarraissi/PetGPT",
        "filepath": "alpaca-lora/.ipynb_checkpoints/generate-checkpoint.py",
        "commit_date": "2023-04-02T23:18:57Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/maziarraissi/PetGPT",
        "filepath": "alpaca-lora/.ipynb_checkpoints/export_hf_checkpoint-checkpoint.py",
        "commit_date": "2023-04-02T23:18:57Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/maziarraissi/PetGPT",
        "filepath": "alpaca-lora/.ipynb_checkpoints/export_state_dict_checkpoint-checkpoint.py",
        "commit_date": "2023-04-02T23:18:57Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/xiaoman-zhang/PMC-VQA",
        "filepath": "src/MedVInT_TE/peft/peft_model.py",
        "commit_date": "2023-08-03T14:34:33Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/xiaoman-zhang/PMC-VQA",
        "filepath": "src/MedVInT_TE/peft/tuners/prompt_tuning.py",
        "commit_date": "2023-08-03T14:34:33Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/mpt/mpt-7b-8k/04_fine_tune_qlora.py",
        "commit_date": "2024-02-15T06:36:11Z",
        "message": "Add script for text generation examples (#103)\n\nAdd a script to generate the example notebooks for text-generation models"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/falcon/falcon-40b/06_fine_tune_qlora.py",
        "commit_date": "2023-08-23T22:13:26Z",
        "message": "Inference code"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/falcon/falcon-40b/06_fine_tune_qlora.py",
        "commit_date": "2023-08-23T21:49:54Z",
        "message": "Add Falcon-40B QLoRA example"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/mistral/mistral-7b/06_fine_tune_qlora.py",
        "commit_date": "2023-10-09T06:16:56Z",
        "message": "Add example notebooks for mistral-7b model (#76)\n\n* Add example notebooks for mistral model\n\n* fix\n\n* Update and address the comments\n\n* address the comments\n\n* add readme\n\n---------\n\nCo-authored-by: lu-wang-dl <lu-wang-dl>"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/llamav2/llamav2-7b/06_fine_tune_qlora.py",
        "commit_date": "2023-11-29T19:30:50Z",
        "message": "proposed fix form tokenizer not found. (#96)"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/llamav2/llamav2-7b/06_fine_tune_qlora.py",
        "commit_date": "2023-11-22T22:09:00Z",
        "message": "Removed dataset version dependency and defaulted to latest for llama2 models (#94)\n\n* Quick-Fix for dataset package\n\n* Fixed datasets package deps for all other llama2 models"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/llamav2/llamav2-7b/06_fine_tune_qlora.py",
        "commit_date": "2023-08-19T00:17:45Z",
        "message": "llama2 7b and llama213b azure instance types"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/llamav2/llamav2-7b/06_fine_tune_qlora.py",
        "commit_date": "2023-08-10T22:00:46Z",
        "message": "Update 06_fine_tune_qlora.py"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/llamav2/llamav2-7b/06_fine_tune_qlora.py",
        "commit_date": "2023-08-09T10:32:09Z",
        "message": "HF transformers fix - https://discuss.huggingface.co/t/help-with-llama-2-finetuning-setup/50035/4"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/llamav2/llamav2-7b/06_fine_tune_qlora.py",
        "commit_date": "2023-08-08T18:08:50Z",
        "message": "Add do_sample"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/llamav2/llamav2-7b/06_fine_tune_qlora.py",
        "commit_date": "2023-08-08T06:37:01Z",
        "message": "Ping accelerate version to avoid bugs"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/llamav2/llamav2-7b/06_fine_tune_qlora.py",
        "commit_date": "2023-08-04T18:29:00Z",
        "message": "Update 06_fine_tune_qlora.py"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/llamav2/llamav2-7b/06_fine_tune_qlora.py",
        "commit_date": "2023-08-04T18:24:34Z",
        "message": "Update 06_fine_tune_qlora.py\n\nUpdate the base model for lora fine tune."
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/llamav2/llamav2-7b/06_fine_tune_qlora.py",
        "commit_date": "2023-07-24T16:51:10Z",
        "message": "Update QLORA example (#33)"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/llamav2/llamav2-7b/06_fine_tune_qlora.py",
        "commit_date": "2023-07-19T04:13:57Z",
        "message": "Update the example (#28)\n\nCo-authored-by: lu-wang-dl <lu-wang-dl>"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/llamav2/llamav2-7b/06_fine_tune_qlora.py",
        "commit_date": "2023-07-18T17:47:30Z",
        "message": "Add examples for llama-2 (#24)\n\n* format\n\n* Add llama-2 example\n\n* Add readme\n\n---------\n\nCo-authored-by: lu-wang-dl <lu-wang-dl>"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/llamav2/llamav2-13b/06_fine_tune_qlora.py",
        "commit_date": "2023-11-22T22:09:00Z",
        "message": "Removed dataset version dependency and defaulted to latest for llama2 models (#94)\n\n* Quick-Fix for dataset package\n\n* Fixed datasets package deps for all other llama2 models"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/llamav2/llamav2-13b/06_fine_tune_qlora.py",
        "commit_date": "2023-08-22T20:18:26Z",
        "message": "Recommend AWS instance types"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/llamav2/llamav2-13b/06_fine_tune_qlora.py",
        "commit_date": "2023-08-19T00:17:45Z",
        "message": "llama2 7b and llama213b azure instance types"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/llamav2/llamav2-13b/06_fine_tune_qlora.py",
        "commit_date": "2023-08-16T05:53:46Z",
        "message": "Fix llama 2 13b qlora example"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/llamav2/llamav2-13b/06_fine_tune_qlora.py",
        "commit_date": "2023-08-10T22:02:41Z",
        "message": "Update 06_fine_tune_qlora.py"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/llamav2/llamav2-13b/06_fine_tune_qlora.py",
        "commit_date": "2023-07-31T19:07:56Z",
        "message": "[ML-33321] Add qlora fine-tuning notebook for llama2-13b (#38)\n\n* Init\n\n* Clean\n\n* A100 comment"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/llamav2/llamav2-70b/06_fine_tune_qlora.py",
        "commit_date": "2023-11-22T22:09:00Z",
        "message": "Removed dataset version dependency and defaulted to latest for llama2 models (#94)\n\n* Quick-Fix for dataset package\n\n* Fixed datasets package deps for all other llama2 models"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/llamav2/llamav2-70b/06_fine_tune_qlora.py",
        "commit_date": "2023-09-07T06:25:53Z",
        "message": "Add qlora fine tune example for 70b model (#61)\n\nCo-authored-by: lu-wang-dl <lu-wang-dl>"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/mistral/mistral-7b/06_fine_tune_qlora_marketplace.py",
        "commit_date": "2023-11-01T19:25:36Z",
        "message": "Add fine tune mistrial_7b from marketplace (#87)\n\nCo-authored-by: lu-wang-dl <lu-wang-dl>"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/llamav2/llamav2-7b/06_fine_tune_qlora_marketplace.py",
        "commit_date": "2023-11-22T22:09:00Z",
        "message": "Removed dataset version dependency and defaulted to latest for llama2 models (#94)\n\n* Quick-Fix for dataset package\n\n* Fixed datasets package deps for all other llama2 models"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/llamav2/llamav2-7b/06_fine_tune_qlora_marketplace.py",
        "commit_date": "2023-10-26T18:45:53Z",
        "message": "Add QLORA fine tune example for the models from marketplace (#85)\n\n* Add qlora from marketplace\n\n* Update the model serving part\n\n* address the comments\n\n* Add examples for 13b model\n\n---------\n\nCo-authored-by: lu-wang-dl <lu-wang-dl>"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/llamav2/llamav2-13b/06_fine_tune_qlora_marketplace.py",
        "commit_date": "2023-11-22T22:09:00Z",
        "message": "Removed dataset version dependency and defaulted to latest for llama2 models (#94)\n\n* Quick-Fix for dataset package\n\n* Fixed datasets package deps for all other llama2 models"
    },
    {
        "repo_url": "github.com/databricks/databricks-ml-examples",
        "filepath": "llm-models/llamav2/llamav2-13b/06_fine_tune_qlora_marketplace.py",
        "commit_date": "2023-10-26T18:45:53Z",
        "message": "Add QLORA fine tune example for the models from marketplace (#85)\n\n* Add qlora from marketplace\n\n* Update the model serving part\n\n* address the comments\n\n* Add examples for 13b model\n\n---------\n\nCo-authored-by: lu-wang-dl <lu-wang-dl>"
    },
    {
        "repo_url": "github.com/SHI-Labs/VCoder",
        "filepath": "vcoder_llava/model/builder.py",
        "commit_date": "2023-12-25T09:50:41Z",
        "message": ":tada: Release VCoder"
    },
    {
        "repo_url": "github.com/llava-rlhf/LLaVA-RLHF",
        "filepath": "Eval/model_vqa.py",
        "commit_date": "2023-11-01T02:19:38Z",
        "message": "add mmhal-bench evaluation script"
    },
    {
        "repo_url": "github.com/llava-rlhf/LLaVA-RLHF",
        "filepath": "Eval/model_vqa.py",
        "commit_date": "2023-10-19T21:33:16Z",
        "message": "eval sft"
    },
    {
        "repo_url": "github.com/llava-rlhf/LLaVA-RLHF",
        "filepath": "demo/model_worker.py",
        "commit_date": "2023-10-08T09:44:58Z",
        "message": "add a minimal example for launching demo"
    },
    {
        "repo_url": "github.com/llava-rlhf/LLaVA-RLHF",
        "filepath": "Eval/model_mmbench.py",
        "commit_date": "2023-10-19T21:33:16Z",
        "message": "eval sft"
    },
    {
        "repo_url": "github.com/llava-rlhf/LLaVA-RLHF",
        "filepath": "demo/model_builder.py",
        "commit_date": "2023-10-08T09:44:58Z",
        "message": "add a minimal example for launching demo"
    },
    {
        "repo_url": "github.com/llava-rlhf/LLaVA-RLHF",
        "filepath": "Eval/model_vqa_mmhal.py",
        "commit_date": "2023-11-01T04:35:20Z",
        "message": "update MMHal evaluation code"
    },
    {
        "repo_url": "github.com/llava-rlhf/LLaVA-RLHF",
        "filepath": "Eval/model_vqa_logit.py",
        "commit_date": "2023-10-19T21:33:16Z",
        "message": "eval sft"
    },
    {
        "repo_url": "github.com/llava-rlhf/LLaVA-RLHF",
        "filepath": "RLHF/models/reward_model.py",
        "commit_date": "2023-10-14T12:25:27Z",
        "message": "add RLHF code"
    },
    {
        "repo_url": "github.com/jackaduma/Vicuna-LoRA-RLHF-PyTorch",
        "filepath": "merge_peft_adapter.py",
        "commit_date": "2023-04-24T15:15:41Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/jackaduma/Vicuna-LoRA-RLHF-PyTorch",
        "filepath": "merge_peft_adapter.py",
        "commit_date": "2023-04-23T16:21:05Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/jackaduma/Vicuna-LoRA-RLHF-PyTorch",
        "filepath": "merge_peft_adapter.py",
        "commit_date": "2023-04-22T06:06:16Z",
        "message": "add merge_peft_adapter in Vicuna"
    },
    {
        "repo_url": "github.com/zjukg/KnowPAT",
        "filepath": "inference.py",
        "commit_date": "2023-11-11T08:45:24Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/zzlgreat/smart_agent",
        "filepath": "special_mind/fllama_api.py",
        "commit_date": "2023-09-24T02:29:39Z",
        "message": "add some factors"
    },
    {
        "repo_url": "github.com/zzlgreat/smart_agent",
        "filepath": "special_mind/fllama_api.py",
        "commit_date": "2023-09-20T06:09:55Z",
        "message": "add some factors"
    },
    {
        "repo_url": "github.com/zzlgreat/smart_agent",
        "filepath": "special_mind/fllama_api.py",
        "commit_date": "2023-09-18T12:48:33Z",
        "message": "add some factors"
    },
    {
        "repo_url": "github.com/zzlgreat/smart_agent",
        "filepath": "special_mind/task_class.py",
        "commit_date": "2023-09-06T10:05:39Z",
        "message": "add some sectors"
    },
    {
        "repo_url": "github.com/zzlgreat/smart_agent",
        "filepath": "special_mind/task_class.py",
        "commit_date": "2023-09-06T09:39:24Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/zzlgreat/smart_agent",
        "filepath": "trainer/func_caller_train.py",
        "commit_date": "2023-09-19T02:41:45Z",
        "message": "add some factors"
    },
    {
        "repo_url": "github.com/mzbac/qlora-fine-tune",
        "filepath": "inference.py",
        "commit_date": "2023-06-01T16:03:03Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/mzbac/qlora-fine-tune",
        "filepath": "merge_peft_adapters.py",
        "commit_date": "2023-06-01T16:03:03Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/HC-Guo/Owl",
        "filepath": "Multiple_Choice/model/lora_llama2.py",
        "commit_date": "2023-10-08T18:56:54Z",
        "message": "Initial open-source owl"
    },
    {
        "repo_url": "github.com/ddzipp/AutoAudit",
        "filepath": "sandbox/manage.py",
        "commit_date": "2023-07-15T06:07:41Z",
        "message": "change directory names and commit django project: SandBox"
    },
    {
        "repo_url": "github.com/ddzipp/AutoAudit",
        "filepath": "sandbox/AutoAudit/apps.py",
        "commit_date": "2023-07-15T08:34:58Z",
        "message": "Update apps.py"
    },
    {
        "repo_url": "github.com/ddzipp/AutoAudit",
        "filepath": "sandbox/AutoAudit/apps.py",
        "commit_date": "2023-07-15T08:30:42Z",
        "message": "Update apps.py"
    },
    {
        "repo_url": "github.com/ddzipp/AutoAudit",
        "filepath": "sandbox/AutoAudit/apps.py",
        "commit_date": "2023-07-15T08:23:29Z",
        "message": "add integrated dataset directory"
    },
    {
        "repo_url": "github.com/ddzipp/AutoAudit",
        "filepath": "sandbox/AutoAudit/apps.py",
        "commit_date": "2023-07-15T07:55:26Z",
        "message": "V0.0.1 (#2)\n\n* change directory names and commit django project: SandBox\n\n* Update apps.py\n\n---------\n\nCo-authored-by: lilBuffaloEric <ericde1920@foxmail.com>\nCo-authored-by: lilBuffaloEric <113868733+lilBuffaloEric@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/ddzipp/AutoAudit",
        "filepath": "sandbox/AutoAudit/apps.py",
        "commit_date": "2023-07-15T06:07:41Z",
        "message": "change directory names and commit django project: SandBox"
    },
    {
        "repo_url": "github.com/sdan/selfextend",
        "filepath": "modeling_utils.py",
        "commit_date": "2024-01-06T04:41:26Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/OpenGVLab/ControlLLM",
        "filepath": "cllm/services/llama2/llama2.py",
        "commit_date": "2023-12-25T15:58:25Z",
        "message": "ControlLLM initial release"
    },
    {
        "repo_url": "github.com/rui-ye/OpenFedLLM",
        "filepath": "utils/merge_lora.py",
        "commit_date": "2024-01-29T08:55:29Z",
        "message": "[rui/eval] include open-ended eval\n\n- MT-Bench (GPT eval)\n- Vicuna Bench (GPT eval)\n- Advbench (Rule-based eval)"
    },
    {
        "repo_url": "github.com/rui-ye/OpenFedLLM",
        "filepath": "evaluation/open_ended/gen_model_answer.py",
        "commit_date": "2024-03-03T15:33:09Z",
        "message": "assign model_name to 'generator' value"
    },
    {
        "repo_url": "github.com/rui-ye/OpenFedLLM",
        "filepath": "evaluation/open_ended/gen_model_answer.py",
        "commit_date": "2024-03-03T15:13:51Z",
        "message": "[evaluation/open_ended] modify gen_model_answer.py"
    },
    {
        "repo_url": "github.com/rui-ye/OpenFedLLM",
        "filepath": "evaluation/open_ended/gen_model_answer.py",
        "commit_date": "2024-01-29T08:55:29Z",
        "message": "[rui/eval] include open-ended eval\n\n- MT-Bench (GPT eval)\n- Vicuna Bench (GPT eval)\n- Advbench (Rule-based eval)"
    },
    {
        "repo_url": "github.com/rui-ye/OpenFedLLM",
        "filepath": "evaluation/open_ended/gen_model_answer_mt.py",
        "commit_date": "2024-02-09T09:05:44Z",
        "message": "[xyd/fixup] fix logic flow and cleanup\n\n- raise error when meet unsupported arg\n- remove commented (thus unused) part\n- construct reference model in case of non-peft\n  - e.g. for DPO"
    },
    {
        "repo_url": "github.com/rui-ye/OpenFedLLM",
        "filepath": "evaluation/open_ended/gen_model_answer_mt.py",
        "commit_date": "2024-01-29T08:55:29Z",
        "message": "[rui/eval] include open-ended eval\n\n- MT-Bench (GPT eval)\n- Vicuna Bench (GPT eval)\n- Advbench (Rule-based eval)"
    },
    {
        "repo_url": "github.com/snap-stanford/MLAgentBench",
        "filepath": "MLAgentBench/benchmarks/llama-inference/env/inference.py",
        "commit_date": "2023-09-01T22:15:46Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/DaoD/INTERS",
        "filepath": "evaluation/qdu-tasks/src/modeling.py",
        "commit_date": "2024-03-01T02:27:01Z",
        "message": "Update modeling.py"
    },
    {
        "repo_url": "github.com/DaoD/INTERS",
        "filepath": "evaluation/qdu-tasks/src/modeling.py",
        "commit_date": "2024-03-01T02:26:43Z",
        "message": "Update modeling.py"
    },
    {
        "repo_url": "github.com/DaoD/INTERS",
        "filepath": "evaluation/qdu-tasks/src/modeling.py",
        "commit_date": "2024-03-01T02:24:36Z",
        "message": "Update modeling.py"
    },
    {
        "repo_url": "github.com/DaoD/INTERS",
        "filepath": "evaluation/qdu-tasks/src/modeling.py",
        "commit_date": "2024-02-18T09:52:50Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/tsb0601/MMVP",
        "filepath": "LLaVA/llava/model/builder.py",
        "commit_date": "2024-01-10T19:01:52Z",
        "message": "Initial Commit"
    },
    {
        "repo_url": "github.com/plm353557719/imClumsyPanda",
        "filepath": "models/loader/loader.py",
        "commit_date": "2023-05-18T14:54:41Z",
        "message": "llm_model_dict \u5904\u7406\u4e86loader\u7684\u4e00\u4e9b\u9884\u8bbe\u884c\u4e3a\uff0c\u5982\u52a0\u8f7d\u4f4d\u7f6e\uff0c\u6a21\u578b\u540d\u79f0\uff0c\u6a21\u578b\u5904\u7406\u5668\u5b9e\u4f8b, \u5b9a\u4e49checkpoint\u540d\u79f0\u548c\u8fdc\u7a0b\u8def\u5f84\nloader.py: \u6a21\u578b\u91cd\u8f7d\n\u5b9a\u4e49 generatorAnswer \u589e\u52a0 AnswerResultStream\n   \u5b9a\u4e49generate_with_callback\u6536\u96c6\u5668\uff0c\u5728\u6bcf\u6b21\u54cd\u5e94\u65f6\u5c06\u961f\u5217\u6570\u636e\u540c\u6b65\u5230AnswerResult\nrequirements.txt \u53d8\u66f4\u9879\u76ee\u4f9d\u8d56"
    },
    {
        "repo_url": "github.com/leapingjagg-dev/SLEB",
        "filepath": "lm-evaluation-harness/lm_eval/models/huggingface.py",
        "commit_date": "2024-02-06T09:52:26Z",
        "message": "now"
    },
    {
        "repo_url": "github.com/sotopia-lab/sotopia",
        "filepath": "lmlib/serve/lm_inference.py",
        "commit_date": "2024-02-05T19:18:34Z",
        "message": "bumped langchain and openai version (#25)\n\n* bumped langchain and openai version\n\n* langchain.openai -> langchain_openai\n\n* remove unused type ignore"
    },
    {
        "repo_url": "github.com/sotopia-lab/sotopia",
        "filepath": "lmlib/serve/lm_inference.py",
        "commit_date": "2024-01-07T21:49:32Z",
        "message": "Sync private repo with this public repo (#12)"
    },
    {
        "repo_url": "github.com/sotopia-lab/sotopia",
        "filepath": "lmlib/serve/lm_inference.py",
        "commit_date": "2023-07-23T03:08:59Z",
        "message": "Fix agentprofile indexing bug (#72)\n\n* \ud83d\udc1b fix agentprofile indexing bug\n\n* \ud83c\udff7\ufe0f removed unused ignore type\n\n* \ud83c\udff7\ufe0f trying to fix the bugs\n\n* \ud83d\udc77 update workflow and remove mypy ignore"
    },
    {
        "repo_url": "github.com/sotopia-lab/sotopia",
        "filepath": "lmlib/serve/lm_inference.py",
        "commit_date": "2023-05-08T00:25:29Z",
        "message": "lmlib api (#46)\n\n* stream and nonstream api\n\n* fix mypy\n\n* add requirements\n\n* fix untype and rm dataset in this pr"
    },
    {
        "repo_url": "github.com/sotopia-lab/sotopia",
        "filepath": "lmlib/serve/lm_inference.py",
        "commit_date": "2023-05-04T19:03:15Z",
        "message": "Load Locally hosted LLMs (#43)\n\n* fix lint\n\n* scripts\n\n* fix name to pass mypy\n\n* add init\n\n* add init\n\n* \ud83c\udff7\ufe0f added some types and stubs\n\n* \ud83c\udff7\ufe0f add  more types\n\n* \ud83c\udff7\ufe0f ignore transformer errors\n\n* \ud83d\udc1b fix mypy errors\n\n* \ud83d\udc9a disable warn unused type ignore\n\n---------\n\nCo-authored-by: Hao Zhu <prokilchu@gmail.com>"
    },
    {
        "repo_url": "github.com/austrian-code-wizard/c3po",
        "filepath": "src/eval.py",
        "commit_date": "2024-02-16T02:12:39Z",
        "message": "fixing local run args"
    },
    {
        "repo_url": "github.com/austrian-code-wizard/c3po",
        "filepath": "src/eval.py",
        "commit_date": "2024-02-13T07:58:41Z",
        "message": "post-submission commit"
    },
    {
        "repo_url": "github.com/austrian-code-wizard/c3po",
        "filepath": "src/eval.py",
        "commit_date": "2024-02-01T04:30:20Z",
        "message": "adding multi feedback and base model"
    },
    {
        "repo_url": "github.com/austrian-code-wizard/c3po",
        "filepath": "src/eval.py",
        "commit_date": "2024-01-28T09:06:57Z",
        "message": "making answer quality eval invariant to feedback"
    },
    {
        "repo_url": "github.com/austrian-code-wizard/c3po",
        "filepath": "src/eval.py",
        "commit_date": "2024-01-27T02:11:49Z",
        "message": "improve GPT response handling"
    },
    {
        "repo_url": "github.com/austrian-code-wizard/c3po",
        "filepath": "src/eval.py",
        "commit_date": "2024-01-27T01:54:23Z",
        "message": "refactored eval"
    },
    {
        "repo_url": "github.com/austrian-code-wizard/c3po",
        "filepath": "src/eval.py",
        "commit_date": "2024-01-26T05:54:36Z",
        "message": "added result quality eval"
    },
    {
        "repo_url": "github.com/austrian-code-wizard/c3po",
        "filepath": "src/eval.py",
        "commit_date": "2024-01-26T04:05:25Z",
        "message": "fixing merge"
    },
    {
        "repo_url": "github.com/austrian-code-wizard/c3po",
        "filepath": "src/eval.py",
        "commit_date": "2024-01-26T03:54:38Z",
        "message": "Merge branch 'main' into annie"
    },
    {
        "repo_url": "github.com/austrian-code-wizard/c3po",
        "filepath": "src/eval.py",
        "commit_date": "2024-01-26T03:51:43Z",
        "message": "addressed PR feedback"
    },
    {
        "repo_url": "github.com/austrian-code-wizard/c3po",
        "filepath": "src/eval.py",
        "commit_date": "2024-01-26T01:01:38Z",
        "message": "Basic cot eval"
    },
    {
        "repo_url": "github.com/austrian-code-wizard/c3po",
        "filepath": "src/eval.py",
        "commit_date": "2024-01-25T18:28:34Z",
        "message": "improved api call robustness"
    },
    {
        "repo_url": "github.com/austrian-code-wizard/c3po",
        "filepath": "src/eval.py",
        "commit_date": "2024-01-25T10:07:55Z",
        "message": "Initial CoT sampling, refactoring"
    },
    {
        "repo_url": "github.com/austrian-code-wizard/c3po",
        "filepath": "src/eval.py",
        "commit_date": "2024-01-24T18:53:21Z",
        "message": "refactored new feedback"
    },
    {
        "repo_url": "github.com/austrian-code-wizard/c3po",
        "filepath": "src/eval.py",
        "commit_date": "2024-01-24T00:03:51Z",
        "message": "fixing eval of general prompts"
    },
    {
        "repo_url": "github.com/austrian-code-wizard/c3po",
        "filepath": "src/eval.py",
        "commit_date": "2024-01-18T23:20:13Z",
        "message": "fixed eval file naming"
    },
    {
        "repo_url": "github.com/austrian-code-wizard/c3po",
        "filepath": "src/eval.py",
        "commit_date": "2024-01-18T01:44:41Z",
        "message": "added parameter sweep feature"
    },
    {
        "repo_url": "github.com/austrian-code-wizard/c3po",
        "filepath": "src/eval.py",
        "commit_date": "2024-01-18T01:21:19Z",
        "message": "fixed revision prompt and added filters"
    },
    {
        "repo_url": "github.com/austrian-code-wizard/c3po",
        "filepath": "src/eval.py",
        "commit_date": "2024-01-16T07:16:14Z",
        "message": "fixed eval bugs"
    },
    {
        "repo_url": "github.com/austrian-code-wizard/c3po",
        "filepath": "src/eval.py",
        "commit_date": "2024-01-16T02:54:03Z",
        "message": "fixed eval not using OOD data"
    },
    {
        "repo_url": "github.com/austrian-code-wizard/c3po",
        "filepath": "src/eval.py",
        "commit_date": "2024-01-16T02:36:57Z",
        "message": "fixed eval bugs"
    },
    {
        "repo_url": "github.com/austrian-code-wizard/c3po",
        "filepath": "src/eval.py",
        "commit_date": "2024-01-16T01:21:51Z",
        "message": "added eval and train"
    },
    {
        "repo_url": "github.com/austrian-code-wizard/c3po",
        "filepath": "src/eval.py",
        "commit_date": "2024-01-14T00:52:51Z",
        "message": "added models"
    },
    {
        "repo_url": "github.com/austrian-code-wizard/c3po",
        "filepath": "src/modal/serve.py",
        "commit_date": "2024-02-15T16:48:08Z",
        "message": "allowing modal serve using different methods"
    },
    {
        "repo_url": "github.com/austrian-code-wizard/c3po",
        "filepath": "src/modal/serve.py",
        "commit_date": "2024-02-14T07:04:44Z",
        "message": "added modal inference endpoint"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-09-07T11:32:29Z",
        "message": "Update collie/config.py"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-09-07T11:32:23Z",
        "message": "Update collie/config.py"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-09-07T11:32:14Z",
        "message": "Update collie/config.py"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-09-07T11:32:07Z",
        "message": "Update collie/config.py"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-09-07T07:34:52Z",
        "message": "\u4f18\u5316collie\u521d\u59cb\u5316\u65b9\u5f0f"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-09-03T12:55:34Z",
        "message": "\u4fee\u590d\u90e8\u5206\u53c2\u6570\u52a0\u8f7dBug\uff0c\u4ece\u96f6\u521d\u59cb\u5316Pipeline\u5e76\u884c\uff0cTensor\u5e76\u884c\uff0c\u975e\u5e76\u884c\u7684Bug"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-08-31T06:13:41Z",
        "message": "fix: import PeftConfig directly from peft"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-08-22T13:28:41Z",
        "message": "small"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-08-22T13:13:23Z",
        "message": "delete setup_deepspeed"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-08-22T10:01:50Z",
        "message": "set ds_config's default type to dict"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-08-22T08:40:58Z",
        "message": "fix set ds_config"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-08-02T06:38:50Z",
        "message": "align train_micro_batch and gradient_accumulate_steps in __post_init__"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-08-01T05:53:50Z",
        "message": "add: worker support for dataloader"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-07-27T12:21:08Z",
        "message": "support saving config to petrel"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-07-26T09:25:50Z",
        "message": "fix: bugs for python3.11"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-07-25T04:33:39Z",
        "message": "set checkpointing to model_config"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-07-20T03:07:31Z",
        "message": "typo of init_method"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-07-06T05:20:41Z",
        "message": "update tutorial-123"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-07-05T02:41:03Z",
        "message": "add: peft utils"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-07-01T14:59:33Z",
        "message": "typo"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-07-01T14:58:29Z",
        "message": "typo"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-06-30T14:51:08Z",
        "message": "add: int4 support"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-06-28T09:19:47Z",
        "message": "fix: pipelinegeneration"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-06-16T11:46:09Z",
        "message": "add: example for further pretraining"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-06-12T05:51:15Z",
        "message": "fix imports of collie.trainer; fix docs"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-06-07T05:08:24Z",
        "message": "conflict"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-06-06T10:24:35Z",
        "message": "add: \u6dfb\u52a0\u6ce8\u91ca"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-06-06T09:41:33Z",
        "message": "add: add peft with zero3"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-06-06T09:25:06Z",
        "message": "add: add peft config"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-06-05T03:27:46Z",
        "message": "Add callback trigger in trainer; move to cpu when gather at metrics"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-06-01T08:23:25Z",
        "message": "Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-06-01T08:22:47Z",
        "message": "fix docs; get slice_group using _pg_groups to be compatible with lower versions"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-06-01T08:14:10Z",
        "message": "fix: bugs in drivers"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-05-31T11:18:55Z",
        "message": "update docs"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-05-30T07:33:38Z",
        "message": "fix: bugs in trainer"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-05-18T11:12:54Z",
        "message": "add: zero3"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-05-18T07:03:46Z",
        "message": "support setattr to model_config"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/config.py",
        "commit_date": "2023-05-18T06:42:53Z",
        "message": "move arguments.py to collie/config.py"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-12-31T06:51:44Z",
        "message": "fix: only permute kv_cache for pp"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-12-31T05:05:50Z",
        "message": "remove useless comments"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-12-30T15:06:25Z",
        "message": "add comments"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-12-23T13:53:41Z",
        "message": "fix print"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-12-23T13:46:07Z",
        "message": "add resume train"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-10-28T13:49:42Z",
        "message": "update kv_cache and train_epoch_end"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-10-17T14:45:55Z",
        "message": "Update trainer.py"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-10-17T14:44:08Z",
        "message": "Revert \"fix: adalomo has no loss_scaler but loss_scale\"\n\nThis reverts commit 1dc252e46522b7e8070bdeda7bb2f4c00e906496."
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-10-17T14:29:31Z",
        "message": "fix: adalomo has no loss_scaler but loss_scale"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-10-17T08:08:06Z",
        "message": "rename _merge_peft  to pp_merge_peft"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-10-17T07:23:09Z",
        "message": "add: adalomo optim"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-10-13T05:40:59Z",
        "message": "refactor: move load_peft to peft_utils"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-09-04T06:06:38Z",
        "message": "misc: format trainer.py by black"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-08-25T05:41:56Z",
        "message": "peft load bug"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-08-23T10:51:57Z",
        "message": "fix config"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-08-23T08:39:15Z",
        "message": "gather on rank 0 to avoid oom"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-08-22T08:47:55Z",
        "message": "Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-08-22T08:47:09Z",
        "message": "complete tests for LoadBestCallback CheckpointCallback and save&load_checkpoint"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-08-10T07:38:05Z",
        "message": "update lomo"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-08-10T07:36:00Z",
        "message": "Merge remote-tracking branch 'upstream/dev' into dev"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-08-09T06:30:46Z",
        "message": "Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-08-09T06:28:09Z",
        "message": "prefix tuning"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-08-07T08:51:30Z",
        "message": "update requments order"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-08-03T11:13:45Z",
        "message": "update lomo"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-08-02T06:38:50Z",
        "message": "align train_micro_batch and gradient_accumulate_steps in __post_init__"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-08-02T03:34:22Z",
        "message": "Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-08-01T06:17:26Z",
        "message": "support p-tuning save&load"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-08-01T05:53:50Z",
        "message": "add: worker support for dataloader"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-08-01T02:54:08Z",
        "message": "Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-08-01T02:54:04Z",
        "message": "support p-tuning save"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-31T12:31:49Z",
        "message": "fix lomo with tp"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-31T05:00:13Z",
        "message": "fix lomo with lr_scheduler"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-30T11:05:49Z",
        "message": "Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-30T11:05:38Z",
        "message": "add: flashv2"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-28T08:28:43Z",
        "message": "Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-28T08:22:38Z",
        "message": "update save_peft and load_peft(lora)"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-28T04:49:05Z",
        "message": "fix: bugs in saveing llama"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-17T09:51:27Z",
        "message": "fix: bugs in moss and trainer"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-16T09:07:52Z",
        "message": "lomo: check dtype in backward"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-14T13:45:48Z",
        "message": "fix bf16+lomo (wip)"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-11T16:09:40Z",
        "message": "fix: bugs in collie"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-11T05:16:45Z",
        "message": "Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev\n\nConflicts:\n\tcollie/__init__.py\n\tcollie/models/__init__.py"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-11T04:45:33Z",
        "message": "Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-11T04:44:40Z",
        "message": "merge peft checkpoint in pp"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-10T13:14:12Z",
        "message": "fix: bugs in wandb"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-10T04:29:54Z",
        "message": "Merge pull request #74 from KaiLv69/dev\n\nfix bf16+zero3"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-09T14:32:51Z",
        "message": "fix bug when save zero3 checkpoint with lomo"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-09T14:28:45Z",
        "message": "empty cache after init model"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-09T07:48:58Z",
        "message": "fix: docs"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-07T09:03:46Z",
        "message": "fix merge"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-07T03:16:54Z",
        "message": "prompt tuning for llama"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-06T08:32:23Z",
        "message": "fix: bugs in trainer"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-06T08:20:01Z",
        "message": "fix: bugs in trainer"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-06T07:13:08Z",
        "message": "merge"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-06T07:07:15Z",
        "message": "fix"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-06T07:05:04Z",
        "message": "Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-06T07:04:01Z",
        "message": "update: save petf"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-06T06:59:36Z",
        "message": "fix: zero3 generation"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-06T02:21:09Z",
        "message": "Merge remote-tracking branch 'origin/dev' into dev"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-06T02:21:04Z",
        "message": "fix: bugs for prompt-tuning"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-05T18:48:56Z",
        "message": "complete save_peft and load_peft"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-05T15:26:53Z",
        "message": "merge"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-05T12:43:07Z",
        "message": "fix: bugs"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-05T12:35:36Z",
        "message": "fix: bugs in lora+pp"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-05T02:41:03Z",
        "message": "add: peft utils"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-03T11:14:20Z",
        "message": "add: callbacks for lora"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-01T12:03:42Z",
        "message": "Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-07-01T12:03:39Z",
        "message": "Add on_setup_parallel_model"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-30T14:51:08Z",
        "message": "add: int4 support"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-28T09:19:47Z",
        "message": "fix: pipelinegeneration"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-27T10:14:07Z",
        "message": "fix: bugs in lomo"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-27T06:51:22Z",
        "message": "fix: trainer"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-26T12:51:28Z",
        "message": "Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-26T12:51:25Z",
        "message": "modify: data structure"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-25T12:59:56Z",
        "message": "fit trainer to new lomo"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-23T12:34:45Z",
        "message": "fix: bugs in focs"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-19T08:04:56Z",
        "message": "rename: lomo"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-19T06:52:13Z",
        "message": "fix: finetuning llama example"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-19T04:45:10Z",
        "message": "fix: bugs in overwrite"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-16T17:01:07Z",
        "message": "add: example for expand vocab"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-15T10:40:17Z",
        "message": "Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-15T10:34:22Z",
        "message": "add: example for translation"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-15T02:59:07Z",
        "message": "Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-14T18:34:02Z",
        "message": "fix: generation"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-14T16:33:46Z",
        "message": "Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-14T16:31:20Z",
        "message": "fix: bugs in pipeline"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-14T13:37:16Z",
        "message": "Add attention_mask to moss; optimize progress bar with reset"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-12T05:51:15Z",
        "message": "fix imports of collie.trainer; fix docs"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-11T07:54:38Z",
        "message": "fix: eval_monitor"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-09T12:32:07Z",
        "message": "Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-09T12:21:06Z",
        "message": "Merge branch 'dev' of github.com:OpenLMLab/collie into dev"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-09T12:18:34Z",
        "message": "fix: lr_monitor"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-09T10:26:19Z",
        "message": "Merge branch 'dev' of github.com:OpenLMLab/collie into dev"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-09T10:25:34Z",
        "message": "fix: lr\u672a\u5b9a\u4e49"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-09T09:14:10Z",
        "message": "Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-09T08:38:02Z",
        "message": "Merge branch 'dev' of github.com:OpenLMLab/collie into dev"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-09T08:37:43Z",
        "message": "fix: data_provider"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-09T07:38:15Z",
        "message": "add: \u6dfb\u52a0\u8bb0\u5f55\u5b66\u4e60\u7387\u7684monitor"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-09T03:01:48Z",
        "message": "Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/trainer.py",
        "commit_date": "2023-06-08T12:16:26Z",
        "message": "Merge branch 'dev' of github.com:OpenLMLab/collie into dev\n\nConflicts:\n\tcollie/__init__.py\n\tcollie/controller/evaluator.py\n\tcollie/controller/trainer.py\n\ttests/trainer/_test_trainer_metric_evaluator.py"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-11-04T14:09:48Z",
        "message": "update docs in evaluator"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-11-04T14:03:22Z",
        "message": "update docs in evaluator"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-09-15T02:44:06Z",
        "message": "fix: bugs with classification in helm style"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-09-14T05:49:16Z",
        "message": "fix: bug with special token of classification in helm style"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-09-14T05:14:39Z",
        "message": "style: format evaluator.py by black"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-08-01T05:53:50Z",
        "message": "add: worker support for dataloader"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-07-30T11:05:38Z",
        "message": "add: flashv2"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-07-12T08:56:00Z",
        "message": "fix: bugs in evaluator"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-07-12T06:44:37Z",
        "message": "update: evaluator for classification"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-07-09T12:54:05Z",
        "message": "fix: bugs in classification training"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-07-09T07:48:58Z",
        "message": "fix: docs"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-07-07T03:16:54Z",
        "message": "prompt tuning for llama"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-07-07T02:12:38Z",
        "message": "add: prompt tuning"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-07-06T07:48:09Z",
        "message": "Merge remote-tracking branch 'origin/dev' into dev"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-07-06T07:48:04Z",
        "message": "fix: bugs in evaluator for prompt tuning"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-07-06T06:59:51Z",
        "message": "fix: zero3 generation"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-07-05T02:41:03Z",
        "message": "add: peft utils"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-06-28T09:19:47Z",
        "message": "fix: pipelinegeneration"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-06-26T12:51:25Z",
        "message": "modify: data structure"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-06-19T08:04:56Z",
        "message": "rename: lomo"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-06-16T17:01:07Z",
        "message": "add: example for expand vocab"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-06-15T10:34:22Z",
        "message": "add: example for translation"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-06-14T18:34:02Z",
        "message": "fix: generation"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-06-14T16:31:20Z",
        "message": "fix: bugs in pipeline"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-06-12T05:51:15Z",
        "message": "fix imports of collie.trainer; fix docs"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-06-11T07:54:38Z",
        "message": "fix: eval_monitor"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-06-09T09:14:10Z",
        "message": "Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-06-09T08:37:43Z",
        "message": "fix: data_provider"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-06-09T03:01:48Z",
        "message": "Merge branch 'dev' of https://github.com/OpenLMLab/collie into dev"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-06-08T12:16:26Z",
        "message": "Merge branch 'dev' of github.com:OpenLMLab/collie into dev\n\nConflicts:\n\tcollie/__init__.py\n\tcollie/controller/evaluator.py\n\tcollie/controller/trainer.py\n\ttests/trainer/_test_trainer_metric_evaluator.py"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "collie/controller/evaluator.py",
        "commit_date": "2023-06-08T12:07:00Z",
        "message": "add: dataset for classification"
    },
    {
        "repo_url": "github.com/OpenMOSS/collie",
        "filepath": "examples/peft/merge_weights.py",
        "commit_date": "2023-10-12T02:40:22Z",
        "message": "refactor: merge_weights.py"
    },
    {
        "repo_url": "github.com/shehuiwojiege/open-llms-next-web",
        "filepath": "utils.py",
        "commit_date": "2023-12-03T09:28:09Z",
        "message": "'\u4fee\u6539utils'"
    },
    {
        "repo_url": "github.com/shehuiwojiege/open-llms-next-web",
        "filepath": "utils.py",
        "commit_date": "2023-12-03T09:01:08Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/liangwq/Chatglm_lora_multi-gpu",
        "filepath": "webui/web_feadback.py",
        "commit_date": "2023-04-02T03:00:24Z",
        "message": "\u589e\u52a0webui\u4ea4\u4e92\u754c\u9762"
    },
    {
        "repo_url": "github.com/liangwq/Chatglm_lora_multi-gpu",
        "filepath": "postprocess/merge_lora.py",
        "commit_date": "2023-04-10T04:14:03Z",
        "message": "add merge lora"
    },
    {
        "repo_url": "github.com/liangwq/Chatglm_lora_multi-gpu",
        "filepath": "postprocess/merge_lora.py",
        "commit_date": "2023-04-06T01:47:32Z",
        "message": "merge  lora"
    },
    {
        "repo_url": "github.com/liangwq/Chatglm_lora_multi-gpu",
        "filepath": "postprocess/merge_lora_basemodel.py",
        "commit_date": "2023-04-10T04:14:03Z",
        "message": "add merge lora"
    },
    {
        "repo_url": "github.com/liangwq/Chatglm_lora_multi-gpu",
        "filepath": "postprocess/merge_lora_basemodel.py",
        "commit_date": "2023-04-04T11:22:05Z",
        "message": "lora\u548cbasemodel\u5408\u5e76\u811a\u672c"
    },
    {
        "repo_url": "github.com/liangwq/Chatglm_lora_multi-gpu",
        "filepath": "APP_example/lora_sd/inference_cript.py",
        "commit_date": "2023-04-14T13:51:31Z",
        "message": "add lora sd\n\n\u65b0\u589estable diffusion\u7684lora\u8bad\u7ec3\n\u540e\u671f\u7ed3\u5408chatglm\u9879\u76ee\u7528"
    },
    {
        "repo_url": "github.com/liangwq/Chatglm_lora_multi-gpu",
        "filepath": "APP_example/auto_prompt_image_genrator/main.py",
        "commit_date": "2023-04-05T03:11:26Z",
        "message": "app fix token bug"
    },
    {
        "repo_url": "github.com/liangwq/Chatglm_lora_multi-gpu",
        "filepath": "APP_example/auto_prompt_image_genrator/main.py",
        "commit_date": "2023-04-02T15:44:05Z",
        "message": "\u589e\u52a0\u751f\u6210\u56feprompt\uff0c\u7136\u540e\u4f5c\u56fe\u5e94\u7528\n\n1.\u65b0\u589echatglm\u751f\u56feprompt\n2.\u65b0\u589e\u751f\u6210\u56fe\u76f4\u63a5sd\u751f\u6210\u56fe\u80fd\u529b"
    },
    {
        "repo_url": "github.com/liangwq/Chatglm_lora_multi-gpu",
        "filepath": "APP_example/chatglm_agent/knowledge_extractor/knowledge_extract_tool.py",
        "commit_date": "2023-07-25T20:40:20Z",
        "message": "add knowledege eatractor tool"
    },
    {
        "repo_url": "github.com/baaivision/JudgeLM",
        "filepath": "judgelm/model/apply_lora.py",
        "commit_date": "2023-10-26T19:41:48Z",
        "message": "[add] initial commit."
    },
    {
        "repo_url": "github.com/baaivision/JudgeLM",
        "filepath": "judgelm/model/model_adapter.py",
        "commit_date": "2023-10-26T19:41:48Z",
        "message": "[add] initial commit."
    },
    {
        "repo_url": "github.com/SeanLee97/AnglE",
        "filepath": "scripts/push_llm_model.py",
        "commit_date": "2023-11-02T03:28:54Z",
        "message": "move to scripts"
    },
    {
        "repo_url": "github.com/kaistAI/CoT-Collection",
        "filepath": "CoT_Finetuning/src/T5.py",
        "commit_date": "2023-09-11T12:28:25Z",
        "message": "[PUSh] backup"
    },
    {
        "repo_url": "github.com/findalexli/SciGraphQA",
        "filepath": "llava/eval/model_vqa.py",
        "commit_date": "2023-06-11T23:01:07Z",
        "message": "Fix bugs"
    },
    {
        "repo_url": "github.com/findalexli/SciGraphQA",
        "filepath": "llava/eval/model_vqa.py",
        "commit_date": "2023-06-11T22:05:20Z",
        "message": "Update scripts and docs"
    },
    {
        "repo_url": "github.com/findalexli/SciGraphQA",
        "filepath": "llava/eval/model_vqa.py",
        "commit_date": "2023-05-01T00:54:16Z",
        "message": "Update eval"
    },
    {
        "repo_url": "github.com/findalexli/SciGraphQA",
        "filepath": "llava/eval/model_vqa.py",
        "commit_date": "2023-04-18T00:16:37Z",
        "message": "LLaVA initial release"
    },
    {
        "repo_url": "github.com/findalexli/SciGraphQA",
        "filepath": "llava/serve/model_worker.py",
        "commit_date": "2023-06-11T23:01:07Z",
        "message": "Fix bugs"
    },
    {
        "repo_url": "github.com/findalexli/SciGraphQA",
        "filepath": "llava/serve/model_worker.py",
        "commit_date": "2023-06-11T22:05:20Z",
        "message": "Update scripts and docs"
    },
    {
        "repo_url": "github.com/findalexli/SciGraphQA",
        "filepath": "llava/serve/model_worker.py",
        "commit_date": "2023-06-10T03:45:11Z",
        "message": "Update"
    },
    {
        "repo_url": "github.com/findalexli/SciGraphQA",
        "filepath": "llava/serve/model_worker.py",
        "commit_date": "2023-05-10T17:58:37Z",
        "message": "Improve worker logic"
    },
    {
        "repo_url": "github.com/findalexli/SciGraphQA",
        "filepath": "llava/serve/model_worker.py",
        "commit_date": "2023-05-07T05:46:19Z",
        "message": "Fix eval prompt"
    },
    {
        "repo_url": "github.com/findalexli/SciGraphQA",
        "filepath": "llava/serve/model_worker.py",
        "commit_date": "2023-05-06T15:58:22Z",
        "message": "Add MPT inference"
    },
    {
        "repo_url": "github.com/findalexli/SciGraphQA",
        "filepath": "llava/serve/model_worker.py",
        "commit_date": "2023-05-06T01:48:49Z",
        "message": "Update MPT serving"
    },
    {
        "repo_url": "github.com/findalexli/SciGraphQA",
        "filepath": "llava/serve/model_worker.py",
        "commit_date": "2023-05-01T01:17:12Z",
        "message": "Fix v0/v1 loading"
    },
    {
        "repo_url": "github.com/findalexli/SciGraphQA",
        "filepath": "llava/serve/model_worker.py",
        "commit_date": "2023-04-30T04:53:25Z",
        "message": "Optimize for v1"
    },
    {
        "repo_url": "github.com/findalexli/SciGraphQA",
        "filepath": "llava/serve/model_worker.py",
        "commit_date": "2023-04-19T02:55:21Z",
        "message": "Optimize worker and demo"
    },
    {
        "repo_url": "github.com/findalexli/SciGraphQA",
        "filepath": "llava/serve/model_worker.py",
        "commit_date": "2023-04-18T00:16:37Z",
        "message": "LLaVA initial release"
    },
    {
        "repo_url": "github.com/NLP-Core-Team/mmlu_ru",
        "filepath": "mmlu_ru.py",
        "commit_date": "2023-07-19T16:35:36Z",
        "message": "feat: added llama2"
    },
    {
        "repo_url": "github.com/NLP-Core-Team/mmlu_ru",
        "filepath": "mmlu_ru.py",
        "commit_date": "2023-06-28T13:56:56Z",
        "message": "feat: inital coommit"
    },
    {
        "repo_url": "github.com/geronimi73/qlora-minimal",
        "filepath": "merge_qlora.py",
        "commit_date": "2023-11-14T06:35:58Z",
        "message": "Update merge_qlora.py"
    },
    {
        "repo_url": "github.com/geronimi73/qlora-minimal",
        "filepath": "merge_qlora.py",
        "commit_date": "2023-11-14T06:32:12Z",
        "message": "Update merge_qlora.py"
    },
    {
        "repo_url": "github.com/geronimi73/qlora-minimal",
        "filepath": "merge_qlora.py",
        "commit_date": "2023-11-05T15:02:55Z",
        "message": "check-in please"
    },
    {
        "repo_url": "github.com/kaistAI/InstructIR",
        "filepath": "eval/eval_instructir_dense.py",
        "commit_date": "2024-02-22T07:05:04Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/shuxueslpi/chatGLM-6B-QLoRA",
        "filepath": "inference_qlora.py",
        "commit_date": "2023-06-04T02:51:29Z",
        "message": "QLoRA for chatglm-6b"
    },
    {
        "repo_url": "github.com/shuxueslpi/chatGLM-6B-QLoRA",
        "filepath": "merge_lora_and_quantize.py",
        "commit_date": "2023-06-28T10:22:08Z",
        "message": "\u652f\u6301chatGLM2-6B"
    },
    {
        "repo_url": "github.com/shuxueslpi/chatGLM-6B-QLoRA",
        "filepath": "merge_lora_and_quantize.py",
        "commit_date": "2023-06-10T03:18:17Z",
        "message": "\u65b0\u589e\u6a21\u578b\u63a8\u7406\u6027\u80fd\u6d4b\u8bd5"
    },
    {
        "repo_url": "github.com/shuxueslpi/chatGLM-6B-QLoRA",
        "filepath": "merge_lora_and_quantize.py",
        "commit_date": "2023-06-07T07:08:39Z",
        "message": "\u589e\u52a0\u8bad\u7ec3\u540elora\u6a21\u578b\u7684\u878d\u5408\u53ca\u91cf\u5316"
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-25T08:16:01Z",
        "message": "update webui"
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-25T07:47:01Z",
        "message": "update rereank."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-25T06:32:00Z",
        "message": "update demo."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-24T12:45:48Z",
        "message": "update rerank model predict."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-24T10:33:02Z",
        "message": "update expand context chunk logic."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-15T03:08:23Z",
        "message": "update torch type."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-11T10:17:40Z",
        "message": "update corpus emb save."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-11T10:10:41Z",
        "message": "update  sim model."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-11T10:04:56Z",
        "message": "update  sim model."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-11T08:53:02Z",
        "message": "update rag demo for bm25 sim model."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-11T08:51:30Z",
        "message": "update rag demo for bm25 sim model."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-11T06:57:47Z",
        "message": "update sim model."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-10T13:15:57Z",
        "message": "update sim model."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-09T10:34:24Z",
        "message": "update batch rag demo."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-09T10:29:25Z",
        "message": "update batch rag demo."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-09T10:13:19Z",
        "message": "update batch rag demo."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-09T09:16:17Z",
        "message": "update logger."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-09T09:00:21Z",
        "message": "update chat with pdf, no history."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-09T08:18:00Z",
        "message": "update demo."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-08T13:35:15Z",
        "message": "update logger."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-08T10:41:36Z",
        "message": "add chunk split."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-08T10:32:06Z",
        "message": "add chunk split."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-08T10:25:25Z",
        "message": "add chunk split."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-08T10:15:33Z",
        "message": "add chunk split."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-08T09:51:57Z",
        "message": "update sim model"
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-08T09:16:27Z",
        "message": "update chatpdf webui."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-08T07:22:28Z",
        "message": "update chatpdf."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-06T13:36:32Z",
        "message": "update chatpdf."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-06T12:30:45Z",
        "message": "update webui."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2024-01-06T09:54:59Z",
        "message": "update sim model."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-11-16T04:06:55Z",
        "message": "update emb save."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-11-16T03:55:53Z",
        "message": "update sim interface."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-08-06T15:43:19Z",
        "message": "update chat webui."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-08-06T15:28:16Z",
        "message": "update chat webui."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-08-06T15:19:04Z",
        "message": "update gen"
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-08-06T15:15:38Z",
        "message": "update gen config."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-08-06T14:58:18Z",
        "message": "update chat stream."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-07-25T11:29:19Z",
        "message": "update chat."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-07-25T10:05:59Z",
        "message": "update int."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-07-25T10:04:45Z",
        "message": "update demo."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-07-25T09:28:39Z",
        "message": "update demo."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-07-25T09:19:37Z",
        "message": "update chatpdf with custom gen model."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-07-18T07:32:09Z",
        "message": "update usage."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-07-18T07:29:07Z",
        "message": "update model."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-07-17T06:40:42Z",
        "message": "+  mps support for Similarity"
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-07-17T05:49:55Z",
        "message": "+ ChatGLM2-6B support  & bug fix"
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-07-10T16:27:06Z",
        "message": "update gen model."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-06-13T08:32:49Z",
        "message": "update chat model."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-04-27T07:19:09Z",
        "message": "Merge branch 'main' of github.com:zhongpei/ChatPDF"
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-04-27T06:35:43Z",
        "message": "add options for chatpdf"
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-04-27T03:19:27Z",
        "message": "update web."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-04-27T02:19:49Z",
        "message": "add topn for webui"
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-04-23T02:36:01Z",
        "message": "update answer."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-04-19T12:35:30Z",
        "message": "update web ui."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-04-17T14:01:38Z",
        "message": "update gen res."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-04-17T13:58:49Z",
        "message": "update gen res."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-04-17T13:53:31Z",
        "message": "update gen."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-04-17T13:51:30Z",
        "message": "update gen."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-04-17T13:30:34Z",
        "message": "update init."
    },
    {
        "repo_url": "github.com/shibing624/ChatPDF",
        "filepath": "chatpdf.py",
        "commit_date": "2023-04-17T13:24:12Z",
        "message": "add chatpdf function."
    },
    {
        "repo_url": "github.com/jiawei-ren/dreamgaussian4d",
        "filepath": "diffusers/src/diffusers/models/modeling_utils.py",
        "commit_date": "2023-12-28T08:22:09Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/jiawei-ren/dreamgaussian4d",
        "filepath": "diffusers/src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-12-28T08:22:09Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/SAI990323/TALLRec",
        "filepath": "evaluate.py",
        "commit_date": "2023-06-13T13:46:02Z",
        "message": "update evaluate"
    },
    {
        "repo_url": "github.com/SAI990323/TALLRec",
        "filepath": "evaluate.py",
        "commit_date": "2023-06-06T09:05:11Z",
        "message": "in batch evaluation"
    },
    {
        "repo_url": "github.com/SAI990323/TALLRec",
        "filepath": "evaluate.py",
        "commit_date": "2023-06-06T09:01:50Z",
        "message": "multi batch"
    },
    {
        "repo_url": "github.com/SAI990323/TALLRec",
        "filepath": "evaluate.py",
        "commit_date": "2023-06-04T08:00:34Z",
        "message": "update evaluate.sh"
    },
    {
        "repo_url": "github.com/SAI990323/TALLRec",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-30T09:44:42Z",
        "message": "update the shell and the evaluate code"
    },
    {
        "repo_url": "github.com/SAI990323/TALLRec",
        "filepath": "evaluate.py",
        "commit_date": "2023-04-10T06:23:11Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/SAI990323/TALLRec",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-04-10T06:23:11Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/SAI990323/TALLRec",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-04-10T06:23:11Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/Jiuzhouh/Uncertainty-Aware-Language-Agent",
        "filepath": "run_mmlu_llama2.py",
        "commit_date": "2024-01-25T05:14:49Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/Jiuzhouh/Uncertainty-Aware-Language-Agent",
        "filepath": "run_hotpotqa_llama2.py",
        "commit_date": "2024-01-25T05:14:49Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/Jiuzhouh/Uncertainty-Aware-Language-Agent",
        "filepath": "run_strategyqa_llama2.py",
        "commit_date": "2024-01-25T05:14:49Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/Synthintel0/MyGirlGPT",
        "filepath": "opendan-text-generation-webui/modules/LoRA.py",
        "commit_date": "2023-06-07T05:52:40Z",
        "message": "update LLM Server"
    },
    {
        "repo_url": "github.com/mbzuai-oryx/GeoChat",
        "filepath": "geochat/model/builder.py",
        "commit_date": "2024-02-27T20:15:10Z",
        "message": "geochat_uploaded"
    },
    {
        "repo_url": "github.com/lc222/BELLE-LORA",
        "filepath": "generate.py",
        "commit_date": "2023-03-24T12:04:44Z",
        "message": "add generate"
    },
    {
        "repo_url": "github.com/namin/llm-verified-with-monte-carlo-tree-search",
        "filepath": "huggingface_generate.py",
        "commit_date": "2023-12-26T10:56:26Z",
        "message": "configs"
    },
    {
        "repo_url": "github.com/namin/llm-verified-with-monte-carlo-tree-search",
        "filepath": "huggingface_generate.py",
        "commit_date": "2023-12-19T21:49:58Z",
        "message": "some tweaks: common can_be_solution and same_for_many_samples option"
    },
    {
        "repo_url": "github.com/namin/llm-verified-with-monte-carlo-tree-search",
        "filepath": "huggingface_generate.py",
        "commit_date": "2023-12-07T07:08:16Z",
        "message": "restore default"
    },
    {
        "repo_url": "github.com/namin/llm-verified-with-monte-carlo-tree-search",
        "filepath": "huggingface_generate.py",
        "commit_date": "2023-12-07T06:39:13Z",
        "message": "fix intent"
    },
    {
        "repo_url": "github.com/namin/llm-verified-with-monte-carlo-tree-search",
        "filepath": "huggingface_generate.py",
        "commit_date": "2023-12-07T05:35:02Z",
        "message": "tweaks"
    },
    {
        "repo_url": "github.com/namin/llm-verified-with-monte-carlo-tree-search",
        "filepath": "huggingface_generate.py",
        "commit_date": "2023-12-07T03:04:56Z",
        "message": "some stats"
    },
    {
        "repo_url": "github.com/namin/llm-verified-with-monte-carlo-tree-search",
        "filepath": "huggingface_generate.py",
        "commit_date": "2023-12-07T02:30:23Z",
        "message": "minor indent"
    },
    {
        "repo_url": "github.com/namin/llm-verified-with-monte-carlo-tree-search",
        "filepath": "huggingface_generate.py",
        "commit_date": "2023-12-07T01:45:21Z",
        "message": "optional beam search"
    },
    {
        "repo_url": "github.com/namin/llm-verified-with-monte-carlo-tree-search",
        "filepath": "huggingface_generate.py",
        "commit_date": "2023-12-03T00:13:39Z",
        "message": "use diversity as a common lib"
    },
    {
        "repo_url": "github.com/namin/llm-verified-with-monte-carlo-tree-search",
        "filepath": "huggingface_generate.py",
        "commit_date": "2023-11-21T01:08:33Z",
        "message": "fix: huggingface typo"
    },
    {
        "repo_url": "github.com/Abbey4799/CuteGPT",
        "filepath": "inference_ft.py",
        "commit_date": "2023-07-20T07:10:37Z",
        "message": "Solve the confusion caused by the default parameters of the preprocessing function; Solve the problem of template inconsistency between training and reasoning"
    },
    {
        "repo_url": "github.com/Abbey4799/CuteGPT",
        "filepath": "inference_ft.py",
        "commit_date": "2023-07-06T09:06:25Z",
        "message": "update readme and finetuning code"
    },
    {
        "repo_url": "github.com/Abbey4799/CuteGPT",
        "filepath": "inference_lora.py",
        "commit_date": "2023-07-20T07:10:37Z",
        "message": "Solve the confusion caused by the default parameters of the preprocessing function; Solve the problem of template inconsistency between training and reasoning"
    },
    {
        "repo_url": "github.com/Abbey4799/CuteGPT",
        "filepath": "inference_lora.py",
        "commit_date": "2023-07-06T09:06:25Z",
        "message": "update readme and finetuning code"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-04-12T03:26:56Z",
        "message": ":new: now rollout normally"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-04-09T21:07:59Z",
        "message": "Update export_hf_checkpoint.py (#302)\n\n* Update export_hf_checkpoint.py\n\n* Update finetune.py\n\nNew tokenizer base model for the current dev branch of transformers\n\n* Update generate.py\n\n* Update export_state_dict_checkpoint.py\n\n* Update export_hf_checkpoint.py"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-04-04T15:05:20Z",
        "message": "Support streaming output on generate (#263)"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-30T15:57:40Z",
        "message": "Fix server_name"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-30T15:57:40Z",
        "message": "Added Dockerfile and docker-compose.yml (#207)\n\n* Added Dockerfile for inference\n\n* Added instructions for Dockerfile\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Pass env through Dockerfile\n\n* Added docker compose setup and instructions\n\n* Added more environment options\n\n* Set a safer default mount point\n\n* add docker-compose changes\n\n* Added Dockerfile for inference\n\n* Added instructions for Dockerfile\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Pass env through Dockerfile\n\n* Added docker compose setup and instructions\n\n* Added more environment options\n\n* Set a safer default mount point\n\n* add to gitignore, update to new generate.py\n\n* add docker ignore, simplify docker compose file\n\n* add back missing requirements\n\n* Adjustments to compose and generate.py, added Docker to README.md\n\n* Linting adjust to Black\n\n* Adjusting import linting\n\n* Update README.md\n\n* Update README.md\n\n* Removed comment by original Dockerfile creator.\n\nComment not necessary.\n\n* cleanup README\n\nCo-authored-by: Francesco Saverio Zuppichini <zuppif@usi.ch>\n\n---------\n\nCo-authored-by: Francesco Saverio Zuppichini <zuppif@usi.ch>\nCo-authored-by: Chris Alexiuk <c.s.alexiuk@gmail.com>\nCo-authored-by: ElRoberto538 <>\nCo-authored-by: Sam Sipe <samsipe@gmail.com>\nCo-authored-by: Eric J. Wang <eric.james.wang@gmail.com>"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-29T23:36:04Z",
        "message": "Templated prompter (#184)\n\n* Templated prompter\n\n* fix dup import\n\n* Set Verbose False by default\n\nI forgot to disable after testing.\n\n* Fix imports order\n\n* Use Black Formatting\n\n* lint\n\n* Re-introduce lost line\n\n* Cleanup\n\n* template default\n\n* isort\n\n---------\n\nCo-authored-by: Eric Wang <eric.james.wang@gmail.com>"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-28T16:43:29Z",
        "message": "Add option to share Gradio demo publicly (#189)\n\n* Add option to share Gradio demo publicly\n\n* gradio_share -> share_gradio\n\n---------\n\nCo-authored-by: Eric Wang <eric.james.wang@gmail.com>"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-28T15:33:47Z",
        "message": "remove asserts"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-27T17:31:44Z",
        "message": "Add HF dataset loading, add linters, pyproject.toml (#175)\n\n* add HF dataset loading, add linters, pyproject.toml\n\n- applied markdownlint\n- add black, black[jupyter], isort\n- fix noqa codes\n- add .github workflow linting\n- update README.md\n\n* restore default settings\n\n* resume_from_checkpoint\n\nCo-authored-by: AngainorDev <54739135+AngainorDev@users.noreply.github.com>\n\n* Print warning on checkpoint not found\n\n* add HF dataset loading, add linters, pyproject.toml\n\n- applied markdownlint\n- add black, black[jupyter], isort\n- fix noqa codes\n- add .github workflow linting\n- update README.md\n\n* Default to local copy and update it\n\n* Typo\n\n* Remove duplicate code block\n\n---------\n\nCo-authored-by: Eric Wang <eric.james.wang@gmail.com>\nCo-authored-by: AngainorDev <54739135+AngainorDev@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-24T21:18:42Z",
        "message": "Use CLI arguments (#159)\n\n* CLI args for finetune\n\n* Update README\n\n* CLI args for generate.py\n\n* reqs.txt\n\n* reorder hyperparams\n\n* lora_target_modules\n\n* cleanup"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-23T20:54:39Z",
        "message": "Remove LLaMA download code, as a precaution"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-23T20:44:45Z",
        "message": "bos, eos in generate.py"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-21T21:31:30Z",
        "message": "fix fp16 inference"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-19T22:53:21Z",
        "message": "slider for tokens generated"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-19T18:22:02Z",
        "message": "Remove messy test code"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-19T06:00:18Z",
        "message": "generate.py tweaks"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-18T23:43:53Z",
        "message": "don't share publicly"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-17T22:07:08Z",
        "message": "min beams = 1"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-17T20:53:21Z",
        "message": "Enable inference on CPU and Mac GPU using pytorch support for MPS (#48)"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-17T02:30:27Z",
        "message": "Update generate.py\n\nAdapting to the input function, a text box for inputting content has been added."
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-16T23:04:06Z",
        "message": "add Gradio interface to generate.py"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-16T19:11:47Z",
        "message": "Catch outdated installs"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-16T19:11:29Z",
        "message": "Update alpaca-lora to use transformers main branch"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-16T16:59:10Z",
        "message": "Expand sampling in generate.py for new test"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-16T07:05:32Z",
        "message": "Add counting test"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-16T00:22:22Z",
        "message": "generate.py memory, perf updates"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-15T18:11:26Z",
        "message": "torch.no_grad"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-15T04:41:02Z",
        "message": "add text-davinci-003 to comparisons"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-15T04:33:12Z",
        "message": "Update README.md with new checkpoint details"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-14T22:10:33Z",
        "message": "Ready to go"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-14T00:23:29Z",
        "message": "decapoda"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "generate.py",
        "commit_date": "2023-03-13T22:00:05Z",
        "message": "Licenses and whatnot"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-04-17T09:04:14Z",
        "message": ":new: add readme"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-04-17T03:37:04Z",
        "message": ":new: add stuff"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-04-12T03:26:56Z",
        "message": ":new: now rollout normally"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-04-09T21:07:59Z",
        "message": "Update export_hf_checkpoint.py (#302)\n\n* Update export_hf_checkpoint.py\n\n* Update finetune.py\n\nNew tokenizer base model for the current dev branch of transformers\n\n* Update generate.py\n\n* Update export_state_dict_checkpoint.py\n\n* Update export_hf_checkpoint.py"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-03-28T15:33:47Z",
        "message": "remove asserts"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-03-27T17:31:44Z",
        "message": "Add HF dataset loading, add linters, pyproject.toml (#175)\n\n* add HF dataset loading, add linters, pyproject.toml\n\n- applied markdownlint\n- add black, black[jupyter], isort\n- fix noqa codes\n- add .github workflow linting\n- update README.md\n\n* restore default settings\n\n* resume_from_checkpoint\n\nCo-authored-by: AngainorDev <54739135+AngainorDev@users.noreply.github.com>\n\n* Print warning on checkpoint not found\n\n* add HF dataset loading, add linters, pyproject.toml\n\n- applied markdownlint\n- add black, black[jupyter], isort\n- fix noqa codes\n- add .github workflow linting\n- update README.md\n\n* Default to local copy and update it\n\n* Typo\n\n* Remove duplicate code block\n\n---------\n\nCo-authored-by: Eric Wang <eric.james.wang@gmail.com>\nCo-authored-by: AngainorDev <54739135+AngainorDev@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-03-23T20:54:39Z",
        "message": "Remove LLaMA download code, as a precaution"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-03-18T23:42:58Z",
        "message": "fix HF export script"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-03-18T00:56:10Z",
        "message": "HF export script"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-04-09T21:07:59Z",
        "message": "Update export_hf_checkpoint.py (#302)\n\n* Update export_hf_checkpoint.py\n\n* Update finetune.py\n\nNew tokenizer base model for the current dev branch of transformers\n\n* Update generate.py\n\n* Update export_state_dict_checkpoint.py\n\n* Update export_hf_checkpoint.py"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-03-28T15:33:47Z",
        "message": "remove asserts"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-03-27T17:31:44Z",
        "message": "Add HF dataset loading, add linters, pyproject.toml (#175)\n\n* add HF dataset loading, add linters, pyproject.toml\n\n- applied markdownlint\n- add black, black[jupyter], isort\n- fix noqa codes\n- add .github workflow linting\n- update README.md\n\n* restore default settings\n\n* resume_from_checkpoint\n\nCo-authored-by: AngainorDev <54739135+AngainorDev@users.noreply.github.com>\n\n* Print warning on checkpoint not found\n\n* add HF dataset loading, add linters, pyproject.toml\n\n- applied markdownlint\n- add black, black[jupyter], isort\n- fix noqa codes\n- add .github workflow linting\n- update README.md\n\n* Default to local copy and update it\n\n* Typo\n\n* Remove duplicate code block\n\n---------\n\nCo-authored-by: Eric Wang <eric.james.wang@gmail.com>\nCo-authored-by: AngainorDev <54739135+AngainorDev@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-03-23T20:54:39Z",
        "message": "Remove LLaMA download code, as a precaution"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-03-16T19:11:47Z",
        "message": "Catch outdated installs"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-03-16T19:11:29Z",
        "message": "Update alpaca-lora to use transformers main branch"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-03-16T07:50:24Z",
        "message": "Fix LoRa weight merging"
    },
    {
        "repo_url": "github.com/bupticybee/FastLoRAChat",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-03-16T00:17:32Z",
        "message": "Add script for converting weights from HF"
    },
    {
        "repo_url": "github.com/BraveGroup/Drive-WM",
        "filepath": "src/diffusers/models/modeling_utils.py",
        "commit_date": "2023-11-22T10:23:44Z",
        "message": "initialize from diffusers"
    },
    {
        "repo_url": "github.com/BraveGroup/Drive-WM",
        "filepath": "src/diffusers/pipelines/pipeline_utils.py",
        "commit_date": "2023-11-22T10:23:44Z",
        "message": "initialize from diffusers"
    },
    {
        "repo_url": "github.com/alipay/mobile-agent",
        "filepath": "code/infer_evaluate.py",
        "commit_date": "2024-01-17T06:16:14Z",
        "message": "new"
    },
    {
        "repo_url": "github.com/alipay/mobile-agent",
        "filepath": "code/llama-recipes-main/src/llama_recipes/inference/model_utils.py",
        "commit_date": "2024-01-17T06:16:14Z",
        "message": "new"
    },
    {
        "repo_url": "github.com/alipay/mobile-agent",
        "filepath": "code/llama-recipes-main/examples/hf_text_generation_inference/merge_lora_weights.py",
        "commit_date": "2024-01-17T06:16:14Z",
        "message": "new"
    },
    {
        "repo_url": "github.com/wxjiao/ParroT",
        "filepath": "train/inference_lora.py",
        "commit_date": "2023-08-23T12:53:08Z",
        "message": "print info for loading lora weights in inference_lora.py"
    },
    {
        "repo_url": "github.com/wxjiao/ParroT",
        "filepath": "train/inference_lora.py",
        "commit_date": "2023-04-14T05:54:56Z",
        "message": "training scripts of full model and lora"
    },
    {
        "repo_url": "github.com/dyabel/AnyTool",
        "filepath": "toolbench/model/model_adapter.py",
        "commit_date": "2024-02-23T07:13:06Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/dyabel/AnyTool",
        "filepath": "toolbench/inference/LLM/tool_llama_lora_model.py",
        "commit_date": "2024-02-23T07:13:06Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/yongzhuo/chatglm-maths",
        "filepath": "chatglm_maths/t10_lora_trl_train_ppo.py",
        "commit_date": "2023-04-09T15:31:58Z",
        "message": "fix ppo of trl(org, lora), padding of outerline,fix predict"
    },
    {
        "repo_url": "github.com/yongzhuo/chatglm-maths",
        "filepath": "chatglm_maths/p00_toy_lora_predict_6b.py",
        "commit_date": "2023-06-03T05:08:03Z",
        "message": "fix chatglm-v0"
    },
    {
        "repo_url": "github.com/yongzhuo/chatglm-maths",
        "filepath": "chatglm_maths/p00_toy_lora_predict_6b.py",
        "commit_date": "2023-04-09T15:31:58Z",
        "message": "fix ppo of trl(org, lora), padding of outerline,fix predict"
    },
    {
        "repo_url": "github.com/yongzhuo/chatglm-maths",
        "filepath": "chatglm_maths/p00_toy_lora_predict_6b.py",
        "commit_date": "2023-03-27T12:02:07Z",
        "message": "fix mask, padding, use_gMASK, func-savemodel"
    },
    {
        "repo_url": "github.com/yongzhuo/chatglm-maths",
        "filepath": "chatglm_maths/p00_toy_lora_predict_6b.py",
        "commit_date": "2023-03-25T03:42:29Z",
        "message": "add ppo-predict, fix init_weight of no pretrain, fix lora-predict"
    },
    {
        "repo_url": "github.com/yongzhuo/chatglm-maths",
        "filepath": "chatglm_maths/p00_toy_lora_predict_6b.py",
        "commit_date": "2023-03-24T16:27:34Z",
        "message": "add PPO of trl, fix README.md, fix encode(need input_dict), update model/tokenizer"
    },
    {
        "repo_url": "github.com/yongzhuo/chatglm-maths",
        "filepath": "chatglm_maths/p00_toy_lora_predict_6b.py",
        "commit_date": "2023-03-23T18:48:03Z",
        "message": "fix README.md, fix encode(need input_dict), update model/tokenizer"
    },
    {
        "repo_url": "github.com/pipilurj/G-LLaVA",
        "filepath": "gllava/model/builder.py",
        "commit_date": "2023-12-22T07:20:51Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/plncmm/guanaco-lora",
        "filepath": "guanaco-test.py",
        "commit_date": "2023-03-23T16:59:26Z",
        "message": "edit instruction"
    },
    {
        "repo_url": "github.com/plncmm/guanaco-lora",
        "filepath": "guanaco-test.py",
        "commit_date": "2023-03-23T15:25:13Z",
        "message": "remove comments"
    },
    {
        "repo_url": "github.com/plncmm/guanaco-lora",
        "filepath": "guanaco-test.py",
        "commit_date": "2023-03-23T14:48:35Z",
        "message": "fix accemts"
    },
    {
        "repo_url": "github.com/plncmm/guanaco-lora",
        "filepath": "guanaco-test.py",
        "commit_date": "2023-03-23T14:33:58Z",
        "message": "upload train and test scripts"
    },
    {
        "repo_url": "github.com/kubeflow/kfp-tekton",
        "filepath": "samples/peft-modelmesh-pipeline/peft_model_server.py",
        "commit_date": "2023-06-20T21:08:40Z",
        "message": "feat(samples): add peft sample with modelmesh (#1258)\n\n* add peft sample with kserve\n\n* lint files"
    },
    {
        "repo_url": "github.com/kubeflow/kfp-tekton",
        "filepath": "samples/huggingface-prompt-tuning/prompt-tuning-demo.py",
        "commit_date": "2023-06-16T20:20:37Z",
        "message": "Upload huggingface demo for tutorial (#1256)\n\n* Add files via upload\n\n* Rename samples/prompt-tuning-demo.py to samples/huggingface-prompt-tuning/prompt-tuning-demo.py"
    },
    {
        "repo_url": "github.com/zhangnn520/znn_chatglm",
        "filepath": "src/utils/common.py",
        "commit_date": "2023-04-28T04:49:32Z",
        "message": "reload project"
    },
    {
        "repo_url": "github.com/zhangnn520/znn_chatglm",
        "filepath": "src/utils/common.py",
        "commit_date": "2023-04-28T04:47:03Z",
        "message": "del project"
    },
    {
        "repo_url": "github.com/zhangnn520/znn_chatglm",
        "filepath": "src/utils/common.py",
        "commit_date": "2023-04-27T15:12:24Z",
        "message": "update readme and fix bug"
    },
    {
        "repo_url": "github.com/zhangnn520/znn_chatglm",
        "filepath": "src/utils/common.py",
        "commit_date": "2023-04-27T12:59:28Z",
        "message": "the first upload code"
    },
    {
        "repo_url": "github.com/jiangjiechen/auction-arena",
        "filepath": "app_modules/utils.py",
        "commit_date": "2023-10-10T01:57:31Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/jacklishufan/InstructAny2Pix",
        "filepath": "instructany2pix/llm/model/builder.py",
        "commit_date": "2023-12-02T02:31:09Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/MikeGu721/XiezhiBenchmark",
        "filepath": "Knowledge_Evaluator/xiezhi_evaluate.py",
        "commit_date": "2023-12-04T04:00:59Z",
        "message": "little fix."
    },
    {
        "repo_url": "github.com/MikeGu721/XiezhiBenchmark",
        "filepath": "Knowledge_Evaluator/xiezhi_evaluate.py",
        "commit_date": "2023-11-12T07:26:47Z",
        "message": "update some little change."
    },
    {
        "repo_url": "github.com/xhan77/web-browsing-llama",
        "filepath": "test_webllama.py",
        "commit_date": "2023-10-11T19:21:06Z",
        "message": "update default adapter"
    },
    {
        "repo_url": "github.com/xhan77/web-browsing-llama",
        "filepath": "test_webllama.py",
        "commit_date": "2023-10-11T10:54:48Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/MikeGu721/EasyLLM",
        "filepath": "deploy.py",
        "commit_date": "2023-04-08T07:03:56Z",
        "message": "First Commit"
    },
    {
        "repo_url": "github.com/MikeGu721/EasyLLM",
        "filepath": "merge_weights.py",
        "commit_date": "2023-04-08T07:03:56Z",
        "message": "First Commit"
    },
    {
        "repo_url": "github.com/MikeGu721/EasyLLM",
        "filepath": "installs/peft-install/src/peft/peft_model.py",
        "commit_date": "2023-04-08T07:03:56Z",
        "message": "First Commit"
    },
    {
        "repo_url": "github.com/MikeGu721/EasyLLM",
        "filepath": "installs/peft-install/src/peft/tuners/prompt_tuning.py",
        "commit_date": "2023-04-08T07:03:56Z",
        "message": "First Commit"
    },
    {
        "repo_url": "github.com/camel-ai/camel_chat",
        "filepath": "camel_chat/model/apply_lora.py",
        "commit_date": "2023-06-19T20:58:21Z",
        "message": "initial commit of training and serving"
    },
    {
        "repo_url": "github.com/jiangxinyang227/LLM-tuning",
        "filepath": "llama_tuning/lora_ddp/generate.py",
        "commit_date": "2023-04-24T14:53:50Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/jiangxinyang227/LLM-tuning",
        "filepath": "chatglm_tuning/lora_ddp/generate.py",
        "commit_date": "2023-04-24T14:53:50Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/jiangxinyang227/LLM-tuning",
        "filepath": "chatglm_tuning/lora_fsdp/generate.py",
        "commit_date": "2023-04-25T13:34:29Z",
        "message": "add deepspeed and fsdp\u7684\u8bad\u7ec3\u4ee3\u7801"
    },
    {
        "repo_url": "github.com/jiangxinyang227/LLM-tuning",
        "filepath": "llama_tuning/lora_deepspeed/generate.py",
        "commit_date": "2023-04-25T13:34:29Z",
        "message": "add deepspeed and fsdp\u7684\u8bad\u7ec3\u4ee3\u7801"
    },
    {
        "repo_url": "github.com/jiangxinyang227/LLM-tuning",
        "filepath": "llama_tuning/lora_single_gpu/generate.py",
        "commit_date": "2023-04-24T14:53:50Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/jiangxinyang227/LLM-tuning",
        "filepath": "chatglm_tuning/lora_deepspeed/generate.py",
        "commit_date": "2023-04-25T13:34:29Z",
        "message": "add deepspeed and fsdp\u7684\u8bad\u7ec3\u4ee3\u7801"
    },
    {
        "repo_url": "github.com/jiangxinyang227/LLM-tuning",
        "filepath": "chatglm_tuning/lora_shared_ddp/generate.py",
        "commit_date": "2023-04-24T14:53:50Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/jiangxinyang227/LLM-tuning",
        "filepath": "chatglm_tuning/lora_single_gpu/generate.py",
        "commit_date": "2023-04-24T14:53:50Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/KyujinHan/Sakura-SOLAR-DPO",
        "filepath": "merge.py",
        "commit_date": "2023-12-27T17:06:28Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/wpydcr/LLM-Kit",
        "filepath": "modules/model/llm_auto.py",
        "commit_date": "2023-10-07T07:37:50Z",
        "message": "update qwen 7B model and 14B model"
    },
    {
        "repo_url": "github.com/wpydcr/LLM-Kit",
        "filepath": "modules/model/llm_auto.py",
        "commit_date": "2023-09-14T08:42:48Z",
        "message": "add baichuan2 7B Chat and 13B Chat"
    },
    {
        "repo_url": "github.com/wpydcr/LLM-Kit",
        "filepath": "modules/model/llm_auto.py",
        "commit_date": "2023-08-31T09:49:35Z",
        "message": "copy handler file and typo fix"
    },
    {
        "repo_url": "github.com/wpydcr/LLM-Kit",
        "filepath": "modules/model/llm_auto.py",
        "commit_date": "2023-08-30T06:54:02Z",
        "message": "update llm_auto"
    },
    {
        "repo_url": "github.com/wpydcr/LLM-Kit",
        "filepath": "modules/model/llm_auto.py",
        "commit_date": "2023-08-28T10:34:11Z",
        "message": "add midjourney"
    },
    {
        "repo_url": "github.com/wpydcr/LLM-Kit",
        "filepath": "modules/model/llm_auto.py",
        "commit_date": "2023-08-28T10:31:53Z",
        "message": "update gitignore"
    },
    {
        "repo_url": "github.com/wpydcr/LLM-Kit",
        "filepath": "modules/model/llm_auto.py",
        "commit_date": "2023-08-09T10:25:19Z",
        "message": "add qwen-7b-chat"
    },
    {
        "repo_url": "github.com/wpydcr/LLM-Kit",
        "filepath": "modules/model/llm_auto.py",
        "commit_date": "2023-08-06T15:10:53Z",
        "message": "add chinses llama2 and chatglm2-6b-32k"
    },
    {
        "repo_url": "github.com/wpydcr/LLM-Kit",
        "filepath": "modules/model/llm_auto.py",
        "commit_date": "2023-07-22T10:01:05Z",
        "message": "new"
    },
    {
        "repo_url": "github.com/runwayIA/alpaca-lora",
        "filepath": "generate.py",
        "commit_date": "2023-03-17T10:54:38Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/runwayIA/alpaca-lora",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-03-17T10:54:38Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/deeppavlov/dream",
        "filepath": "services/transformers_peft_lm/server.py",
        "commit_date": "2023-06-01T06:26:49Z",
        "message": "Feat/use configs and function from common (#478)\n\n* efat: move default configs to common\n\n* fix: configs\n\n* fix: use compose_sending_variables"
    },
    {
        "repo_url": "github.com/deeppavlov/dream",
        "filepath": "services/transformers_peft_lm/server.py",
        "commit_date": "2023-05-29T10:37:42Z",
        "message": "Fix/configs and llm resp selection (#476)\n\n* fix: rename goals prompt\n\n* fix: send hyps to llm based resp selector\n\n* fix: send configs and default values\n\n* fix: configs\n\n* fix: extra file\n\n* fix: extra imports"
    },
    {
        "repo_url": "github.com/deeppavlov/dream",
        "filepath": "services/transformers_peft_lm/server.py",
        "commit_date": "2023-05-25T11:58:26Z",
        "message": "Fix/prompt formatting (#471)\n\n* fix: prompts formatting\n\n* fix: transformers get goals"
    },
    {
        "repo_url": "github.com/deeppavlov/dream",
        "filepath": "services/transformers_peft_lm/server.py",
        "commit_date": "2023-05-24T15:22:12Z",
        "message": "Feat/description based skill selector (#463)\n\n* feat: first version\n\n* feat: template prompted skill generates goals from prompt. and saves to shared memory\n\n* feat: template prompted skill saves to human attributes\n\n* fix: imports\n\n* feat; prompt selector\n\n* feat; prompt goals\n\n* feat: description based selector\n\n* feat: use skill selector\n\n* fix: remove imports\n\n* fix: test json\n\n* fix: get attributes\n\n* fix: comma\n\n* fix: logs\n\n* fix: get prompts\n\n* fix: codeswtyle\n\n* fix: prompts_goals in dff\n\n* fix: save prompts\n\n* fix: use skill selector\n\n* feat: save prompt goals\n\n* feat: update prompt goals\n\n* fix: config with prompt selector\n\n* feat: collect goals ok\n\n* feat: select prompted skill\n\n* feat: skills logs\n\n* fix: wait for prompt goals selector\n\n* fix: pipeline conf\n\n* fix: update prompt goals\n\n* fix: store prompt goals\n\n* fix: working\n\n* codestyle\n\n* fix: no extra logs and tests\n\n* fix: cards for goals collector\n\n* fix: use prompt collector\n\n* fix: universal generates goals\n\n* fix: pop keys\n\n* feat: cards for goals collectors\n\n* fix: codestyle\n\n* fix: codestyle\n\n* feat: meta prompt to file\n\n* feat: meta prompt to file\n\n* fix: exatr comma\n\n* fix: generate goals if an endpoint of llm\n\n* fix: codestyle\n\n* fix: jsonify response\n\n* fix: jsonify response\n\n* fix: transformers also can\n\n* fix: transformers also can\n\n* fix: codestyle\n\n* fix: goals for prompts\n\n* fix: goals for prompts from templated\n\n* fix:  print goals\n\n* fix: when add skills\n\n* fix: config\n\n* fix: no prompt usage\n\n* fix: meta prompt formatting\n\n* fix: generate goals\n\n* fix: input params\n\n* fix: codestyle\n\n* fix: correct requests\n\n* fix: working\n\n* feat: goals for demo prompts\n\n* fix: after review"
    },
    {
        "repo_url": "github.com/deeppavlov/dream",
        "filepath": "services/transformers_peft_lm/server.py",
        "commit_date": "2023-04-26T16:27:08Z",
        "message": "fixed cutoff for AI utterance (#426)\n\n* fixed cutoff for AI utterance\n\n* fix for cases with extra ROBOT: etc\n\n* style\n\n* fix for newline"
    },
    {
        "repo_url": "github.com/deeppavlov/dream",
        "filepath": "services/transformers_peft_lm/server.py",
        "commit_date": "2023-04-24T13:12:35Z",
        "message": "fix: no \\n removing (#402)"
    },
    {
        "repo_url": "github.com/deeppavlov/dream",
        "filepath": "services/transformers_peft_lm/server.py",
        "commit_date": "2023-04-19T14:07:15Z",
        "message": "Feat/ru llama distribution (#383)\n\n* feat: ru prompted dist based on llama\n\n* feat: ru persona\n\n* fix: versions\n\n* fix duplicate endpoints, missing keys (#371)\n\n* Make MTL refer to DeepPavlov 1.1 instead of my branch (#366)\n\n* Update Dockerfile\n\n* Update combined_classifier.json\n\n* Update Dockerfile\n\n* Update Dockerfile\n\n* fix: starting openai services without dev.yml (#373)\n\n* fix: reqs\n\n* fix: dockerfile\n\n* fix: pip update\n\n* fix: reqs\n\n* feat: using peft for gusev's model\n\n* fix: reqs\n\n* fix: working load\n\n* feat: separate folder for peft transformers\n\n* fix: revert to dev\n\n* fix: transformers generation\n\n* fix: use peft model\n\n* fix: component yml\n\n* fix: yml configs\n\n* fix: no half precision\n\n* fix: description\n\n* fix: config is optional for some models\n\n* fix: increase timeout\n\n* fix: formatter\n\n* fix: language\n\n* fixed\n\n* fix: rights\n\n* fix: info ymls\n\n* fix: 5sec timeout\n\n* fix: 10sec timeout\n\n* fix: gpu mem\n\n* feat: ru pipeline and dockers\n\n* feat: badlisted words ru\n\n* feat: use fp16 for faster inference\n\n* feat: rank sentences endpoint\n\n* fix: endpoint func\n\n* fix: ping pong\n\n* fix: rannker url\n\n* fix: prompt selector ru\n\n* fix: env ru\n\n* fix: sentence ranker\n\n* fix: no-scripts selector\n\n* fix: timeout 2.0 for ru toxic\n\n* fix: first try for toxic model before ready\n\n* fix: params for language\n\n* fix: language\n\n* fix: timoeout for dialogrpt\n\n* fix: no use of toxic cls\n\n* fix: revert timeout to 1sec\n\n* feat: ru lang\n\n* fix: ru persona\n\n* feat: new prompt\n\n* fix: prompt\n\n* fix: prompt\n\n* fix: prompt\n\n* Update dream_persona.json\n\n* Update dream_persona.json\n\n* Update dream_persona_ru.json\n\n* fix: rename distr\n\n* fix: formatter\n\n---------\n\nCo-authored-by: Maxim Talimanchuk <mtalimanchuk@gmail.com>\nCo-authored-by: dimakarp1996 <dimakarp1996@yandex.ru>\nCo-authored-by: Fedor Ignatov <ignatov.fedor@gmail.com>\nCo-authored-by: Lidia Ostyakova <55363402+lostyakova@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/BeyonderXX/TRACE",
        "filepath": "inference/infer_single.py",
        "commit_date": "2023-09-21T03:44:59Z",
        "message": "Update infer_single.py"
    },
    {
        "repo_url": "github.com/BeyonderXX/TRACE",
        "filepath": "inference/infer_single.py",
        "commit_date": "2023-09-19T23:10:59Z",
        "message": "Update infer_single.py"
    },
    {
        "repo_url": "github.com/BeyonderXX/TRACE",
        "filepath": "inference/infer_single.py",
        "commit_date": "2023-09-19T23:00:29Z",
        "message": "Update infer_single.py"
    },
    {
        "repo_url": "github.com/BeyonderXX/TRACE",
        "filepath": "inference/infer_single.py",
        "commit_date": "2023-09-19T10:50:28Z",
        "message": "Update infer_single.py"
    },
    {
        "repo_url": "github.com/BeyonderXX/TRACE",
        "filepath": "inference/infer_single.py",
        "commit_date": "2023-09-19T03:53:50Z",
        "message": "fix bugs"
    },
    {
        "repo_url": "github.com/BeyonderXX/TRACE",
        "filepath": "inference/infer_single.py",
        "commit_date": "2023-09-19T03:15:32Z",
        "message": "Update infer_single.py"
    },
    {
        "repo_url": "github.com/BeyonderXX/TRACE",
        "filepath": "inference/infer_single.py",
        "commit_date": "2023-09-13T12:48:55Z",
        "message": "fix bugs"
    },
    {
        "repo_url": "github.com/BeyonderXX/TRACE",
        "filepath": "inference/infer_single.py",
        "commit_date": "2023-09-13T04:22:03Z",
        "message": "Update infer_single.py"
    },
    {
        "repo_url": "github.com/BeyonderXX/TRACE",
        "filepath": "inference/infer_single.py",
        "commit_date": "2023-09-13T04:18:21Z",
        "message": "fix bugs"
    },
    {
        "repo_url": "github.com/BeyonderXX/TRACE",
        "filepath": "inference/infer_single.py",
        "commit_date": "2023-09-12T11:58:57Z",
        "message": "Fix train continual bug"
    },
    {
        "repo_url": "github.com/BeyonderXX/TRACE",
        "filepath": "inference/infer_single.py",
        "commit_date": "2023-09-08T03:16:50Z",
        "message": "fix bugs"
    },
    {
        "repo_url": "github.com/BeyonderXX/TRACE",
        "filepath": "inference/infer_single.py",
        "commit_date": "2023-09-07T07:19:14Z",
        "message": "Fix inference bug and add sigle gpu inference"
    },
    {
        "repo_url": "github.com/BeyonderXX/TRACE",
        "filepath": "utils/my_peft/peft_model.py",
        "commit_date": "2023-09-07T14:12:26Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/BeyonderXX/TRACE",
        "filepath": "utils/my_peft/tuners/prompt_tuning.py",
        "commit_date": "2023-09-07T14:12:26Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/git-disl/PokeLLMon",
        "filepath": "poke_env/player/llama_player.py",
        "commit_date": "2024-02-05T20:57:33Z",
        "message": "Update"
    },
    {
        "repo_url": "github.com/git-disl/PokeLLMon",
        "filepath": "poke_env/player/llama_player.py",
        "commit_date": "2024-01-30T16:40:29Z",
        "message": "Initialization"
    },
    {
        "repo_url": "github.com/git-disl/PokeLLMon",
        "filepath": "poke_env/player/llama_player.py",
        "commit_date": "2024-01-30T16:39:43Z",
        "message": "Initialization"
    },
    {
        "repo_url": "github.com/git-disl/PokeLLMon",
        "filepath": "poke_env/player/llama_player.py",
        "commit_date": "2024-01-30T16:39:38Z",
        "message": "Initialization"
    },
    {
        "repo_url": "github.com/NetEase-FuXi/EETQ",
        "filepath": "examples/models/llama_transformers_example.py",
        "commit_date": "2023-09-12T07:02:38Z",
        "message": "[feat] add cublas gemm kernels"
    },
    {
        "repo_url": "github.com/NetEase-FuXi/EETQ",
        "filepath": "examples/models/llama_transformers_example.py",
        "commit_date": "2023-09-05T08:41:33Z",
        "message": "[feat] add fused attention"
    },
    {
        "repo_url": "github.com/NetEase-FuXi/EETQ",
        "filepath": "examples/models/llama_transformers_example.py",
        "commit_date": "2023-08-31T12:36:01Z",
        "message": "[feat] \u652f\u6301lora\u6a21\u578b\u91cf\u5316"
    },
    {
        "repo_url": "github.com/NetEase-FuXi/EETQ",
        "filepath": "examples/models/llama_transformers_example.py",
        "commit_date": "2023-08-30T07:10:57Z",
        "message": "[feat] \u4ece\u5df2\u6709\u7684huggingface\u6a21\u578b\u76f4\u63a5\u8f6c\u6362 & \u652f\u6301\u4ecept\u6587\u4ef6\u52a0\u8f7d"
    },
    {
        "repo_url": "github.com/NetEase-FuXi/EETQ",
        "filepath": "examples/models/llama_transformers_example.py",
        "commit_date": "2023-08-29T08:03:02Z",
        "message": "[feat] add llama13B int8 example"
    },
    {
        "repo_url": "github.com/jamqd/Group-Preference-Optimization",
        "filepath": "baselines/get_emb/get_embeds.py",
        "commit_date": "2023-10-23T06:47:05Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/whyNLP/Conic10K",
        "filepath": "src/generate.py",
        "commit_date": "2023-10-24T14:40:01Z",
        "message": "fix argparse bugs"
    },
    {
        "repo_url": "github.com/whyNLP/Conic10K",
        "filepath": "src/generate.py",
        "commit_date": "2023-10-23T08:39:27Z",
        "message": "reorganize + semantic parsing evaluation"
    },
    {
        "repo_url": "github.com/whyNLP/Conic10K",
        "filepath": "src/train_clm.py",
        "commit_date": "2023-10-24T14:40:01Z",
        "message": "fix argparse bugs"
    },
    {
        "repo_url": "github.com/whyNLP/Conic10K",
        "filepath": "src/train_clm.py",
        "commit_date": "2023-10-23T08:39:27Z",
        "message": "reorganize + semantic parsing evaluation"
    },
    {
        "repo_url": "github.com/HosnLS/Hierarchical-Language-Agent",
        "filepath": "llm-api/modules/LoRA.py",
        "commit_date": "2023-12-04T07:06:28Z",
        "message": "Init"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/pretrain/run_clm_pt_with_peft.py",
        "commit_date": "2023-09-08T03:36:00Z",
        "message": "Jialin (#36)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* Jialin (#26) (#27)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* \u589e\u52a0\u6307\u4ee4\u5fae\u8c03\u4ee3\u7801\n\n* \u65b0\u589e\u6570\u636e\u96c6\u5927\u5c0f\u65e5\u5fd7\u8bb0\u5f55\uff0c\u589e\u52a0eval\u9636\u6bb5\u6307\u6807\u8bb0\u5f55\n\n* \u4fee\u6539README.md\n\n* \u6307\u4ee4\u5fae\u8c03\u811a\u6b65\u589e\u52a0eval\u9636\u6bb5\u6307\u6807\u8bb0\u5f55\n\n* \u589e\u52a0pretrain/merge_tokenizer README.md\n\n* \u6dfb\u52a0lora_target_modules\n\n* fine tuning \u6dfb\u52a0 lora_target_modules\n\n* \u6dfb\u52a0\u9884\u8bad\u7ec3\u7ed3\u679c\uff0c\u53c2\u4e0e\u8bad\u7ec3\u7f51\u7edc\u5c42:q_proj\u3001v_proj\u3001embed_tokens\u3001lm_head\n\n* \u8bbe\u7f6e\u521d\u59cb\u5316\u968f\u673a\u79cd\u5b50\n\n* \u5220\u9664wandb log"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/pretrain/run_clm_pt_with_peft.py",
        "commit_date": "2023-09-07T13:55:21Z",
        "message": "Jialin (#35)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* Jialin (#26) (#27)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* \u589e\u52a0\u6307\u4ee4\u5fae\u8c03\u4ee3\u7801\n\n* \u65b0\u589e\u6570\u636e\u96c6\u5927\u5c0f\u65e5\u5fd7\u8bb0\u5f55\uff0c\u589e\u52a0eval\u9636\u6bb5\u6307\u6807\u8bb0\u5f55\n\n* \u4fee\u6539README.md\n\n* \u6307\u4ee4\u5fae\u8c03\u811a\u6b65\u589e\u52a0eval\u9636\u6bb5\u6307\u6807\u8bb0\u5f55\n\n* \u589e\u52a0pretrain/merge_tokenizer README.md\n\n* \u6dfb\u52a0lora_target_modules\n\n* fine tuning \u6dfb\u52a0 lora_target_modules\n\n* \u6dfb\u52a0\u9884\u8bad\u7ec3\u7ed3\u679c\uff0c\u53c2\u4e0e\u8bad\u7ec3\u7f51\u7edc\u5c42:q_proj\u3001v_proj\u3001embed_tokens\u3001lm_head"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/pretrain/run_clm_pt_with_peft.py",
        "commit_date": "2023-09-07T12:41:31Z",
        "message": "Jialin (#33)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* Jialin (#26) (#27)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* \u589e\u52a0\u6307\u4ee4\u5fae\u8c03\u4ee3\u7801\n\n* \u65b0\u589e\u6570\u636e\u96c6\u5927\u5c0f\u65e5\u5fd7\u8bb0\u5f55\uff0c\u589e\u52a0eval\u9636\u6bb5\u6307\u6807\u8bb0\u5f55\n\n* \u4fee\u6539README.md\n\n* \u6307\u4ee4\u5fae\u8c03\u811a\u6b65\u589e\u52a0eval\u9636\u6bb5\u6307\u6807\u8bb0\u5f55\n\n* \u589e\u52a0pretrain/merge_tokenizer README.md\n\n* \u6dfb\u52a0lora_target_modules"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/pretrain/run_clm_pt_with_peft.py",
        "commit_date": "2023-09-05T12:33:53Z",
        "message": "Jialin (#29)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* Jialin (#26) (#27)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* \u589e\u52a0\u6307\u4ee4\u5fae\u8c03\u4ee3\u7801\n\n* \u65b0\u589e\u6570\u636e\u96c6\u5927\u5c0f\u65e5\u5fd7\u8bb0\u5f55\uff0c\u589e\u52a0eval\u9636\u6bb5\u6307\u6807\u8bb0\u5f55"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/pretrain/run_clm_pt_with_peft.py",
        "commit_date": "2023-09-04T13:23:48Z",
        "message": "Jialin (#28)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* Jialin (#26) (#27)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* \u589e\u52a0\u6307\u4ee4\u5fae\u8c03\u4ee3\u7801"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/pretrain/run_clm_pt_with_peft.py",
        "commit_date": "2023-09-04T00:30:41Z",
        "message": "Jialin (#26)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/pretrain/run_clm_pt_with_peft.py",
        "commit_date": "2023-09-03T12:58:00Z",
        "message": "Jialin (#24)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/pretrain/run_clm_pt_with_peft.py",
        "commit_date": "2023-09-03T07:45:01Z",
        "message": "Jialin (#18)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/pretrain/run_clm_pt_with_peft.py",
        "commit_date": "2023-09-02T03:58:40Z",
        "message": "Jialin (#13)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/pretrain/run_clm_pt_with_peft.py",
        "commit_date": "2023-09-01T12:21:08Z",
        "message": "Jialin (#12)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/pretrain/run_clm_pt_with_peft.py",
        "commit_date": "2023-08-31T12:18:43Z",
        "message": "Jialin (#11)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/pretrain/run_clm_pt_with_peft.py",
        "commit_date": "2023-08-31T02:10:04Z",
        "message": "Jialin (#10)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/pretrain/run_clm_pt_with_peft.py",
        "commit_date": "2023-08-30T11:41:28Z",
        "message": "Jialin (#9)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/pretrain/run_clm_pt_with_peft.py",
        "commit_date": "2023-08-30T03:31:13Z",
        "message": "Jialin (#8)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/pretrain/run_clm_pt_with_peft_ds.py",
        "commit_date": "2023-09-08T03:36:00Z",
        "message": "Jialin (#36)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* Jialin (#26) (#27)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* \u589e\u52a0\u6307\u4ee4\u5fae\u8c03\u4ee3\u7801\n\n* \u65b0\u589e\u6570\u636e\u96c6\u5927\u5c0f\u65e5\u5fd7\u8bb0\u5f55\uff0c\u589e\u52a0eval\u9636\u6bb5\u6307\u6807\u8bb0\u5f55\n\n* \u4fee\u6539README.md\n\n* \u6307\u4ee4\u5fae\u8c03\u811a\u6b65\u589e\u52a0eval\u9636\u6bb5\u6307\u6807\u8bb0\u5f55\n\n* \u589e\u52a0pretrain/merge_tokenizer README.md\n\n* \u6dfb\u52a0lora_target_modules\n\n* fine tuning \u6dfb\u52a0 lora_target_modules\n\n* \u6dfb\u52a0\u9884\u8bad\u7ec3\u7ed3\u679c\uff0c\u53c2\u4e0e\u8bad\u7ec3\u7f51\u7edc\u5c42:q_proj\u3001v_proj\u3001embed_tokens\u3001lm_head\n\n* \u8bbe\u7f6e\u521d\u59cb\u5316\u968f\u673a\u79cd\u5b50\n\n* \u5220\u9664wandb log"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/pretrain/run_clm_pt_with_peft_ds.py",
        "commit_date": "2023-09-07T13:55:21Z",
        "message": "Jialin (#35)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* Jialin (#26) (#27)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* \u589e\u52a0\u6307\u4ee4\u5fae\u8c03\u4ee3\u7801\n\n* \u65b0\u589e\u6570\u636e\u96c6\u5927\u5c0f\u65e5\u5fd7\u8bb0\u5f55\uff0c\u589e\u52a0eval\u9636\u6bb5\u6307\u6807\u8bb0\u5f55\n\n* \u4fee\u6539README.md\n\n* \u6307\u4ee4\u5fae\u8c03\u811a\u6b65\u589e\u52a0eval\u9636\u6bb5\u6307\u6807\u8bb0\u5f55\n\n* \u589e\u52a0pretrain/merge_tokenizer README.md\n\n* \u6dfb\u52a0lora_target_modules\n\n* fine tuning \u6dfb\u52a0 lora_target_modules\n\n* \u6dfb\u52a0\u9884\u8bad\u7ec3\u7ed3\u679c\uff0c\u53c2\u4e0e\u8bad\u7ec3\u7f51\u7edc\u5c42:q_proj\u3001v_proj\u3001embed_tokens\u3001lm_head"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/pretrain/run_clm_pt_with_peft_ds.py",
        "commit_date": "2023-09-07T12:41:31Z",
        "message": "Jialin (#33)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* Jialin (#26) (#27)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* \u589e\u52a0\u6307\u4ee4\u5fae\u8c03\u4ee3\u7801\n\n* \u65b0\u589e\u6570\u636e\u96c6\u5927\u5c0f\u65e5\u5fd7\u8bb0\u5f55\uff0c\u589e\u52a0eval\u9636\u6bb5\u6307\u6807\u8bb0\u5f55\n\n* \u4fee\u6539README.md\n\n* \u6307\u4ee4\u5fae\u8c03\u811a\u6b65\u589e\u52a0eval\u9636\u6bb5\u6307\u6807\u8bb0\u5f55\n\n* \u589e\u52a0pretrain/merge_tokenizer README.md\n\n* \u6dfb\u52a0lora_target_modules"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/pretrain/run_clm_pt_with_peft_ds.py",
        "commit_date": "2023-09-05T12:33:53Z",
        "message": "Jialin (#29)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* Jialin (#26) (#27)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* \u589e\u52a0\u6307\u4ee4\u5fae\u8c03\u4ee3\u7801\n\n* \u65b0\u589e\u6570\u636e\u96c6\u5927\u5c0f\u65e5\u5fd7\u8bb0\u5f55\uff0c\u589e\u52a0eval\u9636\u6bb5\u6307\u6807\u8bb0\u5f55"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/pretrain/run_clm_pt_with_peft_ds.py",
        "commit_date": "2023-09-04T13:23:48Z",
        "message": "Jialin (#28)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* Jialin (#26) (#27)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* \u589e\u52a0\u6307\u4ee4\u5fae\u8c03\u4ee3\u7801"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/pretrain/run_clm_pt_with_peft_ds.py",
        "commit_date": "2023-09-04T00:30:41Z",
        "message": "Jialin (#26)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/pretrain/run_clm_pt_with_peft_ds.py",
        "commit_date": "2023-09-03T12:58:00Z",
        "message": "Jialin (#24)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/pretrain/run_clm_pt_with_peft_ds.py",
        "commit_date": "2023-09-03T07:45:01Z",
        "message": "Jialin (#18)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/pretrain/run_clm_pt_with_peft_ds.py",
        "commit_date": "2023-09-03T01:34:51Z",
        "message": "Jialin (#17)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/pretrain/run_clm_pt_with_peft_ds.py",
        "commit_date": "2023-09-03T01:10:44Z",
        "message": "Jialin (#14)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/pretrain/run_clm_pt_with_peft_ds.py",
        "commit_date": "2023-09-02T03:58:40Z",
        "message": "Jialin (#13)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/pretrain/run_clm_pt_with_peft_ds.py",
        "commit_date": "2023-08-31T12:18:43Z",
        "message": "Jialin (#11)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/fine_tuning/run_clm_sft_with_peft_ds.py",
        "commit_date": "2023-09-08T03:36:00Z",
        "message": "Jialin (#36)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* Jialin (#26) (#27)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* \u589e\u52a0\u6307\u4ee4\u5fae\u8c03\u4ee3\u7801\n\n* \u65b0\u589e\u6570\u636e\u96c6\u5927\u5c0f\u65e5\u5fd7\u8bb0\u5f55\uff0c\u589e\u52a0eval\u9636\u6bb5\u6307\u6807\u8bb0\u5f55\n\n* \u4fee\u6539README.md\n\n* \u6307\u4ee4\u5fae\u8c03\u811a\u6b65\u589e\u52a0eval\u9636\u6bb5\u6307\u6807\u8bb0\u5f55\n\n* \u589e\u52a0pretrain/merge_tokenizer README.md\n\n* \u6dfb\u52a0lora_target_modules\n\n* fine tuning \u6dfb\u52a0 lora_target_modules\n\n* \u6dfb\u52a0\u9884\u8bad\u7ec3\u7ed3\u679c\uff0c\u53c2\u4e0e\u8bad\u7ec3\u7f51\u7edc\u5c42:q_proj\u3001v_proj\u3001embed_tokens\u3001lm_head\n\n* \u8bbe\u7f6e\u521d\u59cb\u5316\u968f\u673a\u79cd\u5b50\n\n* \u5220\u9664wandb log"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/fine_tuning/run_clm_sft_with_peft_ds.py",
        "commit_date": "2023-09-07T13:55:21Z",
        "message": "Jialin (#35)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* Jialin (#26) (#27)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* \u589e\u52a0\u6307\u4ee4\u5fae\u8c03\u4ee3\u7801\n\n* \u65b0\u589e\u6570\u636e\u96c6\u5927\u5c0f\u65e5\u5fd7\u8bb0\u5f55\uff0c\u589e\u52a0eval\u9636\u6bb5\u6307\u6807\u8bb0\u5f55\n\n* \u4fee\u6539README.md\n\n* \u6307\u4ee4\u5fae\u8c03\u811a\u6b65\u589e\u52a0eval\u9636\u6bb5\u6307\u6807\u8bb0\u5f55\n\n* \u589e\u52a0pretrain/merge_tokenizer README.md\n\n* \u6dfb\u52a0lora_target_modules\n\n* fine tuning \u6dfb\u52a0 lora_target_modules\n\n* \u6dfb\u52a0\u9884\u8bad\u7ec3\u7ed3\u679c\uff0c\u53c2\u4e0e\u8bad\u7ec3\u7f51\u7edc\u5c42:q_proj\u3001v_proj\u3001embed_tokens\u3001lm_head"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/fine_tuning/run_clm_sft_with_peft_ds.py",
        "commit_date": "2023-09-07T12:45:15Z",
        "message": "Jialin (#34)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* Jialin (#26) (#27)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* \u589e\u52a0\u6307\u4ee4\u5fae\u8c03\u4ee3\u7801\n\n* \u65b0\u589e\u6570\u636e\u96c6\u5927\u5c0f\u65e5\u5fd7\u8bb0\u5f55\uff0c\u589e\u52a0eval\u9636\u6bb5\u6307\u6807\u8bb0\u5f55\n\n* \u4fee\u6539README.md\n\n* \u6307\u4ee4\u5fae\u8c03\u811a\u6b65\u589e\u52a0eval\u9636\u6bb5\u6307\u6807\u8bb0\u5f55\n\n* \u589e\u52a0pretrain/merge_tokenizer README.md\n\n* \u6dfb\u52a0lora_target_modules\n\n* fine tuning \u6dfb\u52a0 lora_target_modules"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/fine_tuning/run_clm_sft_with_peft_ds.py",
        "commit_date": "2023-09-05T14:16:08Z",
        "message": "Jialin (#31)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* Jialin (#26) (#27)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* \u589e\u52a0\u6307\u4ee4\u5fae\u8c03\u4ee3\u7801\n\n* \u65b0\u589e\u6570\u636e\u96c6\u5927\u5c0f\u65e5\u5fd7\u8bb0\u5f55\uff0c\u589e\u52a0eval\u9636\u6bb5\u6307\u6807\u8bb0\u5f55\n\n* \u4fee\u6539README.md\n\n* \u6307\u4ee4\u5fae\u8c03\u811a\u6b65\u589e\u52a0eval\u9636\u6bb5\u6307\u6807\u8bb0\u5f55"
    },
    {
        "repo_url": "github.com/jialinzhang/chinese-medical-llama2",
        "filepath": "src/fine_tuning/run_clm_sft_with_peft_ds.py",
        "commit_date": "2023-09-04T13:23:48Z",
        "message": "Jialin (#28)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* Jialin (#26) (#27)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* Jialin (#24) (#25)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#19)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#20)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* Jialin (#18) (#21)\n\n* \u8c03\u6574.gitignorement\u5220\u9664.vscode\n\n* update .gitignore\n\n* torch.distributed.launch -> torchrun\n\n* log\u589e\u52a0rank\u4fe1\u606f\n\n* \u8c03\u6574ddp\u4f4d\u7f6e\n\n* collate_fn\n\n* loss require_grad\n\n* loss = loss / accumulation_steps\n\n* rewrite run_clm_pt.py\n\n* \u4fee\u590dddp_setup\u4f4d\u7f6e\n\n* \u4fee\u590dload checkpoint\u5185\u5b58\u4e0d\u8db3\n\n* add run_clm_pt_with_peft.py\n\n* \u4fee\u590damp\u548cfloat16\u51b2\u7a81\n\n* \u4ee3\u7801\u683c\u5f0f\u5316\n\n* add run_clm_pt_with_peft_ds.py\n\n* \u4fee\u590dmini batch\n\n* \u4fee\u590dDeepSpeedEngine.backward\u635f\u5931\u8ba1\u7b97\n\n* \u89e3\u51b3deepspeed model_engine.eval\u5361\u4f4f\n\n* \u5206\u5e03\u5f0f\u8bc4\u4f30\n\n* \u4fee\u590d\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u786e\u4fdd\u53ea\u6709\u4e3b\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\n\n* \u6dfb\u52a0torch.distributed.barrier,\u8c03\u6574train\u8fc7\u7a0b,\u8c03\u6574evaluate\u8fc7\u7a0b\n\n* \u4fee\u6b63wandb\u65e5\u5fd7\u8bb0\u5f55\n\n* \u5220\u9664\u6309epoch\u4fdd\u5b58\u6a21\u578b\u65b9\u5f0f\uff0c\u4fee\u6b63\u8bad\u7ec3\u53c2\u6570\n\n* \u589e\u52a0\u6307\u4ee4\u5fae\u8c03\u4ee3\u7801"
    },
    {
        "repo_url": "github.com/pandinghao/TouYi-LLM",
        "filepath": "utils.py",
        "commit_date": "2023-11-01T05:31:19Z",
        "message": "final_test"
    },
    {
        "repo_url": "github.com/pandinghao/TouYi-LLM",
        "filepath": "utils.py",
        "commit_date": "2023-10-31T13:41:59Z",
        "message": "final_test"
    },
    {
        "repo_url": "github.com/pandinghao/TouYi-LLM",
        "filepath": "utils.py",
        "commit_date": "2023-10-27T17:39:07Z",
        "message": "revise dataset"
    },
    {
        "repo_url": "github.com/pandinghao/TouYi-LLM",
        "filepath": "utils.py",
        "commit_date": "2023-10-27T14:55:30Z",
        "message": "go"
    },
    {
        "repo_url": "github.com/pandinghao/TouYi-LLM",
        "filepath": "utils.py",
        "commit_date": "2023-10-24T14:03:31Z",
        "message": "finetune linear and attn"
    },
    {
        "repo_url": "github.com/pandinghao/TouYi-LLM",
        "filepath": "utils.py",
        "commit_date": "2023-10-22T05:38:12Z",
        "message": "chat_ex"
    },
    {
        "repo_url": "github.com/wuyike2000/Retrieve-Rewrite-Answer",
        "filepath": "KGQA-MetaQA/rewrite/infer_t5-xl.py",
        "commit_date": "2023-09-17T14:58:30Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/eth-sri/language-model-arithmetic",
        "filepath": "src/model_arithmetic/basic_model_loader.py",
        "commit_date": "2023-11-22T14:59:27Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/vsmolyakov/deep",
        "filepath": "pytorch/peft_finetuning_llm.py",
        "commit_date": "2023-11-24T01:40:38Z",
        "message": "peft finetuning\n\npeft finetuning"
    },
    {
        "repo_url": "github.com/StarRing2022/ChatGPTX-Uni",
        "filepath": "generate.py",
        "commit_date": "2023-07-17T08:37:28Z",
        "message": "Update generate.py"
    },
    {
        "repo_url": "github.com/StarRing2022/ChatGPTX-Uni",
        "filepath": "generate.py",
        "commit_date": "2023-04-09T02:39:03Z",
        "message": "Update generate.py"
    },
    {
        "repo_url": "github.com/StarRing2022/ChatGPTX-Uni",
        "filepath": "generate.py",
        "commit_date": "2023-04-08T08:46:09Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/StarRing2022/ChatGPTX-Uni",
        "filepath": "RingPeft/peft_model.py",
        "commit_date": "2023-07-17T08:22:30Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/StarRing2022/ChatGPTX-Uni",
        "filepath": "RingPeft/tuners/lora.py",
        "commit_date": "2023-07-17T08:23:31Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/StarRing2022/ChatGPTX-Uni",
        "filepath": "RingPeft/tuners/prompt_tuning.py",
        "commit_date": "2023-07-17T08:23:31Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/anchen1011/FireAct",
        "filepath": "models/llama.py",
        "commit_date": "2023-10-08T10:18:49Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/ranchlai/lectures",
        "filepath": "code/lora/peft-turorial/eval.py",
        "commit_date": "2023-06-12T08:05:27Z",
        "message": "add peft-lora tutorial"
    },
    {
        "repo_url": "github.com/zjunlp/Mol-Instructions",
        "filepath": "demo/generate.py",
        "commit_date": "2023-06-13T06:12:59Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/zjunlp/Mol-Instructions",
        "filepath": "demo/generate.py",
        "commit_date": "2023-06-12T13:51:54Z",
        "message": "Delete demo directory"
    },
    {
        "repo_url": "github.com/zjunlp/Mol-Instructions",
        "filepath": "demo/generate.py",
        "commit_date": "2023-06-12T13:51:19Z",
        "message": "Rename generate.py to generate.py"
    },
    {
        "repo_url": "github.com/zjunlp/Mol-Instructions",
        "filepath": "evaluation/biotext/generate_example.py",
        "commit_date": "2023-10-18T10:27:00Z",
        "message": "Rename generate_many.py to generate_example.py"
    },
    {
        "repo_url": "github.com/zjunlp/Mol-Instructions",
        "filepath": "evaluation/molecule/generate_example.py",
        "commit_date": "2023-10-17T09:13:17Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/Unispac/Visual-Adversarial-Examples-Jailbreak-Large-Language-Models",
        "filepath": "llava_llama_2/model/builder.py",
        "commit_date": "2023-08-16T07:57:56Z",
        "message": "add support for other models"
    },
    {
        "repo_url": "github.com/ModelTC/EasyLLM",
        "filepath": "tools/open_hf_infer.py",
        "commit_date": "2024-01-28T13:00:59Z",
        "message": "Add pyproject.toml and make it a python package\n\nSigned-off-by: lizz <lizz@sensetime.com>"
    },
    {
        "repo_url": "github.com/ModelTC/EasyLLM",
        "filepath": "tools/open_hf_infer.py",
        "commit_date": "2023-11-26T10:08:31Z",
        "message": "release: v1.0.0"
    },
    {
        "repo_url": "github.com/ModelTC/EasyLLM",
        "filepath": "tools/merge_base_model_with_lora.py",
        "commit_date": "2023-11-26T10:08:31Z",
        "message": "release: v1.0.0"
    },
    {
        "repo_url": "github.com/OpenThaiGPT/openthaigpt-finetune",
        "filepath": "generate.py",
        "commit_date": "2023-11-11T08:33:55Z",
        "message": "Add support for 1.0.0 beta"
    },
    {
        "repo_url": "github.com/OpenThaiGPT/openthaigpt-finetune",
        "filepath": "generate.py",
        "commit_date": "2023-05-18T12:41:17Z",
        "message": "Init"
    },
    {
        "repo_url": "github.com/OpenThaiGPT/openthaigpt-finetune",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-05-18T12:41:17Z",
        "message": "Init"
    },
    {
        "repo_url": "github.com/OpenThaiGPT/openthaigpt-finetune",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-05-18T12:41:17Z",
        "message": "Init"
    },
    {
        "repo_url": "github.com/heng840/alpaca-lora-chinese",
        "filepath": "generate.py",
        "commit_date": "2023-03-28T03:23:31Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/heng840/alpaca-lora-chinese",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-03-28T03:23:31Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/heng840/alpaca-lora-chinese",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-03-28T03:23:31Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/alexrs/herd",
        "filepath": "herd/run_model.py",
        "commit_date": "2023-09-20T10:00:47Z",
        "message": "Restructure to bring scripts out of package"
    },
    {
        "repo_url": "github.com/alexrs/herd",
        "filepath": "herd/run_model.py",
        "commit_date": "2023-09-20T08:41:18Z",
        "message": "Apply Black formatting and Ruff linting"
    },
    {
        "repo_url": "github.com/alexrs/herd",
        "filepath": "herd/run_model.py",
        "commit_date": "2023-09-11T16:24:42Z",
        "message": "API working"
    },
    {
        "repo_url": "github.com/alexrs/herd",
        "filepath": "herd/run_model.py",
        "commit_date": "2023-09-08T14:05:48Z",
        "message": "Fix issue when adapters were not loaded"
    },
    {
        "repo_url": "github.com/alexrs/herd",
        "filepath": "herd/run_model.py",
        "commit_date": "2023-09-08T10:19:42Z",
        "message": "Experts created and segmented"
    },
    {
        "repo_url": "github.com/alexrs/herd",
        "filepath": "herd/run_model.py",
        "commit_date": "2023-09-06T10:13:01Z",
        "message": "Expert fine-tuning running"
    },
    {
        "repo_url": "github.com/dmis-lab/self-biorag",
        "filepath": "retrieval_lm/inference.py",
        "commit_date": "2024-01-25T09:14:05Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/dmis-lab/self-biorag",
        "filepath": "retrieval_lm/inference.py",
        "commit_date": "2024-01-22T07:40:27Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/git-cloner/llama2-lora-fine-tuning",
        "filepath": "generate.py",
        "commit_date": "2023-07-28T15:23:24Z",
        "message": "fixed generate.py"
    },
    {
        "repo_url": "github.com/git-cloner/llama2-lora-fine-tuning",
        "filepath": "generate.py",
        "commit_date": "2023-07-23T11:47:52Z",
        "message": "\u589e\u52a0\u6d4b\u8bd5\u4ee3\u7801"
    },
    {
        "repo_url": "github.com/yxli2123/LoftQ",
        "filepath": "train_clm.py",
        "commit_date": "2023-12-27T14:58:09Z",
        "message": "update train clm"
    },
    {
        "repo_url": "github.com/yxli2123/LoftQ",
        "filepath": "train_clm.py",
        "commit_date": "2023-12-19T16:30:02Z",
        "message": "upload files"
    },
    {
        "repo_url": "github.com/yxli2123/LoftQ",
        "filepath": "train_clm.py",
        "commit_date": "2023-12-19T16:07:42Z",
        "message": "rewrite gsm8k code"
    },
    {
        "repo_url": "github.com/yxli2123/LoftQ",
        "filepath": "train_clm.py",
        "commit_date": "2023-10-30T03:05:24Z",
        "message": "upload summarization code"
    },
    {
        "repo_url": "github.com/yxli2123/LoftQ",
        "filepath": "train_clm.py",
        "commit_date": "2023-10-23T15:56:02Z",
        "message": "quantize more true quant"
    },
    {
        "repo_url": "github.com/yxli2123/LoftQ",
        "filepath": "train_clm.py",
        "commit_date": "2023-10-23T15:16:37Z",
        "message": "try new profiling"
    },
    {
        "repo_url": "github.com/yxli2123/LoftQ",
        "filepath": "train_clm.py",
        "commit_date": "2023-10-16T02:48:33Z",
        "message": "validate gsm8k"
    },
    {
        "repo_url": "github.com/yxli2123/LoftQ",
        "filepath": "train_clm.py",
        "commit_date": "2023-10-15T23:31:24Z",
        "message": "remove device map"
    },
    {
        "repo_url": "github.com/yxli2123/LoftQ",
        "filepath": "train_clm.py",
        "commit_date": "2023-10-15T22:38:08Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/yxli2123/LoftQ",
        "filepath": "train_clm.py",
        "commit_date": "2023-10-15T22:06:49Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/yxli2123/LoftQ",
        "filepath": "train_clm.py",
        "commit_date": "2023-10-15T21:11:40Z",
        "message": "debugh"
    },
    {
        "repo_url": "github.com/yxli2123/LoftQ",
        "filepath": "train_clm.py",
        "commit_date": "2023-10-15T19:18:59Z",
        "message": "debug"
    },
    {
        "repo_url": "github.com/yxli2123/LoftQ",
        "filepath": "train_clm.py",
        "commit_date": "2023-10-15T16:03:01Z",
        "message": "push repo"
    },
    {
        "repo_url": "github.com/yxli2123/LoftQ",
        "filepath": "train_gsm8k.py",
        "commit_date": "2024-02-05T01:49:32Z",
        "message": "Update train_gsm8k.py\n\nchange the explanation about the full_precision"
    },
    {
        "repo_url": "github.com/yxli2123/LoftQ",
        "filepath": "train_gsm8k.py",
        "commit_date": "2023-12-19T16:30:02Z",
        "message": "upload files"
    },
    {
        "repo_url": "github.com/yxli2123/LoftQ",
        "filepath": "train_gsm8k.py",
        "commit_date": "2023-12-19T16:07:42Z",
        "message": "rewrite gsm8k code"
    },
    {
        "repo_url": "github.com/yxli2123/LoftQ",
        "filepath": "train_gsm8k.py",
        "commit_date": "2023-10-16T02:48:33Z",
        "message": "validate gsm8k"
    },
    {
        "repo_url": "github.com/yxli2123/LoftQ",
        "filepath": "train_summarization.py",
        "commit_date": "2023-12-19T16:30:02Z",
        "message": "upload files"
    },
    {
        "repo_url": "github.com/yxli2123/LoftQ",
        "filepath": "train_summarization.py",
        "commit_date": "2023-12-19T16:07:42Z",
        "message": "rewrite gsm8k code"
    },
    {
        "repo_url": "github.com/yxli2123/LoftQ",
        "filepath": "train_summarization.py",
        "commit_date": "2023-10-30T03:05:24Z",
        "message": "upload summarization code"
    },
    {
        "repo_url": "github.com/yxli2123/LoftQ",
        "filepath": "test_gsm8k.py",
        "commit_date": "2023-12-19T16:30:02Z",
        "message": "upload files"
    },
    {
        "repo_url": "github.com/noir55/japanese_llm_simple_webui",
        "filepath": "llm-webui.py",
        "commit_date": "2024-02-22T13:28:26Z",
        "message": "\u30d7\u30ed\u30f3\u30d7\u30c8\u30bf\u30a4\u30d7gemma\u3092\u8ffd\u52a0"
    },
    {
        "repo_url": "github.com/noir55/japanese_llm_simple_webui",
        "filepath": "llm-webui.py",
        "commit_date": "2023-12-27T14:00:29Z",
        "message": "\u30d7\u30ed\u30f3\u30d7\u30c8\u30bf\u30a4\u30d7elyzallama2\u3092\u8ffd\u52a0"
    },
    {
        "repo_url": "github.com/noir55/japanese_llm_simple_webui",
        "filepath": "llm-webui.py",
        "commit_date": "2023-12-23T05:39:30Z",
        "message": "\u30e2\u30c7\u30eb\u30bf\u30a4\u30d7\u3001\u30d7\u30ed\u30f3\u30d7\u30c8\u30bf\u30a4\u30d7nekomata\u3092\u8ffd\u52a0"
    },
    {
        "repo_url": "github.com/noir55/japanese_llm_simple_webui",
        "filepath": "llm-webui.py",
        "commit_date": "2023-12-20T12:50:35Z",
        "message": "\u30d7\u30ed\u30f3\u30d7\u30c8\u30bf\u30a4\u30d7swallow\u3092\u8ffd\u52a0"
    },
    {
        "repo_url": "github.com/noir55/japanese_llm_simple_webui",
        "filepath": "llm-webui.py",
        "commit_date": "2023-12-20T12:48:43Z",
        "message": "\u30d7\u30ed\u30f3\u30d7\u30c8\u30bf\u30a4\u30d7swallow\u3092\u8ffd\u52a0"
    },
    {
        "repo_url": "github.com/noir55/japanese_llm_simple_webui",
        "filepath": "llm-webui.py",
        "commit_date": "2023-12-17T02:12:06Z",
        "message": "\u30d7\u30ed\u30f3\u30d7\u30c8\u30bf\u30a4\u30d7mixtral\u3092\u8ffd\u52a0"
    },
    {
        "repo_url": "github.com/noir55/japanese_llm_simple_webui",
        "filepath": "llm-webui.py",
        "commit_date": "2023-11-23T03:05:12Z",
        "message": "\u30d7\u30ed\u30f3\u30d7\u30c8\u30bf\u30a4\u30d7chatml\u8ffd\u52a0"
    },
    {
        "repo_url": "github.com/noir55/japanese_llm_simple_webui",
        "filepath": "llm-webui.py",
        "commit_date": "2023-11-19T14:10:08Z",
        "message": "\u30e2\u30c7\u30eb\u30bf\u30a4\u30d7general\u306e\u8ffd\u52a0"
    },
    {
        "repo_url": "github.com/noir55/japanese_llm_simple_webui",
        "filepath": "llm-webui.py",
        "commit_date": "2023-08-20T14:08:00Z",
        "message": "\u30e2\u30c7\u30eb\u5b9a\u7fa9\u306e\u8ffd\u52a0"
    },
    {
        "repo_url": "github.com/noir55/japanese_llm_simple_webui",
        "filepath": "llm-webui.py",
        "commit_date": "2023-08-14T14:07:29Z",
        "message": "Merge pull request #1 from hangingman/main\n\n\u30b3\u30fc\u30c9\u306e\u5fae\u4fee\u6b63"
    },
    {
        "repo_url": "github.com/noir55/japanese_llm_simple_webui",
        "filepath": "llm-webui.py",
        "commit_date": "2023-08-10T13:31:41Z",
        "message": "Japanese StableLM\u30e2\u30c7\u30eb\u5bfe\u5fdc"
    },
    {
        "repo_url": "github.com/noir55/japanese_llm_simple_webui",
        "filepath": "llm-webui.py",
        "commit_date": "2023-08-07T13:14:26Z",
        "message": "if\u3092elif\u306b\u4fee\u6b63\u3057Invalid MODEL_TYPE \"rinna\"\u306e\u30a8\u30e9\u30fc\u3092\u89e3\u6d88"
    },
    {
        "repo_url": "github.com/noir55/japanese_llm_simple_webui",
        "filepath": "llm-webui.py",
        "commit_date": "2023-08-02T15:22:03Z",
        "message": "Rinna 4B\u30e2\u30c7\u30eb\u7b49\u306b\u5bfe\u5fdc"
    },
    {
        "repo_url": "github.com/noir55/japanese_llm_simple_webui",
        "filepath": "llm-webui.py",
        "commit_date": "2023-07-19T16:53:24Z",
        "message": "Llama2\u5bfe\u5fdc"
    },
    {
        "repo_url": "github.com/noir55/japanese_llm_simple_webui",
        "filepath": "llm-webui.py",
        "commit_date": "2023-06-30T15:48:47Z",
        "message": "\u30d7\u30ed\u30f3\u30d7\u30c8\u4fee\u6b63"
    },
    {
        "repo_url": "github.com/noir55/japanese_llm_simple_webui",
        "filepath": "llm-webui.py",
        "commit_date": "2023-06-30T15:26:15Z",
        "message": "4bit\u91cf\u5b50\u5316\u5bfe\u5fdc\u3001\u5bfe\u5fdc\u30e2\u30c7\u30eb\u8ffd\u52a0"
    },
    {
        "repo_url": "github.com/noir55/japanese_llm_simple_webui",
        "filepath": "llm-webui.py",
        "commit_date": "2023-06-20T15:14:59Z",
        "message": "\u30d7\u30ed\u30f3\u30d7\u30c8\u30bf\u30a4\u30d7qa\u3092\u8ffd\u52a0"
    },
    {
        "repo_url": "github.com/noir55/japanese_llm_simple_webui",
        "filepath": "llm-webui.py",
        "commit_date": "2023-06-14T14:02:22Z",
        "message": "\u751f\u6210\u30d1\u30e9\u30e1\u30fc\u30bf\u3092WebUI\u4e0a\u3067\u8a2d\u5b9a\u3067\u304d\u308b\u6a5f\u80fd\u3092\u8ffd\u52a0"
    },
    {
        "repo_url": "github.com/noir55/japanese_llm_simple_webui",
        "filepath": "llm-webui.py",
        "commit_date": "2023-06-10T06:59:05Z",
        "message": "\u5bfe\u5fdc\u30e2\u30c7\u30eb\u3001\u30d7\u30ed\u30f3\u30d7\u30c8\u3092\u8ffd\u52a0"
    },
    {
        "repo_url": "github.com/noir55/japanese_llm_simple_webui",
        "filepath": "llm-webui.py",
        "commit_date": "2023-06-08T11:39:59Z",
        "message": "Rinna\u30e2\u30c7\u30eb\u306e\u6539\u884c\u306e\u6271\u3044\u3092\u4fee\u6b63"
    },
    {
        "repo_url": "github.com/noir55/japanese_llm_simple_webui",
        "filepath": "llm-webui.py",
        "commit_date": "2023-06-06T14:34:58Z",
        "message": "history\u3092\u6700\u521d\u306b\u521d\u671f\u5316\u3059\u308b\u3088\u3046\u4fee\u6b63"
    },
    {
        "repo_url": "github.com/noir55/japanese_llm_simple_webui",
        "filepath": "llm-webui.py",
        "commit_date": "2023-06-05T13:56:05Z",
        "message": "\u30e2\u30c7\u30eb\u3054\u3068\u306e\u63a8\u8ad6\u65b9\u6cd5\u3092\u7d71\u4e00"
    },
    {
        "repo_url": "github.com/noir55/japanese_llm_simple_webui",
        "filepath": "llm-webui.py",
        "commit_date": "2023-06-04T16:50:34Z",
        "message": "gradio\u306elaunch\u6642\u306b\"share=False\"\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u8ffd\u52a0"
    },
    {
        "repo_url": "github.com/noir55/japanese_llm_simple_webui",
        "filepath": "llm-webui.py",
        "commit_date": "2023-06-04T15:00:42Z",
        "message": "\u6700\u521d\u306e\u30d5\u30a1\u30a4\u30eb\u767b\u9332"
    },
    {
        "repo_url": "github.com/gersteinlab/Struc-Bench",
        "filepath": "generate.py",
        "commit_date": "2023-07-08T12:16:17Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/MindSetLib/MS-Education",
        "filepath": "DeepLearning/NLP/Tasks/LLMfinetuning/scripts/ft_rouge.py",
        "commit_date": "2023-09-19T16:53:49Z",
        "message": "Llmft (#1)\n\n* First commit\n\n* Fix readme"
    },
    {
        "repo_url": "github.com/MindSetLib/MS-Education",
        "filepath": "DeepLearning/NLP/Tasks/LLMfinetuning/scripts/rlhf_detox.py",
        "commit_date": "2023-12-15T12:02:24Z",
        "message": "Add speech notebooks (#2)\n\n* First commit\n\n* Fix readme\n\n* Add DLA experiments\n\n* Edit README\n\n* Edit README 2"
    },
    {
        "repo_url": "github.com/MindSetLib/MS-Education",
        "filepath": "DeepLearning/NLP/Tasks/LLMfinetuning/scripts/rlhf_detox.py",
        "commit_date": "2023-09-19T16:53:49Z",
        "message": "Llmft (#1)\n\n* First commit\n\n* Fix readme"
    },
    {
        "repo_url": "github.com/ziweiji/Self_Reflection_Medical",
        "filepath": "alpaca-lora/loop.py",
        "commit_date": "2023-10-09T08:20:03Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/ziweiji/Self_Reflection_Medical",
        "filepath": "alpaca-lora/generate.py",
        "commit_date": "2023-10-09T08:20:03Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/HongzheBi/DocQA",
        "filepath": "model.py",
        "commit_date": "2023-05-23T16:28:32Z",
        "message": "webui"
    },
    {
        "repo_url": "github.com/HongzheBi/DocQA",
        "filepath": "model.py",
        "commit_date": "2023-05-23T09:19:36Z",
        "message": "webui"
    },
    {
        "repo_url": "github.com/HongzheBi/DocQA",
        "filepath": "model.py",
        "commit_date": "2023-05-21T17:06:52Z",
        "message": "doc-qa"
    },
    {
        "repo_url": "github.com/poisonwine/Tianchi-LLM-retrieval",
        "filepath": "LLM.py",
        "commit_date": "2023-11-13T03:12:07Z",
        "message": "Update LLM.py"
    },
    {
        "repo_url": "github.com/poisonwine/Tianchi-LLM-retrieval",
        "filepath": "LLM.py",
        "commit_date": "2023-11-13T02:26:34Z",
        "message": "Update LLM.py"
    },
    {
        "repo_url": "github.com/poisonwine/Tianchi-LLM-retrieval",
        "filepath": "LLM.py",
        "commit_date": "2023-11-13T02:21:07Z",
        "message": "Create LLM.py\n\nllm"
    },
    {
        "repo_url": "github.com/poisonwine/Tianchi-LLM-retrieval",
        "filepath": "embeddings.py",
        "commit_date": "2023-11-13T02:28:28Z",
        "message": "Create embeddings.py\n\nembedding model"
    },
    {
        "repo_url": "github.com/Akirato/LLM-KG-Reasoning",
        "filepath": "src/llm_engine.py",
        "commit_date": "2023-04-25T15:25:02Z",
        "message": "New scripts"
    },
    {
        "repo_url": "github.com/Akirato/LLM-KG-Reasoning",
        "filepath": "src/llm_engine.py",
        "commit_date": "2023-04-08T15:14:04Z",
        "message": "Added scripts and scores"
    },
    {
        "repo_url": "github.com/Akirato/LLM-KG-Reasoning",
        "filepath": "src/llm_engine.py",
        "commit_date": "2023-04-02T16:43:23Z",
        "message": "Corrected the Alpaca class and call from generate answers"
    },
    {
        "repo_url": "github.com/Akirato/LLM-KG-Reasoning",
        "filepath": "src/llm_engine.py",
        "commit_date": "2023-04-02T10:58:43Z",
        "message": "Added Alpaca"
    },
    {
        "repo_url": "github.com/Akirato/LLM-KG-Reasoning",
        "filepath": "src/llm_engine.py",
        "commit_date": "2023-03-31T00:19:37Z",
        "message": "Added LLM classes"
    },
    {
        "repo_url": "github.com/EmoCareAI/ChatPsychiatrist",
        "filepath": "fastchat/model/apply_lora.py",
        "commit_date": "2023-09-27T08:30:00Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/EmoCareAI/ChatPsychiatrist",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-09-27T08:30:00Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "chatglm3.py",
        "commit_date": "2023-12-31T09:38:27Z",
        "message": "\u76f8\u5173\u7a0b\u5e8f\u8bf4\u660e\u5907\u6ce8"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "chatglm3.py",
        "commit_date": "2023-12-20T14:14:22Z",
        "message": "\u521d\u59cb\u5316ai"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "lora-chatglm2.py",
        "commit_date": "2023-12-31T09:38:27Z",
        "message": "\u76f8\u5173\u7a0b\u5e8f\u8bf4\u660e\u5907\u6ce8"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "lora-chatglm2.py",
        "commit_date": "2023-12-20T14:14:22Z",
        "message": "\u521d\u59cb\u5316ai"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "main-local-api.py",
        "commit_date": "2024-01-15T14:12:37Z",
        "message": "\u5bf9\u63a5\u5531\u6b4c"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "main-local-api.py",
        "commit_date": "2023-12-31T09:38:27Z",
        "message": "\u76f8\u5173\u7a0b\u5e8f\u8bf4\u660e\u5907\u6ce8"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "main-local-api.py",
        "commit_date": "2023-12-20T14:14:22Z",
        "message": "\u521d\u59cb\u5316ai"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "main-local-input.py",
        "commit_date": "2023-12-31T09:38:27Z",
        "message": "\u76f8\u5173\u7a0b\u5e8f\u8bf4\u660e\u5907\u6ce8"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "main-local-input.py",
        "commit_date": "2023-12-25T14:19:46Z",
        "message": "ai\u8054\u7f51\u641c\u7d22\u5f15\u64ce\u5bf9\u63a5"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "main-local-input.py",
        "commit_date": "2023-12-23T05:17:09Z",
        "message": "\u63d0\u4ea4\u8054\u7f51\u529f\u80fd"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2024-02-05T08:47:14Z",
        "message": "\u4fee\u590d\u6b63\u5728\u64ad\u653e\u6b4c\u5355\u4e0d\u4f1a\u6e05\u7a7a\u95ee\u9898"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2024-02-04T16:38:15Z",
        "message": "\u4fee\u6539\u6b4c\u5355bug\uff0c\u4e0d\u65ad\u5faa\u73af\u64ad\u653e\u6b4c\u66f2"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2024-02-04T14:12:39Z",
        "message": "\u66f4\u6539\u53c2\u6570"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2024-02-04T13:04:01Z",
        "message": "\u63d0\u4ea4\u6b4c\u66f2\u6e05\u5355\u5217\u8868\u5c55\u793a"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2024-02-04T12:16:02Z",
        "message": "\u6b4c\u66f2\u70b9\u64ad\u5217\u8868\u4e0a\u4f20"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2024-02-04T09:02:18Z",
        "message": "\u4fee\u6539\u5f39\u5e55\u9519\u4e71\u95ee\u9898"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2024-02-01T13:31:20Z",
        "message": "\u8bf4\u660e\u6587\u6863\u66f4\u65b0"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2024-02-01T13:05:43Z",
        "message": "\u9519\u8bef\u65e5\u5fd7\u8f93\u51fa"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2024-02-01T11:25:08Z",
        "message": "\u968f\u673a\u6570\u4fee\u8ba2"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2024-01-29T11:30:35Z",
        "message": "\u8c03\u6574\u6587\u4ef6\u5939"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2024-01-28T16:36:40Z",
        "message": "\u63a7\u5236\u53f0\u65e5\u5fd7\u6253\u5370\u5230\u6587\u4ef6"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2024-01-28T16:33:48Z",
        "message": "\u63a7\u5236\u53f0\u65e5\u5fd7\u6253\u5370\u5230\u6587\u4ef6"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2024-01-28T14:39:06Z",
        "message": "\u5347\u7ea7\u5f39\u5e55\u6837\u5f0f"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2024-01-27T14:29:14Z",
        "message": "\u8c03\u6574\u641c\u56fe\u80fd\u529b\u4e3a\u767e\u5ea6"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2024-01-27T09:12:38Z",
        "message": "ai\u56de\u590d\u5f39\u5e55\u663e\u793a"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2024-01-24T13:30:01Z",
        "message": "\u52a0\u5165\u56de\u590d\u5f39\u5e55"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2024-01-24T12:10:43Z",
        "message": "\u52a0\u5165\u56de\u590d\u5f39\u5e55"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2024-01-22T15:49:11Z",
        "message": "\u52a0\u5165bert-vits2\u8bed\u97f3\u5408\u6210"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2024-01-22T03:49:15Z",
        "message": "\u670d\u52a1\u8bf4\u660e\u6587\u6863\u4fee\u6539"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2024-01-22T03:09:53Z",
        "message": "\u7981\u8bcd\u8c03\u6574"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2024-01-21T11:06:07Z",
        "message": "\u7528\u6237\u63d0\u793a\u8bcd\u52a0\u5165\u52a0\u5f3a\u6743\u91cd"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2024-01-21T08:33:24Z",
        "message": "\u7ed8\u753b\u63d0\u793a\u8bcd\u548c\u5531\u6b4c\u529f\u80fd\u4e0a\u4f20"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2024-01-16T02:33:26Z",
        "message": "\u5531\u6b4c\u8c03\u6574"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2024-01-16T02:32:35Z",
        "message": "\u5531\u6b4c\u8c03\u6574"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2024-01-15T14:12:37Z",
        "message": "\u5bf9\u63a5\u5531\u6b4c"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2024-01-03T12:53:45Z",
        "message": "\u641c\u56fe\u529f\u80fd\u66f4\u65b0"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2023-12-31T11:16:46Z",
        "message": "\u7ed8\u753b\u7ebf\u7a0b\u52a0\u5165"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2023-12-31T09:38:27Z",
        "message": "\u76f8\u5173\u7a0b\u5e8f\u8bf4\u660e\u5907\u6ce8"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2023-12-30T10:41:12Z",
        "message": "\u52a0\u5165\u5728\u7ebf\u7ed8\u753b\u80fd\u529b"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2023-12-25T14:19:46Z",
        "message": "ai\u8054\u7f51\u641c\u7d22\u5f15\u64ce\u5bf9\u63a5"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2023-12-23T05:19:13Z",
        "message": "\u63d0\u4ea4\u8054\u7f51\u529f\u80fd"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2023-12-23T05:17:09Z",
        "message": "\u63d0\u4ea4\u8054\u7f51\u529f\u80fd"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2023-12-23T04:59:36Z",
        "message": "\u63d0\u4ea4ai\u8054\u7f51\u641c\u7d22\u80fd\u529b"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2023-12-22T11:43:32Z",
        "message": "\u589e\u52a0\u8054\u7f51\u67e5\u8be2"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-api.py",
        "commit_date": "2023-12-20T14:14:22Z",
        "message": "\u521d\u59cb\u5316ai"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-web.py",
        "commit_date": "2024-01-21T08:37:30Z",
        "message": "\u9274\u9ec4\u7a0b\u5e8f\u4e0a\u4f20"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-local.py",
        "commit_date": "2023-12-31T11:16:46Z",
        "message": "\u7ed8\u753b\u7ebf\u7a0b\u52a0\u5165"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-local.py",
        "commit_date": "2023-12-30T10:41:12Z",
        "message": "\u52a0\u5165\u5728\u7ebf\u7ed8\u753b\u80fd\u529b"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-local.py",
        "commit_date": "2023-12-25T14:19:46Z",
        "message": "ai\u8054\u7f51\u641c\u7d22\u5f15\u64ce\u5bf9\u63a5"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-local.py",
        "commit_date": "2023-12-23T05:19:13Z",
        "message": "\u63d0\u4ea4\u8054\u7f51\u529f\u80fd"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-local.py",
        "commit_date": "2023-12-23T05:17:09Z",
        "message": "\u63d0\u4ea4\u8054\u7f51\u529f\u80fd"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-local.py",
        "commit_date": "2023-12-23T04:59:36Z",
        "message": "\u63d0\u4ea4ai\u8054\u7f51\u641c\u7d22\u80fd\u529b"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-local.py",
        "commit_date": "2023-12-22T11:43:32Z",
        "message": "\u589e\u52a0\u8054\u7f51\u67e5\u8be2"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "bilibili-live-local.py",
        "commit_date": "2023-12-20T14:14:22Z",
        "message": "\u521d\u59cb\u5316ai"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "text-generation-webui/modules/LoRA.py",
        "commit_date": "2023-12-20T14:14:22Z",
        "message": "\u521d\u59cb\u5316ai"
    },
    {
        "repo_url": "github.com/worm128/AI-YinMei-backup",
        "filepath": "LLaMA-Factory/src/llmtuner/model/adapter.py",
        "commit_date": "2023-12-20T14:14:22Z",
        "message": "\u521d\u59cb\u5316ai"
    },
    {
        "repo_url": "github.com/chaoyi-wu/Finetune_LLAMA",
        "filepath": "Python_Package/peft-llama/src/peft/peft_model.py",
        "commit_date": "2023-03-20T08:45:49Z",
        "message": "changed"
    },
    {
        "repo_url": "github.com/chaoyi-wu/Finetune_LLAMA",
        "filepath": "Python_Package/peft-llama/src/peft/tuners/lora.py",
        "commit_date": "2023-03-20T08:45:49Z",
        "message": "changed"
    },
    {
        "repo_url": "github.com/chaoyi-wu/Finetune_LLAMA",
        "filepath": "Python_Package/peft-llama/src/peft/tuners/prompt_tuning.py",
        "commit_date": "2023-03-20T08:45:49Z",
        "message": "changed"
    },
    {
        "repo_url": "github.com/jina-ai/jerboa",
        "filepath": "jerboa/utils/load_model.py",
        "commit_date": "2023-07-13T11:59:31Z",
        "message": "fix: allow for hf lora weights (#113)\n\nCo-authored-by: Sebastian Weisshaar <sebastian.weisshaar@jina.ai>"
    },
    {
        "repo_url": "github.com/jina-ai/jerboa",
        "filepath": "jerboa/utils/load_model.py",
        "commit_date": "2023-07-13T11:50:24Z",
        "message": "feat: introduce revision parameter"
    },
    {
        "repo_url": "github.com/jina-ai/jerboa",
        "filepath": "jerboa/utils/load_model.py",
        "commit_date": "2023-07-11T09:46:46Z",
        "message": "feat: gradio app (#111)\n\n* feat: add artifact, app runs\n\n* feat: change to falcon-40b\n\n* feat: change device map to balanced\n\n* feat: change device map to auto\n\n* feat: load in 8bit\n\n* style: black and ruff applied\n\n* feat: add slider for new tokens\n\n* feat: increase max new tokens to 1024\n\n* style: apply black and ruff\n\n* style: rename directories for artifacts\n\n* style: move configurations to frotn\n\n* build: update poetry to include gradio\n\n* build: include gradio in dependency, optional\n\n* feat: increase temperature to 0.2\n\n* feat: changed temperature\n\n* fix: remove merge conflict\n\n* refactor: remove artifacts directory\n\n* feat: change app to Typer app\n\n* style: black and ruff\n\n* fix: change base model to falcon 7b\n\n* fix: only use lora repo after wandb prefix\n\n* feat: add public link in launch\n\n* docs: explain gradio app in docs\n\n---------\n\nCo-authored-by: Sebastian Weisshaar <sebastian.weisshaar@jina.ai>"
    },
    {
        "repo_url": "github.com/jina-ai/jerboa",
        "filepath": "jerboa/utils/load_model.py",
        "commit_date": "2023-07-11T07:15:31Z",
        "message": "refactor: make general load model function in utils (#105)\n\n* feat: evaluate first two examples\n\n* feat: trust remote code = True\n\n* feat: add quantization\n\n* feat: correct library imports\n\n* refactor: print(x) for evaluation\n\n* feat: take first element of x for generation\n\n* refactor: device allocation at creation of model\n\n* fix: move model to device not tokenizer\n\n* refactor: remove encode from tokenizer\n\n* feat: put model in eval mode\n\n* refactor: comment out kwargs\n\n* refactor: print(x)\n\n* refactor: print model type\n\n* refactor: change arg to kwarg\n\n* refactor: uncommented generate args\n\n* feat: add generation config\n\n* refactor: add generationconfig import\n\n* feat: get configuation from original model\n\n* fix: typo\n\n* feat: set padding token to eos token\n\n* feat: add penalties for repition and length\n\n* refactor: add generation config back\n\n* refactor: increase length penalty\n\n* refactor: change evaluation file\n\n* feat: increased penalties for length and repition\n\n* feat: extreme penalties\n\n* fix: repitition penalty should be float\n\n* feat: increase max length and length penalty\n\n* refactor: evaluate different instructions\n\n* feat: increase max length\n\n* feat: only get target indices\n\n* feat: lower lentgh and repition penalty\n\n* feat: change back to extreme length and repition penalties\n\n* feat: change back to extreme length and repition penalties\n\n* feat: changed penalties\n\n* style: applied black and ruff\n\n* feat: make code understandable\n\n* feat: create new evaluation file for structure\n\n* feat: organized new generation file, hyperparamters for sensible generation\n\n* feat: create evaluation file\n\n* feat: output file for config adjusted results\n\n* feat: create csv file for comparison\n\n* feat: add typer app\n\n* style: applied black\n\n* feat: unify model loading\n\n* fix: specify lora weights correctly\n\n* test: add test for model loading\n\n* feat: quant config and model config to load model\n\n* feat: remove redundant code in finetune.py\n\n* feat: integrated load_model function into finetuning\n\n* fix: remove typo in load model\n\n* fix: remove typo in load model\n\n* feat: include load model function in save_full_models\n\n* fix: specify correct source of lora weights\n\n* feat: incorporate load model into run_code_eval\n\n* refactor: remove old run_code_eval\n\n* style: apply black and ruff\n\n* refactor: remove eval_corrected.jsonl\n\n* fix: remove main point entry\n\n* feat: include source of lora weights in string\n\n* refactor: remove csv creation file\n\n* refactor: remove csv file\n\n* build: include slow marker in pytoml\n\n* ci: exclude slow parts from ci test\n\n* ci: update flag to exclude slow tests\n\n* ci: change flag for slow test\n\n* ci: remove model loading test\n\n* ci: remove no slow flag from pytest ci\n\n---------\n\nCo-authored-by: Sebastian Weisshaar <sebastian.weisshaar@jina.ai>"
    },
    {
        "repo_url": "github.com/GFNOrg/gfn-lm-tuning",
        "filepath": "infill_subj_arithmetic/eval_infill.py",
        "commit_date": "2023-10-08T22:52:49Z",
        "message": "complete merge"
    },
    {
        "repo_url": "github.com/Scikud/AnythingButWrappers",
        "filepath": "Efficient_RedPajama_Finetuning/inference_example.py",
        "commit_date": "2023-05-06T23:10:41Z",
        "message": "inference: Fix path"
    },
    {
        "repo_url": "github.com/Scikud/AnythingButWrappers",
        "filepath": "Efficient_RedPajama_Finetuning/inference_example.py",
        "commit_date": "2023-05-06T14:41:26Z",
        "message": "smaller training set, inference example"
    },
    {
        "repo_url": "github.com/yuta0306/LLM-JDD",
        "filepath": "demo.py",
        "commit_date": "2023-09-25T08:06:14Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/RecAlpaca",
        "filepath": "generate.py",
        "commit_date": "2023-03-29T15:31:30Z",
        "message": "first version"
    },
    {
        "repo_url": "github.com/AGI-Edgerunners/RecAlpaca",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-03-29T15:31:30Z",
        "message": "first version"
    },
    {
        "repo_url": "github.com/TIGER-AI-Lab/ImagenHub",
        "filepath": "src/imagen_hub/infermodels/wuerstchen.py",
        "commit_date": "2024-02-24T22:33:01Z",
        "message": "Update wuerstchen.py\n\nModifying Import"
    },
    {
        "repo_url": "github.com/TIGER-AI-Lab/ImagenHub",
        "filepath": "src/imagen_hub/infermodels/wuerstchen.py",
        "commit_date": "2024-02-15T19:06:13Z",
        "message": "Modify Wuerstchen by agreeing its argument in infer_one_image function."
    },
    {
        "repo_url": "github.com/TIGER-AI-Lab/ImagenHub",
        "filepath": "src/imagen_hub/infermodels/wuerstchen.py",
        "commit_date": "2024-02-14T17:15:37Z",
        "message": "Modify Wuerstchen and __init__"
    },
    {
        "repo_url": "github.com/TIGER-AI-Lab/ImagenHub",
        "filepath": "src/imagen_hub/infermodels/wuerstchen.py",
        "commit_date": "2024-02-13T03:12:00Z",
        "message": "Modify Wuerstchen by pipeline in diffusers"
    },
    {
        "repo_url": "github.com/TIGER-AI-Lab/ImagenHub",
        "filepath": "src/imagen_hub/infermodels/wuerstchen.py",
        "commit_date": "2024-02-08T23:38:58Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/ukairia777/LLM-Finetuning-tutorial",
        "filepath": "merge.py",
        "commit_date": "2023-12-30T16:26:46Z",
        "message": "'add_code'"
    },
    {
        "repo_url": "github.com/ukairia777/LLM-Finetuning-tutorial",
        "filepath": "inference.py",
        "commit_date": "2023-12-30T16:26:46Z",
        "message": "'add_code'"
    },
    {
        "repo_url": "github.com/WangRongsheng/Aurora",
        "filepath": "app.py",
        "commit_date": "2024-01-23T03:17:01Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/WangRongsheng/Aurora",
        "filepath": "src/llmtuner/model/adapter.py",
        "commit_date": "2023-12-21T09:19:27Z",
        "message": "add src"
    },
    {
        "repo_url": "github.com/Tongyi-EconML/FinQwen",
        "filepath": "solutions/3_hxjj/app/model/llm/qwen.py",
        "commit_date": "2024-01-04T07:59:03Z",
        "message": "\u547d\u540d\u7edf\u4e00"
    },
    {
        "repo_url": "github.com/taprosoft/llm_finetuning",
        "filepath": "merge_lora_checkpoint.py",
        "commit_date": "2023-07-08T02:46:49Z",
        "message": "inital commit"
    },
    {
        "repo_url": "github.com/taprosoft/llm_finetuning",
        "filepath": "utils/loader/gptq_loader.py",
        "commit_date": "2023-07-08T02:46:49Z",
        "message": "inital commit"
    },
    {
        "repo_url": "github.com/UMass-Foundation-Model/CoVLM",
        "filepath": "LLaVA/llava/model/builder.py",
        "commit_date": "2023-11-01T14:42:53Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/UMass-Foundation-Model/CoVLM",
        "filepath": "transformers/src/transformers/trainer.py",
        "commit_date": "2023-11-01T14:42:53Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/UMass-Foundation-Model/CoVLM",
        "filepath": "transformers/src/transformers/modeling_utils.py",
        "commit_date": "2023-11-01T14:42:53Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/UMass-Foundation-Model/CoVLM",
        "filepath": "transformers/src/transformers/integrations/peft.py",
        "commit_date": "2023-11-01T14:42:53Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/MILVLG/imp",
        "filepath": "imp_llava/model/builder.py",
        "commit_date": "2024-02-09T09:25:25Z",
        "message": "add training/evaluation code"
    },
    {
        "repo_url": "github.com/bryanchrist/llama2-70b",
        "filepath": "ppo.py",
        "commit_date": "2023-09-07T21:01:20Z",
        "message": "generated text in 8-bit"
    },
    {
        "repo_url": "github.com/bryanchrist/llama2-70b",
        "filepath": "ppo.py",
        "commit_date": "2023-08-18T12:41:57Z",
        "message": "building PPO model"
    },
    {
        "repo_url": "github.com/bryanchrist/llama2-70b",
        "filepath": "ppo.py",
        "commit_date": "2023-08-04T19:55:08Z",
        "message": "fixing deletion of repository"
    },
    {
        "repo_url": "github.com/OSU-NLP-Group/llm-planning-eval",
        "filepath": "generators/hf_generator.py",
        "commit_date": "2024-02-21T19:31:04Z",
        "message": "Added code. Updated README. 02/21/24/"
    },
    {
        "repo_url": "github.com/OSU-NLP-Group/llm-planning-eval",
        "filepath": "evaluators/codellama_evaluator.py",
        "commit_date": "2024-02-21T19:31:04Z",
        "message": "Added code. Updated README. 02/21/24/"
    },
    {
        "repo_url": "github.com/DeepSoftwareAnalytics/SoTaNa",
        "filepath": "webui/generate.py",
        "commit_date": "2023-09-15T09:31:11Z",
        "message": "add webui"
    },
    {
        "repo_url": "github.com/DeepSoftwareAnalytics/SoTaNa",
        "filepath": "inference/code-generation/inference.py",
        "commit_date": "2023-08-25T14:15:00Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/DeepSoftwareAnalytics/SoTaNa",
        "filepath": "inference/code-summarization/inference.py",
        "commit_date": "2023-08-25T14:15:00Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/DeepSoftwareAnalytics/SoTaNa",
        "filepath": "inference/stackoverflow-question-answering/inference.py",
        "commit_date": "2023-08-25T14:15:00Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/china-ai-law-challenge/CAIL2023",
        "filepath": "ssrd/baseline/eval/inference3_ev.py",
        "commit_date": "2023-09-08T01:39:35Z",
        "message": "add ssrd track"
    },
    {
        "repo_url": "github.com/china-ai-law-challenge/CAIL2023",
        "filepath": "ssrd/baseline/eval/inference2_inter.py",
        "commit_date": "2023-09-08T01:39:35Z",
        "message": "add ssrd track"
    },
    {
        "repo_url": "github.com/china-ai-law-challenge/CAIL2023",
        "filepath": "ssrd/baseline/eval/inference4_final_result.py",
        "commit_date": "2023-09-08T01:39:35Z",
        "message": "add ssrd track"
    },
    {
        "repo_url": "github.com/Azure/app-service-linux-docs",
        "filepath": "HowTo/gRPC/Linux/OpenAI/LangChain/PyServer/venv/Lib/site-packages/transformers/trainer.py",
        "commit_date": "2024-02-29T20:19:37Z",
        "message": "adding windows grpc docs"
    },
    {
        "repo_url": "github.com/choosewhatulike/trainable-agents",
        "filepath": "FastChat/fastchat/model/apply_lora.py",
        "commit_date": "2023-11-14T13:13:35Z",
        "message": "camera ready"
    },
    {
        "repo_url": "github.com/choosewhatulike/trainable-agents",
        "filepath": "FastChat/fastchat/model/model_adapter.py",
        "commit_date": "2023-11-14T13:13:35Z",
        "message": "camera ready"
    },
    {
        "repo_url": "github.com/Efficient-Large-Model/VILA",
        "filepath": "llava/train/train.py",
        "commit_date": "2024-02-23T09:19:39Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/Efficient-Large-Model/VILA",
        "filepath": "export_peft_model.py",
        "commit_date": "2024-02-23T09:19:39Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/Efficient-Large-Model/VILA",
        "filepath": "llava/model/builder.py",
        "commit_date": "2024-02-23T09:19:39Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/auto.py",
        "commit_date": "2023-08-07T14:34:54Z",
        "message": "[`core`] PEFT refactor + introducing `inject_adapter_in_model` public method (#749)\n\nRefactors a bit the internals of some PEFT models and introduces a new\nmethod inject_adapter_in_model for users that want to pass a bare model\nand a peft config to inject adapters in-place into the model. These\nchanges are totally BC with the previous PEFT versions.\n\nThis PR makes things easier for the PEFT integration in transformers\nhuggingface/transformers#25077\n\nThe main goal of the PR is to expose a new API for advanced users that\nwant to integrate PEFT method without the need to use the PeftModel\nwrapper. A simple use case could be someone that wants to inject adapters\ninto a model and wants to keep the original class of the model without\nhaving to offload that to peft that will create a PeftModel. I have\nfaced this issue in huggingface/transformers#25077 Among other things,\nthis PR refactors some internals of PEFT library, while keeping it fully\nbackward compatible.\n\nTo tackle the main motivation I propose to differentiate things between\ntwo type of adapters\n\n1- adapters that are injectable (LoRA, AdaLoRA, IA3)\n2- adapters that are not injectable (the rest)\n\nAs a first iteration this API would be supported only for the scenario\n1- / therefore I decided to create 2 abstract classes to make things\neasy to be able to determine if the adapter layer (e.g. LoraLayer) /\nadapter module (e.g. LoraModel) does follow the minimal\nrequirement (i.e. needed attributes, etc.)\n\nOther related changes:\n\n1- Creates a new property method is_prompt_learning to avoid importing\n   PromptLearningConfig all the way down\n2- Introduces a new object TUNERS_MAPPING, which is a mapping of\n   supported pluggable adapters\n3- Creates two abstract classes\n3.1- BaseTunerLayer: a mixin to check for minimal required attributes\n     that a tuner layer should have active_adapter / _is_plugable\n3.2- BaseTuner: a higher level module mixin that should be used for any\n     injectable adapters in the future.\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/auto.py",
        "commit_date": "2023-07-15T12:18:34Z",
        "message": "[`Auto`] Support `AutoPeftModel` for custom HF models (#707)\n\n* support `AutoPeftModel` for custom HF models\n\n* added documentation."
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/auto.py",
        "commit_date": "2023-07-14T09:07:09Z",
        "message": "Introducing `AutoPeftModelForxxx` (#694)\n\n* working v1 for LMs\n\n* added tests.\n\n* added documentation.\n\n* fixed ruff issues.\n\n* added `AutoPeftModelForFeatureExtraction` .\n\n* replace with `TypeError`\n\n* address last comments\n\n* added comment."
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/config.py",
        "commit_date": "2023-08-07T14:34:54Z",
        "message": "[`core`] PEFT refactor + introducing `inject_adapter_in_model` public method (#749)\n\nRefactors a bit the internals of some PEFT models and introduces a new\nmethod inject_adapter_in_model for users that want to pass a bare model\nand a peft config to inject adapters in-place into the model. These\nchanges are totally BC with the previous PEFT versions.\n\nThis PR makes things easier for the PEFT integration in transformers\nhuggingface/transformers#25077\n\nThe main goal of the PR is to expose a new API for advanced users that\nwant to integrate PEFT method without the need to use the PeftModel\nwrapper. A simple use case could be someone that wants to inject adapters\ninto a model and wants to keep the original class of the model without\nhaving to offload that to peft that will create a PeftModel. I have\nfaced this issue in huggingface/transformers#25077 Among other things,\nthis PR refactors some internals of PEFT library, while keeping it fully\nbackward compatible.\n\nTo tackle the main motivation I propose to differentiate things between\ntwo type of adapters\n\n1- adapters that are injectable (LoRA, AdaLoRA, IA3)\n2- adapters that are not injectable (the rest)\n\nAs a first iteration this API would be supported only for the scenario\n1- / therefore I decided to create 2 abstract classes to make things\neasy to be able to determine if the adapter layer (e.g. LoraLayer) /\nadapter module (e.g. LoraModel) does follow the minimal\nrequirement (i.e. needed attributes, etc.)\n\nOther related changes:\n\n1- Creates a new property method is_prompt_learning to avoid importing\n   PromptLearningConfig all the way down\n2- Introduces a new object TUNERS_MAPPING, which is a mapping of\n   supported pluggable adapters\n3- Creates two abstract classes\n3.1- BaseTunerLayer: a mixin to check for minimal required attributes\n     that a tuner layer should have active_adapter / _is_plugable\n3.2- BaseTuner: a higher level module mixin that should be used for any\n     injectable adapters in the future.\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/helpers.py",
        "commit_date": "2023-08-10T10:14:40Z",
        "message": "Helper function to update model signature (#784)\n\nProvides helper functions in peft.helpers to update the signature of the\nforward or generate method of a PeftModel (or subclass). This can be\nuseful because the wrapping class may override the docstring and type\nannotations of the underlying base model. Applying the helper functions\nwill restore those, leading to better tab completion, help text, etc.\n\nFor the time being, these helper functions are purely optional to use.\nAt a later stage, we may consider applying them automatically, but that\nwould require testing to ensure that nothing breaks."
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2024-01-24T10:58:37Z",
        "message": "add further documentation"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-11-23T12:35:54Z",
        "message": "bug fix for altered SFT-RigL behaviour; added implementation of SFT-accumulative"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-11-13T14:21:04Z",
        "message": "support non-fp32 dtypes for SFT deltas"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-11-11T15:26:42Z",
        "message": "implement L2 regularisation for SftModel"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-09-08T21:39:19Z",
        "message": "initial implementation of SftModel"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-08-30T15:16:22Z",
        "message": "DOC: PeftModel save_pretrained docstring (#881) (#888)"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-08-29T08:53:14Z",
        "message": "FIX: seq2seq prompt tuning (#439) (#809)"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-08-25T06:12:11Z",
        "message": "\ud83c\udf89 Add Multitask Prompt Tuning (#400)\n\n* mpt\n\n* fix save\n\n* fix save\n\n* add jupyter notebook\n\n* add jupyter notebook\n\n* add jupyter notebook\n\n* drop shuffling\n\n* drop classify_dataset\n\n* drop classify_dataset\n\n* fix keys\n\n* fix keys\n\n* add comments\n\n* use EXACT_SOURCE_TASK in the example\n\n* formatting\n\n* Fix dict index in embedding retrieval\n\n* run style and quality\n\n* run style and quality\n\n* run style and quality\n\n* style\n\n* final fix\n\n* style\n\n* comment out failing tests\n\n* fix generation tests\n\n* fix style and save test\n\n* all testcases\n\n* fix import\n\n* add license header\n\n* reformat\n\n* fix encoder-decoder models\n\n* fix tests running multiple times\n\n* fix paper name for IA3 and add MPT paper\n\n* Trigger CI\n\n* address the recommended changes\n\n* reformat\n\n* address suggestions\n\n* address suggestions\n\n* revert reformatting\n\n* revert reformatting\n\n---------\n\nCo-authored-by: Alex-Brooks <Alex.Brooks@ibm.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-08-11T21:31:17Z",
        "message": "GPTQ Integration (#771)\n\n* add gptq lora\n\n* fix peft gptq\n\n* fix condition\n\n* fix test\n\n* remove unused weights\n\n* check type\n\n* style\n\n* change attribute\n\n* remove print\n\n* add exllama\n\n* make style\n\n* refactor + fix tests\n\n* remove print\n\n* remove dep on transformers"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-08-08T12:38:23Z",
        "message": "Update docstring of PeftModel.from_pretrained (#799)\n\n1. Addresses\nhttps://github.com/huggingface/peft/issues/430#issuecomment-1666312815\n2. Reword docstring to not be LoRA-specific"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-08-08T12:35:19Z",
        "message": "Add adapter error handling (#800)\n\nWhen a user tries to add a 2nd adapter, Lora and AdaLora make some checks to\nensure the new adapter is compatible with existing adapters. Currently, that\ncheck is performed halfway through the method. This means that if the check\nfails, the new adapter is partially applied, leaving the model in a bad state.\nThe main purpose of this PR is to ensure that the model state is correct after\nsuch a failure is encountered.\n\nTests were added to catch this potential bug.\n\nWhile working on this, I also did some related, but not strictly necessary\nchanges to the add_adapter methods:\n\n- Previously, the peft_config from the PeftModel was passed to the base\n  model. This meant that sometimes, the base model would hold a reference\n  to PeftModel.peft_config, but not always, as some base models would\n  create new dicts. This is problematic, because some code would rely on\n  the objects being the same. Now, they are never the same, leading to\n  more consistency.\n- I think that the check if multiple adapters have biases (which is not\n  supported) was accidentally removed by #749. It is added back in.\n- Add some type annotations\n- Extend docstrings to contain adapter_name"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-08-07T14:34:54Z",
        "message": "[`core`] PEFT refactor + introducing `inject_adapter_in_model` public method (#749)\n\nRefactors a bit the internals of some PEFT models and introduces a new\nmethod inject_adapter_in_model for users that want to pass a bare model\nand a peft config to inject adapters in-place into the model. These\nchanges are totally BC with the previous PEFT versions.\n\nThis PR makes things easier for the PEFT integration in transformers\nhuggingface/transformers#25077\n\nThe main goal of the PR is to expose a new API for advanced users that\nwant to integrate PEFT method without the need to use the PeftModel\nwrapper. A simple use case could be someone that wants to inject adapters\ninto a model and wants to keep the original class of the model without\nhaving to offload that to peft that will create a PeftModel. I have\nfaced this issue in huggingface/transformers#25077 Among other things,\nthis PR refactors some internals of PEFT library, while keeping it fully\nbackward compatible.\n\nTo tackle the main motivation I propose to differentiate things between\ntwo type of adapters\n\n1- adapters that are injectable (LoRA, AdaLoRA, IA3)\n2- adapters that are not injectable (the rest)\n\nAs a first iteration this API would be supported only for the scenario\n1- / therefore I decided to create 2 abstract classes to make things\neasy to be able to determine if the adapter layer (e.g. LoraLayer) /\nadapter module (e.g. LoraModel) does follow the minimal\nrequirement (i.e. needed attributes, etc.)\n\nOther related changes:\n\n1- Creates a new property method is_prompt_learning to avoid importing\n   PromptLearningConfig all the way down\n2- Introduces a new object TUNERS_MAPPING, which is a mapping of\n   supported pluggable adapters\n3- Creates two abstract classes\n3.1- BaseTunerLayer: a mixin to check for minimal required attributes\n     that a tuner layer should have active_adapter / _is_plugable\n3.2- BaseTuner: a higher level module mixin that should be used for any\n     injectable adapters in the future.\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-08-02T14:59:11Z",
        "message": "Allow passing inputs_embeds instead of input_ids (#757)\n\nResolves #727\n\nRight now, there is an issue with a few PeftModelForXxx classes when\nusers pass only inputs_embeds but not input_ids. First of all, the batch\nsize was being derived on input_ids, now it is derived from\ninputs_embeds instead if input_ids is None. Furthermore, a few forward\ncalls to the base model were not passing the inputs_embeds along, which\nresulted in errors down the line. These issues have been fixed now."
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-08-01T13:46:18Z",
        "message": "Support XPU adapter loading   (#737)"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-19T12:57:14Z",
        "message": "[`Patch`]\u00a0patch trainable params for 4bit layers (#733)\n\n* patch trainable params for 4bit layers\n\n* revert\n\n* added tests.\n\n* added comments.\n\n* addressed final comments"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-19T12:29:36Z",
        "message": "[`Llama2`] Add disabling TP behavior (#728)\n\n* add disabling TP behavior\n\n* add comments\n\n* adapt from new changes of transformers PR"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-19T08:59:55Z",
        "message": "revert change (#731)"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-19T07:52:25Z",
        "message": "fix the param count when using 4-bit bnb"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-19T05:47:15Z",
        "message": "Fix subfolder issue (#721)\n\n* fix subfolder issue\n\n* added tests"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-17T09:58:57Z",
        "message": "better hub kwargs management (#712)"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-17T08:02:30Z",
        "message": "FEAT: Make LoRA work with custom models (#676)\n\nEnable custom models to work with LoRA\n\nThis PR enables custom models to work with LoRA in peft by performing a few\nchanges required for non-transformers models. New tests for linear,\ntransformers conv1d, and conv2d layers were added.\n\nNot yet contained in this PR:\n\n- support for AdaLoRA and IA\u00b3\n- documentation\n- examples\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-15T12:18:34Z",
        "message": "[`Auto`] Support `AutoPeftModel` for custom HF models (#707)\n\n* support `AutoPeftModel` for custom HF models\n\n* added documentation."
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-14T14:28:03Z",
        "message": "[`Feature`] Save only selected adapters for LoRA (#705)\n\n* v1 working for LoRA\n\n* more checks\n\n* fix prompt learning issues\n\n* fix failing test\n\n* Apply suggestions from code review\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* fixed indentation\n\n* move the check above\n\n* added tests for adaption prompt, enc-dec and feature extraction\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-14T14:14:51Z",
        "message": "[Core] Enhancements and refactoring of LoRA method (#695)\n\n* refactor lora and add utils\n\n1. Refactor LoRA code\n2. Add method to delete LoRA adapters\n3. Add method to unload the PEFT LoRA model.\n4. Add `svd` weighted adapter support.\n5. minor fixes\n\n* fixes\n\n* fixes\n\n* Update lora.py\n\n* fixes\n\n* Update lora.py\n\n* docstrings for the added public APIs\n\n* docs\n\n* Update src/peft/tuners/lora.py\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* resolve comments, refactoring and adding tests\n\n* fix the remaining failing tests\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-14T12:33:33Z",
        "message": "[WIP] FIX for disabling adapter, adding tests (#683)\n\nThis PR deals with some issues with disabling adapter:\n\n- typo in active.adapter\n- prompt encoder could be on wrong device\n- when using prompt learning + generate, disabling did not work\n\nFor the last point, there is a somewhat ugly fix in place for now,\npending a more comprehensive refactor (a comment was added to that\neffect).\n\nComprehensive tests were added to check that everything works now.\n\nThe following tests still not working:\n\n- adaption prompt\n- seq2seq with prompt tuning/prompt encoding\n- stable diffusion is a little bit flaky but test is hopefully robust enough\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-13T12:34:28Z",
        "message": "add support for Feature Extraction using PEFT (#647)\n\n* add support for embedding with peft\n\n* add example and resolve code quality issues\n\n* update notebook example post fixing the loss\n\n* adding full example with inference notebook\n\n* quality \u2728\n\n* add tests, docs, guide and rename task_type to be inline with Hub\n\n* fixes\n\n* fixes\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update peft_model.py\n\n* fixes\n\n* final fixes\n\n* Update _toctree.yml\n\n* fixes and make style and make quality\n\n* deberta exception with checkpointing\n\n* Update docs/source/task_guides/semantic-similarity-lora.md\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Update docs/source/task_guides/semantic-similarity-lora.md\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* resolve comments\n\n* testing prompt learning methods\n\n* Update testing_common.py\n\n* fix the tests\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-13T09:12:40Z",
        "message": "FIX: base_model_torch_dtype when using model.half() after init (#688)"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-07-13T07:45:50Z",
        "message": "Add functionality to support IA3 (#578)\n\n* Added initial ia3 code\n\n* Implemented ia3 correctly for feedforward layers; Fixed regex matching\n\n* Fixed module mapping for mt5\n\n* Merged changes from huggingface:main\n\n* Merged changes\n\n* Fixed lora merge conflicts\n\n* Different bloom config\n\n* Added save option for ia3\n\n* Added loading code for ia3\n\n* Added feedforward implementation in utils and seq cls example\n\n* Added feedforward implementation in utils and seq cls example\n\n* Implemented merge, unmerge, enable/disable adapters functionality\n\n* Fixed feedforward during merge\n\n* Debugging Merge\n\n* Removing debug messages\n\n* Cleaned up repo\n\n* Removed non-IA3 changes\n\n* Refactor save and load\n\n* Added support to all models in tests; Added IA3Config for common tests\n\n* Added half-precision support and test for gradient checkpointing; Formatted jupyter notebooks\n\n* Added target modules for new models GPTBigCode and LLama\n\n* Cleaned up code\n\n* Cleaned up code\n\n* Cleaned up example notebook\n\n* Cleaned up  seq2seq notebook\n\n* Corrected function docstrings; refactored find_and_replace\n\n* Corrected function docstrings; refactored find_and_replace\n\n* Added basic docs for IA3\n\n* Added new conceptual guide in source tree for documentation\n\n* Minor fix to documentation\n\n* Minor fixes to docstrings; Added error handling for 4bit quantization; Cleaned unused merge/unmerge methods\n\n* styling changes after merge from main\n\n* Update src/peft/tuners/ia3.py\n\nRemove unused attribute merge_weights\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Abhishek2304 <abhishekgupta2304@gmail.com>\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-28T07:03:16Z",
        "message": "style: tentatively add hints for some public function (#614)\n\n* style: tentatively add hints for some public function\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* fix: import annotations to evaluate to str\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* fix: style\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-27T21:41:51Z",
        "message": "Update peft_model.py (#644)"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-27T12:57:57Z",
        "message": "feat(model): Allow from_pretrained to accept PeftConfig class (#612)\n\n* feat(model): Allow from_pretrained to accept PeftConfig class\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* tests: add test cases for config construction\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* chore: address comments and run tools\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n* fix: style\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Aaron <29749331+aarnphm@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-27T11:26:47Z",
        "message": "fix ptun and prompt tuning generation issue (#543)\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-27T08:27:21Z",
        "message": "fix Prefix-tuning error in clm Float16 evaluation (#520)\n\nSigned-off-by: Wang, Yi <yi.a.wang@intel.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-27T06:15:49Z",
        "message": "Add seq2seq prompt tuning support (#519)\n\n* Added prompt tuning for seq2seq and corresponding notebook examples\n\n* Added prompt tuning for seq2seq and corresponding notebook examples\n\n* Added prompt tuning for seq2seq and corresponding notebook examples\n\n* Call encoder with get_encoder() and update notebook example\n\n* Style formatting\n\n* Add seq2seq p-tuning support, and improve seq2seq prompt tuning support, enabling the use of generate()\n\n* Fix imports\n\n* Fix imports\n\n* Add co-author.\n\nCo-authored-by: ZhengxiangShi michaelszx117@gmail.com\n\n* Add co-author.\n\nCo-authored-by: ZhengxiangShi <michaelszx117@gmail.com>\n\n---------\n\nCo-authored-by: Thomas SCHILLACI <tschilla@px101.prod.exalead.com>\nCo-authored-by: ZhengxiangShi <michaelszx117@gmail.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-19T08:49:41Z",
        "message": "Improve the README when using PEFT (#594)\n\n* add logic\n\n* Update peft_model.py\n\n* fix test failures\n\n* fixes\n\n* fix"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-16T11:23:58Z",
        "message": "feat: Add PeftModelForQuestionAnswering (#473)\n\n* Added first try of supporting QuestionAnswering\n\n* Updated example to be correct\n\n* Added changes from PR 404\n\n* Added missing mapping for task type\n\n* Remove unrelated code\n\n* Run make style"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-16T11:04:07Z",
        "message": "when from_pretrained is called in finetune of lora with flag \"is_trainable\" True, should not call model.eval() (#591)"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-16T08:58:51Z",
        "message": "Fix typo at peft_model.py (#588)\n\nFix typo on description:\n- `imputs_embeds` to `inputs_embeds`"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-15T10:23:05Z",
        "message": "[`core`] Correctly passing the kwargs all over the place (#575)\n\n* v1 of the fix\n\n* forward contrib credits from discussions\n\n* add tests\n\n---------\n\nCo-authored-by: winglian <winglian@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-15T07:35:43Z",
        "message": "enable lora for mpt (#576)\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-09T10:33:13Z",
        "message": "[`core`] Add safetensors integration (#553)\n\n* add v1\n\n* clean up\n\n* more improvements\n\n* add device\n\n* final adjustements\n\n* use `EntryNotFoundError`\n\n* better checks\n\n* add tests and final fixes\n\n* make style && make quality\n\n* remove `push_to_hub` because of the release"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-07T12:39:17Z",
        "message": "add thousands separator in print_trainable_parameters (#443)"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-05T13:14:40Z",
        "message": "add library name to model card (#549)"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-01T09:17:05Z",
        "message": "Fixed problem with duplicate same code. (#517)"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-01T09:16:38Z",
        "message": "return load_result when load_adapter (#481)"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-06-01T09:09:54Z",
        "message": "Enable PeftConfig & PeftModel to load from revision (#433)\n\n* Enable PeftConfig to load from revision\n\n* Add revision to PeftModel\n\n* Fix weights download with revision"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-05-31T10:14:27Z",
        "message": "[`core`] Add gradient checkpointing check (#404)\n\n* add automatic input enable gradients when calling `get_peft_model`\n\n* style\n\n* better check\n\n* add 4bit check"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-05-31T06:08:12Z",
        "message": "Remove merge_weights (#392)"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-05-20T15:47:15Z",
        "message": "4-bit QLoRA via bitsandbytes (4-bit base model + LoRA) (#476)\n\n* 4bit lora\n\n* 4bit test\n\n* fixing 4bits bugs\n\n* fp4 pass variables\n\n* fix inference datatype and generation config\n\n* updating prep for int8 function to work for 4-bit\n\n* Added FP4 LoRA and FP4 fine-tuning example.\n\n* LinearFP4 -> Linear4bit\n\n* fixes\n\n* Fixed 4-bit example.\n\n* Style changes.\n\n* final changes\n\n---------\n\nCo-authored-by: Artidoro Pagnoni <pagnoni.artidoro@gmail.com>\nCo-authored-by: younesbelkada <younesbelkada@gmail.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-05-10T04:39:28Z",
        "message": "do not use self.device. In FSDP cpu offload mode. self.device is \"CPU\" instead of \"cuda\" (#352)\n\nand there's error like \"Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:1\"\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-26T12:56:14Z",
        "message": "Use `try` and `finally` in `disable_adapter()` to catch exceptions (#368)"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-25T06:54:18Z",
        "message": "Implement adaption prompt from Llama-Adapter paper (#268)\n\n* Implement adaption prompt from Llama-Adapter paper\n\n* Support multi-adapters\n\n* Refactor adaption prompt to target attn modules instead of layers\n\n* Refactor adaption prompt to be more generic\n\n* Fix adaption prompt not on right device\n\n* Apply suggestions from code review\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>\n\n* Fix style\n\n* Add support for Llama config use_cache=True\n\n* Fix rebase issues\n\n---------\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-20T10:46:13Z",
        "message": "fix lora modules_to_save issue (#343)\n\n* fix lora modules_to_save issue\n\n* fix quality"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-08T08:27:08Z",
        "message": "Merge pull request #283 from huggingface/smangrul/multi-lora-support\n\nfix trainable params setting"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-08T06:15:32Z",
        "message": "Update peft_model.py"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-07T10:48:22Z",
        "message": "add and fix tests"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-07T10:36:58Z",
        "message": "Merge remote-tracking branch 'upstream/main' into fix-half-prec"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-06T22:38:10Z",
        "message": "fixing adalora saving and loading"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-06T14:01:21Z",
        "message": "\ud83d\ude05"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-06T13:35:31Z",
        "message": "Merge branch 'main' into smangrul/multi-lora-support"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-06T06:00:30Z",
        "message": "Merge pull request #233 from QingruZhang/main\n\nThe Implementation of AdaLoRA (ICLR 2023)"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-05T20:52:12Z",
        "message": "Run make style and make quality"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-04T20:33:31Z",
        "message": "fix \ud83d\udc1b"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-04T14:14:58Z",
        "message": "fixing \ud83d\udc1b"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-04T12:14:24Z",
        "message": "fix \ud83d\udc1b"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-04T11:34:32Z",
        "message": "Merge branch 'main' into smangrul/multi-lora-support"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-04T10:01:47Z",
        "message": "fix half precision forward"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-03T16:28:11Z",
        "message": "Fixing a bug where a wrong parameter name is used."
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-04-01T12:54:46Z",
        "message": "[`core`] Fix offload issue (#248)\n\n* fix offload dir\n\n* remove offload index\n\n* safety checker\n\n* forward contrib credits from previous PR\n\n---------\n\nCo-authored-by: cosimoiaia <cosimoiaia@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-03-31T21:41:14Z",
        "message": "fix kwargs"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-03-31T21:30:05Z",
        "message": "clean up docstrings"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-03-30T06:24:45Z",
        "message": "Merge branch 'huggingface:main' into main"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-03-29T13:20:14Z",
        "message": "Causal LM generation fix for prefix tuning: GPT2 model (#222)\n\n* expand attention mask after preparing generation inputs for prefix tuning\n\n* reformat\n\n* Update src/peft/peft_model.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* reformat as per black\n\n---------\n\nCo-authored-by: Vineet Kumar <vineeku6@in.ibm.com>\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-03-29T03:19:14Z",
        "message": "merge the conflit'"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-03-28T14:10:06Z",
        "message": "Update peft_model.py"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-03-28T14:02:21Z",
        "message": "Update peft_model.py"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-03-28T13:59:24Z",
        "message": "Update peft_model.py"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-03-28T13:26:24Z",
        "message": "multi adapter for training and inference\n\nMight have breaking changes"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-03-10T02:36:11Z",
        "message": "changed: 1. replace base_model.prepare_inputs_for_generation and base_model._prepare_encoder_decoder_kwargs_for_generation temporarily"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-03-08T17:36:25Z",
        "message": "Merge pull request #157 from huggingface/smangrul/lora_fixes_and_updates_wrt_trl\n\nlora fixes and adding 8bitMegredLinear lora"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-03-08T10:41:30Z",
        "message": "fix count \n\nnum_params should be directly used."
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-03-07T12:29:02Z",
        "message": "adding 8bitMegredLinear lora"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-03-07T08:38:25Z",
        "message": "Merge pull request #149 from huggingface/smangrul/fixes\n\nminor fixes to the examples"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-03-07T08:34:19Z",
        "message": "fixing issues and quality \u2728"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-03-03T18:13:25Z",
        "message": "support option for encoder only prompts"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-03-02T01:04:48Z",
        "message": "finish the testing and debugging"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-03-01T21:26:07Z",
        "message": "adalora example"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-02-27T03:31:20Z",
        "message": "fix: count params when zero init'd"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-02-25T06:09:07Z",
        "message": "issue#126: torch.load device issue."
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-02-21T18:44:24Z",
        "message": "add util for getting the base model"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-02-17T12:10:45Z",
        "message": "add `disable_adapter` context manager"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-02-10T04:09:02Z",
        "message": "fp32"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-02-09T18:36:25Z",
        "message": "make `save_pretrained` work in a way training could be resumed"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-02-08T13:13:00Z",
        "message": "quality"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-02-08T12:37:15Z",
        "message": "fixes and updating examples"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-02-08T06:48:03Z",
        "message": "add `load_and_dispatch` to `load_pretrained`"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-02-08T03:19:15Z",
        "message": "fix prefix tuning config to remove function field as it cannot be converted to json"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-02-07T09:37:56Z",
        "message": "add support for `generate` when using `prompt_tuning`"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-02-06T18:20:48Z",
        "message": "fix `generate` because of recent transformers release"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/peft_model.py",
        "commit_date": "2023-02-06T13:27:13Z",
        "message": "seq cls examples update"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-11-09T17:27:31Z",
        "message": "implement no-Trainer RigL-SFT (and break everything else)"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-11-07T13:00:22Z",
        "message": "start implementing cumulative SFT"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-09-07T10:14:37Z",
        "message": "ENH Merge lora module to 8bit model (#875)\n\nAllows merging 8bit weights from bnb.\n\n4bit weight merging was already implemented through the dequantization method\nprovided by bnb but there is no official dequantization method for 8bit weights.\nThis PR works by multiplying the weights to an identity matrix using bnb's\nquantization aware matmul operation. Empirically, this results in a very small\nrounding error."
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-09-06T15:31:55Z",
        "message": "ENH Remove redundant initialization layer calls (#887)\n\nThis should lead to a big speedup when initializing LoRA layers.\n\n---------\n\nCo-authored-by: poedator <ruslansv@gmail.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "src/peft/tuners/lora/model.py",
        "commit_date": "2023-08-29T09:32:29Z",
        "message": "MNT: Move tuners to subpackages (#807)\n\nFor each tuner, created a sub-module that contains at least:\n\n- config.py for config stuff\n- model.py for the actual model/encoder/embedding\n- __init__.py so that imports are preserved\n\nThen, when there was a need, further files were created, like layer.py\nor utils.py.\n\nImports were changed to absolute imports everywhere, except for the\nsub-packages within a tuner directory, as these packages will always \nstay together in the same place.\n\nFor some existing modules, the license comment of the top of the file\nwas missing, I always added it.\n\nThere was a bug in the forward method of 4bit linear lora layers introduced\nin #851, for the case that the model is merged AND adapters are disabled.\nFor that scenario, we need to unmerge first before generating the output,\nsame as we do for the vanilla Linear layer. This step was missing from the\ncode previously and is now implemented correctly. Tests were adjusted to\ncatch that error."
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "examples/fp4_finetuning/finetune_fp4_opt_bnb_peft.py",
        "commit_date": "2023-06-02T08:07:46Z",
        "message": "Remove device_map when training 4,8-bit model. (#534)\n\n* Remove device_map when training 4,8-bit model.\n\n* Fix style"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "examples/fp4_finetuning/finetune_fp4_opt_bnb_peft.py",
        "commit_date": "2023-05-20T15:47:15Z",
        "message": "4-bit QLoRA via bitsandbytes (4-bit base model + LoRA) (#476)\n\n* 4bit lora\n\n* 4bit test\n\n* fixing 4bits bugs\n\n* fp4 pass variables\n\n* fix inference datatype and generation config\n\n* updating prep for int8 function to work for 4-bit\n\n* Added FP4 LoRA and FP4 fine-tuning example.\n\n* LinearFP4 -> Linear4bit\n\n* fixes\n\n* Fixed 4-bit example.\n\n* Style changes.\n\n* final changes\n\n---------\n\nCo-authored-by: Artidoro Pagnoni <pagnoni.artidoro@gmail.com>\nCo-authored-by: younesbelkada <younesbelkada@gmail.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "examples/lora_dreambooth/convert_peft_sd_lora_to_kohya_ss.py",
        "commit_date": "2023-06-27T08:26:54Z",
        "message": "[Bugfix] Inserted adapter_name to get_peft_model_state_dict function (#626)\n\n* Update train_dreambooth.py\n\nAccelerator init updated from logging_dir to project_dir. Newer versions of accelerate uses project_dir. logging_dir is deprecated\n\n* Bugfix: Adapter name variable inserted, when changing LORA_ADAPTER_NAME it causes error\n\n* Adapter name added as kwarg\n\n* Black code formatted\n\n* Style & Quality check"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "examples/lora_dreambooth/convert_peft_sd_lora_to_kohya_ss.py",
        "commit_date": "2023-06-21T14:04:39Z",
        "message": "Added Civitai LoRAs conversion to PEFT, PEFT LoRAs conversion to webui (#596)\n\n* Fixed kohya_ss to peft lora conversion, added script for backward conversion\n\n* Fixed getting alpha from PEFT\n\n---------\n\nCo-authored-by: Alexander Kovalchuk <a.kovalchuk@prequelapp.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "tests/test_hub_features.py",
        "commit_date": "2023-07-19T05:47:15Z",
        "message": "Fix subfolder issue (#721)\n\n* fix subfolder issue\n\n* added tests"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-09-06T15:31:55Z",
        "message": "ENH Remove redundant initialization layer calls (#887)\n\nThis should lead to a big speedup when initializing LoRA layers.\n\n---------\n\nCo-authored-by: poedator <ruslansv@gmail.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-08-30T12:40:56Z",
        "message": "MNT Run tests that were skipped previously (#884)\n\nSome tests were skipped because of an issue with how LoRA weights were\ninitialized for embeddings. This issue has been fixed for some time now,\nso the tests no longer need to be skipped."
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-08-16T08:57:38Z",
        "message": "TST: add test about loading custom models (#827)\n\nPrompted by #808, I added a test that shows that loading a trained\ncustom model works as expected. I only added this to custom models\nbecause it involves a few steps of training and I didn't want to slow\ndown tests too much. LMK if this should be added to all tests.\n\nIn addition, I renamed some custom model tests which had strange\nnames (probably caused by an overeager query-replace)."
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-08-08T12:35:19Z",
        "message": "Add adapter error handling (#800)\n\nWhen a user tries to add a 2nd adapter, Lora and AdaLora make some checks to\nensure the new adapter is compatible with existing adapters. Currently, that\ncheck is performed halfway through the method. This means that if the check\nfails, the new adapter is partially applied, leaving the model in a bad state.\nThe main purpose of this PR is to ensure that the model state is correct after\nsuch a failure is encountered.\n\nTests were added to catch this potential bug.\n\nWhile working on this, I also did some related, but not strictly necessary\nchanges to the add_adapter methods:\n\n- Previously, the peft_config from the PeftModel was passed to the base\n  model. This meant that sometimes, the base model would hold a reference\n  to PeftModel.peft_config, but not always, as some base models would\n  create new dicts. This is problematic, because some code would rely on\n  the objects being the same. Now, they are never the same, leading to\n  more consistency.\n- I think that the check if multiple adapters have biases (which is not\n  supported) was accidentally removed by #749. It is added back in.\n- Add some type annotations\n- Extend docstrings to contain adapter_name"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-07-24T11:23:23Z",
        "message": "FIX: Disabling adapter works with modules_to_save (#736)\n\nResolves #493\n\nFor LoRA and IA\u00b3, there was a bug that even even using the\ndisable_adapter context, if the module was listed in modules_to_save,\nthe updated weights would be used instead of the original weights. This\nmeant that disable_adapter would not return the same results as the base\nmodel without adaptation. This PR fixes the issue and provides a test.\n\nNote: I tried to adjust AdaLoRA too, since it seemed that the same\nreasoning should apply there. However, I think that AdaLoRA does not\nreally support disabling adapters at all. E.g. there is no\ndisable_adapter_layers method. Therefore, AdaLoRA was not changed."
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-07-24T08:34:39Z",
        "message": "ENH: Warn when disabling adapters and bias != 'none' (#741)\n\nFor LoRA, given that bias='all' or bias='none', when doing inference\nwith a model in the disable_adapter context, the output will not be\nidentical to the output of the base model. This may be surprising to\nusers. Therefore, a warning is given. Furthermore, the docstring has\nbeen extended to reflect this fact."
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "tests/test_custom_models.py",
        "commit_date": "2023-07-17T08:02:30Z",
        "message": "FEAT: Make LoRA work with custom models (#676)\n\nEnable custom models to work with LoRA\n\nThis PR enables custom models to work with LoRA in peft by performing a few\nchanges required for non-transformers models. New tests for linear,\ntransformers conv1d, and conv2d layers were added.\n\nNot yet contained in this PR:\n\n- support for AdaLoRA and IA\u00b3\n- documentation\n- examples\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "tests/test_adaption_prompt.py",
        "commit_date": "2023-07-14T14:28:03Z",
        "message": "[`Feature`] Save only selected adapters for LoRA (#705)\n\n* v1 working for LoRA\n\n* more checks\n\n* fix prompt learning issues\n\n* fix failing test\n\n* Apply suggestions from code review\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* fixed indentation\n\n* move the check above\n\n* added tests for adaption prompt, enc-dec and feature extraction\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "tests/test_adaption_prompt.py",
        "commit_date": "2023-07-14T12:33:33Z",
        "message": "[WIP] FIX for disabling adapter, adding tests (#683)\n\nThis PR deals with some issues with disabling adapter:\n\n- typo in active.adapter\n- prompt encoder could be on wrong device\n- when using prompt learning + generate, disabling did not work\n\nFor the last point, there is a somewhat ugly fix in place for now,\npending a more comprehensive refactor (a comment was added to that\neffect).\n\nComprehensive tests were added to check that everything works now.\n\nThe following tests still not working:\n\n- adaption prompt\n- seq2seq with prompt tuning/prompt encoding\n- stable diffusion is a little bit flaky but test is hopefully robust enough\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "tests/test_adaption_prompt.py",
        "commit_date": "2023-06-01T09:14:11Z",
        "message": "[`Llama-Adapter`]\u00a0fix half precision inference + add tests (#456)\n\n* fix + add tests\n\n* forward contrib credits from discussions\n\n---------\n\nCo-authored-by: HamidShojanazeri <HamidShojanazeri@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "tests/test_adaption_prompt.py",
        "commit_date": "2023-04-25T06:54:18Z",
        "message": "Implement adaption prompt from Llama-Adapter paper (#268)\n\n* Implement adaption prompt from Llama-Adapter paper\n\n* Support multi-adapters\n\n* Refactor adaption prompt to target attn modules instead of layers\n\n* Refactor adaption prompt to be more generic\n\n* Fix adaption prompt not on right device\n\n* Apply suggestions from code review\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>\n\n* Fix style\n\n* Add support for Llama config use_cache=True\n\n* Fix rebase issues\n\n---------\n\nCo-authored-by: Sourab Mangrulkar <13534540+pacman100@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/AlanAnsell/peft",
        "filepath": "tests/test_multitask_prompt_tuning.py",
        "commit_date": "2023-08-25T06:12:11Z",
        "message": "\ud83c\udf89 Add Multitask Prompt Tuning (#400)\n\n* mpt\n\n* fix save\n\n* fix save\n\n* add jupyter notebook\n\n* add jupyter notebook\n\n* add jupyter notebook\n\n* drop shuffling\n\n* drop classify_dataset\n\n* drop classify_dataset\n\n* fix keys\n\n* fix keys\n\n* add comments\n\n* use EXACT_SOURCE_TASK in the example\n\n* formatting\n\n* Fix dict index in embedding retrieval\n\n* run style and quality\n\n* run style and quality\n\n* run style and quality\n\n* style\n\n* final fix\n\n* style\n\n* comment out failing tests\n\n* fix generation tests\n\n* fix style and save test\n\n* all testcases\n\n* fix import\n\n* add license header\n\n* reformat\n\n* fix encoder-decoder models\n\n* fix tests running multiple times\n\n* fix paper name for IA3 and add MPT paper\n\n* Trigger CI\n\n* address the recommended changes\n\n* reformat\n\n* address suggestions\n\n* address suggestions\n\n* revert reformatting\n\n* revert reformatting\n\n---------\n\nCo-authored-by: Alex-Brooks <Alex.Brooks@ibm.com>"
    },
    {
        "repo_url": "github.com/chs20/RobustVLM",
        "filepath": "llava/model/builder.py",
        "commit_date": "2024-02-19T18:17:12Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/kotoba-tech/kotoba-recipes",
        "filepath": "src/llama_recipes/inference/model_utils.py",
        "commit_date": "2023-11-28T02:49:13Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/kotoba-tech/kotoba-recipes",
        "filepath": "examples/hf_text_generation_inference/merge_lora_weights.py",
        "commit_date": "2023-11-28T02:49:13Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/yakami129/virtualwife-llm-factory",
        "filepath": "finetune/framework/llama_factory/src/llmtuner/model/adapter.py",
        "commit_date": "2024-01-13T02:50:41Z",
        "message": "update: \u96c6\u6210llama_factory"
    },
    {
        "repo_url": "github.com/unionai-oss/llm-fine-tuning",
        "filepath": "redpajama-lora/workflows/llm_inference.py",
        "commit_date": "2023-06-28T06:23:01Z",
        "message": "modify prompt; modify dataset and model paths\n\nSigned-off-by: Samhita Alla <aallasamhita@gmail.com>"
    },
    {
        "repo_url": "github.com/unionai-oss/llm-fine-tuning",
        "filepath": "redpajama-lora/workflows/llm_inference.py",
        "commit_date": "2023-06-02T16:48:26Z",
        "message": "add fine tuning redpajama using slack data\n\nSigned-off-by: Samhita Alla <aallasamhita@gmail.com>"
    },
    {
        "repo_url": "github.com/StrongResearch/isc-demos",
        "filepath": "llama2/llama-recipes/src/llama_recipes/finetuning.py",
        "commit_date": "2023-12-19T23:19:17Z",
        "message": "validation"
    },
    {
        "repo_url": "github.com/StrongResearch/isc-demos",
        "filepath": "llama2/llama-recipes/src/llama_recipes/inference/model_utils.py",
        "commit_date": "2023-12-19T23:19:17Z",
        "message": "validation"
    },
    {
        "repo_url": "github.com/StrongResearch/isc-demos",
        "filepath": "llama2/llama-recipes/examples/hf_text_generation_inference/merge_lora_weights.py",
        "commit_date": "2023-12-19T23:19:17Z",
        "message": "validation"
    },
    {
        "repo_url": "github.com/Junjie-Ye/ToolEyes",
        "filepath": "Code/Inference/models.py",
        "commit_date": "2024-01-12T16:31:24Z",
        "message": "update code"
    },
    {
        "repo_url": "github.com/Junjie-Ye/ToolEyes",
        "filepath": "Code/model/model_adapter.py",
        "commit_date": "2024-01-12T16:31:24Z",
        "message": "update code"
    },
    {
        "repo_url": "github.com/X-PLUG/mPLUG-HalOwl",
        "filepath": "HaELM/interface.py",
        "commit_date": "2024-01-29T03:34:16Z",
        "message": "Update HaELM"
    },
    {
        "repo_url": "github.com/X-PLUG/mPLUG-HalOwl",
        "filepath": "hacl/llava/model/builder.py",
        "commit_date": "2024-01-22T05:56:36Z",
        "message": "commit hacl"
    },
    {
        "repo_url": "github.com/yanqiangmiffy/llm-finetune",
        "filepath": "bakcup/peft/peft_model.py",
        "commit_date": "2023-11-21T06:06:20Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/yanqiangmiffy/llm-finetune",
        "filepath": "bakcup/peft/tuners/lora.py",
        "commit_date": "2023-11-21T06:06:20Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/yanqiangmiffy/llm-finetune",
        "filepath": "bakcup/peft/tuners/prompt_tuning.py",
        "commit_date": "2023-11-21T06:06:20Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/UCSC-VLAA/vllm-safety-benchmark",
        "filepath": "baselines/llava/model/builder.py",
        "commit_date": "2023-11-24T02:09:04Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/UCSC-VLAA/vllm-safety-benchmark",
        "filepath": "baselines/llava/serve/model_worker.py",
        "commit_date": "2023-11-24T02:09:04Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/UCSC-VLAA/vllm-safety-benchmark",
        "filepath": "baselines/mplug_owl2_utils/model/builder.py",
        "commit_date": "2023-11-24T02:09:04Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/SqueezeAILab/KVQuant",
        "filepath": "gradients/src/transformers/trainer.py",
        "commit_date": "2024-02-06T09:26:27Z",
        "message": "First Commit"
    },
    {
        "repo_url": "github.com/tosiyuki/alpaca-lora-create-news-title",
        "filepath": "generate_news_title.py",
        "commit_date": "2023-03-19T11:41:38Z",
        "message": "[add]\u30b3\u30fc\u30c9\u306a\u3069\u3092\u8ffd\u52a0"
    },
    {
        "repo_url": "github.com/ntunlplab/traditional-chinese-alpaca",
        "filepath": "code/finetune.py",
        "commit_date": "2023-03-25T10:58:00Z",
        "message": "add code and data"
    },
    {
        "repo_url": "github.com/ntunlplab/traditional-chinese-alpaca",
        "filepath": "code/inference.py",
        "commit_date": "2023-03-25T10:58:00Z",
        "message": "add code and data"
    },
    {
        "repo_url": "github.com/tenstorrent/tt-buda",
        "filepath": "pybuda/test/falcon/tests/falcon_modules/falcon.py",
        "commit_date": "2024-02-12T13:08:05Z",
        "message": "Removing t-streaming redundant config overrides from the rest of PyBuda\n\n(cherry picked from commit 5527a0ecb7dd7045cf669dc61601503bb5ec373b)"
    },
    {
        "repo_url": "github.com/tenstorrent/tt-buda",
        "filepath": "pybuda/test/falcon/tests/falcon_modules/falcon.py",
        "commit_date": "2024-02-01T19:21:31Z",
        "message": "Snapshot of pybuda branch main, squashed from commit f955f2fdd3d4dfc1da7846ae80d10a8855988b5b\nNotice regarding the squashed initial commit:\n\nAs we transition from our internal development environment to a public\nrepository, we are committed to ensuring the security and\nconfidentiality of sensitive information.\n\nTo protect against any accidental release of confidential data, we have\nmade a specific decision regarding the initial commit to our public\nrepository. We have opted for a squashed commit for the initial public\nrelease, which consolidates all the changes from our internal history\ninto a single, clean commit. This step ensures that no sensitive\ninformation from our past development is exposed to the public.\n\nWe want to emphasize that moving forward, we are fully dedicated to\nmaintaining a transparent and organized version history for all our\nfuture commits and bug fixes. This approach allows us to maintain a\nclean slate for our public repository while ensuring that our ongoing\nwork is well-documented and traceable.\n\nWe appreciate your understanding and support as we make this transition.\nOur goal is to provide you with a secure and reliable open-source\nproject. If you have any questions or concerns, please don't hesitate to\nreach out to us.\n\nThank you for your continued interest and participation in our project."
    },
    {
        "repo_url": "github.com/IDEA-XL/InstructMol",
        "filepath": "llava/model/builder.py",
        "commit_date": "2023-11-27T08:01:50Z",
        "message": "support evaluate LLM mol-prop-pred"
    },
    {
        "repo_url": "github.com/IDEA-XL/InstructMol",
        "filepath": "llava/model/builder.py",
        "commit_date": "2023-11-07T13:19:29Z",
        "message": "support property prediction"
    },
    {
        "repo_url": "github.com/tabtoyou/KoLLaVA",
        "filepath": "llava/model/builder.py",
        "commit_date": "2023-12-02T18:18:52Z",
        "message": "Update KoLLaVA-v1.5"
    },
    {
        "repo_url": "github.com/tabtoyou/KoLLaVA",
        "filepath": "llava/model/builder.py",
        "commit_date": "2023-08-04T15:17:33Z",
        "message": "Update llama-2, qlora"
    },
    {
        "repo_url": "github.com/HIT-SCIR-SC/QiaoBan",
        "filepath": "interact.py",
        "commit_date": "2023-09-09T01:42:31Z",
        "message": "add interact.py"
    },
    {
        "repo_url": "github.com/KMnO4-zx/huanhuan-chat",
        "filepath": "run/api/model.py",
        "commit_date": "2023-08-23T11:06:09Z",
        "message": "Finish api \u90e8\u7f72"
    },
    {
        "repo_url": "github.com/KMnO4-zx/huanhuan-chat",
        "filepath": "run/gui/webui_demo.py",
        "commit_date": "2023-08-14T10:24:13Z",
        "message": "Finish notebook and gui"
    },
    {
        "repo_url": "github.com/KMnO4-zx/huanhuan-chat",
        "filepath": "run/gui/webui_demo.py",
        "commit_date": "2023-08-13T05:31:06Z",
        "message": "\u8c03\u6574\u76ee\u5f55\u67b6\u6784"
    },
    {
        "repo_url": "github.com/codefuse-ai/CodeFuse-MFT-VLM",
        "filepath": "llava/model/builder.py",
        "commit_date": "2024-03-01T09:37:13Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/codefuse-ai/CodeFuse-MFT-VLM",
        "filepath": "llava/model/builder.py",
        "commit_date": "2024-01-29T08:23:45Z",
        "message": "opensource"
    },
    {
        "repo_url": "github.com/Sshuoshuo/RAG-competition",
        "filepath": "app/LLM.py",
        "commit_date": "2024-01-06T08:36:22Z",
        "message": "add submit code"
    },
    {
        "repo_url": "github.com/Sshuoshuo/RAG-competition",
        "filepath": "app/embeddings.py",
        "commit_date": "2024-01-06T08:36:22Z",
        "message": "add submit code"
    },
    {
        "repo_url": "github.com/williamliujl/LLMRec",
        "filepath": "generate.py",
        "commit_date": "2023-08-19T04:34:04Z",
        "message": "version_0819"
    },
    {
        "repo_url": "github.com/williamliujl/LLMRec",
        "filepath": "infer_rec.py",
        "commit_date": "2023-08-19T04:34:04Z",
        "message": "version_0819"
    },
    {
        "repo_url": "github.com/declare-lab/flacuna",
        "filepath": "peft_flacuna/peft_model.py",
        "commit_date": "2023-07-06T10:20:57Z",
        "message": "Upload files"
    },
    {
        "repo_url": "github.com/declare-lab/flacuna",
        "filepath": "peft_flacuna/peft_model.py",
        "commit_date": "2023-07-06T10:17:11Z",
        "message": "Upload files"
    },
    {
        "repo_url": "github.com/declare-lab/flacuna",
        "filepath": "peft_flacuna/peft_model.py",
        "commit_date": "2023-07-06T10:06:21Z",
        "message": "Upload files"
    },
    {
        "repo_url": "github.com/declare-lab/flacuna",
        "filepath": "fastchat/train/train_lora.py",
        "commit_date": "2023-07-06T10:20:57Z",
        "message": "Upload files"
    },
    {
        "repo_url": "github.com/declare-lab/flacuna",
        "filepath": "fastchat/train/train_lora.py",
        "commit_date": "2023-07-06T10:17:11Z",
        "message": "Upload files"
    },
    {
        "repo_url": "github.com/declare-lab/flacuna",
        "filepath": "fastchat/train/train_lora.py",
        "commit_date": "2023-07-06T10:06:21Z",
        "message": "Upload files"
    },
    {
        "repo_url": "github.com/declare-lab/flacuna",
        "filepath": "fastchat/model/apply_lora.py",
        "commit_date": "2023-07-06T10:20:57Z",
        "message": "Upload files"
    },
    {
        "repo_url": "github.com/declare-lab/flacuna",
        "filepath": "fastchat/model/apply_lora.py",
        "commit_date": "2023-07-06T10:17:11Z",
        "message": "Upload files"
    },
    {
        "repo_url": "github.com/declare-lab/flacuna",
        "filepath": "fastchat/model/apply_lora.py",
        "commit_date": "2023-07-06T10:06:21Z",
        "message": "Upload files"
    },
    {
        "repo_url": "github.com/declare-lab/flacuna",
        "filepath": "peft_flacuna/tuners/prompt_tuning.py",
        "commit_date": "2023-07-06T10:20:57Z",
        "message": "Upload files"
    },
    {
        "repo_url": "github.com/declare-lab/flacuna",
        "filepath": "peft_flacuna/tuners/prompt_tuning.py",
        "commit_date": "2023-07-06T10:17:11Z",
        "message": "Upload files"
    },
    {
        "repo_url": "github.com/declare-lab/flacuna",
        "filepath": "peft_flacuna/tuners/prompt_tuning.py",
        "commit_date": "2023-07-06T10:06:21Z",
        "message": "Upload files"
    },
    {
        "repo_url": "github.com/tingxueronghua/ChartLlama-code",
        "filepath": "model_vqa_lora.py",
        "commit_date": "2023-11-27T11:37:26Z",
        "message": "add inference code"
    },
    {
        "repo_url": "github.com/tingxueronghua/ChartLlama-code",
        "filepath": "llava/model/builder.py",
        "commit_date": "2023-12-04T03:05:39Z",
        "message": "add full llava code"
    },
    {
        "repo_url": "github.com/tingxueronghua/ChartLlama-code",
        "filepath": "llava/eval/model_vqa_lora.py",
        "commit_date": "2023-12-04T03:05:39Z",
        "message": "add full llava code"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "alpaca_lora/generate.py",
        "commit_date": "2023-04-02T03:35:40Z",
        "message": "automl"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "alpaca_lora/generate.py",
        "commit_date": "2023-03-31T16:49:15Z",
        "message": "my alpaca colab"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "alpaca_lora/export_hf_checkpoint.py",
        "commit_date": "2023-03-31T16:49:15Z",
        "message": "my alpaca colab"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_llama.py",
        "commit_date": "2023-04-08T03:11:24Z",
        "message": "modify inference to support sampling"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_llama.py",
        "commit_date": "2023-04-07T07:06:09Z",
        "message": "autodl"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_llama.py",
        "commit_date": "2023-04-07T07:00:21Z",
        "message": "autodl"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_llama.py",
        "commit_date": "2023-04-06T09:14:06Z",
        "message": "modify inference to support sampling"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_llama.py",
        "commit_date": "2023-04-05T14:06:49Z",
        "message": "modify inference to support sampling"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_llama.py",
        "commit_date": "2023-04-05T07:03:44Z",
        "message": "modify llama inference"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_llama.py",
        "commit_date": "2023-04-05T07:02:26Z",
        "message": "modify llama inference"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_llama.py",
        "commit_date": "2023-04-05T06:49:19Z",
        "message": "modify llama inference"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_llama.py",
        "commit_date": "2023-04-02T07:37:35Z",
        "message": "autodl"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_llama.py",
        "commit_date": "2023-04-02T04:12:30Z",
        "message": "autodl"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_llama.py",
        "commit_date": "2023-04-02T04:07:49Z",
        "message": "autodl"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_llama.py",
        "commit_date": "2023-04-02T03:40:53Z",
        "message": "automl"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_llama.py",
        "commit_date": "2023-04-02T03:35:40Z",
        "message": "automl"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_alpaca_lora.py",
        "commit_date": "2023-04-08T03:11:24Z",
        "message": "modify inference to support sampling"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_alpaca_lora.py",
        "commit_date": "2023-04-07T04:52:27Z",
        "message": "autodl"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_alpaca_lora.py",
        "commit_date": "2023-04-05T14:06:49Z",
        "message": "modify inference to support sampling"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_alpaca_lora.py",
        "commit_date": "2023-04-02T07:37:35Z",
        "message": "autodl"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_alpaca_lora.py",
        "commit_date": "2023-04-02T04:12:30Z",
        "message": "autodl"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_alpaca_lora.py",
        "commit_date": "2023-04-02T04:07:49Z",
        "message": "autodl"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_alpaca_lora.py",
        "commit_date": "2023-04-02T03:40:53Z",
        "message": "automl"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_alpaca_lora.py",
        "commit_date": "2023-04-02T03:35:40Z",
        "message": "automl"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_llama_gradio.py",
        "commit_date": "2023-04-06T09:14:06Z",
        "message": "modify inference to support sampling"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_llama_gradio.py",
        "commit_date": "2023-04-05T14:06:49Z",
        "message": "modify inference to support sampling"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "alpaca_lora/export_state_dict_checkpoint.py",
        "commit_date": "2023-03-31T16:49:15Z",
        "message": "my alpaca colab"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_alpaca_lora_gradio.py",
        "commit_date": "2023-04-08T03:11:24Z",
        "message": "modify inference to support sampling"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_alpaca_lora_gradio.py",
        "commit_date": "2023-04-07T05:15:19Z",
        "message": "autodl"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_alpaca_lora_gradio.py",
        "commit_date": "2023-04-07T05:08:14Z",
        "message": "autodl"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_alpaca_lora_gradio.py",
        "commit_date": "2023-04-07T05:00:00Z",
        "message": "autodl"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_alpaca_lora_gradio.py",
        "commit_date": "2023-04-07T03:05:14Z",
        "message": "autodl"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_alpaca_lora_gradio.py",
        "commit_date": "2023-04-06T09:14:06Z",
        "message": "modify inference to support sampling"
    },
    {
        "repo_url": "github.com/l294265421/my-alpaca",
        "filepath": "my_alpaca/autodl/inference_alpaca_lora_gradio.py",
        "commit_date": "2023-04-05T14:06:49Z",
        "message": "modify inference to support sampling"
    },
    {
        "repo_url": "github.com/mbzuai-nlp/bactrian-x",
        "filepath": "generate.py",
        "commit_date": "2023-05-12T07:08:36Z",
        "message": "add bloom model, support hf args, output examples"
    },
    {
        "repo_url": "github.com/mbzuai-nlp/bactrian-x",
        "filepath": "generate.py",
        "commit_date": "2023-05-02T17:04:13Z",
        "message": "support bloom, use short template, better hyper-parameter,  bug fix"
    },
    {
        "repo_url": "github.com/mbzuai-nlp/bactrian-x",
        "filepath": "generate.py",
        "commit_date": "2023-04-24T10:36:05Z",
        "message": "first plenty commit"
    },
    {
        "repo_url": "github.com/mbzuai-nlp/bactrian-x",
        "filepath": "get_lora_model_answer.py",
        "commit_date": "2023-05-12T07:08:36Z",
        "message": "add bloom model, support hf args, output examples"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-12-29T09:31:21Z",
        "message": "fix random seed in gpu environment"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-12-19T06:50:06Z",
        "message": "tweak tokenizing"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-12-15T09:27:42Z",
        "message": "refactoring"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-12-15T08:33:32Z",
        "message": "tweak"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-12-15T00:48:29Z",
        "message": "structure model answers"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-12-15T00:30:02Z",
        "message": "save config as well as result"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-12-14T08:05:41Z",
        "message": "use load_questions"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-12-14T07:29:49Z",
        "message": "declare paths in common.py"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-12-14T02:38:03Z",
        "message": "tweak"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-12-14T01:59:07Z",
        "message": "apply isort"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-28T01:38:25Z",
        "message": "use bfloat16 if available"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-27T05:58:44Z",
        "message": "specify file encoding"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-27T00:48:09Z",
        "message": "fix argument name"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-24T05:31:01Z",
        "message": "use logger instead of print"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-24T05:15:30Z",
        "message": "refactoring"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-22T11:00:19Z",
        "message": "load model configurations from file"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-22T10:11:21Z",
        "message": "fix postprocessing for llm-jp models"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-22T09:58:36Z",
        "message": "fix postprocessing for rinna models"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-22T09:24:31Z",
        "message": "set available GPUs via CUDA_VISIBLE_DEVICES"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-22T09:23:28Z",
        "message": "move argument parsing to main"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-22T09:22:17Z",
        "message": "remove template"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-22T09:20:35Z",
        "message": "simplify data saving"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-22T09:17:35Z",
        "message": "remove unnecessary alies"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-22T09:15:15Z",
        "message": "pass max_new_tokens to generate_response"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-22T09:10:54Z",
        "message": "remove with_prompt option as prompt will be explicitly specified in config file"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-22T09:06:23Z",
        "message": "move the logic to determine temperature to main"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-22T09:03:20Z",
        "message": "wrap only generation process with torch.no_grad"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-22T09:02:21Z",
        "message": "simplify data loading"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-22T08:56:01Z",
        "message": "remove interactive mode"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-22T08:51:26Z",
        "message": "enable eval mode"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-22T08:49:46Z",
        "message": "simplify the peft model initialization"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-22T08:47:00Z",
        "message": "remove print functions to report vocabulary size"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-22T08:45:24Z",
        "message": "remove llama-specific initialization"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-22T08:44:43Z",
        "message": "make tokenizer initalization clear"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-22T08:39:35Z",
        "message": "set appropriate dtype depending on the device"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-22T07:20:03Z",
        "message": "set very small temperature instead of 0 to avoid error"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-22T07:17:55Z",
        "message": "fix random seed"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-22T03:15:51Z",
        "message": "Merge branch 'dev' into refactor"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-21T05:25:07Z",
        "message": "fix the format"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-21T04:09:36Z",
        "message": "Merge remote-tracking branch 'origin/dev' into fix/useless_file"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-21T03:34:45Z",
        "message": "delete more useless files"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-20T07:12:08Z",
        "message": "fix argparse"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-20T02:59:52Z",
        "message": "remove unnecessary lines"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-17T07:59:07Z",
        "message": "fix file gen_model_answer.py"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-17T07:32:31Z",
        "message": "update log language"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-15T15:22:56Z",
        "message": "update llm-jp"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-11T09:25:39Z",
        "message": "support more models"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-10T08:59:03Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/ku-nlp/ja-vicuna-qa-benchmark",
        "filepath": "llm_judge/gen_model_answer.py",
        "commit_date": "2023-11-10T08:21:59Z",
        "message": "optimize"
    },
    {
        "repo_url": "github.com/FreedomIntelligence/GrammarGPT",
        "filepath": "generate.py",
        "commit_date": "2023-07-25T03:17:55Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/jyLin8100/GenSAM",
        "filepath": "llava/model/builder.py",
        "commit_date": "2023-12-11T21:54:53Z",
        "message": "GenSAMv1"
    },
    {
        "repo_url": "github.com/google/maxdiffusion",
        "filepath": "src/maxdiffusion/models/modeling_utils.py",
        "commit_date": "2024-02-16T17:59:56Z",
        "message": "maxdiffusion first commit"
    },
    {
        "repo_url": "github.com/MaverickRen/PixelLM",
        "filepath": "model/llava/model/builder.py",
        "commit_date": "2024-01-20T05:22:01Z",
        "message": "des"
    },
    {
        "repo_url": "github.com/sehyunkwon/ICTC",
        "filepath": "step1/llava/model/builder.py",
        "commit_date": "2023-10-28T01:45:27Z",
        "message": "First uploading"
    },
    {
        "repo_url": "github.com/jackaduma/ChatGLM-LoRA-RLHF-PyTorch",
        "filepath": "merge_peft_adapter.py",
        "commit_date": "2023-04-18T17:21:02Z",
        "message": "merge_peft_adapter"
    },
    {
        "repo_url": "github.com/Dai-shen/LAiW",
        "filepath": "src/interface.py",
        "commit_date": "2023-09-05T07:43:48Z",
        "message": "add tasks including legal_ar"
    },
    {
        "repo_url": "github.com/Dai-shen/LAiW",
        "filepath": "src/financial-evaluation/lm_eval/models/huggingface.py",
        "commit_date": "2024-01-02T08:33:50Z",
        "message": "update eval files"
    },
    {
        "repo_url": "github.com/Dai-shen/LAiW",
        "filepath": "src/financial-evaluation/lm_eval/models/huggingface.py",
        "commit_date": "2023-10-08T01:44:00Z",
        "message": "update src"
    },
    {
        "repo_url": "github.com/Dai-shen/LAiW",
        "filepath": "src/financial-evaluation/lm_eval/models/huggingface.py",
        "commit_date": "2023-09-05T07:42:33Z",
        "message": "add financial-evaluation"
    },
    {
        "repo_url": "github.com/ohtaman/abci-examples",
        "filepath": "202310/src/sft.py",
        "commit_date": "2023-10-22T23:26:49Z",
        "message": "Add 202310"
    },
    {
        "repo_url": "github.com/ohtaman/abci-examples",
        "filepath": "202307/src/finetune.py",
        "commit_date": "2023-07-06T04:47:18Z",
        "message": "Update finetune.py"
    },
    {
        "repo_url": "github.com/ohtaman/abci-examples",
        "filepath": "202307/src/finetune.py",
        "commit_date": "2023-07-06T01:05:45Z",
        "message": "black"
    },
    {
        "repo_url": "github.com/ohtaman/abci-examples",
        "filepath": "202307/src/finetune.py",
        "commit_date": "2023-07-05T19:18:07Z",
        "message": "update scripts"
    },
    {
        "repo_url": "github.com/ohtaman/abci-examples",
        "filepath": "202307/src/finetune.py",
        "commit_date": "2023-07-03T22:28:15Z",
        "message": "add finetune with lora."
    },
    {
        "repo_url": "github.com/ohtaman/abci-examples",
        "filepath": "202307/src/finetune.py",
        "commit_date": "2023-07-03T21:29:41Z",
        "message": "add finetune example for 202307."
    },
    {
        "repo_url": "github.com/ohtaman/abci-examples",
        "filepath": "202307/src/finetune_lora.py",
        "commit_date": "2023-07-06T04:47:35Z",
        "message": "Update finetune_lora.py"
    },
    {
        "repo_url": "github.com/ohtaman/abci-examples",
        "filepath": "202307/src/finetune_lora.py",
        "commit_date": "2023-07-06T01:05:45Z",
        "message": "black"
    },
    {
        "repo_url": "github.com/ohtaman/abci-examples",
        "filepath": "202307/src/finetune_lora.py",
        "commit_date": "2023-07-05T19:18:07Z",
        "message": "update scripts"
    },
    {
        "repo_url": "github.com/ohtaman/abci-examples",
        "filepath": "202307/src/finetune_lora.py",
        "commit_date": "2023-07-05T19:14:51Z",
        "message": "update script"
    },
    {
        "repo_url": "github.com/ohtaman/abci-examples",
        "filepath": "202307/src/finetune_lora.py",
        "commit_date": "2023-07-05T03:35:34Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/ohtaman/abci-examples",
        "filepath": "202307/src/finetune_lora.py",
        "commit_date": "2023-07-03T22:28:15Z",
        "message": "add finetune with lora."
    },
    {
        "repo_url": "github.com/ohtaman/abci-examples",
        "filepath": "202310/src/finetune_lora.py",
        "commit_date": "2023-10-23T01:46:56Z",
        "message": "Update scripts."
    },
    {
        "repo_url": "github.com/ohtaman/abci-examples",
        "filepath": "202310/src/finetune_lora.py",
        "commit_date": "2023-10-22T23:26:49Z",
        "message": "Add 202310"
    },
    {
        "repo_url": "github.com/ohtaman/abci-examples",
        "filepath": "202310/src/generate_text_lora.py",
        "commit_date": "2023-10-23T07:41:08Z",
        "message": "Add generate_text_lora"
    },
    {
        "repo_url": "github.com/ohtaman/abci-examples",
        "filepath": "202307/src/finetune_lora_distribute.py",
        "commit_date": "2023-07-21T21:18:43Z",
        "message": "change to be able to save peft model even if zero3 is used"
    },
    {
        "repo_url": "github.com/ohtaman/abci-examples",
        "filepath": "202307/src/finetune_lora_distribute.py",
        "commit_date": "2023-07-08T08:44:30Z",
        "message": "add multinode example"
    },
    {
        "repo_url": "github.com/ohtaman/abci-examples",
        "filepath": "202307/src/finetune_lora_distribute.py",
        "commit_date": "2023-07-06T01:05:45Z",
        "message": "black"
    },
    {
        "repo_url": "github.com/ohtaman/abci-examples",
        "filepath": "202307/src/finetune_lora_distribute.py",
        "commit_date": "2023-07-05T19:18:07Z",
        "message": "update scripts"
    },
    {
        "repo_url": "github.com/ohtaman/abci-examples",
        "filepath": "202307/src/finetune_lora_distribute.py",
        "commit_date": "2023-07-05T19:14:51Z",
        "message": "update script"
    },
    {
        "repo_url": "github.com/ohtaman/abci-examples",
        "filepath": "202307/src/finetune_lora_distribute.py",
        "commit_date": "2023-07-05T19:02:01Z",
        "message": "tmp"
    },
    {
        "repo_url": "github.com/ohtaman/abci-examples",
        "filepath": "202307/src/finetune_lora_distribute.py",
        "commit_date": "2023-07-05T03:27:02Z",
        "message": "add deepspeed"
    },
    {
        "repo_url": "github.com/ohtaman/abci-examples",
        "filepath": "202310/src/finetune_lora_distribute.py",
        "commit_date": "2023-10-23T01:46:56Z",
        "message": "Update scripts."
    },
    {
        "repo_url": "github.com/ohtaman/abci-examples",
        "filepath": "202310/src/finetune_lora_distribute.py",
        "commit_date": "2023-10-22T23:26:49Z",
        "message": "Add 202310"
    },
    {
        "repo_url": "github.com/ExpertiseModel/MuTAP",
        "filepath": "llama_util/model_utils.py",
        "commit_date": "2024-01-17T16:07:47Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/WangRongsheng/Chinese-LLaMA-Alpaca-Usage",
        "filepath": "scripts/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-05-15T11:32:49Z",
        "message": "add merge_llama_with_chinese_lora usage"
    },
    {
        "repo_url": "github.com/WangRongsheng/Chinese-LLaMA-Alpaca-Usage",
        "filepath": "scripts/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-05-15T10:14:34Z",
        "message": "add usage"
    },
    {
        "repo_url": "github.com/taishan1994/qlora-chinese-LLM",
        "filepath": "chat.py",
        "commit_date": "2023-05-27T08:48:47Z",
        "message": "commit"
    },
    {
        "repo_url": "github.com/taishan1994/qlora-chinese-LLM",
        "filepath": "model_hub/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-05-27T09:56:13Z",
        "message": "commit"
    },
    {
        "repo_url": "github.com/git-cloner/llama-lora-fine-tuning",
        "filepath": "generate.py",
        "commit_date": "2023-07-28T15:27:31Z",
        "message": "fiexed generate.py"
    },
    {
        "repo_url": "github.com/git-cloner/llama-lora-fine-tuning",
        "filepath": "generate.py",
        "commit_date": "2023-06-08T15:00:29Z",
        "message": "add test model"
    },
    {
        "repo_url": "github.com/songyingxin/Novel-GPT",
        "filepath": "Baichuan-Finetune-Lora/run_one.py",
        "commit_date": "2023-10-09T07:48:54Z",
        "message": "10-9"
    },
    {
        "repo_url": "github.com/AGI-Collective/Robin",
        "filepath": "robin/model/builder.py",
        "commit_date": "2023-12-01T01:52:18Z",
        "message": "Updated llava to robin"
    },
    {
        "repo_url": "github.com/j-min/VPGen",
        "filepath": "llama.py",
        "commit_date": "2023-07-25T05:05:30Z",
        "message": "minor refactor"
    },
    {
        "repo_url": "github.com/j-min/VPGen",
        "filepath": "llama.py",
        "commit_date": "2023-05-25T00:14:56Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/SUFE-AIFLM-Lab/FinEval",
        "filepath": "code/evaluators/unify_evaluator.py",
        "commit_date": "2023-08-13T07:21:45Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/microsoft/windows-ai-studio-templates",
        "filepath": "configs/phi-2/inference/utils.py",
        "commit_date": "2023-12-13T19:40:26Z",
        "message": "Adding base only inferencing (#87)"
    },
    {
        "repo_url": "github.com/microsoft/windows-ai-studio-templates",
        "filepath": "configs/phi-2/inference/utils.py",
        "commit_date": "2023-12-13T03:00:30Z",
        "message": "Remove DataParallel for preview version"
    },
    {
        "repo_url": "github.com/microsoft/windows-ai-studio-templates",
        "filepath": "configs/phi-2/inference/utils.py",
        "commit_date": "2023-12-12T17:32:22Z",
        "message": "Hello Templates!"
    },
    {
        "repo_url": "github.com/microsoft/windows-ai-studio-templates",
        "filepath": "configs/phi-1_5/inference/utils.py",
        "commit_date": "2023-12-13T19:40:26Z",
        "message": "Adding base only inferencing (#87)"
    },
    {
        "repo_url": "github.com/microsoft/windows-ai-studio-templates",
        "filepath": "configs/phi-1_5/inference/utils.py",
        "commit_date": "2023-12-13T03:00:30Z",
        "message": "Remove DataParallel for preview version"
    },
    {
        "repo_url": "github.com/microsoft/windows-ai-studio-templates",
        "filepath": "configs/phi-1_5/inference/utils.py",
        "commit_date": "2023-12-12T17:32:22Z",
        "message": "Hello Templates!"
    },
    {
        "repo_url": "github.com/microsoft/windows-ai-studio-templates",
        "filepath": "configs/mistral-7b/inference/utils.py",
        "commit_date": "2023-12-13T19:40:26Z",
        "message": "Adding base only inferencing (#87)"
    },
    {
        "repo_url": "github.com/microsoft/windows-ai-studio-templates",
        "filepath": "configs/mistral-7b/inference/utils.py",
        "commit_date": "2023-12-13T03:00:30Z",
        "message": "Remove DataParallel for preview version"
    },
    {
        "repo_url": "github.com/microsoft/windows-ai-studio-templates",
        "filepath": "configs/mistral-7b/inference/utils.py",
        "commit_date": "2023-12-12T17:32:22Z",
        "message": "Hello Templates!"
    },
    {
        "repo_url": "github.com/microsoft/windows-ai-studio-templates",
        "filepath": "configs/llama-v2-7b/inference/utils.py",
        "commit_date": "2023-12-13T19:40:26Z",
        "message": "Adding base only inferencing (#87)"
    },
    {
        "repo_url": "github.com/microsoft/windows-ai-studio-templates",
        "filepath": "configs/llama-v2-7b/inference/utils.py",
        "commit_date": "2023-12-13T03:00:30Z",
        "message": "Remove DataParallel for preview version"
    },
    {
        "repo_url": "github.com/microsoft/windows-ai-studio-templates",
        "filepath": "configs/llama-v2-7b/inference/utils.py",
        "commit_date": "2023-12-12T17:32:22Z",
        "message": "Hello Templates!"
    },
    {
        "repo_url": "github.com/microsoft/windows-ai-studio-templates",
        "filepath": "configs/zephyr-7b-beta/inference/utils.py",
        "commit_date": "2023-12-13T19:40:26Z",
        "message": "Adding base only inferencing (#87)"
    },
    {
        "repo_url": "github.com/microsoft/windows-ai-studio-templates",
        "filepath": "configs/zephyr-7b-beta/inference/utils.py",
        "commit_date": "2023-12-13T03:00:30Z",
        "message": "Remove DataParallel for preview version"
    },
    {
        "repo_url": "github.com/microsoft/windows-ai-studio-templates",
        "filepath": "configs/zephyr-7b-beta/inference/utils.py",
        "commit_date": "2023-12-12T17:32:22Z",
        "message": "Hello Templates!"
    },
    {
        "repo_url": "github.com/079035/WizardLM-mirror",
        "filepath": "training/src/generate.py",
        "commit_date": "2023-10-10T20:26:49Z",
        "message": "create WizardLM"
    },
    {
        "repo_url": "github.com/079035/WizardLM-mirror",
        "filepath": "WizardLM/src/infer_wizardlm13b.py",
        "commit_date": "2023-10-10T20:26:49Z",
        "message": "create WizardLM"
    },
    {
        "repo_url": "github.com/SqueezeAILab/KVQuant",
        "filepath": "gradients/src/transformers/modeling_utils.py",
        "commit_date": "2024-02-06T09:26:27Z",
        "message": "First Commit"
    },
    {
        "repo_url": "github.com/SqueezeAILab/KVQuant",
        "filepath": "gradients/src/transformers/integrations/peft.py",
        "commit_date": "2024-02-06T09:26:27Z",
        "message": "First Commit"
    },
    {
        "repo_url": "github.com/SqueezeAILab/KVQuant",
        "filepath": "deployment/transformers/src/transformers/trainer.py",
        "commit_date": "2024-02-16T20:56:11Z",
        "message": "Deployment Support"
    },
    {
        "repo_url": "github.com/SqueezeAILab/KVQuant",
        "filepath": "deployment/transformers/src/transformers/modeling_utils.py",
        "commit_date": "2024-02-16T20:56:11Z",
        "message": "Deployment Support"
    },
    {
        "repo_url": "github.com/SqueezeAILab/KVQuant",
        "filepath": "deployment/transformers/src/transformers/integrations/peft.py",
        "commit_date": "2024-02-16T20:56:11Z",
        "message": "Deployment Support"
    },
    {
        "repo_url": "github.com/Ascend/ModelZoo-PyTorch",
        "filepath": "PyTorch/built-in/foundation/FlagAI/flagai/model/base_model.py",
        "commit_date": "2023-08-18T02:02:56Z",
        "message": "!5367 [\u81ea\u7814][PyTorch][FlagAI Aquila Model]\u521d\u6b21\u63d0\u4ea4\n* original FlagAI Aquila"
    },
    {
        "repo_url": "github.com/Ascend/ModelZoo-PyTorch",
        "filepath": "PyTorch/built-in/foundation/LLaMA-13B/fastchat/model/apply_lora.py",
        "commit_date": "2023-11-24T07:15:55Z",
        "message": "!5865 \u66f4\u65b0\u65b0\u7248\u672cLLaMA-13B(fsdp\u8bad\u7ec3)\nMerge pull request !5865 from zhangbin/master"
    },
    {
        "repo_url": "github.com/Ascend/ModelZoo-PyTorch",
        "filepath": "PyTorch/built-in/foundation/LLaMA-13B/transformers_modify/trainer.py",
        "commit_date": "2023-11-24T07:15:55Z",
        "message": "!5865 \u66f4\u65b0\u65b0\u7248\u672cLLaMA-13B(fsdp\u8bad\u7ec3)\nMerge pull request !5865 from zhangbin/master"
    },
    {
        "repo_url": "github.com/Ascend/ModelZoo-PyTorch",
        "filepath": "PyTorch/built-in/foundation/LLaMA-13B/transformers_modify/trainer.py",
        "commit_date": "2023-08-25T09:27:31Z",
        "message": "fix bugs while doing inference, update README.md"
    },
    {
        "repo_url": "github.com/Ascend/ModelZoo-PyTorch",
        "filepath": "PyTorch/built-in/foundation/LLaMA-13B/transformers_modify/trainer.py",
        "commit_date": "2023-07-05T06:48:30Z",
        "message": "Add modifications to adapt NPU devices"
    },
    {
        "repo_url": "github.com/Ascend/ModelZoo-PyTorch",
        "filepath": "PyTorch/built-in/foundation/LLaMA-13B/fastchat/model/model_adapter.py",
        "commit_date": "2023-11-24T07:15:55Z",
        "message": "!5865 \u66f4\u65b0\u65b0\u7248\u672cLLaMA-13B(fsdp\u8bad\u7ec3)\nMerge pull request !5865 from zhangbin/master"
    },
    {
        "repo_url": "github.com/Ascend/ModelZoo-PyTorch",
        "filepath": "PyTorch/built-in/foundation/FlagAI/flagai/model/tools/peft/peft_model.py",
        "commit_date": "2023-08-18T02:02:56Z",
        "message": "!5367 [\u81ea\u7814][PyTorch][FlagAI Aquila Model]\u521d\u6b21\u63d0\u4ea4\n* original FlagAI Aquila"
    },
    {
        "repo_url": "github.com/Ascend/ModelZoo-PyTorch",
        "filepath": "PyTorch/built-in/foundation/FlagAI/flagai/model/tools/peft/tuners/prompt_tuning.py",
        "commit_date": "2023-08-18T02:02:56Z",
        "message": "!5367 [\u81ea\u7814][PyTorch][FlagAI Aquila Model]\u521d\u6b21\u63d0\u4ea4\n* original FlagAI Aquila"
    },
    {
        "repo_url": "github.com/adamkarvonen/chess_gpt_eval",
        "filepath": "llama/llama_module.py",
        "commit_date": "2024-01-21T15:06:26Z",
        "message": "Move llama_module into separate folder"
    },
    {
        "repo_url": "github.com/anyscale/endpoint-cookbook",
        "filepath": "App_FireAct/models/llama.py",
        "commit_date": "2023-10-27T03:33:24Z",
        "message": "FireAct"
    },
    {
        "repo_url": "github.com/suu990901/LLaMA-InfoEntropy-Loss",
        "filepath": "transformers/trainer.py",
        "commit_date": "2023-11-04T09:18:48Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/suu990901/LLaMA-InfoEntropy-Loss",
        "filepath": "lm-evaluation-harness/lm_eval/models/huggingface.py",
        "commit_date": "2023-11-04T09:18:48Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/ankur56/ChemLoRA",
        "filepath": "old_scripts/peft_test1_g4mp2.py",
        "commit_date": "2023-04-19T00:40:35Z",
        "message": "moved files"
    },
    {
        "repo_url": "github.com/ankur56/ChemLoRA",
        "filepath": "old_scripts/peft_test1_b3lyp.py",
        "commit_date": "2023-04-19T00:40:35Z",
        "message": "moved files"
    },
    {
        "repo_url": "github.com/yangjianxin1/LongQLoRA",
        "filepath": "component/utils.py",
        "commit_date": "2023-10-26T16:27:01Z",
        "message": "update loading model"
    },
    {
        "repo_url": "github.com/yangjianxin1/LongQLoRA",
        "filepath": "component/utils.py",
        "commit_date": "2023-10-26T16:21:56Z",
        "message": "add evaluation"
    },
    {
        "repo_url": "github.com/yangjianxin1/LongQLoRA",
        "filepath": "component/utils.py",
        "commit_date": "2023-10-24T03:29:42Z",
        "message": "init components"
    },
    {
        "repo_url": "github.com/yangjianxin1/LongQLoRA",
        "filepath": "script/merge_lora.py",
        "commit_date": "2023-11-07T10:49:00Z",
        "message": "update merge_lora.py"
    },
    {
        "repo_url": "github.com/yangjianxin1/LongQLoRA",
        "filepath": "script/merge_lora.py",
        "commit_date": "2023-10-26T16:37:07Z",
        "message": "merge model params"
    },
    {
        "repo_url": "github.com/Gentopia-AI/Gentopia",
        "filepath": "gentopia/llm/loaders/bloom.py",
        "commit_date": "2023-06-16T14:32:36Z",
        "message": "Refine llm.loaders (#12)\n\n* Refine llm.loaders\n\n* fix bug\n\n* fix bug\n\n---------\n\nCo-authored-by: Satan <liuxk2019@mail.sustech.edu.cn>"
    },
    {
        "repo_url": "github.com/Gentopia-AI/Gentopia",
        "filepath": "gentopia/llm/loaders/bloom.py",
        "commit_date": "2023-06-16T08:15:02Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/Gentopia-AI/Gentopia",
        "filepath": "gentopia/llm/loaders/bloom.py",
        "commit_date": "2023-06-16T00:43:44Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/Gentopia-AI/Gentopia",
        "filepath": "gentopia/llm/loaders/baize.py",
        "commit_date": "2023-06-16T14:32:36Z",
        "message": "Refine llm.loaders (#12)\n\n* Refine llm.loaders\n\n* fix bug\n\n* fix bug\n\n---------\n\nCo-authored-by: Satan <liuxk2019@mail.sustech.edu.cn>"
    },
    {
        "repo_url": "github.com/Gentopia-AI/Gentopia",
        "filepath": "gentopia/llm/loaders/baize.py",
        "commit_date": "2023-06-16T08:15:02Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/Gentopia-AI/Gentopia",
        "filepath": "gentopia/llm/loaders/baize.py",
        "commit_date": "2023-06-16T00:43:44Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/Gentopia-AI/Gentopia",
        "filepath": "gentopia/llm/loaders/alpaca.py",
        "commit_date": "2023-06-16T14:32:36Z",
        "message": "Refine llm.loaders (#12)\n\n* Refine llm.loaders\n\n* fix bug\n\n* fix bug\n\n---------\n\nCo-authored-by: Satan <liuxk2019@mail.sustech.edu.cn>"
    },
    {
        "repo_url": "github.com/Gentopia-AI/Gentopia",
        "filepath": "gentopia/llm/loaders/alpaca.py",
        "commit_date": "2023-06-16T08:15:02Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/Gentopia-AI/Gentopia",
        "filepath": "gentopia/llm/loaders/alpaca.py",
        "commit_date": "2023-06-16T00:43:44Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/Gentopia-AI/Gentopia",
        "filepath": "gentopia/llm/loaders/replit.py",
        "commit_date": "2023-06-16T08:15:02Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/Gentopia-AI/Gentopia",
        "filepath": "gentopia/llm/loaders/replit.py",
        "commit_date": "2023-06-16T00:43:44Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/Gentopia-AI/Gentopia",
        "filepath": "gentopia/llm/loaders/guanaco.py",
        "commit_date": "2023-06-16T14:32:36Z",
        "message": "Refine llm.loaders (#12)\n\n* Refine llm.loaders\n\n* fix bug\n\n* fix bug\n\n---------\n\nCo-authored-by: Satan <liuxk2019@mail.sustech.edu.cn>"
    },
    {
        "repo_url": "github.com/Gentopia-AI/Gentopia",
        "filepath": "gentopia/llm/loaders/guanaco.py",
        "commit_date": "2023-06-16T08:15:02Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/Gentopia-AI/Gentopia",
        "filepath": "gentopia/llm/loaders/guanaco.py",
        "commit_date": "2023-06-16T00:43:44Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/HillZhang1999/ICD",
        "filepath": "transformers/src/transformers/trainer.py",
        "commit_date": "2023-12-23T08:01:08Z",
        "message": "add transformers"
    },
    {
        "repo_url": "github.com/HillZhang1999/ICD",
        "filepath": "transformers/src/transformers/modeling_utils.py",
        "commit_date": "2023-12-23T08:01:08Z",
        "message": "add transformers"
    },
    {
        "repo_url": "github.com/HillZhang1999/ICD",
        "filepath": "transformers/src/transformers/integrations/peft.py",
        "commit_date": "2023-12-23T08:01:08Z",
        "message": "add transformers"
    },
    {
        "repo_url": "github.com/clcarwin/alpaca-weight",
        "filepath": "generate_llama.py",
        "commit_date": "2023-03-15T13:03:03Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/clcarwin/alpaca-weight",
        "filepath": "generate_alpaca.py",
        "commit_date": "2023-03-15T13:03:03Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/clcarwin/alpaca-weight",
        "filepath": "merge_step1_lora2patch.py",
        "commit_date": "2023-06-13T07:52:05Z",
        "message": "Updates transformers library and fixes missing Seq2SeqTrainerArguments"
    },
    {
        "repo_url": "github.com/clcarwin/alpaca-weight",
        "filepath": "merge_step1_lora2patch.py",
        "commit_date": "2023-03-15T13:03:03Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/clcarwin/alpaca-weight",
        "filepath": "generate_llama_with_lora.py",
        "commit_date": "2023-03-15T13:03:03Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/clcarwin/alpaca-weight",
        "filepath": "merge_step2_patch2alpaca.py",
        "commit_date": "2023-06-13T07:52:05Z",
        "message": "Updates transformers library and fixes missing Seq2SeqTrainerArguments"
    },
    {
        "repo_url": "github.com/clcarwin/alpaca-weight",
        "filepath": "merge_step2_patch2alpaca.py",
        "commit_date": "2023-03-15T13:03:03Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/AlpinDale/RPTQ-for-LLaMA",
        "filepath": "lm-evaluation/lm_eval/models/huggingface.py",
        "commit_date": "2023-05-18T22:27:34Z",
        "message": "moved from main"
    },
    {
        "repo_url": "github.com/AlpinDale/RPTQ-for-LLaMA",
        "filepath": "lm_evaluation/lm_eval/models/huggingface.py",
        "commit_date": "2023-05-04T10:02:06Z",
        "message": "added the repo as a fork"
    },
    {
        "repo_url": "github.com/AlpinDale/RPTQ-for-LLaMA",
        "filepath": "lm_evaluation/lm_eval/models/huggingface.py",
        "commit_date": "2023-04-03T04:02:17Z",
        "message": "code release"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-05-23T22:34:37Z",
        "message": "Lora llama deepspeed (#12)\n\n* working tensorized peft training & prediction\n\n* I have found and slain the memory leak"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-04-16T19:31:33Z",
        "message": "Update tensorize (#8)\n\n* working fast tensorization + testing code for posterity"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-04-08T01:06:05Z",
        "message": "a few quality of life modifications"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-04-07T23:14:21Z",
        "message": "whitespace"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-04-07T23:05:15Z",
        "message": "fixing default"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-04-07T22:50:48Z",
        "message": "working tensorized llama training"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-04-07T00:24:57Z",
        "message": "working llama fine-tune"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-04-06T03:34:56Z",
        "message": "actually working multi-gpu training"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-04-05T20:08:04Z",
        "message": "fixed streaming"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-04-05T05:14:03Z",
        "message": "black & isort"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-04-05T05:10:04Z",
        "message": "tensorizer test"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-04-05T04:54:31Z",
        "message": "use builtin function to remove </s>"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-04-05T04:54:31Z",
        "message": "porting daanelson output streaming work\n\n- this is a port from @daanelson work in replicate/cog-llama to T5\nhttps://github.com/replicate/cog-llama/pull/4/files\n\nThis patch is for base weights, not tensorized weights"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-04-04T06:10:51Z",
        "message": "working multi-gpu training"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-04-03T00:10:03Z",
        "message": "reworked template"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-04-02T20:31:26Z",
        "message": "now with ignoring"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-03-31T23:54:52Z",
        "message": "refactoring to use cog weights"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-03-31T19:46:26Z",
        "message": "hopeful fix to path issues"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-03-31T00:31:54Z",
        "message": "fixed predict"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-03-31T00:30:07Z",
        "message": "working tensorization"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-03-30T23:45:50Z",
        "message": "initial attempt at tensorization"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-03-30T21:23:53Z",
        "message": "prepping for tensorization of model"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-03-30T18:22:07Z",
        "message": "working train"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-03-30T05:56:40Z",
        "message": "working generic train and predict scripts"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-03-29T22:58:32Z",
        "message": "working, scrappy train -> predict handoff"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-03-29T22:00:09Z",
        "message": "wip reformatting"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-03-28T22:02:19Z",
        "message": "working training"
    },
    {
        "repo_url": "github.com/replicate/cog-llama",
        "filepath": "predict.py",
        "commit_date": "2023-03-27T22:33:33Z",
        "message": "not quite working"
    },
    {
        "repo_url": "github.com/Manuel030/alpaca-opt",
        "filepath": "inference.py",
        "commit_date": "2023-04-06T17:29:11Z",
        "message": "prepare readme"
    },
    {
        "repo_url": "github.com/Manuel030/alpaca-opt",
        "filepath": "inference.py",
        "commit_date": "2023-04-06T16:52:11Z",
        "message": "use 6.7 version"
    },
    {
        "repo_url": "github.com/Manuel030/alpaca-opt",
        "filepath": "inference.py",
        "commit_date": "2023-04-04T16:12:49Z",
        "message": "add cpu support and weight mixin"
    },
    {
        "repo_url": "github.com/Manuel030/alpaca-opt",
        "filepath": "inference.py",
        "commit_date": "2023-04-03T15:37:36Z",
        "message": "change inference parameters"
    },
    {
        "repo_url": "github.com/Manuel030/alpaca-opt",
        "filepath": "inference.py",
        "commit_date": "2023-03-26T19:40:20Z",
        "message": "run training for opt 6.7b"
    },
    {
        "repo_url": "github.com/Manuel030/alpaca-opt",
        "filepath": "inference.py",
        "commit_date": "2023-03-16T16:49:04Z",
        "message": "gitignore model outputs"
    },
    {
        "repo_url": "github.com/Manuel030/alpaca-opt",
        "filepath": "mixin_lora_weights.py",
        "commit_date": "2023-04-06T17:29:11Z",
        "message": "prepare readme"
    },
    {
        "repo_url": "github.com/Manuel030/alpaca-opt",
        "filepath": "mixin_lora_weights.py",
        "commit_date": "2023-04-06T16:52:11Z",
        "message": "use 6.7 version"
    },
    {
        "repo_url": "github.com/Manuel030/alpaca-opt",
        "filepath": "mixin_lora_weights.py",
        "commit_date": "2023-04-06T15:55:42Z",
        "message": "support push tu hub"
    },
    {
        "repo_url": "github.com/Manuel030/alpaca-opt",
        "filepath": "mixin_lora_weights.py",
        "commit_date": "2023-04-04T16:12:49Z",
        "message": "add cpu support and weight mixin"
    },
    {
        "repo_url": "github.com/intel/neural-speed",
        "filepath": "scripts/huggingface.py",
        "commit_date": "2024-02-05T02:12:09Z",
        "message": "add autoround and remove name in path (#112)"
    },
    {
        "repo_url": "github.com/intel/neural-speed",
        "filepath": "scripts/huggingface.py",
        "commit_date": "2024-02-04T08:38:52Z",
        "message": "enable pre-commit CI (#113)"
    },
    {
        "repo_url": "github.com/intel/neural-speed",
        "filepath": "scripts/huggingface.py",
        "commit_date": "2024-01-19T02:30:12Z",
        "message": "miagrate pr: Calculate accuracy of runtime (#46)"
    },
    {
        "repo_url": "github.com/intel/neural-speed",
        "filepath": "scripts/load_peft_and_merge.py",
        "commit_date": "2023-12-27T06:39:56Z",
        "message": "Update python API and reorg scripts (#16)"
    },
    {
        "repo_url": "github.com/xiaol/Huggingface-RWKV-World",
        "filepath": "generate_hf_cfg.py",
        "commit_date": "2023-07-15T18:08:01Z",
        "message": "fix equation"
    },
    {
        "repo_url": "github.com/xiaol/Huggingface-RWKV-World",
        "filepath": "generate_hf_cfg.py",
        "commit_date": "2023-07-15T17:13:52Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/xiaol/Huggingface-RWKV-World",
        "filepath": "generate_hf_cfg.py",
        "commit_date": "2023-07-15T15:10:02Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/xiaol/Huggingface-RWKV-World",
        "filepath": "generate_hf_cfg.py",
        "commit_date": "2023-07-15T12:57:58Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/xiaol/Huggingface-RWKV-World",
        "filepath": "generate_hf_cfg.py",
        "commit_date": "2023-07-15T11:48:24Z",
        "message": "fix bugs"
    },
    {
        "repo_url": "github.com/eval4nlp/SharedTask2023",
        "filepath": "baselines/model_dict.py",
        "commit_date": "2023-08-28T10:41:06Z",
        "message": "add additional model selection"
    },
    {
        "repo_url": "github.com/eval4nlp/SharedTask2023",
        "filepath": "baselines/model_dict.py",
        "commit_date": "2023-08-21T15:11:57Z",
        "message": "base_name property no longer required for TheBloke models"
    },
    {
        "repo_url": "github.com/eval4nlp/SharedTask2023",
        "filepath": "baselines/model_dict.py",
        "commit_date": "2023-08-07T20:55:42Z",
        "message": "tested baselines"
    },
    {
        "repo_url": "github.com/eval4nlp/SharedTask2023",
        "filepath": "baselines/model_dict.py",
        "commit_date": "2023-08-07T17:52:40Z",
        "message": "adding another baseline"
    },
    {
        "repo_url": "github.com/DataTunerX/datatunerx",
        "filepath": "cmd/tuning/train.py",
        "commit_date": "2024-02-05T03:42:24Z",
        "message": "fix(datatunerx): fix go.mod\n\nchange finetune-experiment-controller to datatunerx"
    },
    {
        "repo_url": "github.com/yao8839836/kg-llm",
        "filepath": "lora_infer_wn11.py",
        "commit_date": "2023-06-24T15:19:01Z",
        "message": "update readme"
    },
    {
        "repo_url": "github.com/yao8839836/kg-llm",
        "filepath": "lora_infer_wn11.py",
        "commit_date": "2023-06-24T15:07:47Z",
        "message": "update readme"
    },
    {
        "repo_url": "github.com/yao8839836/kg-llm",
        "filepath": "lora_infer_wn11.py",
        "commit_date": "2023-06-24T14:53:42Z",
        "message": "add Readme"
    },
    {
        "repo_url": "github.com/yao8839836/kg-llm",
        "filepath": "lora_infer_yago_rel.py",
        "commit_date": "2023-06-24T15:19:01Z",
        "message": "update readme"
    },
    {
        "repo_url": "github.com/yao8839836/kg-llm",
        "filepath": "lora_infer_yago_rel.py",
        "commit_date": "2023-06-24T14:53:42Z",
        "message": "add Readme"
    },
    {
        "repo_url": "github.com/eth-easl/fmengine",
        "filepath": "cli/generate.py",
        "commit_date": "2023-12-12T09:44:08Z",
        "message": "formatting"
    },
    {
        "repo_url": "github.com/eth-easl/fmengine",
        "filepath": "cli/generate.py",
        "commit_date": "2023-12-11T20:40:42Z",
        "message": "Feat/usability (#24)\n\n* add utils to upload model to HF\n\n* minor\n\n* add peft utils\n\n* lora on all layers\n\n* minor\n\n* fix lora converting\n\n* feat: converts all steps\n\n* minor\n\n* initial peft cli\n\n* peft environments\n\n* minor\n\n* minor"
    },
    {
        "repo_url": "github.com/Actable-AI/llm-utils",
        "filepath": "qlora2ct2/convert_qlora2_ct2.py",
        "commit_date": "2023-06-27T07:15:03Z",
        "message": "add convert code"
    },
    {
        "repo_url": "github.com/Actable-AI/llm-utils",
        "filepath": "qlora2ct2/convert_qlora2_ct2.py",
        "commit_date": "2023-06-27T07:14:10Z",
        "message": "re commit"
    },
    {
        "repo_url": "github.com/Actable-AI/llm-utils",
        "filepath": "qlora2ct2/convert_qlora2_ct2.py",
        "commit_date": "2023-06-27T07:00:47Z",
        "message": "push convert qlora to ct2 code"
    },
    {
        "repo_url": "github.com/Actable-AI/llm-utils",
        "filepath": "qlora2ct2/create_dummy_qlora_model.py",
        "commit_date": "2023-06-27T07:15:03Z",
        "message": "add convert code"
    },
    {
        "repo_url": "github.com/Actable-AI/llm-utils",
        "filepath": "qlora2ct2/create_dummy_qlora_model.py",
        "commit_date": "2023-06-27T07:14:10Z",
        "message": "re commit"
    },
    {
        "repo_url": "github.com/Actable-AI/llm-utils",
        "filepath": "qlora2ct2/create_dummy_qlora_model.py",
        "commit_date": "2023-06-27T07:00:47Z",
        "message": "push convert qlora to ct2 code"
    },
    {
        "repo_url": "github.com/wegodev2/virtual-prompt-injection",
        "filepath": "code_injection/evaluation/modeling.py",
        "commit_date": "2023-10-18T21:18:54Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/camenduru/Video-LLaVA-hf",
        "filepath": "llava/model/builder.py",
        "commit_date": "2023-11-18T05:51:50Z",
        "message": "m"
    },
    {
        "repo_url": "github.com/hengjiUSTC/learn-llm",
        "filepath": "merge_lora.py",
        "commit_date": "2024-01-12T09:05:14Z",
        "message": "add dpo"
    },
    {
        "repo_url": "github.com/hengjiUSTC/learn-llm",
        "filepath": "merge_lora.py",
        "commit_date": "2024-01-09T14:52:44Z",
        "message": "update tokenzier and model loader"
    },
    {
        "repo_url": "github.com/hengjiUSTC/learn-llm",
        "filepath": "merge_lora.py",
        "commit_date": "2024-01-09T09:55:21Z",
        "message": "fix inference and merge bugs"
    },
    {
        "repo_url": "github.com/hengjiUSTC/learn-llm",
        "filepath": "merge_lora.py",
        "commit_date": "2024-01-08T09:36:53Z",
        "message": "update merge for tokenizer"
    },
    {
        "repo_url": "github.com/hengjiUSTC/learn-llm",
        "filepath": "merge_lora.py",
        "commit_date": "2024-01-05T03:20:41Z",
        "message": "add lora merge and inference"
    },
    {
        "repo_url": "github.com/valohai/mistral-example",
        "filepath": "inference-mistral.py",
        "commit_date": "2023-11-20T12:08:49Z",
        "message": "Prepare for public (#5)\n\n* fix dataprep error / prompt prep in inference\n\n* undo changes in inference parser\n\n* Update data-preprocess self.data_path argument\n\n* Add os.path.dirname to data_path\n\n* data-preprocess: change load dataset\n\n* data-preprocess: change the prompt mr->target sentence\n\n* finetune-mistral: change hyperparams for trainer\n\n* inference-mistral: Make the whole prompt as parameter / Add postproc method\n\n* valohai.yaml: update environment for trial users\n\n* inference-mistral: delete duplication\n\n* Fix lint error\n\n---------\n\nCo-authored-by: sofiacharnota <sofiacharnota@gmail.com>\nCo-authored-by: Aarni Koskela <akx@iki.fi>"
    },
    {
        "repo_url": "github.com/valohai/mistral-example",
        "filepath": "inference-mistral.py",
        "commit_date": "2023-11-07T22:21:28Z",
        "message": "Additional fixes (#4)\n\n\nCo-authored-by: sofiacharnota <sofiacharnota@gmail.com>"
    },
    {
        "repo_url": "github.com/valohai/mistral-example",
        "filepath": "inference-mistral.py",
        "commit_date": "2023-11-06T11:37:18Z",
        "message": "inference-mistral: clean up and add some logging"
    },
    {
        "repo_url": "github.com/valohai/mistral-example",
        "filepath": "inference-mistral.py",
        "commit_date": "2023-11-06T11:37:18Z",
        "message": "inference-mistral: remove unused code"
    },
    {
        "repo_url": "github.com/valohai/mistral-example",
        "filepath": "inference-mistral.py",
        "commit_date": "2023-11-06T11:37:18Z",
        "message": "Wrap entry points in functions to avoid leaky globals"
    },
    {
        "repo_url": "github.com/valohai/mistral-example",
        "filepath": "inference-mistral.py",
        "commit_date": "2023-11-06T10:55:42Z",
        "message": "Apply code style, again"
    },
    {
        "repo_url": "github.com/valohai/mistral-example",
        "filepath": "inference-mistral.py",
        "commit_date": "2023-11-02T19:00:31Z",
        "message": "Prepare for public use / Test new docker image"
    },
    {
        "repo_url": "github.com/valohai/mistral-example",
        "filepath": "inference-mistral.py",
        "commit_date": "2023-11-02T10:15:02Z",
        "message": "Apply code style"
    },
    {
        "repo_url": "github.com/valohai/mistral-example",
        "filepath": "inference-mistral.py",
        "commit_date": "2023-10-31T14:34:35Z",
        "message": "Add readme \\ refactoring"
    },
    {
        "repo_url": "github.com/valohai/mistral-example",
        "filepath": "inference-mistral.py",
        "commit_date": "2023-10-31T11:16:12Z",
        "message": "minor"
    },
    {
        "repo_url": "github.com/FartyPants/Playground",
        "filepath": "script.py",
        "commit_date": "2024-01-05T05:28:42Z",
        "message": "Update script.py\n\nremoved exllama v1"
    },
    {
        "repo_url": "github.com/FartyPants/Playground",
        "filepath": "script.py",
        "commit_date": "2023-12-24T04:10:47Z",
        "message": "PEFT unload ? Does it actually work? Nope\n\nI'm pretty sure something is broken in PEFT unload because it keeps the adapters in memory"
    },
    {
        "repo_url": "github.com/FartyPants/Playground",
        "filepath": "script.py",
        "commit_date": "2023-10-24T03:07:09Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/FartyPants/Playground",
        "filepath": "script.py",
        "commit_date": "2023-10-09T17:35:21Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/FartyPants/Playground",
        "filepath": "script.py",
        "commit_date": "2023-10-06T05:29:41Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/FartyPants/Playground",
        "filepath": "script.py",
        "commit_date": "2023-09-29T20:49:11Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/FartyPants/Playground",
        "filepath": "script.py",
        "commit_date": "2023-09-19T21:24:27Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/FartyPants/Playground",
        "filepath": "script.py",
        "commit_date": "2023-09-18T06:53:35Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/FartyPants/Playground",
        "filepath": "script.py",
        "commit_date": "2023-08-27T14:40:11Z",
        "message": "Update script.py"
    },
    {
        "repo_url": "github.com/FartyPants/Playground",
        "filepath": "script.py",
        "commit_date": "2023-07-06T01:44:42Z",
        "message": "Added token view"
    },
    {
        "repo_url": "github.com/FartyPants/Playground",
        "filepath": "script.py",
        "commit_date": "2023-07-03T07:13:36Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/FartyPants/Playground",
        "filepath": "script.py",
        "commit_date": "2023-07-03T01:33:07Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/FartyPants/Playground",
        "filepath": "script.py",
        "commit_date": "2023-06-26T01:59:15Z",
        "message": "ABility to save note to Lora"
    },
    {
        "repo_url": "github.com/FartyPants/Playground",
        "filepath": "script.py",
        "commit_date": "2023-06-25T20:09:13Z",
        "message": "More fixes after last update"
    },
    {
        "repo_url": "github.com/FartyPants/Playground",
        "filepath": "script.py",
        "commit_date": "2023-06-25T19:06:50Z",
        "message": "Keeping up with oobabooga changes"
    },
    {
        "repo_url": "github.com/FartyPants/Playground",
        "filepath": "script.py",
        "commit_date": "2023-06-23T18:51:55Z",
        "message": "Added ability to scale LORA"
    },
    {
        "repo_url": "github.com/FartyPants/Playground",
        "filepath": "script.py",
        "commit_date": "2023-06-21T04:20:31Z",
        "message": "minor error"
    },
    {
        "repo_url": "github.com/FartyPants/Playground",
        "filepath": "script.py",
        "commit_date": "2023-06-20T18:25:15Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/FartyPants/Playground",
        "filepath": "script.py",
        "commit_date": "2023-06-14T17:53:49Z",
        "message": "Added LORA switch"
    },
    {
        "repo_url": "github.com/FartyPants/Playground",
        "filepath": "script.py",
        "commit_date": "2023-06-14T15:31:32Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/FartyPants/Playground",
        "filepath": "script.py",
        "commit_date": "2023-06-14T07:54:13Z",
        "message": "Perma Memory and Memory Limit"
    },
    {
        "repo_url": "github.com/FartyPants/Playground",
        "filepath": "script.py",
        "commit_date": "2023-06-10T02:30:02Z",
        "message": "a few changes, OpenAssistant prompt added"
    },
    {
        "repo_url": "github.com/FartyPants/Playground",
        "filepath": "script.py",
        "commit_date": "2023-06-09T19:52:20Z",
        "message": "The correct playground file"
    },
    {
        "repo_url": "github.com/FartyPants/Playground",
        "filepath": "script.py",
        "commit_date": "2023-06-09T06:58:06Z",
        "message": "Added generate from selection"
    },
    {
        "repo_url": "github.com/FartyPants/Playground",
        "filepath": "script.py",
        "commit_date": "2023-06-07T20:48:40Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/FartyPants/Playground",
        "filepath": "script.py",
        "commit_date": "2023-06-07T19:11:14Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/poteminr/instruct-ner",
        "filepath": "instruction_ner/inference_instruct.py",
        "commit_date": "2024-02-13T17:58:18Z",
        "message": "refactoring, add metrics for extended instructions"
    },
    {
        "repo_url": "github.com/poteminr/instruct-ner",
        "filepath": "instruction_ner/inference_instruct.py",
        "commit_date": "2024-02-09T13:28:23Z",
        "message": "refactoring, add extended instructions for conll2003 rudrec"
    },
    {
        "repo_url": "github.com/poteminr/instruct-ner",
        "filepath": "instruction_ner/inference_instruct.py",
        "commit_date": "2024-02-04T15:50:05Z",
        "message": "refactoring + add RWKV-LM (HF)"
    },
    {
        "repo_url": "github.com/poteminr/instruct-ner",
        "filepath": "instruction_ner/inference_instruct.py",
        "commit_date": "2024-01-07T14:10:31Z",
        "message": "add calculations of overall metrics"
    },
    {
        "repo_url": "github.com/poteminr/instruct-ner",
        "filepath": "instruction_ner/inference_instruct.py",
        "commit_date": "2023-12-27T18:16:06Z",
        "message": "update inferece script for multiconer2023"
    },
    {
        "repo_url": "github.com/poteminr/instruct-ner",
        "filepath": "instruction_ner/inference_instruct.py",
        "commit_date": "2023-12-23T18:05:07Z",
        "message": "add mistral to inference"
    },
    {
        "repo_url": "github.com/poteminr/instruct-ner",
        "filepath": "instruction_ner/inference_instruct.py",
        "commit_date": "2023-09-28T06:21:04Z",
        "message": "add conll2003 dataset"
    },
    {
        "repo_url": "github.com/poteminr/instruct-ner",
        "filepath": "instruction_ner/inference_instruct.py",
        "commit_date": "2023-09-18T17:12:48Z",
        "message": "add batch_size param for inference"
    },
    {
        "repo_url": "github.com/poteminr/instruct-ner",
        "filepath": "instruction_ner/inference_instruct.py",
        "commit_date": "2023-08-16T14:19:04Z",
        "message": "extend inference for new dataset"
    },
    {
        "repo_url": "github.com/poteminr/instruct-ner",
        "filepath": "instruction_ner/inference_instruct.py",
        "commit_date": "2023-08-15T16:03:41Z",
        "message": "fix inference scripts (temporarily only for rudrec dataset)"
    },
    {
        "repo_url": "github.com/poteminr/instruct-ner",
        "filepath": "instruction_ner/inference_instruct.py",
        "commit_date": "2023-08-13T12:12:35Z",
        "message": "fix import paths"
    },
    {
        "repo_url": "github.com/poteminr/instruct-ner",
        "filepath": "instruction_ner/inference_instruct.py",
        "commit_date": "2023-07-26T07:52:58Z",
        "message": "huge refactoring according to google pyguide, add compute metric for trainer"
    },
    {
        "repo_url": "github.com/poteminr/instruct-ner",
        "filepath": "instruction_ner/inference_instruct.py",
        "commit_date": "2023-07-22T16:01:39Z",
        "message": "add llama_cpp inference script"
    },
    {
        "repo_url": "github.com/poteminr/instruct-ner",
        "filepath": "instruction_ner/inference_instruct.py",
        "commit_date": "2023-07-22T09:55:22Z",
        "message": "refactor dataset and increase speed of llm inference"
    },
    {
        "repo_url": "github.com/poteminr/instruct-ner",
        "filepath": "instruction_ner/inference_instruct.py",
        "commit_date": "2023-07-21T16:24:02Z",
        "message": "small refactoring"
    },
    {
        "repo_url": "github.com/poteminr/instruct-ner",
        "filepath": "instruction_ner/inference_instruct.py",
        "commit_date": "2023-07-20T16:09:53Z",
        "message": "add code for f1 metric, update inference"
    },
    {
        "repo_url": "github.com/poteminr/instruct-ner",
        "filepath": "instruction_ner/inference_instruct.py",
        "commit_date": "2023-07-20T10:25:47Z",
        "message": "add extract_classes from string for f1 metric calculation"
    },
    {
        "repo_url": "github.com/poteminr/instruct-ner",
        "filepath": "instruction_ner/inference_instruct.py",
        "commit_date": "2023-07-20T09:48:25Z",
        "message": "update inference script and fix bug with max_instances"
    },
    {
        "repo_url": "github.com/YerayL/FinChina-SA",
        "filepath": "src/inference.py",
        "commit_date": "2023-08-12T06:04:28Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-11-24T06:10:52Z",
        "message": "Refactoring Trainer, adds `save_only_model` arg and simplifying FSDP integration (#27652)\n\n* add code changes\n\n1. Refactor FSDP\n2. Add `--save_only_model` option: When checkpointing, whether to only save the model, or also the optimizer, scheduler & rng state.\n3. Bump up the minimum `accelerate` version to `0.21.0`\n\n* quality\n\n* fix quality?\n\n* Revert \"fix quality?\"\n\nThis reverts commit 149330a6abc078827be274db84c8a2d26a76eba1.\n\n* fix fsdp doc strings\n\n* fix quality\n\n* Update src/transformers/training_args.py\n\nCo-authored-by: Zach Mueller <muellerzr@gmail.com>\n\n* please fix the quality issue \ud83d\ude05\n\n* Apply suggestions from code review\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* address comment\n\n* simplify conditional check as per the comment\n\n* update documentation\n\n---------\n\nCo-authored-by: Zach Mueller <muellerzr@gmail.com>\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-11-21T16:09:35Z",
        "message": "remove the deprecated method `init_git_repo` (#27617)\n\n* remove deprecated method `init_git_repo`\n\n* make style"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-11-14T20:31:04Z",
        "message": "Track the number of tokens seen to metrics (#27274)\n\n* Add tokens seen\n\n* Address comments, add to TrainingArgs\n\n* Update log\n\n* Apply suggestions from code review\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Use self.args\n\n* Fix docstring\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n---------\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-11-14T19:54:44Z",
        "message": "Have seq2seq just use gather (#27025)\n\n* Have seq2seq just use gather\n\n* Change\n\n* Reset after\n\n* Make slow\n\n* Apply suggestions from code review\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Clean\n\n* Simplify and just use gather\n\n* Update tests/trainer/test_trainer_seq2seq.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* gather always for seq2seq\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-11-07T21:40:00Z",
        "message": "Allow scheduler parameters (#26480)\n\n* Allow for scheduler kwargs\n\n* Formatting\n\n* Arguments checks, passing the tests\n\n* Black failed somehow\n\n---------\n\nCo-authored-by: Pierre <pierre@avatarin.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-11-02T20:08:03Z",
        "message": "Fixed base model class name extraction from PeftModels (#27162)\n\n* Fixed base model class name extraction from PeftModels\n\n* Changes to first unwrap the model then extract the base model name\n\n* Changed base_model to base_model.model to stay consistent with peft model abstractions"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-11-02T10:27:13Z",
        "message": "Reproducible checkpoint for npu (#27208)\n\n* save NPU's RNG states when saving a checkpoint and set after all the\ndata skip phase when resuming training.\n\n* re-trigger ci\n\n* re-trigger ci"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-11-01T18:42:38Z",
        "message": "Enable split_batches through TrainingArguments (#26798)\n\n* Enable split_batches through TrainingArguments\n\n* Extra dispatch_batches\n\n* Keep as default false\n\n* Add to docstring\n\n* Add to docstring\n\n* Remove the capturewarnings change\n\n* Comma"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-10-31T15:03:59Z",
        "message": "[FEAT] Add Neftune into transformers Trainer (#27141)\n\n* add v1 neftune\n\n* use `unwrap_model` instead\n\n* add test + docs\n\n* Apply suggestions from code review\n\nCo-authored-by: Zach Mueller <muellerzr@gmail.com>\n\n* more details\n\n* fixup\n\n* Update docs/source/en/main_classes/trainer.md\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* refactor a bit\n\n* more elaborated test\n\n* fix unwrap issue\n\n---------\n\nCo-authored-by: Zach Mueller <muellerzr@gmail.com>\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-10-30T11:55:03Z",
        "message": "remove the obsolete code related to fairscale FSDP (#26651)\n\n* remove the obsolete code related to fairscale FSDP\n\n* apple review suggestion"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-10-30T11:41:48Z",
        "message": "[`Trainer` / `GC`] Add `gradient_checkpointing_kwargs` in trainer and training arguments (#27068)\n\n* add `gradient_checkpointing_kwargs` in trainer and training arguments\n\n* add comment\n\n* add test - currently failing\n\n* now tests pass"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-10-26T16:13:19Z",
        "message": "Save TB logs as part of push_to_hub (#27022)\n\n* Support runs/\n\n* Upload runs folder as part of push to hub\n\n* Add a test\n\n* Add to test deps\n\n* Update with proposed solution from Slack\n\n* Ensure that repo gets deleted in tests"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-10-26T15:46:17Z",
        "message": "Correct docstrings and a typo in comments (#27047)\n\n* docs(training_args): correct docstrings\n\nCorrect docstrings of these methods in `TrainingArguments`:\n\n- `set_save`\n- `set_logging`\n\n* docs(training_args): adjust words in docstrings\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* docs(trainer): correct a typo in comments\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-10-26T09:20:11Z",
        "message": "Bring back `set_epoch` for Accelerate-based dataloaders (#26850)\n\n* Working tests!\n\n* Fix sampler\n\n* Fix\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Fix check\n\n* Clean\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-10-16T13:29:47Z",
        "message": "fix resume_from_checkpoint bug (#26739)\n\n* fix resume_from_checkpoint bug\n\n* update code"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-10-12T08:28:40Z",
        "message": "Add many missing spaces in adjacent strings (#26751)\n\nAdd missing spaces in adjacent strings"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-10-06T14:03:11Z",
        "message": "remove SharedDDP as it is deprecated (#25702)\n\n* remove SharedDDP as it was drepracated\n\n* apply review suggestion\n\n* make style\n\n* Oops,forgot to remove the compute_loss context manager in Seq2SeqTrainer.\n\n* remove the unnecessary conditional statement\n\n* keep the logic of IPEX\n\n* clean code\n\n* mix precision setup & make fixup\n\n---------\n\nCo-authored-by: statelesshz <jihuazhong1@huawei.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-10-04T13:13:37Z",
        "message": "Docstring check (#26052)\n\n* Fix number of minimal calls to the Hub with peft integration\n\n* Alternate design\n\n* And this way?\n\n* Revert\n\n* Nits to fix\n\n* Add util\n\n* Print when changes are made\n\n* Add list to ignore\n\n* Add more rules\n\n* Manual fixes\n\n* deal with kwargs\n\n* deal with enum defaults\n\n* avoid many digits for floats\n\n* Manual fixes\n\n* Fix regex\n\n* Fix regex\n\n* Auto fix\n\n* Style\n\n* Apply script\n\n* Add ignored list\n\n* Add check that templates are filled\n\n* Adding to CI checks\n\n* Add back semi-fix\n\n* Ignore more objects\n\n* More auto-fixes\n\n* Ignore missing objects\n\n* Remove temp semi-fix\n\n* Fixes\n\n* Update src/transformers/models/pvt/configuration_pvt.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update utils/check_docstrings.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update src/transformers/utils/quantization_config.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Deal with float defaults\n\n* Fix small defaults\n\n* Address review comment\n\n* Treat\n\n* Post-rebase cleanup\n\n* Address review comment\n\n* Update src/transformers/models/deprecated/mctct/configuration_mctct.py\n\nCo-authored-by: Lysandre Debut <lysandre.debut@reseau.eseo.fr>\n\n* Address review comment\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\nCo-authored-by: Lysandre Debut <lysandre.debut@reseau.eseo.fr>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-10-04T12:57:11Z",
        "message": "Extend Trainer to enable Ascend NPU to use the fused Adamw optimizer when training (#26194)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-26T17:27:09Z",
        "message": "Add torch `RMSProp` optimizer (#26425)\n\nadd rmsprop"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-22T01:33:29Z",
        "message": "[QUICK FIX LINK] Update trainer.py (#26293)\n\n* Update trainer.py\n\nFix link\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update trainer.py\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-20T15:38:59Z",
        "message": "[`Trainer`] Refactor trainer + bnb logic (#26248)\n\n* refactor trainer + bnb logic\n\n* remove logger.info\n\n* oops"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-20T04:56:16Z",
        "message": "FSDP tests and checkpointing fixes (#26180)\n\n* add fsdp tests\n\n* Update test_fsdp.py\n\n* Update test_fsdp.py\n\n* fixes\n\n* checks\n\n* Update trainer.py\n\n* fix\n\n* fixes for saving/resuming checkpoints\n\n* fixes\n\n* add tests and delete debug statements\n\n* fixing tests\n\n* Update test_fsdp.py\n\n* fix tests\n\n* fix tests\n\n* minor nits\n\n* fix code style and quality\n\n* refactor and modularize test code\n\n* reduce the time of tests\n\n* reduce the test time\n\n* fix test\n\n* reduce test time\n\n* reduce test time\n\n* fix failing tests\n\n* fix\n\n* Apply suggestions from code review\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* resolve comments\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-18T15:40:11Z",
        "message": "refactor decay_parameters production into its own function (#26152)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-14T09:57:47Z",
        "message": "Fix eval accumulation when `accelerate` > 0.20.3 (#26060)\n\nAs mentioned in: https://github.com/huggingface/transformers/issues/25641\n\nEval accumulation will never happen with `accelerate > 0.20.3`, so this change ensures that `sync_gradients` is ignored if accelerate is > 0.20.3"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-12T17:01:22Z",
        "message": "enable optuna multi-objectives feature (#25969)\n\n* enable optuna multi-objectives feature\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* update hpo doc\n\n* update docstring\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>\n\n* extend direction to List[str] type\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>\n\n* Update src/transformers/integrations/integration_utils.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-11T11:56:36Z",
        "message": "only main process should call _save on deepspeed zero3 (#25959)\n\nonly main process should call _save when deepspeed zero3"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-07T19:00:22Z",
        "message": "Try to fix training Loss inconsistent after resume from old checkpoint (#25872)\n\n* fix loss inconsistent after resume  #25340\n\n* fix typo\n\n* clean code\n\n* reformatted code\n\n* adjust code according to comments\n\n* adjust check_dataloader_randomsampler location\n\n* return sampler only\n\n* handle sampler is None\n\n* Update src/transformers/trainer_pt_utils.py\n\nthanks @amyeroberts\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n---------\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-07T16:17:30Z",
        "message": "Add `tgs` speed metrics (#25858)\n\n* Add tgs metrics\n\n* bugfix and black formatting\n\n* workaround for tokens counting\n\n* formating and bugfix\n\n* Fix\n\n* Add opt-in for tgs metrics\n\n* make style and fix error\n\n* Fix doc\n\n* fix docbuild\n\n* hf-doc-build\n\n* fix\n\n* test\n\n* Update src/transformers/training_args.py\n\nrenaming\n\nCo-authored-by: Zach Mueller <muellerzr@gmail.com>\n\n* Update src/transformers/training_args.py\n\nrenaming\n\nCo-authored-by: Zach Mueller <muellerzr@gmail.com>\n\n* Fix some symbol\n\n* test\n\n* Update src/transformers/trainer_utils.py\n\nmatch nameing patterns\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/training_args.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/trainer.py\n\nnice\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Fix reviews\n\n* Fix\n\n* Fix black\n\n---------\n\nCo-authored-by: Zach Mueller <muellerzr@gmail.com>\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-07T04:22:53Z",
        "message": "Fix err with FSDP (#25991)\n\n* Fix err\n\n* Use version check"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-05T17:01:20Z",
        "message": "deepspeed resume from ckpt fixes and adding support for deepspeed optimizer and HF scheduler (#25863)\n\n* Add support for deepspeed optimizer and HF scheduler\n\n* fix bug\n\n* fix the import\n\n* fix issue with deepspeed scheduler saving for hf optim + hf scheduler scenario\n\n* fix loading of hf scheduler when loading deepspeed checkpoint\n\n* fix import of `DeepSpeedSchedulerWrapper`\n\n* add tests\n\n* add the comment and skip the failing tests\n\n* address comment"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-01T15:24:12Z",
        "message": "Revert frozen training arguments (#25903)\n\n* Revert frozen training arguments\n\n* TODO"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-09-01T09:50:42Z",
        "message": "fix FSDP model resume optimizer & scheduler (#25852)\n\n* fix FSDP resume optimizer & scheduler\n\n* improve trainer code quality\n\n---------\n\nCo-authored-by: machi04 <machi04@meituan.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-31T09:47:53Z",
        "message": "fix ds z3 checkpointing when  `stage3_gather_16bit_weights_on_model_save=False` (#25817)\n\n* fix ds z3 checkpointing when  `stage3_gather_16bit_weights_on_model_save=False`\n\n* refactoring"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-29T14:06:41Z",
        "message": "Error with checking args.eval_accumulation_steps to gather tensors (#25819)\n\n* Update trainer.py (error with checking steps in args.eval_accumulation_steps to gather tensors)\n\nWhile the deprecated code has the correct check (line 3772): \n\"if args.eval_accumulation_steps is not None and (step + 1) % args.eval_accumulation_steps == 0:\"\n\nThe current code does not (line 3196):\n\"if args.eval_accumulation_steps is not None and self.accelerator.sync_gradients:\"\n\nWe need to check \"(step + 1) % args.eval_accumulation_steps == 0\". Hence, the line 3196 should be modified to:\n\"if args.eval_accumulation_steps is not None and (step + 1) % args.eval_accumulation_steps == 0 and self.accelerator.sync_gradients:\"\n\n* Fix error with checking args.eval_accumulation_steps to gather tensors"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-29T07:22:14Z",
        "message": "Arde/fsdp activation checkpointing (#25771)\n\n* add FSDP config option to enable activation-checkpointing\n\n* update docs\n\n* add checks and remove redundant code\n\n* fix formatting error"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-25T15:13:34Z",
        "message": "\ud83d\udea8\ud83d\udea8\ud83d\udea8 [`Refactor`] Move third-party related utility files into `integrations/` folder \ud83d\udea8\ud83d\udea8\ud83d\udea8 (#25599)\n\n* move deepspeed to `lib_integrations.deepspeed`\n\n* more refactor\n\n* oops\n\n* fix slow tests\n\n* Fix docs\n\n* fix docs\n\n* addess feedback\n\n* address feedback\n\n* final modifs for PEFT\n\n* fixup\n\n* ok now\n\n* trigger CI\n\n* trigger CI again\n\n* Update docs/source/en/main_classes/deepspeed.md\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* import from `integrations`\n\n* address feedback\n\n* revert removal of `deepspeed` module\n\n* revert removal of `deepspeed` module\n\n* fix conflicts\n\n* ooops\n\n* oops\n\n* add deprecation warning\n\n* place it on the top\n\n* put `FutureWarning`\n\n* fix conflicts with not_doctested.txt\n\n* add back `bitsandbytes` module with a depr warning\n\n* fix\n\n* fix\n\n* fixup\n\n* oops\n\n* fix doctests\n\n---------\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-18T17:08:03Z",
        "message": "[`PEFT`] Peft integration alternative design  (#25077)\n\n* a draft version\n\n* v2 integration\n\n* fix\n\n* make it more generic and works for IA3\n\n* add set adapter and multiple adapters support\n\n* fixup\n\n* adapt a bit\n\n* oops\n\n* oops\n\n* oops\n\n* adapt more\n\n* fix\n\n* add more refactor\n\n* now works with model class\n\n* change it to instance method as it causes issues with `jit`.\n\n* add CR\n\n* change method name\n\n* add `add_adapter` method\n\n* clean up\n\n* Update src/transformers/adapters/peft_mixin.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* add moe utils\n\n* fixup\n\n* Update src/transformers/adapters/peft_mixin.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* adapt\n\n* oops\n\n* fixup\n\n* add is_peft_available\n\n* remove `requires_backend`\n\n* trainer compatibility\n\n* fixup + docstring\n\n* more details\n\n* trigger CI\n\n* Apply suggestions from code review\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* Update src/transformers/modeling_utils.py\n\n* fixup + is_main_process\n\n* added `save_peft_format` in save_pretrained\n\n* up\n\n* fix nits here and there\n\n* nits here and there.\n\n* docs\n\n* revert `encoding=\"utf-8\"`\n\n* comment\n\n* added slow tests before the PEFT release.\n\n* fixup and nits\n\n* let's be on the safe zone\n\n* added more comments\n\n* v1 docs\n\n* add remaining docs\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* move to `lib_integrations`\n\n* fixup\n\n* this time fixup\n\n* Apply suggestions from code review\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* address final comments\n\n* refactor to use `token`\n\n* add PEFT to DockerFile for slow tests.\n\n* added pipeline support.\n\n---------\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-17T18:48:58Z",
        "message": "add warning for 8bit optimizers (#25575)\n\n* add warning for 8bit optimizers\n\n* protect import"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-17T16:23:34Z",
        "message": "add util for ram efficient loading of model when using fsdp (#25107)\n\n* add util for ram efficient loading of model when using fsdp\n\n* make fix-copies\n\n* fixes \ud83d\ude05\n\n* docs\n\n* making it further easier to use\n\n* rename the function\n\n* refactor to handle fsdp ram efficiency in `from_pretrained`\n\n* fixes\n\n* fixes\n\n* fixes\n\n* update\n\n* fixes\n\n* revert `load_pretrained_model_only_on_rank0`\n\n* resolve `load_from_checkpoint`"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-17T15:44:01Z",
        "message": "Revert \"change version (#25387)\" (#25573)\n\nThis reverts commit 3a05e010e0c7e8abd3e5357dd4e89e28cc69003e."
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-17T06:10:33Z",
        "message": "Update trainer.py (#25553)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-16T16:19:51Z",
        "message": "More frozen args (#25540)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-10T20:06:29Z",
        "message": "GPTQ integration (#25062)\n\n* GTPQ integration\n\n* Add tests for gptq\n\n* support for more quantization model\n\n* fix style\n\n* typo\n\n* fix method\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* add dataclass and fix quantization_method\n\n* fix doc\n\n* Update tests/quantization/gptq/test_gptq.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* modify dataclass\n\n* add gtpqconfig import\n\n* fix typo\n\n* fix tests\n\n* remove dataset as req arg\n\n* remove tokenizer import\n\n* add offload cpu quantization test\n\n* fix check dataset\n\n* modify dockerfile\n\n* protect trainer\n\n* style\n\n* test for config\n\n* add more log\n\n* overwrite torch_dtype\n\n* draft doc\n\n* modify quantization_config docstring\n\n* fix class name in docstring\n\n* Apply suggestions from code review\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* more warning\n\n* fix 8bit kwargs tests\n\n* peft compatibility\n\n* remove var\n\n* fix is_gptq_quantized\n\n* remove is_gptq_quantized\n\n* fix wrap\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* add exllama\n\n* skip test\n\n* overwrite float16\n\n* style\n\n* fix skip test\n\n* Apply suggestions from code review\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* fix docsting formatting\n\n* add doc\n\n* better test\n\n---------\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-10T15:07:32Z",
        "message": "Fix issue with ratio evaluation steps and auto find batch size (#25436)\n\n* Fully rebased solution\n\n* 500"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-09T07:31:24Z",
        "message": "rm useless condition since the previous condition contains it. (#25403)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-08T17:05:41Z",
        "message": "change version (#25387)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-07T15:47:22Z",
        "message": "Migrate Trainer from `Repository` to `upload_folder` (#25095)\n\n* First draft\n\n* Deal with progress bars\n\n* Update src/transformers/utils/hub.py\n\nCo-authored-by: Lucain <lucainp@gmail.com>\n\n* Address review comments\n\n* Forgot one\n\n* Pin hf_hub\n\n* Add argument for push all and fix tests\n\n* Fix tests\n\n* Address review comments\n\n---------\n\nCo-authored-by: Lucain <lucainp@gmail.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-08-02T07:29:00Z",
        "message": "Fix set of model parallel in the Trainer when no GPUs are available (#25239)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-07-28T09:40:08Z",
        "message": "Fix `.push_to_hub` and cleanup `get_full_repo_name` usage (#25120)\n\n* Fix .push_to_hub and cleanup get_full_repo_name usage\n\n* Do not rely on Python bool conversion magic\n\n* request changes"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-07-27T12:34:02Z",
        "message": "fix delete all checkpoints when save_total_limit is set to 1 (#25136)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-07-27T01:41:43Z",
        "message": "fix deepspeed load best model at end when the model gets sharded (#25057)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-07-24T14:53:10Z",
        "message": "compute_loss in trainer failing to label shift for PEFT model when label smoothing enabled. (#25044)\n\n* added PeftModelForCausalLM to MODEL_FOR_CAUSAL_LM_MAPPING_NAMES dict\n\n* check for PEFT model in compute_loss section\n\n---------\n\nCo-authored-by: Nathan Brake <nbrake3@mmm.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-07-24T13:27:19Z",
        "message": "Add dispatch_batches to training arguments (#25038)\n\n* Dispatch batches\n\n* Copy items"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-07-21T14:30:17Z",
        "message": "Use main_input_name for include_inputs_for_metrics (#24993)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-07-21T12:22:48Z",
        "message": "fsdp fixes and enhancements (#24980)\n\n* fix fsdp prepare to remove the warnings and fix excess memory usage\n\n* Update training_args.py\n\n* parity for FSDP+XLA\n\n* Update trainer.py"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-07-21T06:47:26Z",
        "message": "fix fsdp checkpointing issues (#24926)\n\n* fix fsdp load\n\n* Update trainer.py\n\n* remove saving duplicate state_dict"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-07-17T18:45:59Z",
        "message": "Remove deprecated codes (#24837)\n\n* remove `xpu_backend` training argument\n\n* always call `contextlib.nullcontext()` since transformers updated to\npython3.8\n\n* these codes will not be executed"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-07-12T15:47:21Z",
        "message": "Rm duplicate pad_across_processes (#24780)\n\nRm duplicate"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-07-12T14:01:51Z",
        "message": "Fix pad across processes dim in trainer and not being able to set the timeout (#24775)\n\n* dim, and rm copy\n\n* Don't rm copy for now\n\n* Oops\n\n* pad index\n\n* Should be a working test\n\n* Tickle down ddp timeout\n\n* Put fix back in now that testing locally is done\n\n* Better comment specifying timeout\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-07-12T09:49:12Z",
        "message": "Fix eval_accumulation_steps leading to incorrect metrics (#24756)\n\nFix eval steps"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-07-11T20:37:04Z",
        "message": "Fix lr scheduler not being reset on reruns (#24758)\n\n* Try this\n\n* Solved!\n\n* Rm extranious\n\n* Rm extranious\n\n* self\n\n* Args'\n\n* Check for if we created the lr scheduler\n\n* Move comment\n\n* Clean"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-07-11T15:21:29Z",
        "message": "Docs: add `kwargs` type to fix formatting (#24733)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-07-06T18:12:16Z",
        "message": "Fix integration with Accelerate and failing test (#24691)\n\nFix integration"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-07-06T09:33:25Z",
        "message": "DeepSpeed/FSDP ckpt saving utils fixes and FSDP training args fixes (#24591)\n\n* update ds and fsdp ckpt logic\n\n* refactoring\n\n* fix \ud83d\udc1b\n\n* resolve comment\n\n* fix issue with overriding of the fsdp config set by accelerate"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-29T18:37:44Z",
        "message": "fix peft ckpts not being pushed to hub  (#24578)\n\n* fix push to hub for peft ckpts\n\n* oops"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-27T17:28:26Z",
        "message": "Fix LR scheduler based on bs from auto bs finder (#24521)\n\n* One solution\n\n* args -> self"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-27T14:09:38Z",
        "message": "set model to training mode before accelerate.prepare (#24520)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-27T12:33:21Z",
        "message": "use accelerate autocast in jit eval path, since mix precision logic is\u2026 (#24460)\n\nuse accelerate autocast in jit eval path, since mix precision logic is in accelerator currently\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-26T17:38:29Z",
        "message": "Fix 'local_rank' AttiributeError in Trainer class (#24297)\n\nfix attribute error"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-26T12:15:37Z",
        "message": "deepspeed z1/z2 state dict fix (#24489)\n\n* deepspeed z2/z1 state_dict bloating fix\n\n* update\n\n* version check"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-26T12:07:27Z",
        "message": "when resume from peft checkpoint, the model should be trainable (#24463)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-23T12:33:57Z",
        "message": "fixes issue when saving fsdp via accelerate's FSDP plugin (#24446)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-23T12:13:07Z",
        "message": "fix the grad_acc issue at epoch boundaries (#24415)\n\n* fix the grad_acc issue at epoch boundaries\n\nCo-Authored-By: Zach Mueller <7831895+muellerzr@users.noreply.github.com>\n\n* add contributors.\n\nCo-authored-by: sumpster\n\n* address comments\n\n---------\n\nCo-authored-by: Zach Mueller <7831895+muellerzr@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-23T11:35:04Z",
        "message": "[`Trainer`] Fix `.to` call on 4bit models (#24444)\n\n* fix `.to` call on 4bit models\n\n* better check"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-22T18:46:20Z",
        "message": "Clarify batch size displayed when using DataParallel (#24430)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-22T18:28:25Z",
        "message": "Refactor hyperparameter search backends (#24384)\n\n* Refactor hyperparameter search backends\n\n* Simpler refactoring without abstract base class\n\n* black\n\n* review comments:\nspecify name in class\nuse methods instead of callable class attributes\nname constant better\n\n* review comments: safer bool checking, log multiple available backends\n\n* test ALL_HYPERPARAMETER_SEARCH_BACKENDS vs HPSearchBackend in unit test, not module. format with black.\n\n* copyright"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-21T11:24:41Z",
        "message": "[Trainer] Fix optimizer step on PyTorch TPU (#24389)\n\n* update optimizer step for tpu\n\n* add comment"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-20T11:57:08Z",
        "message": "Fix resuming PeftModel checkpoints in Trainer  (#24274)\n\n* Fix resuming checkpoints for PeftModels\n\nFix an error occurred when resuming a PeftModel from a training checkpoint. That was caused since PeftModel.pre_trained saves only adapter-related data while _load_from_checkpoint was expecting a torch sved model. This PR fix this issue and allows the adapter checkpoint to be loaded.\n\nResolves: #24252\n\n* fix last comment\n\n* fix nits\n\n---------\n\nCo-authored-by: younesbelkada <younesbelkada@gmail.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-16T19:14:03Z",
        "message": "Adding ddp_broadcast_buffers argument to Trainer (#24326)\n\nadding ddp_broadcast_buffers argument"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-16T14:38:23Z",
        "message": "Byebye pytorch 1.9 (#24080)\n\nbyebye\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-15T13:17:09Z",
        "message": "deepspeed init during eval fix (#24298)\n\n* deepspeed init during eval fix\n\n* commit suggestions\n\nCo-Authored-By: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-14T16:44:09Z",
        "message": "Clean up old Accelerate checks (#24279)\n\n* Clean up old Accelerate checks\n\n* Put back imports"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-13T19:19:15Z",
        "message": "update FSDP save and load logic (#24249)\n\n* update fsdp save and load logic\n\n* fix\n\n* see if this resolves the failing tests"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-12T17:26:17Z",
        "message": "Finish dataloader integration (#24201)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-12T15:23:37Z",
        "message": "\ud83d\udea8\ud83d\udea8\ud83d\udea8 Replace DataLoader logic for Accelerate in Trainer, remove unneeded tests \ud83d\udea8\ud83d\udea8\ud83d\udea8 (#24028)\n\n* Working integration\n\n* Fix failing test\n\n* Revert label host logic\n\n* Bring it back!"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-09T12:24:53Z",
        "message": "fix bugs with trainer (#24134)\n\n* fix the deepspeed test failures\n\n* apex fix\n\n* FSDP save ckpt fix\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-08T13:38:30Z",
        "message": "[`Trainer`] Correct behavior of `_load_best_model` for PEFT models (#24103)\n\n* v1\n\n* some refactor\n\n- add ST format as well\n\n* fix\n\n* add `ADAPTER_WEIGHTS_NAME` & `ADAPTER_SAFE_WEIGHTS_NAME`"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-07T19:33:13Z",
        "message": "fix accelerator prepare during eval only mode (#24014)\n\n* fix mixed precision prep during eval only mode\n\n* update to address comments\n\n* update to reflect the changes in accelerate"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-07T19:31:32Z",
        "message": "Do not prepare lr scheduler as it as the right number of steps (#24088)\n\n* Do not prepare lr scheduler as it as the right number of steps\n\n* Trigger CI\n\n* Trigger CI\n\n* Trigger CI\n\n* Add fake comment\n\n* Remove fake comment\n\n* Trigger CI please!"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-07T16:38:04Z",
        "message": "fix executable batch size issue (#24067)\n\n* fix executable batch size issue\n\n* fix\n\n* undo"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-07T12:30:55Z",
        "message": "Support PEFT models when saving the model using trainer (#24073)\n\n* support PEFT models when saving the model using trainer\n\n* fixup"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-05T12:28:10Z",
        "message": "fix trainer slow tests related to hyperparam search (#24011)\n\n* fix trainer slow tests\n\n* commit 2"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-06-02T12:53:48Z",
        "message": "Trainer: fixed evaluate raising `KeyError` for ReduceLROnPlateau (#23952)\n\nTrainer: fixed KeyError on evaluate for ReduceLROnPlateau\n\nCo-authored-by: Claudius Kienle <claudius.kienle@artiminds.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-05-31T17:34:55Z",
        "message": "remove the extra `accelerator.prepare`  (#23914)\n\nremove the extra `accelerator.prepare` that slipped in with multiple update from main \ud83d\ude05"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-05-31T11:54:26Z",
        "message": "Fix Trainer when model is loaded on a different GPU (#23792)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-05-31T09:46:22Z",
        "message": "accelerate deepspeed and gradient accumulation integrate (#23236)\n\n* mixed precision support via accelerate\n\n* fix issues\n\n* fix for the sharded ddp case\n\n* fix flax and tf failing tests\n\n* `refactor the place to create `Accelerator` object\n\n* move ddp prep to accelerate\n\n* fix \ud83d\ude05\n\n* resolving comments\n\n* move fsdp handling to accelerate\n\n* fixex\n\n* fix saving\n\n* shift torch dynamo handling to accelerate\n\n* shift deepspeed integration and save & load utils to accelerate\n\n* fix accelerate launcher support\n\n* oops\n\n* fix \ud83d\udc1b\n\n* save ckpt fix\n\n* Trigger CI\n\n* nasty \ud83d\udc1b \ud83d\ude05\n\n* as deepspeed needs grad_acc fixes, transfer grad_acc to accelerate\n\n* make tests happy\n\n* quality \u2728\n\n* loss tracked needs to account for grad_acc\n\n* fixing the deepspeed tests\n\n* quality \u2728\n\n* \ud83d\ude05\ud83d\ude05\ud83d\ude05\n\n* tests \ud83d\ude21\n\n* quality \u2728\n\n* Trigger CI\n\n* resolve comments and fix the issue with the previous merge from branch\n\n* Trigger CI\n\n* accelerate took over deepspeed integration\n\n---------\n\nCo-authored-by: Stas Bekman <stas@stason.org>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-05-31T09:38:20Z",
        "message": "Fix last instances of kbit -> quantized (#23797)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-05-31T09:12:07Z",
        "message": "shift torch dynamo handling to accelerate (#23168)\n\n* mixed precision support via accelerate\n\n* fix issues\n\n* fix for the sharded ddp case\n\n* fix flax and tf failing tests\n\n* `refactor the place to create `Accelerator` object\n\n* move ddp prep to accelerate\n\n* fix \ud83d\ude05\n\n* resolving comments\n\n* move fsdp handling to accelerate\n\n* fixex\n\n* fix saving\n\n* shift torch dynamo handling to accelerate"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/trainer.py",
        "commit_date": "2023-05-31T08:40:46Z",
        "message": "move fsdp handling to accelerate (#23158)\n\n* mixed precision support via accelerate\n\n* fix issues\n\n* fix for the sharded ddp case\n\n* fix flax and tf failing tests\n\n* `refactor the place to create `Accelerator` object\n\n* move ddp prep to accelerate\n\n* fix \ud83d\ude05\n\n* resolving comments\n\n* move fsdp handling to accelerate\n\n* fixex\n\n* fix saving"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-11-24T06:10:52Z",
        "message": "Refactoring Trainer, adds `save_only_model` arg and simplifying FSDP integration (#27652)\n\n* add code changes\n\n1. Refactor FSDP\n2. Add `--save_only_model` option: When checkpointing, whether to only save the model, or also the optimizer, scheduler & rng state.\n3. Bump up the minimum `accelerate` version to `0.21.0`\n\n* quality\n\n* fix quality?\n\n* Revert \"fix quality?\"\n\nThis reverts commit 149330a6abc078827be274db84c8a2d26a76eba1.\n\n* fix fsdp doc strings\n\n* fix quality\n\n* Update src/transformers/training_args.py\n\nCo-authored-by: Zach Mueller <muellerzr@gmail.com>\n\n* please fix the quality issue \ud83d\ude05\n\n* Apply suggestions from code review\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* address comment\n\n* simplify conditional check as per the comment\n\n* update documentation\n\n---------\n\nCo-authored-by: Zach Mueller <muellerzr@gmail.com>\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-11-21T17:51:48Z",
        "message": "Fix `resize_token_embeddings` (#26861) (#26865)\n\n* Fix `resize_token_embeddings` about `requires_grad`\n\nThe method `resize_token_embeddings` should keep `requires_grad`\nunchanged for all parameters in embeddings.\n\nPreviously, `resize_token_embeddings` always set `requires_grad`\nto `True`. After fixed, `resize_token_embeddings` copy the\n`requires_grad` attribute in the old embeddings."
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-11-21T10:03:30Z",
        "message": "[`core` / `gradient_checkpointing`] add support for old GC method (#27610)\n\n* add support for old GC method\n\n* add also disable\n\n* up\n\n* oops"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-11-20T15:45:55Z",
        "message": "[`FA-2`] Add fa2 support for `from_config` (#26914)\n\n* add fa2 support for from_config\n\n* Update test_modeling_common.py"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-11-16T16:43:19Z",
        "message": "[`Styling`] stylify using ruff (#27144)\n\n* try to stylify using ruff\n\n* might need to remove these changes?\n\n* use ruf format andruff check\n\n* use isinstance instead of type comparision\n\n* use # fmt: skip\n\n* use # fmt: skip\n\n* nits\n\n* soem styling changes\n\n* update ci job\n\n* nits isinstance\n\n* more files update\n\n* nits\n\n* more nits\n\n* small nits\n\n* check and format\n\n* revert wrong changes\n\n* actually use formatter instead of checker\n\n* nits\n\n* well docbuilder is overwriting this commit\n\n* revert notebook changes\n\n* try to nuke docbuilder\n\n* style\n\n* fix feature exrtaction test\n\n* remve `indent-width = 4`\n\n* fixup\n\n* more nits\n\n* update the ruff version that we use\n\n* style\n\n* nuke docbuilder styling\n\n* leve the print for detected changes\n\n* nits\n\n* Remove file I/O\n\nCo-authored-by: charliermarsh\n <charlie.r.marsh@gmail.com>\n\n* style\n\n* nits\n\n* revert notebook changes\n\n* Add # fmt skip when possible\n\n* Add # fmt skip when possible\n\n* Fix\n\n* More `  # fmt: skip` usage\n\n* More `  # fmt: skip` usage\n\n* More `  # fmt: skip` usage\n\n* NIts\n\n* more fixes\n\n* fix tapas\n\n* Another way to skip\n\n* Recommended way\n\n* Fix two more fiels\n\n* Remove asynch\nRemove asynch\n\n---------\n\nCo-authored-by: charliermarsh <charlie.r.marsh@gmail.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-11-16T15:35:40Z",
        "message": "Raise error when quantizing a quantized model (#27500)\n\nadd error msg"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-11-15T19:58:08Z",
        "message": "Fix offload disk for loading derivated model checkpoint into base model (#27253)\n\n* fix\n\n* style\n\n* add test"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-11-02T11:03:51Z",
        "message": "[`core` / `Quantization`] Fix for 8bit serialization tests (#27234)\n\n* fix for 8bit serialization\n\n* added regression tests.\n\n* fixup"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-11-01T18:25:23Z",
        "message": "Fix CPU offload + disk offload tests (#27204)\n\nFix disk offload tests + weight sharing issues"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-11-01T17:09:21Z",
        "message": "Add exllamav2 better (#27111)\n\n* add_ xllamav2 arg\n\n* add test\n\n* style\n\n* add check\n\n* add doc\n\n* replace by use_exllama_v2\n\n* fix tests\n\n* fix doc\n\n* style\n\n* better condition\n\n* fix logic\n\n* add deprecate msg\n\n* deprecate exllama\n\n* remove disable_exllama from the linter\n\n* remove\n\n* fix warning\n\n* Revert the commits deprecating exllama\n\n* deprecate disable_exllama for use_exllama\n\n* fix\n\n* fix loading attribute\n\n* better handling of args\n\n* remove disable_exllama from init and linter\n\n* Apply suggestions from code review\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* better arg\n\n* fix warning\n\n* Apply suggestions from code review\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* switch to dict\n\n* Apply suggestions from code review\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* style\n\n* nits\n\n* style\n\n* better tests\n\n* style\n\n---------\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-11-01T08:06:31Z",
        "message": "[`core` / `Quantization` ] AWQ integration (#27045)\n\n* working v1\n\n* oops\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* fixup\n\n* oops\n\n* push\n\n* more changes\n\n* add docs\n\n* some fixes\n\n* fix copies\n\n* add v1 doc\n\n* added installation guide\n\n* relax constraints\n\n* revert\n\n* attempt llm-awq\n\n* oops\n\n* oops\n\n* fixup\n\n* raise error when incorrect cuda compute capability\n\n* nit\n\n* add instructions for llm-awq\n\n* fixup\n\n* fix copies\n\n* fixup and docs\n\n* change\n\n* few changes + add demo\n\n* add v1 tests\n\n* add autoawq in dockerfile\n\n* finalize\n\n* Update tests/quantization/autoawq/test_awq.py\n\n* fix test\n\n* fix\n\n* fix issue\n\n* Update src/transformers/integrations/awq.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update docs/source/en/main_classes/quantization.md\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update docs/source/en/main_classes/quantization.md\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update src/transformers/integrations/awq.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update src/transformers/integrations/awq.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* add link to example script\n\n* Update docs/source/en/main_classes/quantization.md\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* add more content\n\n* add more details\n\n* add link to quantization docs\n\n* camel case + change backend class name\n\n* change to string\n\n* fixup\n\n* raise errors if libs not installed\n\n* change to `bits` and `group_size`\n\n* nit\n\n* nit\n\n* Apply suggestions from code review\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* disable training\n\n* address some comments and fix nits\n\n* fix\n\n* final nits and fix tests\n\n* adapt to our new runners\n\n* make fix-copies\n\n* Update src/transformers/utils/quantization_config.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/utils/quantization_config.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/integrations/awq.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/integrations/awq.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* move to top\n\n* add conversion test\n\n* final nit\n\n* add more elaborated test\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-31T18:16:49Z",
        "message": "Safetensors serialization by default (#27064)\n\n* Safetensors serialization by default\n\n* First pass on the tests\n\n* Second pass on the tests\n\n* Third pass on the tests\n\n* Fix TF weight loading from TF-format safetensors\n\n* Specific encoder-decoder fixes for weight crossloading\n\n* Add VisionEncoderDecoder fixes for TF too\n\n* Change filename test for pt-to-tf\n\n* One missing fix for TFVisionEncoderDecoder\n\n* Fix the other crossload test\n\n* Support for flax + updated tests\n\n* Apply suggestions from code review\n\nCo-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>\n\n* Sanchit's comments\n\n* Sanchit's comments 2\n\n* Nico's comments\n\n* Fix tests\n\n* cleanup\n\n* Apply suggestions from code review\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Matt <rocketknight1@gmail.com>\nCo-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-31T13:45:23Z",
        "message": "Add support for loading GPTQ models on CPU (#26719)\n\n* Add support for loading GPTQ models on CPU\n\nRight now, we can only load the GPTQ Quantized model on the CUDA\ndevice. The attribute `gptq_supports_cpu` checks if the current\nauto_gptq version is the one which has the cpu support for the\nmodel or not.\nThe larger variants of the model are hard to load/run/trace on\nthe GPU and that's the rationale behind adding this attribute.\n\nSigned-Off By: Vivek Khandelwal <vivek@nod-labs.com>\n\n* Update quantization.md\n\n* Update quantization.md\n\n* Update quantization.md"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-30T20:08:29Z",
        "message": "Fix import of torch.utils.checkpoint (#27155)\n\n* Fix import\n\n* Apply suggestions from code review\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-27T14:15:22Z",
        "message": "[`core`/ `gradient_checkpointing`] Refactor GC - part 2 (#27073)\n\n* fix\n\n* more fixes\n\n* fix other models\n\n* fix long t5\n\n* use `gradient_checkpointing_func` instead\n\n* fix copies\n\n* set `gradient_checkpointing_func` as a private attribute and retrieve previous behaviour\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* replace it with `is_gradient_checkpointing_set`\n\n* remove default\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* fixup\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-27T13:49:20Z",
        "message": "Fix no split modules underlying modules (#27090)\n\n* fix no split\n\n* style\n\n* remove comm\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* rename modules\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-27T12:32:54Z",
        "message": "Provide alternative when warning on use_auth_token (#27105)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-27T09:23:06Z",
        "message": "Revert \"add exllamav2 arg\" (#27102)\n\nRevert \"add exllamav2 arg (#26437)\"\n\nThis reverts commit 8214d6e7b1d6ac25859ad745ccebdf73434e166d."
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-26T14:15:05Z",
        "message": "add exllamav2 arg (#26437)\n\n* add_ xllamav2 arg\n\n* add test\n\n* style\n\n* add check\n\n* add doc\n\n* replace by use_exllama_v2\n\n* fix tests\n\n* fix doc\n\n* style\n\n* better condition\n\n* fix logic\n\n* add deprecate msg"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-26T09:21:04Z",
        "message": "Bump`flash_attn` version to `2.1` (#27079)\n\n* pin FA-2 to `2.1`\n\n* fix on modeling"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-25T10:16:15Z",
        "message": "[`core`] Refactor of `gradient_checkpointing` (#27020)\n\n* v1\n\n* fix\n\n* remove `create_custom_forward`\n\n* fixup\n\n* fixup\n\n* add test and fix all failing GC tests\n\n* remove all remaining `create_custom_forward` methods\n\n* fix idefics bug\n\n* fixup\n\n* replace with `__call__`\n\n* add comment\n\n* quality"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-24T17:05:37Z",
        "message": "Fix config silent copy in from_pretrained (#27043)\n\n* Fix config modeling utils\n\n* fix more\n\n* fix attn mask bug\n\n* Update src/transformers/modeling_utils.py"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-24T13:10:23Z",
        "message": "Add fuyu device map (#26949)\n\n* add _no_split_modules\n\n* style\n\n* fix _no_split_modules\n\n* add doc"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-23T12:25:48Z",
        "message": "Change default `max_shard_size` to smaller value (#26942)\n\n* Update modeling_utils.py\n\n* fixup\n\n* let's change it to 5GB\n\n* fix"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-16T17:56:53Z",
        "message": "\ud83d\udea8\ud83d\udea8\ud83d\udea8 [`Quantization`] Store the original dtype in the config as a private attribute \ud83d\udea8\ud83d\udea8\ud83d\udea8 (#26761)\n\n* First step\n\n* fix\n\n* add adjustements for gptq\n\n* change to `_pre_quantization_dtype`\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* fix serialization\n\n* Apply suggestions from code review\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* fixup\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-16T13:29:01Z",
        "message": "Make fsdp ram efficient loading optional (#26631)\n\nmake fsdp ram efficient loading optional"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-13T10:56:50Z",
        "message": "[`core`] Fix fa-2 import (#26785)\n\n* fix fa-2 import\n\n* nit"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-12T08:28:40Z",
        "message": "Add many missing spaces in adjacent strings (#26751)\n\nAdd missing spaces in adjacent strings"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-05T12:44:31Z",
        "message": "[`core`] fix silent bug `keep_in_fp32` modules (#26589)\n\n* fix silent bug `keep_in_fp32` modules\n\n* final fix\n\n* added a common test.\n\n* Trigger CI\n\n* revert"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-03T12:53:09Z",
        "message": "[`PEFT`] Final fixes (#26559)\n\n* fix issues with PEFT\n\n* logger warning futurewarning issues\n\n* fixup\n\n* adapt from suggestions\n\n* oops\n\n* rm test"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-03T06:55:39Z",
        "message": "[RFC, Logging] Change warning to info (#26545)\n\n[Logging] Change warning to info"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-02T12:59:24Z",
        "message": "[`PEFT`] Protect `adapter_kwargs` check (#26537)\n\nUpdate modeling_utils.py"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-10-02T09:23:03Z",
        "message": "[`PEFT`] Pass token when calling `find_adapter_config` (#26488)\n\n* try\n\n* nit\n\n* nits"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-28T09:13:03Z",
        "message": "[`PEFT`]\u00a0introducing `adapter_kwargs` for loading adapters from different Hub location (`subfolder`, `revision`) than the base model (#26270)\n\n* make use of adapter_revision\n\n* v1 adapter kwargs\n\n* fix CI\n\n* fix CI\n\n* fix CI\n\n* fixup\n\n* add BC\n\n* Update src/transformers/integrations/peft.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* fixup\n\n* change it to error\n\n* Update src/transformers/modeling_utils.py\n\n* Update src/transformers/modeling_utils.py\n\n* fixup\n\n* change\n\n* Update src/transformers/integrations/peft.py\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-27T14:45:31Z",
        "message": "[`PEFT`] Fix PEFT multi adapters support (#26407)\n\n* fix PEFT multi adapters support\n\n* refactor a bit\n\n* save pretrained + BC + added tests\n\n* Update src/transformers/integrations/peft.py\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* add more tests\n\n* add suggestion\n\n* final changes\n\n* adapt a bit\n\n* fixup\n\n* Update src/transformers/integrations/peft.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* adapt from suggestions\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-22T15:42:10Z",
        "message": "[`core` ]\u00a0Integrate Flash attention 2 in most used models (#25598)\n\n* v1\n\n* oops\n\n* working v1\n\n* fixup\n\n* add some TODOs\n\n* fixup\n\n* padding support + try with module replacement\n\n* nit\n\n* alternative design\n\n* oops\n\n* add `use_cache` support for llama\n\n* v1 falcon\n\n* nit\n\n* a bit of refactor\n\n* nit\n\n* nits nits\n\n* add v1 padding support falcon (even though it seemed to work before)\n\n* nit\n\n* falcon works\n\n* fixup\n\n* v1 tests\n\n* nit\n\n* fix generation llama flash\n\n* update tests\n\n* fix tests + nits\n\n* fix copies\n\n* fix nit\n\n* test- padding mask\n\n* stype\n\n* add more mem efficient support\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fixup\n\n* nit\n\n* fixup\n\n* remove it from config when saving\n\n* fixup\n\n* revert docstring\n\n* add more checks\n\n* use values\n\n* oops\n\n* new version\n\n* fixup\n\n* add same trick for falcon\n\n* nit\n\n* add another test\n\n* change tests\n\n* fix issues with GC and also falcon\n\n* fixup\n\n* oops\n\n* Update src/transformers/models/falcon/modeling_falcon.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* add init_rope\n\n* updates\n\n* fix copies\n\n* fixup\n\n* fixup\n\n* more clarification\n\n* fixup\n\n* right padding tests\n\n* add docs\n\n* add FA in docker image\n\n* more clarifications\n\n* add some figures\n\n* add todo\n\n* rectify comment\n\n* Change to FA2\n\n* Update docs/source/en/perf_infer_gpu_one.md\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* split in two lines\n\n* change test name\n\n* add more tests\n\n* some clean up\n\n* remove `rearrange` deps\n\n* add more docs\n\n* revert changes on dockerfile\n\n* Revert \"revert changes on dockerfile\"\n\nThis reverts commit 8d72a66b4b9b771abc3f15a9b9506b4246d62d8e.\n\n* revert changes on dockerfile\n\n* Apply suggestions from code review\n\nCo-authored-by: Lysandre Debut <hi@lysand.re>\n\n* address some comments\n\n* docs\n\n* use inheritance\n\n* Update src/transformers/testing_utils.py\n\nCo-authored-by: Lysandre Debut <hi@lysand.re>\n\n* fixup\n\n* Apply suggestions from code review\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update src/transformers/modeling_utils.py\n\n* final comments\n\n* clean up\n\n* style\n\n* add cast + warning for PEFT models\n\n* fixup\n\n---------\n\nCo-authored-by: Felix Marty <9808326+fxmarty@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\nCo-authored-by: Lysandre Debut <hi@lysand.re>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-21T10:00:03Z",
        "message": "Keep relevant weights in fp32 when `model._keep_in_fp32_modules` is set even when `accelerate` is not installed (#26225)\n\n* fix bug where weight would not be kept in fp32\n\n* nit\n\n* address review comments\n\n* fix test"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-20T04:56:16Z",
        "message": "FSDP tests and checkpointing fixes (#26180)\n\n* add fsdp tests\n\n* Update test_fsdp.py\n\n* Update test_fsdp.py\n\n* fixes\n\n* checks\n\n* Update trainer.py\n\n* fix\n\n* fixes for saving/resuming checkpoints\n\n* fixes\n\n* add tests and delete debug statements\n\n* fixing tests\n\n* Update test_fsdp.py\n\n* fix tests\n\n* fix tests\n\n* minor nits\n\n* fix code style and quality\n\n* refactor and modularize test code\n\n* reduce the time of tests\n\n* reduce the test time\n\n* fix test\n\n* reduce test time\n\n* reduce test time\n\n* fix failing tests\n\n* fix\n\n* Apply suggestions from code review\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* resolve comments\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-19T19:44:41Z",
        "message": "[FIX] resize_token_embeddings (#26102)\n\n* fix roundup command\n\n* add test for resize_token_embeddings\n\n* Update tests/test_modeling_common.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* style\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-19T19:04:56Z",
        "message": "DeepSpeed ZeRO-3 handling when resizing embedding layers (#26259)\n\n* fix failing deepspeed slow tests\n\n* fixes"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-15T15:53:39Z",
        "message": "Fix pad to multiple of (#25732)\n\n* nits\n\n* update the test\n\n* nits\n\n* update\n\n* fix bark\n\n* fix bark tests and allow padding to multiple of without new tokens"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-14T11:01:58Z",
        "message": "[`PEFT`] Fix PEFT + gradient checkpointing (#25846)\n\n* fix PEFT + gradient checkpointing\n\n* add disable RG\n\n* polish tests\n\n* fix comment\n\n* Revert \"fix comment\"\n\nThis reverts commit b85386f50d2b104bac522e823c47b7e232116a47.\n\n* final explanations and tests"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-13T12:12:35Z",
        "message": "[`core`] fix 4bit `num_parameters` (#26132)\n\n* fix 4bit `num_parameters`\n\n* stronger check"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-13T04:56:37Z",
        "message": "safeguard torch distributed check (#26056)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-08T19:13:33Z",
        "message": "Skip warning if tracing with dynamo (#25581)\n\n* Ignore warning if tracing with dynamo\n\n* fix import error\n\n* separate to function\n\n* add test"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-07T09:10:40Z",
        "message": "fix _resize_token_embeddings will set lm head size to 0 when enabled deepspeed zero3 (#26024)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-06T15:45:47Z",
        "message": "modify context length for GPTQ + version bump (#25899)\n\n* add new arg for gptq\n\n* add tests\n\n* add min version autogptq\n\n* fix order\n\n* skip test\n\n* fix\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* fix style\n\n* change model path\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-09-05T09:37:54Z",
        "message": "nn.Identity is not required to be compatible with PyTorch < 1.1.0 as the minimum PyTorch version we currently support is 1.10.0 (#25974)\n\nnn.Identity is not required to be compatible with PyTorch < 1.1.0 as the\nminimum PyTorch version we currently support is 1.10.0"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-31T21:38:14Z",
        "message": "remove torch_dtype override (#25894)\n\n* remove torch_dtype override\n\n* style\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-30T15:00:36Z",
        "message": "fix max_memory for bnb (#25842)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-29T19:10:46Z",
        "message": "Generate: models with custom `generate()` return `True` in `can_generate()` (#25838)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-29T11:32:19Z",
        "message": "Resolving Attribute error when using the FSDP ram efficient feature (#25820)\n\nfix bug"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-25T15:46:56Z",
        "message": "fix a typo in docsting (#25759)\n\n* fix a typo in docsting\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n---------\n\nCo-authored-by: statelesshz <jihuazhong1@huawei.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-25T15:13:34Z",
        "message": "\ud83d\udea8\ud83d\udea8\ud83d\udea8 [`Refactor`] Move third-party related utility files into `integrations/` folder \ud83d\udea8\ud83d\udea8\ud83d\udea8 (#25599)\n\n* move deepspeed to `lib_integrations.deepspeed`\n\n* more refactor\n\n* oops\n\n* fix slow tests\n\n* Fix docs\n\n* fix docs\n\n* addess feedback\n\n* address feedback\n\n* final modifs for PEFT\n\n* fixup\n\n* ok now\n\n* trigger CI\n\n* trigger CI again\n\n* Update docs/source/en/main_classes/deepspeed.md\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* import from `integrations`\n\n* address feedback\n\n* revert removal of `deepspeed` module\n\n* revert removal of `deepspeed` module\n\n* fix conflicts\n\n* ooops\n\n* oops\n\n* add deprecation warning\n\n* place it on the top\n\n* put `FutureWarning`\n\n* fix conflicts with not_doctested.txt\n\n* add back `bitsandbytes` module with a depr warning\n\n* fix\n\n* fix\n\n* fixup\n\n* oops\n\n* fix doctests\n\n---------\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-24T16:48:41Z",
        "message": "[`from_pretrained`] Fix failing PEFT tests (#25733)\n\nfix failing PEFT tests"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-24T14:18:39Z",
        "message": "[`from_pretrained`]  Simpler code for peft (#25726)\n\n* refactor complicated from pretrained for peft\n\n* nits\n\n* more nits\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* make tests happy\n\n* fixup after merge\n\n---------\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-24T12:56:11Z",
        "message": "Fix number of minimal calls to the Hub with peft integration (#25715)\n\n* Fix number of minimal calls to the Hub with peft integration\n\n* Alternate design\n\n* And this way?\n\n* Revert\n\n* Address comments"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-24T09:14:27Z",
        "message": "docs: Resolve typos in warning text (#25711)\n\nResolve typos in warning text"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-24T06:00:42Z",
        "message": "fix ram efficient fsdp init (#25686)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-18T21:30:29Z",
        "message": "reattach hooks when using `resize_token_embeddings` (#25596)\n\n* reattach hooks\n\n* fix style"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-18T17:08:03Z",
        "message": "[`PEFT`] Peft integration alternative design  (#25077)\n\n* a draft version\n\n* v2 integration\n\n* fix\n\n* make it more generic and works for IA3\n\n* add set adapter and multiple adapters support\n\n* fixup\n\n* adapt a bit\n\n* oops\n\n* oops\n\n* oops\n\n* adapt more\n\n* fix\n\n* add more refactor\n\n* now works with model class\n\n* change it to instance method as it causes issues with `jit`.\n\n* add CR\n\n* change method name\n\n* add `add_adapter` method\n\n* clean up\n\n* Update src/transformers/adapters/peft_mixin.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* add moe utils\n\n* fixup\n\n* Update src/transformers/adapters/peft_mixin.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* adapt\n\n* oops\n\n* fixup\n\n* add is_peft_available\n\n* remove `requires_backend`\n\n* trainer compatibility\n\n* fixup + docstring\n\n* more details\n\n* trigger CI\n\n* Apply suggestions from code review\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* Update src/transformers/modeling_utils.py\n\n* fixup + is_main_process\n\n* added `save_peft_format` in save_pretrained\n\n* up\n\n* fix nits here and there\n\n* nits here and there.\n\n* docs\n\n* revert `encoding=\"utf-8\"`\n\n* comment\n\n* added slow tests before the PEFT release.\n\n* fixup and nits\n\n* let's be on the safe zone\n\n* added more comments\n\n* v1 docs\n\n* add remaining docs\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* move to `lib_integrations`\n\n* fixup\n\n* this time fixup\n\n* Apply suggestions from code review\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* address final comments\n\n* refactor to use `token`\n\n* add PEFT to DockerFile for slow tests.\n\n* added pipeline support.\n\n---------\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-18T08:32:46Z",
        "message": "Added missing parenthesis in call to is_fsdp_enabled (#25585)\n\nCalling function is_fsdp_enabled instead of checking if it is not None"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-17T16:23:34Z",
        "message": "add util for ram efficient loading of model when using fsdp (#25107)\n\n* add util for ram efficient loading of model when using fsdp\n\n* make fix-copies\n\n* fixes \ud83d\ude05\n\n* docs\n\n* making it further easier to use\n\n* rename the function\n\n* refactor to handle fsdp ram efficiency in `from_pretrained`\n\n* fixes\n\n* fixes\n\n* fixes\n\n* update\n\n* fixes\n\n* revert `load_pretrained_model_only_on_rank0`\n\n* resolve `load_from_checkpoint`"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-17T15:44:01Z",
        "message": "Revert \"change version (#25387)\" (#25573)\n\nThis reverts commit 3a05e010e0c7e8abd3e5357dd4e89e28cc69003e."
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-17T15:19:54Z",
        "message": "Inconsistency in PreTrainedModel.resize_token_embeddings When ZeRO3 Is Enabled (#25394)\n\n* Inconsistency in PreTrainedModel.resize_token_embeddings\n\nThis PR addresses https://github.com/huggingface/transformers/issues/25241.\n\nIn previous implementation when ZeRO stage 3 was enbaled, resize_token_embeddings would create independent PyTorch weights on each device. Here we ensure that new embeddings are created with DeepSpeed init, and are properly partitioned accros devices.\n\n* formatting with black\n\n* adding the removed comments back in\n\n---------\n\nCo-authored-by: Sina Moeini <smoeini@amazon.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-17T15:00:32Z",
        "message": "[`resize_embedding`] Introduce `pad_to_multiple_of` and guidance (#25088)\n\n* fix\n\n* revert cahnges and update resizing of embedding layer\n\n* use wraning\n\n* fixup\n\n* more styling nits\n\n* fix all tests that overload the embedding tests\n\n* \ud83d\udc40\ud83d\udc40 remove breakpoint\n\n* remove useless overload + overload correctly where needed\n\n* resize lm head with new vocab size\n\n* reverse not necessary changes\n\n* style\n\n* fix CIs!\n\n* fix last CI tests, adapt bark and Marian\n\n* fixup"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-10T20:06:29Z",
        "message": "GPTQ integration (#25062)\n\n* GTPQ integration\n\n* Add tests for gptq\n\n* support for more quantization model\n\n* fix style\n\n* typo\n\n* fix method\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* add dataclass and fix quantization_method\n\n* fix doc\n\n* Update tests/quantization/gptq/test_gptq.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* modify dataclass\n\n* add gtpqconfig import\n\n* fix typo\n\n* fix tests\n\n* remove dataset as req arg\n\n* remove tokenizer import\n\n* add offload cpu quantization test\n\n* fix check dataset\n\n* modify dockerfile\n\n* protect trainer\n\n* style\n\n* test for config\n\n* add more log\n\n* overwrite torch_dtype\n\n* draft doc\n\n* modify quantization_config docstring\n\n* fix class name in docstring\n\n* Apply suggestions from code review\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* more warning\n\n* fix 8bit kwargs tests\n\n* peft compatibility\n\n* remove var\n\n* fix is_gptq_quantized\n\n* remove is_gptq_quantized\n\n* fix wrap\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\n\n* add exllama\n\n* skip test\n\n* overwrite float16\n\n* style\n\n* fix skip test\n\n* Apply suggestions from code review\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* fix docsting formatting\n\n* add doc\n\n* better test\n\n---------\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-10T09:54:26Z",
        "message": "Generate: Load generation config when `device_map` is passed (#25413)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-08T17:05:41Z",
        "message": "change version (#25387)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-08T08:49:21Z",
        "message": "Add warning for missing attention mask when pad tokens are detected (#25345)\n\n* Add attention mask and pad token warning to many of the models\n\n* Remove changes under examples/research_projects\n\nThese files are not maintained by HG.\n\n* Skip the warning check during torch.fx or JIT tracing\n\n* Switch ordering for the warning and input shape assignment\n\nThis ordering is a little cleaner for some of the cases.\n\n* Add missing line break in one of the files"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-04T11:42:05Z",
        "message": "Move usage of deprecated logging.warn to logging.warning (#25310)\n\nThe former spelling is deprecated and has been discouraged for a\nwhile. The latter spelling seems to be more common in this project\nanyway, so this change ought to be safe.\n\nFixes https://github.com/huggingface/transformers/issues/25283"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-02T16:11:15Z",
        "message": "[MMS] Fix mms (#25267)\n\n* [MMS] Fix mms\n\n* [MMS] Fix mms\n\n* fix mms loading\n\n* Apply suggestions from code review\n\n* make style\n\n* Update tests/models/wav2vec2/test_modeling_wav2vec2.py"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-08-01T08:56:52Z",
        "message": "[`Docs`/`quantization`] Clearer explanation on how things works under the hood. + remove outdated info (#25216)\n\n* clearer explanation on how things works under the hood.\n\n* Update docs/source/en/main_classes/quantization.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/main_classes/quantization.md\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* add `load_in_4bit` in `from_pretrained`\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-07-31T15:25:09Z",
        "message": "[`PreTrainedModel`] Wrap `cuda` and `to` method correctly (#25206)\n\nwrap `cuda` and `to` method correctly"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-07-28T12:17:24Z",
        "message": "override .cuda() to check if model is already quantized (#25166)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-07-27T13:09:27Z",
        "message": "Clarify 4/8 bit loading log message (#25134)\n\n* clarify 4/8 bit loading log message\n\n* make style"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-07-26T13:09:59Z",
        "message": "update `use_auth_token` -> `token` (#25083)\n\n* update\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-07-25T22:08:45Z",
        "message": "fix tied_params for meta tensor (#25101)\n\n* fix tied_params for meta tensor\n\n* remove duplicate"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-07-21T19:39:28Z",
        "message": "improve from_pretrained for zero3 multi gpus mode (#24964)\n\n* improve from_pretrained for zero3 multi gpus mode\n\n* Add check if torch.distributed.is_initialized\n\n* Revert torch.distributed\n\n---------\n\nCo-authored-by: Stas Bekman <stas@stason.org>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-07-11T16:40:21Z",
        "message": "Skip keys not in the state dict when finding mismatched weights (#24749)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-07-11T15:21:29Z",
        "message": "Docs: add `kwargs` type to fix formatting (#24733)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-06-30T12:19:39Z",
        "message": "Show a warning for missing attention masks when pad_token_id is not None (#24510)\n\n* Adding warning messages to BERT for missing attention masks\n\nThese warning messages when there are pad tokens within the input ids and\nno attention masks are given. The warning message should only show up once.\n\n* Adding warning messages to BERT for missing attention masks\n\nThese warning messages are shown when the pad_token_id is not None\nand no attention masks are given. The warning message should only\nshow up once.\n\n* Ran fix copies to copy over the changes to some of the other models\n\n* Add logger.warning_once.cache_clear() to the test\n\n* Shows warning when there are no attention masks and input_ids start/end with pad tokens\n\n* Using warning_once() instead and fix indexing in input_ids check\n\n---------\n\nCo-authored-by: JB Lau <hckyn@voyager2.local>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-06-28T05:22:39Z",
        "message": "\u26a0\ufe0f Time to say goodbye to py37 (#24091)\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-06-27T18:45:40Z",
        "message": "Clean load keys (#24505)\n\n* Preliminary work on some models\n\n* Fix test load missing and make sure nonpersistent buffers are tested\n\n* Always ignore nonpersistent buffers if in state_dict\n\n* Treat models\n\n* More models\n\n* Treat remaining models\n\n* Fix quality\n\n* Fix tests\n\n* Remove draft\n\n* This test is not needed anymore\n\n* Fix copies\n\n* Fix last test\n\n* Newly added models\n\n* Fix last tests\n\n* Address review comments"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-06-22T13:40:38Z",
        "message": "[`bnb`]\u00a0Fix bnb serialization issue with new release (#24416)\n\n* fix bnb issue\n\n* fixup\n\n* revert and do simple patching instead\n\n* add more details"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-06-21T17:24:11Z",
        "message": "Explicit arguments in `from_pretrained` (#24306)\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-06-16T14:55:42Z",
        "message": "Tied weights load (#24310)\n\n* Use tied weight keys\n\n* More\n\n* Fix tied weight missing warning\n\n* Only give info on unexpected keys with different classes\n\n* Deal with empty archs\n\n* Fix tests\n\n* Refine test"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-06-15T16:31:38Z",
        "message": "Make `can_generate` as class method (#24299)\n\nfix\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-06-14T16:44:09Z",
        "message": "Clean up old Accelerate checks (#24279)\n\n* Clean up old Accelerate checks\n\n* Put back imports"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-06-13T15:38:39Z",
        "message": "Tied params cleanup (#24211)\n\n* First test\n\n* Add info for all models\n\n* style\n\n* Repo consistency\n\n* Fix last model and cleanup prints\n\n* Repo consistency\n\n* Use consistent function for detecting tied weights"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-06-13T13:07:00Z",
        "message": "Improving error message when using `use_safetensors=True`. (#24232)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-06-12T15:31:06Z",
        "message": "Fix `_load_pretrained_model` (#24200)\n\nFix test"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-06-06T13:12:46Z",
        "message": "Add check for tied parameters (#24029)\n\n* Add check for tied parameters\n\n* Fix style\n\n* fix style\n\n* Fix versioning\n\n* Change if to elif"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-06-01T17:21:22Z",
        "message": "Modify device_map behavior when loading a model using from_pretrained (#23922)\n\n* Modify device map behavior for 4/8 bits model\n\n* Remove device_map arg for training 4/8 bit model\n\n* Remove index\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* Add Exceptions\n\n* Modify comment\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* Fix formatting\n\n* Get current device with accelerate\n\n* Revert \"Get current device with accelerate\"\n\nThis reverts commit 46f00799103bbe15bd58762ba029aab35363c4f7.\n\n* Fix Exception\n\n* Modify quantization doc\n\n* Fix error\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-05-31T19:32:21Z",
        "message": "Skip device placement for past key values in decoder models (#23919)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-05-31T14:40:07Z",
        "message": "[`bnb`] add warning when no linear  (#23894)\n\n* add warning for gpt2-like models\n\n* more details\n\n* adapt from suggestions"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-05-31T13:42:30Z",
        "message": "Support shared tensors (#23871)\n\n* Suport shared storage\n\n* Really be sure we have the same storage\n\n* Make style\n\n* - Refactor storage identifier mechanism\n - Group everything into a single for loop\n\n* Make style\n\n* PR\n\n* make style\n\n* Update src/transformers/pytorch_utils.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-05-31T09:38:20Z",
        "message": "Fix last instances of kbit -> quantized (#23797)"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-05-30T15:12:14Z",
        "message": "[from_pretrained] imporve the error message when `_no_split_modules` is not defined (#23861)\n\n* Better warning\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* format line\n\n---------\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/modeling_utils.py",
        "commit_date": "2023-05-25T11:48:48Z",
        "message": "Fix `pip install --upgrade accelerate` command in modeling_utils.py (#23747)\n\nFix command in modeling_utils.py"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/integrations/peft.py",
        "commit_date": "2023-11-14T09:32:57Z",
        "message": "[`Peft`] `modules_to_save` support for peft integration (#27466)\n\n* `modules_to_save` support for peft integration\n\n* Update docs/source/en/peft.md\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* slightly elaborate test\n\n---------\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/integrations/peft.py",
        "commit_date": "2023-11-13T13:20:54Z",
        "message": "Remove-auth-token (#27060)\n\n* don't use `use_auth_token`internally\n\n* let's use token everywhere\n\n* fixup"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/integrations/peft.py",
        "commit_date": "2023-10-03T12:53:09Z",
        "message": "[`PEFT`] Final fixes (#26559)\n\n* fix issues with PEFT\n\n* logger warning futurewarning issues\n\n* fixup\n\n* adapt from suggestions\n\n* oops\n\n* rm test"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/integrations/peft.py",
        "commit_date": "2023-09-28T09:13:03Z",
        "message": "[`PEFT`]\u00a0introducing `adapter_kwargs` for loading adapters from different Hub location (`subfolder`, `revision`) than the base model (#26270)\n\n* make use of adapter_revision\n\n* v1 adapter kwargs\n\n* fix CI\n\n* fix CI\n\n* fix CI\n\n* fixup\n\n* add BC\n\n* Update src/transformers/integrations/peft.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* fixup\n\n* change it to error\n\n* Update src/transformers/modeling_utils.py\n\n* Update src/transformers/modeling_utils.py\n\n* fixup\n\n* change\n\n* Update src/transformers/integrations/peft.py\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/integrations/peft.py",
        "commit_date": "2023-09-27T14:45:31Z",
        "message": "[`PEFT`] Fix PEFT multi adapters support (#26407)\n\n* fix PEFT multi adapters support\n\n* refactor a bit\n\n* save pretrained + BC + added tests\n\n* Update src/transformers/integrations/peft.py\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* add more tests\n\n* add suggestion\n\n* final changes\n\n* adapt a bit\n\n* fixup\n\n* Update src/transformers/integrations/peft.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* adapt from suggestions\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/integrations/peft.py",
        "commit_date": "2023-09-15T16:22:01Z",
        "message": "[PEFT] Allow PEFT model dict to be loaded (#25721)\n\n* Allow PEFT model dict to be loaded\n\n* make style\n\n* make style\n\n* Apply suggestions from code review\n\n* address comments\n\n* fixup\n\n* final change\n\n* added tests\n\n* fix test\n\n* better logic for handling if adapter has been loaded\n\n* Update tests/peft_integration/test_peft_integration.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n---------\n\nCo-authored-by: younesbelkada <younesbelkada@gmail.com>\nCo-authored-by: Younes Belkada <49240599+younesbelkada@users.noreply.github.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/integrations/peft.py",
        "commit_date": "2023-08-30T10:56:05Z",
        "message": "minor typo fix in PeftAdapterMixin docs (#25829)\n\nfix minor documentation typo"
    },
    {
        "repo_url": "github.com/kssteven418/SqueezeLLM-gradients",
        "filepath": "src/transformers/integrations/peft.py",
        "commit_date": "2023-08-25T15:13:34Z",
        "message": "\ud83d\udea8\ud83d\udea8\ud83d\udea8 [`Refactor`] Move third-party related utility files into `integrations/` folder \ud83d\udea8\ud83d\udea8\ud83d\udea8 (#25599)\n\n* move deepspeed to `lib_integrations.deepspeed`\n\n* more refactor\n\n* oops\n\n* fix slow tests\n\n* Fix docs\n\n* fix docs\n\n* addess feedback\n\n* address feedback\n\n* final modifs for PEFT\n\n* fixup\n\n* ok now\n\n* trigger CI\n\n* trigger CI again\n\n* Update docs/source/en/main_classes/deepspeed.md\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* import from `integrations`\n\n* address feedback\n\n* revert removal of `deepspeed` module\n\n* revert removal of `deepspeed` module\n\n* fix conflicts\n\n* ooops\n\n* oops\n\n* add deprecation warning\n\n* place it on the top\n\n* put `FutureWarning`\n\n* fix conflicts with not_doctested.txt\n\n* add back `bitsandbytes` module with a depr warning\n\n* fix\n\n* fix\n\n* fixup\n\n* oops\n\n* fix doctests\n\n---------\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/mzbac/llama2-fine-tune",
        "filepath": "merge_peft_adapters.py",
        "commit_date": "2023-08-13T14:24:09Z",
        "message": "Update merge_peft_adapters.py"
    },
    {
        "repo_url": "github.com/mzbac/llama2-fine-tune",
        "filepath": "merge_peft_adapters.py",
        "commit_date": "2023-08-13T14:10:45Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/wordweb/langchain-ChatGLM-and-TigerBot",
        "filepath": "models/loader/loader.py",
        "commit_date": "2023-06-12T06:12:06Z",
        "message": "langchain-chatglm up"
    },
    {
        "repo_url": "github.com/wordweb/langchain-ChatGLM-and-TigerBot",
        "filepath": "models/loader/loader.py",
        "commit_date": "2023-06-08T11:56:22Z",
        "message": "Merge remote-tracking branch 'origin/main'"
    },
    {
        "repo_url": "github.com/OpenGVLab/LAMM",
        "filepath": "src/model/llava/model/builder.py",
        "commit_date": "2023-12-22T07:36:37Z",
        "message": "llava1.5"
    },
    {
        "repo_url": "github.com/OpenGVLab/LAMM",
        "filepath": "src/ChEF/models/test_llava15.py",
        "commit_date": "2023-12-22T07:36:37Z",
        "message": "llava1.5"
    },
    {
        "repo_url": "github.com/ymcui/Chinese-Mixtral",
        "filepath": "scripts/merge_mixtral_with_chinese_lora_low_mem.py",
        "commit_date": "2024-01-29T02:55:06Z",
        "message": "v1.0: release Chinese-Mixtral and Chinese-Mixtral-Instruct (#1)\n\n* doc: init doc\n\n* Update README.md\n\n* doc: update gguf model perf.\n\n* doc: finish quant perf.\n\n* doc: update intro\n\n* doc: add longbench info\n\n* doc: update iq2_xs, iq2_xxs perf.\n\n* doc: add chinese-mixtral baidu links\n\n* doc: add ppl v.s. ctx figure\n\n* add merge_lora script\n\n* Update merge_mixtral_with_chinese_lora_low_mem.py\n\nclean up old naming\n\n* doc: update template\n\n* doc: update numbers, mixtral arch\n\n* llamacpp: add chat script\n\n* Update chinese-mixtral-ppl.png\n\n* doc: add perf.\n\n* doc: update baidu link\n\n* doc: update gpt-4 rating\n\n* doc: finsh perf.\n\n* script: change default temp.\n\n* doc: add gpt-4 score\n\n* doc: update context desc.\n\n* doc: init en readme\n\n* doc: minor fixes\n\n* update based on Codacy\n\n---------\n\nCo-authored-by: ymcui <16095339+ymcui@users.noreply.github.com>\nCo-authored-by: iMountTai <2506700016@qq.com>"
    },
    {
        "repo_url": "github.com/ArtificialZeng/baichuan-speedup",
        "filepath": "pyfastllm/fastllm/convert.py",
        "commit_date": "2023-08-10T05:04:50Z",
        "message": "add c++ code"
    },
    {
        "repo_url": "github.com/telexyz/GPT4VN",
        "filepath": "chatbot.py",
        "commit_date": "2023-05-21T14:01:08Z",
        "message": "Update chatbot.py"
    },
    {
        "repo_url": "github.com/telexyz/GPT4VN",
        "filepath": "chatbot.py",
        "commit_date": "2023-04-09T00:58:22Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/telexyz/GPT4VN",
        "filepath": "chatbot.py",
        "commit_date": "2023-04-09T00:56:12Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/telexyz/GPT4VN",
        "filepath": "chatbot.py",
        "commit_date": "2023-04-05T03:11:50Z",
        "message": "thay d\u1eef li\u1ec7u finetune m\u1edbi nh\u1ea5t"
    },
    {
        "repo_url": "github.com/telexyz/GPT4VN",
        "filepath": "chatbot.py",
        "commit_date": "2023-04-03T11:10:39Z",
        "message": "chatbot use best model"
    },
    {
        "repo_url": "github.com/telexyz/GPT4VN",
        "filepath": "chatbot.py",
        "commit_date": "2023-04-03T09:32:28Z",
        "message": "hu\u1ea5n luy\u1ec7n ch\u1ec9 d\u1eabn v\u00e0 ch\u1ea1y chatbot tr\u00ean to\u00e0n b\u1ed9 t\u1eadp d\u1eef li\u1ec7u"
    },
    {
        "repo_url": "github.com/telexyz/GPT4VN",
        "filepath": "chatbot.py",
        "commit_date": "2023-04-02T15:41:20Z",
        "message": "another way to extract response"
    },
    {
        "repo_url": "github.com/telexyz/GPT4VN",
        "filepath": "chatbot.py",
        "commit_date": "2023-04-02T15:21:28Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/telexyz/GPT4VN",
        "filepath": "chatbot.py",
        "commit_date": "2023-04-02T07:42:45Z",
        "message": "refine"
    },
    {
        "repo_url": "github.com/telexyz/GPT4VN",
        "filepath": "chatbot.py",
        "commit_date": "2023-03-31T12:17:11Z",
        "message": "load in 8bit to save vram"
    },
    {
        "repo_url": "github.com/telexyz/GPT4VN",
        "filepath": "chatbot.py",
        "commit_date": "2023-03-31T12:08:30Z",
        "message": "move make_prompt to separate file"
    },
    {
        "repo_url": "github.com/telexyz/GPT4VN",
        "filepath": "chatbot.py",
        "commit_date": "2023-03-31T12:06:53Z",
        "message": "S\u1eed d\u1ee5ng model vietai gpt-j 6b"
    },
    {
        "repo_url": "github.com/telexyz/GPT4VN",
        "filepath": "chatbot.py",
        "commit_date": "2023-03-31T11:58:10Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/ictnlp/TruthX",
        "filepath": "transformers/src/transformers/trainer.py",
        "commit_date": "2024-02-27T13:46:43Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/ictnlp/TruthX",
        "filepath": "transformers/src/transformers/modeling_utils.py",
        "commit_date": "2024-02-27T13:46:43Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/ictnlp/TruthX",
        "filepath": "transformers/src/transformers/integrations/peft.py",
        "commit_date": "2024-02-27T13:46:43Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/BUPT-LawLLM/LawLLM",
        "filepath": "evaluator.py",
        "commit_date": "2023-11-15T19:11:18Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/FederatedAI/FATE-LLM",
        "filepath": "python/fate_llm/model_zoo/pellm/parameter_efficient_llm.py",
        "commit_date": "2023-08-30T06:33:51Z",
        "message": "LLM-1.3 Update\n1. FedIPR:\n- backdoor watermark dataset\n- feature-based water mark modules: conv and layernorm\n- built-in feature-based watermark model: alexnet, resnet18, distilbert, gpt2\n- fedipr trainer\n2. Offsite-tuning\n- offsite-tuning models: gpt2 family and bloom-7b\n- offsite-tuning trainer\n\nSigned-off-by: cwj <talkingwallace@sohu.com>"
    },
    {
        "repo_url": "github.com/FederatedAI/FATE-LLM",
        "filepath": "python/fate_llm/model_zoo/pellm/parameter_efficient_llm.py",
        "commit_date": "2023-05-29T12:36:58Z",
        "message": "update fate_llm v1.1: support chatglm\n\nSigned-off-by: mgqa34 <mgq3374541@163.com>"
    },
    {
        "repo_url": "github.com/parity-asia/hackathon-2023-winter",
        "filepath": "projects/37-MonteCarlo/src/poc-ai-script/adhoc_unit_test.py",
        "commit_date": "2023-12-20T19:24:40Z",
        "message": "update readme"
    },
    {
        "repo_url": "github.com/cmagganas/CoverLetter-GenAI-adapter",
        "filepath": "app/app.py",
        "commit_date": "2023-04-27T17:09:34Z",
        "message": "Create app.py"
    },
    {
        "repo_url": "github.com/bigcode-project/astraios",
        "filepath": "peft/src/peft/auto.py",
        "commit_date": "2023-12-14T14:00:17Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/bigcode-project/astraios",
        "filepath": "peft/src/peft/config.py",
        "commit_date": "2023-12-14T14:00:17Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/bigcode-project/astraios",
        "filepath": "peft/src/peft/helpers.py",
        "commit_date": "2023-12-14T14:00:17Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/bigcode-project/astraios",
        "filepath": "peft/src/peft/peft_model.py",
        "commit_date": "2023-12-14T14:00:17Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/bigcode-project/astraios",
        "filepath": "peft/src/peft/tuners/ia3.py",
        "commit_date": "2023-12-14T14:00:17Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/bigcode-project/astraios",
        "filepath": "peft/src/peft/tuners/lora.py",
        "commit_date": "2023-12-14T14:00:17Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/bigcode-project/astraios",
        "filepath": "peft/src/peft/tuners/bottleneck.py",
        "commit_date": "2023-12-14T14:00:17Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/bigcode-project/astraios",
        "filepath": "peft/src/peft/tuners/prompt_tuning.py",
        "commit_date": "2023-12-14T14:00:17Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/bigcode-project/astraios",
        "filepath": "peft/examples/fp4_finetuning/finetune_fp4_opt_bnb_peft.py",
        "commit_date": "2023-12-14T14:00:17Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/bigcode-project/astraios",
        "filepath": "peft/examples/lora_dreambooth/convert_peft_sd_lora_to_kohya_ss.py",
        "commit_date": "2023-12-14T14:00:17Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/bigcode-project/astraios",
        "filepath": "peft/tests/test_hub_features.py",
        "commit_date": "2023-12-14T14:00:17Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/bigcode-project/astraios",
        "filepath": "peft/tests/test_custom_models.py",
        "commit_date": "2023-12-14T14:00:17Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/bigcode-project/astraios",
        "filepath": "peft/tests/test_adaption_prompt.py",
        "commit_date": "2023-12-14T14:00:17Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/bigcode-project/astraios",
        "filepath": "peft/tests/test_multitask_prompt_tuning.py",
        "commit_date": "2023-12-14T14:00:17Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/liyucheng09/Selective_Context",
        "filepath": "qa_manager.py",
        "commit_date": "2023-06-04T16:26:46Z",
        "message": "fix a lot of bugs in adding new models"
    },
    {
        "repo_url": "github.com/liyucheng09/Selective_Context",
        "filepath": "qa_manager.py",
        "commit_date": "2023-05-19T22:20:59Z",
        "message": "add open-sourced LLMs in experiments"
    },
    {
        "repo_url": "github.com/liyucheng09/Selective_Context",
        "filepath": "qa_manager.py",
        "commit_date": "2023-04-24T14:58:38Z",
        "message": "stable version with all element for the paper"
    },
    {
        "repo_url": "github.com/liyucheng09/Selective_Context",
        "filepath": "qa_manager.py",
        "commit_date": "2023-04-19T12:57:44Z",
        "message": "add Filelock in `context_manager.py`\nFix bug in `load_articles`\nEnhance robustness in `qa_manager.py`\nadd results parsing in `utils.py`"
    },
    {
        "repo_url": "github.com/liyucheng09/Selective_Context",
        "filepath": "qa_manager.py",
        "commit_date": "2023-04-17T22:00:00Z",
        "message": "add reconstruction task\nseperate Evaluator from TaskManager"
    },
    {
        "repo_url": "github.com/liyucheng09/Selective_Context",
        "filepath": "qa_manager.py",
        "commit_date": "2023-04-17T15:18:02Z",
        "message": "error handle in `qa_manager.py`\ndataset_statistic checker in `utils.py`"
    },
    {
        "repo_url": "github.com/liyucheng09/Selective_Context",
        "filepath": "qa_manager.py",
        "commit_date": "2023-04-12T23:45:17Z",
        "message": "tokenizer bug fixed\nspacy.noun_chunk doesn't align to GPT tokenizer\n\nadd num_article variable to context_manager"
    },
    {
        "repo_url": "github.com/liyucheng09/Selective_Context",
        "filepath": "qa_manager.py",
        "commit_date": "2023-04-12T21:05:34Z",
        "message": "add slicing to context_manager\nbug fix"
    },
    {
        "repo_url": "github.com/liyucheng09/Selective_Context",
        "filepath": "qa_manager.py",
        "commit_date": "2023-04-12T18:12:19Z",
        "message": "add checkpoints supports"
    },
    {
        "repo_url": "github.com/liyucheng09/Selective_Context",
        "filepath": "qa_manager.py",
        "commit_date": "2023-04-12T16:57:55Z",
        "message": "add `qa` task and `phrase` level masking"
    },
    {
        "repo_url": "github.com/liyucheng09/Selective_Context",
        "filepath": "qa_manager.py",
        "commit_date": "2023-04-11T14:50:22Z",
        "message": "bugs fix"
    },
    {
        "repo_url": "github.com/liyucheng09/Selective_Context",
        "filepath": "qa_manager.py",
        "commit_date": "2023-04-07T14:37:40Z",
        "message": "context_manager and qa_manager"
    },
    {
        "repo_url": "github.com/lfy79001/TableQAKit",
        "filepath": "TableQAKit/llama/model.py",
        "commit_date": "2023-07-31T03:13:35Z",
        "message": "\u589e\u52a0icl\uff0c\u66f4\u6539\u90e8\u5206\u4ee3\u7801"
    },
    {
        "repo_url": "github.com/huggingface/lighteval",
        "filepath": "src/lighteval/models/adapter_model.py",
        "commit_date": "2024-02-07T14:38:46Z",
        "message": "Adding inference endpoints models (#12)\n\nThis PR:\n    uses Requests instead of passing tuples (which are more error prones) in the Datasets\n    introduces an Abstract Model class which defines the minimum functions we need to have in a model for it to be lighteval compatible\n    cleans up the BaseModel code\n    introduces inference endpoints models\n\nInference endpoints models are these ones: https://huggingface.co/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints\nNot to be confused with TGI models (which need a local deployment)\n\n\n---------\n\nCo-authored-by: Nathan Habib <30601243+NathanHB@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/huggingface/lighteval",
        "filepath": "src/lighteval/models/adapter_model.py",
        "commit_date": "2024-01-26T14:38:32Z",
        "message": "Init\n\nCo-authored-by: Nathan Habib <nathan.habib@huggingface.co>\nCo-authored-by: Thom Wolf <thom@huggingface.co>"
    },
    {
        "repo_url": "github.com/mikeybellissimo/LoRA-MPT",
        "filepath": "src/generate.py",
        "commit_date": "2023-06-06T21:06:13Z",
        "message": "uncommented generate.py usage of finetuned weights"
    },
    {
        "repo_url": "github.com/mikeybellissimo/LoRA-MPT",
        "filepath": "src/generate.py",
        "commit_date": "2023-06-06T16:12:30Z",
        "message": "updated ReadME"
    },
    {
        "repo_url": "github.com/mikeybellissimo/LoRA-MPT",
        "filepath": "src/generate.py",
        "commit_date": "2023-06-05T17:42:19Z",
        "message": "Enabled it to be built as pip"
    },
    {
        "repo_url": "github.com/mikeybellissimo/LoRA-MPT",
        "filepath": "src/upload_to_hub.py",
        "commit_date": "2023-06-07T21:46:57Z",
        "message": "Got eval working correctly"
    },
    {
        "repo_url": "github.com/mikeybellissimo/LoRA-MPT",
        "filepath": "src/export_state_dict_checkpoint.py",
        "commit_date": "2023-06-05T17:42:19Z",
        "message": "Enabled it to be built as pip"
    },
    {
        "repo_url": "github.com/linto-ai/linto-stt",
        "filepath": "whisper/stt/processing/load_model.py",
        "commit_date": "2024-02-16T16:34:14Z",
        "message": "Support more model formats, and add log about precision (ct2/faster_whisper)"
    },
    {
        "repo_url": "github.com/linto-ai/linto-stt",
        "filepath": "whisper/stt/processing/load_model.py",
        "commit_date": "2023-11-30T16:49:13Z",
        "message": "fix coding style"
    },
    {
        "repo_url": "github.com/linto-ai/linto-stt",
        "filepath": "whisper/stt/processing/load_model.py",
        "commit_date": "2023-11-29T17:49:00Z",
        "message": "Isolate what is specific to Whisper in a folder"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "vae_infer.py",
        "commit_date": "2023-10-22T04:56:01Z",
        "message": "Add classifier free guidance"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "vae_infer.py",
        "commit_date": "2023-10-13T02:25:04Z",
        "message": "Fix samplers so they generate good prose"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "vae_infer.py",
        "commit_date": "2023-10-11T13:03:32Z",
        "message": "Update training code and add tuned AdaVAE samplers"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "vae_infer.py",
        "commit_date": "2023-10-03T14:46:42Z",
        "message": "Quick fix indices for default bigvae generation"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "vae_infer.py",
        "commit_date": "2023-10-03T10:54:08Z",
        "message": "Make default model Mistral"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "vae_infer.py",
        "commit_date": "2023-10-03T10:48:59Z",
        "message": "Add BigVAE inference code"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "lora_tune.py",
        "commit_date": "2023-06-14T08:13:43Z",
        "message": "Clear CUDA cache when tuning reward model"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "lora_tune.py",
        "commit_date": "2023-06-05T23:16:06Z",
        "message": "Tune in 8-bit, with gradient checkpointing, on multiple GPUs"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "lora_tune.py",
        "commit_date": "2023-06-05T23:14:33Z",
        "message": "Make inference server actually use tuned model"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "lora_tune.py",
        "commit_date": "2023-06-05T13:23:54Z",
        "message": "Add ability to lora tune evaluator model"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-12-10T13:01:13Z",
        "message": "Add 1st draft of rewrite button"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-12-02T16:17:53Z",
        "message": "Fix node updates on edit, make base roll settings control gen"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-11-27T23:22:32Z",
        "message": "Add AdaVAE support"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-11-03T20:19:40Z",
        "message": "Switch default model over to Mistral"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-07-19T05:00:26Z",
        "message": "Add generator LoRA support"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-07-14T20:56:59Z",
        "message": "Use RiversHaveWings/minihf_evaluator_openllama_7b as evaluator"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-07-12T07:29:54Z",
        "message": "Generate using batch size 1"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-07-04T18:07:49Z",
        "message": "Use OpenLLaMA 7B as generator, LoRA of it as evaluator"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-07-03T08:03:46Z",
        "message": "Make /generate produce multiple gens based on weave_beam_width"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-07-01T03:06:37Z",
        "message": "Add toggle to use weave gen or manual loom"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-06-29T07:29:06Z",
        "message": "Expose ability to do normal gen, misc fixes\n\n- Separate reroll button into crayon (normal gen) and pen (weave)\n- Fix bug where generation from root node didn't work\n- Change rendering decision where nodes without scores were shown as 100% probable"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-06-25T14:53:39Z",
        "message": "Expose weave hyperparams and fix editor for root node"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-06-20T15:03:37Z",
        "message": "Refactor MiniHF client data structure to be a tree"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-06-14T08:13:43Z",
        "message": "Clear CUDA cache when tuning reward model"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-06-13T08:00:11Z",
        "message": "Fix reward model loading"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-06-13T01:08:32Z",
        "message": "Fix context handling bugs in evaluator"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-06-05T23:14:33Z",
        "message": "Make inference server actually use tuned model"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-06-05T13:23:54Z",
        "message": "Add ability to lora tune evaluator model"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-06-04T07:59:57Z",
        "message": "Merge in Crowson's weave algorithm and zero-shot evaluation"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-05-28T10:21:21Z",
        "message": "Don't try to open directories in a zip as files"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-05-28T05:06:59Z",
        "message": "Fix reward head tuning on Mac OS X"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-05-27T09:12:07Z",
        "message": "Fix bug where only new text segment is stored during expansion"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-05-27T02:38:07Z",
        "message": "Fix overflow in fp16 when computing mean embedding during training"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-05-26T07:48:30Z",
        "message": "Exclude padding tokens from mean embedding when training reward models"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-05-26T06:11:10Z",
        "message": "Create reward_heads directory during training if not exists"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-05-26T05:34:35Z",
        "message": "Add ability to train reward head with zip upload within interface"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-05-25T23:29:12Z",
        "message": "Make the MLP actually an MLP"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "minihf_infer.py",
        "commit_date": "2023-05-25T19:55:44Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "find_ae_scale.py",
        "commit_date": "2023-10-05T22:37:09Z",
        "message": "Add ae_scale finder"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "sft_generator.py",
        "commit_date": "2023-07-26T03:16:08Z",
        "message": "Decode text files read from user datasets"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "sft_generator.py",
        "commit_date": "2023-07-11T05:00:00Z",
        "message": "Change default SFT generator quantization to 4 bits"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "sft_generator.py",
        "commit_date": "2023-07-11T03:55:02Z",
        "message": "Actually load the (roughly) 10 megabyte minimum data for finetune"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "sft_generator.py",
        "commit_date": "2023-07-10T13:00:23Z",
        "message": "First draft of generator supervised finetune script"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "sft_evaluator.py",
        "commit_date": "2023-07-14T20:56:59Z",
        "message": "Use RiversHaveWings/minihf_evaluator_openllama_7b as evaluator"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "sft_evaluator.py",
        "commit_date": "2023-07-12T00:52:15Z",
        "message": "Rename make_evaluator.py to sft_evaluator.py"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "rlaif_generator.py",
        "commit_date": "2023-11-24T22:10:51Z",
        "message": "Fix gradient checkpointing in RLAIF script"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "rlaif_generator.py",
        "commit_date": "2023-07-26T21:54:40Z",
        "message": "Add multi-gpu RLAIF tuning"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "rlaif_generator.py",
        "commit_date": "2023-07-21T22:24:47Z",
        "message": "Add RLAIF tuning"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "train_vae_router.py",
        "commit_date": "2023-10-03T10:54:08Z",
        "message": "Make default model Mistral"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "train_vae_router.py",
        "commit_date": "2023-10-03T10:48:59Z",
        "message": "Add BigVAE inference code"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "train_vae_router.py",
        "commit_date": "2023-09-29T17:59:56Z",
        "message": "Add MoE VAE training code"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "train_vae_overlap.py",
        "commit_date": "2023-10-11T13:03:32Z",
        "message": "Update training code and add tuned AdaVAE samplers"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "train_vae_overlap.py",
        "commit_date": "2023-10-03T10:54:08Z",
        "message": "Make default model Mistral"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "train_vae_overlap.py",
        "commit_date": "2023-10-03T10:48:59Z",
        "message": "Add BigVAE inference code"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "train_vae_overlap.py",
        "commit_date": "2023-09-29T17:59:56Z",
        "message": "Add MoE VAE training code"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "rlaif_generator_dpo.py",
        "commit_date": "2023-12-02T16:26:33Z",
        "message": "Use evaluator logit scale argument"
    },
    {
        "repo_url": "github.com/JD-P/minihf",
        "filepath": "rlaif_generator_dpo.py",
        "commit_date": "2023-12-02T16:14:39Z",
        "message": "Add the Direct Preference Optimization RLAIF generator script"
    },
    {
        "repo_url": "github.com/DRSY/EMO",
        "filepath": "continual_finetuning/merge_lora.py",
        "commit_date": "2023-10-22T07:14:27Z",
        "message": "update code structure"
    },
    {
        "repo_url": "github.com/khaimt/qa_expert",
        "filepath": "train/merge_weight.py",
        "commit_date": "2023-11-12T16:50:13Z",
        "message": "upload reame and fix some minor things"
    },
    {
        "repo_url": "github.com/khaimt/qa_expert",
        "filepath": "train/merge_weight.py",
        "commit_date": "2023-11-03T01:59:29Z",
        "message": "change added_tokens --> add_special_tokens"
    },
    {
        "repo_url": "github.com/khaimt/qa_expert",
        "filepath": "train/merge_weight.py",
        "commit_date": "2023-10-13T16:16:53Z",
        "message": "update readme and inference"
    },
    {
        "repo_url": "github.com/khaimt/qa_expert",
        "filepath": "train/merge_weight.py",
        "commit_date": "2023-10-12T13:30:20Z",
        "message": "change the structure"
    },
    {
        "repo_url": "github.com/jayli/langchain-ChatGLM",
        "filepath": "models/chatglm_llm.py",
        "commit_date": "2023-05-27T19:21:21Z",
        "message": "add all"
    },
    {
        "repo_url": "github.com/jayli/langchain-ChatGLM",
        "filepath": "models/chatglm_llm.py",
        "commit_date": "2023-05-27T05:32:34Z",
        "message": "add all"
    },
    {
        "repo_url": "github.com/jayli/langchain-ChatGLM",
        "filepath": "models/chatglm_llm.py",
        "commit_date": "2023-05-27T04:23:27Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/jayli/langchain-ChatGLM",
        "filepath": "models/loader/loader.py",
        "commit_date": "2023-05-27T04:23:27Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/seudl/JurisLMs",
        "filepath": "ailawyer/demo/generate.py",
        "commit_date": "2023-06-16T08:06:05Z",
        "message": "fix"
    },
    {
        "repo_url": "github.com/seudl/JurisLMs",
        "filepath": "ailawyer/demo/generate.py",
        "commit_date": "2023-05-20T10:21:13Z",
        "message": "\u6a21\u578b\u63a8\u7406\u548c\u90e8\u7f72"
    },
    {
        "repo_url": "github.com/seudl/JurisLMs",
        "filepath": "ailawyer/demo/web_demo_llama_13B.py",
        "commit_date": "2023-05-20T10:21:13Z",
        "message": "\u6a21\u578b\u63a8\u7406\u548c\u90e8\u7f72"
    },
    {
        "repo_url": "github.com/seudl/JurisLMs",
        "filepath": "ailawyer/utils/merge_llama_with_chinese_lora_to_hf.py",
        "commit_date": "2023-05-30T16:38:31Z",
        "message": "add data and test"
    },
    {
        "repo_url": "github.com/debjitpaul/refiner",
        "filepath": "src/refiner.py",
        "commit_date": "2023-04-24T20:57:14Z",
        "message": "add lora"
    },
    {
        "repo_url": "github.com/debjitpaul/refiner",
        "filepath": "src/refiner.py",
        "commit_date": "2023-04-24T17:26:42Z",
        "message": "added new code"
    },
    {
        "repo_url": "github.com/DemoGit4LIANG/Chat2Anything",
        "filepath": "Chat2Anything/fastchat/model/apply_lora.py",
        "commit_date": "2023-08-04T05:19:14Z",
        "message": "v0.1"
    },
    {
        "repo_url": "github.com/brando90/ultimate-utils",
        "filepath": "tutorials_for_myself/my_hf_hugging_face_pg/opt/opt_6.7b/finetune_opt_bnb_peft.py",
        "commit_date": "2023-02-16T01:16:12Z",
        "message": "opt 6.7B fine tuning"
    },
    {
        "repo_url": "github.com/grasses/PromptCARE",
        "filepath": "soft_prompt/training/trainer.py",
        "commit_date": "2023-11-28T16:39:26Z",
        "message": "add new version"
    },
    {
        "repo_url": "github.com/LC1332/Luotuo-QA",
        "filepath": "app/infer.py",
        "commit_date": "2023-04-24T14:43:01Z",
        "message": "add continue_generate"
    },
    {
        "repo_url": "github.com/LC1332/Luotuo-QA",
        "filepath": "app/infer.py",
        "commit_date": "2023-04-22T13:32:22Z",
        "message": "add app/infer.ipynb"
    },
    {
        "repo_url": "github.com/LC1332/Luotuo-QA",
        "filepath": "app/infer.py",
        "commit_date": "2023-04-22T03:59:33Z",
        "message": "clean app code"
    },
    {
        "repo_url": "github.com/declare-lab/red-instruct",
        "filepath": "starling_training/fastchat/model/apply_lora.py",
        "commit_date": "2023-08-22T02:03:59Z",
        "message": "add codes and data"
    },
    {
        "repo_url": "github.com/declare-lab/red-instruct",
        "filepath": "starling_training/fastchat/model/model_adapter.py",
        "commit_date": "2023-08-22T02:03:59Z",
        "message": "add codes and data"
    },
    {
        "repo_url": "github.com/YeonwooSung/ai_book",
        "filepath": "LLMs/llama2/llama2-fine-tune/merge_peft_adapters.py",
        "commit_date": "2024-03-02T12:08:04Z",
        "message": "feat: Add scripts for finetuning llama-2"
    },
    {
        "repo_url": "github.com/lighttransport/japanese-llama-experiment",
        "filepath": "sandbox/eval_checkpoint.py",
        "commit_date": "2023-10-31T19:03:34Z",
        "message": "Pass tokenized inputs object."
    },
    {
        "repo_url": "github.com/lighttransport/japanese-llama-experiment",
        "filepath": "sandbox/eval_checkpoint.py",
        "commit_date": "2023-10-31T17:15:33Z",
        "message": "eval from checkpoint weights."
    },
    {
        "repo_url": "github.com/lighttransport/japanese-llama-experiment",
        "filepath": "sandbox/eval_checkpoint.py",
        "commit_date": "2023-10-28T11:41:53Z",
        "message": "update incr pretrain script."
    },
    {
        "repo_url": "github.com/lighttransport/japanese-llama-experiment",
        "filepath": "10_incremental_pretrain/publish_checkpoint.py",
        "commit_date": "2023-11-05T17:38:05Z",
        "message": "hf -> llama.cpp GGUF convert."
    },
    {
        "repo_url": "github.com/lighttransport/japanese-llama-experiment",
        "filepath": "10_incremental_pretrain/publish_checkpoint.py",
        "commit_date": "2023-11-05T13:17:48Z",
        "message": "Publish script for checkpoint data."
    },
    {
        "repo_url": "github.com/lizhongyi123/llama2_chat_fine",
        "filepath": "merge.py",
        "commit_date": "2023-11-09T09:50:15Z",
        "message": "redme"
    },
    {
        "repo_url": "github.com/lizhongyi123/llama2_chat_fine",
        "filepath": "merge.py",
        "commit_date": "2023-11-08T17:15:08Z",
        "message": "1"
    },
    {
        "repo_url": "github.com/lizhongyi123/llama2_chat_fine",
        "filepath": "inference/model_utils.py",
        "commit_date": "2023-11-08T17:15:08Z",
        "message": "1"
    },
    {
        "repo_url": "github.com/pengwei-iie/Llama2-Chinese",
        "filepath": "examples/chat_gradio_no_merge.py",
        "commit_date": "2023-10-18T15:20:13Z",
        "message": "message"
    },
    {
        "repo_url": "github.com/pengwei-iie/Llama2-Chinese",
        "filepath": "train/merge_peft_model/merge_peft_adapter.py",
        "commit_date": "2023-10-18T15:20:13Z",
        "message": "message"
    },
    {
        "repo_url": "github.com/pengwei-iie/Llama2-Chinese",
        "filepath": "train/merge_peft_model/merge_muilt_peft_adapter.py",
        "commit_date": "2023-10-18T15:20:13Z",
        "message": "message"
    },
    {
        "repo_url": "github.com/ssbuild/deep_training",
        "filepath": "src/deep_training/trainer/hf/trainer.py",
        "commit_date": "2023-11-13T09:25:18Z",
        "message": "fix hf trainer\n\nSigned-off-by: ssbuild <462304@qq.cn>"
    },
    {
        "repo_url": "github.com/ssbuild/deep_training",
        "filepath": "src/deep_training/trainer/hf/trainer.py",
        "commit_date": "2023-10-10T06:25:40Z",
        "message": "fix baichuan gradient_checkpointing\n\nSigned-off-by: ssbuild <462304@qq.cn>"
    },
    {
        "repo_url": "github.com/ssbuild/deep_training",
        "filepath": "src/deep_training/trainer/hf/trainer.py",
        "commit_date": "2023-10-10T03:18:39Z",
        "message": "update\n\nSigned-off-by: ssbuild <462304@qq.cn>"
    },
    {
        "repo_url": "github.com/ssbuild/deep_training",
        "filepath": "src/deep_training/trainer/hf/trainer.py",
        "commit_date": "2023-10-10T02:26:08Z",
        "message": "fix some new bug\n\nSigned-off-by: ssbuild <462304@qq.cn>"
    },
    {
        "repo_url": "github.com/ssbuild/deep_training",
        "filepath": "src/deep_training/trainer/hf/trainer.py",
        "commit_date": "2023-10-09T11:40:49Z",
        "message": "fix hf and acc\n\nSigned-off-by: ssbuild <462304@qq.com>"
    },
    {
        "repo_url": "github.com/ssbuild/deep_training",
        "filepath": "src/deep_training/trainer/hf/trainer.py",
        "commit_date": "2023-09-27T00:52:35Z",
        "message": "fix hf trainer save min weight\n\nSigned-off-by: ssbuild <462304@qq.cn>"
    },
    {
        "repo_url": "github.com/ssbuild/deep_training",
        "filepath": "src/deep_training/trainer/hf/trainer.py",
        "commit_date": "2023-09-26T16:48:27Z",
        "message": "update\n\nSigned-off-by: ssbuild <462304@qq.com>"
    },
    {
        "repo_url": "github.com/ssbuild/deep_training",
        "filepath": "src/deep_training/trainer/hf/trainer.py",
        "commit_date": "2023-09-26T16:28:17Z",
        "message": "fix hf trainer\n\nSigned-off-by: ssbuild <462304@qq.com>"
    },
    {
        "repo_url": "github.com/ssbuild/deep_training",
        "filepath": "src/deep_training/trainer/hf/trainer.py",
        "commit_date": "2023-09-26T06:04:24Z",
        "message": "0.2.4\n\nSigned-off-by: ssbuild <462304@qq.cn>"
    },
    {
        "repo_url": "github.com/ssbuild/deep_training",
        "filepath": "src/deep_training/trainer/hf/trainer.py",
        "commit_date": "2023-09-26T04:26:54Z",
        "message": "support hf\n\nSigned-off-by: ssbuild <462304@qq.cn>"
    },
    {
        "repo_url": "github.com/ssbuild/deep_training",
        "filepath": "src/deep_training/trainer/hf/trainer.py",
        "commit_date": "2023-09-26T02:41:16Z",
        "message": "support for hf trainer\n\nSigned-off-by: ssbuild <462304@qq.cn>"
    },
    {
        "repo_url": "github.com/ssbuild/deep_training",
        "filepath": "src/deep_training/trainer/hf/trainer.py",
        "commit_date": "2023-09-26T01:28:53Z",
        "message": "update hf\n\nSigned-off-by: ssbuild <462304@qq.cn>"
    },
    {
        "repo_url": "github.com/ssbuild/deep_training",
        "filepath": "src/deep_training/trainer/hf/trainer.py",
        "commit_date": "2023-09-25T16:36:48Z",
        "message": "support hf\n\nSigned-off-by: ssbuild <462304@qq.com>"
    },
    {
        "repo_url": "github.com/ssbuild/deep_training",
        "filepath": "src/deep_training/trainer/hf/trainer.py",
        "commit_date": "2023-09-25T09:20:27Z",
        "message": "update\n\nSigned-off-by: ssbuild <462304@qq.cn>"
    },
    {
        "repo_url": "github.com/ssbuild/deep_training",
        "filepath": "src/deep_training/trainer/hf/trainer.py",
        "commit_date": "2023-08-23T04:03:02Z",
        "message": "0.2.0\n\nSigned-off-by: ssbuild <462304@qq.cn>"
    },
    {
        "repo_url": "github.com/ssbuild/deep_training",
        "filepath": "src/deep_training/trainer/hf/trainer.py",
        "commit_date": "2023-07-31T06:53:06Z",
        "message": "internal change\n\nSigned-off-by: ssbuild <462304@qq.cn>"
    },
    {
        "repo_url": "github.com/ssbuild/deep_training",
        "filepath": "src/deep_training/nlp/models/petl/lora/model.py",
        "commit_date": "2023-11-13T04:26:07Z",
        "message": "0.2.8\n\nSigned-off-by: ssbuild <462304@qq.cn>"
    },
    {
        "repo_url": "github.com/ssbuild/deep_training",
        "filepath": "src/deep_training/nlp/models/petl/lora/model.py",
        "commit_date": "2023-11-13T01:40:21Z",
        "message": "0.3.0 rc\n\nSigned-off-by: ssbuild <462304@qq.cn>"
    },
    {
        "repo_url": "github.com/AbaciNLP/InvestLM",
        "filepath": "inference.py",
        "commit_date": "2023-09-14T13:53:40Z",
        "message": "create project"
    },
    {
        "repo_url": "github.com/cauyxy/bilivideos",
        "filepath": "llm-train/inference.py",
        "commit_date": "2023-06-22T14:31:28Z",
        "message": "finish llm-fintunig"
    },
    {
        "repo_url": "github.com/cauyxy/bilivideos",
        "filepath": "llm-train/merge_weights.py",
        "commit_date": "2023-06-22T14:31:28Z",
        "message": "finish llm-fintunig"
    },
    {
        "repo_url": "github.com/avocardio/Zicklein",
        "filepath": "predict.py",
        "commit_date": "2023-05-20T22:01:24Z",
        "message": "Added scripts, data, sources and assets"
    },
    {
        "repo_url": "github.com/avocardio/Zicklein",
        "filepath": "generate.py",
        "commit_date": "2023-05-20T22:01:24Z",
        "message": "Added scripts, data, sources and assets"
    },
    {
        "repo_url": "github.com/avocardio/Zicklein",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-05-20T22:01:24Z",
        "message": "Added scripts, data, sources and assets"
    },
    {
        "repo_url": "github.com/avocardio/Zicklein",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-05-20T22:01:24Z",
        "message": "Added scripts, data, sources and assets"
    },
    {
        "repo_url": "github.com/xufangzhi/Symbol-LLM",
        "filepath": "demo-webui/modules/LoRA.py",
        "commit_date": "2024-01-02T07:57:53Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/zhiweihu1103/AgriMa",
        "filepath": "src/llmtuner/model/adapter.py",
        "commit_date": "2024-02-21T11:07:25Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/SYSU-MUCFC-FinTech-Research-Center/ZhiLu",
        "filepath": "scripts/merge_llama_with_chinese_lora_low_mem.py",
        "commit_date": "2023-10-28T03:56:16Z",
        "message": "Update merge_llama_with_chinese_lora_low_mem.py"
    },
    {
        "repo_url": "github.com/SYSU-MUCFC-FinTech-Research-Center/ZhiLu",
        "filepath": "scripts/merge_llama_with_chinese_lora_low_mem.py",
        "commit_date": "2023-10-28T03:18:20Z",
        "message": "add merge-scripts"
    },
    {
        "repo_url": "github.com/ZetangForward/Detox-CoT",
        "filepath": "train_cmd/peft_model_clm.py",
        "commit_date": "2024-02-20T07:03:40Z",
        "message": "cmd"
    },
    {
        "repo_url": "github.com/ZetangForward/Detox-CoT",
        "filepath": "utils/continuation_inference.py",
        "commit_date": "2024-02-20T07:03:40Z",
        "message": "cmd"
    },
    {
        "repo_url": "github.com/ZetangForward/Detox-CoT",
        "filepath": "utils/continuation_inference.py",
        "commit_date": "2023-11-09T14:41:45Z",
        "message": "add span-cnn"
    },
    {
        "repo_url": "github.com/huangb23/VTimeLLM",
        "filepath": "vtimellm/model/builder.py",
        "commit_date": "2024-01-01T13:28:42Z",
        "message": "vtimellm_chatglm training"
    },
    {
        "repo_url": "github.com/huangb23/VTimeLLM",
        "filepath": "vtimellm/model/builder.py",
        "commit_date": "2023-12-04T15:32:58Z",
        "message": "inference"
    },
    {
        "repo_url": "github.com/runpod/serverless-workers",
        "filepath": "workers/Dolly-Tuner/rp_handler.py",
        "commit_date": "2023-04-23T22:56:03Z",
        "message": "no pre-loaded model"
    },
    {
        "repo_url": "github.com/runpod/serverless-workers",
        "filepath": "workers/Dolly-Tuner/rp_handler.py",
        "commit_date": "2023-04-22T18:46:15Z",
        "message": "Update rp_handler.py"
    },
    {
        "repo_url": "github.com/runpod/serverless-workers",
        "filepath": "workers/Dolly-Tuner/rp_handler.py",
        "commit_date": "2023-04-22T18:44:25Z",
        "message": "Update rp_handler.py"
    },
    {
        "repo_url": "github.com/runpod/serverless-workers",
        "filepath": "workers/Dolly-Tuner/rp_handler.py",
        "commit_date": "2023-04-22T17:27:09Z",
        "message": "upload trained model when done"
    },
    {
        "repo_url": "github.com/runpod/serverless-workers",
        "filepath": "workers/Dolly-Tuner/rp_handler.py",
        "commit_date": "2023-04-22T17:01:29Z",
        "message": "Update rp_handler.py"
    },
    {
        "repo_url": "github.com/runpod/serverless-workers",
        "filepath": "workers/Dolly-Tuner/rp_handler.py",
        "commit_date": "2023-04-22T16:56:01Z",
        "message": "Update rp_handler.py"
    },
    {
        "repo_url": "github.com/runpod/serverless-workers",
        "filepath": "workers/Dolly-Tuner/rp_handler.py",
        "commit_date": "2023-04-22T16:51:14Z",
        "message": "Update rp_handler.py"
    },
    {
        "repo_url": "github.com/runpod/serverless-workers",
        "filepath": "workers/Dolly-Tuner/rp_handler.py",
        "commit_date": "2023-04-22T16:43:35Z",
        "message": "feat: added Dolly-Tuner"
    },
    {
        "repo_url": "github.com/hkust-nlp/PEM_composition",
        "filepath": "AlpacaLoRAcomposition/generate.py",
        "commit_date": "2023-11-26T09:40:14Z",
        "message": "Release Alpaca-LoRA composition exps code"
    },
    {
        "repo_url": "github.com/hkust-nlp/PEM_composition",
        "filepath": "AlpacaLoRAcomposition/generate_in_batch.py",
        "commit_date": "2023-11-26T09:40:14Z",
        "message": "Release Alpaca-LoRA composition exps code"
    },
    {
        "repo_url": "github.com/hkust-nlp/PEM_composition",
        "filepath": "AlpacaLoRAcomposition/export_hf_checkpoint.py",
        "commit_date": "2023-11-26T09:40:14Z",
        "message": "Release Alpaca-LoRA composition exps code"
    },
    {
        "repo_url": "github.com/hkust-nlp/PEM_composition",
        "filepath": "AlpacaLoRAcomposition/export_state_dict_checkpoint.py",
        "commit_date": "2023-11-26T09:40:14Z",
        "message": "Release Alpaca-LoRA composition exps code"
    },
    {
        "repo_url": "github.com/serp-ai/LLaMA-8bit-LoRA",
        "filepath": "merge_adapter_weights.py",
        "commit_date": "2023-03-22T10:10:30Z",
        "message": "Replace automodel and tokenizer with llama version"
    },
    {
        "repo_url": "github.com/serp-ai/LLaMA-8bit-LoRA",
        "filepath": "merge_adapter_weights.py",
        "commit_date": "2023-03-20T22:23:47Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/weijiang2023/algmon-kb",
        "filepath": "script/fine_tune.py",
        "commit_date": "2024-02-01T09:05:39Z",
        "message": "ADD script for download lookbooks"
    },
    {
        "repo_url": "github.com/A-baoYang/LLM-Finetune-Guide",
        "filepath": "efficient-finetune/lora/serve/model.py",
        "commit_date": "2023-05-01T04:12:58Z",
        "message": "[feature] Add Efficient Finetune Method: LoRA"
    },
    {
        "repo_url": "github.com/A-baoYang/LLM-Finetune-Guide",
        "filepath": "efficient-finetune/lora/peftv2/peft_model.py",
        "commit_date": "2024-02-07T02:59:40Z",
        "message": "[LLaMA2] Add lora for sft LLaMA2"
    },
    {
        "repo_url": "github.com/A-baoYang/LLM-Finetune-Guide",
        "filepath": "efficient-finetune/lora/peftv2/tuners/lora.py",
        "commit_date": "2024-02-07T02:59:40Z",
        "message": "[LLaMA2] Add lora for sft LLaMA2"
    },
    {
        "repo_url": "github.com/A-baoYang/LLM-Finetune-Guide",
        "filepath": "efficient-finetune/lora/validation/generate.py",
        "commit_date": "2023-05-02T10:08:46Z",
        "message": "[feature] Add LoRA validation code & cpp format convert code"
    },
    {
        "repo_url": "github.com/A-baoYang/LLM-Finetune-Guide",
        "filepath": "cpp_format_convert/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-07-10T05:59:27Z",
        "message": "[feature] Arranged Chinese-LLaMA-Alpaca model merge method and llama.cpp ggjt format convert methods"
    },
    {
        "repo_url": "github.com/A-baoYang/LLM-Finetune-Guide",
        "filepath": "cpp_format_convert/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-05-02T10:08:46Z",
        "message": "[feature] Add LoRA validation code & cpp format convert code"
    },
    {
        "repo_url": "github.com/A-baoYang/LLM-Finetune-Guide",
        "filepath": "efficient-finetune/lora/validation/batch_generate.py",
        "commit_date": "2023-08-27T11:05:54Z",
        "message": "[mix] Runnable updates"
    },
    {
        "repo_url": "github.com/A-baoYang/LLM-Finetune-Guide",
        "filepath": "efficient-finetune/lora/validation/batch_generate.py",
        "commit_date": "2023-05-02T10:08:46Z",
        "message": "[feature] Add LoRA validation code & cpp format convert code"
    },
    {
        "repo_url": "github.com/A-baoYang/LLM-Finetune-Guide",
        "filepath": "efficient-finetune/lora/peftv2/tuners/prompt_tuning.py",
        "commit_date": "2024-02-07T02:59:40Z",
        "message": "[LLaMA2] Add lora for sft LLaMA2"
    },
    {
        "repo_url": "github.com/mu-cai/ViP-LLaVA",
        "filepath": "llava/model/builder.py",
        "commit_date": "2023-12-03T23:54:12Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-api",
        "filepath": "src/model/llama/__init__.py",
        "commit_date": "2023-05-29T10:06:54Z",
        "message": "feat(blip2chatglm&chatglm): new generation code"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-api",
        "filepath": "src/model/llama/__init__.py",
        "commit_date": "2023-05-08T12:55:13Z",
        "message": "Squashed commit of the following:\n\ncommit 614367c8518a697987ba25b6443de96f5c70c66d\nAuthor: shwu <xipotatonium@outlook.com>\nDate:   Mon May 8 12:53:33 2023 +0000\n\n    doc\n\ncommit 668f5ce22b1de0f791e5f13cb5254ef04be7c4ea\nAuthor: shwu <xipotatonium@outlook.com>\nDate:   Thu Apr 20 18:00:04 2023 +0800\n\n    feat: more lora models\n\n    * InstructGLM\n    * raw llama\n    * fix api and more generation configs\n    * new examples\n\ncommit 4dbc483dade0914514d0be8bf106e42dbbeb3688\nAuthor: Yongliang Shen <syl@zju.edu.cn>\nDate:   Wed Apr 19 20:59:17 2023 +0800\n\n    update API\n\ncommit d197f51776d7dd00bdfb31a73846b0dca2a16cdb\nMerge: 26919f1 f0816b3\nAuthor: Yongliang Shen <syl@zju.edu.cn>\nDate:   Tue Apr 18 20:13:58 2023 +0800\n\n    Merge branch 'main' of https://github.com/XiPotatonium/chatbot-api into dev\n\ncommit 26919f1aa55a2d82a35f981f1932521279d99661\nAuthor: shwu <xipotatonium@outlook.com>\nDate:   Tue Apr 18 20:05:37 2023 +0800\n\n    refactor: refactor ChatModel interface"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-api",
        "filepath": "src/model/llama/__init__.py",
        "commit_date": "2023-04-08T16:59:27Z",
        "message": "feat: llama and chatglm"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-api",
        "filepath": "src/model/chatglm/__init__.py",
        "commit_date": "2023-05-29T10:06:54Z",
        "message": "feat(blip2chatglm&chatglm): new generation code"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-api",
        "filepath": "src/model/chatglm/__init__.py",
        "commit_date": "2023-05-08T12:55:13Z",
        "message": "Squashed commit of the following:\n\ncommit 614367c8518a697987ba25b6443de96f5c70c66d\nAuthor: shwu <xipotatonium@outlook.com>\nDate:   Mon May 8 12:53:33 2023 +0000\n\n    doc\n\ncommit 668f5ce22b1de0f791e5f13cb5254ef04be7c4ea\nAuthor: shwu <xipotatonium@outlook.com>\nDate:   Thu Apr 20 18:00:04 2023 +0800\n\n    feat: more lora models\n\n    * InstructGLM\n    * raw llama\n    * fix api and more generation configs\n    * new examples\n\ncommit 4dbc483dade0914514d0be8bf106e42dbbeb3688\nAuthor: Yongliang Shen <syl@zju.edu.cn>\nDate:   Wed Apr 19 20:59:17 2023 +0800\n\n    update API\n\ncommit d197f51776d7dd00bdfb31a73846b0dca2a16cdb\nMerge: 26919f1 f0816b3\nAuthor: Yongliang Shen <syl@zju.edu.cn>\nDate:   Tue Apr 18 20:13:58 2023 +0800\n\n    Merge branch 'main' of https://github.com/XiPotatonium/chatbot-api into dev\n\ncommit 26919f1aa55a2d82a35f981f1932521279d99661\nAuthor: shwu <xipotatonium@outlook.com>\nDate:   Tue Apr 18 20:05:37 2023 +0800\n\n    refactor: refactor ChatModel interface"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-api",
        "filepath": "src/model/chatglm/__init__.py",
        "commit_date": "2023-04-15T10:58:18Z",
        "message": "refactor: for new blip2chatglm"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-api",
        "filepath": "src/model/chatglm/__init__.py",
        "commit_date": "2023-04-11T12:16:26Z",
        "message": "feat: add model scheduling"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-api",
        "filepath": "src/model/chatglm/__init__.py",
        "commit_date": "2023-04-09T03:52:10Z",
        "message": "fix(iter_message):"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-api",
        "filepath": "src/model/chatglm/__init__.py",
        "commit_date": "2023-04-08T16:59:27Z",
        "message": "feat: llama and chatglm"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-api",
        "filepath": "src/model/blip2chatglm/__init__.py",
        "commit_date": "2023-05-29T10:06:54Z",
        "message": "feat(blip2chatglm&chatglm): new generation code"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-api",
        "filepath": "src/model/blip2chatglm/__init__.py",
        "commit_date": "2023-05-08T12:55:13Z",
        "message": "Squashed commit of the following:\n\ncommit 614367c8518a697987ba25b6443de96f5c70c66d\nAuthor: shwu <xipotatonium@outlook.com>\nDate:   Mon May 8 12:53:33 2023 +0000\n\n    doc\n\ncommit 668f5ce22b1de0f791e5f13cb5254ef04be7c4ea\nAuthor: shwu <xipotatonium@outlook.com>\nDate:   Thu Apr 20 18:00:04 2023 +0800\n\n    feat: more lora models\n\n    * InstructGLM\n    * raw llama\n    * fix api and more generation configs\n    * new examples\n\ncommit 4dbc483dade0914514d0be8bf106e42dbbeb3688\nAuthor: Yongliang Shen <syl@zju.edu.cn>\nDate:   Wed Apr 19 20:59:17 2023 +0800\n\n    update API\n\ncommit d197f51776d7dd00bdfb31a73846b0dca2a16cdb\nMerge: 26919f1 f0816b3\nAuthor: Yongliang Shen <syl@zju.edu.cn>\nDate:   Tue Apr 18 20:13:58 2023 +0800\n\n    Merge branch 'main' of https://github.com/XiPotatonium/chatbot-api into dev\n\ncommit 26919f1aa55a2d82a35f981f1932521279d99661\nAuthor: shwu <xipotatonium@outlook.com>\nDate:   Tue Apr 18 20:05:37 2023 +0800\n\n    refactor: refactor ChatModel interface"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-api",
        "filepath": "src/model/blip2chatglm/__init__.py",
        "commit_date": "2023-04-15T10:58:18Z",
        "message": "refactor: for new blip2chatglm"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-api",
        "filepath": "src/model/blip2chatglm/__init__.py",
        "commit_date": "2023-04-11T12:16:26Z",
        "message": "feat: add model scheduling"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-api",
        "filepath": "src/model/blip2chatglm/__init__.py",
        "commit_date": "2023-04-09T03:52:10Z",
        "message": "fix(iter_message):"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-api",
        "filepath": "src/model/blip2chatglm/__init__.py",
        "commit_date": "2023-04-08T16:59:27Z",
        "message": "feat: llama and chatglm"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-api",
        "filepath": "src/model/blip2chatglm/__init__.py",
        "commit_date": "2023-04-08T14:36:11Z",
        "message": "feat: basically ok"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-api",
        "filepath": "src/model/blip2chatglm/__init__.py",
        "commit_date": "2023-04-07T07:48:34Z",
        "message": "feat: connection ok"
    },
    {
        "repo_url": "github.com/usail-hkust/LLMTSCS",
        "filepath": "finetune/merge_lora.py",
        "commit_date": "2024-02-13T16:02:57Z",
        "message": "update LLMLight"
    },
    {
        "repo_url": "github.com/invoke-ai/invoke-training",
        "filepath": "src/invoke_training/_shared/stable_diffusion/lora_checkpoint_utils.py",
        "commit_date": "2024-01-29T15:20:35Z",
        "message": "Move _shared/ up a directory."
    },
    {
        "repo_url": "github.com/kukrishna/evidence_inspector",
        "filepath": "backend/serve_seq2seq_qlora.py",
        "commit_date": "2023-08-28T02:35:35Z",
        "message": "added intro modal showing config of backend models"
    },
    {
        "repo_url": "github.com/kukrishna/evidence_inspector",
        "filepath": "backend/serve_seq2seq_qlora.py",
        "commit_date": "2023-08-25T04:10:15Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/JuneYaooo/medical_kb_chatbot",
        "filepath": "finetune/pulse/src/trainer.py",
        "commit_date": "2023-06-28T13:40:55Z",
        "message": "v0"
    },
    {
        "repo_url": "github.com/JuneYaooo/medical_kb_chatbot",
        "filepath": "loader/models/loader/loader.py",
        "commit_date": "2023-07-03T03:08:31Z",
        "message": "fix: input not on same device"
    },
    {
        "repo_url": "github.com/JuneYaooo/medical_kb_chatbot",
        "filepath": "loader/models/loader/loader.py",
        "commit_date": "2023-06-28T13:40:55Z",
        "message": "v0"
    },
    {
        "repo_url": "github.com/opendatalab/HA-DPO",
        "filepath": "ha_dpo/models/llava-v1_5/train_dpo.py",
        "commit_date": "2024-01-30T07:29:53Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/opendatalab/HA-DPO",
        "filepath": "ha_dpo/models/minigpt4/merge_peft_adapter.py",
        "commit_date": "2024-01-30T07:29:53Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/opendatalab/HA-DPO",
        "filepath": "ha_dpo/models/llava-v1_5/llava/model/builder.py",
        "commit_date": "2024-01-30T07:29:53Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/opendatalab/HA-DPO",
        "filepath": "ha_dpo/models/instructblip/merge_peft_adapter.py",
        "commit_date": "2024-01-30T07:29:53Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/OpenMOSS/Say-I-Dont-Know",
        "filepath": "src/Inference/infer_llama.py",
        "commit_date": "2024-01-29T16:32:17Z",
        "message": "add training methods"
    },
    {
        "repo_url": "github.com/OpenMOSS/Say-I-Dont-Know",
        "filepath": "src/llama_recipes/inference/model_utils.py",
        "commit_date": "2024-01-29T16:32:17Z",
        "message": "add training methods"
    },
    {
        "repo_url": "github.com/batmen-lab/BioMANIA",
        "filepath": "src/models/model.py",
        "commit_date": "2024-01-27T23:01:43Z",
        "message": "update openai version, fix details in chitchat"
    },
    {
        "repo_url": "github.com/batmen-lab/BioMANIA",
        "filepath": "src/models/model.py",
        "commit_date": "2024-01-18T16:30:23Z",
        "message": "update instruction generation setting"
    },
    {
        "repo_url": "github.com/batmen-lab/BioMANIA",
        "filepath": "src/models/model.py",
        "commit_date": "2024-01-16T23:33:07Z",
        "message": "minor update for prompt and new libs"
    },
    {
        "repo_url": "github.com/batmen-lab/BioMANIA",
        "filepath": "src/models/model.py",
        "commit_date": "2023-12-06T15:56:44Z",
        "message": "minor update before v1.1.9"
    },
    {
        "repo_url": "github.com/batmen-lab/BioMANIA",
        "filepath": "src/models/model.py",
        "commit_date": "2023-12-01T14:00:43Z",
        "message": "v1.1.7 update"
    },
    {
        "repo_url": "github.com/batmen-lab/BioMANIA",
        "filepath": "src/models/model.py",
        "commit_date": "2023-11-06T13:27:51Z",
        "message": "add chat2jupyter for converting chat json into jupyter notebook"
    },
    {
        "repo_url": "github.com/batmen-lab/BioMANIA",
        "filepath": "src/models/model.py",
        "commit_date": "2023-11-03T14:45:20Z",
        "message": "add source code"
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/machine_learning/hugging_face_peft_test.py",
        "commit_date": "2023-03-23T02:48:15Z",
        "message": "[Chore] hugging_face_test.py was moved from sw_dev/python/rnd/test/language_processing to sw_dev/python/rnd/test/machine_learning.\n[Update] The examples for Hugging Face PEFT library were moved from sw_dev/python/rnd/test/machine_learning/hugging_face_test.py to sw_dev/python/rnd/test/machine_learning/hugging_face_peft_test.py."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2024-02-27T11:11:47Z",
        "message": "[Update] An example for Gemma model was initially added.\n[Update] An example for Transformer Reinforcement Learning (TRL) library to train small LLMs (sLLMs) was initially added."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2024-02-21T12:45:57Z",
        "message": "[Update] Examples for DINO and DPT models were implemented in hugging_face_transformers_test.py."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2024-02-16T06:16:19Z",
        "message": "[Update] Placeholders for examples of Hugging Face TRL library were added."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2024-02-08T03:52:31Z",
        "message": "[Update] Several examples for stable diffusion models of Stability AI were implemented.\n[Update] Information about Transformer Reinforcement Learning (TRL) library was moved from hugging_face_test.py to hugging_face_transformers_test.py."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2024-01-31T08:31:27Z",
        "message": "[Update] A test was added to fine-tune a ResNet model using Dog/Cat dataset in pytorch_transfer_learning.py."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2024-01-28T13:07:24Z",
        "message": "[Update] An example of Zephyr-7B was added in hugging_face_transformers_test.py, but was not tested.\n[Update] The information about Transformer Reinforcement Learning (TRL) was described in hugging_face_test.py."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2024-01-11T04:04:23Z",
        "message": "[Update] An example for retrieval-augmented generation (RAG) was added in hugging_face_transformers_test.py.\n[Update] Information about TableMASTER-mmocr was removed from mmocr_usage_guide.txt."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-12-24T02:40:32Z",
        "message": "[Update] A few examples for Mistral-7B and Mixtral-8x7B models were added.\n[Update] The installation of TensorFlow 2 were updated."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-12-23T12:07:02Z",
        "message": "[Update] An example of ORCA-2 model was added.\n[Update] The examples for ViT, ViLT, BEiT, LayoutLM, and Donut models were merged respectively.\n[Update] The information about data preparation, training, evaluation, visualization, and model export were supplemented in paddle_ocr_usage_guide.txt."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-11-24T08:49:41Z",
        "message": "[Update] A customized version of ViT model was implemented and tested, which doesn't have classification token and head."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-11-20T04:48:59Z",
        "message": "[Update] A few examples for Falcon model were initially implemented.\n[Update] A few examples for StarCoder and Replit models were implemented, but yet tested."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-11-10T01:27:50Z",
        "message": "[Update] LLMs Yi-6B & Yi-34B were tested."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-11-06T12:24:53Z",
        "message": "[Update] The forecasts of transformers.TimeSeriesTransformerForPrediction were evaluated by MASE, MAPE, sMAPE.\n[Update] Metrics for evaluating ML models' performance in evaluate library were tested."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-11-03T10:46:04Z",
        "message": "[Update] Two examples for phi-1 & phi-1.5 models were initially added.\n[Update] A few examples for Kosmos-2 model were initially added."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-10-12T12:13:43Z",
        "message": "[Update] A few examples for Probabilistic time series transformer model were added."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-10-07T08:51:44Z",
        "message": "[Update] Several examples for Perceiver IO model were initially committed."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-09-26T10:56:44Z",
        "message": "[Update] A simple example for Code Llama model was initially added.\n[Update] A few commands were explained to profile Python scripts."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-09-21T12:31:58Z",
        "message": "[Update] A simple example for OpenLLaMA models was implemented."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-09-04T03:59:25Z",
        "message": "[New] A few examples for PID, MPC, LQR using python-control library was initially added.\n[Update] A simple example for trajectory transformer models was added in hugging_face_transformers_test.py."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-09-02T12:30:02Z",
        "message": "[New] An example for model predictive control (MPC) was initially committed.\n[Update] A test for CodeParrot model was initially implemented in hugging_face_transformers_test.py."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-08-31T06:01:24Z",
        "message": "[Update] Memory footprint and computing performance (FLOPS) Hugging Face transformers models was measured.\n[Update] A simple test for Hugging Face datasets was added."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-08-04T08:13:21Z",
        "message": "[Update] An example for OpenFlamingo library was added."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-07-25T12:07:07Z",
        "message": "[Update] A few examples for Llama 2 model were added in hugging_face_transformers_test.py.\n[Update] Model parallelism was tested based on Hugging Face Accelerate library in hugging_face_transformers_test.py.\n[Update] Information about Hugging Face Accelerate library was reinforced."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-07-22T06:48:12Z",
        "message": "[Update] Cross references about transformer and ViT models were reinforced.\n[Chore] vit_test.py was moved from sw_dev/python/rnd/test/machine_learning/vit_test.py to sw_dev/python/rnd/test/machine_vision/vit_test.py.\n[Update] The installation of node.js was explained."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-07-17T01:20:26Z",
        "message": "[Update] Several examples for CodeBERT, CodeBERTa, CodeT5+, CodeGen2, & CodeGen2.5 models were added.\n[Update] An example of SpeechT5 model was divied into 3 examples: ASR, TTS, & speech-to-speech."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-07-12T07:54:25Z",
        "message": "[Update] An example for Decision Transformer was initially committed.\n[Update] A few examples for NVIDIA Megatron-LM, ASR, TTS models were implemented.\n[Update] An example for SegFormer model was initially added."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-06-30T14:20:26Z",
        "message": "[Update] The method, chain of thoughts (CoT) was tested on two LLMs, LLaMA & MPT."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-06-27T04:15:37Z",
        "message": "[Update] A few examples for MPT & TVLT models were implemented in hugging_face_transformers_test.py.\n[Chore] table_generation_usage_guide.txt was moved to SWLP repository."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-06-22T14:49:33Z",
        "message": "[Update] Three language models were tested: LLaMA, Galactica, & OPT models."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-06-15T07:51:37Z",
        "message": "[Update] The example of LLaMA model was reinforced."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-05-26T18:49:16Z",
        "message": "[Update] A test for data parallelism was implemented in PyTorch library."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-05-05T03:23:29Z",
        "message": "[Update] Two simple tests for two Facebook's language models, OPT and Galactica were initially implemented."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-04-30T14:34:59Z",
        "message": "[New] A simple tutorial for MMSegmentation library was initially committed."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-04-06T03:27:45Z",
        "message": "[Update] A couple of simple examples for LLaMa model were implemented, but they were not tested due to library version issue."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-04-04T11:52:41Z",
        "message": "[Update] Several examples for CodeT5 and CodeGen models were implemented to generate code.\n[Update] Several examples for GIT and BLIP models for vision-and-language modeling models were implemented.\n[Update] A few examples for TaPEx model were implemented to understand tables."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-04-01T14:10:48Z",
        "message": "[New] A couple of examples for GPT4All models were added, but they were not correctly working."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-04-01T06:26:08Z",
        "message": "[New] Two simple examples for PaLM and PaLM+RLHF models were implemented, but the models were not trained in the examples."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-03-30T08:06:47Z",
        "message": "[New] Several examples for table processing (Table Transformer, TATR), OCR (TrOCR), speech processing (SpeechT5) were initially added to hugging_face_transformers_test.py."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-03-29T05:25:41Z",
        "message": "[Update] Several examples for speech recognition (whisper) & synthesis (tacotron2 & fastspeech2) were newly implemented in hugging_face_transformers_test.py."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-03-26T07:02:13Z",
        "message": "[New] A few examples for ALIGN model were implemented in hugging_face_transformers_test.py.\n[Update] A couple of examples for CLIP model were added in hugging_face_transformers_test.py.\n[Update] Useful information about Transformer architectures was described."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-03-24T06:24:31Z",
        "message": "[Update] A simple example about dataclass in Python was added."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-03-22T14:48:31Z",
        "message": "[Update] A couple of examples of seq-to-seq models for LoRA & Prefix Tuning were implemented in hugging_face_test.py.\n[Update] An example for CLIP model was implemented in hugging_face_transformers_test.py.\n[Update] A few examples for Whisper model were implemented in hugging_face_transformers_test.py.\n[New] A usage guide for Hugging Face library was added."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-03-21T07:10:38Z",
        "message": "[Update] A simple example of Parameter-Efficient Fine-Tuning (PEFT) library was added to hugging_face_test.py.\n[Update] A simple test of tokenizers was added to hugging_face_transformers_test.py."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-03-18T11:41:29Z",
        "message": "[Update] A couple of examples for diffusion models of StabilityAI and CompVis were added."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-03-18T05:09:16Z",
        "message": "[Update] A few examples for Flan-T5 model were reinforced in hugging_face_transformers_test.py."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-03-16T11:52:13Z",
        "message": "[Update] Information about Hugging Face models was supplemented."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-03-15T12:08:24Z",
        "message": "[Update] An example for question answering using GPT-neo was added."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-03-14T02:43:49Z",
        "message": "[Update] A few examples for BLOOM and Flan-T5 models were implemented."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-03-09T01:51:43Z",
        "message": "[Update] A test for KLUE BERT models was implemented."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-03-09T01:49:32Z",
        "message": "[Update] A few examples of text summarization for Korean & English were implemented, but their results were not good.\n[Update] A few examples for T5 model were added."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-03-07T17:44:21Z",
        "message": "[Update] A few tests were added for GPT & BERT models."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-03-01T14:37:43Z",
        "message": "[New] A few guides were initially committed for Hugging Face Hub library."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-01-19T10:11:50Z",
        "message": "[Update] Several examples for vision, vision and language models were added in HuggingFace library."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2023-01-19T08:06:44Z",
        "message": "[Update] Several examples for LayoutLM & Donut models were implemented in HugggingFace library."
    },
    {
        "repo_url": "github.com/sangwook236/SWDT",
        "filepath": "sw_dev/python/rnd/test/language_processing/hugging_face_transformers_test.py",
        "commit_date": "2022-04-01T07:00:58Z",
        "message": "[Chore] transformers_test.py was renamed to hugging_face_transformers_test.py."
    },
    {
        "repo_url": "github.com/liuqidong07/MOELoRA-peft",
        "filepath": "src/MLoRA/main.py",
        "commit_date": "2023-10-13T08:43:26Z",
        "message": "commit the code"
    },
    {
        "repo_url": "github.com/liuqidong07/MOELoRA-peft",
        "filepath": "src/MLoRA/main_offline.py",
        "commit_date": "2023-10-13T08:43:26Z",
        "message": "commit the code"
    },
    {
        "repo_url": "github.com/liuqidong07/MOELoRA-peft",
        "filepath": "src/MLoRA/peft/peft_model.py",
        "commit_date": "2023-10-13T08:43:26Z",
        "message": "commit the code"
    },
    {
        "repo_url": "github.com/liuqidong07/MOELoRA-peft",
        "filepath": "src/MLoRA/peft/tuners/prompt_tuning.py",
        "commit_date": "2023-10-13T08:43:26Z",
        "message": "commit the code"
    },
    {
        "repo_url": "github.com/liuqidong07/MOELoRA-peft",
        "filepath": "src/MLoRA/test.py",
        "commit_date": "2023-10-13T08:43:26Z",
        "message": "commit the code"
    },
    {
        "repo_url": "github.com/taishan1994/baichuan-Qlora-Tuning",
        "filepath": "predict.py",
        "commit_date": "2023-06-22T11:44:42Z",
        "message": "commit"
    },
    {
        "repo_url": "github.com/victor-iyi/rlhf-trl",
        "filepath": "scripts/evaluate.py",
        "commit_date": "2023-06-14T07:37:34Z",
        "message": "Update README.md"
    },
    {
        "repo_url": "github.com/victor-iyi/rlhf-trl",
        "filepath": "scripts/evaluate.py",
        "commit_date": "2023-06-14T06:54:12Z",
        "message": "Remove default arguments"
    },
    {
        "repo_url": "github.com/victor-iyi/rlhf-trl",
        "filepath": "scripts/evaluate.py",
        "commit_date": "2023-06-13T22:48:24Z",
        "message": "Add training and evaluation scripts"
    },
    {
        "repo_url": "github.com/SmithaUpadhyaya/fashion_image_caption",
        "filepath": "app.py",
        "commit_date": "2023-06-27T13:37:38Z",
        "message": "Modify streamlit app and include demo gif file in README"
    },
    {
        "repo_url": "github.com/SmithaUpadhyaya/fashion_image_caption",
        "filepath": "app.py",
        "commit_date": "2023-06-26T16:35:58Z",
        "message": "Streamlit deploy code for HFSpaces"
    },
    {
        "repo_url": "github.com/ChiYeungLaw/LLaMa-EasyFT",
        "filepath": "src/generate.py",
        "commit_date": "2023-04-14T03:34:20Z",
        "message": "initial"
    },
    {
        "repo_url": "github.com/yysirs/ChatDoc",
        "filepath": "models/moss_llm.py",
        "commit_date": "2023-05-26T03:18:50Z",
        "message": "push code"
    },
    {
        "repo_url": "github.com/yysirs/ChatDoc",
        "filepath": "models/chatglm_llm.py",
        "commit_date": "2023-05-26T03:18:50Z",
        "message": "push code"
    },
    {
        "repo_url": "github.com/hppRC/llm-lora-classification",
        "filepath": "src/models.py",
        "commit_date": "2023-07-17T03:42:57Z",
        "message": ":tada: Initial commit"
    },
    {
        "repo_url": "github.com/weiyifan1023/Neeko",
        "filepath": "src/model.py",
        "commit_date": "2024-02-20T10:39:58Z",
        "message": "add moelora"
    },
    {
        "repo_url": "github.com/weiyifan1023/Neeko",
        "filepath": "src/model.py",
        "commit_date": "2024-01-21T06:24:27Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/weiyifan1023/Neeko",
        "filepath": "eval/infer.py",
        "commit_date": "2024-02-21T08:52:02Z",
        "message": "eval"
    },
    {
        "repo_url": "github.com/weiyifan1023/Neeko",
        "filepath": "moelora/peft_model.py",
        "commit_date": "2024-02-20T10:39:58Z",
        "message": "add moelora"
    },
    {
        "repo_url": "github.com/weiyifan1023/Neeko",
        "filepath": "moelora/tuners/prompt_tuning.py",
        "commit_date": "2024-02-20T10:39:58Z",
        "message": "add moelora"
    },
    {
        "repo_url": "github.com/hdong920/LESS",
        "filepath": "src/lm_eval/models/huggingface.py",
        "commit_date": "2024-02-14T15:34:36Z",
        "message": "train and eval code"
    },
    {
        "repo_url": "github.com/song-wx/SIFT",
        "filepath": "exp/mmlu/eval_mmlu.py",
        "commit_date": "2023-12-18T13:37:16Z",
        "message": "first add exp"
    },
    {
        "repo_url": "github.com/HanGuo97/lq-lora",
        "filepath": "run_oasst1.py",
        "commit_date": "2023-11-21T01:42:48Z",
        "message": "initial"
    },
    {
        "repo_url": "github.com/GCYZSL/MoLA",
        "filepath": "src/mola_lora_hacked.py",
        "commit_date": "2024-02-13T15:31:45Z",
        "message": "MoLA"
    },
    {
        "repo_url": "github.com/GCYZSL/MoLA",
        "filepath": "evaluation_scienceqa.py",
        "commit_date": "2024-02-13T15:31:45Z",
        "message": "MoLA"
    },
    {
        "repo_url": "github.com/GCYZSL/MoLA",
        "filepath": "src/mola_peft_model_hacked.py",
        "commit_date": "2024-02-13T15:31:45Z",
        "message": "MoLA"
    },
    {
        "repo_url": "github.com/zhiqix/NL2GQL",
        "filepath": "Smaller_LLM.py",
        "commit_date": "2023-11-06T13:40:08Z",
        "message": "add final_version"
    },
    {
        "repo_url": "github.com/wwxu21/CUT",
        "filepath": "merge.py",
        "commit_date": "2023-12-22T07:47:13Z",
        "message": "src code"
    },
    {
        "repo_url": "github.com/wwxu21/CUT",
        "filepath": "modeling_llama_unlikelihood.py",
        "commit_date": "2024-01-09T05:46:35Z",
        "message": "fix batch size issue"
    },
    {
        "repo_url": "github.com/wwxu21/CUT",
        "filepath": "modeling_llama_unlikelihood.py",
        "commit_date": "2023-12-22T07:43:44Z",
        "message": "src code"
    },
    {
        "repo_url": "github.com/taishan1994/PPO_Chinese_Generate",
        "filepath": "roberta-chinese.py",
        "commit_date": "2023-04-24T10:38:40Z",
        "message": "commit"
    },
    {
        "repo_url": "github.com/taishan1994/PPO_Chinese_Generate",
        "filepath": "roberta-chinese.py",
        "commit_date": "2023-04-24T09:51:25Z",
        "message": "commit"
    },
    {
        "repo_url": "github.com/morecry/CharacterChat",
        "filepath": "model/merge_lora.py",
        "commit_date": "2023-08-20T09:37:24Z",
        "message": "First Commit"
    },
    {
        "repo_url": "github.com/rjmacarthy/quintus",
        "filepath": "src/inference/local/model.py",
        "commit_date": "2023-05-18T14:05:57Z",
        "message": "fix folders"
    },
    {
        "repo_url": "github.com/rjmacarthy/quintus",
        "filepath": "src/inference/local/model.py",
        "commit_date": "2023-05-18T14:05:21Z",
        "message": "gitignore cache"
    },
    {
        "repo_url": "github.com/rjmacarthy/quintus",
        "filepath": "src/inference/local/model.py",
        "commit_date": "2023-05-18T14:03:19Z",
        "message": "rename lib t src"
    },
    {
        "repo_url": "github.com/rjmacarthy/quintus",
        "filepath": "src/inference/local/model.py",
        "commit_date": "2023-04-13T09:40:10Z",
        "message": "add pip support"
    },
    {
        "repo_url": "github.com/rjmacarthy/quintus",
        "filepath": "src/inference/local/model.py",
        "commit_date": "2023-04-12T15:59:29Z",
        "message": "add main.py for openai inference"
    },
    {
        "repo_url": "github.com/wuhy68/Parameter-Efficient-MoE",
        "filepath": "merge_moe_lora.py",
        "commit_date": "2024-02-18T02:44:39Z",
        "message": "merge"
    },
    {
        "repo_url": "github.com/Yiwei98/TDG",
        "filepath": "code/peft_NAT/peft_model.py",
        "commit_date": "2024-01-10T13:26:06Z",
        "message": "add dataset"
    },
    {
        "repo_url": "github.com/Yiwei98/TDG",
        "filepath": "code/peft_NAT/tuners/lora.py",
        "commit_date": "2024-01-10T13:26:06Z",
        "message": "add dataset"
    },
    {
        "repo_url": "github.com/Yiwei98/TDG",
        "filepath": "code/generate_batch_adaptive.py",
        "commit_date": "2024-01-10T13:26:06Z",
        "message": "add dataset"
    },
    {
        "repo_url": "github.com/Yiwei98/TDG",
        "filepath": "code/peft_NAT/tuners/prompt_tuning.py",
        "commit_date": "2024-01-10T13:26:06Z",
        "message": "add dataset"
    },
    {
        "repo_url": "github.com/mlrun/functions",
        "filepath": "huggingface_auto_trainer/huggingface_auto_trainer.py",
        "commit_date": "2023-08-27T10:00:42Z",
        "message": "[huggingface-auto-trainer] add llm fine tuner (#651)\n\n* huggingface-auto-trainer\n\n* after yoni's review\n\n* Update huggingface_auto_trainer/item.yaml\n\nCo-authored-by: Yonatan Shelach <92271540+yonishelach@users.noreply.github.com>\n\n* fixes for demo\n\n---------\n\nCo-authored-by: Yonatan Shelach <92271540+yonishelach@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/Percent-BFD/neurips_submission",
        "filepath": "neurips_submission_1/helper.py",
        "commit_date": "2023-10-27T06:10:09Z",
        "message": "fist commit"
    },
    {
        "repo_url": "github.com/Percent-BFD/neurips_submission",
        "filepath": "neurips_submission_2/helper.py",
        "commit_date": "2023-10-27T06:10:09Z",
        "message": "fist commit"
    },
    {
        "repo_url": "github.com/LLM360/Analysis360",
        "filepath": "eval/harness/lm_eval/models/huggingface.py",
        "commit_date": "2024-02-06T15:19:23Z",
        "message": "fixes"
    },
    {
        "repo_url": "github.com/VITA-Group/LLaGA",
        "filepath": "model/builder.py",
        "commit_date": "2024-02-12T06:04:00Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/Q-Future/Q-Align",
        "filepath": "q_align/model/builder.py",
        "commit_date": "2024-01-06T18:02:50Z",
        "message": "lora fine-tune on one_align enabled"
    },
    {
        "repo_url": "github.com/Q-Future/Q-Align",
        "filepath": "q_align/model/builder.py",
        "commit_date": "2024-01-03T11:42:52Z",
        "message": "quick fix"
    },
    {
        "repo_url": "github.com/Q-Future/Q-Align",
        "filepath": "q_align/model/builder.py",
        "commit_date": "2023-12-20T15:30:24Z",
        "message": "bump"
    },
    {
        "repo_url": "github.com/flurryunicorn/virtualgf-gpt",
        "filepath": "opendan-text-generation-webui/modules/LoRA.py",
        "commit_date": "2023-06-07T05:52:40Z",
        "message": "update LLM Server"
    },
    {
        "repo_url": "github.com/insightbuilder/python_de_learners_data",
        "filepath": "code_script_notebooks/projects/exploring_modal/openLlama_onModal.py",
        "commit_date": "2023-06-04T10:00:05Z",
        "message": "ready for samantha"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "examples/text-generation/utils.py",
        "commit_date": "2024-02-23T08:29:15Z",
        "message": "Enable internal kv bucket in llama (#720)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "examples/text-generation/utils.py",
        "commit_date": "2024-02-19T15:43:10Z",
        "message": "Enable torch_compile mode for distributed (#659)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "examples/text-generation/utils.py",
        "commit_date": "2024-02-03T01:53:30Z",
        "message": "Bring back workaround for Falcon with SynapseAI 1.13 (#685)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "examples/text-generation/utils.py",
        "commit_date": "2024-01-25T19:01:23Z",
        "message": "Upgrade to Synapse 1.14 (#664)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "examples/text-generation/utils.py",
        "commit_date": "2024-01-24T19:02:28Z",
        "message": "Fix error in PR#654 (#661)\n\n* Falcon changes for 1.14.0\n\n1) fix for invalid input shape with DS 0.12.4\n2) revert --skip_hash_with_views changes\n3) add falcon to model_on_meta()\n\n* address comments\n\n* README change\n\n* make PR#654 backward compatible for other models\n\n* address comments"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "examples/text-generation/utils.py",
        "commit_date": "2024-01-24T01:14:37Z",
        "message": "Hqt (#648)\n\n* enable HQT"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "examples/text-generation/utils.py",
        "commit_date": "2024-01-24T01:00:52Z",
        "message": "Falcon changes for v1.14.0 release (#654)\n\n* Falcon changes for 1.14.0\n\n1) fix for invalid input shape with DS 0.12.4\n2) revert --skip_hash_with_views changes\n3) add falcon to model_on_meta()\n\n* address comments\n\n* README change"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "examples/text-generation/utils.py",
        "commit_date": "2024-01-23T18:23:59Z",
        "message": "Run Llama2 with torch.compile on Gaudi2 (#616)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "examples/text-generation/utils.py",
        "commit_date": "2023-12-29T15:32:51Z",
        "message": "Update generation config to enable flash attention for inference (#609)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "examples/text-generation/utils.py",
        "commit_date": "2023-12-18T08:40:56Z",
        "message": "Dyn prompt after refactor (#543)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "examples/text-generation/utils.py",
        "commit_date": "2023-12-11T13:46:44Z",
        "message": "Refactoring LLama Attention and mlp layers (#589)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "examples/text-generation/utils.py",
        "commit_date": "2023-12-08T09:08:31Z",
        "message": " Fix hash_with_views error (#587)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "examples/text-generation/utils.py",
        "commit_date": "2023-11-27T18:44:57Z",
        "message": "Add fallback for PEFT when the base model doesn't exist (#557)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "examples/text-generation/utils.py",
        "commit_date": "2023-11-23T16:03:06Z",
        "message": "Add Llama2 fp8 inference (#542)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "examples/text-generation/utils.py",
        "commit_date": "2023-11-21T08:32:41Z",
        "message": "Automate skip_hash_with_views for text generation with Falcon (#544)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "examples/text-generation/utils.py",
        "commit_date": "2023-11-20T20:14:30Z",
        "message": "Fix loading on meta device for PEFT models with DS-inference (#528)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "examples/text-generation/utils.py",
        "commit_date": "2023-11-18T15:29:27Z",
        "message": "Add hash_with_views arg for Falcon inference perf (#534)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "examples/text-generation/utils.py",
        "commit_date": "2023-11-17T19:05:03Z",
        "message": "Refactor run generation (#523)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "examples/trl/merge_peft_adapter.py",
        "commit_date": "2024-01-16T11:55:53Z",
        "message": "Restructure example/trl/stack_llama_2 for generic DPO  (#635)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2024-02-14T07:06:32Z",
        "message": "To fix LLAMA-V2-70B-FT-HF (8x) for eager mode (#709)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2024-02-12T06:34:09Z",
        "message": "Upgrade to Transformers 4.37 (#651)\n\nCo-authored-by: Sayantan Sarkar <sasarkar@habana.ai>\nCo-authored-by: Libin Tang <litang@habana.ai>\nCo-authored-by: Jimin Ha <jha@habana.ai>\nCo-authored-by: Yeonsil Yoon <yyoon@habana.ai>\nCo-authored-by: Sayantan Sarkar <supersarkar@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2024-01-23T18:09:33Z",
        "message": "Add changes to support FSDP (#598)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2024-01-03T18:11:00Z",
        "message": "Adding support for bf16_full_eval (#610)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-12-29T15:24:37Z",
        "message": "Fix crash if gaudi_config is not passed to GaudiTrainer (#613)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-12-19T08:15:17Z",
        "message": "In peft, only the trainable parameters need to be saved (#576)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-12-14T09:06:11Z",
        "message": "Integrate Habana flash attention to Llama2-70B finetune (#596)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-12-12T10:42:29Z",
        "message": "Support for FlashAttention in Llama2 (#584)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-11-30T22:58:23Z",
        "message": "Upgrade to SynapseAI 1.13 (#563)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-11-30T21:54:03Z",
        "message": "Add workaround for GPT2 gaudi config for autocast"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-11-24T13:23:44Z",
        "message": "Remove HMP from optimum-habana (#349)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-11-21T21:23:00Z",
        "message": "Fix save DS scheduler (#549)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-11-17T09:32:40Z",
        "message": "Add infra to enable/disable dynamic shapes feature through gaudi_config (#513)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-11-16T16:48:52Z",
        "message": "Fix for attn_softmax_bf16 when generation_config is None (#531)\n\n* Enable llama attention softmax in bf16\n\n* apply attn_softmax_bf16 only for llama\n\n* separte 2 if blocks\n\n* Fix CI tests error from DummyModel\n\n* Fix for attn_softmax_bf16 when generation_config is None\n\n* Fix typo"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-11-15T17:24:39Z",
        "message": "Enable llama attention softmax in bf16 (#521)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-11-07T09:04:34Z",
        "message": "Upgrade to Transformers 4.34 (#475)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-11-02T16:42:05Z",
        "message": "Add maximum hpugraphs and disable_tensor_cache arguments to GaudiTrainer (#493)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-10-26T12:02:26Z",
        "message": "Enable/disable gradient_checkpointing as per training_args.gradient_checkpointing value (#484)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-10-21T19:17:11Z",
        "message": "Update GPT2-XL notebook (#481)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-10-09T07:55:39Z",
        "message": "Remove `sharded_ddp` (#456)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-09-27T17:35:39Z",
        "message": "No need to wrap DDP when using Fast DDP (#430)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-09-25T08:09:08Z",
        "message": "Makes the record_shapes of the profiler changeable. (#418)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-09-19T20:03:11Z",
        "message": "Upgrade to Transformers 4.33 (#412)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-09-12T17:10:39Z",
        "message": "Add support for autocast custom ops in `GaudiTrainer` (#308)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-08-24T20:09:11Z",
        "message": "Upgrade to Accelerate v0.22 (#362)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-08-23T09:09:30Z",
        "message": "Upgrade to Transformers v4.32 (#354)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-08-17T09:43:58Z",
        "message": "Upgrade to Synapse 1.11 (#333)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-08-13T09:36:01Z",
        "message": "Upgrade to Transformers v4.31 (#312)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-07-28T01:02:16Z",
        "message": "Revert \"Upgrade to Transformers v4.31\""
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-07-26T08:40:17Z",
        "message": "Fix tests"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-07-25T10:37:54Z",
        "message": "Make style"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-07-25T10:09:19Z",
        "message": "Add support for DeepSpeed"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-07-24T16:41:50Z",
        "message": "[skip ci] Update `GaudiTrainer` and `GaudiTrainingArguments`"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-06-28T22:08:12Z",
        "message": "Add Bridgetower example (#283)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-06-22T15:28:08Z",
        "message": "Enable/Optimize flan t5 xxl on deepspeed z3 (#257)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-06-16T17:03:57Z",
        "message": "Add support for HPU graphs for training and Fast DDP (#200)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-06-16T06:48:42Z",
        "message": "Improve DeepSpeed support (#272)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-06-15T20:20:25Z",
        "message": "Add test and doc for Torch Autocast (#269)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-06-14T07:09:11Z",
        "message": "Enable usage of PyTorch autocast on Gaudi during training (#226)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-06-07T23:21:46Z",
        "message": "Fix profiling in GaudiTrainer (#260)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-06-06T20:58:57Z",
        "message": "Added an option to remove log/save/evaluate time from throughput calculation (#237)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-05-30T21:45:29Z",
        "message": "Enable profiling (#250)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-04-26T08:19:22Z",
        "message": "Enable asynchronous data copy to get a better performance (#211)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-04-17T17:21:44Z",
        "message": "Upgrade to SynapseAI 1.9.0 (#193)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-04-15T19:12:59Z",
        "message": "Add mark_step between fwd and bwd for better performance (#189)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-04-14T23:06:09Z",
        "message": "Upgrade to Transformers 4.28 (#202)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-03-16T09:58:57Z",
        "message": "Update to Transformers 4.27 (#181)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-03-09T16:57:07Z",
        "message": "Enable HPU graphs for distributed runs and generation (#179)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-03-05T22:37:31Z",
        "message": "Add copyrights (#180)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-02-25T17:59:07Z",
        "message": "Improve data sampling for training in lazy mode (#152)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-02-25T10:14:52Z",
        "message": "Deprecate the usage of `hmp_opt_level` rather than remove it (#173)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-02-22T08:08:12Z",
        "message": "Remove O1/O2 in Gaudi config (#169)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-02-12T22:05:20Z",
        "message": "Update CI baseline for speech recognition (#164)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-02-08T22:54:34Z",
        "message": "Update to SynapseAI v1.8.0 (#160)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-02-08T08:22:36Z",
        "message": "Reformat files (#161)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-02-01T17:48:10Z",
        "message": "Update the CLM and MLM examples (#156)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-01-31T20:53:29Z",
        "message": "Add support for inference through HPU graphs in GaudiTrainer (#151)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2023-01-24T22:29:00Z",
        "message": "Update to Transformers 4.26 (#143)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2022-12-29T22:22:24Z",
        "message": "Add support for multi-node training (#116)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2022-12-13T00:25:01Z",
        "message": "Enable DeepSpeed activation checkpointing (#142)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2022-12-10T10:38:03Z",
        "message": "Update to Diffusers 0.10.0 (#139)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2022-12-02T10:14:44Z",
        "message": "Update to Transformers 4.25.1 (#136)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/transformers/trainer.py",
        "commit_date": "2022-11-30T23:06:04Z",
        "message": "Add support for Stable Diffusion (#131)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/trl/trainer/dpo_trainer.py",
        "commit_date": "2024-02-12T06:34:09Z",
        "message": "Upgrade to Transformers 4.37 (#651)\n\nCo-authored-by: Sayantan Sarkar <sasarkar@habana.ai>\nCo-authored-by: Libin Tang <litang@habana.ai>\nCo-authored-by: Jimin Ha <jha@habana.ai>\nCo-authored-by: Yeonsil Yoon <yyoon@habana.ai>\nCo-authored-by: Sayantan Sarkar <supersarkar@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/trl/trainer/dpo_trainer.py",
        "commit_date": "2024-01-09T14:18:03Z",
        "message": "Fix dpo graph compile error in evaluation (#630)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/trl/trainer/dpo_trainer.py",
        "commit_date": "2023-12-25T23:59:12Z",
        "message": "add DPO and SFT of TRL support in Gaudi and example (#601)\n\n* add DPO and SFT of TRL support in Gaudi and example\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>\n\n* upgrade SFTTrainer/DPO trainer and stack_llama_2 example to v0.7.6\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>\n\n---------\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/trl/trainer/sft_trainer.py",
        "commit_date": "2024-02-12T06:34:09Z",
        "message": "Upgrade to Transformers 4.37 (#651)\n\nCo-authored-by: Sayantan Sarkar <sasarkar@habana.ai>\nCo-authored-by: Libin Tang <litang@habana.ai>\nCo-authored-by: Jimin Ha <jha@habana.ai>\nCo-authored-by: Yeonsil Yoon <yyoon@habana.ai>\nCo-authored-by: Sayantan Sarkar <supersarkar@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/trl/trainer/sft_trainer.py",
        "commit_date": "2024-01-09T14:18:03Z",
        "message": "Fix dpo graph compile error in evaluation (#630)"
    },
    {
        "repo_url": "github.com/huggingface/optimum-habana",
        "filepath": "optimum/habana/trl/trainer/sft_trainer.py",
        "commit_date": "2023-12-25T23:59:12Z",
        "message": "add DPO and SFT of TRL support in Gaudi and example (#601)\n\n* add DPO and SFT of TRL support in Gaudi and example\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>\n\n* upgrade SFTTrainer/DPO trainer and stack_llama_2 example to v0.7.6\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>\n\n---------\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>"
    },
    {
        "repo_url": "github.com/vermouthdky/SimTeG",
        "filepath": "src/model/lms/lm_modeling.py",
        "commit_date": "2023-07-01T03:08:11Z",
        "message": "tem save, update with tape"
    },
    {
        "repo_url": "github.com/vermouthdky/SimTeG",
        "filepath": "src/model/lms/lm_modeling.py",
        "commit_date": "2023-06-23T05:20:49Z",
        "message": "tem save"
    },
    {
        "repo_url": "github.com/vermouthdky/SimTeG",
        "filepath": "src/model/lms/lm_modeling.py",
        "commit_date": "2023-06-09T08:01:51Z",
        "message": "tem save"
    },
    {
        "repo_url": "github.com/vermouthdky/SimTeG",
        "filepath": "src/model/lms/lm_modeling.py",
        "commit_date": "2023-06-07T12:02:23Z",
        "message": "tem save"
    },
    {
        "repo_url": "github.com/vermouthdky/SimTeG",
        "filepath": "src/model/lms/lm_modeling.py",
        "commit_date": "2023-05-17T08:50:12Z",
        "message": "tem save"
    },
    {
        "repo_url": "github.com/vermouthdky/SimTeG",
        "filepath": "src/model/lms/lm_modeling.py",
        "commit_date": "2023-05-17T05:27:29Z",
        "message": "tem save"
    },
    {
        "repo_url": "github.com/vermouthdky/SimTeG",
        "filepath": "src/model/lms/lm_modeling.py",
        "commit_date": "2023-05-09T09:24:02Z",
        "message": "optimize optuna"
    },
    {
        "repo_url": "github.com/vermouthdky/SimTeG",
        "filepath": "src/model/lms/lm_modeling.py",
        "commit_date": "2023-05-08T07:00:00Z",
        "message": "tem save"
    },
    {
        "repo_url": "github.com/vermouthdky/SimTeG",
        "filepath": "src/model/lms/lm_modeling.py",
        "commit_date": "2023-04-26T03:29:00Z",
        "message": "tem save"
    },
    {
        "repo_url": "github.com/vermouthdky/SimTeG",
        "filepath": "src/model/lms/lm_modeling.py",
        "commit_date": "2023-04-21T06:55:16Z",
        "message": "tem save"
    },
    {
        "repo_url": "github.com/vermouthdky/SimTeG",
        "filepath": "src/model/lms/lm_modeling.py",
        "commit_date": "2023-03-30T03:26:12Z",
        "message": "add huggingface trainer"
    },
    {
        "repo_url": "github.com/vermouthdky/SimTeG",
        "filepath": "src/model/lms/lm_modeling.py",
        "commit_date": "2023-03-10T07:28:34Z",
        "message": "tem save"
    },
    {
        "repo_url": "github.com/vermouthdky/SimTeG",
        "filepath": "src/model/lms/lm_modeling.py",
        "commit_date": "2023-03-08T06:54:19Z",
        "message": "reformat train.sh"
    },
    {
        "repo_url": "github.com/vermouthdky/SimTeG",
        "filepath": "src/model/lms/lm_modeling.py",
        "commit_date": "2023-03-05T03:09:49Z",
        "message": "tem save"
    },
    {
        "repo_url": "github.com/vermouthdky/SimTeG",
        "filepath": "src/model/lms/link_lm_modeling.py",
        "commit_date": "2023-07-01T03:08:11Z",
        "message": "tem save, update with tape"
    },
    {
        "repo_url": "github.com/vermouthdky/SimTeG",
        "filepath": "src/model/lms/link_lm_modeling.py",
        "commit_date": "2023-06-13T06:43:16Z",
        "message": "tem save"
    },
    {
        "repo_url": "github.com/vermouthdky/SimTeG",
        "filepath": "src/model/lms/link_lm_modeling.py",
        "commit_date": "2023-06-07T12:02:23Z",
        "message": "tem save"
    },
    {
        "repo_url": "github.com/vermouthdky/SimTeG",
        "filepath": "src/model/lms/link_lm_modeling.py",
        "commit_date": "2023-05-29T09:50:42Z",
        "message": "tem save"
    },
    {
        "repo_url": "github.com/vermouthdky/SimTeG",
        "filepath": "src/model/lms/link_lm_modeling.py",
        "commit_date": "2023-05-24T05:51:56Z",
        "message": "link prediction task"
    },
    {
        "repo_url": "github.com/huu4ontocord/MDEL",
        "filepath": "lora-x/experimental/qlora_lomo.py",
        "commit_date": "2023-08-20T16:33:22Z",
        "message": "Rename lora-x/qlora_lomo.py to lora-x/experimental/qlora_lomo.py"
    },
    {
        "repo_url": "github.com/jmhessel/caption_contest_corpus",
        "filepath": "llama2/inference.py",
        "commit_date": "2023-09-02T05:24:32Z",
        "message": "updated inference to account for prompt delim arg"
    },
    {
        "repo_url": "github.com/jmhessel/caption_contest_corpus",
        "filepath": "llama2/inference.py",
        "commit_date": "2023-08-31T21:40:28Z",
        "message": "v0.1, initial commit"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-webui",
        "filepath": "modules/model/llama/__init__.py",
        "commit_date": "2023-05-08T12:52:12Z",
        "message": "refactor: better chat history"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-webui",
        "filepath": "modules/model/llama/__init__.py",
        "commit_date": "2023-04-15T03:52:20Z",
        "message": "refactor(blip2zh): new blip2zh modeling"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-webui",
        "filepath": "modules/model/llama/__init__.py",
        "commit_date": "2023-03-31T06:32:26Z",
        "message": "feat: chatgpt support"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-webui",
        "filepath": "modules/model/llama/__init__.py",
        "commit_date": "2023-03-29T10:44:07Z",
        "message": "Squashed commit of the following:\n\ncommit 65dc9d36bba4c2b3c4e39f6a5003f2fcb5db2037\nAuthor: shwu <xipotatonium@outlook.com>\nDate:   Wed Mar 29 10:43:35 2023 +0000\n\n    refactor: do not use history"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-webui",
        "filepath": "modules/model/llama/__init__.py",
        "commit_date": "2023-03-28T06:11:01Z",
        "message": "feat(model & ui): support stream"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-webui",
        "filepath": "modules/model/llama/__init__.py",
        "commit_date": "2023-03-25T12:35:33Z",
        "message": "feat(ui): simplify ui"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-webui",
        "filepath": "modules/model/llama/__init__.py",
        "commit_date": "2023-03-22T12:30:37Z",
        "message": "feat(all): llama and belle\n\n* add llama and belle\n* use configs\n* refactor history"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-webui",
        "filepath": "modules/model/blip2chatglm/__init__.py",
        "commit_date": "2023-05-08T12:52:12Z",
        "message": "refactor: better chat history"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-webui",
        "filepath": "modules/model/blip2chatglm/__init__.py",
        "commit_date": "2023-04-15T03:52:20Z",
        "message": "refactor(blip2zh): new blip2zh modeling"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-webui",
        "filepath": "modules/model/blip2chatglm/__init__.py",
        "commit_date": "2023-03-29T10:44:07Z",
        "message": "Squashed commit of the following:\n\ncommit 65dc9d36bba4c2b3c4e39f6a5003f2fcb5db2037\nAuthor: shwu <xipotatonium@outlook.com>\nDate:   Wed Mar 29 10:43:35 2023 +0000\n\n    refactor: do not use history"
    },
    {
        "repo_url": "github.com/XiPotatonium/chatbot-webui",
        "filepath": "modules/model/blip2chatglm/__init__.py",
        "commit_date": "2023-03-28T13:25:42Z",
        "message": "feat: blip2chatglm"
    },
    {
        "repo_url": "github.com/feizc/Visual-ChatGLM",
        "filepath": "fine_tune.py",
        "commit_date": "2023-03-21T09:23:03Z",
        "message": "basic frame"
    },
    {
        "repo_url": "github.com/feizc/Visual-ChatGLM",
        "filepath": "peft/peft_model.py",
        "commit_date": "2023-03-21T09:23:03Z",
        "message": "basic frame"
    },
    {
        "repo_url": "github.com/feizc/Visual-ChatGLM",
        "filepath": "peft/tuners/lora.py",
        "commit_date": "2023-03-21T09:23:03Z",
        "message": "basic frame"
    },
    {
        "repo_url": "github.com/feizc/Visual-ChatGLM",
        "filepath": "peft/tuners/prompt_tuning.py",
        "commit_date": "2023-03-21T09:23:03Z",
        "message": "basic frame"
    },
    {
        "repo_url": "github.com/yuhuixu1993/qa-lora",
        "filepath": "peft_utils.py",
        "commit_date": "2023-10-08T09:00:45Z",
        "message": "\u66f4\u65b0 peft_utils.py"
    },
    {
        "repo_url": "github.com/yuhuixu1993/qa-lora",
        "filepath": "peft_utils.py",
        "commit_date": "2023-10-05T16:47:48Z",
        "message": "fixing lint issues"
    },
    {
        "repo_url": "github.com/yuhuixu1993/qa-lora",
        "filepath": "peft_utils.py",
        "commit_date": "2023-10-02T00:24:38Z",
        "message": "\u66f4\u65b0 peft_utils.py\n\nfix bugs"
    },
    {
        "repo_url": "github.com/yuhuixu1993/qa-lora",
        "filepath": "peft_utils.py",
        "commit_date": "2023-10-01T21:33:52Z",
        "message": "Add missing parenthesis in peft_utils.py"
    },
    {
        "repo_url": "github.com/yuhuixu1993/qa-lora",
        "filepath": "peft_utils.py",
        "commit_date": "2023-09-27T03:26:25Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/alexrame/rewardedsoups",
        "filepath": "llama/utils/ppo_utils.py",
        "commit_date": "2023-06-08T07:20:10Z",
        "message": "First commit"
    },
    {
        "repo_url": "github.com/alexrame/rewardedsoups",
        "filepath": "llama/utils/inference_utils.py",
        "commit_date": "2023-06-08T07:20:10Z",
        "message": "First commit"
    },
    {
        "repo_url": "github.com/mlrun/demo-llm-tuning",
        "filepath": "src/serving.py",
        "commit_date": "2023-07-12T06:06:21Z",
        "message": "Updated model to falcon (#8)"
    },
    {
        "repo_url": "github.com/mlrun/demo-llm-tuning",
        "filepath": "src/serving.py",
        "commit_date": "2023-06-07T11:44:12Z",
        "message": "code documentation and formatting"
    },
    {
        "repo_url": "github.com/mlrun/demo-llm-tuning",
        "filepath": "src/serving.py",
        "commit_date": "2023-05-24T20:53:56Z",
        "message": "moving from cluster to new repo"
    },
    {
        "repo_url": "github.com/X-PLUG/Multi-LLM-Agent",
        "filepath": "GLPFT/utils/model_adapter.py",
        "commit_date": "2024-02-01T04:01:17Z",
        "message": "v0"
    },
    {
        "repo_url": "github.com/X-PLUG/Multi-LLM-Agent",
        "filepath": "GLPFT/inference_utils/toolbench/inference.py",
        "commit_date": "2024-02-01T04:01:17Z",
        "message": "v0"
    },
    {
        "repo_url": "github.com/X-PLUG/Multi-LLM-Agent",
        "filepath": "GLPFT/inference_utils/toolbench/infer_pipeline.py",
        "commit_date": "2024-02-01T04:01:17Z",
        "message": "v0"
    },
    {
        "repo_url": "github.com/X-PLUG/Multi-LLM-Agent",
        "filepath": "GLPFT/inference_utils/toolbench/inference_lora.py",
        "commit_date": "2024-02-01T04:01:17Z",
        "message": "v0"
    },
    {
        "repo_url": "github.com/X-PLUG/Multi-LLM-Agent",
        "filepath": "ToolBench-multiLLM/toolbench/model/model_adapter.py",
        "commit_date": "2024-02-01T04:01:17Z",
        "message": "v0"
    },
    {
        "repo_url": "github.com/X-PLUG/Multi-LLM-Agent",
        "filepath": "ToolBench-multiLLM/toolbench/inference/LLM/collab_agent_model.py",
        "commit_date": "2024-02-01T04:01:17Z",
        "message": "v0"
    },
    {
        "repo_url": "github.com/X-PLUG/Multi-LLM-Agent",
        "filepath": "ToolBench-multiLLM/toolbench/inference/LLM/tool_llama_lora_model.py",
        "commit_date": "2024-02-01T04:01:17Z",
        "message": "v0"
    },
    {
        "repo_url": "github.com/Jiuzhouh/PiVe",
        "filepath": "unified_verifier/inference.py",
        "commit_date": "2023-05-21T11:09:37Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/TIGER-AI-Lab/TIGERScore",
        "filepath": "tigerscore/finetune/trainer.py",
        "commit_date": "2023-11-10T17:11:16Z",
        "message": "Clean Code"
    },
    {
        "repo_url": "github.com/TIGER-AI-Lab/TIGERScore",
        "filepath": "tigerscore/finetune/trainer.py",
        "commit_date": "2023-10-23T02:14:34Z",
        "message": "update Readme\n\u0018y"
    },
    {
        "repo_url": "github.com/TIGER-AI-Lab/TIGERScore",
        "filepath": "tigerscore/finetune/test_llama.py",
        "commit_date": "2023-11-10T17:11:16Z",
        "message": "Clean Code"
    },
    {
        "repo_url": "github.com/TIGER-AI-Lab/TIGERScore",
        "filepath": "tigerscore/finetune/test_llama.py",
        "commit_date": "2023-10-23T02:14:34Z",
        "message": "update Readme\n\u0018y"
    },
    {
        "repo_url": "github.com/ntropy-network/enrichment_models",
        "filepath": "enrichment_models/llms/llama.py",
        "commit_date": "2023-05-31T22:00:32Z",
        "message": "publish benchmark"
    },
    {
        "repo_url": "github.com/DeepLink-org/AIChipBenchmark",
        "filepath": "llm/Alpaca-Lora/trainer.py",
        "commit_date": "2023-09-01T06:01:30Z",
        "message": "Add tgs"
    },
    {
        "repo_url": "github.com/princeton-nlp/QuRating",
        "filepath": "training/train_language_model.py",
        "commit_date": "2024-02-27T19:41:08Z",
        "message": "Add training scripts and instructions"
    },
    {
        "repo_url": "github.com/princeton-nlp/QuRating",
        "filepath": "training/train_preference_model.py",
        "commit_date": "2024-02-27T19:41:08Z",
        "message": "Add training scripts and instructions"
    },
    {
        "repo_url": "github.com/princeton-nlp/QuRating",
        "filepath": "eval/lm-evaluation-harness/lm_eval/models/huggingface.py",
        "commit_date": "2024-02-16T02:43:46Z",
        "message": "lm-evaluation-harness v0.3.0 with minor modifications (e.g., use modeling_flash_llama)"
    },
    {
        "repo_url": "github.com/Enderfga/FineRewards",
        "filepath": "fastchat/model/apply_lora.py",
        "commit_date": "2023-05-30T13:20:29Z",
        "message": "FineRewards"
    },
    {
        "repo_url": "github.com/AdaCheng/EgoThink",
        "filepath": "models/test_llava.py",
        "commit_date": "2023-12-18T08:41:44Z",
        "message": "update code"
    },
    {
        "repo_url": "github.com/xxm1668/ChatGLM-Efficient-LORA",
        "filepath": "src/inference_rm.py",
        "commit_date": "2023-06-19T02:32:00Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/xxm1668/ChatGLM-Efficient-LORA",
        "filepath": "src/inference_sft.py",
        "commit_date": "2023-06-19T02:32:00Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/xxm1668/ChatGLM-Efficient-LORA",
        "filepath": "src/inference_sft_deepspeed.py",
        "commit_date": "2023-06-19T02:32:00Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/Jiacheng-Zhu-AIML/AsymmetryLoRA",
        "filepath": "LoRASYM_peft/local_lorasym_all.py",
        "commit_date": "2024-02-26T05:00:11Z",
        "message": "Keep initializing thigns"
    },
    {
        "repo_url": "github.com/Jiacheng-Zhu-AIML/AsymmetryLoRA",
        "filepath": "LoRASYM_peft/local_lorasym_all.py",
        "commit_date": "2024-02-26T03:49:31Z",
        "message": "initializing the files"
    },
    {
        "repo_url": "github.com/Jiacheng-Zhu-AIML/AsymmetryLoRA",
        "filepath": "LoRASYM_peft/local_peft_model_all.py",
        "commit_date": "2024-02-26T05:00:11Z",
        "message": "Keep initializing thigns"
    },
    {
        "repo_url": "github.com/Jiacheng-Zhu-AIML/AsymmetryLoRA",
        "filepath": "LoRASYM_peft/local_peft_model_all.py",
        "commit_date": "2024-02-26T03:49:31Z",
        "message": "initializing the files"
    },
    {
        "repo_url": "github.com/WuNein/vllm4mteb",
        "filepath": "run_array_decoder_lora.py",
        "commit_date": "2023-11-26T03:01:41Z",
        "message": "add vllm"
    },
    {
        "repo_url": "github.com/WuNein/vllm4mteb",
        "filepath": "run_array_decoder_lora.py",
        "commit_date": "2023-11-26T02:58:05Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/WuNein/vllm4mteb",
        "filepath": "run_array_decoder_qwen.py",
        "commit_date": "2024-01-18T07:23:00Z",
        "message": "fix"
    },
    {
        "repo_url": "github.com/WuNein/vllm4mteb",
        "filepath": "run_array_decoder_qwen.py",
        "commit_date": "2023-11-26T03:01:41Z",
        "message": "add vllm"
    },
    {
        "repo_url": "github.com/WuNein/vllm4mteb",
        "filepath": "run_array_decoder_qwen.py",
        "commit_date": "2023-11-26T02:58:05Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/WuNein/vllm4mteb",
        "filepath": "run_array_decoder_vllm.py",
        "commit_date": "2023-11-26T03:01:41Z",
        "message": "add vllm"
    },
    {
        "repo_url": "github.com/UX-Decoder/LLaVA-Grounding",
        "filepath": "llava/model/builder.py",
        "commit_date": "2023-12-04T17:01:16Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/UX-Decoder/LLaVA-Grounding",
        "filepath": "llava/eval/LLaVA_G_Eval.py",
        "commit_date": "2023-12-04T19:08:11Z",
        "message": "Demo-related code. And readme."
    },
    {
        "repo_url": "github.com/IvanaXu/TianChiProj",
        "filepath": "2023.07-1.CCKS2023_1/KnowLM-main/examples/generate_lora.py",
        "commit_date": "2023-07-21T10:27:55Z",
        "message": "2023.07 #38"
    },
    {
        "repo_url": "github.com/IvanaXu/TianChiProj",
        "filepath": "2023.07-1.CCKS2023_1/KnowLM-main/examples/generate_lora_web.py",
        "commit_date": "2023-07-21T10:27:55Z",
        "message": "2023.07 #38"
    },
    {
        "repo_url": "github.com/IvanaXu/TianChiProj",
        "filepath": "2023.07-1.CCKS2023_1/KnowLM-main/tools/export_hf_checkpoint.py",
        "commit_date": "2023-07-21T10:27:55Z",
        "message": "2023.07 #38"
    },
    {
        "repo_url": "github.com/IvanaXu/TianChiProj",
        "filepath": "2023.07-1.CCKS2023_1/EasyInstruct-main/easyinstruct/engines/llama_engine.py",
        "commit_date": "2023-07-21T10:27:55Z",
        "message": "2023.07 #38"
    },
    {
        "repo_url": "github.com/IvanaXu/TianChiProj",
        "filepath": "2023.07-5.CCKS2023_PromptCBLUE_12/PromptCBLUE-main/src/ft_chatglm_lora/main.py",
        "commit_date": "2023-07-18T07:31:03Z",
        "message": "2023.07-5.CCKS2023_PromptCBLUE_12"
    },
    {
        "repo_url": "github.com/IvanaXu/TianChiProj",
        "filepath": "2023.07-5.CCKS2023_PromptCBLUE_12/PromptCBLUE-main/src/ft_chatglm_lora/peft/peft_model.py",
        "commit_date": "2023-07-18T07:31:03Z",
        "message": "2023.07-5.CCKS2023_PromptCBLUE_12"
    },
    {
        "repo_url": "github.com/IvanaXu/TianChiProj",
        "filepath": "2023.07-5.CCKS2023_PromptCBLUE_12/PromptCBLUE-main/src/ft_chatglm_lora/peft/tuners/prompt_tuning.py",
        "commit_date": "2023-07-18T07:31:03Z",
        "message": "2023.07-5.CCKS2023_PromptCBLUE_12"
    },
    {
        "repo_url": "github.com/mindspore-lab/mindnlp",
        "filepath": "mindnlp/peft/peft_model.py",
        "commit_date": "2023-12-15T13:14:48Z",
        "message": "Tuning `gpt_bigcode-santacoder` with peft lora (#805)"
    },
    {
        "repo_url": "github.com/mindspore-lab/mindnlp",
        "filepath": "mindnlp/peft/peft_model.py",
        "commit_date": "2023-09-14T01:57:42Z",
        "message": "add PEFT:LORA support (#657)\n\nCo-authored-by: cjl99 <cj2559@163.com>"
    },
    {
        "repo_url": "github.com/mindspore-lab/mindnlp",
        "filepath": "mindnlp/peft/peft_model.py",
        "commit_date": "2023-08-21T11:56:50Z",
        "message": "Basic PEFT support. (#633)\n\nCo-authored-by: cjl99 <cj2559@163.com>"
    },
    {
        "repo_url": "github.com/mindspore-lab/mindnlp",
        "filepath": "mindnlp/peft/peft_model.py",
        "commit_date": "2023-07-13T10:52:36Z",
        "message": "add 'peft' dir & download files to current work dir (#600)"
    },
    {
        "repo_url": "github.com/mindspore-lab/mindnlp",
        "filepath": "mindnlp/peft/tuners/lora.py",
        "commit_date": "2023-12-03T16:02:37Z",
        "message": "add mt5 model & rename 'gamma, beta, embedding_table' to 'weight, bias' (#768)"
    },
    {
        "repo_url": "github.com/mindspore-lab/mindnlp",
        "filepath": "mindnlp/peft/tuners/lora.py",
        "commit_date": "2023-10-18T12:40:59Z",
        "message": "refactor models to transformers for huggingface compatible (#698)"
    },
    {
        "repo_url": "github.com/mindspore-lab/mindnlp",
        "filepath": "mindnlp/peft/tuners/lora.py",
        "commit_date": "2023-09-14T01:57:42Z",
        "message": "add PEFT:LORA support (#657)\n\nCo-authored-by: cjl99 <cj2559@163.com>"
    },
    {
        "repo_url": "github.com/mindspore-lab/mindnlp",
        "filepath": "mindnlp/peft/tuners/lora.py",
        "commit_date": "2023-08-21T11:56:50Z",
        "message": "Basic PEFT support. (#633)\n\nCo-authored-by: cjl99 <cj2559@163.com>"
    },
    {
        "repo_url": "github.com/mindspore-lab/mindnlp",
        "filepath": "mindnlp/peft/tuners/lora.py",
        "commit_date": "2023-07-13T10:52:36Z",
        "message": "add 'peft' dir & download files to current work dir (#600)"
    },
    {
        "repo_url": "github.com/jyFengGoGo/InstructDet",
        "filepath": "modules/llava/llava/model/builder.py",
        "commit_date": "2024-01-30T02:22:06Z",
        "message": "release code,dataset"
    },
    {
        "repo_url": "github.com/jyFengGoGo/InstructDet",
        "filepath": "modules/fastchat/fastchat/model/apply_lora.py",
        "commit_date": "2024-01-30T02:22:06Z",
        "message": "release code,dataset"
    },
    {
        "repo_url": "github.com/DLCV-BUAA/TinyLLaVABench",
        "filepath": "tinyllava/model/builder.py",
        "commit_date": "2024-02-25T11:18:45Z",
        "message": "fix typos when load 2.0B model"
    },
    {
        "repo_url": "github.com/DLCV-BUAA/TinyLLaVABench",
        "filepath": "tinyllava/model/builder.py",
        "commit_date": "2024-02-23T10:27:04Z",
        "message": "add eval code"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "scripts/dpo_eval.py",
        "commit_date": "2023-11-29T18:06:26Z",
        "message": "Lots of new robustness, reliability experiments."
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "scripts/generate_outs.py",
        "commit_date": "2024-02-16T16:11:37Z",
        "message": "New analysis, code cleanup"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "scripts/generate_outs.py",
        "commit_date": "2024-01-29T12:29:31Z",
        "message": "Lots more code, contrastive dataset setup"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "scripts/generate_outs.py",
        "commit_date": "2024-01-17T15:18:22Z",
        "message": "lots of new stuff"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "scripts/generate_outs.py",
        "commit_date": "2023-12-25T17:34:37Z",
        "message": "More updates, ready to do some more eval"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "scripts/generate_outs.py",
        "commit_date": "2023-12-25T14:46:03Z",
        "message": "Lots more analysis / updates"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "scripts/generate_outs.py",
        "commit_date": "2023-12-12T17:26:24Z",
        "message": "Many new rollout / analysis goodies"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "scripts/generate_outs.py",
        "commit_date": "2023-11-16T22:53:02Z",
        "message": "More RM stuff"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "scripts/generate_outs.py",
        "commit_date": "2023-10-06T01:55:29Z",
        "message": "\"renamed to scripts\""
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rm-attacks/debug_utils.py",
        "commit_date": "2023-08-23T17:11:26Z",
        "message": "Analysis experiments"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "scripts/ultra_cross_eval.py",
        "commit_date": "2023-11-29T18:06:26Z",
        "message": "Lots of new robustness, reliability experiments."
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "scripts/ultra_cross_eval.py",
        "commit_date": "2023-11-16T22:53:02Z",
        "message": "More RM stuff"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "scripts/ultra_cross_eval.py",
        "commit_date": "2023-11-14T15:32:09Z",
        "message": "Scoring code"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "scripts/ultra_cross_eval.py",
        "commit_date": "2023-11-06T17:37:22Z",
        "message": "further analysis, set up synthetic experiment"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "scripts/stack_cross_eval.py",
        "commit_date": "2023-11-04T05:12:28Z",
        "message": "Change magnitude default behaviour"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "scripts/stack_cross_eval.py",
        "commit_date": "2023-10-23T20:13:47Z",
        "message": "Stack cross-rm evaluation code"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/rmcode.py",
        "commit_date": "2024-02-16T16:11:37Z",
        "message": "New analysis, code cleanup"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/rmcode.py",
        "commit_date": "2024-01-29T12:29:31Z",
        "message": "Lots more code, contrastive dataset setup"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/rmcode.py",
        "commit_date": "2024-01-17T15:18:22Z",
        "message": "lots of new stuff"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/rmcode.py",
        "commit_date": "2023-12-10T03:00:40Z",
        "message": "Multi-checkpoint/customizable rollouts, DPOvsRM"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/rmcode.py",
        "commit_date": "2023-11-29T18:06:26Z",
        "message": "Lots of new robustness, reliability experiments."
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/rmcode.py",
        "commit_date": "2023-11-16T22:53:02Z",
        "message": "More RM stuff"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/rmcode.py",
        "commit_date": "2023-11-14T15:32:09Z",
        "message": "Scoring code"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/rmcode.py",
        "commit_date": "2023-11-06T17:37:22Z",
        "message": "further analysis, set up synthetic experiment"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/rmcode.py",
        "commit_date": "2023-11-04T05:12:28Z",
        "message": "Change magnitude default behaviour"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/rmcode.py",
        "commit_date": "2023-10-23T20:13:47Z",
        "message": "Stack cross-rm evaluation code"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/rmcode.py",
        "commit_date": "2023-10-23T16:29:56Z",
        "message": "Moing files around a bit, RM analysis"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/rmcode.py",
        "commit_date": "2023-10-23T01:52:49Z",
        "message": "Working RM code with absolute magnitudes"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/rmcode.py",
        "commit_date": "2023-10-22T21:44:44Z",
        "message": "Code for ultra-feedback data, magnitude loss"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/rmcode.py",
        "commit_date": "2023-10-05T22:04:29Z",
        "message": "Remove unnecessary code"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/rmcode.py",
        "commit_date": "2023-09-09T20:27:38Z",
        "message": "truncation logic added"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/rmcode.py",
        "commit_date": "2023-09-09T04:23:15Z",
        "message": "Data Carto Setup / Analysis"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/rmcode.py",
        "commit_date": "2023-09-07T18:39:05Z",
        "message": "RM code, RL Fix"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/rmcode.py",
        "commit_date": "2023-09-06T20:19:08Z",
        "message": "Code unification logic RLHF"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/rmcode.py",
        "commit_date": "2023-09-01T18:37:48Z",
        "message": "More training jobs, apf setup"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/rmcode.py",
        "commit_date": "2023-08-30T21:11:51Z",
        "message": "Small adjustments"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/rmcode.py",
        "commit_date": "2023-08-30T14:10:27Z",
        "message": "More experiments, TFRM stuff"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "scripts/merge_peft_adapter.py",
        "commit_date": "2023-10-23T20:13:47Z",
        "message": "Stack cross-rm evaluation code"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "scripts/merge_peft_adapter.py",
        "commit_date": "2023-10-06T01:55:29Z",
        "message": "\"renamed to scripts\""
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/debug_utils.py",
        "commit_date": "2024-01-21T18:07:08Z",
        "message": "more updates"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/debug_utils.py",
        "commit_date": "2024-01-17T15:18:22Z",
        "message": "lots of new stuff"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/debug_utils.py",
        "commit_date": "2023-11-29T18:06:26Z",
        "message": "Lots of new robustness, reliability experiments."
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/debug_utils.py",
        "commit_date": "2023-11-16T22:53:02Z",
        "message": "More RM stuff"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/debug_utils.py",
        "commit_date": "2023-11-14T15:32:09Z",
        "message": "Scoring code"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/debug_utils.py",
        "commit_date": "2023-09-06T16:04:36Z",
        "message": "More RM code, more  PPO runs"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/debug_utils.py",
        "commit_date": "2023-09-01T18:37:48Z",
        "message": "More training jobs, apf setup"
    },
    {
        "repo_url": "github.com/PrasannS/rlhf-length-biases",
        "filepath": "rlhfutils/rlhfutils/debug_utils.py",
        "commit_date": "2023-08-31T02:29:37Z",
        "message": "Getting up some intrinsic stuff"
    },
    {
        "repo_url": "github.com/andersonbcdefg/dpo-lora",
        "filepath": "models.py",
        "commit_date": "2023-09-30T23:18:33Z",
        "message": "i refactored like everything to make DPO work with DDP..."
    },
    {
        "repo_url": "github.com/andersonbcdefg/dpo-lora",
        "filepath": "models.py",
        "commit_date": "2023-09-30T21:38:53Z",
        "message": "add ddp to see what happens"
    },
    {
        "repo_url": "github.com/andersonbcdefg/dpo-lora",
        "filepath": "models.py",
        "commit_date": "2023-09-30T19:08:49Z",
        "message": "fix quantization thing"
    },
    {
        "repo_url": "github.com/andersonbcdefg/dpo-lora",
        "filepath": "models.py",
        "commit_date": "2023-09-30T18:56:11Z",
        "message": "add mistral, see if that works"
    },
    {
        "repo_url": "github.com/andersonbcdefg/dpo-lora",
        "filepath": "models.py",
        "commit_date": "2023-09-29T21:34:10Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/Macielyoung/Baichuan-QLora",
        "filepath": "generation/generate.py",
        "commit_date": "2023-07-09T14:43:49Z",
        "message": "update readme"
    },
    {
        "repo_url": "github.com/Macielyoung/Baichuan-QLora",
        "filepath": "generation/generate.py",
        "commit_date": "2023-07-02T07:44:54Z",
        "message": "add repo"
    },
    {
        "repo_url": "github.com/Macielyoung/Baichuan-QLora",
        "filepath": "generation/qlora_eval.py",
        "commit_date": "2023-07-02T07:44:54Z",
        "message": "add repo"
    },
    {
        "repo_url": "github.com/Macielyoung/Baichuan-QLora",
        "filepath": "generation/multi_qlora_eval.py",
        "commit_date": "2023-07-09T14:43:49Z",
        "message": "update readme"
    },
    {
        "repo_url": "github.com/UCSC-VLAA/Sight-Beyond-Text",
        "filepath": "llava/model/builder.py",
        "commit_date": "2023-09-13T23:59:00Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/UCSC-VLAA/Sight-Beyond-Text",
        "filepath": "llava/eval/mmlu/mmlu_modeling.py",
        "commit_date": "2023-09-13T23:59:00Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/tmylla/HackMentor",
        "filepath": "chat.py",
        "commit_date": "2023-07-05T15:14:59Z",
        "message": "init commit, pre-hackmentor"
    },
    {
        "repo_url": "github.com/tmylla/HackMentor",
        "filepath": "eval/infer.py",
        "commit_date": "2023-07-05T15:14:59Z",
        "message": "init commit, pre-hackmentor"
    },
    {
        "repo_url": "github.com/tmylla/HackMentor",
        "filepath": "eval/ZenoEval/infer.py",
        "commit_date": "2023-07-05T15:14:59Z",
        "message": "init commit, pre-hackmentor"
    },
    {
        "repo_url": "github.com/tmylla/HackMentor",
        "filepath": "eval/ZenoEval/modeling.py",
        "commit_date": "2023-07-05T15:14:59Z",
        "message": "init commit, pre-hackmentor"
    },
    {
        "repo_url": "github.com/tmylla/HackMentor",
        "filepath": "train/fastchat/model/apply_lora.py",
        "commit_date": "2023-09-30T17:16:34Z",
        "message": "code released & readme updated"
    },
    {
        "repo_url": "github.com/claws-lab/XLingEval",
        "filepath": "correctness/MedAlpaca/inferer.py",
        "commit_date": "2023-10-20T15:53:04Z",
        "message": "Add the correctness experiment files for MedAlpaca"
    },
    {
        "repo_url": "github.com/claws-lab/XLingEval",
        "filepath": "consistency/Medalpaca/inferer.py",
        "commit_date": "2023-10-22T21:59:45Z",
        "message": "Update Consistency on Medalpaca"
    },
    {
        "repo_url": "github.com/claws-lab/XLingEval",
        "filepath": "consistency/Medalpaca/inferer.py",
        "commit_date": "2023-10-22T20:53:15Z",
        "message": "Add Medalpaca for consistency experiments"
    },
    {
        "repo_url": "github.com/claws-lab/XLingEval",
        "filepath": "verifiability/Medalpaca/inferer.py",
        "commit_date": "2023-10-22T22:02:37Z",
        "message": "Add Medalpaca model to Verifiability"
    },
    {
        "repo_url": "github.com/chuanyangjin/MMToM-QA",
        "filepath": "BIP-ALM/inverse_symbolic_planner.py",
        "commit_date": "2024-01-11T22:52:47Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/Wusiwei0410/SciMMIR",
        "filepath": "src/LLM_models/MyLLaVA/builder.py",
        "commit_date": "2024-01-25T06:03:58Z",
        "message": "add codes"
    },
    {
        "repo_url": "github.com/Wusiwei0410/SciMMIR",
        "filepath": "src/LLM_models/Mymplug_owl2/builder.py",
        "commit_date": "2024-01-25T06:03:58Z",
        "message": "add codes"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/deepspeed_predictor.py",
        "commit_date": "2024-02-28T07:10:56Z",
        "message": "Enable mllm api (#107)\n\n* Necessary codes fixing and enable openai API in webui\n\nSigned-off-by: Xue, Chendi <chendi.xue@intel.com>\n\n* Fix ruff error\n\nSigned-off-by: Xue, Chendi <chendi.xue@intel.com>\n\n* add new MLLM predcitor to LLM-on-Ray\n\nSigned-off-by: Xue, Chendi <chendi.xue@intel.com>\n\n* Add langchain and openai test\n\nSigned-off-by: Xue, Chendi <chendi.xue@intel.com>\n\n* Fix formatting\n\nSigned-off-by: Xue, Chendi <chendi.xue@intel.com>\n\n* Fix issues per Carson's request\n\nSigned-off-by: Xue, Chendi <chendi.xue@intel.com>\n\n* MLLM predictor update per Carson's request\n\nSigned-off-by: Xue, Chendi <chendi.xue@intel.com>\n\n* Fix parsing issue\n\nSigned-off-by: Xue, Chendi <chendi.xue@intel.com>\n\n* Modifications for UI format\n\n* Update codes per second review comments\n\nSigned-off-by: Xue, Chendi <chendi.xue@intel.com>\n\n* Add two example for inference with openai API and langchain API\n\nSigned-off-by: Xue, Chendi <chendi.xue@intel.com>\n\n* Add start_ui fixing\n\nSigned-off-by: Xue, Chendi <chendi.xue@intel.com>\n\n* using os env for OPENAI_API_KEY and OPENAI_BASE_URL\n\nSigned-off-by: Xue, Chendi <chendi.xue@intel.com>\n\n* update format\n\nSigned-off-by: Xue, Chendi <chendi.xue@intel.com>\n\n---------\n\nSigned-off-by: Xue, Chendi <chendi.xue@intel.com>\nSigned-off-by: Chendi.Xue <chendi.xue@intel.com>\nCo-authored-by: KepingYan <keping.yan@intel.com>"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/deepspeed_predictor.py",
        "commit_date": "2024-02-07T02:11:55Z",
        "message": "[Inference] Fix auth token and add models starcoder and llama2 (#39)\n\n* add starcoder and enable llama2\n\n* nit\n\n* nit\n\n* revert\n\n* add token\n\n* dedup\n\n* add token to from_pretrained\n\n* pass auth token to from_pretrained\n\n* nit\n\n* add auth tokens\n\n* lint\n\n* fix lint\n\n* nit\n\n* deepspeed not support starcoder\n\n* nit\n\n* remove from ci\n\n* remove direct auth token\n\n* add back ci workflow temporarily\n\n* remove from ci\n\n* add load environment and enable 2 models again\n\n* add dir\n\n* add load environment and enable 2 models again\n\n* change proxy\n\n* revert proxy\n\n* change proxy\n\n* revert proxy\n\n* remove 2 models from ci\n\n---------\n\nSigned-off-by: Yizhong Zhang <zyzzxycj@163.com>"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/deepspeed_predictor.py",
        "commit_date": "2024-02-05T12:29:34Z",
        "message": "Support vllm for openai api, refactor openai non-streaming api (#86)\n\n* add vllm support for openai mode\n\n* fix streaming response\n\n* refactor openai call function\n\n* fix tokens' length\n\n* fix tokens' length for vllm\n\n* run ci\n\n* modify\n\n* remove return_shape param, add data class\n\n* modify\n\n* modify\n\n* address comments\n\n* modify\n\n* minimal modify after testing again"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/deepspeed_predictor.py",
        "commit_date": "2024-01-22T08:32:41Z",
        "message": "[Serving] Support multiple prompts for generate (#62)\n\n* support multiple prompts\n\n* fix error response\n\n* fix\n\n* edit type\n\n* update\n\n* update"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/deepspeed_predictor.py",
        "commit_date": "2024-01-18T07:43:11Z",
        "message": "Add vllm Predictor (#20)\n\n* add vllm_predictor\n\n* add tests skeleton\n\n* add tests skeleton\n\n* add pytest.ini\n\n* wip\n\n* complete, debug wip\n\n* nit\n\n* nit\n\n* nit\n\n* complete generate supporting str and List[str]\n\n* add model\n\n* add streaming\n\n* remove tests\n\n* Add install-vllm-cpu script\n\n* nit\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>\n\n* nit\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>\n\n* nit\n\n* fix package inference\n\n* update install script and add doc\n\n* nit\n\n* nit\n\n* nit\n\n* add dtype support\n\n* nit\n\n* nit\n\n* nit\n\n* add ci\n\n* nit\n\n* nit\n\n* add libpthread-stubs0-dev\n\n* fix install-vllm-cpu\n\n* fix\n\n* revert inference.inference_config\n\n* debug ci\n\n* debug ci\n\n* debug ci\n\n* debug ci\n\n* debug ci\n\n* debug ci\n\n* debug ci\n\n* debug ci\n\n* update\n\n---------\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/deepspeed_predictor.py",
        "commit_date": "2024-01-11T13:05:03Z",
        "message": "[Lint] add lint (#34)\n\n* add pre-commit, port from https://github.com/intel-sandbox/llm-ray/pull/185\n\n* move lint check to github\n\n* modify permission\n\n* test\n\n* test\n\n* test\n\n* test\n\n* move to ubuntu-latest\n\n* fix lint\n\n* fix parameter error\n\n* Recover lines that should not be deleted\n\n* test lint in ci\n\n* done\n\n* add needs in ci\n\n* move hpu tokenizer"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/deepspeed_predictor.py",
        "commit_date": "2024-01-09T08:45:21Z",
        "message": "make ui demo independent (#29)\n\n* make ui demo independent\n\n* fix\n\n* upd path in doc"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/deepspeed_predictor.py",
        "commit_date": "2023-12-21T06:05:27Z",
        "message": "Sync with internal (cherry-picked from 644488 to 25118e3) (#9)\n\n* merged [common] unified conf to yaml\n\nSigned-off-by: Jiafu Zhang <jiafu.zhang@intel.com>\n\n* reconstruct config by moving ipex and precision to ipex struct (#168)\n\n* reconstruct config by moving ipex and precision to ipex struct\n\nSigned-off-by: Jiafu Zhang <jiafu.zhang@intel.com>\n\n* reconstruct config by moving ipex and precision to ipex struct\n\nSigned-off-by: Jiafu Zhang <jiafu.zhang@intel.com>\n\n---------\n\nSigned-off-by: Jiafu Zhang <jiafu.zhang@intel.com>\n\n* [Inference] Add Neural-chat inference support (#149)\n\n* add neural chat inference\n\n* change transformers from 4.31 to 4.35\n\n* update prompt\n\n* nit\n\n* trigger ci\n\n* remove from ci\n\n* add auth token to all models\n\n* revert\n\n* merged [Inference] Add Neural-chat inference support\n\nSigned-off-by: Jiafu Zhang <jiafu.zhang@intel.com>\n\n* remove llama-2-7b in inferencce ci since ipex failed to optimize it\n\nSigned-off-by: Jiafu Zhang <jiafu.zhang@intel.com>\n\n* Add the HFTonkenizer patch for Model-References (#169)\n\nAdd the HFTokenizer patch\nAdd the pretrain_module to invoke different pretrain module\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\n\n* Abstract common features into Predictor (#166)\n\n* fix bug of precess config; use tokenizer.__call__\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* init\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* create utils & move tokenizer to predictor\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* fix\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* change inferCfg to infer_conf; simplify code\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* fix\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* replace inferenceConfig with infer_conf\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* fix deepspeed\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* further simplify\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* move actor_options to utils\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* fix\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* remove input len\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* remove input len follow-up\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n---------\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* Update the Dockerfile.optimum.habana (#184)\n\nUpdathe the dockerfile\nFix the HABANA_VISIBLE_MODULES  envs issue\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\n\n* Update the Pretrain ReadME (#186)\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\n\n* Move the dockerfiles of pretrain into pretrain/docker (#187)\n\nDelete the useless dp dockerfile\n\nChange the nvidia GPU dockerfile name, because it use the same\ndockerfile for both megatron-deepspeed and huggingface trainer\n\nrefactor the folder path of pretrain\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\n\n* renamed workflow_orders to workflow_orders_on_pr\n\nSigned-off-by: Jiafu Zhang <jiafu.zhang@intel.com>\n\n---------\n\nSigned-off-by: Jiafu Zhang <jiafu.zhang@intel.com>\nSigned-off-by: yuanwu <yuan.wu@intel.com>\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\nCo-authored-by: harborn <gangsheng.wu@intel.com>\nCo-authored-by: Yizhong Zhang <zyzzxycj@163.com>\nCo-authored-by: yuanwu2017 <yuan.wu@intel.com>\nCo-authored-by: Zhi Lin <zhi.lin@intel.com>"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/deepspeed_predictor.py",
        "commit_date": "2023-11-30T05:36:40Z",
        "message": "change the way to build project by using pyproject.toml (#143)\n\n* change the way to build project by using pyproject.toml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* change the way to build project by using pyproject.toml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* change the way to build project by using pyproject.toml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* change the way to build project by using pyproject.toml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* change the way to build project by using pyproject.toml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* change the way to build project by using pyproject.toml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* remove unused import and variable\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n---------\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/deepspeed_predictor.py",
        "commit_date": "2023-11-28T08:22:51Z",
        "message": "add gaudi support for single process serving (#117)\n\n* add gaudi support for single process serving\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* rollback deepspeed predictor & deal with hpu\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* optimize logic of ipex and hpu graph\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* add dockerfile and readme for habana\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* move import inside to avoid deepspeed import\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* fix deepspeed test\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* add comments\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* fix\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* add resource requirement for HPU\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* fix\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* change device_name to device.type\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* update base image\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* fix\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* fix model_id\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* deduplicate code\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* use_hpu_graphs default to True and print warning\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* import ipex in actor if ipex is enabled\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* add dep in Dockerfile\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* import when necessary\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n---------\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/deepspeed_predictor.py",
        "commit_date": "2023-11-23T08:18:03Z",
        "message": "refactor argument passing and model configs by using yaml (#127)\n\n* refactoring\n\nSigned-off-by: Jiafu Zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* debugging gpt-j-6b\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n---------\n\nSigned-off-by: Jiafu Zhang <jiafu.zhang@intel.com>\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/deepspeed_predictor.py",
        "commit_date": "2023-11-16T07:34:03Z",
        "message": "Ipex upgrade (#121)\n\n* merge and update\n\n* upd\n\n* update asyncio.sleep time"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/deepspeed_predictor.py",
        "commit_date": "2023-11-15T01:27:18Z",
        "message": "[Enhancement] Upgrade Ray from 2.5 to 3.0 dev and Remove RayDP (#112)\n\n* Upgrade Ray from 2.5 to 3.0 dev\n\n* remove raydp\n\n* replace Checkpoint to TorchCheckpoint"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/deepspeed_predictor.py",
        "commit_date": "2023-11-14T02:02:03Z",
        "message": "deltatuner integration (#87)\n\n* add dependency\n\n* enable delta model in common lib\n\n* add sample best structure for deltatuner\n\n* add workflow file\n\n* update peft version\n\n* update deltatuner version\n\n* enable empty config for peft and deltatuner\n\n* path fix\n\n* add version in requirements file\n\n* update params doc\n\n* added models in the CICD configure file\n\n* added the best model structures\n\n* enable transformer predictor\n\n* enable deepspeed predictor\n\n* refine args\n\n* update workflow file\n\n* using latest deltatuner\n\n* remove external best structure\n\n* update best structure path\n\n* empty matrix fix\n\n* add default params\n\n* update version\n\n* fix workflow yml\n\n* update version\n\n* fix workflow yml\n\n* fix workflow yml\n\n* fix path in workflow yml\n\n* fix workflow yml\n\n* update version\n\n* update version\n\n* refine input model to peft model\n\n* refine workflow yml\n\n* remove tokenizer & add the condition on lora\n\n* reduce workflow checks\n\n* fix workflow yml\n\n* fix regular exp\n\n* fix eof\n\n* fix eof\n\n* fix format\n\n* update models\n\n* update models\n\n---------\n\nCo-authored-by: tianyil1 <tianyi.liu@intel.com>"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/deepspeed_predictor.py",
        "commit_date": "2023-11-13T03:20:15Z",
        "message": "Remove Dialogue actor and refactor streaming (#111)\n\n* remove dialogue actor and refactor streaming\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* add to_device\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* fix\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* fix deepspeed streaming\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* fix\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n---------\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/deepspeed_predictor.py",
        "commit_date": "2023-11-09T02:02:13Z",
        "message": "support gpu device (#71)\n\n* support gpu device\n\n* support gpu for inference\n\n* set cpus_per_worker\n\n* set gpus_per_worker\n\n* support pvc in Inference"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/deepspeed_predictor.py",
        "commit_date": "2023-11-02T10:31:45Z",
        "message": "Adapt the latest code for ui demo | add llama2 model (#73)\n\n* Adapt the latest code for ui demo\n\n* fix bug about progress bar\n\n* modify trust_remote_code"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/deepspeed_predictor.py",
        "commit_date": "2023-11-01T08:03:43Z",
        "message": "Fix deepspeed streaming_generate (#98)\n\n* Fix deepspeed streaming_generate\n\n* add dummy streamer\n\n* nit\n\n* nit\n\n* add --streaming_response to CI\n\n* rewrite DummyStreamer\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>\n\n* nit\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>\n\n* update\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>\n\n* update\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>\n\n* disable mpt\n\n---------\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/deepspeed_predictor.py",
        "commit_date": "2023-09-15T03:51:44Z",
        "message": "Enable DeepSpeed for CPU inference (#42)\n\n* Add deepspeed_inference.py\n\n* update\n\n* fix input\n\n* update\n\n* update scaling config\n\n* Add streaming_generate\n\n* Add DeepSpeedPredictor to model serve\n\n* update README\n\n* Add deepspeed_inference_example\n\n* remove test\n\n* fix return value bug\n\n* Fix DeepSpeedPredictor\n\n* fix generate\n\n* remove examples and debug print\n\n* remove *.sh\n\n* Add deepspeed requirements\n\n* enable DeepSpeed in CI\n\n* update Dockerfile\n\n* add device support\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* nit\n\n* nit\n\n* update\n\n* add install oneapi and start-ray-cluster scripts\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>\n\n* remove sudo for docker\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>\n\n* fix path\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>\n\n* nit\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>\n\n* nit\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>\n\n* nit\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>\n\n* nit\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>\n\n* nit\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>\n\n* update\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>\n\n* update deepspeed version\n\n---------\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/transformer_predictor.py",
        "commit_date": "2024-02-07T02:11:55Z",
        "message": "[Inference] Fix auth token and add models starcoder and llama2 (#39)\n\n* add starcoder and enable llama2\n\n* nit\n\n* nit\n\n* revert\n\n* add token\n\n* dedup\n\n* add token to from_pretrained\n\n* pass auth token to from_pretrained\n\n* nit\n\n* add auth tokens\n\n* lint\n\n* fix lint\n\n* nit\n\n* deepspeed not support starcoder\n\n* nit\n\n* remove from ci\n\n* remove direct auth token\n\n* add back ci workflow temporarily\n\n* remove from ci\n\n* add load environment and enable 2 models again\n\n* add dir\n\n* add load environment and enable 2 models again\n\n* change proxy\n\n* revert proxy\n\n* change proxy\n\n* revert proxy\n\n* remove 2 models from ci\n\n---------\n\nSigned-off-by: Yizhong Zhang <zyzzxycj@163.com>"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/transformer_predictor.py",
        "commit_date": "2024-02-05T12:29:34Z",
        "message": "Support vllm for openai api, refactor openai non-streaming api (#86)\n\n* add vllm support for openai mode\n\n* fix streaming response\n\n* refactor openai call function\n\n* fix tokens' length\n\n* fix tokens' length for vllm\n\n* run ci\n\n* modify\n\n* remove return_shape param, add data class\n\n* modify\n\n* modify\n\n* address comments\n\n* modify\n\n* minimal modify after testing again"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/transformer_predictor.py",
        "commit_date": "2024-01-22T08:32:41Z",
        "message": "[Serving] Support multiple prompts for generate (#62)\n\n* support multiple prompts\n\n* fix error response\n\n* fix\n\n* edit type\n\n* update\n\n* update"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/transformer_predictor.py",
        "commit_date": "2024-01-18T07:43:11Z",
        "message": "Add vllm Predictor (#20)\n\n* add vllm_predictor\n\n* add tests skeleton\n\n* add tests skeleton\n\n* add pytest.ini\n\n* wip\n\n* complete, debug wip\n\n* nit\n\n* nit\n\n* nit\n\n* complete generate supporting str and List[str]\n\n* add model\n\n* add streaming\n\n* remove tests\n\n* Add install-vllm-cpu script\n\n* nit\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>\n\n* nit\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>\n\n* nit\n\n* fix package inference\n\n* update install script and add doc\n\n* nit\n\n* nit\n\n* nit\n\n* add dtype support\n\n* nit\n\n* nit\n\n* nit\n\n* add ci\n\n* nit\n\n* nit\n\n* add libpthread-stubs0-dev\n\n* fix install-vllm-cpu\n\n* fix\n\n* revert inference.inference_config\n\n* debug ci\n\n* debug ci\n\n* debug ci\n\n* debug ci\n\n* debug ci\n\n* debug ci\n\n* debug ci\n\n* debug ci\n\n* update\n\n---------\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/transformer_predictor.py",
        "commit_date": "2024-01-11T13:05:03Z",
        "message": "[Lint] add lint (#34)\n\n* add pre-commit, port from https://github.com/intel-sandbox/llm-ray/pull/185\n\n* move lint check to github\n\n* modify permission\n\n* test\n\n* test\n\n* test\n\n* test\n\n* move to ubuntu-latest\n\n* fix lint\n\n* fix parameter error\n\n* Recover lines that should not be deleted\n\n* test lint in ci\n\n* done\n\n* add needs in ci\n\n* move hpu tokenizer"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/transformer_predictor.py",
        "commit_date": "2024-01-09T08:45:21Z",
        "message": "make ui demo independent (#29)\n\n* make ui demo independent\n\n* fix\n\n* upd path in doc"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/transformer_predictor.py",
        "commit_date": "2023-12-21T06:05:27Z",
        "message": "Sync with internal (cherry-picked from 644488 to 25118e3) (#9)\n\n* merged [common] unified conf to yaml\n\nSigned-off-by: Jiafu Zhang <jiafu.zhang@intel.com>\n\n* reconstruct config by moving ipex and precision to ipex struct (#168)\n\n* reconstruct config by moving ipex and precision to ipex struct\n\nSigned-off-by: Jiafu Zhang <jiafu.zhang@intel.com>\n\n* reconstruct config by moving ipex and precision to ipex struct\n\nSigned-off-by: Jiafu Zhang <jiafu.zhang@intel.com>\n\n---------\n\nSigned-off-by: Jiafu Zhang <jiafu.zhang@intel.com>\n\n* [Inference] Add Neural-chat inference support (#149)\n\n* add neural chat inference\n\n* change transformers from 4.31 to 4.35\n\n* update prompt\n\n* nit\n\n* trigger ci\n\n* remove from ci\n\n* add auth token to all models\n\n* revert\n\n* merged [Inference] Add Neural-chat inference support\n\nSigned-off-by: Jiafu Zhang <jiafu.zhang@intel.com>\n\n* remove llama-2-7b in inferencce ci since ipex failed to optimize it\n\nSigned-off-by: Jiafu Zhang <jiafu.zhang@intel.com>\n\n* Add the HFTonkenizer patch for Model-References (#169)\n\nAdd the HFTokenizer patch\nAdd the pretrain_module to invoke different pretrain module\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\n\n* Abstract common features into Predictor (#166)\n\n* fix bug of precess config; use tokenizer.__call__\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* init\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* create utils & move tokenizer to predictor\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* fix\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* change inferCfg to infer_conf; simplify code\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* fix\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* replace inferenceConfig with infer_conf\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* fix deepspeed\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* further simplify\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* move actor_options to utils\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* fix\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* remove input len\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* remove input len follow-up\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n---------\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* Update the Dockerfile.optimum.habana (#184)\n\nUpdathe the dockerfile\nFix the HABANA_VISIBLE_MODULES  envs issue\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\n\n* Update the Pretrain ReadME (#186)\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\n\n* Move the dockerfiles of pretrain into pretrain/docker (#187)\n\nDelete the useless dp dockerfile\n\nChange the nvidia GPU dockerfile name, because it use the same\ndockerfile for both megatron-deepspeed and huggingface trainer\n\nrefactor the folder path of pretrain\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\n\n* renamed workflow_orders to workflow_orders_on_pr\n\nSigned-off-by: Jiafu Zhang <jiafu.zhang@intel.com>\n\n---------\n\nSigned-off-by: Jiafu Zhang <jiafu.zhang@intel.com>\nSigned-off-by: yuanwu <yuan.wu@intel.com>\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\nCo-authored-by: harborn <gangsheng.wu@intel.com>\nCo-authored-by: Yizhong Zhang <zyzzxycj@163.com>\nCo-authored-by: yuanwu2017 <yuan.wu@intel.com>\nCo-authored-by: Zhi Lin <zhi.lin@intel.com>"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/transformer_predictor.py",
        "commit_date": "2023-12-06T11:41:35Z",
        "message": "integrate bigl-llm[cpu] (#150)\n\n* change the way to build project by using pyproject.toml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* change the way to build project by using pyproject.toml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* change the way to build project by using pyproject.toml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* change the way to build project by using pyproject.toml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* change the way to build project by using pyproject.toml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* change the way to build project by using pyproject.toml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* remove unused import and variable\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* integrate bigdl-llm[cpu]\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* fix deepspeed path\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* fix deepspeed path\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* add ci for bigdl\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* add ci for bigdl\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* add ci for bigdl\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* add ci for bigdl\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* add ci for bigdl\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* add ci for bigdl\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* add ci for bigdl\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* add ci for bigdl\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* restore dict() method since deepspeed has lower pydantic version deps\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* separate Dockerfiles for ci efficiency\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* remove load_in_8bit since our GPU doesn't support bitsandbytes\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* remove ds_report and oneapi related commands in bigdl-cpu dockerfile\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* remove ds_report and oneapi related commands in bigdl-cpu dockerfile\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* escape inference with deepspeed for bigdl model\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* added hpus_per_worker to inference config template\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* rename model_source to bigdl and updated docs\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n---------\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/transformer_predictor.py",
        "commit_date": "2023-11-28T08:22:51Z",
        "message": "add gaudi support for single process serving (#117)\n\n* add gaudi support for single process serving\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* rollback deepspeed predictor & deal with hpu\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* optimize logic of ipex and hpu graph\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* add dockerfile and readme for habana\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* move import inside to avoid deepspeed import\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* fix deepspeed test\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* add comments\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* fix\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* add resource requirement for HPU\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* fix\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* change device_name to device.type\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* update base image\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* fix\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* fix model_id\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* deduplicate code\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* use_hpu_graphs default to True and print warning\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* import ipex in actor if ipex is enabled\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* add dep in Dockerfile\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* import when necessary\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n---------\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/transformer_predictor.py",
        "commit_date": "2023-11-23T08:18:03Z",
        "message": "refactor argument passing and model configs by using yaml (#127)\n\n* refactoring\n\nSigned-off-by: Jiafu Zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* refactor argument passing and model configs by using yaml\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n* debugging gpt-j-6b\n\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>\n\n---------\n\nSigned-off-by: Jiafu Zhang <jiafu.zhang@intel.com>\nSigned-off-by: jiafu zhang <jiafu.zhang@intel.com>"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/transformer_predictor.py",
        "commit_date": "2023-11-16T07:34:03Z",
        "message": "Ipex upgrade (#121)\n\n* merge and update\n\n* upd\n\n* update asyncio.sleep time"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/transformer_predictor.py",
        "commit_date": "2023-11-14T02:02:03Z",
        "message": "deltatuner integration (#87)\n\n* add dependency\n\n* enable delta model in common lib\n\n* add sample best structure for deltatuner\n\n* add workflow file\n\n* update peft version\n\n* update deltatuner version\n\n* enable empty config for peft and deltatuner\n\n* path fix\n\n* add version in requirements file\n\n* update params doc\n\n* added models in the CICD configure file\n\n* added the best model structures\n\n* enable transformer predictor\n\n* enable deepspeed predictor\n\n* refine args\n\n* update workflow file\n\n* using latest deltatuner\n\n* remove external best structure\n\n* update best structure path\n\n* empty matrix fix\n\n* add default params\n\n* update version\n\n* fix workflow yml\n\n* update version\n\n* fix workflow yml\n\n* fix workflow yml\n\n* fix path in workflow yml\n\n* fix workflow yml\n\n* update version\n\n* update version\n\n* refine input model to peft model\n\n* refine workflow yml\n\n* remove tokenizer & add the condition on lora\n\n* reduce workflow checks\n\n* fix workflow yml\n\n* fix regular exp\n\n* fix eof\n\n* fix eof\n\n* fix format\n\n* update models\n\n* update models\n\n---------\n\nCo-authored-by: tianyil1 <tianyi.liu@intel.com>"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/transformer_predictor.py",
        "commit_date": "2023-11-13T03:20:15Z",
        "message": "Remove Dialogue actor and refactor streaming (#111)\n\n* remove dialogue actor and refactor streaming\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* add to_device\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* fix\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* fix deepspeed streaming\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n* fix\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>\n\n---------\n\nSigned-off-by: Zhi Lin <zhi.lin@intel.com>"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/transformer_predictor.py",
        "commit_date": "2023-11-09T02:02:13Z",
        "message": "support gpu device (#71)\n\n* support gpu device\n\n* support gpu for inference\n\n* set cpus_per_worker\n\n* set gpus_per_worker\n\n* support pvc in Inference"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/transformer_predictor.py",
        "commit_date": "2023-11-02T10:31:45Z",
        "message": "Adapt the latest code for ui demo | add llama2 model (#73)\n\n* Adapt the latest code for ui demo\n\n* fix bug about progress bar\n\n* modify trust_remote_code"
    },
    {
        "repo_url": "github.com/intel/llm-on-ray",
        "filepath": "inference/transformer_predictor.py",
        "commit_date": "2023-09-15T03:51:44Z",
        "message": "Enable DeepSpeed for CPU inference (#42)\n\n* Add deepspeed_inference.py\n\n* update\n\n* fix input\n\n* update\n\n* update scaling config\n\n* Add streaming_generate\n\n* Add DeepSpeedPredictor to model serve\n\n* update README\n\n* Add deepspeed_inference_example\n\n* remove test\n\n* fix return value bug\n\n* Fix DeepSpeedPredictor\n\n* fix generate\n\n* remove examples and debug print\n\n* remove *.sh\n\n* Add deepspeed requirements\n\n* enable DeepSpeed in CI\n\n* update Dockerfile\n\n* add device support\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* nit\n\n* nit\n\n* update\n\n* add install oneapi and start-ray-cluster scripts\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>\n\n* remove sudo for docker\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>\n\n* fix path\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>\n\n* nit\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>\n\n* nit\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>\n\n* nit\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>\n\n* nit\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>\n\n* nit\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>\n\n* update\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>\n\n* update deepspeed version\n\n---------\n\nSigned-off-by: Wu, Xiaochang <xiaochang.wu@intel.com>"
    },
    {
        "repo_url": "github.com/TongjiFinLab/CFGPT",
        "filepath": "code/test/eval-generate.py",
        "commit_date": "2023-11-01T07:04:42Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/HKUNLP/multilingual-transfer",
        "filepath": "finetune.py",
        "commit_date": "2023-06-13T06:17:53Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/HKUNLP/multilingual-transfer",
        "filepath": "evaluation.py",
        "commit_date": "2023-06-13T06:17:53Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/volotat/Anagnorisis",
        "filepath": "llm_engine.py",
        "commit_date": "2024-02-24T22:41:30Z",
        "message": "v 0.0.6"
    },
    {
        "repo_url": "github.com/volotat/Anagnorisis",
        "filepath": "llm_engine.py",
        "commit_date": "2024-01-13T00:41:59Z",
        "message": "wiki rendering"
    },
    {
        "repo_url": "github.com/volotat/Anagnorisis",
        "filepath": "llm_engine.py",
        "commit_date": "2024-01-06T09:58:56Z",
        "message": "v0.0.4"
    },
    {
        "repo_url": "github.com/volotat/Anagnorisis",
        "filepath": "llm_engine.py",
        "commit_date": "2023-12-16T00:16:39Z",
        "message": "Version 0.0.3"
    },
    {
        "repo_url": "github.com/volotat/Anagnorisis",
        "filepath": "llm_engine.py",
        "commit_date": "2023-12-09T21:57:29Z",
        "message": "v 0.0.2"
    },
    {
        "repo_url": "github.com/volotat/Anagnorisis",
        "filepath": "llm_engine.py",
        "commit_date": "2023-10-08T21:32:06Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/volotat/Anagnorisis",
        "filepath": "src/fine_tune_page.py",
        "commit_date": "2024-02-24T22:41:30Z",
        "message": "v 0.0.6"
    },
    {
        "repo_url": "github.com/volotat/Anagnorisis",
        "filepath": "src/fine_tune_page.py",
        "commit_date": "2024-01-13T00:41:59Z",
        "message": "wiki rendering"
    },
    {
        "repo_url": "github.com/volotat/Anagnorisis",
        "filepath": "src/fine_tune_page.py",
        "commit_date": "2023-12-22T23:32:58Z",
        "message": "fine-tunning improvements"
    },
    {
        "repo_url": "github.com/volotat/Anagnorisis",
        "filepath": "src/fine_tune_page.py",
        "commit_date": "2023-12-09T21:57:29Z",
        "message": "v 0.0.2"
    },
    {
        "repo_url": "github.com/volotat/Anagnorisis",
        "filepath": "src/fine_tune_page.py",
        "commit_date": "2023-10-08T21:32:06Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/fengredrum/finetune-whisper-lora",
        "filepath": "eval_lora.py",
        "commit_date": "2023-10-12T01:47:11Z",
        "message": "Setup LoRA and kbit parameters"
    },
    {
        "repo_url": "github.com/fengredrum/finetune-whisper-lora",
        "filepath": "eval_lora.py",
        "commit_date": "2023-09-13T03:20:08Z",
        "message": "Update eval_lora.py"
    },
    {
        "repo_url": "github.com/fengredrum/finetune-whisper-lora",
        "filepath": "eval_lora.py",
        "commit_date": "2023-09-11T06:25:06Z",
        "message": "Add ArgumentParser for evaluation"
    },
    {
        "repo_url": "github.com/fengredrum/finetune-whisper-lora",
        "filepath": "eval_lora.py",
        "commit_date": "2023-09-10T09:12:28Z",
        "message": "Update eval_lora.py"
    },
    {
        "repo_url": "github.com/fengredrum/finetune-whisper-lora",
        "filepath": "eval_lora.py",
        "commit_date": "2023-09-10T06:59:31Z",
        "message": "Update eval_lora.py for no stream mode"
    },
    {
        "repo_url": "github.com/fengredrum/finetune-whisper-lora",
        "filepath": "eval_lora.py",
        "commit_date": "2023-09-07T04:39:34Z",
        "message": "Add datasets support"
    },
    {
        "repo_url": "github.com/fengredrum/finetune-whisper-lora",
        "filepath": "eval_lora.py",
        "commit_date": "2023-08-29T14:53:17Z",
        "message": "Finetune lora with stream"
    },
    {
        "repo_url": "github.com/fengredrum/finetune-whisper-lora",
        "filepath": "eval_lora.py",
        "commit_date": "2023-08-07T03:11:42Z",
        "message": "Update eval_lora.py"
    },
    {
        "repo_url": "github.com/fengredrum/finetune-whisper-lora",
        "filepath": "eval_lora.py",
        "commit_date": "2023-08-06T14:08:34Z",
        "message": "Create eval_lora.py"
    },
    {
        "repo_url": "github.com/fengredrum/finetune-whisper-lora",
        "filepath": "convert_model/using_optimum/vanilla_onnx_convert.py",
        "commit_date": "2023-10-10T09:48:35Z",
        "message": "Add LoRa support"
    },
    {
        "repo_url": "github.com/fengredrum/finetune-whisper-lora",
        "filepath": "convert_model/using_optimum/vanilla_onnx_convert.py",
        "commit_date": "2023-09-27T01:28:43Z",
        "message": "Reorganize directory"
    },
    {
        "repo_url": "github.com/georgechen1827/ChatGLM-text-embedding",
        "filepath": "example_with_embeddings/models.py",
        "commit_date": "2023-04-09T04:13:20Z",
        "message": "add finetuned model and results"
    },
    {
        "repo_url": "github.com/FreedomIntelligence/ALLaVA",
        "filepath": "allava/model/builder.py",
        "commit_date": "2024-02-20T18:42:07Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/linhduongtuan/doctorwithbloom",
        "filepath": "chat.py",
        "commit_date": "2023-03-29T07:39:37Z",
        "message": "Update chat.py"
    },
    {
        "repo_url": "github.com/linhduongtuan/doctorwithbloom",
        "filepath": "chat.py",
        "commit_date": "2023-03-29T07:35:36Z",
        "message": "Update chat.py"
    },
    {
        "repo_url": "github.com/linhduongtuan/doctorwithbloom",
        "filepath": "chat.py",
        "commit_date": "2023-03-28T14:53:42Z",
        "message": "Update chat.py"
    },
    {
        "repo_url": "github.com/linhduongtuan/doctorwithbloom",
        "filepath": "chat.py",
        "commit_date": "2023-03-28T14:10:56Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/linhduongtuan/doctorwithbloom",
        "filepath": "generate.py",
        "commit_date": "2023-03-28T19:03:10Z",
        "message": "Update generate.py"
    },
    {
        "repo_url": "github.com/linhduongtuan/doctorwithbloom",
        "filepath": "generate.py",
        "commit_date": "2023-03-28T18:53:53Z",
        "message": "Update generate.py"
    },
    {
        "repo_url": "github.com/linhduongtuan/doctorwithbloom",
        "filepath": "generate.py",
        "commit_date": "2023-03-28T18:43:23Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/linhduongtuan/doctorwithbloom",
        "filepath": "generate.py",
        "commit_date": "2023-03-28T13:19:55Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/linhduongtuan/doctorwithbloom",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-03-28T18:43:23Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/linhduongtuan/doctorwithbloom",
        "filepath": "export_hf_checkpoint.py",
        "commit_date": "2023-03-28T13:19:55Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/linhduongtuan/doctorwithbloom",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-03-28T18:43:23Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/linhduongtuan/doctorwithbloom",
        "filepath": "export_state_dict_checkpoint.py",
        "commit_date": "2023-03-28T13:19:55Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/lawrence-cj/LLaMA-DiffFit",
        "filepath": "generate.py",
        "commit_date": "2023-05-02T05:59:21Z",
        "message": "train and inference code update"
    },
    {
        "repo_url": "github.com/lawrence-cj/LLaMA-DiffFit",
        "filepath": "generate.py",
        "commit_date": "2023-05-01T15:34:18Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/lawrence-cj/LLaMA-DiffFit",
        "filepath": "peft/src/peft/peft_model.py",
        "commit_date": "2023-05-01T15:34:18Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/lawrence-cj/LLaMA-DiffFit",
        "filepath": "peft/src/peft/tuners/prompt_tuning.py",
        "commit_date": "2023-05-01T15:34:18Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/jackaduma/Recurrent-LLM",
        "filepath": "models/baichuan_hf.py",
        "commit_date": "2023-06-20T06:05:04Z",
        "message": "update Baichuan LLM"
    },
    {
        "repo_url": "github.com/jackaduma/Recurrent-LLM",
        "filepath": "models/baichuan_hf.py",
        "commit_date": "2023-06-20T06:02:53Z",
        "message": "update Baichuan LLM :  it's so bad on writing novel."
    },
    {
        "repo_url": "github.com/jackaduma/Recurrent-LLM",
        "filepath": "models/baichuan_hf.py",
        "commit_date": "2023-06-16T12:17:36Z",
        "message": "support Baichuan LLM"
    },
    {
        "repo_url": "github.com/Umi7899/langchain-ChatGLM-My",
        "filepath": "models/loader/loader.py",
        "commit_date": "2023-06-26T06:44:36Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/CSHaitao/ChatGLM_mutli_gpu_tuning",
        "filepath": "infer_lora.py",
        "commit_date": "2023-05-15T14:30:18Z",
        "message": "Update infer_lora.py"
    },
    {
        "repo_url": "github.com/CSHaitao/ChatGLM_mutli_gpu_tuning",
        "filepath": "infer_lora.py",
        "commit_date": "2023-05-09T06:28:56Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/cascip/ChatAlpaca",
        "filepath": "utils/apply_lora.py",
        "commit_date": "2023-06-01T00:46:05Z",
        "message": "new data and models released"
    },
    {
        "repo_url": "github.com/pacman100/DHS-LLM-Workshop",
        "filepath": "chat_assistant/dpo/utils.py",
        "commit_date": "2024-02-02T11:06:26Z",
        "message": "support for merge and unload in DPO"
    },
    {
        "repo_url": "github.com/pacman100/DHS-LLM-Workshop",
        "filepath": "chat_assistant/dpo/utils.py",
        "commit_date": "2024-02-02T07:48:33Z",
        "message": "Support unsloth"
    },
    {
        "repo_url": "github.com/pacman100/DHS-LLM-Workshop",
        "filepath": "chat_assistant/dpo/utils.py",
        "commit_date": "2024-02-02T07:09:36Z",
        "message": "fix 4-bit quantization misspelling\n\nCo-Authored-By: zhewei li <43416079+we1k@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/pacman100/DHS-LLM-Workshop",
        "filepath": "chat_assistant/dpo/utils.py",
        "commit_date": "2024-02-01T08:12:17Z",
        "message": "Update utils.py"
    },
    {
        "repo_url": "github.com/pacman100/DHS-LLM-Workshop",
        "filepath": "chat_assistant/dpo/utils.py",
        "commit_date": "2024-02-01T08:10:29Z",
        "message": "Code refactor and addition of DPO finetuning script"
    },
    {
        "repo_url": "github.com/pacman100/DHS-LLM-Workshop",
        "filepath": "5_Module/chat_gradio_app/app.py",
        "commit_date": "2023-07-31T22:33:00Z",
        "message": "add content"
    },
    {
        "repo_url": "github.com/pacman100/DHS-LLM-Workshop",
        "filepath": "5_Module/chat_gradio_app/app.py",
        "commit_date": "2023-07-29T10:46:48Z",
        "message": "Update app.py"
    },
    {
        "repo_url": "github.com/pacman100/DHS-LLM-Workshop",
        "filepath": "5_Module/chat_gradio_app/app.py",
        "commit_date": "2023-07-29T08:40:27Z",
        "message": "Update app.py"
    },
    {
        "repo_url": "github.com/pacman100/DHS-LLM-Workshop",
        "filepath": "5_Module/chat_gradio_app/app.py",
        "commit_date": "2023-07-29T08:19:58Z",
        "message": "add gradio app"
    },
    {
        "repo_url": "github.com/usail-hkust/UrbanKGent",
        "filepath": "utils/llama_lora_app.py",
        "commit_date": "2024-02-17T03:46:59Z",
        "message": "Update llama_lora_app.py"
    },
    {
        "repo_url": "github.com/usail-hkust/UrbanKGent",
        "filepath": "utils/llama_lora_app.py",
        "commit_date": "2024-02-15T02:17:58Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/PKU-RL/LLaMA-Rider",
        "filepath": "skills/llm_planner.py",
        "commit_date": "2023-11-05T07:01:08Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/mobiusml/hqq",
        "filepath": "examples/lora/train_hqq_lora_example.py",
        "commit_date": "2024-01-08T17:16:20Z",
        "message": "v0.1.2 (LoRA + optimizer V2 + refactor)"
    },
    {
        "repo_url": "github.com/mobiusml/hqq",
        "filepath": "examples/lora/train_hqq_lora_example.py",
        "commit_date": "2024-01-04T15:47:26Z",
        "message": "Add LoRA + optimizer_v2"
    },
    {
        "repo_url": "github.com/zjunlp/IEPile",
        "filepath": "src/model/adapter.py",
        "commit_date": "2024-02-23T15:18:48Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/zjunlp/IEPile",
        "filepath": "src/model/adapter.py",
        "commit_date": "2024-02-22T16:52:50Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/vmware/vSphere-machine-learning-extension",
        "filepath": "examples/model_training/llm/bloom_fine_tune/merge_peft_adapters.py",
        "commit_date": "2023-08-14T10:24:30Z",
        "message": "Add Bloom fine-tune example"
    },
    {
        "repo_url": "github.com/git-cloner/Llama2-chinese",
        "filepath": "generate.py",
        "commit_date": "2023-07-29T15:44:34Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/git-cloner/Llama2-chinese",
        "filepath": "api_stream.py",
        "commit_date": "2023-07-30T04:37:09Z",
        "message": "increase max_token_size"
    },
    {
        "repo_url": "github.com/git-cloner/Llama2-chinese",
        "filepath": "api_stream.py",
        "commit_date": "2023-07-29T15:44:34Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/git-cloner/Llama2-chinese",
        "filepath": "inference/model_utils.py",
        "commit_date": "2023-07-29T15:44:34Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/git-cloner/Llama2-chinese",
        "filepath": "inference/safety_utils.py",
        "commit_date": "2023-07-29T15:44:34Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/git-cloner/Llama2-chinese",
        "filepath": "inference/chat_completion.py",
        "commit_date": "2023-07-29T15:44:34Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/git-cloner/Llama2-chinese",
        "filepath": "inference/hf-text-generation-inference/merge_lora_weights.py",
        "commit_date": "2023-07-29T15:44:34Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/andysingal/llm-course",
        "filepath": "transformers/AlpaGasus2-QLoRA/evaluation/AlpaGasus-Evaluation/response_data/generate.py",
        "commit_date": "2023-08-31T08:35:18Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/feizc/Visual-LLaMA",
        "filepath": "finetune.py",
        "commit_date": "2023-04-16T07:20:49Z",
        "message": "debug for issue #2"
    },
    {
        "repo_url": "github.com/feizc/Visual-LLaMA",
        "filepath": "finetune.py",
        "commit_date": "2023-04-05T08:15:06Z",
        "message": "lora for llm"
    },
    {
        "repo_url": "github.com/feizc/Visual-LLaMA",
        "filepath": "finetune.py",
        "commit_date": "2023-04-05T06:59:14Z",
        "message": "support lora"
    },
    {
        "repo_url": "github.com/feizc/Visual-LLaMA",
        "filepath": "finetune.py",
        "commit_date": "2023-04-04T07:25:57Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/hf-lin/ChatMusician",
        "filepath": "model/train/merge.py",
        "commit_date": "2023-12-13T18:35:30Z",
        "message": "add files"
    },
    {
        "repo_url": "github.com/hf-lin/ChatMusician",
        "filepath": "model/infer/predict.py",
        "commit_date": "2023-12-13T18:35:30Z",
        "message": "add files"
    },
    {
        "repo_url": "github.com/A-baoYang/alpaca-7b-chinese",
        "filepath": "serve/model.py",
        "commit_date": "2023-04-24T14:26:43Z",
        "message": "[feature] serving add support on AutoModel, BloomForCasualLM"
    },
    {
        "repo_url": "github.com/A-baoYang/alpaca-7b-chinese",
        "filepath": "serve/model.py",
        "commit_date": "2023-03-31T04:07:23Z",
        "message": "[other] Fix path to avoid confusing"
    },
    {
        "repo_url": "github.com/A-baoYang/alpaca-7b-chinese",
        "filepath": "serve/model.py",
        "commit_date": "2023-03-30T06:13:53Z",
        "message": "[other] dataset fix & path update"
    },
    {
        "repo_url": "github.com/A-baoYang/alpaca-7b-chinese",
        "filepath": "serve/model.py",
        "commit_date": "2023-03-29T09:30:31Z",
        "message": "[serve] Update API & UIUI& UII serving"
    },
    {
        "repo_url": "github.com/A-baoYang/alpaca-7b-chinese",
        "filepath": "validation/generate.py",
        "commit_date": "2023-04-24T14:25:37Z",
        "message": "[feature] Add validation scripts (inference)"
    },
    {
        "repo_url": "github.com/A-baoYang/alpaca-7b-chinese",
        "filepath": "validation/batch_generate.py",
        "commit_date": "2023-04-24T14:25:37Z",
        "message": "[feature] Add validation scripts (inference)"
    },
    {
        "repo_url": "github.com/aws-samples/aws-ai-ml-workshop-kr",
        "filepath": "genai/jumpstart/text_to_text/flan_t5_xl_with_LoRA/run_clm.py",
        "commit_date": "2023-06-26T22:26:50Z",
        "message": "genai-initialize"
    },
    {
        "repo_url": "github.com/aws-samples/aws-ai-ml-workshop-kr",
        "filepath": "genai/jumpstart/text_to_text/flan_t5_xl_with_LoRA/inference.py",
        "commit_date": "2023-06-26T22:26:50Z",
        "message": "genai-initialize"
    },
    {
        "repo_url": "github.com/RupertLuo/Valley",
        "filepath": "valley/inference/run_valley.py",
        "commit_date": "2023-11-09T08:56:14Z",
        "message": "fix some bugs"
    },
    {
        "repo_url": "github.com/RupertLuo/Valley",
        "filepath": "valley/inference/run_valley.py",
        "commit_date": "2023-09-22T05:03:03Z",
        "message": "Fix projector no grad bug. Thanks to @TonyXuQAQ"
    },
    {
        "repo_url": "github.com/RupertLuo/Valley",
        "filepath": "valley/inference/run_valley.py",
        "commit_date": "2023-08-15T02:42:53Z",
        "message": "update valley cn"
    },
    {
        "repo_url": "github.com/RupertLuo/Valley",
        "filepath": "valley/inference/run_valley.py",
        "commit_date": "2023-08-14T08:03:12Z",
        "message": "update chinese valley"
    },
    {
        "repo_url": "github.com/RupertLuo/Valley",
        "filepath": "valley/inference/run_valley.py",
        "commit_date": "2023-08-14T05:47:56Z",
        "message": "update code"
    },
    {
        "repo_url": "github.com/RupertLuo/Valley",
        "filepath": "valley/inference/run_valley.py",
        "commit_date": "2023-06-28T04:17:45Z",
        "message": "upload train code and shell conv code"
    },
    {
        "repo_url": "github.com/RupertLuo/Valley",
        "filepath": "valley/inference/run_valley.py",
        "commit_date": "2023-06-13T06:28:24Z",
        "message": "update model delta weight apply/make code"
    },
    {
        "repo_url": "github.com/RupertLuo/Valley",
        "filepath": "valley/inference/run_valley.py",
        "commit_date": "2023-06-12T13:05:53Z",
        "message": "update readme"
    },
    {
        "repo_url": "github.com/RupertLuo/Valley",
        "filepath": "valley/inference/run_valley.py",
        "commit_date": "2023-06-09T10:51:49Z",
        "message": "update valley inference code"
    },
    {
        "repo_url": "github.com/ParisNeo/lollms_bindings_zoo",
        "filepath": "TGI/special/llava_tools.py",
        "commit_date": "2024-01-17T21:41:08Z",
        "message": "upgraded zoo"
    },
    {
        "repo_url": "github.com/ParisNeo/lollms_bindings_zoo",
        "filepath": "bs_exllamav2/special/llava_tools.py",
        "commit_date": "2024-01-17T23:16:18Z",
        "message": "exllama is back"
    },
    {
        "repo_url": "github.com/ParisNeo/lollms_bindings_zoo",
        "filepath": "hugging_face/special/llava_tools.py",
        "commit_date": "2023-11-21T00:06:52Z",
        "message": "llava advanced"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-02-25T06:21:20Z",
        "message": "support CodeFuse-DeepSeek"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-02-24T12:27:43Z",
        "message": "support of Gemma."
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-02-18T10:05:58Z",
        "message": "all QWen1.5 models are now supported."
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-02-16T04:10:27Z",
        "message": "support of Qwen 1.5"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-02-14T01:46:48Z",
        "message": "add support of Persimmon"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-02-06T06:18:02Z",
        "message": "support of MiniCPM"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-02-04T03:04:41Z",
        "message": "support of InterLM2"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-01-30T11:49:26Z",
        "message": "support BCE-ReRanker"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-01-30T05:52:52Z",
        "message": "add Unigram tokenizer"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-01-29T09:41:50Z",
        "message": "support of BCE-Embedding"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-01-28T13:56:42Z",
        "message": "work on XLMRobertaModel"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-01-24T14:40:19Z",
        "message": "support Orion model"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-01-22T00:16:58Z",
        "message": "support of NeuralBeagle14 7B & refactor."
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-01-17T10:13:46Z",
        "message": "support StableCode 3B"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-01-15T07:04:07Z",
        "message": "support Dolphin (Phi2)"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-01-12T09:43:00Z",
        "message": "support of Phi-2 latest revision\n\neb8bbd1d37d258ea74fb082c53346d33056a83d4"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-01-12T06:24:59Z",
        "message": "support of InternLM 20B; convertor updated for QWen."
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-01-10T09:57:12Z",
        "message": "implement `ntkmixed` for BlueLM"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-01-10T01:51:08Z",
        "message": "support of BlueLM\n\n32k context is not supported yet."
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-01-09T03:33:46Z",
        "message": "support of TigerBot"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-01-08T10:01:47Z",
        "message": "support of QWen"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-01-07T07:31:34Z",
        "message": "fix converter for Baichuan2 7B"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-01-06T08:06:51Z",
        "message": "support of Mixtral."
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-01-06T04:56:07Z",
        "message": "support of OpenChat. incremental `convert.py`."
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-01-05T04:24:33Z",
        "message": "Support WizardMath-7B"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-01-04T09:07:45Z",
        "message": "initial support of Mistral"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-01-04T04:13:30Z",
        "message": "support WizardLM 7B, 13B"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-01-04T03:37:42Z",
        "message": "support of WizardCoder-Python-7B"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2024-01-03T09:33:00Z",
        "message": "support Phi: lots of updates to make it happen."
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2023-12-27T02:59:34Z",
        "message": "support of Yi."
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2023-12-19T04:27:45Z",
        "message": "add support of CodeLlaMA"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2023-12-11T06:35:58Z",
        "message": "InternLM v1.1 is supported too"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2023-12-11T04:35:04Z",
        "message": "support of InternLM (elder version)"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2023-12-10T10:46:55Z",
        "message": "support of CodeGeeX2"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2023-12-10T07:23:16Z",
        "message": "support of ChatGLM3"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2023-12-09T09:05:40Z",
        "message": "support of Baichuan-2"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2023-12-09T06:23:21Z",
        "message": "support of DeepSeek Coder"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2023-12-08T10:55:17Z",
        "message": "redesign tokenizer"
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2023-12-04T10:10:28Z",
        "message": "sync with ggml rev a399be0977\n\n1. use ggml revision: a399be09771d9ac56fad7f81dae40df391286a62\n2. Known issues: GLM/GLM2 is broken because of  `ggml_view_2d`\n   (ggerganov/ggml#4322)."
    },
    {
        "repo_url": "github.com/foldl/chatllm.cpp",
        "filepath": "convert.py",
        "commit_date": "2023-12-02T09:51:54Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/blazerye/DrugAssist",
        "filepath": "merge_model.py",
        "commit_date": "2023-12-27T07:57:45Z",
        "message": "initial commit\n\ninitial commit"
    },
    {
        "repo_url": "github.com/blazerye/DrugAssist",
        "filepath": "gradio_service.py",
        "commit_date": "2023-12-27T07:57:45Z",
        "message": "initial commit\n\ninitial commit"
    },
    {
        "repo_url": "github.com/sshh12/multi_token",
        "filepath": "multi_token/inference.py",
        "commit_date": "2023-10-20T01:45:12Z",
        "message": "tweaks"
    },
    {
        "repo_url": "github.com/sshh12/multi_token",
        "filepath": "multi_token/inference.py",
        "commit_date": "2023-10-20T01:03:23Z",
        "message": "tweaks"
    },
    {
        "repo_url": "github.com/sshh12/multi_token",
        "filepath": "multi_token/inference.py",
        "commit_date": "2023-10-18T03:37:20Z",
        "message": "tweak"
    },
    {
        "repo_url": "github.com/yegcjs/DiffusionLLM",
        "filepath": "src/utils.py",
        "commit_date": "2023-08-24T15:00:43Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety",
        "filepath": "llama2/inference/model_utils.py",
        "commit_date": "2023-10-07T05:07:55Z",
        "message": "public release"
    },
    {
        "repo_url": "github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety",
        "filepath": "llama2/safety_evaluation/question_inference.py",
        "commit_date": "2023-10-07T05:07:55Z",
        "message": "public release"
    },
    {
        "repo_url": "github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety",
        "filepath": "llama2/safety_evaluation/eval_utils/model_utils.py",
        "commit_date": "2023-10-07T05:07:55Z",
        "message": "public release"
    },
    {
        "repo_url": "github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety",
        "filepath": "llama2/utility_evaluation/mt_bench/gen_model_answer.py",
        "commit_date": "2023-10-07T05:07:55Z",
        "message": "public release"
    },
    {
        "repo_url": "github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety",
        "filepath": "llama2/inference/hf-text-generation-inference/merge_lora_weights.py",
        "commit_date": "2023-10-07T05:07:55Z",
        "message": "public release"
    },
    {
        "repo_url": "github.com/cassanof/finetuning-harness",
        "filepath": "scripts/load_and_push_to_hub.py",
        "commit_date": "2023-09-26T23:01:18Z",
        "message": "clean"
    },
    {
        "repo_url": "github.com/hplt-project/monolingual-multilingual-instruction-tuning",
        "filepath": "loraft/generate.py",
        "commit_date": "2023-10-29T14:02:19Z",
        "message": "update scripts"
    },
    {
        "repo_url": "github.com/kuutsav/llm-toys",
        "filepath": "llm_toys/hf/peft/auto.py",
        "commit_date": "2023-07-15T14:30:57Z",
        "message": "updated hf refs"
    },
    {
        "repo_url": "github.com/kuutsav/llm-toys",
        "filepath": "llm_toys/tasks/predict.py",
        "commit_date": "2023-07-16T13:24:22Z",
        "message": "updated dialogue summarization and theme detection"
    },
    {
        "repo_url": "github.com/kuutsav/llm-toys",
        "filepath": "llm_toys/tasks/predict.py",
        "commit_date": "2023-07-15T18:43:41Z",
        "message": "refactoring + README update"
    },
    {
        "repo_url": "github.com/kuutsav/llm-toys",
        "filepath": "llm_toys/tasks/predict.py",
        "commit_date": "2023-07-15T14:52:35Z",
        "message": "updates related to pypi issues"
    },
    {
        "repo_url": "github.com/kuutsav/llm-toys",
        "filepath": "llm_toys/tasks/predict.py",
        "commit_date": "2023-07-15T14:30:57Z",
        "message": "updated hf refs"
    },
    {
        "repo_url": "github.com/kuutsav/llm-toys",
        "filepath": "llm_toys/tasks/predict.py",
        "commit_date": "2023-07-15T07:06:01Z",
        "message": "trying device compatibility"
    },
    {
        "repo_url": "github.com/kuutsav/llm-toys",
        "filepath": "llm_toys/tasks/predict.py",
        "commit_date": "2023-07-15T05:22:28Z",
        "message": "added 3b and 7b models for paraphrasing and changing the tone"
    },
    {
        "repo_url": "github.com/kuutsav/llm-toys",
        "filepath": "llm_toys/hf/peft/peft_model.py",
        "commit_date": "2023-07-15T19:10:59Z",
        "message": "updated readme and pre-commit checks"
    },
    {
        "repo_url": "github.com/kuutsav/llm-toys",
        "filepath": "llm_toys/hf/peft/peft_model.py",
        "commit_date": "2023-07-15T14:30:57Z",
        "message": "updated hf refs"
    },
    {
        "repo_url": "github.com/kuutsav/llm-toys",
        "filepath": "llm_toys/hf/peft/tuners/lora.py",
        "commit_date": "2023-07-15T14:30:57Z",
        "message": "updated hf refs"
    },
    {
        "repo_url": "github.com/kuutsav/llm-toys",
        "filepath": "llm_toys/hf/transformers/trainer.py",
        "commit_date": "2023-07-15T19:10:59Z",
        "message": "updated readme and pre-commit checks"
    },
    {
        "repo_url": "github.com/kuutsav/llm-toys",
        "filepath": "llm_toys/hf/transformers/trainer.py",
        "commit_date": "2023-07-15T14:30:57Z",
        "message": "updated hf refs"
    },
    {
        "repo_url": "github.com/kuutsav/llm-toys",
        "filepath": "llm_toys/hf/peft/tuners/prompt_tuning.py",
        "commit_date": "2023-07-15T14:30:57Z",
        "message": "updated hf refs"
    },
    {
        "repo_url": "github.com/AgainstEntropy/kanji",
        "filepath": "train_lora/train_lcm_distill_lora_sd.py",
        "commit_date": "2024-02-26T04:28:46Z",
        "message": "update README, add StreamDiffusionIO"
    },
    {
        "repo_url": "github.com/LCS2-IIITD/DaSLaM",
        "filepath": "LLAMA13B/Context/ppo_train_13B.py",
        "commit_date": "2023-10-22T05:12:09Z",
        "message": "Adding code for RL finetuning"
    },
    {
        "repo_url": "github.com/teticio/llama-squad",
        "filepath": "model.py",
        "commit_date": "2023-08-10T18:37:54Z",
        "message": "add app"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-03-03T12:50:14Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-03-03T12:43:28Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-03-03T12:31:00Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-03-03T12:26:11Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-03-03T12:23:01Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-03-03T12:16:10Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-03-03T12:03:38Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-03-03T11:56:13Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-03-03T11:52:04Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-03-03T11:50:08Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-03-03T11:40:32Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-03-03T11:15:22Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-03-03T10:23:25Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-03-03T10:14:46Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-03-03T10:13:46Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-03-03T10:07:42Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-03-01T09:30:12Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-03-01T09:16:36Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-03-01T07:56:17Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-03-01T07:27:23Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-03-01T07:23:22Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-03-01T07:19:09Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-03-01T06:55:34Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-02-27T03:12:46Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-02-27T03:04:22Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-02-26T09:37:10Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-02-26T03:56:20Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-02-21T02:22:06Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-02-21T02:18:20Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-02-21T02:07:53Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-02-20T14:59:50Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-02-20T14:50:13Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-02-19T02:15:46Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-01-24T07:40:01Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-01-24T07:34:16Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-01-23T08:43:41Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-01-22T12:32:11Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-01-22T12:31:16Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-01-22T12:27:06Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-01-22T11:12:02Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-01-22T10:58:24Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-01-22T09:25:20Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-01-22T09:24:56Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-01-22T08:43:26Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-01-11T07:27:14Z",
        "message": "Refactor backend parameters in init_model function"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-01-05T12:32:39Z",
        "message": "Fix input_tokens_count in async_vllm_chat"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-01-05T11:59:56Z",
        "message": "bakcend parameters auto setup in vllm"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-01-05T09:07:40Z",
        "message": "Add SingleOutputMeta class to SingleOutput in utils module"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2024-01-01T11:35:31Z",
        "message": "Refactor code to support chat templates in tokenizer"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-29T10:43:42Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-29T10:11:01Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-29T10:04:53Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-29T08:45:05Z",
        "message": "Add support  streaming chat in qianwen saas"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-27T07:06:43Z",
        "message": "Add prob variable to async_vllm_chat function"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-26T14:17:01Z",
        "message": "Add import math and fix formatting in stream_chat function"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-26T14:13:45Z",
        "message": "Add stopping criteria and tokenization improvements in stream_chat of auto"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-26T08:19:07Z",
        "message": "Refactor stream_chat and get_meta in auto"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-26T08:12:08Z",
        "message": "Add trust_remote_code parameter to AutoTokenizer.from_pretrained()"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-26T08:09:13Z",
        "message": "Refactor init_model function in auto to remove unsupported backend and unused code"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-25T15:51:42Z",
        "message": "Add get_meta  to saas model"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-25T08:32:44Z",
        "message": "Fix backend parameter assignment in auto/__init__.py and update test_model_meta.py"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-25T08:30:27Z",
        "message": "Refactor backend parameters in auto/__init__.py"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-25T08:16:34Z",
        "message": "Refactor async_get_meta function to include additional model information"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-25T07:38:31Z",
        "message": "Remove unnecessary parameter from async_get_meta function"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-25T07:22:50Z",
        "message": "Add async_get_meta method and  get_meta method which\ncan show model meta info"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-23T12:40:43Z",
        "message": "Fix print statement formatting in async_vllm_chat function"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-20T04:32:26Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-14T07:52:47Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-14T07:45:38Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-14T07:24:15Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-14T06:59:10Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-14T06:35:52Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-14T06:04:02Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-14T06:02:21Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-14T05:52:35Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-14T05:31:26Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-14T05:23:17Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-14T05:12:45Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-14T04:57:04Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-14T04:15:44Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-14T04:05:57Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-14T03:55:30Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-14T03:51:33Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-14T01:26:43Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-14T00:57:03Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-14T00:41:13Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-13T14:57:38Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-13T14:41:12Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-13T14:25:09Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-13T12:43:48Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-13T12:42:20Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-13T12:39:17Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-13T05:22:11Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-13T04:15:15Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-13T04:08:26Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-06T04:52:00Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-02T11:38:40Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-02T10:07:02Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-02T10:01:29Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/auto/__init__.py",
        "commit_date": "2023-12-02T09:48:10Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/qwen/__init__.py",
        "commit_date": "2024-03-01T06:55:34Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/qwen/__init__.py",
        "commit_date": "2023-12-26T07:58:43Z",
        "message": "Add get_meta method to models"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/qwen/__init__.py",
        "commit_date": "2023-12-26T00:32:52Z",
        "message": "make sure the client works fine with saas/proprietary model"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/qwen/__init__.py",
        "commit_date": "2023-12-02T05:24:41Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/qwen/__init__.py",
        "commit_date": "2023-12-02T05:22:52Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/qwen/__init__.py",
        "commit_date": "2023-12-02T05:22:32Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2024-03-01T06:55:34Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-12-26T07:58:43Z",
        "message": "Add get_meta method to models"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-12-26T00:32:52Z",
        "message": "make sure the client works fine with saas/proprietary model"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-09-12T05:40:39Z",
        "message": "default using int4 quantization"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-09-12T03:30:07Z",
        "message": "refactor quantization config"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-09-11T10:21:24Z",
        "message": "llama, llama2, and falcon support 4bit and 8bit configurations when quantization is turned on"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-08-11T10:34:21Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-25T05:18:06Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-25T05:17:30Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-25T05:14:29Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-25T04:58:05Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-25T03:58:50Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-25T02:49:37Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-25T02:47:19Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-25T02:31:27Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-25T01:37:41Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-25T01:32:17Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-25T01:21:05Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-24T11:16:49Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-24T11:04:17Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-24T10:50:57Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-24T10:38:51Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-24T10:32:23Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-24T10:21:51Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-24T09:50:00Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-24T04:04:14Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-21T07:32:44Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-21T04:16:25Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-21T03:56:41Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-21T03:50:52Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-21T03:43:34Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-21T03:42:09Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-21T03:40:00Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-21T03:15:06Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-20T09:05:51Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-20T08:50:51Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-20T08:24:07Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-20T08:18:35Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-20T08:16:22Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-20T06:54:50Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-16T03:17:55Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-16T01:34:39Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-10T09:16:23Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama/__init__.py",
        "commit_date": "2023-07-10T03:00:50Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama2/__init__.py",
        "commit_date": "2024-03-01T06:55:34Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama2/__init__.py",
        "commit_date": "2023-12-26T07:58:43Z",
        "message": "Add get_meta method to models"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama2/__init__.py",
        "commit_date": "2023-12-26T00:32:52Z",
        "message": "make sure the client works fine with saas/proprietary model"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama2/__init__.py",
        "commit_date": "2023-12-20T07:52:09Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama2/__init__.py",
        "commit_date": "2023-12-20T07:46:49Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama2/__init__.py",
        "commit_date": "2023-10-18T10:31:17Z",
        "message": "remove to_bettertransformer in llama2 temporal"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama2/__init__.py",
        "commit_date": "2023-09-12T05:40:39Z",
        "message": "default using int4 quantization"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama2/__init__.py",
        "commit_date": "2023-09-12T03:30:07Z",
        "message": "refactor quantization config"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama2/__init__.py",
        "commit_date": "2023-09-11T10:21:24Z",
        "message": "llama, llama2, and falcon support 4bit and 8bit configurations when quantization is turned on"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama2/__init__.py",
        "commit_date": "2023-08-31T04:04:14Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama2/__init__.py",
        "commit_date": "2023-08-05T03:51:03Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/llama2/__init__.py",
        "commit_date": "2023-08-03T02:08:50Z",
        "message": "support llama2 model"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-12-26T07:58:43Z",
        "message": "Add get_meta method to models"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-09-12T05:40:39Z",
        "message": "default using int4 quantization"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-09-12T03:30:07Z",
        "message": "refactor quantization config"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-09-11T10:21:24Z",
        "message": "llama, llama2, and falcon support 4bit and 8bit configurations when quantization is turned on"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-08-31T04:04:14Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-08-26T01:09:21Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-07-21T07:32:44Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-07-20T00:59:15Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-07-19T06:12:51Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-07-17T06:23:37Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-07-17T04:53:27Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-07-17T04:36:56Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-07-16T03:24:31Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-07-16T02:41:56Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-07-16T02:35:00Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-07-16T01:49:57Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-07-16T01:39:13Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-07-16T01:34:39Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-07-16T01:10:10Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-07-15T12:39:52Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-07-15T12:37:13Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-07-15T12:32:13Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-07-15T12:21:44Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-07-15T11:56:41Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-07-14T14:28:22Z",
        "message": "upate"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-07-11T08:52:19Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-07-10T09:16:23Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/falcon/__init__.py",
        "commit_date": "2023-07-10T03:00:50Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/zephyr/__init__.py",
        "commit_date": "2024-03-01T06:55:34Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/zephyr/__init__.py",
        "commit_date": "2023-11-14T12:17:08Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/baichuan/__init__.py",
        "commit_date": "2023-12-26T07:58:43Z",
        "message": "Add get_meta method to models"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/baichuan/__init__.py",
        "commit_date": "2023-12-25T15:51:42Z",
        "message": "Add get_meta  to saas model"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/baichuan/__init__.py",
        "commit_date": "2023-09-12T08:55:36Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/baichuan/__init__.py",
        "commit_date": "2023-07-16T01:34:39Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/baichuan/__init__.py",
        "commit_date": "2023-07-10T03:00:50Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/chatglm2/__init__.py",
        "commit_date": "2024-01-02T12:17:46Z",
        "message": "Fix model meta setup and chat template usage"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/chatglm2/__init__.py",
        "commit_date": "2023-12-26T07:58:43Z",
        "message": "Add get_meta method to models"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/chatglm2/__init__.py",
        "commit_date": "2023-08-31T04:12:25Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/chatglm2/__init__.py",
        "commit_date": "2023-07-16T01:34:39Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/chatglm2/__init__.py",
        "commit_date": "2023-07-10T03:00:50Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/visualglm/__init__.py",
        "commit_date": "2023-07-16T01:34:39Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/visualglm/__init__.py",
        "commit_date": "2023-07-10T03:00:50Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/utils/sft/merge_lora.py",
        "commit_date": "2023-09-13T09:14:59Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/utils/sft/merge_lora.py",
        "commit_date": "2023-09-13T09:07:24Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/utils/sft/merge_lora.py",
        "commit_date": "2023-09-13T08:52:41Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/utils/sft/merge_lora.py",
        "commit_date": "2023-09-13T08:42:02Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/utils/sft/merge_lora.py",
        "commit_date": "2023-09-13T08:38:44Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/utils/sft/merge_lora.py",
        "commit_date": "2023-09-13T08:33:44Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/utils/sft/merge_lora.py",
        "commit_date": "2023-09-13T08:28:53Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/utils/sft/merge_lora.py",
        "commit_date": "2023-09-13T08:12:47Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/utils/sft/merge_lora.py",
        "commit_date": "2023-09-12T08:34:55Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/utils/sft/merge_lora.py",
        "commit_date": "2023-09-11T11:29:56Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/utils/sft/merge_lora.py",
        "commit_date": "2023-09-11T11:24:31Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/utils/sft/merge_lora.py",
        "commit_date": "2023-09-11T11:10:35Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/qwen_vl_chat/__init__.py",
        "commit_date": "2024-01-05T13:16:43Z",
        "message": "Update output format in stream_chat function"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/qwen_vl_chat/__init__.py",
        "commit_date": "2024-01-05T01:42:41Z",
        "message": "Refactor input history parsing in stream_chat function"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/qwen_vl_chat/__init__.py",
        "commit_date": "2024-01-05T01:37:55Z",
        "message": "optimize vl-chat"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/qwen_vl_chat/__init__.py",
        "commit_date": "2024-01-04T11:12:35Z",
        "message": "Add tiktoken to demo-requirements.txt and requirements.txt, update CUDA version to 12.2.2, and remove unused imports in __init__.py"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/qwen_vl_chat/__init__.py",
        "commit_date": "2023-12-26T07:58:43Z",
        "message": "Add get_meta method to models"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/qwen_vl_chat/__init__.py",
        "commit_date": "2023-08-26T08:55:00Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/qwen_vl_chat/__init__.py",
        "commit_date": "2023-08-26T08:55:00Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/qwen_vl_chat/__init__.py",
        "commit_date": "2023-08-26T08:55:00Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/qwen_vl_chat/__init__.py",
        "commit_date": "2023-08-26T08:55:00Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/qwen_vl_chat/__init__.py",
        "commit_date": "2023-08-26T08:55:00Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/qwen_vl_chat/__init__.py",
        "commit_date": "2023-08-26T08:55:00Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/allwefantasy/byzer-llm",
        "filepath": "src/byzerllm/qwen_vl_chat/__init__.py",
        "commit_date": "2023-08-26T08:55:00Z",
        "message": "add qwen_vl_chat"
    },
    {
        "repo_url": "github.com/huggingface/amused",
        "filepath": "training/generate_images.py",
        "commit_date": "2023-12-08T01:13:00Z",
        "message": "some things\n\nmore\n\nmore\n\ncommit running scripts\n\nmore\n\nremove commit files"
    },
    {
        "repo_url": "github.com/huggingface/amused",
        "filepath": "training/generate_images.py",
        "commit_date": "2023-12-01T08:59:43Z",
        "message": "fix"
    },
    {
        "repo_url": "github.com/huggingface/amused",
        "filepath": "training/generate_images.py",
        "commit_date": "2023-12-01T07:43:00Z",
        "message": "fix"
    },
    {
        "repo_url": "github.com/huggingface/amused",
        "filepath": "training/generate_images.py",
        "commit_date": "2023-12-01T07:10:48Z",
        "message": "misc"
    },
    {
        "repo_url": "github.com/openfeedback/superhf",
        "filepath": "experiments/evaluations/model_loading.py",
        "commit_date": "2023-09-29T04:37:59Z",
        "message": "Add ability to mock RM on local mps"
    },
    {
        "repo_url": "github.com/openfeedback/superhf",
        "filepath": "experiments/evaluations/model_loading.py",
        "commit_date": "2023-09-29T04:00:20Z",
        "message": "Fix model loading for mac apple silicon"
    },
    {
        "repo_url": "github.com/openfeedback/superhf",
        "filepath": "experiments/evaluations/model_loading.py",
        "commit_date": "2023-05-10T21:34:54Z",
        "message": "Add tokenizer saving and left padding lm"
    },
    {
        "repo_url": "github.com/openfeedback/superhf",
        "filepath": "experiments/evaluations/model_loading.py",
        "commit_date": "2023-05-10T21:11:52Z",
        "message": "Load both lms and rms in bfloat16 not just rm"
    },
    {
        "repo_url": "github.com/openfeedback/superhf",
        "filepath": "experiments/evaluations/model_loading.py",
        "commit_date": "2023-05-10T00:32:57Z",
        "message": "First attempt at running evals"
    },
    {
        "repo_url": "github.com/openfeedback/superhf",
        "filepath": "experiments/evaluations/model_loading.py",
        "commit_date": "2023-05-09T06:32:17Z",
        "message": "Default revision None"
    },
    {
        "repo_url": "github.com/openfeedback/superhf",
        "filepath": "experiments/evaluations/model_loading.py",
        "commit_date": "2023-05-09T06:24:24Z",
        "message": "Support loading a particular peft adapter revision"
    },
    {
        "repo_url": "github.com/openfeedback/superhf",
        "filepath": "experiments/evaluations/model_loading.py",
        "commit_date": "2023-05-09T06:21:33Z",
        "message": "Reimplement peft model loading into our model loader"
    },
    {
        "repo_url": "github.com/openfeedback/superhf",
        "filepath": "experiments/evaluations/model_loading.py",
        "commit_date": "2023-05-07T17:55:30Z",
        "message": "Free memory"
    },
    {
        "repo_url": "github.com/openfeedback/superhf",
        "filepath": "experiments/evaluations/model_loading.py",
        "commit_date": "2023-05-06T23:46:10Z",
        "message": "Add pad token to eval tokenizer if not set"
    },
    {
        "repo_url": "github.com/openfeedback/superhf",
        "filepath": "experiments/evaluations/model_loading.py",
        "commit_date": "2023-05-06T06:51:45Z",
        "message": "Fix for misnamed Llama tokenizer config class"
    },
    {
        "repo_url": "github.com/openfeedback/superhf",
        "filepath": "experiments/evaluations/model_loading.py",
        "commit_date": "2023-05-06T06:44:25Z",
        "message": "Finish the model loader main stuff"
    },
    {
        "repo_url": "github.com/openfeedback/superhf",
        "filepath": "experiments/evaluations/model_loading.py",
        "commit_date": "2023-05-06T05:48:47Z",
        "message": "Initial model_loading"
    },
    {
        "repo_url": "github.com/laiviet/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-06-13T06:25:11Z",
        "message": "Update dataset definition and task definition"
    },
    {
        "repo_url": "github.com/laiviet/lm-evaluation-harness",
        "filepath": "lm_eval/models/huggingface.py",
        "commit_date": "2023-05-28T17:36:39Z",
        "message": "Init"
    },
    {
        "repo_url": "github.com/crowsonkb/LDLM",
        "filepath": "train_vae.py",
        "commit_date": "2023-08-04T19:18:58Z",
        "message": "Cast input_ids to long in train_vae.py"
    },
    {
        "repo_url": "github.com/crowsonkb/LDLM",
        "filepath": "train_vae.py",
        "commit_date": "2023-08-02T19:04:32Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/crowsonkb/LDLM",
        "filepath": "train_ldlm.py",
        "commit_date": "2023-08-02T19:04:32Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/crowsonkb/LDLM",
        "filepath": "infer_ldlm.py",
        "commit_date": "2023-08-04T06:02:56Z",
        "message": "Fix indexing bug in inference code"
    },
    {
        "repo_url": "github.com/crowsonkb/LDLM",
        "filepath": "infer_ldlm.py",
        "commit_date": "2023-08-04T05:56:50Z",
        "message": "Use 25 steps of DPM++ 2M to sample"
    },
    {
        "repo_url": "github.com/crowsonkb/LDLM",
        "filepath": "infer_ldlm.py",
        "commit_date": "2023-08-03T17:09:49Z",
        "message": "Fix inference script bugs"
    },
    {
        "repo_url": "github.com/crowsonkb/LDLM",
        "filepath": "infer_ldlm.py",
        "commit_date": "2023-08-02T19:04:32Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/zejunwang1/chatglm_tuning",
        "filepath": "cli_demo.py",
        "commit_date": "2023-05-17T01:12:14Z",
        "message": "upgrade"
    },
    {
        "repo_url": "github.com/zejunwang1/chatglm_tuning",
        "filepath": "cli_demo.py",
        "commit_date": "2023-05-16T01:08:32Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/zxy556677/EasyGen",
        "filepath": "fastchat/model/apply_lora.py",
        "commit_date": "2023-10-08T10:16:30Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/zxy556677/EasyGen",
        "filepath": "fastchat/model/model_adapter.py",
        "commit_date": "2023-10-08T10:16:30Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/zxy556677/EasyGen",
        "filepath": "fastchat/serve/inference_llama.py",
        "commit_date": "2023-10-18T08:21:38Z",
        "message": "Update inference_llama.py"
    },
    {
        "repo_url": "github.com/zxy556677/EasyGen",
        "filepath": "fastchat/serve/inference_llama.py",
        "commit_date": "2023-10-17T13:10:22Z",
        "message": "Update inference_llama.py"
    },
    {
        "repo_url": "github.com/zxy556677/EasyGen",
        "filepath": "fastchat/serve/inference_llama.py",
        "commit_date": "2023-10-08T10:15:44Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/zxy556677/EasyGen",
        "filepath": "fastchat/serve/inference_easygen.py",
        "commit_date": "2023-10-18T08:10:57Z",
        "message": "Update inference_easygen.py"
    },
    {
        "repo_url": "github.com/zxy556677/EasyGen",
        "filepath": "fastchat/serve/inference_easygen.py",
        "commit_date": "2023-10-18T08:06:29Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/amitpuri/Ask-picturize-it",
        "filepath": "notebooks/huggingface/sagemaker/24_train_bloom_peft_lora/scripts/run_clm.py",
        "commit_date": "2023-07-05T18:08:28Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/swj0419/in-context-pretraining",
        "filepath": "lib/huggingface_pytorch-transformers_main/src/transformers/trainer.py",
        "commit_date": "2024-01-13T23:12:12Z",
        "message": "lib"
    },
    {
        "repo_url": "github.com/swj0419/in-context-pretraining",
        "filepath": "lib/huggingface_pytorch-transformers_main/src/transformers/modeling_utils.py",
        "commit_date": "2024-01-13T23:12:12Z",
        "message": "lib"
    },
    {
        "repo_url": "github.com/swj0419/in-context-pretraining",
        "filepath": "lib/huggingface_pytorch-transformers_main/src/transformers/integrations/peft.py",
        "commit_date": "2024-01-13T23:12:12Z",
        "message": "lib"
    },
    {
        "repo_url": "github.com/aldraus/quilt-llava",
        "filepath": "llava/model/builder.py",
        "commit_date": "2024-02-05T06:50:25Z",
        "message": "llava model"
    },
    {
        "repo_url": "github.com/StarRing2022/RingRWKV",
        "filepath": "example/generate_hf.py",
        "commit_date": "2023-07-17T06:23:22Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/yongzhuo/LLM-SFT",
        "filepath": "llm_sft/ft_lora/post_api.py",
        "commit_date": "2023-06-12T14:36:24Z",
        "message": "init, add SFT of chatglm, llama, bloomz, qlora"
    },
    {
        "repo_url": "github.com/yongzhuo/LLM-SFT",
        "filepath": "llm_sft/ft_qlora/post_api.py",
        "commit_date": "2023-06-12T14:36:24Z",
        "message": "init, add SFT of chatglm, llama, bloomz, qlora"
    },
    {
        "repo_url": "github.com/yongzhuo/LLM-SFT",
        "filepath": "llm_sft/ft_lora/evaluation.py",
        "commit_date": "2023-06-12T14:36:24Z",
        "message": "init, add SFT of chatglm, llama, bloomz, qlora"
    },
    {
        "repo_url": "github.com/yongzhuo/LLM-SFT",
        "filepath": "llm_sft/ft_qlora/evaluation.py",
        "commit_date": "2023-06-12T14:36:24Z",
        "message": "init, add SFT of chatglm, llama, bloomz, qlora"
    },
    {
        "repo_url": "github.com/Abbey4799/CELLO",
        "filepath": "code/evaluators/vicuna_else.py",
        "commit_date": "2023-09-17T04:28:57Z",
        "message": "upload CELLO"
    },
    {
        "repo_url": "github.com/Abbey4799/CELLO",
        "filepath": "code/evaluators/wizardlm13b.py",
        "commit_date": "2023-09-17T04:28:57Z",
        "message": "upload CELLO"
    },
    {
        "repo_url": "github.com/Abbey4799/CELLO",
        "filepath": "code/evaluators/cutegpt_lora.py",
        "commit_date": "2023-09-17T04:28:57Z",
        "message": "upload CELLO"
    },
    {
        "repo_url": "github.com/Abbey4799/CELLO",
        "filepath": "code/evaluators/baichuan13b_chat.py",
        "commit_date": "2023-09-17T04:28:57Z",
        "message": "upload CELLO"
    },
    {
        "repo_url": "github.com/l294265421/multi-turn-alpaca",
        "filepath": "data/original_data/AlpacaDataCleaned/eval/eval.py",
        "commit_date": "2023-04-10T07:54:03Z",
        "message": "training multi-turn alpaca"
    },
    {
        "repo_url": "github.com/l294265421/multi-turn-alpaca",
        "filepath": "multi_turn_alpaca/training_model/alpaca_chatbot.py",
        "commit_date": "2023-04-21T06:39:58Z",
        "message": "update chatbot"
    },
    {
        "repo_url": "github.com/l294265421/multi-turn-alpaca",
        "filepath": "multi_turn_alpaca/training_model/alpaca_chatbot.py",
        "commit_date": "2023-04-17T06:19:11Z",
        "message": "update readme"
    },
    {
        "repo_url": "github.com/l294265421/multi-turn-alpaca",
        "filepath": "multi_turn_alpaca/training_model/alpaca_chatbot.py",
        "commit_date": "2023-04-13T13:01:42Z",
        "message": "run inference background"
    },
    {
        "repo_url": "github.com/l294265421/multi-turn-alpaca",
        "filepath": "multi_turn_alpaca/training_model/alpaca_chatbot.py",
        "commit_date": "2023-04-13T03:55:02Z",
        "message": "add CDial-GPT dataset"
    },
    {
        "repo_url": "github.com/l294265421/multi-turn-alpaca",
        "filepath": "multi_turn_alpaca/training_model/alpaca_chatbot.py",
        "commit_date": "2023-04-12T14:17:19Z",
        "message": "improve chatbot"
    },
    {
        "repo_url": "github.com/l294265421/multi-turn-alpaca",
        "filepath": "multi_turn_alpaca/training_model/alpaca_chatbot.py",
        "commit_date": "2023-04-12T13:49:23Z",
        "message": "add chatbot"
    },
    {
        "repo_url": "github.com/l294265421/multi-turn-alpaca",
        "filepath": "multi_turn_alpaca/training_model/alpaca_chatbot.py",
        "commit_date": "2023-04-12T11:43:21Z",
        "message": "add alpaca chatbot"
    },
    {
        "repo_url": "github.com/ducdauge/sft-llm",
        "filepath": "eval/utils.py",
        "commit_date": "2024-01-29T11:12:35Z",
        "message": "cleaned"
    },
    {
        "repo_url": "github.com/SAI990323/Grounding4Rec",
        "filepath": "inference.py",
        "commit_date": "2023-10-29T08:40:58Z",
        "message": "fix bugs"
    },
    {
        "repo_url": "github.com/SAI990323/Grounding4Rec",
        "filepath": "inference.py",
        "commit_date": "2023-08-19T08:42:08Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/kotoba-tech/kotomamba",
        "filepath": "src/llama_recipes/inference/model_utils.py",
        "commit_date": "2023-12-23T01:50:33Z",
        "message": "feat: integrate kotoba-recipes"
    },
    {
        "repo_url": "github.com/FSoft-AI4Code/RepoPilot",
        "filepath": "scripts/merge_model.py",
        "commit_date": "2023-12-18T03:15:52Z",
        "message": "add local vllm and instruction tuning script"
    },
    {
        "repo_url": "github.com/taishan1994/chinese_llm_pretrained",
        "filepath": "test_pretrained_model.py",
        "commit_date": "2023-06-25T07:42:54Z",
        "message": "commit"
    },
    {
        "repo_url": "github.com/BangLab-UdeM-Mila/NLP4MatSci-HoneyBee",
        "filepath": "generate.py",
        "commit_date": "2024-01-13T14:15:58Z",
        "message": "update generate.py"
    },
    {
        "repo_url": "github.com/alex000kim/ML-Pipeline-With-DVC-SkyPilot-HuggingFace",
        "filepath": "src/merge_model.py",
        "commit_date": "2023-09-01T18:03:28Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/vilm-ai/vietcuna",
        "filepath": "launch.py",
        "commit_date": "2023-07-08T11:03:09Z",
        "message": "update vietcuna-7b-alpha settings"
    },
    {
        "repo_url": "github.com/vilm-ai/vietcuna",
        "filepath": "launch.py",
        "commit_date": "2023-07-08T11:01:45Z",
        "message": "update vietcuna-7b-alpha"
    },
    {
        "repo_url": "github.com/vilm-ai/vietcuna",
        "filepath": "launch.py",
        "commit_date": "2023-06-21T17:41:32Z",
        "message": "fix 4bit"
    },
    {
        "repo_url": "github.com/vilm-ai/vietcuna",
        "filepath": "launch.py",
        "commit_date": "2023-06-19T15:07:50Z",
        "message": "Update launch.py"
    },
    {
        "repo_url": "github.com/vilm-ai/vietcuna",
        "filepath": "launch.py",
        "commit_date": "2023-06-17T17:34:39Z",
        "message": "update vietcuna3b"
    },
    {
        "repo_url": "github.com/vilm-ai/vietcuna",
        "filepath": "launch.py",
        "commit_date": "2023-06-17T17:33:42Z",
        "message": "update vicuna-3b"
    },
    {
        "repo_url": "github.com/vilm-ai/vietcuna",
        "filepath": "launch.py",
        "commit_date": "2023-06-16T14:34:19Z",
        "message": "Update launch.py"
    },
    {
        "repo_url": "github.com/vilm-ai/vietcuna",
        "filepath": "launch.py",
        "commit_date": "2023-06-16T14:24:34Z",
        "message": "Create launch.py"
    },
    {
        "repo_url": "github.com/EvilFreelancer/ruGPT-3.5-training",
        "filepath": "convert_to_native.py",
        "commit_date": "2023-10-08T11:03:35Z",
        "message": "Layers converter added"
    },
    {
        "repo_url": "github.com/EvilFreelancer/ruGPT-3.5-training",
        "filepath": "test_lora.py",
        "commit_date": "2023-10-08T10:59:10Z",
        "message": "Testing scripts added"
    },
    {
        "repo_url": "github.com/EvilFreelancer/ruGPT-3.5-training",
        "filepath": "test_gigasaiga.py",
        "commit_date": "2023-10-08T10:59:10Z",
        "message": "Testing scripts added"
    },
    {
        "repo_url": "github.com/eltociear/qa-lora",
        "filepath": "peft_utils.py",
        "commit_date": "2023-09-27T03:26:25Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/bigai-nlco/LSTP-Chat",
        "filepath": "eval/utils/builder_utils.py",
        "commit_date": "2024-02-26T04:21:58Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/ChenDelong1999/polite-flamingo",
        "filepath": "polite_flamingo/src/factory.py",
        "commit_date": "2023-07-06T10:45:41Z",
        "message": "update_readme"
    },
    {
        "repo_url": "github.com/HiBugEnterprise/HiBug-6B",
        "filepath": "models/loader/loader.py",
        "commit_date": "2023-08-10T10:50:21Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/pleisto/yuren-baichuan-7b",
        "filepath": "tools/merge_lora/src/merge_lora/__main__.py",
        "commit_date": "2023-06-27T08:48:36Z",
        "message": "fix: lint and github action"
    },
    {
        "repo_url": "github.com/pleisto/yuren-baichuan-7b",
        "filepath": "tools/merge_lora/src/merge_lora/__main__.py",
        "commit_date": "2023-06-27T16:27:46Z",
        "message": "add webui"
    },
    {
        "repo_url": "github.com/pleisto/yuren-baichuan-7b",
        "filepath": "tools/merge_lora/src/merge_lora/__main__.py",
        "commit_date": "2023-06-26T00:37:16Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/pleisto/yuren-baichuan-7b",
        "filepath": "apps/yuren_trainer/src/yuren_trainer/text_sft.py",
        "commit_date": "2023-06-27T08:48:36Z",
        "message": "fix: lint and github action"
    },
    {
        "repo_url": "github.com/pleisto/yuren-baichuan-7b",
        "filepath": "apps/yuren_trainer/src/yuren_trainer/text_sft.py",
        "commit_date": "2023-06-27T16:27:46Z",
        "message": "add webui"
    },
    {
        "repo_url": "github.com/pleisto/yuren-baichuan-7b",
        "filepath": "apps/yuren_trainer/src/yuren_trainer/text_sft.py",
        "commit_date": "2023-06-26T00:37:16Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/pleisto/yuren-baichuan-7b",
        "filepath": "apps/yuren_trainer/src/yuren_trainer/peft_trainer.py",
        "commit_date": "2023-06-27T08:48:36Z",
        "message": "fix: lint and github action"
    },
    {
        "repo_url": "github.com/pleisto/yuren-baichuan-7b",
        "filepath": "apps/yuren_trainer/src/yuren_trainer/peft_trainer.py",
        "commit_date": "2023-06-26T00:37:16Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/ImKeTT/Alpaca-Light",
        "filepath": "generate.py",
        "commit_date": "2023-03-28T10:02:34Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/IBM/SALMON",
        "filepath": "inference/demo.py",
        "commit_date": "2023-10-09T13:45:13Z",
        "message": "clean the code of step 1"
    },
    {
        "repo_url": "github.com/IBM/SALMON",
        "filepath": "inference/demo.py",
        "commit_date": "2023-10-09T11:51:29Z",
        "message": "add demo code"
    },
    {
        "repo_url": "github.com/IBM/SALMON",
        "filepath": "training/models/reward_model.py",
        "commit_date": "2023-10-09T15:34:31Z",
        "message": "add training code"
    },
    {
        "repo_url": "github.com/IBM/SALMON",
        "filepath": "training/step1_synthetic_preference_collection/synthetic_preference.py",
        "commit_date": "2023-10-09T13:45:13Z",
        "message": "clean the code of step 1"
    },
    {
        "repo_url": "github.com/ruleGreen/SAFARI",
        "filepath": "lora/peft/peft_model.py",
        "commit_date": "2023-12-02T03:56:04Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/ruleGreen/SAFARI",
        "filepath": "lora/peft/tuners/prompt_tuning.py",
        "commit_date": "2023-12-02T03:56:04Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/Montinger/Transformer-Workbench",
        "filepath": "LoRA-from-scratch/Train-QLoRA-with-PEFT.py",
        "commit_date": "2023-12-10T16:02:53Z",
        "message": "Added LoRA-from scratch code and results"
    },
    {
        "repo_url": "github.com/Montinger/Transformer-Workbench",
        "filepath": "LoRA-from-scratch/Load-LoRA-Weights-PEFT.py",
        "commit_date": "2023-12-10T16:02:53Z",
        "message": "Added LoRA-from scratch code and results"
    },
    {
        "repo_url": "github.com/WangRongsheng/Knowledge-Base-LLMs-QA",
        "filepath": "models/moss_llm.py",
        "commit_date": "2023-05-21T17:07:40Z",
        "message": "init Knowledge-Base-LLMs-QA"
    },
    {
        "repo_url": "github.com/WangRongsheng/Knowledge-Base-LLMs-QA",
        "filepath": "models/chatglm_llm.py",
        "commit_date": "2023-05-21T17:07:40Z",
        "message": "init Knowledge-Base-LLMs-QA"
    },
    {
        "repo_url": "github.com/Neph0s/InCharacter",
        "filepath": "code/ChatHaruhi/ChatGLM2GPT.py",
        "commit_date": "2024-01-09T11:38:51Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/Neph0s/InCharacter",
        "filepath": "code/ChatHaruhi/BaiChuan2GPT.py",
        "commit_date": "2024-01-09T11:38:51Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-25T01:36:08Z",
        "message": "Remove an assert"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-23T20:43:45Z",
        "message": "Seperate method use cases"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-23T19:42:40Z",
        "message": "Simplify code regarding saving, loading adapters dict"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-22T18:53:07Z",
        "message": "Remove unnecessary code"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-21T17:50:26Z",
        "message": "Removal of commented sections"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-20T11:11:28Z",
        "message": "Automatically set adapters for from_pretrained"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-12T12:37:20Z",
        "message": "Merge"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-12T11:35:32Z",
        "message": "Make public"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-12T11:27:30Z",
        "message": "Make public"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-12T01:43:18Z",
        "message": "Add 'set_global_scaling_weight'"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-10T20:44:03Z",
        "message": "Merge branch 'master' into save_all_adapters"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-10T20:40:31Z",
        "message": "Formatting"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-10T20:40:15Z",
        "message": "Add method to override and specify adapters"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-10T20:18:52Z",
        "message": "Add hf_hub_subdir to loading fns"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-10T19:51:49Z",
        "message": "Fix removal of a fn"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-10T19:05:34Z",
        "message": "Make function public"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-09T21:56:17Z",
        "message": "Add sketch impl"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-09T19:39:57Z",
        "message": "Add scaling pass value to config, default is now 0"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-09T19:38:01Z",
        "message": "Add loading from hf hub"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-07T18:19:57Z",
        "message": "Update type ignore"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-07T18:16:50Z",
        "message": "Update ci"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-07T18:13:50Z",
        "message": "Update for mypy"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-07T15:20:44Z",
        "message": "Ensure use_cache is false"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-04T22:43:11Z",
        "message": "Move a util fn"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-02T19:47:28Z",
        "message": "Ensure non instantiation"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-01T17:10:38Z",
        "message": "Patch the generate override fn"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-01T15:20:18Z",
        "message": "Patch .generate to ensure frozen"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-01T13:16:59Z",
        "message": "Add more verbose print"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-02-01T10:34:08Z",
        "message": "Update the saving, loading safetensors system"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-31T23:37:06Z",
        "message": "Add class hack for type hints"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-31T20:31:39Z",
        "message": "Fix via ruff"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-31T20:27:38Z",
        "message": "Merge branch 'master' into seqlen_scalings"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-31T20:16:00Z",
        "message": "Update verbose impl"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-31T20:14:47Z",
        "message": "Update verbose impl"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-31T20:14:38Z",
        "message": "Update verbose impl"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-31T20:11:37Z",
        "message": "Extend output"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-31T20:11:15Z",
        "message": "Implement load scalings log"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-31T20:09:48Z",
        "message": "Implement load scalings log"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-31T18:59:37Z",
        "message": "Pass seq len in payload"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-30T10:23:25Z",
        "message": "Fix some bugs"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-30T00:02:38Z",
        "message": "Simplify implementation"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-30T00:02:38Z",
        "message": "Simplify implementation"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-29T22:17:40Z",
        "message": "Add manual setting of scalings"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-29T17:54:28Z",
        "message": "Reimplement topk"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-28T18:17:27Z",
        "message": "Update based on master state"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-27T22:43:33Z",
        "message": "Remove requires_grad in temp scalings during class pass"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-27T19:30:14Z",
        "message": "Typing update"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-27T19:13:18Z",
        "message": "Move bound name correctly"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-27T18:59:57Z",
        "message": "Remove generate"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-27T18:50:43Z",
        "message": "Use hooks"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-26T23:16:54Z",
        "message": "Properly insert generate"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-26T22:34:55Z",
        "message": "Fix recursion err"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-26T22:00:14Z",
        "message": "*MAJOR CHANGE* Remove all global state"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-25T12:30:53Z",
        "message": "Update impl of disable"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-22T20:32:55Z",
        "message": "Remove top_k_lora"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-22T15:41:52Z",
        "message": "Remove need for looping over batch *major change*"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-20T20:03:32Z",
        "message": "Update the API"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-20T19:47:10Z",
        "message": "Specify use trainable in config"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-20T19:28:32Z",
        "message": "Fix small bug with adapters name"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-20T19:23:10Z",
        "message": "Update the kwargs default"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-20T19:15:00Z",
        "message": "Break the recursion"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-20T18:49:41Z",
        "message": "Save the adapters if enabled"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-20T18:19:38Z",
        "message": "Allow config to enable trainable adapters"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-20T11:23:06Z",
        "message": "Calculate seq lens with new option"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-20T11:14:36Z",
        "message": "Remove the old comments"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-20T11:11:57Z",
        "message": "Just formatting"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-19T21:36:30Z",
        "message": "Use new attention mask seq length"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-19T21:35:14Z",
        "message": "Fixed sequence length, and a few other things"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-19T11:13:55Z",
        "message": "FIxed some issues"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-18T23:31:49Z",
        "message": "Ruff fixes, bugfix for topklora"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-18T23:11:30Z",
        "message": "Fix bug with device and config"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-18T23:11:29Z",
        "message": "Implement, debug from_pretrained, also update config saving"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-18T23:11:15Z",
        "message": "Use the new dummy scalings"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-18T20:59:51Z",
        "message": "Clean up the code"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-18T20:58:37Z",
        "message": "Fix circular imports"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-18T20:47:56Z",
        "message": "Enable, disable the xlora layers to not use dummy scalings"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-18T20:33:25Z",
        "message": "Clean up the code"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-18T18:44:32Z",
        "message": "Fixed issue of lost gradients with multiple batches"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-18T15:46:41Z",
        "message": "Fixed method to freeze LoRA layers"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-18T01:38:24Z",
        "message": "Fix docstring"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-18T01:29:34Z",
        "message": "Improve tqdm loading behavior"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-17T22:03:51Z",
        "message": "Update the verbose print"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-17T22:02:33Z",
        "message": "Add, update the xlora conversion API"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-17T21:58:58Z",
        "message": "Fix docstring"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-17T21:57:28Z",
        "message": "Add scalings logging back"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-17T20:55:16Z",
        "message": "Reversion and bugfix"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-16T17:54:12Z",
        "message": "Fix layerwise ID number"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-16T01:05:18Z",
        "message": "Update docs"
    },
    {
        "repo_url": "github.com/EricLBuehler/xlora",
        "filepath": "src/xlora/xlora.py",
        "commit_date": "2024-01-15T19:21:58Z",
        "message": "Execute the rename"
    },
    {
        "repo_url": "github.com/liucongg/MolChat",
        "filepath": "predict_glm.py",
        "commit_date": "2023-05-12T06:01:12Z",
        "message": "update code"
    },
    {
        "repo_url": "github.com/liucongg/MolChat",
        "filepath": "predict_llama.py",
        "commit_date": "2023-05-12T06:01:12Z",
        "message": "update code"
    },
    {
        "repo_url": "github.com/vmware-labs/research-and-development-artificial-intelligence-lab",
        "filepath": "instruction-tuning/peft-seq2seq/merge_weights.py",
        "commit_date": "2023-06-05T20:03:14Z",
        "message": "Add copyright & spdx lines"
    },
    {
        "repo_url": "github.com/vmware-labs/research-and-development-artificial-intelligence-lab",
        "filepath": "instruction-tuning/peft-seq2seq/merge_weights.py",
        "commit_date": "2023-06-01T20:45:47Z",
        "message": "added LLM Peft-Seq2seq instruction tuning code"
    },
    {
        "repo_url": "github.com/brightjade/SimCKP",
        "filepath": "models/modeling_utils.py",
        "commit_date": "2023-10-09T02:51:39Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/tiansztiansz/langchain-chatyuan",
        "filepath": "models/moss_llm.py",
        "commit_date": "2023-05-20T15:04:13Z",
        "message": "\u63d0\u4ea4"
    },
    {
        "repo_url": "github.com/tiansztiansz/langchain-chatyuan",
        "filepath": "models/chatglm_llm.py",
        "commit_date": "2023-05-20T15:04:13Z",
        "message": "\u63d0\u4ea4"
    },
    {
        "repo_url": "github.com/taishan1994/Chinese-LLaMA-Alpaca-LoRA-Tuning",
        "filepath": "model_hub/merge_llama_with_chinese_lora.py",
        "commit_date": "2023-05-24T03:53:39Z",
        "message": "commit"
    },
    {
        "repo_url": "github.com/totallylegitco/healthinsurance-llm",
        "filepath": "test_new_model.py",
        "commit_date": "2023-06-17T16:00:46Z",
        "message": "Progress."
    },
    {
        "repo_url": "github.com/totallylegitco/healthinsurance-llm",
        "filepath": "test_new_model.py",
        "commit_date": "2023-06-17T15:46:46Z",
        "message": "Refactor"
    },
    {
        "repo_url": "github.com/totallylegitco/healthinsurance-llm",
        "filepath": "test_new_model.py",
        "commit_date": "2023-06-03T19:01:12Z",
        "message": "Handle UTF8 chars.\n\nFix\n\nFix"
    },
    {
        "repo_url": "github.com/totallylegitco/healthinsurance-llm",
        "filepath": "test_new_model.py",
        "commit_date": "2023-06-03T19:01:11Z",
        "message": "Fix pipeline usage\n\nFix\n\nFix"
    },
    {
        "repo_url": "github.com/totallylegitco/healthinsurance-llm",
        "filepath": "test_new_model.py",
        "commit_date": "2023-06-03T19:01:08Z",
        "message": "Don't use quantization for testing the new model and also (I know it's bad to use our train data for test but just for introspection) print out what the new model looks like.\n\nFix\n\nFix\n\nFix"
    },
    {
        "repo_url": "github.com/totallylegitco/healthinsurance-llm",
        "filepath": "test_new_model.py",
        "commit_date": "2023-06-03T01:43:03Z",
        "message": "Update the README with the latest of trying to run training on an AGX (no luck for now)"
    },
    {
        "repo_url": "github.com/AUGMXNT/shisa",
        "filepath": "eval/merge-lora.py",
        "commit_date": "2023-11-28T07:39:38Z",
        "message": "more eval stuff"
    },
    {
        "repo_url": "github.com/AUGMXNT/shisa",
        "filepath": "train/qwen/qmerge.py",
        "commit_date": "2023-11-11T16:10:12Z",
        "message": "for merging fine tune"
    },
    {
        "repo_url": "github.com/PKU-YuanGroup/Peer-review-in-LLMs",
        "filepath": "llm_judge/config/apply_lora.py",
        "commit_date": "2024-02-07T02:29:57Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/PKU-YuanGroup/Peer-review-in-LLMs",
        "filepath": "llm_judge/config/model_adapter.py",
        "commit_date": "2024-02-07T02:29:57Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/HKUDS/GraphEdit",
        "filepath": "LLM/scripts/apply_lora.py",
        "commit_date": "2024-02-21T05:52:33Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/HKUDS/GraphEdit",
        "filepath": "LLM/graphedit/model/apply_lora.py",
        "commit_date": "2024-02-22T14:08:17Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/HKUDS/GraphEdit",
        "filepath": "LLM/graphedit/model/apply_lora.py",
        "commit_date": "2024-02-21T05:52:33Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/HKUDS/GraphEdit",
        "filepath": "LLM/graphedit/model/model_adapter.py",
        "commit_date": "2024-02-22T14:08:17Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/HKUDS/GraphEdit",
        "filepath": "LLM/graphedit/model/model_adapter.py",
        "commit_date": "2024-02-21T05:52:33Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/williamliujl/CMExam",
        "filepath": "src/LoRA/infer.py",
        "commit_date": "2023-08-26T14:30:27Z",
        "message": "add infer"
    },
    {
        "repo_url": "github.com/williamliujl/CMExam",
        "filepath": "src/LoRA/infer.py",
        "commit_date": "2023-08-25T04:14:37Z",
        "message": "add infer"
    },
    {
        "repo_url": "github.com/williamliujl/CMExam",
        "filepath": "src/LoRA/generate.py",
        "commit_date": "2023-08-26T14:32:23Z",
        "message": "add generate.py"
    },
    {
        "repo_url": "github.com/boom-R123/ChatWK",
        "filepath": "merge_lora.py",
        "commit_date": "2023-07-22T12:50:56Z",
        "message": "ChatWK version 0.5"
    },
    {
        "repo_url": "github.com/fecet/alpaca-lora-Chinese",
        "filepath": "utils.py",
        "commit_date": "2023-03-27T17:10:29Z",
        "message": "upd"
    },
    {
        "repo_url": "github.com/hppRC/simple-simcse-ja",
        "filepath": "src/experiment.py",
        "commit_date": "2023-10-04T10:38:15Z",
        "message": "Update README.md"
    },
    {
        "repo_url": "github.com/hppRC/simple-simcse-ja",
        "filepath": "src/experiment.py",
        "commit_date": "2023-09-28T08:23:54Z",
        "message": "format"
    },
    {
        "repo_url": "github.com/hppRC/simple-simcse-ja",
        "filepath": "src/experiment.py",
        "commit_date": "2023-09-24T14:47:48Z",
        "message": ":+1: Add results"
    },
    {
        "repo_url": "github.com/hppRC/simple-simcse-ja",
        "filepath": "src/experiment.py",
        "commit_date": "2023-03-11T03:14:00Z",
        "message": "chore"
    },
    {
        "repo_url": "github.com/hppRC/simple-simcse-ja",
        "filepath": "src/experiment.py",
        "commit_date": "2023-03-02T01:03:15Z",
        "message": "chore"
    },
    {
        "repo_url": "github.com/WeihaoTan/TWOSOME",
        "filepath": "twosome/virtualhome/policy_v1.py",
        "commit_date": "2023-10-25T02:12:38Z",
        "message": "reach for to close"
    },
    {
        "repo_url": "github.com/WeihaoTan/TWOSOME",
        "filepath": "twosome/virtualhome/policy_v1.py",
        "commit_date": "2023-09-30T08:58:13Z",
        "message": "update basemodel"
    },
    {
        "repo_url": "github.com/WeihaoTan/TWOSOME",
        "filepath": "twosome/virtualhome/policy_v1.py",
        "commit_date": "2023-09-15T03:56:01Z",
        "message": "fix critic bug"
    },
    {
        "repo_url": "github.com/WeihaoTan/TWOSOME",
        "filepath": "twosome/virtualhome/policy_v1.py",
        "commit_date": "2023-09-14T02:50:17Z",
        "message": "fix bugs"
    },
    {
        "repo_url": "github.com/WeihaoTan/TWOSOME",
        "filepath": "twosome/virtualhome/policy_v1.py",
        "commit_date": "2023-06-23T12:30:46Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/WeihaoTan/TWOSOME",
        "filepath": "twosome/virtualhome/policy_v2.py",
        "commit_date": "2023-10-25T02:12:38Z",
        "message": "reach for to close"
    },
    {
        "repo_url": "github.com/WeihaoTan/TWOSOME",
        "filepath": "twosome/virtualhome/policy_v2.py",
        "commit_date": "2023-09-30T08:58:13Z",
        "message": "update basemodel"
    },
    {
        "repo_url": "github.com/WeihaoTan/TWOSOME",
        "filepath": "twosome/virtualhome/policy_v2.py",
        "commit_date": "2023-09-23T08:05:23Z",
        "message": "fix bugs"
    },
    {
        "repo_url": "github.com/WeihaoTan/TWOSOME",
        "filepath": "twosome/virtualhome/policy_v2.py",
        "commit_date": "2023-09-15T03:56:01Z",
        "message": "fix critic bug"
    },
    {
        "repo_url": "github.com/WeihaoTan/TWOSOME",
        "filepath": "twosome/virtualhome/policy_v2.py",
        "commit_date": "2023-09-14T02:50:17Z",
        "message": "fix bugs"
    },
    {
        "repo_url": "github.com/WeihaoTan/TWOSOME",
        "filepath": "twosome/virtualhome/policy_v2.py",
        "commit_date": "2023-06-23T12:30:46Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/WeihaoTan/TWOSOME",
        "filepath": "twosome/overcooked/policy_pomdp.py",
        "commit_date": "2023-10-01T11:18:06Z",
        "message": "update save config"
    },
    {
        "repo_url": "github.com/WeihaoTan/TWOSOME",
        "filepath": "twosome/overcooked/policy_pomdp.py",
        "commit_date": "2023-09-30T08:58:13Z",
        "message": "update basemodel"
    },
    {
        "repo_url": "github.com/WeihaoTan/TWOSOME",
        "filepath": "twosome/overcooked/policy_pomdp.py",
        "commit_date": "2023-09-15T03:56:01Z",
        "message": "fix critic bug"
    },
    {
        "repo_url": "github.com/WeihaoTan/TWOSOME",
        "filepath": "twosome/overcooked/policy_pomdp.py",
        "commit_date": "2023-06-23T12:30:46Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/centerforaisafety/HarmBench",
        "filepath": "adversarial_training/alignment-handbook/scripts/run_dpo.py",
        "commit_date": "2024-02-27T03:30:43Z",
        "message": "HarmBench 1.0 update"
    },
    {
        "repo_url": "github.com/FartyPants/VirtualLora",
        "filepath": "script.py",
        "commit_date": "2024-02-28T08:15:38Z",
        "message": "Removed add_lora_exllama"
    },
    {
        "repo_url": "github.com/FartyPants/VirtualLora",
        "filepath": "script.py",
        "commit_date": "2023-12-24T04:29:11Z",
        "message": "Deleteing adapters manually"
    },
    {
        "repo_url": "github.com/FartyPants/VirtualLora",
        "filepath": "script.py",
        "commit_date": "2023-12-24T04:08:30Z",
        "message": "Trying to go around weird Unload adapters in PEFT"
    },
    {
        "repo_url": "github.com/FartyPants/VirtualLora",
        "filepath": "script.py",
        "commit_date": "2023-11-07T19:35:27Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/FartyPants/VirtualLora",
        "filepath": "script.py",
        "commit_date": "2023-10-18T20:14:57Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/FartyPants/VirtualLora",
        "filepath": "script.py",
        "commit_date": "2023-10-17T04:07:47Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/FartyPants/VirtualLora",
        "filepath": "script.py",
        "commit_date": "2023-10-17T04:03:31Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/FartyPants/VirtualLora",
        "filepath": "script.py",
        "commit_date": "2023-10-16T02:46:51Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/FartyPants/VirtualLora",
        "filepath": "script.py",
        "commit_date": "2023-10-16T01:40:35Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/GreenBitAI/low_bit_llama",
        "filepath": "model.py",
        "commit_date": "2023-10-05T14:51:04Z",
        "message": "fix repeat memory allocation issue"
    },
    {
        "repo_url": "github.com/GreenBitAI/low_bit_llama",
        "filepath": "model.py",
        "commit_date": "2023-09-29T09:40:50Z",
        "message": "fix quantization version mismatch issue"
    },
    {
        "repo_url": "github.com/GreenBitAI/low_bit_llama",
        "filepath": "model.py",
        "commit_date": "2023-09-28T23:10:18Z",
        "message": "fix quantization version mismatch issue"
    },
    {
        "repo_url": "github.com/GreenBitAI/low_bit_llama",
        "filepath": "model.py",
        "commit_date": "2023-08-16T01:02:01Z",
        "message": "add 2-bit openllama 3B"
    },
    {
        "repo_url": "github.com/GreenBitAI/low_bit_llama",
        "filepath": "model.py",
        "commit_date": "2023-07-17T14:47:59Z",
        "message": "update for instruction-tuned model"
    },
    {
        "repo_url": "github.com/GreenBitAI/low_bit_llama",
        "filepath": "model.py",
        "commit_date": "2023-07-09T17:41:24Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/zhangnn520/chinese_llama_alpaca_lora",
        "filepath": "inference_hf.py",
        "commit_date": "2023-04-20T02:53:25Z",
        "message": "the first upload"
    },
    {
        "repo_url": "github.com/zhangnn520/chinese_llama_alpaca_lora",
        "filepath": "scripts/merge_llama_with_chinese_lora_to_hf.py",
        "commit_date": "2023-04-20T02:53:25Z",
        "message": "the first upload"
    },
    {
        "repo_url": "github.com/18907305772/FuseLLM",
        "filepath": "FuseChat/train/model/apply_lora.py",
        "commit_date": "2024-02-26T12:16:14Z",
        "message": "Update FuseChat project."
    },
    {
        "repo_url": "github.com/18907305772/FuseLLM",
        "filepath": "FuseChat/train/model/model_adapter.py",
        "commit_date": "2024-02-26T12:16:14Z",
        "message": "Update FuseChat project."
    },
    {
        "repo_url": "github.com/UW-Madison-Lee-Lab/CoBSAT",
        "filepath": "load_models/call_qwen.py",
        "commit_date": "2024-01-26T17:43:35Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/UW-Madison-Lee-Lab/CoBSAT",
        "filepath": "load_models/call_qwen.py",
        "commit_date": "2024-01-25T23:05:31Z",
        "message": "sync"
    },
    {
        "repo_url": "github.com/UW-Madison-Lee-Lab/CoBSAT",
        "filepath": "load_models/call_qwen.py",
        "commit_date": "2024-01-25T18:11:00Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/UW-Madison-Lee-Lab/CoBSAT",
        "filepath": "load_models/call_qwen.py",
        "commit_date": "2024-01-25T17:31:20Z",
        "message": "update finetuned model inference + evaluation"
    },
    {
        "repo_url": "github.com/UW-Madison-Lee-Lab/CoBSAT",
        "filepath": "load_models/call_qwen.py",
        "commit_date": "2024-01-25T16:53:52Z",
        "message": "add qwen to evaluation"
    },
    {
        "repo_url": "github.com/UW-Madison-Lee-Lab/CoBSAT",
        "filepath": "load_models/call_qwen.py",
        "commit_date": "2024-01-25T03:15:55Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/UW-Madison-Lee-Lab/CoBSAT",
        "filepath": "load_models/call_qwen.py",
        "commit_date": "2024-01-25T02:56:52Z",
        "message": "update finetuning"
    },
    {
        "repo_url": "github.com/UW-Madison-Lee-Lab/CoBSAT",
        "filepath": "load_models/call_qwen.py",
        "commit_date": "2024-01-24T22:27:20Z",
        "message": "sync"
    },
    {
        "repo_url": "github.com/UW-Madison-Lee-Lab/CoBSAT",
        "filepath": "load_models/call_qwen.py",
        "commit_date": "2024-01-24T22:13:44Z",
        "message": "fix the bug"
    },
    {
        "repo_url": "github.com/UW-Madison-Lee-Lab/CoBSAT",
        "filepath": "load_models/call_qwen.py",
        "commit_date": "2024-01-24T21:31:22Z",
        "message": "update history saving"
    },
    {
        "repo_url": "github.com/UW-Madison-Lee-Lab/CoBSAT",
        "filepath": "load_models/call_qwen.py",
        "commit_date": "2024-01-24T17:27:34Z",
        "message": "fix the bug"
    },
    {
        "repo_url": "github.com/UW-Madison-Lee-Lab/CoBSAT",
        "filepath": "load_models/call_qwen.py",
        "commit_date": "2024-01-24T07:54:02Z",
        "message": "implemented cot"
    },
    {
        "repo_url": "github.com/UW-Madison-Lee-Lab/CoBSAT",
        "filepath": "load_models/call_qwen.py",
        "commit_date": "2024-01-24T05:25:32Z",
        "message": "finished implementing caption experiment"
    },
    {
        "repo_url": "github.com/UW-Madison-Lee-Lab/CoBSAT",
        "filepath": "load_models/call_qwen.py",
        "commit_date": "2024-01-24T04:52:19Z",
        "message": "update new prompt"
    },
    {
        "repo_url": "github.com/UW-Madison-Lee-Lab/CoBSAT",
        "filepath": "load_models/call_qwen.py",
        "commit_date": "2024-01-23T18:08:28Z",
        "message": "organized the instruction dict"
    },
    {
        "repo_url": "github.com/UW-Madison-Lee-Lab/CoBSAT",
        "filepath": "load_models/call_qwen.py",
        "commit_date": "2024-01-05T09:43:12Z",
        "message": "update the default shots and qwen prompt"
    },
    {
        "repo_url": "github.com/UW-Madison-Lee-Lab/CoBSAT",
        "filepath": "load_models/call_qwen.py",
        "commit_date": "2024-01-05T09:30:00Z",
        "message": "half implemented evaluation"
    },
    {
        "repo_url": "github.com/UW-Madison-Lee-Lab/CoBSAT",
        "filepath": "load_models/call_qwen.py",
        "commit_date": "2024-01-05T09:26:06Z",
        "message": "update the prompts"
    },
    {
        "repo_url": "github.com/UW-Madison-Lee-Lab/CoBSAT",
        "filepath": "load_models/call_qwen.py",
        "commit_date": "2024-01-04T08:36:19Z",
        "message": "update prompts"
    },
    {
        "repo_url": "github.com/UW-Madison-Lee-Lab/CoBSAT",
        "filepath": "load_models/call_qwen.py",
        "commit_date": "2024-01-04T08:07:31Z",
        "message": "updated prompt for qwen"
    },
    {
        "repo_url": "github.com/UW-Madison-Lee-Lab/CoBSAT",
        "filepath": "load_models/call_qwen.py",
        "commit_date": "2024-01-04T06:27:20Z",
        "message": "update"
    },
    {
        "repo_url": "github.com/UW-Madison-Lee-Lab/CoBSAT",
        "filepath": "load_models/call_qwen.py",
        "commit_date": "2024-01-03T06:48:57Z",
        "message": "update load_model"
    },
    {
        "repo_url": "github.com/UW-Madison-Lee-Lab/CoBSAT",
        "filepath": "load_models/call_qwen.py",
        "commit_date": "2023-12-26T09:56:05Z",
        "message": "reimplement text_inference"
    },
    {
        "repo_url": "github.com/UW-Madison-Lee-Lab/CoBSAT",
        "filepath": "load_models/call_qwen.py",
        "commit_date": "2023-12-26T08:59:13Z",
        "message": "implement qwen"
    },
    {
        "repo_url": "github.com/UW-Madison-Lee-Lab/CoBSAT",
        "filepath": "load_models/call_qwen.py",
        "commit_date": "2023-12-26T05:47:28Z",
        "message": "update code"
    },
    {
        "repo_url": "github.com/georgepullen/Merge-LoRA-Into-GGUF",
        "filepath": "merge_lora_into_gguf.py",
        "commit_date": "2023-09-06T18:07:37Z",
        "message": "Update merge_lora_into_gguf.py"
    },
    {
        "repo_url": "github.com/georgepullen/Merge-LoRA-Into-GGUF",
        "filepath": "merge_lora_into_gguf.py",
        "commit_date": "2023-09-06T16:31:49Z",
        "message": "Update and rename merge_lora_into_ggml.py to merge_lora_into_gguf.py"
    },
    {
        "repo_url": "github.com/xxm1668/Chatglm_tune_gpus",
        "filepath": "infer_lora.py",
        "commit_date": "2023-05-15T08:58:49Z",
        "message": "\u5fae\u8c03\u63a8\u7406"
    },
    {
        "repo_url": "github.com/xxm1668/Chatglm_tune_gpus",
        "filepath": "infer_lora.py",
        "commit_date": "2023-05-15T08:57:35Z",
        "message": "\u5fae\u8c03\u63a8\u7406"
    },
    {
        "repo_url": "github.com/xxm1668/Chatglm_tune_gpus",
        "filepath": "infer_lora.py",
        "commit_date": "2023-05-15T08:40:59Z",
        "message": "\u5fae\u8c03"
    },
    {
        "repo_url": "github.com/photomz/BabyDoctor",
        "filepath": "llava/model/merger.py",
        "commit_date": "2023-07-30T00:24:55Z",
        "message": "stuff"
    },
    {
        "repo_url": "github.com/official-elinas/zeus-llm-trainer",
        "filepath": "generate.py",
        "commit_date": "2023-05-09T00:48:43Z",
        "message": "Update to latest"
    },
    {
        "repo_url": "github.com/official-elinas/zeus-llm-trainer",
        "filepath": "generate.py",
        "commit_date": "2023-03-16T23:04:06Z",
        "message": "add Gradio interface to generate.py"
    },
    {
        "repo_url": "github.com/official-elinas/zeus-llm-trainer",
        "filepath": "generate.py",
        "commit_date": "2023-03-16T19:11:47Z",
        "message": "Catch outdated installs"
    },
    {
        "repo_url": "github.com/official-elinas/zeus-llm-trainer",
        "filepath": "generate.py",
        "commit_date": "2023-03-16T19:11:29Z",
        "message": "Update alpaca-lora to use transformers main branch"
    },
    {
        "repo_url": "github.com/official-elinas/zeus-llm-trainer",
        "filepath": "generate.py",
        "commit_date": "2023-03-16T16:59:10Z",
        "message": "Expand sampling in generate.py for new test"
    },
    {
        "repo_url": "github.com/official-elinas/zeus-llm-trainer",
        "filepath": "generate.py",
        "commit_date": "2023-03-16T07:05:32Z",
        "message": "Add counting test"
    },
    {
        "repo_url": "github.com/official-elinas/zeus-llm-trainer",
        "filepath": "generate.py",
        "commit_date": "2023-03-16T00:22:22Z",
        "message": "generate.py memory, perf updates"
    },
    {
        "repo_url": "github.com/official-elinas/zeus-llm-trainer",
        "filepath": "generate.py",
        "commit_date": "2023-03-15T18:11:26Z",
        "message": "torch.no_grad"
    },
    {
        "repo_url": "github.com/official-elinas/zeus-llm-trainer",
        "filepath": "generate.py",
        "commit_date": "2023-03-15T04:41:02Z",
        "message": "add text-davinci-003 to comparisons"
    },
    {
        "repo_url": "github.com/official-elinas/zeus-llm-trainer",
        "filepath": "generate.py",
        "commit_date": "2023-03-15T04:33:12Z",
        "message": "Update README.md with new checkpoint details"
    },
    {
        "repo_url": "github.com/official-elinas/zeus-llm-trainer",
        "filepath": "generate.py",
        "commit_date": "2023-03-14T22:10:33Z",
        "message": "Ready to go"
    },
    {
        "repo_url": "github.com/official-elinas/zeus-llm-trainer",
        "filepath": "generate.py",
        "commit_date": "2023-03-14T00:23:29Z",
        "message": "decapoda"
    },
    {
        "repo_url": "github.com/official-elinas/zeus-llm-trainer",
        "filepath": "generate.py",
        "commit_date": "2023-03-13T22:00:05Z",
        "message": "Licenses and whatnot"
    },
    {
        "repo_url": "github.com/official-elinas/zeus-llm-trainer",
        "filepath": "scripts/export_hf_checkpoint.py",
        "commit_date": "2023-05-14T20:32:04Z",
        "message": "move scripts to scripts dir"
    },
    {
        "repo_url": "github.com/official-elinas/zeus-llm-trainer",
        "filepath": "scripts/merge_lora_hf_checkpoint.py",
        "commit_date": "2023-05-14T20:32:04Z",
        "message": "move scripts to scripts dir"
    },
    {
        "repo_url": "github.com/official-elinas/zeus-llm-trainer",
        "filepath": "scripts/export_state_dict_checkpoint.py",
        "commit_date": "2023-05-14T20:32:04Z",
        "message": "move scripts to scripts dir"
    },
    {
        "repo_url": "github.com/tpdmason/TabbyML",
        "filepath": "python/tabby/trainer.py",
        "commit_date": "2023-06-13T19:48:27Z",
        "message": "feat: cleanup trainer with new data format"
    },
    {
        "repo_url": "github.com/pipilurj/MLLM-protector",
        "filepath": "llava/model/builder.py",
        "commit_date": "2024-02-28T17:21:45Z",
        "message": "submit code"
    },
    {
        "repo_url": "github.com/huxiaosheng123/open-llama2",
        "filepath": "scripts/scripts/inference/gradio_demo.py",
        "commit_date": "2023-08-31T16:23:16Z",
        "message": "update code"
    },
    {
        "repo_url": "github.com/huxiaosheng123/open-llama2",
        "filepath": "scripts/scripts/inference/inference_hf_tool.py",
        "commit_date": "2023-08-31T16:23:16Z",
        "message": "update code"
    },
    {
        "repo_url": "github.com/huxiaosheng123/open-llama2",
        "filepath": "scripts/scripts/openai_server_demo/openai_api_server.py",
        "commit_date": "2023-08-31T16:23:16Z",
        "message": "update code"
    },
    {
        "repo_url": "github.com/huxiaosheng123/open-llama2",
        "filepath": "scripts/scripts/merge_llama2_with_chinese_lora_low_mem.py",
        "commit_date": "2023-08-31T16:23:16Z",
        "message": "update code"
    },
    {
        "repo_url": "github.com/EdVince/whisper-trtllm",
        "filepath": "transformers/src/transformers/trainer.py",
        "commit_date": "2023-08-22T07:50:00Z",
        "message": "add:transformers"
    },
    {
        "repo_url": "github.com/EdVince/whisper-trtllm",
        "filepath": "transformers/src/transformers/modeling_utils.py",
        "commit_date": "2023-08-22T07:50:00Z",
        "message": "add:transformers"
    },
    {
        "repo_url": "github.com/EdVince/whisper-trtllm",
        "filepath": "transformers/src/transformers/lib_integrations/peft/peft_mixin.py",
        "commit_date": "2023-08-22T07:50:00Z",
        "message": "add:transformers"
    },
    {
        "repo_url": "github.com/HKUDS/HiGPT",
        "filepath": "higpt/model/builder.py",
        "commit_date": "2024-02-23T13:50:20Z",
        "message": "up"
    },
    {
        "repo_url": "github.com/HKUDS/HiGPT",
        "filepath": "higpt/model/apply_lora.py",
        "commit_date": "2024-02-23T13:50:20Z",
        "message": "up"
    },
    {
        "repo_url": "github.com/X-D-Lab/KarmaVLM",
        "filepath": "llava/model/builder.py",
        "commit_date": "2024-02-25T12:09:23Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/RBDash-Team/RBDash",
        "filepath": "rbdash/model/builder.py",
        "commit_date": "2023-12-20T01:33:52Z",
        "message": "[feat] Add all files"
    }
]