[
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2020-08-11T19:15:55Z",
        "message": "Merge pull request #422 from stanfordnlp/master\n\nBack-merge documentation changes from master to dev"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2020-06-22T23:32:20Z",
        "message": "Rename stanza.resource to stanza.resources; rename resources.py to common.py"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2020-06-22T22:49:38Z",
        "message": "Move all resources related scripts to stanza/resource"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2020-06-11T06:04:53Z",
        "message": "Fix demo script"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2020-03-06T07:45:58Z",
        "message": "renaming stanfordnlp to stanza!!"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-02-19T08:18:42Z",
        "message": "Use sys.exit in place of exit"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-31T02:24:01Z",
        "message": "Revert temporary fix for download logic"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-31T02:18:05Z",
        "message": "Fix download logic; Fix help message in demo script"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-31T01:53:22Z",
        "message": "Quick fix for demo script"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-26T05:33:04Z",
        "message": "Separate the classes for Tokens and syntactic Words"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-25T09:25:55Z",
        "message": "Undo changes to use_gpu argument"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-25T09:16:37Z",
        "message": "Modify the way devices are specified in pipeline"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-25T00:25:14Z",
        "message": "Swap French demo input with one that has MWT in the first sentence"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-24T22:22:28Z",
        "message": "Accommodate Vietnamese in the pipeline tokenizer processor; Add a Vietnamese example to demo script"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-23T08:51:06Z",
        "message": "Demo script goes multi-lingual; don't confirm overwrite by default (by do it in the demo script)"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-23T08:51:06Z",
        "message": "Change demo to match logic of new download function: existence of models is checked in download() now; Add easy toggle for other demo languages"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-22T23:48:13Z",
        "message": "Modernizing and reusing build_default_config; Path fix in demo pipeline construction"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-22T23:31:32Z",
        "message": "Modernizing the demo script; make the download interface more user-friendly"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-20T08:10:41Z",
        "message": "update and simplify basic demo"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "demo/pipeline_demo.py",
        "commit_date": "2019-01-11T23:37:09Z",
        "message": "add demo"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2023-11-04T20:42:52Z",
        "message": "Oops, need to pass the transformer args - otherwise --use_bert without the model name is not correctly testing the dev/test sets"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2023-09-10T04:06:24Z",
        "message": "Properly connect run_ete.py to the charlms (actually, it had even been throwing previous to this change, so the 1.5.1 version is broken)"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2023-08-26T07:45:10Z",
        "message": "Add a filter for just the models that have transformers"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2023-08-22T03:35:43Z",
        "message": "Move the transformer choice to the resources directory and refactor a method that can be used by each of the processors which use the transformers"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2023-08-20T19:01:06Z",
        "message": "Set the depparse filename based on its model type - need to refactor some more pieces"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2023-08-18T00:09:25Z",
        "message": "Track whether or not the POS model is using charlm or transformer in the filename by default\n(or pretrain)\n\nRespect an existing --save_name and --save_dir when naming the pos models with run_pos"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2023-08-17T06:29:41Z",
        "message": "If given a command line flag, filter out the models which don't have charlm when running the lemmatizer"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2023-08-17T06:29:41Z",
        "message": "Log which treebanks are being used when expanding ud_all"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2023-08-17T06:29:41Z",
        "message": "Functionality for run_lemma.py to know what the correct model filename is before rejecting a model rebuild"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2023-08-17T06:29:41Z",
        "message": "Add the charlm to the run_lemma script"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2023-08-14T21:44:46Z",
        "message": "Nest things a little more cleanly inside an if statement"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2023-07-04T12:41:15Z",
        "message": "When running the charlm using run_charlm, add the arguments from the charlm's parser to the help message"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2023-06-30T23:56:48Z",
        "message": "Downloads of missing models for the tests etc should go to the chosen model_dir"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2023-06-30T15:04:34Z",
        "message": "Refactor choosing the pos_charlm to a separate method which already has the imported default_charlms & pos_charlms"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2023-06-28T18:57:37Z",
        "message": "Add defaults to the argparser when asked for --help (although some None values get cluttered)"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2023-06-22T15:27:56Z",
        "message": "Add notes on a Chinese Electra model which does better than the Roberta model we had used"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2023-05-18T04:55:54Z",
        "message": "Add some notes on FR POS tagging bert results"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2023-05-07T23:09:27Z",
        "message": "Save some notes on which HE transformer gets the best scores on POS & depparse"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2023-05-04T01:07:32Z",
        "message": "Log some EN results for conparser w/ finetuning"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2023-05-02T16:14:27Z",
        "message": "More extensive experiments on the differences between various EN transformers"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2023-04-30T03:07:29Z",
        "message": "Add a block on a POS experiment using Electra"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2023-04-28T01:03:02Z",
        "message": "Add some comments on HE experiments we should run"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2023-03-15T20:25:13Z",
        "message": "Add a chart to the ID transformers for constituency"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2023-03-13T23:11:06Z",
        "message": "Added code for hy_armtdp - Armenian NER dataset"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2023-03-13T01:43:44Z",
        "message": "Notes on Indonesian transformers based on some initial experiments on POS.  Need to do constituency and/or NER as well"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2023-02-28T21:55:26Z",
        "message": "Add notes on which Italian Bert models are working better or worse"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-12-08T07:56:08Z",
        "message": "Add some notes on roberta-base vs roberta-large"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-12-05T00:51:45Z",
        "message": "Update scores for a bunch of Danish transformer models"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-11-22T04:24:56Z",
        "message": "As a backup, download charlm if lang_charlm was asked for"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-11-22T01:50:16Z",
        "message": "Give a more informative error when a charlm cannot be found, but a possibly wrong name was used.  Even better would be to try again with the correct name"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-10-23T18:25:20Z",
        "message": "Notes on default bert model.  More hidden layers for VI bert by default"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-09-26T15:44:46Z",
        "message": "Add a tiny chart of our own to show what the various columns mean when outputting an evaluation.  Answers #1134"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-09-26T07:27:12Z",
        "message": "Note that combined_bert POS uses the same pretrain as default.  This requires a few minor changes elsewhere to the pretrain chooser"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-09-04T03:51:24Z",
        "message": "NER Polish (#1110)\n\n* Add NER dataset for Polish\n\nCo-authored-by: ryszardtuora <ryszardtuora@gmail.com>\nCo-authored-by: Karol Saputa <ksaputa@gputrain.dariah.ipipan.waw.pl>\n\nThis PR adds Polish NER dataset\n\n#1070"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-09-01T07:37:08Z",
        "message": "no enhanced dependencies"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-08-25T05:10:31Z",
        "message": "Refactor another call to ud_scores"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-08-25T05:07:44Z",
        "message": "Restore the functionality of the ETE (was broken after adding charlm to pos and other changes)"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-08-25T01:07:44Z",
        "message": "Add a simple test of ner_tagger.evaluate()"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-08-03T06:35:42Z",
        "message": "Make the error for an unfinished language (hopefully) more useful"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-07-20T21:14:33Z",
        "message": "Load the pretrained charlm, adds it as inputs to the POS model\n\nThis improves accuracy on almost all POS models\n\nDoing the same thing for depparse would also make sense, but is\ncurrently not done.  However, the downstream scores of depparse don't\nseem to be negatively affected by using the different (better) POS\ntags produced by models using the pretrained charlm\n\nAdd a pos-specific charlm map for the medical EN datasets and the one dataset which appears to be hurt by the charlm (tr_boun)\n\ncraft, genia -> None\n\nProduces resources.json with pos charlms\n\nMake the Pipeline pass in charlm paths if present in resources.json\nTODO: use the foundation_cache to load them"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-05-30T07:32:52Z",
        "message": "Notes on which embeddings are used for which NER, in the form of a map of default pretrains\n\nUse the default pretrains when using run_ner.py\n\nUpdate prepare_resources to use the new ner embedding info"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-05-30T04:01:43Z",
        "message": "Use save_name to check if a model already exists"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-04-28T06:59:11Z",
        "message": "Add notes on a German model"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-04-27T05:38:20Z",
        "message": "Better FA option"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-04-24T02:56:27Z",
        "message": "Notes on a Persian bert model choice"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Move the SHORTNAME_RE from the dataset/training tools to treebank_to_short_name.  Seems to work fine"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Rename and clarify a comment"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Automatically download missing pretrain or charlm when training.  Will hopefully make things more convenient"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Alphabetize bert options"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Some notes on a FI bert choice"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "A note about which IT roberta to use"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Refactor the BERT usage from run_constituency to run_ner"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Convert the run_charlm.sh script to run_charlm.py"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Get the default pretrain for a conparser or ner model from the resources"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Reuse the NER code for choosing a charlm to choose the default charlm when running the conparser"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Refactor (most of) the charlm args so we can add the charlm to run_constituency.py"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Fix typo"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Add a wrapper script for the constituency parser.\n\nSupports each of --train --score_dev and --score_test"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2021-07-10T01:33:40Z",
        "message": "Add some comments on the common.py main method"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2021-05-20T22:54:34Z",
        "message": "Process the --save_dir argument in the training scripts to avoid clobbering existing models when saving somewhere other than the default path"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2021-05-19T15:47:25Z",
        "message": "Call the eval script as a module instead of as a script.  This is necessary as, when installing using conda and pip to install directly from github, the executable flag was lost on the eval script."
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2021-05-18T19:22:21Z",
        "message": "Don't include the program name when parsing args"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2020-12-21T05:21:51Z",
        "message": "Make it possible to run ETE using one model on a different model's data"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2020-12-21T05:20:27Z",
        "message": "Convert the end-to-end script to python"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2020-12-11T03:50:39Z",
        "message": "Oops, introduced a bug into running on all_ud"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2020-12-08T23:13:15Z",
        "message": "Add a - in the RE for zh-hans, zh-hant, etc"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2020-12-08T16:16:34Z",
        "message": "Repeat back the command line arguments we were run with"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2020-12-03T02:19:10Z",
        "message": "Add the ability to specify shortname directly in the training scripts"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2020-12-02T04:08:47Z",
        "message": "Oops, bugfix"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2020-12-02T04:06:09Z",
        "message": "Add a flag to force retraining of existing models.  Otherwise, skip models even when a single model name is given"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2020-12-02T01:44:43Z",
        "message": "Cut off a trailing /"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2020-12-01T22:40:19Z",
        "message": "Make a temp output file name in common.py for the run scripts to use as needed"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2020-12-01T21:02:56Z",
        "message": "Turn the ad-hoc command line parsing into an argparser.  Includes a temp file option which is currently just a suggestion"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2020-11-28T18:02:38Z",
        "message": "Move the eval script usage to a common file"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2020-11-28T08:15:28Z",
        "message": "Lower the noise a little"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/training/common.py",
        "commit_date": "2020-11-28T04:48:46Z",
        "message": "Refactor some stuff out of run_tokenizer"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2024-03-01T03:41:09Z",
        "message": "Add some debug logging when building a retag_pipeline - goal is to make sure it's calling the correct POS model"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2024-02-24T07:41:20Z",
        "message": "Add an option to turn off saving the optimizer when saving each constituency model"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2024-02-24T07:41:20Z",
        "message": "Add flags to control how often the save_each models get saved.  Intent is to make it so that we can skip 10 models at a time and use a series of these to measure the variance in a silver dataset's scores"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2024-02-24T07:41:20Z",
        "message": "Remove unnecessary repetition of existing save_each argument"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2024-02-22T01:31:45Z",
        "message": "Set the trainer logging level to DEBUG when starting a constituency training run.  Switch out a bunch of training log calls to use the training logger specifically to make it easier to log things like the optimizer creation"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2024-02-22T01:31:45Z",
        "message": "Integrate peft with the constituency parser.\n\nAdd a test that the two stage peft is correctly turning off finetuning for the second half of training"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2024-02-03T03:35:51Z",
        "message": "Add several resolutions of ambiguous transitions to the top down dynamic oracle.\nAlso, add a flag which turns on selective oracle transitions, and\nautomatically ignore transitions after UNKNOWN unless --oracle_level is set\n\nAdd an immediate close version of the close/open error.\n\nAdd an immediate close version of the close/shift error.\n\nAdd an alternate version of the close/shift and close/open resolutions - close at the end of the outer constituent\n\nProposed fix of Open/Open: turn the new Open into a Unary\n\nAdd a fix where open/open is closed at the end of the outer constituent\n\nAdd a shift/open ambiguous fix - turn them into Unary\n\nPut the Close for a shift/open at the end, rather than earlier\n\nIncludes stats for dev & test scores for various ambiguous transitions"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2024-01-22T23:47:58Z",
        "message": "Add oracle_level as a parameter to the save name"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-12-13T07:27:13Z",
        "message": "Add a flag to not check the transitions or constituents in the dev set"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-09-17T00:03:44Z",
        "message": "Add a bit to the name expansion for when to begin finetuning a transformer"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-09-02T17:52:46Z",
        "message": "Use the fancy new savename constructions in the run_constituency script"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-08-30T18:40:03Z",
        "message": "Comments on an experiment which didn't work out for mixing bert layers"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-08-25T05:45:01Z",
        "message": "Add a log of the tensor sizes"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-08-14T22:07:08Z",
        "message": "Refactor build_model_filename for run_constituency"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-08-14T22:00:12Z",
        "message": "Add the constituency parser arguments to the run_constituency script, similar to what is done for the lemmatizer & charlm"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-07-24T21:52:49Z",
        "message": "Make a bit of a generalized save name - will add more options as they come up"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-06-04T19:13:59Z",
        "message": "Model names were being created with blank spaces in them :/"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-05-24T07:30:26Z",
        "message": "Update con learning rates using a reduce-on-plateau scheme instead of the warmup\n\nThe warmup never did anything, so we leave it non-functional for now"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-05-03T21:42:11Z",
        "message": "Add a parameter which attempts to download a saved model for futher processing"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-04-04T16:44:09Z",
        "message": "Add a --tokenized_dir argument to the conparser and ensemble scripts which tokenizes all of the text files in a directory in one invocation.  The VLSP group wanted a text repo parsed"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-03-27T08:16:55Z",
        "message": "Add notes on an experiment that did nothing in the conparser"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-03-24T16:01:17Z",
        "message": "Add a long description of some experiments with low LR for bert and adamw as the second stage optimizer"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-03-22T22:04:48Z",
        "message": "backprop into the transformer...\n\nAdd parameters to decide whether or not finetune the transformer\nBy default, train Bert with a low LR and very low WD to avoid it overreacting to the training data\n\nCheck bert_finetune when loading models.  Set it to false if not in the model so that we can preserve old models\nAlso, check bert_finetune for whether or not to use the foundation_cache when building a new model\n\nAlso backprop phobert, xlnet, and bart\n\nSkip the first N steps for finetuning the transformer based on a parameter\nAdd an endpoint for the bert finetuning as well (another option would be separate initial phase and second phase values)\nAdd a separate argument for stage1_bert_finetuning\nIncludes functionality to finetune a limited number of layers\n\nInstead of using the FoundationCache, we need to Load a new copy of\nthe bert_model if the bert_model was saved in the constituency file\n(otherwise, the cached model would be clobered for any other users of\nthe model)\n\nAdd a single epoch test of the bert finetuning\n\nAdd a multiepoch test which checks that the weights are or aren't close to the final model in a manner expected by the bert training\n\nAdd a per-layer test of the bert backprop - check that if bert_finetune_layers is set, it only backprops the number of layers requested\n\nInclude xlnet versions of a couple of the bert tests.  These tests use a copied\ntiny-random-xlnet so that the numbers aren't expected to change between iterations\n\nAdd a tiny test of the Bart training in conparser\nphobert uses a different tokenizer, so we can't directly test it the same way we test the others\n\nA few notes are left behind on things TODO:\n\nTODO: Experiment with a low level of finetuning when using adamw\nIt would not work well with madgrad yet, since there is an issue where LR==0 still results in learning\nhttps://github.com/facebookresearch/madgrad/issues/16\n\nTODO: when saving a trainer which had its own transformer copy in it,\nwe need to resave the transformer - currently it gets put into\nunsaved_modules if the model wasn't training the transformer\nWe even leave behind a test which covers that situation, commented out\n\nTODO: when creating an LSTMModel in the Trainer for training a new model,\nwe need to use a non-cached version of it.  In particular, reusing the\nPOS tagger later on in training would be a problem\n(except we currently don't do that)"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-03-15T20:25:13Z",
        "message": "Fix a comment on the experimental differences"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-02-11T01:53:25Z",
        "message": "Add a flag for reversing the treebanks.  The model will reverse the sentences when making initial word queues, so that the bert & charlm models work as expected\n\nAdd a parameter to build_treebank to reverse the trees when building sequences.  Will facilitate parsing in reverse\n\nReverse sentences at the end of parsing in reverse mode so that we get the correct order back\n\nReverse sentences when verifying the transitions\n\nTest reversing a sentence when building the transition sequence"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2023-02-01T07:51:06Z",
        "message": "Clarify some comments on a conparser argument"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-12-07T07:06:06Z",
        "message": "Start passing around device instead of use_cuda\n\nAll models now work via the pipeline.  Need to double check that they work everywhere"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-12-05T20:41:18Z",
        "message": "Determine if cuda() is available and use that to set the random seed rather than making that an argument"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-12-01T19:20:16Z",
        "message": "Add a reference to more info on nonlinearity"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-12-01T19:20:13Z",
        "message": "Add a maxout linear such as in\n\nhttps://arxiv.org/pdf/1302.4389v4.pdf\n\n.get() the arg to keep old models alive\n\nIncludes some comments on accuracy with maxout"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-11-30T23:43:27Z",
        "message": "Add large_margin as an option for the loss.\n\nAdd a note on a large margin experiment (no improvement)"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-11-25T07:25:04Z",
        "message": "Add a focal loss option using an external library\n\nUnfortunately, it doesn't seem to improve results"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-11-22T22:07:41Z",
        "message": "Add a bunch of comments on the file layout for the model.  Add a missing citation"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-11-22T19:26:01Z",
        "message": "Default to multistage w/ madgrad if available, adamw if not\n\nDon't set training defaults if we're not training"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-11-17T15:40:32Z",
        "message": "Add an --oracle_level flag which optionally restricts which level to use for the oracle"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-11-15T00:12:17Z",
        "message": "Refactor adjusting the prediction string format"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-11-14T16:50:55Z",
        "message": "Add an option to not delete duplicate trees.  Relevant in the case of Vietnamese trees including multiple copies of quad, for example"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-11-14T05:57:59Z",
        "message": "Add nonlinearity experiments numbers\n\nUpdate numbers for the tests on nonlinearity. The numbers for the optimizers will follow later."
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-11-14T02:40:06Z",
        "message": "Add a format specifier which includes the index for the Vietnamese bakeoff"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-11-07T20:56:04Z",
        "message": "Add a variant of multihead attention where there's one key or one key per label\nSeems competitive with MAX but not a big improvement of any kind\n\nAdds an optional position encoding to the KEY / UNTIED_KEY constituency compositions\n(using reduce_position as a parameter for the size, 0 -> no position)\n\nreduce_position needs to be an unsaved module so that it doesn't barf if it gets reloaded later with a different size\n\nIncudes comments on a couple variants that didn't work - linear after the attention or a double position vector"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-11-01T16:03:20Z",
        "message": "Add an argument for partitioning / not partitioning lattn"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-11-01T01:14:11Z",
        "message": "Use some words from the silver dataset (currently |gold| words are added, even if that means some overlaps)"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-10-30T08:16:34Z",
        "message": "Add a separate argument for --silver_epoch_size, just in case people want that"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-10-30T06:20:37Z",
        "message": "Rough draft of using silver trees.\n\nMostly untested.  Includes an unfinished test of the silver data"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-10-29T02:20:00Z",
        "message": "refactor predict dir,file,format args so they can be used elsewhere if needed"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-10-29T00:33:40Z",
        "message": "Add functionality to turn a tokenized text file into a file of parse trees"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-10-28T01:49:18Z",
        "message": "Refactor the retagging args & pipeline creation into a separate modeule"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-10-25T15:45:13Z",
        "message": "Add a --predict_format option which will allow the user to specify how to write trees when outputting the model predictions"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-10-23T19:02:18Z",
        "message": "By default, turn off pattn & lattn (at least until we figure out how to extract value from them)"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-10-07T16:38:34Z",
        "message": "Add a flag to control the learning rate in the adadelta portion of --multistage"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-10-07T16:20:35Z",
        "message": "Add learning_ in front of momentum and weight_decay to make it clear those parameters are for the optimizer"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-09-26T17:11:39Z",
        "message": "Add a learned weighting between bert layers\n\nMake the number of hidden layers an option and start from zeros\nGeneralize the num_layers for Phobert and XLNet.  Keep old models alive\n\nIncludes an option to use the layers from the older versions of conparse"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-09-25T05:30:49Z",
        "message": "Add a TREE_LSTM node combination method.\n\nInstead of using a complicated bi-lstm or the much simpler max method,\ncombine children into a new item using a max-tree-lstm\n\nAttempts to integrate this method with the other combination methods\n\nNotes on variants\n\nAttempts to preserve old models & old model behavior - arguments are\nset to be like the original defaults\n\nAttempt to come up with an initial tree_cx for the TREE_LSTM method\nSupport both TREE_LSTM and the version with the CX, TREE_LSTM_CX"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-09-25T02:56:28Z",
        "message": "Add an option to choose an exact model path for retagging in the conparser"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-09-25T02:56:28Z",
        "message": "pos_package was not the correct flag for the retag_pipeline to choose a different package"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-09-22T02:33:21Z",
        "message": "Transformer stack using MHA instead of LSTM as an option for the transitions and constituents\n\nBegin integrating the TransformerTreeStack into the parser\nAdd arguments for converting transformer_stack and constituent_stack into ATTN\nUses a position encoding vector in the TransformerTreeStack\n\nKeep old models alive by setting StackHistory when loading a model\n\nAdd a length_limit option to the TransformerTreeStack (currently not used)\n\nUse the positions list to remember the length of a stack, since the stack might be shorter than expected when using length_limit\n\nMay want to save memory in some way for the stacked key and value fields.\n\nUnfortunately, in general this doesn't seem like an\nimprovement... maybe there's a bug in it, or maybe there's a set of\nhyperparameters which will work better, or maybe the LSTM is just\nbetter in general."
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-09-12T20:21:40Z",
        "message": "Don't double save_dir if the user gives save_dir as part of the model filename"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-09-12T06:26:56Z",
        "message": "Fix remove_optimizer mode"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-09-10T02:06:46Z",
        "message": "Always save checkpoints.  Always load from a checkpoint if one exists.\n\nBuild the constituency optimizer using knowledge of how far you are in the training process - multistage part 1 gets Adadelta, for example\nTest that a multistage training process builds the correct optimizers, including when reloading\n\nWhen continuing training from a checkpoint, use the existing epochs_trained\nRestart epochs count when doing a finetune"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-08-26T20:15:37Z",
        "message": "Grad clipping in the constituency parser.  Not a clear benefit yet, so just leaving this as an option."
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-08-23T03:57:52Z",
        "message": "Notes on a couple updates to defaults"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-08-22T23:15:24Z",
        "message": "Reorganize --optim flag to under the optimizer parameters"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-08-22T21:31:27Z",
        "message": "Make momentum an argument for the optimizer"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-08-17T21:08:37Z",
        "message": "Add a brief comment on beta2"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-08-17T20:47:27Z",
        "message": "Add a couple more nonlinearity options.  Simplify the creation of the nonlinearity layers"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-08-17T17:48:23Z",
        "message": "Fix some doc and some defaults"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-08-17T07:26:33Z",
        "message": "Add a projection to a smaller dimension to the lattn inputs"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-08-10T07:10:03Z",
        "message": "A couple notes on other options in the conparser training"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-07-31T22:27:29Z",
        "message": "Making combining the input the default for the lattn layer"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-06-24T06:19:05Z",
        "message": "Add an option to build the lattn out of the entire input, not just pattn\n\nFudging a flag when reading in the models makes existing models work with the new argument"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-06-24T06:17:44Z",
        "message": "AdaDelta warmup for the conparser.  Motivation: AdaDelta results in\nhigher scores overall, but learns 0s for the weights of the pattn and\nlattn layers.  AdamW learns weights for pattn, and the models are more\naccurate than models trained without pattn using AdamW, but the models\nare lower scores overall than the AdaDelta models.\n\nThis improves that by first running AdaDelta, then switching.\n\nNow, if --multistage is set, run AdaDelta for half the epochs with no\npattn or lattn.  Then start the specified optimizer for the rest of\nthe time with the full model.  If pattn and lattn are both present,\nthe model is 1/2 no attn, 1/4 pattn, 1/4 pattn and lattn\n\nIncluded is a test of both the two and three step versions\n\nAlso add a false version of the option - we may very well want to make\n--multistage the default"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-06-24T06:17:44Z",
        "message": "move some defaults from constituency_parser to utils (will be useful for using them elsewhere)"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-06-20T21:34:23Z",
        "message": "currently lattn positional dim is assumed to be 1/2 of pattn"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-06-20T18:57:07Z",
        "message": "Save all conparser models if a particular command line option is set"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-06-16T22:14:52Z",
        "message": "Adding an LR warmup to the optimizer\n\nsave & load scheduler as part of the trainer files"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-06-16T07:14:14Z",
        "message": "Finetune a model that was learned with no pattn to now use pattn\n\nFinally, a method that successfully trains pattn layers (seriously, the scores go up compared to adadelta or adamw by themselves)"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-06-14T08:17:21Z",
        "message": "Fix broken conparse finetune :/"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-06-08T08:03:33Z",
        "message": "Automagically fix xpos being None for the POS tagger used for a conparser retagging"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-06-05T00:18:21Z",
        "message": "Very simple wandb integration for NER, tokenizer, mwt, lemma, depparse, pos, charlm, classifier, conparse"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-05-26T20:03:05Z",
        "message": "Add arguments for epsilon and beta2 to initializing an AdamW optimizer"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-05-08T21:28:59Z",
        "message": "Another note on a failed experiment"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-05-08T02:46:23Z",
        "message": "Add some notes on an MLP experiment which didn't work"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-05-06T20:58:48Z",
        "message": "ATTN method to build larger constituents out of smaller constituents"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-05-02T13:51:13Z",
        "message": "This comment is wrong - it was fixed a long time ago!"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "A couple minor changes that came up when removing optimizers from conparser models"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Add a note on a failed experiment"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Remove the specialized unary transform\n\nAdd a note on the use of TOP_DOWN_UNARY"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "One last comment on a possible bilstm modification"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Remove an unused option which hurts scores.  (Not having this option makes other subsequent changes easier.)  Leave a bunch of comments explaining how it hurt the scores"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Init the embeddngs at a scale closer to the scale the Adadelta learns\n\nIn most cases the model mostly throws away the initial values and learns new ones, especially for the delta_embedding\nHowever, there is a slight but noticeable improvement in scores\nFor example, ja_alt at 200 iterations goes from 0.8980 to 0.8985"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "This WD seems to work better?  needs more testing"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Some notes on an adamw hill climb"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Add a regex to output the norms and grads of a subset of weights if requested"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Update notes on failed experiments with a couple results"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Log norms of matrices in the model when given a flag"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Always use the output dimension of the pattn layer for the input of the lattn layer, since nothing else currently works anyway"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Add a comment and a citation on what label_attention does"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "add an initial label attention to the lstm_model.py\n\nincorporates the labeled representation into the input\n\nuses command line options for label attention\n\neverything is in a single label_attention_module that can be reused, as long as the embeddings and the appropriate maskings are available"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "When combining children nodes, take the max value instead of applying a fancy LSTM\n\nThis seems to perform better in some test cases\n\nKeeps an old version of the constituent building around as an options\nAdds a test of different constituency composition functions"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/models/constituency_parser.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "changes to the partitioned transformer so that it's more compact\n\npartitioned attention is separated as a module so that it can be easily reused\n\nalso, add an option for bias (defaults to false)"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/prepare_depparse_treebank.py",
        "commit_date": "2024-02-10T02:54:41Z",
        "message": "... from e"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/prepare_depparse_treebank.py",
        "commit_date": "2024-02-10T02:48:07Z",
        "message": "Flesh out an error a bit more so it hopefully helps people figure out how to fix it when preparing depparse data"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/prepare_depparse_treebank.py",
        "commit_date": "2023-12-01T20:28:57Z",
        "message": "Oops, needed to drop the usage of pos_batch_size from prepare_depparse_treebank as well"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/prepare_depparse_treebank.py",
        "commit_date": "2023-08-17T22:33:38Z",
        "message": "Try to pick the best POS candidate when gathering the depparse dataset - in order of transformer, charlm, no charlm"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/prepare_depparse_treebank.py",
        "commit_date": "2023-08-17T18:35:48Z",
        "message": "Add a flag to not download the tagger for prepare_depparse_treebank.  Addresses part of #1272"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/prepare_depparse_treebank.py",
        "commit_date": "2023-06-05T04:50:56Z",
        "message": "Refactor many of the default package, charlm, and pretrain knowledge into a separate file.  Will avoid circular imports if something imported by stanza (the root package) needs to import the constants.  One example would be a model trying to find its own default charlm"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/prepare_depparse_treebank.py",
        "commit_date": "2023-05-06T03:11:48Z",
        "message": "Treat --eval_file the same as the gold file in the tagger, but add an option to not score the final results in the event the eval file doesn't have tags and we just want to see the outputs\n\n(Also, remove the --gold_file flag when preparing the depparse treebank)"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/prepare_depparse_treebank.py",
        "commit_date": "2023-04-12T04:02:27Z",
        "message": "Pass around model types using an enum specifically for the dataset preparation"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/prepare_depparse_treebank.py",
        "commit_date": "2023-01-01T19:20:11Z",
        "message": "fix an issue with prepare_depparse_treebank\n\nprepare_depparse_treebank does not use the pretrained file passed via\nwordvec_pretrain_file option.\nwordvec_args expects the list of options as the last argument, but args\nis namespace object.\nNow if wordvec_pretrain_file is set we add it to the base_args,\notherwise invoke wordvec_args function"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/prepare_depparse_treebank.py",
        "commit_date": "2022-09-07T22:15:57Z",
        "message": "Eliminate a redundant function call"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/prepare_depparse_treebank.py",
        "commit_date": "2022-08-05T18:48:13Z",
        "message": "handle relative paths to tagger in prepare_depparse_treebank"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/prepare_depparse_treebank.py",
        "commit_date": "2022-08-02T07:14:55Z",
        "message": "Automatically download POS (with charlm & wordvec) when redoing depparse"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/prepare_depparse_treebank.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "fix args tagmethod error message, formats, types (#833)\n\n* fix args tagmethod error message, formats, types"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/prepare_depparse_treebank.py",
        "commit_date": "2021-05-22T07:04:38Z",
        "message": "Add a flag for specifying which word vectors to use when using a tagger for building the depparse data"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/prepare_depparse_treebank.py",
        "commit_date": "2020-12-21T05:21:51Z",
        "message": "Make prepare_depparse runnable with it_combined"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/prepare_depparse_treebank.py",
        "commit_date": "2020-12-21T05:20:27Z",
        "message": "Batch size needs to be lower when tagging the train set for building the depparse data"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/utils/datasets/prepare_depparse_treebank.py",
        "commit_date": "2020-12-21T05:20:27Z",
        "message": "prepare_depparse_treebank in python instead of shell.  Can do both gold and predicted tags"
    },
    {
        "repo_url": "github.com/microsoft/presidio",
        "filepath": "presidio-analyzer/install_nlp_models.py",
        "commit_date": "2023-10-04T11:42:41Z",
        "message": "added image processing class to preprocess the image before running OCR (#1166)"
    },
    {
        "repo_url": "github.com/microsoft/presidio",
        "filepath": "presidio-analyzer/install_nlp_models.py",
        "commit_date": "2023-01-25T07:06:02Z",
        "message": "Install transformers model into the docker image (#912)"
    },
    {
        "repo_url": "github.com/microsoft/presidio",
        "filepath": "presidio-analyzer/install_nlp_models.py",
        "commit_date": "2021-02-16T15:10:19Z",
        "message": "docs on adding recognizers (#508)"
    },
    {
        "repo_url": "github.com/microsoft/presidio",
        "filepath": "presidio-analyzer/install_nlp_models.py",
        "commit_date": "2021-02-08T08:18:03Z",
        "message": "Install NLP models in Docker (#468)"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2021-10-01T09:33:56Z",
        "message": "update format"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2021-07-09T15:34:15Z",
        "message": "Revert \"Bug fix\"\n\nThis reverts commit b30d4f06cbe742d9a7aae8b8ea30c68e0bd1a178."
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2021-07-07T14:33:21Z",
        "message": "Bug fix"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2021-06-30T10:53:40Z",
        "message": "Merge branch 'master' of https://github.com/QData/TextAttack into multilingual\n\n\u0001 Conflicts:\n\u0001\ttextattack/constraints/semantics/sentence_encoders/bert/bert.py\n\u0001\ttextattack/datasets/dataset.py\n\u0001\ttextattack/models/helpers/utils.py\n\u0001\ttextattack/shared/word_embeddings.py"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2021-06-04T05:32:33Z",
        "message": "WIP: finish attack args, attacker, cli commands"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2021-06-04T05:31:09Z",
        "message": "WIP: completed attack, dataset, model, logging args"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2021-03-11T14:11:21Z",
        "message": "Merge branch 'master' of https://github.com/QData/TextAttack into multilingual"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2021-02-15T09:55:00Z",
        "message": "Fixed flake8 error (undefined variable and formatting)"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2021-02-15T09:55:00Z",
        "message": "introduce new download method from any url"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2021-02-12T16:27:54Z",
        "message": "update test to fix inconsistency introduced by new stanza resource"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-10-31T04:49:53Z",
        "message": "add install of stanza resource when package is present"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-10-30T21:31:00Z",
        "message": "make some dependencies optional"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-10-30T20:58:39Z",
        "message": "revert changes and fix circular imports"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-10-30T16:17:53Z",
        "message": "formatting"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-10-30T16:17:52Z",
        "message": "add lazy loading"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-10-16T21:15:03Z",
        "message": "Add CoLA based constraint\n\nAdded a constraint that uses a model pre-trained on the CoLA dataset to check that the attacked text has a certain number of linguistically acceptable sentences as compared to the reference text"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-10-02T00:20:23Z",
        "message": "Add stanza support for part-of-speech constraint"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-09-29T02:37:21Z",
        "message": "change all mentions of nlp to datasets"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-08-18T14:17:28Z",
        "message": "add French attack example"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-07-13T03:12:39Z",
        "message": "update f-string placeholders"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-07-12T23:07:41Z",
        "message": "remove dead imports"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-07-11T05:01:22Z",
        "message": "Flake8 Fix with 'make format'"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-07-11T01:49:20Z",
        "message": "Reformatting-try2"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-07-10T23:21:02Z",
        "message": "Reformatting-try1"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-06-30T00:31:14Z",
        "message": "update tests for ' and -"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-06-26T02:26:12Z",
        "message": "remove spacy; add 3 MR models"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-06-24T15:02:24Z",
        "message": "automatically make cache dir (instead of throwing an error)"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-06-24T03:13:02Z",
        "message": "remove pyyaml as a dependency"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-06-24T03:11:15Z",
        "message": "throw error for nonexistent cache dir"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-06-24T02:35:00Z",
        "message": "smart cache dir, synchronous post-install hook, better constraint caching"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-06-17T18:18:53Z",
        "message": "support multiple workers on travis"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-06-16T00:40:44Z",
        "message": "add isort and fix import loops, preparing to shorten tests"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-06-15T22:36:04Z",
        "message": "code formatting; tokenizers->models/tokenizers"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-06-15T21:07:12Z",
        "message": "makefile and setup; need to fix imports"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-06-09T21:30:18Z",
        "message": "figured out huggingface tokenizers for benchmarking, now integrating into attack"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-06-05T23:06:22Z",
        "message": "pretrained & file models/datasets; tests; get_logger() -> logger"
    },
    {
        "repo_url": "github.com/QData/TextAttack",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2020-06-05T21:35:52Z",
        "message": "support custom models; waiting on datasets integration"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/setup.py",
        "commit_date": "2024-02-03T03:34:40Z",
        "message": "Potentially lowercase the data in a lemmatizer if all of the training data (or a user flag) requested it\n\nTesting additions:\n\nAdd a basic unit test of the all_lowercase function\nAdd a test of the caseless lemmatizer in the Pipeline\nTest that the Latin ITTB lemmatizer is marked as caseless.  Check that the results for capitalized text is as expected\n\nAddresses https://github.com/stanfordnlp/stanza/issues/1330"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/setup.py",
        "commit_date": "2023-10-05T16:51:07Z",
        "message": "Add Arabic to the models downloaded as part of the test suite"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/setup.py",
        "commit_date": "2023-02-02T00:19:39Z",
        "message": "Script to track labeling tasks from aws sagemaker private workforce\n\nIncludes basic test file using a supplementary folder (stored in a zip) for AWS labeling tracker"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/setup.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "At long last, add a mechanism for using multiple NER models at once\n\nDownload ncbi_disease as part of the testing infrastructure.  Use it to run two NER models at once"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/setup.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Download corenlp and the relevant models as part of the test setup script"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/setup.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Attempt to replace the .sh test setup script with a .py script"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_core.py",
        "commit_date": "2023-11-10T00:15:59Z",
        "message": "Add mwt to expected results for several more EN downloads"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_core.py",
        "commit_date": "2023-10-03T04:17:33Z",
        "message": "Update NER packages to reflect the new ontonotes_charlm default NER"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_core.py",
        "commit_date": "2023-02-04T07:43:37Z",
        "message": "Pick a language name we know won't exist for the test of an unknown language"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_core.py",
        "commit_date": "2023-02-04T07:30:22Z",
        "message": "Allow for loading a pipeline for a language that doesn't exist, as long as everything is specified"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_core.py",
        "commit_date": "2022-07-05T18:34:58Z",
        "message": "Add a test that download_method=None doesn't clobber the wrong model"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_core.py",
        "commit_date": "2022-07-05T18:34:55Z",
        "message": "Allow strings for the DownloadMethod.  Will hopefully make the interface simpler for people"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_core.py",
        "commit_date": "2022-05-30T07:32:52Z",
        "message": "Notes on which embeddings are used for which NER, in the form of a map of default pretrains\n\nUse the default pretrains when using run_ner.py\n\nUpdate prepare_resources to use the new ner embedding info"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_core.py",
        "commit_date": "2022-05-13T21:59:08Z",
        "message": "write temp directories to test working dir not present dir"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_core.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Allow for selectively using processors.  Answers #945"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_core.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Download missing models when creating a pipeline\n\nLess noisy logging when creating a pipeline from already downloaded items\nFilters variants before trying to download them\nDoesn't try to download models for annotators which don't exist\nAttempt to download a missing resources.json\nDownloads missing resources if REUSE_RESOURCES is set\n\nAdds tests of the reuse_resources and download_resources options\nincluding checks using mod times to verify that downloads occurred,\nand checks that various download methods will overwrite an incorrect model"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/pipeline/test_core.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Move a bunch of tests to more appropriate homes"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_common.py",
        "commit_date": "2023-10-03T04:17:33Z",
        "message": "Update NER packages to reflect the new ontonotes_charlm default NER"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_common.py",
        "commit_date": "2023-03-22T03:12:50Z",
        "message": "Add a utility method for extracting the resources for a specific language, possibly following 'alias' links"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_common.py",
        "commit_date": "2022-08-02T06:30:14Z",
        "message": "Add a piece of doc"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_common.py",
        "commit_date": "2022-06-16T22:14:52Z",
        "message": "Change a bunch of tempdirs to be under TEST_WORKING_DIR"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_common.py",
        "commit_date": "2022-05-30T07:32:52Z",
        "message": "Notes on which embeddings are used for which NER, in the form of a map of default pretrains\n\nUse the default pretrains when using run_ner.py\n\nUpdate prepare_resources to use the new ner embedding info"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_common.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Add the ability to use an alternate_md5 to accept a file.  Will allow resource files to denote an older md5 which we allow when downloading a file.  In particular, that lets us get around the non-atomic nature of putting up a new resources.json file while the models are still uploading to huggingface"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_common.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Use ValueError and FileNotFoundError instead of assert for unexpected md5"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_common.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Allow specifying multiple packages of NER to download at the same time"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_common.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Add a test for the download of a single model and its dependencies"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_common.py",
        "commit_date": "2022-04-23T04:36:46Z",
        "message": "Allow the package parameter to be a dict when specifying a list of processors.  Will make it easier to limit exactly which processors to choose, as package=None is no longer necessary"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_common.py",
        "commit_date": "2021-09-08T21:22:41Z",
        "message": "Separate tests of different components into different files"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_installation.py",
        "commit_date": "2022-06-16T22:14:52Z",
        "message": "Change a bunch of tempdirs to be under TEST_WORKING_DIR"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_installation.py",
        "commit_date": "2021-10-04T05:11:13Z",
        "message": "Merge branch 'main' into dev"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_installation.py",
        "commit_date": "2021-09-08T21:22:41Z",
        "message": "Separate tests of different components into different files"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza",
        "filepath": "stanza/tests/resources/test_installation.py",
        "commit_date": "2021-09-08T21:22:41Z",
        "message": "Move the installation test to its own directory"
    },
    {
        "repo_url": "github.com/stanford-oval/WikiChat",
        "filepath": "wikiextractor/split_passages.py",
        "commit_date": "2023-12-06T03:19:45Z",
        "message": "Add code and instructions"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2023-05-10T10:53:55Z",
        "message": "Use the dict merge operation supported by 3.8"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2023-05-10T10:53:55Z",
        "message": "Fix potential bugs of `add_tokens`"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2023-05-10T10:53:55Z",
        "message": "Support vocab extension"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2023-05-10T10:53:55Z",
        "message": "Strip tokenized tokens in extended vocab"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2023-05-10T10:53:55Z",
        "message": "Consider added tokens as well"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-10-20T13:19:18Z",
        "message": "Add `tokens` property"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-10-05T12:56:26Z",
        "message": "Syntax sugars for dist training"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-09-27T12:22:06Z",
        "message": "Handle tok with `ByteLevel` pre_tokenizer properly"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-07-30T13:47:07Z",
        "message": "Try loading local transformers files first"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-07-30T13:47:07Z",
        "message": "Handle unk case for `TransformerTokenizer`"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-07-30T13:47:07Z",
        "message": "Skip special tokens and keep spaces while decoding"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-07-30T13:47:07Z",
        "message": "Fix potential bugs"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-07-30T13:47:07Z",
        "message": "Fix vocab bug within BPE decoding"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-07-30T13:47:07Z",
        "message": "Support subword-nmt backend"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-07-30T13:47:07Z",
        "message": "Provide `min_freq` for BPE Tokenizer"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-07-30T13:46:55Z",
        "message": "More usable BPE tokenizers"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-07-01T07:09:51Z",
        "message": "Specify word suffix for BPE Trainer"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-06-25T06:29:43Z",
        "message": "Wrapper for BPE Tokenizer"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-06-24T09:52:29Z",
        "message": "Syntactic sugar for Transformer tokenizers"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-04-01T05:39:07Z",
        "message": "Minor revisions"
    },
    {
        "repo_url": "github.com/yzhangcs/parser",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2022-04-01T05:39:07Z",
        "message": "Integrated tokenizer (#47)"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2023-09-25T04:56:14Z",
        "message": "for backslash-escape"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2023-09-24T13:36:07Z",
        "message": "revert"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2023-09-24T12:06:15Z",
        "message": "https://github.com/KoichiYasuoka/UniDic2UD/issues/2"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2023-04-17T15:55:15Z",
        "message": "\u8fd1\u73fe\u4ee3\u53e3\u8a9e\u5c0f\u8aacUniDic"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2023-04-17T14:54:13Z",
        "message": "cwj-3.1.1 and csj-3.1.1"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2022-04-24T13:52:06Z",
        "message": "use UniDic-202203"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2022-04-24T10:33:27Z",
        "message": "ChamameWebAPI changed"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2022-04-15T14:55:30Z",
        "message": "URL update"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2021-11-07T04:59:15Z",
        "message": "URL changed"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2021-04-04T00:59:04Z",
        "message": "force download UniDic"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2021-04-03T06:37:59Z",
        "message": "gendai and spoken URLs changed"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2021-03-06T14:27:15Z",
        "message": "bug fix"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2021-02-21T08:58:06Z",
        "message": "Chamame moved to chamame.ninjal.ac.jp"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2021-02-20T14:05:29Z",
        "message": "Stanza==1.2 support"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2021-02-04T03:48:40Z",
        "message": "unidic-lite support"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2021-02-01T15:12:07Z",
        "message": "progressbar outputs sys.stderr"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2021-01-25T05:03:26Z",
        "message": "flush=True"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2021-01-23T13:54:49Z",
        "message": "AUX improved"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-12-28T03:38:53Z",
        "message": "ipadic support"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-11-23T10:20:15Z",
        "message": "bug fix"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-09-21T06:35:26Z",
        "message": "bug fix"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-09-19T05:06:35Z",
        "message": "use deplacy.PACKAGE_DIR"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-08-12T12:49:54Z",
        "message": "\u5f62\u5bb9\u8a5e-\u975e\u81ea\u7acb\u53ef\u80fd improved"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-08-12T08:46:24Z",
        "message": "\u52d5\u8a5e-\u975e\u81ea\u7acb\u53ef\u80fd improved"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-07-29T05:32:35Z",
        "message": "stanza_ja support"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-07-25T15:59:49Z",
        "message": "\u540d\u8a5e-\u52a9\u52d5\u8a5e\u8a9e\u5e79 support"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-07-08T05:39:49Z",
        "message": "unidic2ud.deprelja -> deplacy.deprelja"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-07-07T12:58:46Z",
        "message": "mecab-python3 1.0 support"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-07-02T01:46:35Z",
        "message": "to_tree(CatenaAnalysis) support"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-06-25T03:09:00Z",
        "message": "to_tree() uses deplacy"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-04-12T06:47:16Z",
        "message": "progress bar improved"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-03-31T04:30:21Z",
        "message": "to_tree() changed into BOX DRAWINGS DOUBLE"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-03-26T01:18:01Z",
        "message": "version 2.0.0"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-03-26T01:09:31Z",
        "message": "version 1.9.9"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-03-21T01:14:07Z",
        "message": "version 1.9.8"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-03-20T16:18:43Z",
        "message": "version 1.9.7"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-02-10T06:13:01Z",
        "message": "version 1.9.3"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-02-09T07:13:24Z",
        "message": "version 1.9.2"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-02-08T13:12:18Z",
        "message": "version 1.9.0"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-02-08T05:50:13Z",
        "message": "version 1.8.9"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-01-17T05:20:21Z",
        "message": "version 1.8.5"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-01-14T16:43:47Z",
        "message": "ipadic deleted from distribution"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-01-14T15:15:03Z",
        "message": "japanese-modern supported"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-01-13T06:39:34Z",
        "message": "unidic-csj-3.0.1 supported"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2020-01-03T00:42:30Z",
        "message": "version 1.7.6"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-12-29T01:48:04Z",
        "message": "version 1.7.3"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-12-27T08:44:07Z",
        "message": "version 1.6.8"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-12-27T06:35:22Z",
        "message": "fugashi.GenericTagger supported"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-12-26T14:15:50Z",
        "message": "version 1.6.6"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-12-20T10:32:53Z",
        "message": "version 1.6.2"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-12-18T07:40:06Z",
        "message": "version 1.5.9"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-12-14T23:51:39Z",
        "message": "version 1.5.8"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-12-10T06:46:38Z",
        "message": "version 1.5.3"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-11-22T05:42:52Z",
        "message": "to_tree(Japanese) supported"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-11-21T14:31:08Z",
        "message": "version 1.4.7"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-11-14T02:32:09Z",
        "message": "to_tree() released"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-11-13T03:00:10Z",
        "message": "\u6fc1\u70b9/\u534a\u6fc1\u70b9 process"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-11-10T14:47:36Z",
        "message": "PUNCT for ipadic"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-10-20T03:42:30Z",
        "message": "dictlist supported"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-09-15T07:00:41Z",
        "message": "version 1.1.4"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-09-15T06:38:16Z",
        "message": "to_svg(item) enhanced"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-09-14T23:20:56Z",
        "message": "to_svg() supported"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-31T23:12:40Z",
        "message": "bug fix for \u30d5\u30a3\u30e9\u30fc of ipadic"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-31T23:00:16Z",
        "message": "bug fix for Translit"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-31T22:38:28Z",
        "message": "bug fix for U+3000"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-29T04:18:55Z",
        "message": "unidic2ud.spacy supported"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-28T08:39:07Z",
        "message": "version 0.9.8 raw option supported"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-27T09:39:00Z",
        "message": "index for UDPipeEntry supported"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-27T05:49:26Z",
        "message": "bug fix for Translit of \"spoken\""
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-27T05:06:43Z",
        "message": "bug fix for Translit of \"gendai\""
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-27T04:50:26Z",
        "message": "bug fix for \"gendai\""
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-27T02:20:01Z",
        "message": "UDPipe models renamed"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-27T01:56:01Z",
        "message": "bug fix"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-26T22:22:32Z",
        "message": "UniDic=None as default"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-26T16:10:59Z",
        "message": "bug fix"
    },
    {
        "repo_url": "github.com/KoichiYasuoka/UniDic2UD",
        "filepath": "unidic2ud/unidic2ud.py",
        "commit_date": "2019-08-26T15:45:01Z",
        "message": "initial release"
    },
    {
        "repo_url": "github.com/consuldemocracy/consuldemocracy",
        "filepath": "public/machine_learning/scripts/budgets_related_content_and_tags_nmf.py",
        "commit_date": "2022-11-11T00:40:04Z",
        "message": "Use a different machine learning folder per tenant\n\nWe're using the \"tenants\" subfolder for consistency with the folder\nstructure we use in ActiveStorage and because some CONSUL installations\nmight have folders inside the `data` folder which might conflict with\nthe folders created by tenants.\n\nNote that the Python scripts have a lot of duplication, meaning we need\nto change all of them. I'm not refactoring them because I'm not familiar\nenough with these scripts (or with Python, for that matter).\n\nAlso note that the scripts folder is still shared by all tenants,\nmeaning it isn't possible to have different scripts for different\ntenants. I'm not sure how this situation should be handled; again, I'm\nnot familiar enough with this feature."
    },
    {
        "repo_url": "github.com/consuldemocracy/consuldemocracy",
        "filepath": "public/machine_learning/scripts/budgets_related_content_and_tags_nmf.py",
        "commit_date": "2021-09-07T16:18:47Z",
        "message": "Update machine learning scripts with NNMF and TextRank-GloVe techniques"
    },
    {
        "repo_url": "github.com/consuldemocracy/consuldemocracy",
        "filepath": "public/machine_learning/scripts/budgets_related_content_and_tags_nmf.py",
        "commit_date": "2021-08-16T14:31:04Z",
        "message": "Add experimental machine learning"
    },
    {
        "repo_url": "github.com/consuldemocracy/consuldemocracy",
        "filepath": "public/machine_learning/scripts/proposals_related_content_and_tags_nmf.py",
        "commit_date": "2022-11-11T00:40:04Z",
        "message": "Use a different machine learning folder per tenant\n\nWe're using the \"tenants\" subfolder for consistency with the folder\nstructure we use in ActiveStorage and because some CONSUL installations\nmight have folders inside the `data` folder which might conflict with\nthe folders created by tenants.\n\nNote that the Python scripts have a lot of duplication, meaning we need\nto change all of them. I'm not refactoring them because I'm not familiar\nenough with these scripts (or with Python, for that matter).\n\nAlso note that the scripts folder is still shared by all tenants,\nmeaning it isn't possible to have different scripts for different\ntenants. I'm not sure how this situation should be handled; again, I'm\nnot familiar enough with this feature."
    },
    {
        "repo_url": "github.com/consuldemocracy/consuldemocracy",
        "filepath": "public/machine_learning/scripts/proposals_related_content_and_tags_nmf.py",
        "commit_date": "2021-09-07T16:18:47Z",
        "message": "Update machine learning scripts with NNMF and TextRank-GloVe techniques"
    },
    {
        "repo_url": "github.com/consuldemocracy/consuldemocracy",
        "filepath": "public/machine_learning/scripts/proposals_related_content_and_tags_nmf.py",
        "commit_date": "2021-08-16T14:31:04Z",
        "message": "Add experimental machine learning"
    },
    {
        "repo_url": "github.com/futurulus/coop-nets",
        "filepath": "third-party/stanza/stanza/util/resource.py",
        "commit_date": "2016-04-04T06:17:46Z",
        "message": "Merge commit '37db677eec1f83fae502d253f790c6f54e34abf5'"
    },
    {
        "repo_url": "github.com/futurulus/coop-nets",
        "filepath": "third-party/stanza/stanza/util/resource.py",
        "commit_date": "2016-02-02T01:36:53Z",
        "message": "Squashed 'third-party/stanza/' changes from 09f78e3..2e33ff2\n\n2e33ff2 add slack integration\nc1d7f48 added TensorBoardLogger\nae0e116 Tools for working with CodaLab\n7db1754 Document and test TensorBoard logging\n7eedffb Cast histogram values to ordinary floats\n\ngit-subtree-dir: third-party/stanza\ngit-subtree-split: 2e33ff2822d5f627fdd282645efa398721f0eb21"
    },
    {
        "repo_url": "github.com/futurulus/coop-nets",
        "filepath": "third-party/stanza/stanza/util/resource.py",
        "commit_date": "2016-01-28T23:56:59Z",
        "message": "Merge commit 'ec1b3ee0a9f85151068bb72462dd7275bb8c0f17' as 'third-party/stanza'"
    },
    {
        "repo_url": "github.com/honghanhh/ner-combining-contextual-and-global-features",
        "filepath": "enconll03_baselines/conll_stanza.py",
        "commit_date": "2020-05-06T15:31:08Z",
        "message": "double check stanza, flair and evaluate results"
    },
    {
        "repo_url": "github.com/honghanhh/ner-combining-contextual-and-global-features",
        "filepath": "enconll03_baselines/conll_stanza.py",
        "commit_date": "2020-04-25T17:43:47Z",
        "message": "Evaluate Flair/Stanza on eng.test.2.examples.txt"
    },
    {
        "repo_url": "github.com/honghanhh/ner-combining-contextual-and-global-features",
        "filepath": "enconll03_baselines/conll_stanza.py",
        "commit_date": "2020-04-24T16:04:19Z",
        "message": "Update working status and stanza"
    },
    {
        "repo_url": "github.com/honghanhh/ner-combining-contextual-and-global-features",
        "filepath": "enconll03_baselines/conll_stanza.py",
        "commit_date": "2020-04-24T16:02:29Z",
        "message": "Test with stanza, reconfig downloaded model to fix error"
    },
    {
        "repo_url": "github.com/honghanhh/ner-combining-contextual-and-global-features",
        "filepath": "enconll03_baselines/conll_stanza.py",
        "commit_date": "2020-04-24T14:38:06Z",
        "message": "Test with flair embeddings, error pipeline in stanza"
    },
    {
        "repo_url": "github.com/honghanhh/ner-combining-contextual-and-global-features",
        "filepath": "enconll03_baselines/conll_stanza.py",
        "commit_date": "2020-04-20T08:08:20Z",
        "message": "Update task 3 and baseline code using stanza and flair"
    },
    {
        "repo_url": "github.com/huggingface/api-inference-community",
        "filepath": "docker_images/stanza/app/pipelines/token_classification.py",
        "commit_date": "2023-04-19T12:57:59Z",
        "message": "[Diffusers] Add text-guided image to image (#223)\n\n* [Diffusers] Add image to image\n\n* fix some stuff\n\n* make style\n\n* make style\n\n* Update docker_images/diffusers/app/pipelines/image_to_image.py\n\nCo-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>\n\n* Fixing diffusers.\n\n* Apply suggestions from code review\n\nCo-authored-by: Omar Sanseviero <osanseviero@gmail.com>\n\n* Apply suggestions from code review\n\n* up\n\n* make style\n\n* Apply suggestions from code review\n\n* finalize\n\n---------\n\nCo-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>\nCo-authored-by: Omar Sanseviero <osanseviero@gmail.com>"
    },
    {
        "repo_url": "github.com/huggingface/api-inference-community",
        "filepath": "docker_images/stanza/app/pipelines/token_classification.py",
        "commit_date": "2022-03-16T10:53:41Z",
        "message": "MNT move api-inference-community to root of the repo"
    },
    {
        "repo_url": "github.com/alexandrainst/danlp",
        "filepath": "examples/benchmarks/dependency_benchmarks.py",
        "commit_date": "2021-09-29T11:39:16Z",
        "message": "Fixing errors for the documentation page (Tasks, Datasets, Frameworks) (#139)\n\n* Fix Windows compatibility error\n\n* Fix Windows compatibility error 2\n\n* added test script to gitignore\n\n* Accidentally pushed local test artifacts.\n\n* Fix Windows compatibility error 3\n\n* Removed a parentheses\n\n* Fix windows Compatibility error 4\n\n* Updated comments and changed input string\n\n* Updated another comment\n\n* Removed unexpected key word arguments\n\n* example sentences for Frameworks flair 'Your dataset'\n\n* Fixed typo and updated documentation to fit with flair 0.8\n\n* Now includes line to download the required stanza model\n\n* Avoiding division by zero\n\n* added comment that describes how to run the dacy benchmark functions\n\n* removed relative import, which doesn't work for Windows and Mac\n\n* Updated version of pyicu\n\n* Updated print on how to download and where to place model\n\n* added line that downloads needed resources\n\n* compatibility with flair 0.8\n\n* Windows compatibility\n\n* reverting gitignore\n\n* fix pycache ignore\n\n* compatibility with flair 0.8\n\n* coherence between model_path and output\n\n* coherence between model_path and output\n\n* removed relative import\n\n* Revert changed version of PyICU\n\n* Fixed comments\n\nCo-authored-by: Oliver Kinch <oliver@alex5925.local>"
    },
    {
        "repo_url": "github.com/alexandrainst/danlp",
        "filepath": "examples/benchmarks/dependency_benchmarks.py",
        "commit_date": "2021-04-20T09:31:13Z",
        "message": "Add Stanza benchmark (#128)\n\n* Add stanza benchmarking\n\n* Update benchmark requirements\n\n* Add documentation"
    },
    {
        "repo_url": "github.com/ServiceNow/duorat",
        "filepath": "duorat/utils/tokenization.py",
        "commit_date": "2020-10-23T01:48:17Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/explosion/projects",
        "filepath": "benchmarks/speed/scripts/download_models.py",
        "commit_date": "2020-10-15T11:09:29Z",
        "message": "Format"
    },
    {
        "repo_url": "github.com/explosion/projects",
        "filepath": "benchmarks/speed/scripts/download_models.py",
        "commit_date": "2020-09-28T19:24:51Z",
        "message": "add flair models"
    },
    {
        "repo_url": "github.com/explosion/projects",
        "filepath": "benchmarks/speed/scripts/download_models.py",
        "commit_date": "2020-09-28T15:36:03Z",
        "message": "run each lib in separate python call to avoid mem accumulation"
    },
    {
        "repo_url": "github.com/explosion/projects",
        "filepath": "benchmarks/speed/scripts/download_models.py",
        "commit_date": "2020-09-28T13:03:46Z",
        "message": "run stanza"
    },
    {
        "repo_url": "github.com/cltk/cltk",
        "filepath": "src/cltk/dependency/stanza_wrapper.py",
        "commit_date": "2024-02-06T01:48:11Z",
        "message": "Add typing (#1242)\n\n* misc typing fixes\n\n* more cleanup\n\n* more cleanup\n\n* rm mkdocs\n\n* formatting changes\n\n* add typing to corpora latin\n\n* add types to stanza wrapper\n\n* rm check of spacy vers from build\n\n* add more types\n\n* change all old Dict[] List[] Tuple[] types\n\n* change deepcopy to copy for 3.11 error\n\n* more type improvements\n\n* change all type annotations to hints\n\n* change all type annotations to hints\n\n* fix four errors after code analysis"
    },
    {
        "repo_url": "github.com/cltk/cltk",
        "filepath": "src/cltk/dependency/stanza_wrapper.py",
        "commit_date": "2023-12-27T07:55:02Z",
        "message": "Updates to Clemsciences's spacy process (#1239)\n\n* Added spaCy process\n\n* Improved spaCy to CLTK wrapper\n\n* Use correct Token attributes\n\n* Use spaCy download function instead of shell command\n\n* Update poetry.lock\n\n* Removed unrelated changes\n\n* Fixed SpacyWrapper, StanzaWrapper and download_all_models.py\n\n* Improved SpacyWrapper\n\n* update dependencies\n\n* demo notebook not working\n\n* Added spaCy process\n\n* Improved spaCy to CLTK wrapper\n\n* Use correct Token attributes\n\n* Use spaCy download function instead of shell command\n\n* Removed unrelated changes\n\n* Fixed SpacyWrapper, StanzaWrapper and download_all_models.py\n\n* Improved SpacyWrapper\n\n* Improved SpacyWrapper\n\n* Renamed spacy_dep.py to spacy_wrapper.py and lint fixes\n\n* add morphology from spacy to cltk Doc\n\n* downgrade spacy to 3.6.1\n\n* Download spaCy model if the model is absent\n\n* update dev dependencies, improve spacy wrapper\n\n* spacy wrapper working\n\n* fix .get_dependencies()\n\n* make e2e work with LatinCy\n\n* re-add Latin Stops Process\n\n* add spacy dl to build script\n\n* trigger CI rerun\n\n* load spacy directly\n\n* dl spacy model with subprocess\n\n* load model after dl\n\n* load w/ spacy (wrapper seems to err\n\n* don't check only dl\n\n* bump spacy\n\n* why old spacy on ci?\n\n* deepcopy to copy\n\n* basic lat tests pass\n\n* rewrite bad UD types from Mood to VerbForm\n\n* finish more cleanup or LatinCy release\n\n* add citation printing\n\n* rename latincy proc\n\n---------\n\nCo-authored-by: Cl\u00e9ment Besnier <clemsciences@aol.com>\nCo-authored-by: Cl\u00e9ment Besnier <clem@clementbesnier.fr>"
    },
    {
        "repo_url": "github.com/cltk/cltk",
        "filepath": "src/cltk/dependency/stanza_wrapper.py",
        "commit_date": "2023-11-04T10:15:39Z",
        "message": "Fixed type in stanza_wrapper.py that prevented using stanza"
    },
    {
        "repo_url": "github.com/cltk/cltk",
        "filepath": "src/cltk/dependency/stanza_wrapper.py",
        "commit_date": "2023-11-04T10:15:39Z",
        "message": "Version 1.1.6a1"
    },
    {
        "repo_url": "github.com/cltk/cltk",
        "filepath": "src/cltk/dependency/stanza_wrapper.py",
        "commit_date": "2023-11-04T10:15:39Z",
        "message": "Make path cross-platform"
    },
    {
        "repo_url": "github.com/cltk/cltk",
        "filepath": "src/cltk/dependency/stanza_wrapper.py",
        "commit_date": "2023-11-04T10:15:39Z",
        "message": "Improved annotations and let less tasks to dataclasses"
    },
    {
        "repo_url": "github.com/rali-udem/jsRealB",
        "filepath": "demos/UDregenerator/text2ud.py",
        "commit_date": "2022-10-29T20:17:57Z",
        "message": "text2ud: show progress during Stanza parsing"
    },
    {
        "repo_url": "github.com/rali-udem/jsRealB",
        "filepath": "demos/UDregenerator/text2ud.py",
        "commit_date": "2022-09-12T18:11:25Z",
        "message": "Version 4.5 - Code reorganization\n\nNo new features but the implementation now use \"real\" JavaScript classes and modules.\n- building relies on webpack which integrates JSON lexicons in the source. This simplifies greatly the integration process that previously used a series of scripts with a Makefile\n- demos have been ported to this organization\n- documentation has been updated\n- source files now have jsdoc comments\n- src/jsdoc is now available"
    },
    {
        "repo_url": "github.com/rali-udem/jsRealB",
        "filepath": "demos/UDregenerator/text2ud.py",
        "commit_date": "2021-05-19T01:02:10Z",
        "message": "Add variationsFromText to UDgenerator"
    },
    {
        "repo_url": "github.com/rali-udem/jsRealB",
        "filepath": "demos/UDregenerator/text2ud-new.py",
        "commit_date": "2023-05-17T20:58:21Z",
        "message": "add showTree for variationsFromText"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/word_generator.py",
        "commit_date": "2020-05-17T04:13:56Z",
        "message": "UD in word service, bug fixes"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/word_generator.py",
        "commit_date": "2020-05-12T09:00:07Z",
        "message": "Add word already exists warning"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/word_generator.py",
        "commit_date": "2020-05-09T07:52:13Z",
        "message": "even better hail maries"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/word_generator.py",
        "commit_date": "2020-05-09T07:36:44Z",
        "message": "Bug fixzzz"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/word_generator.py",
        "commit_date": "2020-05-09T07:25:23Z",
        "message": "Word generation: add a hail mary when failing, only do one pass"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/word_generator.py",
        "commit_date": "2020-05-08T23:29:03Z",
        "message": "Webpack, website that queries backend"
    },
    {
        "repo_url": "github.com/turtlesoupy/this-word-does-not-exist",
        "filepath": "title_maker_pro/word_generator.py",
        "commit_date": "2020-05-07T20:48:32Z",
        "message": "Wire up basic word service, move word generator interface to own file"
    },
    {
        "repo_url": "github.com/nipunsadvilkar/pySBD",
        "filepath": "benchmarks/genia_benchmark.py",
        "commit_date": "2020-07-19T17:25:34Z",
        "message": "\u26a1\ufe0f  Benchmarks scripts - genia & bigtext file"
    },
    {
        "repo_url": "github.com/nipunsadvilkar/pySBD",
        "filepath": "benchmarks/genia_benchmark.py",
        "commit_date": "2020-07-12T17:58:12Z",
        "message": "use format, delete other scripts"
    },
    {
        "repo_url": "github.com/nipunsadvilkar/pySBD",
        "filepath": "benchmarks/genia_benchmark.py",
        "commit_date": "2020-07-11T21:59:57Z",
        "message": "my benchmarks"
    },
    {
        "repo_url": "github.com/nipunsadvilkar/pySBD",
        "filepath": "benchmarks/benchmark_sbd_tools.py",
        "commit_date": "2020-07-19T17:25:34Z",
        "message": "\u26a1\ufe0f  Benchmarks scripts - genia & bigtext file"
    },
    {
        "repo_url": "github.com/nipunsadvilkar/pySBD",
        "filepath": "benchmarks/bigtext_speed_benchmark.py",
        "commit_date": "2020-07-19T17:25:34Z",
        "message": "\u26a1\ufe0f  Benchmarks scripts - genia & bigtext file"
    },
    {
        "repo_url": "github.com/explosion/spacy-stanza",
        "filepath": "tests/test_language.py",
        "commit_date": "2022-05-27T14:12:39Z",
        "message": "Extend to stanza v1.4"
    },
    {
        "repo_url": "github.com/explosion/spacy-stanza",
        "filepath": "tests/test_language.py",
        "commit_date": "2021-11-09T09:18:13Z",
        "message": "Fallback to upos instead of to feats (#77)\n\n* fallback to upos instead of to feats\n\n* Use test data from UD train set\n\n* more future-proof test for Spanish\n\n* delete comment\n\n* make black"
    },
    {
        "repo_url": "github.com/explosion/spacy-stanza",
        "filepath": "tests/test_language.py",
        "commit_date": "2021-11-09T08:50:11Z",
        "message": "Updates for stanza v1.3.0, set version to v1.0.1 (#79)\n\n* Extend to stanza v1.3.x\n\n* Set version to v1.0.1\n\n* Update tests for more recent models"
    },
    {
        "repo_url": "github.com/explosion/spacy-stanza",
        "filepath": "tests/test_language.py",
        "commit_date": "2021-03-02T13:10:15Z",
        "message": "Fix trailing whitespace token handling (#64)"
    },
    {
        "repo_url": "github.com/explosion/spacy-stanza",
        "filepath": "tests/test_language.py",
        "commit_date": "2021-03-02T09:01:16Z",
        "message": "Refactor for spaCy v3.0 (#58)\n\n* Refactor for spaCy v3.0\n\n* Implement the stanza processing as a custom registered tokenizer\n`spacy_stanza.PipelineAsTokenizer.v1`\n  * Include all `stanza.Pipeline` settings in the tokenizer config block\n  * Move the custom user hooks into the tokenizer processing\n\n* Add the helper method `spacy_stanza.blank()` for pipeline\ninitialization, which sets up the custom tokenizer in the config and\npasses the config on to `spacy.blank()`\n\n* Update README\n\n* Update README [ci skip]\n\n* Simplify config setup\n\n* Clean up and reformat\n\n* Use plain pip install in README\n\n* Fix dir Pipeline arg\n\n* Add test that loads directly from a minimal config\n\n* Support kwargs directly in pipeline init method\n\nSupport passing kwargs directly from pipeline init method rather than\nrequiring them to be set in `[nlp.tokenizer.kwargs]`.\n\n* Follow Pipeline API instead of spacy.blank API\n\nRename `spacy_stanza.blank` to `spacy_stanza.load_pipeline`.\n\n* Set version to 1.0.0 and update contact in setup"
    },
    {
        "repo_url": "github.com/explosion/spacy-stanza",
        "filepath": "tests/test_language.py",
        "commit_date": "2021-02-03T15:50:16Z",
        "message": "Update for spacy v3 (#55)\n\n* Update for spacy v3.0.0\n\n* Update requirements in requirements.txt\n\n* Sync requirements.txt and setup.py\n\n* Update for spacy v3.0 and stanza v1.2"
    },
    {
        "repo_url": "github.com/explosion/spacy-stanza",
        "filepath": "tests/test_language.py",
        "commit_date": "2020-09-02T11:10:11Z",
        "message": "Fix alignment for spacy tokenizer (#44)\n\nFix the alignment when spacy is used as the tokenizer and may return\nwhitespace tokens."
    },
    {
        "repo_url": "github.com/explosion/spacy-stanza",
        "filepath": "tests/test_language.py",
        "commit_date": "2020-06-26T08:38:04Z",
        "message": "Rewrite alignment to preserve whitespace tokens (#41)\n\nRewrite the alignment algorithm to create the words and spaces using a\ncopy of `spacy.util.get_words_and_spaces` and align the stanza\nannotation to the `Doc`, adjusting the positions and offsets around the\nadditional whitespace tokens."
    },
    {
        "repo_url": "github.com/explosion/spacy-stanza",
        "filepath": "tests/test_language.py",
        "commit_date": "2020-03-19T09:27:43Z",
        "message": "Show warning if entities don't map to tokens"
    },
    {
        "repo_url": "github.com/explosion/spacy-stanza",
        "filepath": "tests/test_language.py",
        "commit_date": "2020-03-17T18:15:17Z",
        "message": "StanfordNLP -> Stanza (#26)\n\n* stanfordnlp -> stanza\n\n* Create azure-pipelines.yml\n\n* Update README.md\n\n* Try to fix PyTorch installation for windows\n\n* Don't test on Windows for now\n\n* Update README.md\n\n* Fix typo [ci skip]"
    },
    {
        "repo_url": "github.com/explosion/spacy-stanza",
        "filepath": "tests/test_language.py",
        "commit_date": "2019-05-31T09:56:50Z",
        "message": "Relax version requirement for stanfordnlp to include v0.2.0. (#15)\n\n* For compatibility with native Spacy language classes allow passing of empty text strings. This will produce 0-length docs, rather than raising an exception.\n\n* Increment minor version.\n\n* Relax version requirement for stanfordnlp to include new 0.2.0.\n\n* Make tests pass with stanfordnlp 0.2.0. Some POS tag predictions have flipped in ambiguous cases (e.g. DET/PRON, VERB/AUX)."
    },
    {
        "repo_url": "github.com/explosion/spacy-stanza",
        "filepath": "tests/test_language.py",
        "commit_date": "2019-01-31T14:24:22Z",
        "message": "Update test_language.py"
    },
    {
        "repo_url": "github.com/explosion/spacy-stanza",
        "filepath": "tests/test_language.py",
        "commit_date": "2019-01-31T14:19:04Z",
        "message": "Move tests"
    },
    {
        "repo_url": "github.com/marian-nmt/marian-examples",
        "filepath": "forced-translation/scripts/lemmatize.py",
        "commit_date": "2022-02-23T19:00:33Z",
        "message": "Forced translation\n\n* add forced-translation examples\n* Update .gitignore\n* Update README.md\n* remove glossary tokenization\n* add eval scripts\n* Update EXPERIMENTS.md\n\n    Added information regarding the two new testsets used for en-ro and\n    en-nb, that contained a specific domained, annotated with a glossary\n    with terms only specific to that domain to mimic better the Tilde's\n    ATS testset and glossary, used for en-lv and en-de. Also added the\n    human evaluation results for this two LPs.\n\n* Add time estimations to run end-2-end pipeline\n\nCo-authored-by: Pedro Coelho <pedro.coelho@unbabel.com>\nCo-authored-by: Toms Bergmanis <tomsbergmanis@gmail.com>"
    },
    {
        "repo_url": "github.com/marian-nmt/marian-examples",
        "filepath": "forced-translation/scripts/eval_lemmatized_glossary.py",
        "commit_date": "2022-02-23T19:00:33Z",
        "message": "Forced translation\n\n* add forced-translation examples\n* Update .gitignore\n* Update README.md\n* remove glossary tokenization\n* add eval scripts\n* Update EXPERIMENTS.md\n\n    Added information regarding the two new testsets used for en-ro and\n    en-nb, that contained a specific domained, annotated with a glossary\n    with terms only specific to that domain to mimic better the Tilde's\n    ATS testset and glossary, used for en-lv and en-de. Also added the\n    human evaluation results for this two LPs.\n\n* Add time estimations to run end-2-end pipeline\n\nCo-authored-by: Pedro Coelho <pedro.coelho@unbabel.com>\nCo-authored-by: Toms Bergmanis <tomsbergmanis@gmail.com>"
    },
    {
        "repo_url": "github.com/marian-nmt/marian-examples",
        "filepath": "forced-translation/scripts/add_target_lemma_annotations.py",
        "commit_date": "2022-02-23T19:00:33Z",
        "message": "Forced translation\n\n* add forced-translation examples\n* Update .gitignore\n* Update README.md\n* remove glossary tokenization\n* add eval scripts\n* Update EXPERIMENTS.md\n\n    Added information regarding the two new testsets used for en-ro and\n    en-nb, that contained a specific domained, annotated with a glossary\n    with terms only specific to that domain to mimic better the Tilde's\n    ATS testset and glossary, used for en-lv and en-de. Also added the\n    human evaluation results for this two LPs.\n\n* Add time estimations to run end-2-end pipeline\n\nCo-authored-by: Pedro Coelho <pedro.coelho@unbabel.com>\nCo-authored-by: Toms Bergmanis <tomsbergmanis@gmail.com>"
    },
    {
        "repo_url": "github.com/renke999/PerturbScore",
        "filepath": "TextAttack/textattack/shared/utils/install.py",
        "commit_date": "2023-10-10T15:39:04Z",
        "message": "Add TextAttack as part of the repository"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "utils/wl_downloader_ci.py",
        "commit_date": "2024-01-05T09:11:05Z",
        "message": "Dependencies: Remove Dostoevsky; Utils: Remove Dostoevsky's Russian sentiment analyzer"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "utils/wl_downloader_ci.py",
        "commit_date": "2024-01-05T07:57:03Z",
        "message": "Misc: Update copyright year"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "utils/wl_downloader_ci.py",
        "commit_date": "2024-01-02T12:47:42Z",
        "message": "Dependencies: Add VADER; Utils: Add VADER's sentiment analyzers"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "utils/wl_downloader_ci.py",
        "commit_date": "2023-09-03T18:08:00Z",
        "message": "Dependencies: Add Stanza; Utils: Add Stanza's sentence tokenizers, word tokenizers, part-of-speech taggers, lemmatizers, and dependency parsers"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "utils/wl_downloader_ci.py",
        "commit_date": "2023-08-11T16:36:09Z",
        "message": "Dependencies: Add Dostoevsky; Settings: Add Settings - Sentiment Analysis; Utils: Add Dostoevsky's Russian sentiment analyzer"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "utils/wl_downloader_ci.py",
        "commit_date": "2023-08-08T13:51:22Z",
        "message": "Utils: Update spaCy's sentence recognizers, word tokenizers, part-of-speech taggers, lemmatizers, and dependency parsers"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "utils/wl_downloader_ci.py",
        "commit_date": "2023-07-23T06:44:18Z",
        "message": "Utils: Add spaCy's Slovenian sentence recognizer, part-of-speech tagger, lemmatizer, and dependency parser"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "utils/wl_downloader_ci.py",
        "commit_date": "2023-07-22T17:31:56Z",
        "message": "Utils: Add spaCy's Korean sentence recognizer, word tokenizer, part-of-speech tagger, lemmatizer, and dependency parser"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "utils/wl_downloader_ci.py",
        "commit_date": "2023-01-19T16:15:38Z",
        "message": "Misc: Update copyright year"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "utils/wl_downloader_ci.py",
        "commit_date": "2022-09-20T19:50:34Z",
        "message": "Utils: Add spaCy's Ukrainian part-of-speech tagger and lemmatizer"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "utils/wl_downloader_ci.py",
        "commit_date": "2022-09-04T13:36:51Z",
        "message": "Utils: Add spaCy's Finnish part-of-speech tagger and lemmatizer"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "utils/wl_downloader_ci.py",
        "commit_date": "2022-09-03T18:31:20Z",
        "message": "Utils: Add spaCy's Croatian and Swedish part-of-speech taggers"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "utils/wl_downloader_ci.py",
        "commit_date": "2022-08-25T16:55:28Z",
        "message": "Utils: Add NLTK's legality syllable tokenizer and sonority sequencing syllable tokenizer"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "utils/wl_downloader_ci.py",
        "commit_date": "2022-08-10T19:50:25Z",
        "message": "Tests: Relocate test files"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "utils/wl_downloader_ci.py",
        "commit_date": "2022-03-07T13:25:25Z",
        "message": "Misc: Add Chinese (Simplified) translation"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "utils/wl_downloader_ci.py",
        "commit_date": "2022-03-02T09:10:56Z",
        "message": "Tests: Add CI - LGTM"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "utils/wl_downloader_ci.py",
        "commit_date": "2022-01-10T12:39:40Z",
        "message": "Misc: Update copyright information"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "utils/wl_downloader_ci.py",
        "commit_date": "2022-01-07T16:43:52Z",
        "message": "Utils: Add spaCy's Japanese word tokenizer, POS tagger, and lemmatizer"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "utils/wl_downloader_ci.py",
        "commit_date": "2022-01-04T01:02:01Z",
        "message": "Tests: Update CI - script for downloading data"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "utils/wl_downloader_ci.py",
        "commit_date": "2022-01-01T08:59:01Z",
        "message": "Misc: Update copyright year"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "utils/wl_downloader_ci.py",
        "commit_date": "2021-10-13T09:07:05Z",
        "message": "Tests: Update CI"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2024-01-13T02:59:55Z",
        "message": "Settings: Settings - Part-of-speeach Tagging - Tagsets - Mapping Settings - Allow editing of tagset mapping of Stanza's Armenian (Eastern), Armenian (Western), Basque, Buryat (Russia), Danish, French, Greek (Modern), Hebrew (Modern), Hungarian, Ligurian, Manx, Marathi, Nigerian Pidgin, Pomak, Portuguese, Russian, Sanskrit, Sindhi, Sorbian (Upper), and Telugu part-of-speech taggers"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2024-01-05T07:57:03Z",
        "message": "Misc: Update copyright year"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2024-01-02T12:47:42Z",
        "message": "Dependencies: Add VADER; Utils: Add VADER's sentiment analyzers"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-12-30T10:49:39Z",
        "message": "Dependencies: Upgrade Stanza to 1.7.0; Utils: Add Stanza's Sindhi part-of-speech tagger"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-10-02T14:14:35Z",
        "message": "Dependencies: Upgrade spaCy to 3.7.2; Utils: Fix downloading of Stanza models"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-09-27T10:45:09Z",
        "message": "Dependencies: Upgrade PyInstaller to 6.0"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-09-23T11:43:01Z",
        "message": "Utils: Update spaCy's sentencizer"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-09-10T09:30:26Z",
        "message": "Utils: Add Stanza's French (Old) lemmatizer"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-09-10T08:49:05Z",
        "message": "Utils: Remove Stanza's Swedish Sign Language sentence tokenizer, word tokenizer, part-of-speech tagger, and dependency parser"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-09-10T08:43:01Z",
        "message": "Dependencies: Upgrade Stanza to 1.5.1; Utils: Add Stanza's Hebrew (Ancient), Kyrgyz, Manx, and Pomak sentence tokenizers / word tokenizers / part-of-speech taggers / lemmatizers / dependency parsers"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-09-04T13:45:52Z",
        "message": "Utils: Add Stanza's sentiment analyzers"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-09-03T18:08:00Z",
        "message": "Dependencies: Add Stanza; Utils: Add Stanza's sentence tokenizers, word tokenizers, part-of-speech taggers, lemmatizers, and dependency parsers"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-08-17T17:46:36Z",
        "message": "Misc: Update translations"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-08-11T16:36:09Z",
        "message": "Dependencies: Add Dostoevsky; Settings: Add Settings - Sentiment Analysis; Utils: Add Dostoevsky's Russian sentiment analyzer"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-08-08T13:51:22Z",
        "message": "Utils: Update spaCy's sentence recognizers, word tokenizers, part-of-speech taggers, lemmatizers, and dependency parsers"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-07-23T06:44:18Z",
        "message": "Utils: Add spaCy's Slovenian sentence recognizer, part-of-speech tagger, lemmatizer, and dependency parser"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-07-22T17:31:56Z",
        "message": "Utils: Add spaCy's Korean sentence recognizer, word tokenizer, part-of-speech tagger, lemmatizer, and dependency parser"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-03-13T19:09:27Z",
        "message": "Dependencies: Add python-mecab-ko; Utils: Add python-mecab-ko's MeCab"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-02-10T13:24:56Z",
        "message": "Releases: Update packaging script"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-02-08T15:13:01Z",
        "message": "Dependencies: 1. Add pymorphy3 2. Remove pymorphy2"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-02-02T06:42:01Z",
        "message": "Settings: Update global settings - measures"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-01-30T16:05:14Z",
        "message": "Utils: Speed up n-gram/skip-gram generation"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2023-01-19T16:15:38Z",
        "message": "Misc: Update copyright year"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-10-30T15:50:33Z",
        "message": "Work Area: Add Dependency Parser"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-10-09T20:25:53Z",
        "message": "Utils: Update spaCy's sentence tokenizers, word tokenizers, part-of-speech taggers, and lemmatizers"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-10-06T19:35:05Z",
        "message": "Utils: Update spaCy's sentence recognizers"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-09-23T19:51:17Z",
        "message": "Releases: Update packaging script"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-09-20T19:50:34Z",
        "message": "Utils: Add spaCy's Ukrainian part-of-speech tagger and lemmatizer"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-09-04T13:36:51Z",
        "message": "Utils: Add spaCy's Finnish part-of-speech tagger and lemmatizer"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-09-03T18:31:20Z",
        "message": "Utils: Add spaCy's Croatian and Swedish part-of-speech taggers"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-09-03T14:08:35Z",
        "message": "Utils: 1. Add spaCy's Sorbian (Lower) word tokenizer and stop word list 2. Add spaCy's Sorbian (Upper) word tokenizer and stop word list"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-09-02T18:45:32Z",
        "message": "Utils: Add Pyphen's Catalan syllable tokenizer"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-08-25T16:55:28Z",
        "message": "Utils: Add NLTK's legality syllable tokenizer and sonority sequencing syllable tokenizer"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-08-25T09:29:24Z",
        "message": "Tests: Add CI - SonarCloud"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-08-23T08:58:22Z",
        "message": "Utils: Add NLTK's regular-expression tokenizer"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-08-22T10:35:55Z",
        "message": "Dependencies: 1. Add spacy-pkuseg 2. Remove pkuseg"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-08-21T14:09:14Z",
        "message": "Work Area: Add Profiler - Count of Sentence Segments / Paragraph Length in Sentence Segments / Sentence Segment Length in Tokens / Count of n-length Sentence Segments"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-08-11T20:10:54Z",
        "message": "Doc: Add README - Pylint badge"
    },
    {
        "repo_url": "github.com/BLKSerene/Wordless",
        "filepath": "wordless/wl_nlp/wl_nlp_utils.py",
        "commit_date": "2022-08-10T19:50:25Z",
        "message": "Tests: Relocate test files"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2023-01-17T10:00:30Z",
        "message": "fix pretokenized input format\n\nNow, a list of tokens is not valid input anymore"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2022-04-04T12:14:58Z",
        "message": "make style"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2022-04-04T12:11:49Z",
        "message": "add create_spacy_disable_sentence_segmentation as entrypoint"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2022-04-04T12:03:37Z",
        "message": "move merge_dicts_strict to utils"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2022-03-05T19:57:36Z",
        "message": "Allow disable_sbd for stanza"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2022-03-05T16:26:10Z",
        "message": "Allow users to disable spacy components and fix disable_sbd issue\n\nFixes #17"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2021-09-05T08:52:39Z",
        "message": "ignore import quality checks"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2021-06-29T13:48:38Z",
        "message": "avoid duplicate arguments: pop"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2021-06-29T13:11:02Z",
        "message": "make style"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2021-06-29T11:23:52Z",
        "message": "Update utils.py"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2021-06-29T10:04:58Z",
        "message": "remove default spacy parser"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2021-06-25T13:58:50Z",
        "message": "move parsing code to dataclass"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2021-06-23T16:04:23Z",
        "message": "auto-download udpipe models"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2021-06-23T15:41:12Z",
        "message": "update to spacy v3 pipeline"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2021-06-23T09:29:25Z",
        "message": "make style"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2020-05-11T16:47:25Z",
        "message": "make style && make quality"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2020-05-11T16:44:28Z",
        "message": "add ability to use is_tokenized for udpipe"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2020-05-05T16:48:44Z",
        "message": "docstring & typing"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2020-05-05T16:36:09Z",
        "message": "replace tokens_from_list by custom tokenizer\n\nSince tokens_from_list is deprecated, switch to a custom tokenizer when `is_tokenized` is requested. Input can be a string to be split on whitespace or a list of tokens. Works only for spaCy. Inspired by adrianeboyd https://github.com/explosion/spaCy/issues/5399#issuecomment-623593208"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2020-05-03T15:39:12Z",
        "message": "style and quality"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2020-05-03T15:15:14Z",
        "message": "rename pipeline_opts to parser_opts"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2020-05-01T16:51:59Z",
        "message": "allow pipeline_opts and kwargs in init_parser"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2020-05-01T13:22:57Z",
        "message": "require keywords for less important kwargs"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2020-04-30T12:26:27Z",
        "message": "add kwargs"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2020-04-30T11:12:16Z",
        "message": "re-order arguments in init_nlp"
    },
    {
        "repo_url": "github.com/BramVanroy/spacy_conll",
        "filepath": "spacy_conll/utils.py",
        "commit_date": "2020-04-29T13:13:32Z",
        "message": "restructure and improve entrypoint\n\n- move ConllFormatter to separate file\n- add utils and expose init_parser (might be useful for users)\n- improve entrypoint usage"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/misc.py",
        "commit_date": "2023-11-20T14:22:05Z",
        "message": "importlib_resources for Python<3.9"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/misc.py",
        "commit_date": "2023-11-20T04:20:39Z",
        "message": "clean up tests; fix phonetic()"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/misc.py",
        "commit_date": "2021-04-22T22:32:47Z",
        "message": "debug action"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/misc.py",
        "commit_date": "2021-04-22T03:49:50Z",
        "message": "use gzip to avoid git-lfs"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/misc.py",
        "commit_date": "2021-04-13T16:37:25Z",
        "message": "for reals"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/misc.py",
        "commit_date": "2021-04-13T09:07:43Z",
        "message": "modern structure/tools (src/ and setup.cfg/pyproject.toml)"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/sentence.py",
        "commit_date": "2023-11-20T04:20:39Z",
        "message": "clean up tests; fix phonetic()"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/sentence.py",
        "commit_date": "2023-08-04T04:34:48Z",
        "message": "to_html()"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/sentence.py",
        "commit_date": "2022-09-18T01:16:06Z",
        "message": "don\u2019t attempt to re-download punkt tokenizer (#50)\n\n* don\u2019t attempt to re-download punkt tokenizer\n\nPreviously an attempt was made to redownload `punkt` with each request for the in-use tokenizer, causing nltk to emit warnings about `punkt` already being downloaded.  This change attempts to find `punkt` and if not present to _then_ download it.\n\n* Update sentence.py\n\nCo-authored-by: Rob Reynolds <reynoldsnlp@users.noreply.github.com>"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/sentence.py",
        "commit_date": "2021-08-20T22:33:51Z",
        "message": "code formatting"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/sentence.py",
        "commit_date": "2021-06-08T16:38:28Z",
        "message": "force utf8 for cg3"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/sentence.py",
        "commit_date": "2021-05-26T15:36:16Z",
        "message": "to_json()"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/sentence.py",
        "commit_date": "2021-04-22T20:51:04Z",
        "message": "fix pmhfst path"
    },
    {
        "repo_url": "github.com/reynoldsnlp/udar",
        "filepath": "src/udar/sentence.py",
        "commit_date": "2021-04-13T09:07:43Z",
        "message": "modern structure/tools (src/ and setup.cfg/pyproject.toml)"
    },
    {
        "repo_url": "github.com/jackfromeast/CyberCoref",
        "filepath": "dataLoader.py",
        "commit_date": "2022-06-20T03:11:55Z",
        "message": "for gold types"
    },
    {
        "repo_url": "github.com/jackfromeast/CyberCoref",
        "filepath": "dataLoader.py",
        "commit_date": "2022-06-20T01:35:33Z",
        "message": "update 0620"
    },
    {
        "repo_url": "github.com/openredact/nerwhal",
        "filepath": "nerwhal/nlp_utils.py",
        "commit_date": "2020-08-18T15:39:02Z",
        "message": "Add and improve PyDocs"
    },
    {
        "repo_url": "github.com/openredact/nerwhal",
        "filepath": "nerwhal/nlp_utils.py",
        "commit_date": "2020-08-17T14:22:47Z",
        "message": "Fix caught error"
    },
    {
        "repo_url": "github.com/openredact/nerwhal",
        "filepath": "nerwhal/nlp_utils.py",
        "commit_date": "2020-07-31T06:37:48Z",
        "message": "Catch Exception instead of FileNotFoundError"
    },
    {
        "repo_url": "github.com/openredact/nerwhal",
        "filepath": "nerwhal/nlp_utils.py",
        "commit_date": "2020-07-31T05:30:00Z",
        "message": "Clean tokenizer and nlp interfaces"
    },
    {
        "repo_url": "github.com/openredact/nerwhal",
        "filepath": "nerwhal/nlp_utils.py",
        "commit_date": "2020-07-30T17:05:26Z",
        "message": "Use context words to boost score"
    },
    {
        "repo_url": "github.com/openredact/nerwhal",
        "filepath": "nerwhal/nlp_utils.py",
        "commit_date": "2020-07-29T13:58:20Z",
        "message": "Use spaCy tokenizer because stanza's mwts alter the original text"
    },
    {
        "repo_url": "github.com/openredact/nerwhal",
        "filepath": "nerwhal/nlp_utils.py",
        "commit_date": "2020-07-28T15:37:51Z",
        "message": "Refactor and fix bugs WIP"
    },
    {
        "repo_url": "github.com/zzsfornlp/zmsp",
        "filepath": "mspx/tools/annotate/ann_stanza.py",
        "commit_date": "2023-06-20T01:13:08Z",
        "message": "Add some more."
    },
    {
        "repo_url": "github.com/zzsfornlp/zmsp",
        "filepath": "msp2/tools/annotate/ann_stanza.py",
        "commit_date": "2022-10-12T18:51:22Z",
        "message": "Updates"
    },
    {
        "repo_url": "github.com/zzsfornlp/zmsp",
        "filepath": "msp2/tools/annotate/ann_stanza.py",
        "commit_date": "2021-06-06T20:35:18Z",
        "message": "Update for srl_span with msp2."
    },
    {
        "repo_url": "github.com/zzsfornlp/zmsp",
        "filepath": "msp2/scripts/event/prep/s2_tokenize.py",
        "commit_date": "2022-10-12T18:51:22Z",
        "message": "Updates"
    },
    {
        "repo_url": "github.com/utcsnlp/lfqa_discourse",
        "filepath": "role_classifier/generate_t5_input_csv_file.py",
        "commit_date": "2022-03-21T02:21:04Z",
        "message": "add code and data"
    },
    {
        "repo_url": "github.com/utcsnlp/lfqa_discourse",
        "filepath": "role_classifier/generate_t5_input_json_file.py",
        "commit_date": "2023-10-13T14:29:31Z",
        "message": "update role prediction script"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/models.py",
        "commit_date": "2023-10-30T14:35:26Z",
        "message": "Use lazy formatting in some logging"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/models.py",
        "commit_date": "2023-02-20T13:00:32Z",
        "message": "Move ISO 639 functions to util package"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/models.py",
        "commit_date": "2023-02-20T12:59:59Z",
        "message": "refactor: use LangaugeRegistry"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/models.py",
        "commit_date": "2021-06-01T15:52:07Z",
        "message": "Move util package to api"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/models.py",
        "commit_date": "2021-05-17T15:12:30Z",
        "message": "Move everything needed by Sparv modules to sparv.api package"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/models.py",
        "commit_date": "2021-04-29T13:46:31Z",
        "message": "add Stanza support for English"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/models.py",
        "commit_date": "2020-12-10T13:24:48Z",
        "message": "download Stanza models and embeddings separately; download specific SVN version (fixes #89)"
    },
    {
        "repo_url": "github.com/spraakbanken/sparv-pipeline",
        "filepath": "sparv/modules/stanza/models.py",
        "commit_date": "2020-11-25T15:44:54Z",
        "message": "add lemmatiser to stanza, re-organise module"
    },
    {
        "repo_url": "github.com/thunlp/OpenBackdoor",
        "filepath": "openbackdoor/attackers/poisoners/trojanlm_poisoner.py",
        "commit_date": "2022-08-14T00:04:23Z",
        "message": "dmodify ripple_config and sos_poisoner"
    },
    {
        "repo_url": "github.com/thunlp/OpenBackdoor",
        "filepath": "openbackdoor/attackers/poisoners/trojanlm_poisoner.py",
        "commit_date": "2022-08-10T12:27:29Z",
        "message": "doc"
    },
    {
        "repo_url": "github.com/thunlp/OpenBackdoor",
        "filepath": "openbackdoor/attackers/poisoners/trojanlm_poisoner.py",
        "commit_date": "2022-08-10T12:17:34Z",
        "message": "update doc"
    },
    {
        "repo_url": "github.com/thunlp/OpenBackdoor",
        "filepath": "openbackdoor/attackers/poisoners/trojanlm_poisoner.py",
        "commit_date": "2022-08-08T16:47:02Z",
        "message": "update poisoner"
    },
    {
        "repo_url": "github.com/thunlp/OpenBackdoor",
        "filepath": "openbackdoor/attackers/poisoners/trojanlm_poisoner.py",
        "commit_date": "2022-08-06T15:15:40Z",
        "message": "fix bugs"
    },
    {
        "repo_url": "github.com/thunlp/OpenBackdoor",
        "filepath": "openbackdoor/attackers/poisoners/trojanlm_poisoner.py",
        "commit_date": "2022-08-05T09:44:12Z",
        "message": "fix bugs"
    },
    {
        "repo_url": "github.com/thunlp/OpenBackdoor",
        "filepath": "openbackdoor/attackers/poisoners/trojanlm_poisoner.py",
        "commit_date": "2022-04-21T11:53:18Z",
        "message": "fix trojanlm, por, neuba"
    },
    {
        "repo_url": "github.com/thunlp/OpenBackdoor",
        "filepath": "openbackdoor/attackers/poisoners/trojanlm_poisoner.py",
        "commit_date": "2022-04-07T06:06:28Z",
        "message": "fix trojanlm"
    },
    {
        "repo_url": "github.com/thunlp/OpenBackdoor",
        "filepath": "openbackdoor/attackers/poisoners/trojanlm_poisoner.py",
        "commit_date": "2022-04-02T14:52:09Z",
        "message": "fix bugs"
    },
    {
        "repo_url": "github.com/thunlp/OpenBackdoor",
        "filepath": "openbackdoor/attackers/poisoners/trojanlm_poisoner.py",
        "commit_date": "2022-03-20T13:28:42Z",
        "message": "add neuba and trojanlm"
    },
    {
        "repo_url": "github.com/thunlp/OpenBackdoor",
        "filepath": "openbackdoor/attackers/poisoners/trojanlm_poisoner.py",
        "commit_date": "2022-01-09T11:52:26Z",
        "message": "update ep sos"
    },
    {
        "repo_url": "github.com/facebookresearch/minihack",
        "filepath": "minihack/wiki.py",
        "commit_date": "2021-09-27T23:37:57Z",
        "message": "Initial commit"
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/models/passage_qg/answer_sampler.py",
        "commit_date": "2022-10-06T12:41:49Z",
        "message": "Rework data loading and processing design\n\nThis change separates data loading from processing so that any dataset\nin the appropriate format can be used for generating questions and to\nallow for simple extension to other modalities.\n\n - There are now separate processors depending on the modality.\n\n - Also, the tests have been updated accordingly."
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/models/passage_qg/answer_sampler.py",
        "commit_date": "2022-07-29T17:56:41Z",
        "message": "include id for passage qg"
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/models/passage_qg/answer_sampler.py",
        "commit_date": "2022-07-06T15:07:45Z",
        "message": "Updated inference notebook for PassageQG"
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/models/passage_qg/answer_sampler.py",
        "commit_date": "2022-07-05T07:09:29Z",
        "message": "PassageQG can  take answers as input"
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/models/passage_qg/answer_sampler.py",
        "commit_date": "2022-07-01T11:40:25Z",
        "message": "Merge branch 'passageqg' into tableqg"
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/models/passage_qg/answer_sampler.py",
        "commit_date": "2022-07-01T08:52:39Z",
        "message": "Stanza added to setup.py"
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/models/passage_qg/answer_sampler.py",
        "commit_date": "2022-06-30T18:47:05Z",
        "message": "minor changes from code review suggestions-2"
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/models/passage_qg/answer_sampler.py",
        "commit_date": "2022-06-30T14:10:22Z",
        "message": "Answer sampler uses NERs  from Stanza: en, ru, ar, fi"
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/models/passage_qg/answer_sampler.py",
        "commit_date": "2022-06-29T10:44:31Z",
        "message": "Inference notebooks for TableQG and TyDi"
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/models/passage_qg/answer_sampler.py",
        "commit_date": "2022-06-26T21:41:12Z",
        "message": "PassageQG inference works. en_core_web_sm downloades with setup"
    },
    {
        "repo_url": "github.com/primeqa/primeqa",
        "filepath": "primeqa/qg/models/passage_qg/answer_sampler.py",
        "commit_date": "2022-06-26T20:51:20Z",
        "message": "inference for passage QG"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza-old",
        "filepath": "stanza/util/resource.py",
        "commit_date": "2016-04-02T07:14:58Z",
        "message": "Fix documentation for other modules."
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza-old",
        "filepath": "stanza/util/resource.py",
        "commit_date": "2016-01-14T22:08:17Z",
        "message": "adding glove vectors"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza-old",
        "filepath": "stanza/util/resource.py",
        "commit_date": "2015-11-04T08:02:16Z",
        "message": "use logging instead of print"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza-old",
        "filepath": "stanza/util/resource.py",
        "commit_date": "2015-10-28T06:06:30Z",
        "message": "minor fix: do not use reversed keyword 'dir' as variable name"
    },
    {
        "repo_url": "github.com/stanfordnlp/stanza-old",
        "filepath": "stanza/util/resource.py",
        "commit_date": "2015-10-28T05:49:37Z",
        "message": "add util for downloading files to local storage, senna embeddings for #2, refactored tests into fast unit tests and slow tests"
    },
    {
        "repo_url": "github.com/gregorycrane/Homerica",
        "filepath": "stanza-greek.py",
        "commit_date": "2021-09-28T15:16:18Z",
        "message": "base workflow"
    },
    {
        "repo_url": "github.com/quadrismegistus/cadence",
        "filepath": "cadence/parsers/stanzanlp.py",
        "commit_date": "2022-01-20T22:29:11Z",
        "message": "fixes and readme updated"
    },
    {
        "repo_url": "github.com/quadrismegistus/cadence",
        "filepath": "cadence/parsers/stanzanlp.py",
        "commit_date": "2022-01-15T18:05:11Z",
        "message": "working on web app"
    },
    {
        "repo_url": "github.com/quadrismegistus/cadence",
        "filepath": "cadence/parsers/stanzanlp.py",
        "commit_date": "2022-01-13T16:15:06Z",
        "message": "mtree fixes"
    },
    {
        "repo_url": "github.com/quadrismegistus/cadence",
        "filepath": "cadence/parsers/stanzanlp.py",
        "commit_date": "2022-01-13T14:52:21Z",
        "message": "fix"
    },
    {
        "repo_url": "github.com/quadrismegistus/cadence",
        "filepath": "cadence/parsers/stanzanlp.py",
        "commit_date": "2022-01-13T14:43:32Z",
        "message": "fix stanza bug"
    },
    {
        "repo_url": "github.com/quadrismegistus/cadence",
        "filepath": "cadence/parsers/stanzanlp.py",
        "commit_date": "2022-01-13T12:48:50Z",
        "message": "stanza edits"
    },
    {
        "repo_url": "github.com/quadrismegistus/cadence",
        "filepath": "cadence/parsers/stanzanlp.py",
        "commit_date": "2022-01-13T12:48:21Z",
        "message": "fix"
    },
    {
        "repo_url": "github.com/quadrismegistus/cadence",
        "filepath": "cadence/parsers/stanzanlp.py",
        "commit_date": "2022-01-11T03:48:52Z",
        "message": "major text obj improvements"
    },
    {
        "repo_url": "github.com/quadrismegistus/cadence",
        "filepath": "cadence/parsers/stanzanlp.py",
        "commit_date": "2022-01-10T21:08:22Z",
        "message": "updates"
    },
    {
        "repo_url": "github.com/quadrismegistus/cadence",
        "filepath": "cadence/parsers/stanzanlp.py",
        "commit_date": "2022-01-09T17:13:55Z",
        "message": "updates"
    },
    {
        "repo_url": "github.com/quadrismegistus/cadence",
        "filepath": "cadence/parsers/stanzanlp.py",
        "commit_date": "2022-01-08T22:42:37Z",
        "message": "updates"
    },
    {
        "repo_url": "github.com/zhjohnchan/PTUnifier",
        "filepath": "ptunifier/metrics/jb_scorers/RadEntityMatchExact/RadEntityMatchExact.py",
        "commit_date": "2023-08-10T06:54:38Z",
        "message": "init"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2022-04-30T07:16:44Z",
        "message": "WIP"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2022-01-22T21:11:59Z",
        "message": "Various improvements, clients and clients docs."
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2021-11-19T17:33:01Z",
        "message": "Updated pythondocs, clean up code"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2021-09-12T18:30:33Z",
        "message": "Fix #130"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2021-06-26T08:44:58Z",
        "message": "Better stanza token conversion"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2021-06-26T08:37:19Z",
        "message": "Working AnnStanza.pipe"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2021-06-26T07:31:08Z",
        "message": "Require newer Stanza, test for Stanza pipe"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2021-03-19T07:52:03Z",
        "message": "Fix some code style issues"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2021-03-17T08:04:55Z",
        "message": "Address #69"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2021-02-19T17:58:08Z",
        "message": "Add support for space tokens."
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2021-02-19T17:28:43Z",
        "message": "Proper MWTs."
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2021-02-19T16:55:34Z",
        "message": "Better squeezing of multiword tokens"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2021-02-16T16:28:25Z",
        "message": "Better error handling in the executor"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2020-11-22T08:33:27Z",
        "message": "black reformatting"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2020-11-18T19:19:53Z",
        "message": "WIP"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2020-11-13T21:06:43Z",
        "message": "Require min version for bs4.\nOtherwise the suppression of the parser warning does not work."
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2020-10-17T11:21:32Z",
        "message": "Use pdoc3 for documentation."
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2020-10-14T20:16:50Z",
        "message": "WIP"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2020-10-14T19:01:58Z",
        "message": "WIP"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2020-08-20T12:07:12Z",
        "message": "Fix stanza lib problem.\nOddly something seems to have changed in how stanza represents ids.\nApparently this was string before and is int now. Just to be safe,\nwe always convert to string for now."
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2020-08-19T13:26:37Z",
        "message": "A few more for the changed annset.add method."
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2020-08-19T09:35:27Z",
        "message": "More API changes."
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2020-08-18T17:50:26Z",
        "message": "Re-implementation of Features, updated documentation."
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "gatenlp/lib_stanza.py",
        "commit_date": "2020-04-29T17:16:07Z",
        "message": "Add support for Stanford Stanza"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2022-02-06T11:04:17Z",
        "message": "Fixes #158"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2021-06-26T08:37:19Z",
        "message": "Working AnnStanza.pipe"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2021-06-26T07:31:08Z",
        "message": "Require newer Stanza, test for Stanza pipe"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2021-04-14T11:33:20Z",
        "message": "Stanza unit test: do not use GPU"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2021-04-14T11:26:38Z",
        "message": "Improve unit tests.\nAdd basic test for GateWorker"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2021-04-10T07:21:43Z",
        "message": "Cleanup code."
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2021-03-19T07:52:03Z",
        "message": "Fix some code style issues"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2021-02-06T17:27:45Z",
        "message": "Add unit tests, fix bugs, regenerate PythonDoc."
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2021-01-30T13:14:19Z",
        "message": "And yet another!"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2021-01-30T13:11:41Z",
        "message": "And yet another"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2020-11-22T08:33:27Z",
        "message": "black reformatting"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2020-10-27T22:22:25Z",
        "message": "Avoid downloading the model if it is already there."
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2020-10-27T15:58:10Z",
        "message": "Try to make the test work on travis"
    },
    {
        "repo_url": "github.com/GateNLP/python-gatenlp",
        "filepath": "tests/test_stanza.py",
        "commit_date": "2020-10-27T14:01:19Z",
        "message": "Fix bug in setup, remove stanfordnlp.\nAdd instead a test for stanza"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "download_models.py",
        "commit_date": "2022-05-02T18:48:02Z",
        "message": "Update to spacy 3.3.0"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "download_models.py",
        "commit_date": "2022-04-18T18:02:49Z",
        "message": "Trankit large"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "download_models.py",
        "commit_date": "2022-04-03T08:51:32Z",
        "message": "move Stanza and Trankit to the models directory"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "download_models.py",
        "commit_date": "2022-04-02T16:28:21Z",
        "message": "UralicNLP, only lemmatization"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "download_models.py",
        "commit_date": "2022-04-02T13:33:07Z",
        "message": "trankit"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "download_models.py",
        "commit_date": "2022-04-02T06:39:14Z",
        "message": "Remove spacy_stanfordnlp, add Stanza"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "download_models.py",
        "commit_date": "2019-10-28T18:03:40Z",
        "message": "Move stanfordnlp resources under data"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "download_models.py",
        "commit_date": "2019-10-27T16:49:41Z",
        "message": "spacy-stanfordnlp"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "download_models.py",
        "commit_date": "2019-10-27T16:28:10Z",
        "message": "udpipe Finnish FTB model"
    },
    {
        "repo_url": "github.com/aajanki/finnish-pos-accuracy",
        "filepath": "download_models.py",
        "commit_date": "2019-10-27T13:19:19Z",
        "message": "udpipe evaluation"
    },
    {
        "repo_url": "github.com/EclipseCN/PhenoBERT",
        "filepath": "setup.py",
        "commit_date": "2021-06-09T13:22:42Z",
        "message": "fix installation"
    },
    {
        "repo_url": "github.com/EclipseCN/PhenoBERT",
        "filepath": "setup.py",
        "commit_date": "2020-12-01T06:59:56Z",
        "message": "ver1.0.5"
    },
    {
        "repo_url": "github.com/EclipseCN/PhenoBERT",
        "filepath": "setup.py",
        "commit_date": "2020-12-01T06:56:34Z",
        "message": "del"
    },
    {
        "repo_url": "github.com/EclipseCN/PhenoBERT",
        "filepath": "setup.py",
        "commit_date": "2020-11-30T10:25:49Z",
        "message": "ver1.0.0"
    },
    {
        "repo_url": "github.com/EclipseCN/PhenoBERT",
        "filepath": "setup.py",
        "commit_date": "2020-11-30T06:11:36Z",
        "message": "add pypi"
    },
    {
        "repo_url": "github.com/EclipseCN/PhenoBERT",
        "filepath": "setup.py",
        "commit_date": "2020-11-30T05:28:19Z",
        "message": "ver1.0.0"
    },
    {
        "repo_url": "github.com/EclipseCN/PhenoBERT",
        "filepath": "setup.py",
        "commit_date": "2020-11-30T05:24:13Z",
        "message": "ver1.0.0"
    },
    {
        "repo_url": "github.com/EclipseCN/PhenoBERT",
        "filepath": "setup.py",
        "commit_date": "2020-11-30T03:06:47Z",
        "message": "ver1.0.0"
    },
    {
        "repo_url": "github.com/text-processing/prose-rhythm-detector",
        "filepath": "app/statistic_utils/PoS_ngram.py",
        "commit_date": "2022-05-13T13:43:59Z",
        "message": "Update PoS_ngram.py"
    },
    {
        "repo_url": "github.com/text-processing/prose-rhythm-detector",
        "filepath": "app/statistic_utils/PoS_ngram.py",
        "commit_date": "2022-05-12T10:35:00Z",
        "message": "Add PoS count utils"
    },
    {
        "repo_url": "github.com/text-processing/prose-rhythm-detector",
        "filepath": "app/statistic_utils/PoS_count.py",
        "commit_date": "2022-05-13T13:43:45Z",
        "message": "Update PoS_count.py"
    },
    {
        "repo_url": "github.com/text-processing/prose-rhythm-detector",
        "filepath": "app/statistic_utils/PoS_count.py",
        "commit_date": "2022-05-12T10:35:00Z",
        "message": "Add PoS count utils"
    },
    {
        "repo_url": "github.com/text-processing/prose-rhythm-detector",
        "filepath": "app/statistic_utils/statistic_util.py",
        "commit_date": "2020-11-18T14:33:42Z",
        "message": "Add the new version of ProseRhythmDetector"
    },
    {
        "repo_url": "github.com/text-processing/prose-rhythm-detector",
        "filepath": "app/statistic_utils/statistic_util.py",
        "commit_date": "2020-06-05T13:52:49Z",
        "message": "Add classification"
    },
    {
        "repo_url": "github.com/text-processing/prose-rhythm-detector",
        "filepath": "app/statistic_utils/statistic_util.py",
        "commit_date": "2020-02-24T16:05:52Z",
        "message": "Add the license to all files"
    },
    {
        "repo_url": "github.com/luohongyin/RGX",
        "filepath": "rgx_doc.py",
        "commit_date": "2022-06-29T15:30:47Z",
        "message": "huggingface hub and demo"
    },
    {
        "repo_url": "github.com/luohongyin/RGX",
        "filepath": "rgx_doc.py",
        "commit_date": "2022-06-28T22:12:23Z",
        "message": "stanza download"
    },
    {
        "repo_url": "github.com/luohongyin/RGX",
        "filepath": "rgx_doc.py",
        "commit_date": "2022-06-27T23:16:10Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/pmchozas/termitup",
        "filepath": "modules_api/postprocess.py",
        "commit_date": "2021-07-04T16:09:25Z",
        "message": "fixed non-term pattern deletion + updated req"
    },
    {
        "repo_url": "github.com/pmchozas/termitup",
        "filepath": "modules_api/postprocess.py",
        "commit_date": "2021-03-04T16:31:00Z",
        "message": "added postproc"
    },
    {
        "repo_url": "github.com/pmchozas/termitup",
        "filepath": "modules_api/postprocess.py",
        "commit_date": "2021-03-04T16:30:17Z",
        "message": "Delete postprocess.py"
    },
    {
        "repo_url": "github.com/pmchozas/termitup",
        "filepath": "modules_api/postprocess.py",
        "commit_date": "2021-02-17T11:49:09Z",
        "message": "tag verb"
    },
    {
        "repo_url": "github.com/pmchozas/termitup",
        "filepath": "modules_api/postprocess.py",
        "commit_date": "2021-02-15T11:42:24Z",
        "message": "tags"
    },
    {
        "repo_url": "github.com/pmchozas/termitup",
        "filepath": "modules_api/postprocess.py",
        "commit_date": "2021-02-12T16:21:54Z",
        "message": "corenlp server updated"
    },
    {
        "repo_url": "github.com/pmchozas/termitup",
        "filepath": "modules_api/postprocess.py",
        "commit_date": "2021-02-09T11:42:19Z",
        "message": "corenlp server updated"
    },
    {
        "repo_url": "github.com/pmchozas/termitup",
        "filepath": "modules_api/postprocess.py",
        "commit_date": "2020-10-16T13:18:49Z",
        "message": "bug fixed"
    },
    {
        "repo_url": "github.com/pmchozas/termitup",
        "filepath": "modules_api/postprocess.py",
        "commit_date": "2020-10-15T17:59:25Z",
        "message": "changes in swagger methods"
    },
    {
        "repo_url": "github.com/pmchozas/termitup",
        "filepath": "modules_api/postprocess.py",
        "commit_date": "2020-10-14T17:56:23Z",
        "message": "many changes with small success"
    },
    {
        "repo_url": "github.com/pmchozas/termitup",
        "filepath": "modules_api/postprocess.py",
        "commit_date": "2020-09-14T14:26:13Z",
        "message": "API advances"
    },
    {
        "repo_url": "github.com/pmchozas/termitup",
        "filepath": "modules_api/delete_patterns_flair.py",
        "commit_date": "2021-07-04T16:09:25Z",
        "message": "fixed non-term pattern deletion + updated req"
    },
    {
        "repo_url": "github.com/lang-uk/ner-uk",
        "filepath": "scripts/eval_ner_models.py",
        "commit_date": "2021-08-02T19:52:43Z",
        "message": "fix: Cleanup of eval script\n\n- whitespaces\n- more docs\n- cleanup of debug output"
    },
    {
        "repo_url": "github.com/lang-uk/ner-uk",
        "filepath": "scripts/eval_ner_models.py",
        "commit_date": "2021-07-28T07:36:24Z",
        "message": "Minor cleanup and doc udpate"
    },
    {
        "repo_url": "github.com/lang-uk/ner-uk",
        "filepath": "scripts/eval_ner_models.py",
        "commit_date": "2021-07-28T07:29:40Z",
        "message": "feat: Evaluation logic for mitie and stanza models\n\n- converting model output to iob format\n- classification report with sklearn"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2022-12-19T21:04:18Z",
        "message": "fixes issues caused by pymorphy and unnamed unicode characters"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2022-05-25T18:12:12Z",
        "message": "fixed pep8 issues"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-11-03T22:52:24Z",
        "message": "changed stanza chinese processing to use neural model"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-11-03T22:38:06Z",
        "message": "finished jieba integration"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-11-03T12:33:51Z",
        "message": "standalone jieba tokenizer"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-10-19T20:55:32Z",
        "message": "temporarily adds jieba back through stanza - stanza model loading and downloading needs work"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-10-18T18:14:14Z",
        "message": "Fixes #2 removes hard coded locations for models"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-10-18T18:04:54Z",
        "message": "update text processing to remove dependencies on install locations"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-07-20T13:18:26Z",
        "message": "using named arguments"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-07-11T12:59:14Z",
        "message": "adds updated parsivar dependency for Farsi stemming"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-07-09T00:00:02Z",
        "message": "adds farsi stemmer"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-06-29T17:50:09Z",
        "message": "catches recursion error in porter stemmer and passes the token through unchanged"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-06-23T18:17:25Z",
        "message": "cleaned up lowercase normalization"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-06-09T21:59:24Z",
        "message": "limit stanza to a single thread for now"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-06-09T21:09:27Z",
        "message": "stanza also needs to the package turned off when using jieba to prevent it from loading the kitchen sink"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-06-08T19:24:48Z",
        "message": "only download resources when not already there for stanza"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-28T14:26:40Z",
        "message": "Merge branch 'load_psq_table' into 'master'\n\nHandles loading PSQ translation tables\n\nSee merge request research/scale2021!124"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-28T12:44:30Z",
        "message": "can use ~ in model path"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-28T11:54:24Z",
        "message": "fixed error message that had old config path"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-27T23:30:26Z",
        "message": "put most of the infrastructure in place to generate PSQs"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-27T19:48:38Z",
        "message": "adds pre and post functions for normalization"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-25T20:20:04Z",
        "message": "delay loading nlp models until the pipeline begins"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-25T20:16:05Z",
        "message": "cleaned up inheritance for text processing"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-24T22:16:42Z",
        "message": "only load some nlp libraries when needed"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-22T19:31:53Z",
        "message": "Fixes #101 stop words works with stemming now"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-22T18:07:08Z",
        "message": "removed multiplex support from patapsco"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-22T15:49:29Z",
        "message": "switch to ISO 639-3"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-22T14:40:10Z",
        "message": "better construction of tokenizers and stemmers"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-22T13:08:46Z",
        "message": "porter stemming shouldn't also lowercase the stems"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-21T20:21:12Z",
        "message": "first part of porter stemmer implementation"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-21T19:33:09Z",
        "message": "improve configuration of text processing including model path"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-21T18:56:50Z",
        "message": "removed a debugging statement"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-20T12:29:47Z",
        "message": "adds stemming for spaCy"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-20T11:26:31Z",
        "message": "adds tests for stanza lemmatization and fixes issue with Arabic"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-19T22:52:47Z",
        "message": "restructure how tokenizers are built"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-19T20:25:38Z",
        "message": "Merge branch 'master' into 'normalize_text'\n\n# Conflicts:\n#   infrastructure/pipeline/environment.yml\n#   infrastructure/pipeline/patapsco/text.py\n#   infrastructure/pipeline/setup.py"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-19T14:58:49Z",
        "message": "adds character ngrams"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-19T13:13:25Z",
        "message": "adds moses tokenizer"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-18T13:10:57Z",
        "message": "correctly using model name or path to model directory in loader"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-18T12:36:57Z",
        "message": "fixed style issue and set spacy version"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-18T12:35:02Z",
        "message": "update model locations"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-18T12:16:03Z",
        "message": "updated spacy code and tests"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-18T11:35:06Z",
        "message": "adds spacy tokenization"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-15T23:23:46Z",
        "message": "switch to our own normalization code"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-11T00:21:16Z",
        "message": "fixed typo on stanza location on grid"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-10T22:59:41Z",
        "message": "the model path for stanza can be set now"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-04T13:00:11Z",
        "message": "Merge branch 'master' into 'add_stanza'\n\n# Conflicts:\n#   infrastructure/pipeline/patapsco/text.py"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-04T12:53:19Z",
        "message": "adds chinese tokenization using jieba"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-04T12:00:10Z",
        "message": "better logging for stanza"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-04T11:34:16Z",
        "message": "fixes spelling mistake"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-04T11:29:53Z",
        "message": "added StanzaTokenizer for english"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-05-04T11:08:15Z",
        "message": "Refs #9 integrates scriptnorm"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-04-24T16:05:51Z",
        "message": "better name for mock stemmer"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-04-24T15:45:54Z",
        "message": "moved config schema into a single file"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-04-23T18:03:56Z",
        "message": "better validation of config with unit tests for building the pipelines"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-04-23T13:55:30Z",
        "message": "standardizes text processing config"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-04-20T12:50:19Z",
        "message": "fixed bug with a single split"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-04-19T20:35:27Z",
        "message": "adds splitter for text processing"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-04-19T19:26:27Z",
        "message": "adds stop words"
    },
    {
        "repo_url": "github.com/hltcoe/patapsco",
        "filepath": "patapsco/text.py",
        "commit_date": "2021-04-16T22:52:22Z",
        "message": "renamed the pipeline package to patapsco"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/stanza_tagger.py",
        "commit_date": "2023-03-09T09:39:14Z",
        "message": "Refactored StanzaSyntax(Ensemble)Tagger: moved prepare_input_doc & feats_to_ordereddict to common_utils"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/stanza_tagger.py",
        "commit_date": "2023-03-08T16:16:18Z",
        "message": "Refactored & fixed StanzaSyntax(Ensemble)Tagger's input preparation: should now also work on detached layers"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/stanza_tagger.py",
        "commit_date": "2023-01-27T13:58:15Z",
        "message": "Updated StanzaSyntax(Ensemble)Tagger: added guarding exceptions against processing too long sentences with GPU / CUDA"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/stanza_tagger.py",
        "commit_date": "2023-01-19T12:05:04Z",
        "message": "Updated StanzaSyntax(Ensemble)Tagger: relocated initialization of Random obj into the constructor. Also made ensemble_tagger's random choice of max score dependency parses configurable by seed"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/stanza_tagger.py",
        "commit_date": "2023-01-18T09:54:46Z",
        "message": "Updated StanzaSyntax(Ensemble)Tagger: made random pick seed of ambiguous analyses configurable"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/stanza_tagger.py",
        "commit_date": "2022-04-22T10:41:17Z",
        "message": "Fixed StanzaSyntax(Ensemble)Tagger's conf_param (removed missing parameters, set default values to None)"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/stanza_tagger.py",
        "commit_date": "2022-04-22T09:15:51Z",
        "message": "Made StanzaSyntax(Ensemble)Tagger to automatically download missing resources + removed usage of environment variables"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/stanza_tagger.py",
        "commit_date": "2022-03-30T15:01:35Z",
        "message": "Fixed estnltk_neural setup: added missing stanza_resources json file"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/stanza_tagger.py",
        "commit_date": "2022-03-30T14:37:16Z",
        "message": "Made StanzaSyntax(Ensemble)Tagger to use STANZA_SYNTAX_MODELS_PATH & added smoke test for ensemble_tagger"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/stanza_tagger.py",
        "commit_date": "2021-11-17T13:57:32Z",
        "message": "Updated StanzaSyntaxTagger's docstring and added check for mismatching input_type and input_morph_layer"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/stanza_tagger.py",
        "commit_date": "2021-11-17T09:57:04Z",
        "message": "Relocated morph_analysis, text_segmentation, syntax, ner, timexes into estnltk.taggers.standard"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/stanza_tagger.py",
        "commit_date": "2021-10-22T15:24:58Z",
        "message": "estnltk neural additions and moving around other files"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/ensemble_tagger.py",
        "commit_date": "2023-04-24T08:16:32Z",
        "message": "Updated StanzaSyntaxEnsembleTagger: added majority_voting aggregation algorithm"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/ensemble_tagger.py",
        "commit_date": "2023-03-09T09:39:14Z",
        "message": "Refactored StanzaSyntax(Ensemble)Tagger: moved prepare_input_doc & feats_to_ordereddict to common_utils"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/ensemble_tagger.py",
        "commit_date": "2023-03-08T16:16:18Z",
        "message": "Refactored & fixed StanzaSyntax(Ensemble)Tagger's input preparation: should now also work on detached layers"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/ensemble_tagger.py",
        "commit_date": "2023-01-27T13:58:15Z",
        "message": "Updated StanzaSyntax(Ensemble)Tagger: added guarding exceptions against processing too long sentences with GPU / CUDA"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/ensemble_tagger.py",
        "commit_date": "2023-01-19T12:05:04Z",
        "message": "Updated StanzaSyntax(Ensemble)Tagger: relocated initialization of Random obj into the constructor. Also made ensemble_tagger's random choice of max score dependency parses configurable by seed"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/ensemble_tagger.py",
        "commit_date": "2023-01-18T09:54:46Z",
        "message": "Updated StanzaSyntax(Ensemble)Tagger: made random pick seed of ambiguous analyses configurable"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/ensemble_tagger.py",
        "commit_date": "2022-04-22T10:41:17Z",
        "message": "Fixed StanzaSyntax(Ensemble)Tagger's conf_param (removed missing parameters, set default values to None)"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/ensemble_tagger.py",
        "commit_date": "2022-04-22T09:15:51Z",
        "message": "Made StanzaSyntax(Ensemble)Tagger to automatically download missing resources + removed usage of environment variables"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/ensemble_tagger.py",
        "commit_date": "2022-03-30T14:37:16Z",
        "message": "Made StanzaSyntax(Ensemble)Tagger to use STANZA_SYNTAX_MODELS_PATH & added smoke test for ensemble_tagger"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/ensemble_tagger.py",
        "commit_date": "2022-03-05T09:03:17Z",
        "message": "Import fixes in estnltk_neural + added missing package_data"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/ensemble_tagger.py",
        "commit_date": "2021-11-17T09:57:04Z",
        "message": "Relocated morph_analysis, text_segmentation, syntax, ner, timexes into estnltk.taggers.standard"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/ensemble_tagger.py",
        "commit_date": "2021-10-22T15:24:58Z",
        "message": "estnltk neural additions and moving around other files"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/taggers/syntax/stanza_tagger/phrase_extraction_decorator/stanza_syntax_tagger.py",
        "commit_date": "2023-06-05T10:40:53Z",
        "message": "move StanzaSytaxTaggerWithIgnore and ConsistencyDecorator to estnltk_neural"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/tests/taggers/syntax/test_stanza_tagger.py",
        "commit_date": "2023-03-08T16:16:18Z",
        "message": "Refactored & fixed StanzaSyntax(Ensemble)Tagger's input preparation: should now also work on detached layers"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/tests/taggers/syntax/test_stanza_tagger.py",
        "commit_date": "2023-01-23T09:17:26Z",
        "message": "Updated conllu importer tutorial about removing empty nodes"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/tests/taggers/syntax/test_stanza_tagger.py",
        "commit_date": "2023-01-19T12:05:04Z",
        "message": "Updated StanzaSyntax(Ensemble)Tagger: relocated initialization of Random obj into the constructor. Also made ensemble_tagger's random choice of max score dependency parses configurable by seed"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/tests/taggers/syntax/test_stanza_tagger.py",
        "commit_date": "2022-04-22T09:15:51Z",
        "message": "Made StanzaSyntax(Ensemble)Tagger to automatically download missing resources + removed usage of environment variables"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/tests/taggers/syntax/test_stanza_tagger.py",
        "commit_date": "2022-03-30T11:35:58Z",
        "message": "Added abs_path to estnltk_neural and fixed tests using abs_path"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/tests/taggers/syntax/test_stanza_tagger.py",
        "commit_date": "2022-01-27T11:24:50Z",
        "message": "Fix: added missing 'secondary_attributes'"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/tests/taggers/syntax/test_stanza_tagger.py",
        "commit_date": "2021-10-22T15:24:58Z",
        "message": "estnltk neural additions and moving around other files"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/tests/taggers/syntax/test_stanza_ensemble_tagger.py",
        "commit_date": "2023-04-24T08:16:32Z",
        "message": "Updated StanzaSyntaxEnsembleTagger: added majority_voting aggregation algorithm"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/tests/taggers/syntax/test_stanza_ensemble_tagger.py",
        "commit_date": "2023-03-08T16:16:18Z",
        "message": "Refactored & fixed StanzaSyntax(Ensemble)Tagger's input preparation: should now also work on detached layers"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/tests/taggers/syntax/test_stanza_ensemble_tagger.py",
        "commit_date": "2023-01-19T12:05:04Z",
        "message": "Updated StanzaSyntax(Ensemble)Tagger: relocated initialization of Random obj into the constructor. Also made ensemble_tagger's random choice of max score dependency parses configurable by seed"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/tests/taggers/syntax/test_stanza_ensemble_tagger.py",
        "commit_date": "2022-04-22T09:15:51Z",
        "message": "Made StanzaSyntax(Ensemble)Tagger to automatically download missing resources + removed usage of environment variables"
    },
    {
        "repo_url": "github.com/estnltk/estnltk",
        "filepath": "estnltk_neural/estnltk_neural/tests/taggers/syntax/test_stanza_ensemble_tagger.py",
        "commit_date": "2022-03-30T14:37:16Z",
        "message": "Made StanzaSyntax(Ensemble)Tagger to use STANZA_SYNTAX_MODELS_PATH & added smoke test for ensemble_tagger"
    },
    {
        "repo_url": "github.com/VPeterV/Structured-MFVI",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2024-01-05T10:39:05Z",
        "message": "Structured MFVI"
    },
    {
        "repo_url": "github.com/dh-miami/narratives_covid19",
        "filepath": "scripts/topic_modelling/topic_modelling.py",
        "commit_date": "2020-08-31T22:08:37Z",
        "message": "add NER and parallelization"
    },
    {
        "repo_url": "github.com/dh-miami/narratives_covid19",
        "filepath": "scripts/topic_modelling/topic_modelling.py",
        "commit_date": "2020-08-04T02:21:44Z",
        "message": "topic modelling update"
    },
    {
        "repo_url": "github.com/s-nlp/multilingual-fake-news",
        "filepath": "tools/features_extraction.py",
        "commit_date": "2021-02-01T21:47:44Z",
        "message": "ACL 2021 update"
    },
    {
        "repo_url": "github.com/zhijing-jin/ARTS_TestSet",
        "filepath": "code/utils.py",
        "commit_date": "2020-06-14T20:06:18Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/princeton-nlp/MultilingualAnalysis",
        "filepath": "preprocessing/xnli/convert_dataset_to_dependency.py",
        "commit_date": "2021-04-08T06:05:05Z",
        "message": "XNLI syntax modifications"
    },
    {
        "repo_url": "github.com/princeton-nlp/MultilingualAnalysis",
        "filepath": "preprocessing/ner_pos/convert_dataset_to_dependency.py",
        "commit_date": "2021-10-06T04:28:02Z",
        "message": "Modified unicode handling"
    },
    {
        "repo_url": "github.com/princeton-nlp/MultilingualAnalysis",
        "filepath": "preprocessing/ner_pos/convert_dataset_to_dependency.py",
        "commit_date": "2021-04-09T04:56:12Z",
        "message": "NER and POS syntax modif"
    },
    {
        "repo_url": "github.com/princeton-nlp/MultilingualAnalysis",
        "filepath": "preprocessing/sentence_retrieval/convert_dataset_to_dependency.py",
        "commit_date": "2021-04-11T02:20:45Z",
        "message": "Preprocessing for tatoeba"
    },
    {
        "repo_url": "github.com/princeton-nlp/MultilingualAnalysis",
        "filepath": "preprocessing/dependency_parsing/convert_sentences_to_dependency.py",
        "commit_date": "2021-04-04T17:35:01Z",
        "message": "Slurm files"
    },
    {
        "repo_url": "github.com/princeton-nlp/MultilingualAnalysis",
        "filepath": "preprocessing/dependency_parsing/convert_sentences_to_dependency.py",
        "commit_date": "2021-03-22T04:33:09Z",
        "message": "Convert MNLI to syntax synthetic"
    },
    {
        "repo_url": "github.com/princeton-nlp/MultilingualAnalysis",
        "filepath": "preprocessing/dependency_parsing/convert_sentences_to_dependency.py",
        "commit_date": "2021-03-21T05:02:06Z",
        "message": "Fixed error in dependency parsing"
    },
    {
        "repo_url": "github.com/princeton-nlp/MultilingualAnalysis",
        "filepath": "preprocessing/dependency_parsing/convert_sentences_to_dependency.py",
        "commit_date": "2021-03-20T06:03:04Z",
        "message": "File clean up"
    },
    {
        "repo_url": "github.com/mehdi-mirzapour/French-CRS",
        "filepath": "french_crs/fast_text2corefchains.py",
        "commit_date": "2020-06-16T02:00:49Z",
        "message": "Closest-First Strategy Added"
    },
    {
        "repo_url": "github.com/IreneZihuiLi/EHRKit-2022",
        "filepath": "wrapper_functions/utils.py",
        "commit_date": "2022-04-05T16:00:34Z",
        "message": "First Commit"
    },
    {
        "repo_url": "github.com/IreneZihuiLi/EHRKit-2022",
        "filepath": "wrapper_functions/stanza_functions.py",
        "commit_date": "2022-04-05T16:00:34Z",
        "message": "First Commit"
    },
    {
        "repo_url": "github.com/IreneZihuiLi/EHRKit-2022",
        "filepath": "collated_tasks/tasks/utils/get_clusters.py",
        "commit_date": "2022-04-05T16:00:34Z",
        "message": "First Commit"
    },
    {
        "repo_url": "github.com/IreneZihuiLi/EHRKit-2022",
        "filepath": "collated_tasks/tasks/utils/get_sentences.py",
        "commit_date": "2022-04-05T16:00:34Z",
        "message": "First Commit"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-11-13T12:39:27Z",
        "message": "Changed default mongoDB & remove unused method in analysis"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-11-11T16:06:38Z",
        "message": "Similarity now returns -1 instead of raising an error. The error should be handled in the frontend."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-11-11T14:38:11Z",
        "message": "Removed an unnecessary parameter & fixed token counting"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-10-30T10:41:07Z",
        "message": "Merge branch 'multi-processing' into master"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-10-22T16:25:36Z",
        "message": "Removed deprecated keywords"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-10-21T17:39:52Z",
        "message": "Api calls now respect default values correctly (done by **kwargs mapping).\nSome api keywords have been changed to match the function keywords of analysis.py.\nFixed some minor bugs in analysis & api.\nTested all api functions."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-10-13T12:20:25Z",
        "message": "Added two different execution queues for celery (small & big corpora).\nAdded configuration file:\nServer mode (=> use celery task queue or not)\nMongoDB constants\nAnalysis threads\nCelery configuration"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-10-11T12:47:13Z",
        "message": "Added a self-compiled version of blackstone.\nThe project is now compatible with spaCy 2.1.9.\nUpdated requirments.txt\nExchanged a linux-only cpu core determiner function with a cross-platform one (however this function does not account for artifical cpu restrictions of the process!)."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-10-09T16:09:01Z",
        "message": "Added filtering for part of speech tags."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-10-07T15:26:11Z",
        "message": "Added multi-threaded pipeline execution.\nCelex numbers are now part of the corpus (get via get_celex_numbers).\nAdjusted api keywords and fixed none type errors in api_functions for _per_doc functions."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-30T17:23:31Z",
        "message": "Deleted duplicate functions from analysis (e.g. word & token count use the same logic with other params).\nUpdated api functions accordingly.\nAdded initalisation of analysers to flask start up."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-29T11:37:55Z",
        "message": "Added n_grams_per_doc.\nAdded imports for model packages (so python throws an error, when they are not correctly installed)\nUpdated pre-processing\nAdded stanza downloading every setup (no other way currently known to download them via pip)"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-28T15:02:46Z",
        "message": "Adjusted api functions to work with modified analysis."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-25T13:29:49Z",
        "message": "Generalized CorpusAnalysis class to also work for single documents.\nRemoved obsolete Analysis class.\nAdded detailed Python docstrings to public functions."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-25T10:07:25Z",
        "message": "Added sentiment calculation for a whole corpus.\nAdded readbility score for each document in a corpus.\nCode clean up."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-22T17:52:01Z",
        "message": "Added compound cases to NER detection"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-22T15:36:46Z",
        "message": "Added sentiment analysis to single documents.\nImproved pre-processing with another RegEx for parentheses.\nAdded compound cases to NER detection for english texts.\nAdded legal text sentencer to german pipeline.\nShould have fixed problems with sentence segementer being already present."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-21T14:55:12Z",
        "message": "Generalized get_tokens to also account for various part of speech tags & frequency.\nFixed a bug, where empty tokens haven't been removed.\nFixed n_grams for CorpusAnalysis.\nAdded key word extraction for single documents."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-19T18:01:13Z",
        "message": "Added a new regex to remove old headers.\nFixed some typos"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-16T16:49:48Z",
        "message": "Removed deprecated files.\nAdded sentence counting."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-16T16:23:50Z",
        "message": "Added _per_doc functions for corpus analysis:\n- tokens\n- sentences\n- part of speech tags\n- lemmata\n- named entities\nFixed list comprehention errors."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-15T19:54:23Z",
        "message": "Added n-grams for single documents.\nAdded a class for corpus representation and analysis (n_doc > 1)"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-14T22:06:38Z",
        "message": "Rewrote readbility scores.\nAdded working lda implementation for a corpus."
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-14T14:10:44Z",
        "message": "Rewrote the implementation which is now based on another library (old were too slow).\nAdded document similarity based on word-embeddings (aka word2vec).\nAdditionally added more sophisticated preprocessing (mostly regex):\n- older texts are now getting tokenized better\n- headlines of all older formats (until 2003) are getting removed\n- all paragraphs numbers are getting removed correctly (except for lines with in-text headlines)"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-02T13:44:32Z",
        "message": "Added german lemmatisation and pos tagging"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-02T12:29:20Z",
        "message": "Added text complexity & basic LDA"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-01T16:23:08Z",
        "message": "Added most frequent words & avg token length"
    },
    {
        "repo_url": "github.com/phHartl/eu-judgement-analyse",
        "filepath": "analysis.py",
        "commit_date": "2020-09-01T15:46:08Z",
        "message": "Added tokenisation, lemmatisation & POS tagging"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-10-07T09:58:10Z",
        "message": "make style"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-06-15T12:37:34Z",
        "message": "make logging level adjustable for stanza"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-04-02T12:53:18Z",
        "message": "allow loading spacy shortcuts\n\neven though that is not possible anymore in v3. We look for available models and load the first one that starts with \"{model_name}_\""
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-04-01T14:01:46Z",
        "message": "fix spacy warning"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-04-01T10:44:01Z",
        "message": "check spaCy version\n\nregistering Language.component is a v3 feature"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-04-01T10:08:27Z",
        "message": "make style & quality"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-04-01T10:01:11Z",
        "message": "typing improvements"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-04-01T08:36:42Z",
        "message": "allow disabling auto_download\n\nThis may help in scenarios where there is no more access to the internet"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-03-31T17:27:51Z",
        "message": "do not exclude tok2vec which seems important"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-03-31T16:46:01Z",
        "message": "Download stanza model if not available"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-03-31T16:32:50Z",
        "message": "cached_property for Python <3.8"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-03-31T15:31:35Z",
        "message": "allow instantiation of spaCy (but require 3+)"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-03-31T12:50:16Z",
        "message": "make stanza, aligner, spacy optional"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-03-31T09:54:05Z",
        "message": "make style & quality"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-02-24T14:05:04Z",
        "message": "Do not show verbose INFO logs for stanza"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-02-17T17:48:03Z",
        "message": "MWT should be automatically added in new stanza"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-02-08T16:55:25Z",
        "message": "new stanza does not silently ignore missing processors anymore"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-01-13T08:47:12Z",
        "message": "style and typing"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2021-01-07T15:41:17Z",
        "message": "full refactor"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2020-06-04T14:47:08Z",
        "message": "refactoring n1"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2020-06-02T09:55:55Z",
        "message": "fix typing"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2020-06-02T09:37:28Z",
        "message": "Rename AlignmentPair to AlignedIdxs and add Alignment class"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2020-03-25T13:13:28Z",
        "message": "Make style"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2020-03-25T11:13:50Z",
        "message": "include MWT processor for languages that support it"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2020-03-25T10:17:13Z",
        "message": "move get_distance to utils"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2020-03-19T13:49:58Z",
        "message": "remove unused import"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2020-03-19T13:47:22Z",
        "message": "style"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2020-03-18T15:57:39Z",
        "message": "Don't download\n\nModels need to be downloaded manually by users"
    },
    {
        "repo_url": "github.com/BramVanroy/astred",
        "filepath": "astred/utils.py",
        "commit_date": "2020-03-18T15:00:59Z",
        "message": "use astred as package name"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/demo/pipeline_demo.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/stanza/tests/resources/test_common.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/mit-ccc/TweebankNLP",
        "filepath": "twitter-stanza/stanza/tests/resources/test_installation.py",
        "commit_date": "2022-01-17T22:39:52Z",
        "message": "add/twitter-stanza-pipeline"
    },
    {
        "repo_url": "github.com/xjtu-intsoft/chase",
        "filepath": "Benchmark_Approaches/DuoratChar/duorat/utils/tokenization.py",
        "commit_date": "2021-08-02T03:02:02Z",
        "message": "Add benchmark approaches"
    },
    {
        "repo_url": "github.com/QtacierP/PRIOR",
        "filepath": "codes/prior/data/pretrain/text_process.py",
        "commit_date": "2023-08-03T06:36:32Z",
        "message": "pre-train codes"
    },
    {
        "repo_url": "github.com/philip-mueller/lovt",
        "filepath": "src/data/text_utils/sentence_splitting.py",
        "commit_date": "2021-11-05T09:06:35Z",
        "message": "Added code"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SENNA_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SENNA_util.py",
        "commit_date": "2023-05-30T16:00:54Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI;\n4. added the chart visualization to BERT NER;\n5. uniformed the output layout of spaCy NER to all other NER packages;\n6. fixed a bug in reminders;\n7 fixed bugs in the opening of output files in the CoNLL_table_analyzer_main GUI;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/SENNA_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2024-02-06T02:24:23Z",
        "message": "1. added a vocab csv output file when computing 1-grams;\n2. fixed potential bug with Stanza not installed in NLP_menu_main when selecting Setup default NLP parsers & annotators...\n3. fixed a bug in data_visualization_main_1 with the Treemap option."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2023-11-21T19:51:41Z",
        "message": "1. fixed a bug with wordcloud visualization when using a csv file in input;\n2. extended the POS NPOUN to include proper nouns in Wordclouds;\n3. added a checkbox to compute corpus statistics by POS (Part of Speech) tag value in the statistics_txt_main GUI;\n4. uniformed the handling of ,'GUIs available for more options' in all GUIs that rely on the checkbox and menu."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2023-10-05T00:06:18Z",
        "message": "1. fixed a bug with Gephi;\n2. fixed bugs in the function statistics_csv_util.compute_csv_column_frequencies with no groups;\n3. fixed the wrong display of Infinitive verbs frequency."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2023-09-30T15:43:33Z",
        "message": "1. fixed bugs in the creation of charts for document statistics and other charts;\n2. improved the output layout for sentiment analysis algorithms."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2023-09-29T16:49:39Z",
        "message": "1. fixed an inconsequential bug in the export of sentiment analysis scores for Stanza;\n2. fixed a bug in the export of charts for sentiment analysis scores."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2023-09-22T13:41:01Z",
        "message": "1. Fixed a minor issue in the parsers_annotators_main with the CoNLL_table_analyzer checkbox state (normal/disabled);\n2. uniformed the MAC & Windows width for OK + Reset Show buttons in all GUIs."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2023-09-21T12:05:01Z",
        "message": "1. uniformed the width of Open file/dictionary buttons in all GUIs;\n2. improved the layout of the html_gender_annotator GUI;\n3. fixed a bug with language selection in Stanza."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2023-06-17T08:29:03Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2023-06-14T07:27:54Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2023-06-12T12:59:48Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2023-06-04T17:32:29Z",
        "message": "1. rewrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI;\n4. added the chart visualization to BERT NER;\n5. uniformed the output layout of spaCy NER to all other NER packages;\n6. fixed a bug in reminders;\n7 fixed bugs in the opening of output files in the CoNLL_table_analyzer_main GUI;\n8. fixed bugs in the statistics_csv_main GUIs;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2023-05-30T16:00:54Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI;\n4. added the chart visualization to BERT NER;\n5. uniformed the output layout of spaCy NER to all other NER packages;\n6. fixed a bug in reminders;\n7 fixed bugs in the opening of output files in the CoNLL_table_analyzer_main GUI;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/wordclouds_util.py",
        "commit_date": "2023-11-25T11:01:41Z",
        "message": "1. fixed a bug in the creation of a wordcloud with an empty input file."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/wordclouds_util.py",
        "commit_date": "2023-11-22T09:13:51Z",
        "message": "1. in the SVO swordclouds visualization, set the collocations parameter to False to avoid potential repetition of the same words;\n2. added new options to the dropdown menus in NL_menu_main for statistical tools of textual analysis;\n 3. fixed chart display bugs in the computation of clause, noun, verb, function words in the CoNLL table analyzer;\n4. added computation of overall noun and verb lists and frequencies in the CoNLL table analyzer;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/wordclouds_util.py",
        "commit_date": "2023-11-21T19:51:41Z",
        "message": "1. fixed a bug with wordcloud visualization when using a csv file in input;\n2. extended the POS NPOUN to include proper nouns in Wordclouds;\n3. added a checkbox to compute corpus statistics by POS (Part of Speech) tag value in the statistics_txt_main GUI;\n4. uniformed the handling of ,'GUIs available for more options' in all GUIs that rely on the checkbox and menu."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/wordclouds_util.py",
        "commit_date": "2023-10-12T13:34:06Z",
        "message": "1. added M.K.A. Halliday's high-value, median-value, and low-value classification of modal verbs in CoNLL_verb_analysis_util;\n2. added a wordcloud display to the output of a CoNLL_table_search_util.py."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/wordclouds_util.py",
        "commit_date": "2023-10-11T15:48:06Z",
        "message": "1. In the Style analysis GUI, modified the Vocabulary analysis option for Short words (<4 characters) to compute, instead, the word length for all words in a corpus;\n2. fixed bug in wordclouds_util word_str referenced before assignment."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/wordclouds_util.py",
        "commit_date": "2023-10-08T17:24:19Z",
        "message": "1. fixed a config filename bug in coreference_main.py."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/wordclouds_util.py",
        "commit_date": "2023-09-08T02:05:20Z",
        "message": "1. fixed a bug in wordclouds_main GUI with wrong filename;\n2. fixed a bug with Mac external software installation."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/wordclouds_util.py",
        "commit_date": "2023-06-04T17:32:29Z",
        "message": "1. rewrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI;\n4. added the chart visualization to BERT NER;\n5. uniformed the output layout of spaCy NER to all other NER packages;\n6. fixed a bug in reminders;\n7 fixed bugs in the opening of output files in the CoNLL_table_analyzer_main GUI;\n8. fixed bugs in the statistics_csv_main GUIs;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/wordclouds_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_functions.py",
        "commit_date": "2024-02-07T03:09:41Z",
        "message": "1. fixed bug in file_search_byWord_main when running the case sensitive/insensitive option."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_functions.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/IO_libraries_util.py",
        "commit_date": "2024-02-07T03:09:41Z",
        "message": "1. fixed bug in file_search_byWord_main when running the case sensitive/insensitive option."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/IO_libraries_util.py",
        "commit_date": "2024-01-25T17:09:34Z",
        "message": "1. fixed a bug in the NLP_menu_main when selecting the Co-Occurrences VIEWER and N-Grams VIEWER;\n3. removed the unnamed column in n-grams csv output thuus removing a bug in the Search word(s) option in the N-grams Co-Occurrences GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/IO_libraries_util.py",
        "commit_date": "2023-11-17T13:59:01Z",
        "message": "1. added CLOSE button to coref results and passed the right file back to SVO when doing manual coreferencing."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/IO_libraries_util.py",
        "commit_date": "2023-10-28T00:20:05Z",
        "message": "1. nominalization creates a line chart with values by date if the files embed a date."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/IO_libraries_util.py",
        "commit_date": "2023-10-27T02:08:00Z",
        "message": "1. completed the work on generalizing the special charts to process any number and types of csv file fields."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/IO_libraries_util.py",
        "commit_date": "2023-10-05T00:06:18Z",
        "message": "1. fixed a bug with Gephi;\n2. fixed bugs in the function statistics_csv_util.compute_csv_column_frequencies with no groups;\n3. fixed the wrong display of Infinitive verbs frequency."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/IO_libraries_util.py",
        "commit_date": "2023-09-08T02:05:20Z",
        "message": "1. fixed a bug in wordclouds_main GUI with wrong filename;\n2. fixed a bug with Mac external software installation."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/IO_libraries_util.py",
        "commit_date": "2023-09-01T18:08:08Z",
        "message": "1. Improved the display of information when clicking on a Google Earth Pro pin in GIS_main;\n2. improved the user interface in the functions behind the NLP_setup_external_software_main.py GUI;\n3. fixed a filename bug in opening the config file NLP_default_package_language_config.csv;\n4. fixed a bug in GUI-specific I/O configuration not updating."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/IO_libraries_util.py",
        "commit_date": "2023-09-01T11:50:28Z",
        "message": "1. Improved the display of information when clicking on a Google Earth Pro pin in GIS_main;\n2. improved the user interface in the functions behind the NLP_setup_external_software_main.py GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/IO_libraries_util.py",
        "commit_date": "2023-06-10T18:01:13Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/IO_libraries_util.py",
        "commit_date": "2023-05-30T16:00:54Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI;\n4. added the chart visualization to BERT NER;\n5. uniformed the output layout of spaCy NER to all other NER packages;\n6. fixed a bug in reminders;\n7 fixed bugs in the opening of output files in the CoNLL_table_analyzer_main GUI;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/IO_libraries_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2024-02-11T15:52:51Z",
        "message": "1. fixed a bug in the display of headers in file_search_byWord;\n2. added the lemmatize option in the file_search_byWord search."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2024-02-06T02:24:23Z",
        "message": "1. added a vocab csv output file when computing 1-grams;\n2. fixed potential bug with Stanza not installed in NLP_menu_main when selecting Setup default NLP parsers & annotators...\n3. fixed a bug in data_visualization_main_1 with the Treemap option."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2024-01-29T11:32:14Z",
        "message": "1. fixed a bug in a function call in the file_checker_converter_cleaner_main due to wrong number of parameters passed."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2024-01-26T03:30:54Z",
        "message": "1. fixed potential bug in importing stanza;\n2. fixed bug in computing document(s) statistics."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2024-01-11T11:31:01Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI;\n3. fixed wrong warning in SVO GUI for gender and quote annotators;\n4. added Sankey charts to SVO output."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-11-27T12:12:33Z",
        "message": "1. fixed a bug in the creation of a wordcloud with an empty input file;\n2. added an Organizations column in the SVO output;\n3. improved Stanford CoreNLP SVO using entitymentions values for Subjects and Objects;\n4. introduced a new GUI for a pipeline of data quality algorithms (file_checker_pre_processing_pipeline_main.py)."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-11-25T11:01:41Z",
        "message": "1. fixed a bug in the creation of a wordcloud with an empty input file."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-11-21T19:51:41Z",
        "message": "1. fixed a bug with wordcloud visualization when using a csv file in input;\n2. extended the POS NPOUN to include proper nouns in Wordclouds;\n3. added a checkbox to compute corpus statistics by POS (Part of Speech) tag value in the statistics_txt_main GUI;\n4. uniformed the handling of ,'GUIs available for more options' in all GUIs that rely on the checkbox and menu."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-30T16:21:03Z",
        "message": "1. moved the corpus statistics under the text statistics GUI and out of style analysis."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-29T12:07:38Z",
        "message": "1. improved the n-grams scripts;\n2. added a line chart to the n-gram search for 1-grams;\n3. corrected the sort in the function  get_data_to_be_plotted_with_counts leaving sorting in the order of the x-axis values;\n4. organized N-grams output ny n-gram number."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-28T15:36:33Z",
        "message": "1. improved the n-grams scripts."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-28T12:41:58Z",
        "message": "1. improved the n-grams scripts."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-27T02:08:00Z",
        "message": "1. completed the work on generalizing the special charts to process any number and types of csv file fields."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-24T12:36:09Z",
        "message": "1. prepared the data_visualization_1_main.py GUI to process multiple selections of fields/search values. TO BE CONTINUED;\n2. added Sankey charts to the n-grams search in NGrams_CoOccurrences_main GUI;\n3. fixed an Excel chart display with MALLET topic modelling;\n4. fixed a bug in style_analysis_main for the hapax_words variable not assigned."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-22T14:30:14Z",
        "message": "1. added the N-grams search option in the NGrams_CoOccurrences_main GUI;\n2. added the code to deal with multi-word NER values."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-21T12:55:43Z",
        "message": "1. reorganized the style_analysis and NGrams_CoOccurrences GUI;\n2. in the CoNLL_table_analyzer and file_search_byWord GUIs added the options of searching for a word and extracting neighboring words (TO BE COMPLETED);\n3. improved the efficiency of the ngrams algorithms."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-20T00:21:16Z",
        "message": "1. hapax"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-19T23:28:20Z",
        "message": "1. hapax"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-18T10:53:21Z",
        "message": "1. completed the n-grams/co-occurence algorithms;\n2. added error trapping in the knowledge_graphs_WordNet_main;\n3. added a chart of countries found by the geocoder used for GIS."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-17T23:32:23Z",
        "message": "1. completed the n-grams algorithms."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-17T23:26:53Z",
        "message": "1. completed the n-grams algorithms."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-17T17:41:19Z",
        "message": "1. fixed a bug in the N-grams VIEWER when splitting multiple-word search words (e.g., Hong Kong)."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-17T00:28:57Z",
        "message": "1. completed the development of GIS pipeline;\n2. rewrote the n-grams algorithms leading to greater efficiency (15 minutes instead of 5 hours for the GWR corpus)."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-16T21:39:03Z",
        "message": "1. completed the development of GIS pipeline;\n2. rewrote the n-grams algorithms leading to greater efficiency (15 minutes instead of 5 hours for the GWR corpus)."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-10-11T15:48:06Z",
        "message": "1. In the Style analysis GUI, modified the Vocabulary analysis option for Short words (<4 characters) to compute, instead, the word length for all words in a corpus;\n2. fixed bug in wordclouds_util word_str referenced before assignment."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-09-30T15:43:33Z",
        "message": "1. fixed bugs in the creation of charts for document statistics and other charts;\n2. improved the output layout for sentiment analysis algorithms."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-09-29T21:33:51Z",
        "message": "1. fixed a display problem with charts by group columns (e.g., By Document)."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-09-29T16:49:39Z",
        "message": "1. fixed an inconsequential bug in the export of sentiment analysis scores for Stanza;\n2. fixed a bug in the export of charts for sentiment analysis scores."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-09-16T18:20:12Z",
        "message": "1. Added the POTUS_webscraper based on beautifulsoup to extract POTUS speeches;\n2.fixed an inconsequential display but in NLP_setup_package_language_main\n3. fixed bugs in the creation of Excel charts;\n4. fixed bugs in TIPS filenames;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-08-13T08:49:24Z",
        "message": "Release 3.8.8 reflects a large number of bug fixes and improvements, the result of summer 2023 work on the NLP Suite."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-08-09T16:58:34Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-08-07T13:49:20Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-08-06T14:00:48Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-06-14T16:57:31Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-06-14T16:08:15Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-06-12T12:59:48Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-06-10T18:01:13Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-05-30T16:00:54Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI;\n4. added the chart visualization to BERT NER;\n5. uniformed the output layout of spaCy NER to all other NER packages;\n6. fixed a bug in reminders;\n7 fixed bugs in the opening of output files in the CoNLL_table_analyzer_main GUI;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/statistics_txt_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_functions_util.py",
        "commit_date": "2024-02-07T03:09:41Z",
        "message": "1. fixed bug in file_search_byWord_main when running the case sensitive/insensitive option."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_functions_util.py",
        "commit_date": "2024-01-26T03:30:54Z",
        "message": "1. fixed potential bug in importing stanza;\n2. fixed bug in computing document(s) statistics."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/Stanza_functions_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util.py",
        "commit_date": "2023-12-11T10:02:47Z",
        "message": "1. prepared all scripts for user-selected data transformations for plotting (e.g., log);\n2. fixed bugs in the computing of hapax values from Style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util.py",
        "commit_date": "2023-11-08T11:34:46Z",
        "message": "1. fixed various bugs in the search functions;\n2. added the option of filtering data when drawing a sunburst or treemap charts."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util.py",
        "commit_date": "2023-09-30T15:43:33Z",
        "message": "1. fixed bugs in the creation of charts for document statistics and other charts;\n2. improved the output layout for sentiment analysis algorithms."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util.py",
        "commit_date": "2023-09-01T18:08:08Z",
        "message": "1. Improved the display of information when clicking on a Google Earth Pro pin in GIS_main;\n2. improved the user interface in the functions behind the NLP_setup_external_software_main.py GUI;\n3. fixed a filename bug in opening the config file NLP_default_package_language_config.csv;\n4. fixed a bug in GUI-specific I/O configuration not updating."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util.py",
        "commit_date": "2023-08-13T08:49:24Z",
        "message": "Release 3.8.8 reflects a large number of bug fixes and improvements, the result of summer 2023 work on the NLP Suite."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util.py",
        "commit_date": "2023-08-07T13:49:20Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util.py",
        "commit_date": "2023-06-14T16:08:15Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util.py",
        "commit_date": "2023-06-12T12:59:48Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util.py",
        "commit_date": "2023-06-10T18:01:13Z",
        "message": "fixed typo and broken link"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util.py",
        "commit_date": "2023-05-30T16:00:54Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI;\n4. added the chart visualization to BERT NER;\n5. uniformed the output layout of spaCy NER to all other NER packages;\n6. fixed a bug in reminders;\n7 fixed bugs in the opening of output files in the CoNLL_table_analyzer_main GUI;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/file_spell_checker_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/knowledge_graphs_YAGO_util.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NLP_setup_package_language_main.py",
        "commit_date": "2023-09-26T14:14:20Z",
        "message": "1. revised the data_visualization_1 and data_visualization_2 scripts"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NLP_setup_package_language_main.py",
        "commit_date": "2023-09-22T13:41:01Z",
        "message": "1. Fixed a minor issue in the parsers_annotators_main with the CoNLL_table_analyzer checkbox state (normal/disabled);\n2. uniformed the MAC & Windows width for OK + Reset Show buttons in all GUIs."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NLP_setup_package_language_main.py",
        "commit_date": "2023-09-21T12:05:01Z",
        "message": "1. uniformed the width of Open file/dictionary buttons in all GUIs;\n2. improved the layout of the html_gender_annotator GUI;\n3. fixed a bug with language selection in Stanza."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NLP_setup_package_language_main.py",
        "commit_date": "2023-09-18T11:12:26Z",
        "message": "1. added a series of checks to the NLP_setup_package_language_main.py to avoid setup errors;\n2. Added the POTUS_webscraper based on beautifulsoup to extract POTUS speeches;\n3. fixed bugs in the creation of Excel charts;\n4. fixed bugs in TIPS filenames."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NLP_setup_package_language_main.py",
        "commit_date": "2023-09-01T14:00:46Z",
        "message": "1. Improved the display of information when clicking on a Google Earth Pro pin in GIS_main;\n2. improved the user interface in the functions behind the NLP_setup_external_software_main.py GUI;\n3. fixed a filename bug in opening the config file NLP_default_package_language_config.csv."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NLP_setup_package_language_main.py",
        "commit_date": "2023-05-30T16:00:54Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI;\n4. added the chart visualization to BERT NER;\n5. uniformed the output layout of spaCy NER to all other NER packages;\n6. fixed a bug in reminders;\n7 fixed bugs in the opening of output files in the CoNLL_table_analyzer_main GUI;"
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/NLP_setup_package_language_main.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/NLP-Suite/NLP-Suite",
        "filepath": "src/knowledge_graphs_DBpedia_util_SPARQL.py",
        "commit_date": "2023-05-15T12:27:18Z",
        "message": "1. reqrote the SVO_util functions, fixing bugs with spaCy and Stanza SVO;\n2. made the SVO GUI more user-friendly;\n3. fixed bug in style analysis GUI."
    },
    {
        "repo_url": "github.com/jim-schwoebel/allie",
        "filepath": "features/text_features/helpers/blabla/blabla/document_processor.py",
        "commit_date": "2020-12-03T03:04:48Z",
        "message": "added allosaurus, surfboard, and blabla featurizers"
    },
    {
        "repo_url": "github.com/mudabek/encoding-cxr-report-gen",
        "filepath": "infer.py",
        "commit_date": "2022-11-24T13:17:45Z",
        "message": "full"
    },
    {
        "repo_url": "github.com/mudabek/encoding-cxr-report-gen",
        "filepath": "train.py",
        "commit_date": "2022-11-24T13:17:45Z",
        "message": "full"
    },
    {
        "repo_url": "github.com/mudabek/encoding-cxr-report-gen",
        "filepath": "ner_reports.py",
        "commit_date": "2022-11-24T13:17:45Z",
        "message": "full"
    },
    {
        "repo_url": "github.com/arianhosseini/negation-learning",
        "filepath": "transformers/examples/get_conll.py",
        "commit_date": "2020-09-27T16:25:48Z",
        "message": "stable version emnlp"
    },
    {
        "repo_url": "github.com/arianhosseini/negation-learning",
        "filepath": "transformers/examples/new_get_conll.py",
        "commit_date": "2020-09-27T16:25:48Z",
        "message": "stable version emnlp"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2023-08-11T11:51:46Z",
        "message": "pre-aligned pair upload: upload one pair per annotator"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2023-08-11T11:07:43Z",
        "message": "fix bug uploading pre-aligned & pre-split n:m pairs"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2023-08-11T10:57:08Z",
        "message": "fix bug error annotation"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2022-11-17T14:39:11Z",
        "message": "align documents with different simplified versions (references) and export them"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2022-10-06T12:30:04Z",
        "message": "bug fix: counting sentence ids of pre-split data\n\naddressing #1"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2022-05-16T10:01:42Z",
        "message": "bugfixes"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2022-04-14T12:02:50Z",
        "message": "update webcrawling"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2022-04-05T14:13:21Z",
        "message": "bug fixing"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2022-03-07T16:57:02Z",
        "message": "downgrade to spacy 2.3.7 (from 3.2.0) II"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2022-03-07T16:40:43Z",
        "message": "downgrade to spacy 2.3.7 (from 3.2.0)"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2022-03-05T13:16:11Z",
        "message": "\"show most similar sentence\"-button for alignment support"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2022-03-03T17:55:29Z",
        "message": "remove add_par_nr"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2022-03-03T17:53:33Z",
        "message": "uncomment add_par_nr"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2022-01-18T14:05:40Z",
        "message": "export document level (aligned and not-aligned)"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2022-01-14T15:58:19Z",
        "message": "add right-to-left reading support"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-12-30T17:38:13Z",
        "message": "improvement of export functions"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-12-30T12:45:28Z",
        "message": "huge update including user fields, local and web import, IAA alignment, config files annotation, web scraping"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-04-16T13:04:41Z",
        "message": "manual simplification module"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-04-07T17:59:33Z",
        "message": "add data with copyright threshold"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-04-07T12:35:50Z",
        "message": "add overview of all corpora and overview per corpus"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-04-06T18:12:10Z",
        "message": "remove django-language"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-04-06T10:39:54Z",
        "message": "add pre aligned data"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-03-01T14:34:40Z",
        "message": "import title in file header"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-03-01T11:02:55Z",
        "message": "fix reading file ending"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-02-25T17:34:33Z",
        "message": "visibiliy changes"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-02-04T14:07:53Z",
        "message": "add possible to align button"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-02-04T13:33:02Z",
        "message": "report malformed sentence"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-01-22T14:18:42Z",
        "message": "disable stanza download"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-01-22T13:36:26Z",
        "message": "new url structure, prev and next button, better HTML structure, change_log"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-01-15T13:49:38Z",
        "message": "add downloading stanza model"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-01-15T13:39:41Z",
        "message": "change spacy to stanza"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-01-15T12:15:51Z",
        "message": "automatic transformations in evaluation added"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-01-13T14:20:25Z",
        "message": "code clean up (new model structure) Part II"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-01-12T19:53:51Z",
        "message": "code clean up (new model structure) Part I"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-01-12T11:25:01Z",
        "message": "document table overview"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-01-08T15:57:05Z",
        "message": "upload parallel documents without sentence alignment"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-01-07T15:19:02Z",
        "message": "save and edit alignments"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2021-01-05T20:19:30Z",
        "message": "separate transformations and rating"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2020-10-09T08:37:39Z",
        "message": "overview of alignment and rating"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2020-10-07T15:42:36Z",
        "message": "alignment changing and rating"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2020-10-02T17:30:49Z",
        "message": "option to upload records"
    },
    {
        "repo_url": "github.com/rstodden/TS_annotation_tool",
        "filepath": "data/models.py",
        "commit_date": "2020-09-30T15:49:20Z",
        "message": "added app per task"
    },
    {
        "repo_url": "github.com/centre-for-humanities-computing/newsFluxus",
        "filepath": "downloader.py",
        "commit_date": "2021-01-14T14:58:32Z",
        "message": "add Stanza"
    },
    {
        "repo_url": "github.com/centre-for-humanities-computing/newsFluxus",
        "filepath": "downloader.py",
        "commit_date": "2020-06-16T09:00:48Z",
        "message": "clean"
    },
    {
        "repo_url": "github.com/centre-for-humanities-computing/newsFluxus",
        "filepath": "downloader.py",
        "commit_date": "2020-06-04T09:58:42Z",
        "message": "add downloader"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-04-05T17:58:13Z",
        "message": "Fix frequency weighting"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-04-03T14:40:35Z",
        "message": "Clean up"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-03-31T12:47:58Z",
        "message": "Minor"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-03-29T22:48:44Z",
        "message": "Using JSONL in JSD script"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-03-29T12:14:18Z",
        "message": "Use JSONL format for the entire pipeline"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-03-28T21:58:03Z",
        "message": "Added UDPipe to the lemmatizer\nFixes #2"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-03-20T17:37:34Z",
        "message": "Gensim vectors and ELMo backward-forward comb"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-02-28T13:02:39Z",
        "message": "Use numpy log function"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-02-28T12:53:26Z",
        "message": "Optional frequency correction"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-02-28T12:34:05Z",
        "message": "Prior word probability distributions"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-02-23T14:44:46Z",
        "message": "Fix log error"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-02-23T13:48:55Z",
        "message": "Fix log/exp issues"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-02-23T13:29:30Z",
        "message": "Fix log/exp issues"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-02-08T17:14:34Z",
        "message": "Inject lexical similarity with static embedding model"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-01-12T18:08:39Z",
        "message": "Use argparse for script arguments"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-01-12T14:44:39Z",
        "message": "Divide by temperature in postprocessing"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-01-07T19:08:40Z",
        "message": "Correct Swedish language code"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2021-01-07T16:43:59Z",
        "message": "Obtain all forms of target words given a corpus"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2020-12-28T12:53:54Z",
        "message": "Minor"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2020-12-28T10:22:15Z",
        "message": "Use stanza lemmatizer"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2020-12-28T09:20:01Z",
        "message": "Use stanza lemmatizer"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2020-12-27T11:27:29Z",
        "message": "Fix options for frequency correction"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2020-12-27T09:29:14Z",
        "message": "Clean up"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2020-12-27T09:28:39Z",
        "message": "Clean up"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/postprocessing.py",
        "commit_date": "2020-12-27T09:24:46Z",
        "message": "Refactor"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/processing/find_word_forms.py",
        "commit_date": "2021-01-07T19:08:40Z",
        "message": "Correct Swedish language code"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/processing/find_word_forms.py",
        "commit_date": "2021-01-07T18:41:57Z",
        "message": "Minor"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/processing/find_word_forms.py",
        "commit_date": "2021-01-07T18:07:35Z",
        "message": "Use pos tags for english to filter out irrelevant forms"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/processing/find_word_forms.py",
        "commit_date": "2021-01-07T17:49:32Z",
        "message": "Obtain all forms of target words given a corpus"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/processing/find_word_forms.py",
        "commit_date": "2021-01-07T17:44:31Z",
        "message": "Obtain all forms of target words given a corpus"
    },
    {
        "repo_url": "github.com/akutuzov/semeval2020",
        "filepath": "code/processing/find_word_forms.py",
        "commit_date": "2021-01-07T16:43:59Z",
        "message": "Obtain all forms of target words given a corpus"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/BohdanYatsyna/homework2/task-02-3-2.py",
        "commit_date": "2020-03-26T15:51:25Z",
        "message": "\u0432\u0438\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u044f \u0437\u0430\u0443\u0432\u0430\u0436\u0435\u043d\u044c \u041c\u0430\u0440\u044f\u043d\u0438\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398049316\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398050175\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398053698\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398054208\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398058079\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398058962\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398060221\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398064295\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398065397"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/BohdanYatsyna/homework2/task-02-3-2.py",
        "commit_date": "2020-03-26T15:46:43Z",
        "message": "\u0432\u0438\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u044f \u0437\u0430\u0443\u0432\u0430\u0436\u0435\u043d\u044c \u041c\u0430\u0440\u044f\u043d\u0438\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398049316\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398050175\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398053698\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398054208\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398058079\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398058962\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398060221\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398064295\nhttps://github.com/vseloved/prj-nlp-2020/pull/33#discussion_r398065397"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/BohdanYatsyna/homework2/task-02-3-2.py",
        "commit_date": "2020-03-23T17:57:51Z",
        "message": "\u0437\u0430\u043c\u0435\u0440\u0436\u0438\u0432 2\u0433\u0443 \u0434\u043e\u043c\u0430\u0448\u043a\u0443 \u0432 \u043c\u0430\u0441\u0442\u0435\u0440"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/BohdanYatsyna/homework2/task-02-3-2.py",
        "commit_date": "2020-03-23T17:53:00Z",
        "message": "\u0437\u0430\u043c\u0435\u0440\u0436\u0438\u0432 2\u0433\u0443 \u0434\u043e\u043c\u0430\u0448\u043a\u0443 \u0432 \u043c\u0430\u0441\u0442\u0435\u0440"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/BohdanYatsyna/homework2/task-02-3-2.py",
        "commit_date": "2020-03-23T17:45:46Z",
        "message": "3\u044f \u0447\u0430\u0441\u0442\u0438\u043d\u0430 \u0434\u043e\u043c\u0430\u0448\u043a\u0438"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/DmytroMindra/05-bag-of-words/src/04-frequency_analysis.py",
        "commit_date": "2020-04-15T12:18:59Z",
        "message": "Filtered input added notes to readme.md"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/DmytroMindra/05-bag-of-words/src/04-frequency_analysis.py",
        "commit_date": "2020-04-14T20:06:57Z",
        "message": "Added more data to the experiment"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/DmytroMindra/05-bag-of-words/src/04-frequency_analysis.py",
        "commit_date": "2020-04-11T20:44:05Z",
        "message": "Adding homework-05"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/DmytroMindra/06-language-as-sequence/src/02-prepare-datasets.py",
        "commit_date": "2020-04-20T07:31:11Z",
        "message": "Added the evaluation on run-on-test.json"
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/DmytroMindra/06-language-as-sequence/src/02-prepare-datasets.py",
        "commit_date": "2020-04-16T21:35:47Z",
        "message": "Added more features. Improved score."
    },
    {
        "repo_url": "github.com/vseloved/prj-nlp-2020",
        "filepath": "students/DmytroMindra/06-language-as-sequence/src/02-prepare-datasets.py",
        "commit_date": "2020-04-16T09:23:18Z",
        "message": "Baseline"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-11-07T11:03:53Z",
        "message": "add doc string to krnnt analyzer, add gsutil command to synchronize data"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-05-19T14:12:35Z",
        "message": "add warning swallow for polylot detect lang function"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-05-19T13:54:32Z",
        "message": "fox jsondecoderError in krnnt tagger"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-05-18T13:55:31Z",
        "message": "fix quotes"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-05-18T13:54:10Z",
        "message": "add passing url parma to krnnt analyzer"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-05-16T10:55:07Z",
        "message": "comment morfeusz2 import, it should be removed"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-05-15T14:22:29Z",
        "message": "add stats to process lines"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-05-15T07:39:41Z",
        "message": "add oscar processing, move data procesing to another notebook"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-05-11T05:59:04Z",
        "message": "add new test sentences to check"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-05-08T14:56:43Z",
        "message": "add krnnttagger,add aux and propn"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-05-07T18:11:57Z",
        "message": "add stanza sentence validator, add linguistics heuristic for sentences"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-04-08T15:39:09Z",
        "message": "add testing pos taggers"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-04-05T11:48:10Z",
        "message": "add checking if sentence is in polish"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-03-30T14:48:16Z",
        "message": "move wiki and text data processing to another jupyter, test sentence validator"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "text_utils.py",
        "commit_date": "2020-03-26T21:46:22Z",
        "message": "wip: refactoring and checking sentence"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "playground_taggers.py",
        "commit_date": "2020-05-15T07:39:41Z",
        "message": "add oscar processing, move data procesing to another notebook"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "playground_taggers.py",
        "commit_date": "2020-05-11T05:59:04Z",
        "message": "add new test sentences to check"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "playground_taggers.py",
        "commit_date": "2020-05-08T14:56:43Z",
        "message": "add krnnttagger,add aux and propn"
    },
    {
        "repo_url": "github.com/Ermlab/PoLitBert",
        "filepath": "playground_taggers.py",
        "commit_date": "2020-05-07T18:11:57Z",
        "message": "add stanza sentence validator, add linguistics heuristic for sentences"
    },
    {
        "repo_url": "github.com/argosopentech/argos-train",
        "filepath": "argostrain/train.py",
        "commit_date": "2023-09-02T17:45:03Z",
        "message": "Remove dataset size limiting\n\nGoing forward I'm planning to exclude\nlarge datasets from the data index alltogether."
    },
    {
        "repo_url": "github.com/argosopentech/argos-train",
        "filepath": "argostrain/train.py",
        "commit_date": "2023-09-02T17:35:29Z",
        "message": "Test references fix"
    },
    {
        "repo_url": "github.com/argosopentech/argos-train",
        "filepath": "argostrain/train.py",
        "commit_date": "2023-08-20T14:01:53Z",
        "message": "Delete data if exists"
    },
    {
        "repo_url": "github.com/argosopentech/argos-train",
        "filepath": "argostrain/train.py",
        "commit_date": "2023-08-20T13:57:30Z",
        "message": "Add check before deleting data"
    },
    {
        "repo_url": "github.com/argosopentech/argos-train",
        "filepath": "argostrain/train.py",
        "commit_date": "2023-05-03T00:58:22Z",
        "message": "Add Python formatting"
    },
    {
        "repo_url": "github.com/argosopentech/argos-train",
        "filepath": "argostrain/train.py",
        "commit_date": "2023-05-03T00:57:03Z",
        "message": "Remove unused epochs_count variable\n\nThis is configured in config.yml"
    },
    {
        "repo_url": "github.com/argosopentech/argos-train",
        "filepath": "argostrain/train.py",
        "commit_date": "2022-11-14T00:39:04Z",
        "message": "Improve reverse data code"
    },
    {
        "repo_url": "github.com/argosopentech/argos-train",
        "filepath": "argostrain/train.py",
        "commit_date": "2022-11-09T13:10:34Z",
        "message": "Improve dataset filtering"
    },
    {
        "repo_url": "github.com/argosopentech/argos-train",
        "filepath": "argostrain/train.py",
        "commit_date": "2022-11-09T13:07:29Z",
        "message": "Typo fix"
    },
    {
        "repo_url": "github.com/argosopentech/argos-train",
        "filepath": "argostrain/train.py",
        "commit_date": "2022-11-09T12:58:05Z",
        "message": "Limit max dataset size before downloading\n\n- When using reversed data the datasets are downloaded before they are\n  filtered for being too long which defeats the purpose of filtering out\n  large datasets."
    },
    {
        "repo_url": "github.com/argosopentech/argos-train",
        "filepath": "argostrain/train.py",
        "commit_date": "2022-11-06T21:20:02Z",
        "message": "Bug fix"
    },
    {
        "repo_url": "github.com/argosopentech/argos-train",
        "filepath": "argostrain/train.py",
        "commit_date": "2022-11-06T21:00:09Z",
        "message": "Training script improvements"
    },
    {
        "repo_url": "github.com/argosopentech/argos-train",
        "filepath": "argostrain/train.py",
        "commit_date": "2022-10-23T13:58:42Z",
        "message": "Assert that there is data available\n\nhttps://forum.opennmt.net/t/traceback-assertionerror-while-training-in-vast-ai/5037"
    },
    {
        "repo_url": "github.com/argosopentech/argos-train",
        "filepath": "argostrain/train.py",
        "commit_date": "2022-10-22T16:41:43Z",
        "message": "Format"
    },
    {
        "repo_url": "github.com/argosopentech/argos-train",
        "filepath": "argostrain/train.py",
        "commit_date": "2022-10-22T16:41:11Z",
        "message": "Automate epochs count config"
    },
    {
        "repo_url": "github.com/argosopentech/argos-train",
        "filepath": "argostrain/train.py",
        "commit_date": "2022-10-22T16:09:49Z",
        "message": "Change averaging to 50k epochs"
    },
    {
        "repo_url": "github.com/argosopentech/argos-train",
        "filepath": "argostrain/train.py",
        "commit_date": "2022-09-15T13:20:44Z",
        "message": "Formatting"
    },
    {
        "repo_url": "github.com/argosopentech/argos-train",
        "filepath": "argostrain/train.py",
        "commit_date": "2022-07-24T13:08:40Z",
        "message": "Fix data_exists"
    },
    {
        "repo_url": "github.com/argosopentech/argos-train",
        "filepath": "argostrain/train.py",
        "commit_date": "2022-07-24T13:05:21Z",
        "message": "Bug fix"
    },
    {
        "repo_url": "github.com/argosopentech/argos-train",
        "filepath": "argostrain/train.py",
        "commit_date": "2022-07-24T13:01:38Z",
        "message": "Add data_exists option"
    },
    {
        "repo_url": "github.com/argosopentech/argos-train",
        "filepath": "argostrain/train.py",
        "commit_date": "2022-07-23T13:33:52Z",
        "message": "Bug fix"
    },
    {
        "repo_url": "github.com/argosopentech/argos-train",
        "filepath": "argostrain/train.py",
        "commit_date": "2022-07-23T13:32:48Z",
        "message": "Bug fix"
    },
    {
        "repo_url": "github.com/argosopentech/argos-train",
        "filepath": "argostrain/train.py",
        "commit_date": "2022-07-23T13:31:35Z",
        "message": "Add train function"
    },
    {
        "repo_url": "github.com/deeppavlov/dream",
        "filepath": "annotators/bot_emotion_classifier/server.py",
        "commit_date": "2023-11-16T08:59:51Z",
        "message": "Dream emotion dist (#568)\n\n* add dream_emotion dist with annotators\n\n* add component cards\n\n* fix codestyle flake8\n\n* fix codestyle\n\n* another codestyle fix\n\n* PR fixes\n\n* fixed batches\n\n* move preprocessing to annotator\n\n* fix codestyle black\n\n* minor fixes\n\n* removed unnecessary api_configs\n\n* changed type of annotator to bot utts annotator\n\n* fixed component file\n\n* fixed service.yml files\n\n* fixed formatters\n\n* fixed emotional response server and dist files\n\n* moved prompt into separate file\n\n* fixed annotators\n\n* fixed annotators again\n\n* fixed emotion response selector\n\n* fixed emotion dist and components\n\n* fixed codestyle\n\n* fixed codestyle again\n\n* fixed codestyle black\n\n* fixed naming of emotion response selector\n\n* fixed emotional_bot_response component\n\n* fixed emotion response selector server\n\n* added prev_services in annotator in pipeline_conf\n\n* fixed result formatting of bot_emo_cls\n\n* fixed codestyle\n\n* fixed codestyle again\n\n* changed description of emo_bot_response\n\n* added necessary component cards\n\n* fix: timeout\n\n* fix: timeout\n\n---------\n\nCo-authored-by: Dilyara Zharikova (Baymurzina) <dilyara.rimovna@gmail.com>"
    },
    {
        "repo_url": "github.com/deeppavlov/dream",
        "filepath": "annotators/bot_emotion_classifier/dscript_scheme_classifier.py",
        "commit_date": "2023-11-16T08:59:51Z",
        "message": "Dream emotion dist (#568)\n\n* add dream_emotion dist with annotators\n\n* add component cards\n\n* fix codestyle flake8\n\n* fix codestyle\n\n* another codestyle fix\n\n* PR fixes\n\n* fixed batches\n\n* move preprocessing to annotator\n\n* fix codestyle black\n\n* minor fixes\n\n* removed unnecessary api_configs\n\n* changed type of annotator to bot utts annotator\n\n* fixed component file\n\n* fixed service.yml files\n\n* fixed formatters\n\n* fixed emotional response server and dist files\n\n* moved prompt into separate file\n\n* fixed annotators\n\n* fixed annotators again\n\n* fixed emotion response selector\n\n* fixed emotion dist and components\n\n* fixed codestyle\n\n* fixed codestyle again\n\n* fixed codestyle black\n\n* fixed naming of emotion response selector\n\n* fixed emotional_bot_response component\n\n* fixed emotion response selector server\n\n* added prev_services in annotator in pipeline_conf\n\n* fixed result formatting of bot_emo_cls\n\n* fixed codestyle\n\n* fixed codestyle again\n\n* changed description of emo_bot_response\n\n* added necessary component cards\n\n* fix: timeout\n\n* fix: timeout\n\n---------\n\nCo-authored-by: Dilyara Zharikova (Baymurzina) <dilyara.rimovna@gmail.com>"
    },
    {
        "repo_url": "github.com/pengr/IKD-mmt",
        "filepath": "scripts/extract_image_labels.py",
        "commit_date": "2023-10-12T07:03:37Z",
        "message": "new commit"
    },
    {
        "repo_url": "github.com/pengr/IKD-mmt",
        "filepath": "scripts/extract_image_labels.py",
        "commit_date": "2023-10-12T06:56:07Z",
        "message": "Delete scripts directory"
    },
    {
        "repo_url": "github.com/pengr/IKD-mmt",
        "filepath": "scripts/extract_image_labels.py",
        "commit_date": "2022-10-08T14:10:12Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_parser.py",
        "commit_date": "2023-04-17T19:22:47Z",
        "message": "Updated for mtap 1.2.1 (#167)\n\n* Updated for mtap 1.2.1\n\n* Added a scaleout test\n\n* Tweaked the default scaleout settings a little\n\n* Made write-config options consistent"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_parser.py",
        "commit_date": "2023-03-16T18:44:26Z",
        "message": "Issues/138 (#144)\n\n* Fix for deepen error\n\n* Fix for sentences failing when CUDA available"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_parser.py",
        "commit_date": "2020-09-25T17:55:48Z",
        "message": "Updates for mtap 0.7.0"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_parser.py",
        "commit_date": "2020-09-10T12:42:38Z",
        "message": "Fixes to the stanza dependencies\n\nSwitched the method for creating the referential dependency graph from a queue over stanza dependencies to graph construction followed by breadth-first search."
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_parser.py",
        "commit_date": "2020-05-11T15:29:40Z",
        "message": "Features/dependencies (#55)\n\n* Fixed mipacq / ptb / concepts performance tests\n\n* Minor import cleanup\n\n* Last little bits of non-PHI performance test fixes\n\n* Stanza dependency parser, updates for MTAP\n\n* Removed test-results.yml files"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/deployment/_data_downloading.py",
        "commit_date": "2023-01-17T19:48:58Z",
        "message": "Update data to UMLS 2022AB (#121)"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/deployment/_data_downloading.py",
        "commit_date": "2023-01-13T02:05:50Z",
        "message": "biomedicus-client and updates for mtap compatibility (#120)\n\n* biomedicus-client and updates for mtap compatibility\n\n* Update build action to install biomedicus_client"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_selective_parser.py",
        "commit_date": "2023-04-17T19:22:47Z",
        "message": "Updated for mtap 1.2.1 (#167)\n\n* Updated for mtap 1.2.1\n\n* Added a scaleout test\n\n* Tweaked the default scaleout settings a little\n\n* Made write-config options consistent"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_selective_parser.py",
        "commit_date": "2023-03-16T18:44:26Z",
        "message": "Issues/138 (#144)\n\n* Fix for deepen error\n\n* Fix for sentences failing when CUDA available"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_selective_parser.py",
        "commit_date": "2023-02-07T16:20:12Z",
        "message": "Release/v3.0.0 rc.7 (#126)\n\n* Various\n\n- Made default and rtf-to-text deployment scripts consistent\n- Fixed issues with the rtf-to-text deployment script.\n- Cleaned up some issues with resource loading.\n- Made all processors use standard non-multiprocessing server by default.\n- Parameterized bi_lstm and selective dependencies to allow use of multiprocessing.\n\n* Clean up flake errors\n* Attempt to re-enable integration tests"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_selective_parser.py",
        "commit_date": "2023-01-13T02:05:50Z",
        "message": "biomedicus-client and updates for mtap compatibility (#120)\n\n* biomedicus-client and updates for mtap compatibility\n\n* Update build action to install biomedicus_client"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_selective_parser.py",
        "commit_date": "2022-12-08T22:34:26Z",
        "message": "Various changes to support mtap>=1.0rc3 (#119)\n\n- Pyproject.toml support\n- Works with Python 3.10 / newest pytorch"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_selective_parser.py",
        "commit_date": "2022-08-17T19:17:22Z",
        "message": "Fixed various RTF issues (#111)"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_selective_parser.py",
        "commit_date": "2021-09-07T15:49:30Z",
        "message": "Update for mtap 1.0"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_selective_parser.py",
        "commit_date": "2021-09-07T15:49:30Z",
        "message": "Various rtf and performance related changes"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_selective_parser.py",
        "commit_date": "2021-03-08T15:18:22Z",
        "message": "Batched sentences for dependency parsing"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_selective_parser.py",
        "commit_date": "2020-09-25T17:55:48Z",
        "message": "Updates for mtap 0.7.0"
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_selective_parser.py",
        "commit_date": "2020-09-10T12:42:38Z",
        "message": "Fixes to the stanza dependencies\n\nSwitched the method for creating the referential dependency graph from a queue over stanza dependencies to graph construction followed by breadth-first search."
    },
    {
        "repo_url": "github.com/nlpie/biomedicus",
        "filepath": "python/biomedicus/dependencies/stanza_selective_parser.py",
        "commit_date": "2020-07-20T17:38:18Z",
        "message": "deepen (#63)"
    },
    {
        "repo_url": "github.com/AlonEirew/CoreSearch",
        "filepath": "scripts/extract_min_span.py",
        "commit_date": "2023-03-23T09:48:18Z",
        "message": "Adding scripts to clean the mentions spans and release a new version CoreSearchV2 dataset"
    },
    {
        "repo_url": "github.com/princeton-nlp/align-mlm",
        "filepath": "preprocessing/xnli/convert_dataset_to_dependency.py",
        "commit_date": "2022-01-25T01:34:48Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/princeton-nlp/align-mlm",
        "filepath": "preprocessing/ner_pos/convert_dataset_to_dependency.py",
        "commit_date": "2022-01-25T01:34:48Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/princeton-nlp/align-mlm",
        "filepath": "preprocessing/sentence_retrieval/convert_dataset_to_dependency.py",
        "commit_date": "2022-01-25T01:34:48Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/princeton-nlp/align-mlm",
        "filepath": "preprocessing/dependency_parsing/convert_sentences_to_dependency.py",
        "commit_date": "2022-01-25T01:34:48Z",
        "message": "initial commit"
    },
    {
        "repo_url": "github.com/mo-xiaoxi/HDiff",
        "filepath": "SR_Finder.py",
        "commit_date": "2022-05-04T09:51:23Z",
        "message": "refactor: first commit"
    },
    {
        "repo_url": "github.com/mo-xiaoxi/HDiff",
        "filepath": "SR_Finder.py",
        "commit_date": "2022-05-04T09:50:43Z",
        "message": "refactor: first commit"
    },
    {
        "repo_url": "github.com/microsoft/ContextualSP",
        "filepath": "unified_parser_text_to_sql/step1_schema_linking.py",
        "commit_date": "2022-04-14T04:34:57Z",
        "message": "init unisar"
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/nqg_dataset.py",
        "commit_date": "2020-06-24T10:55:13Z",
        "message": "Build data pipeline for RepeatQ. Implement embedding layer."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/nqg_dataset.py",
        "commit_date": "2020-06-10T06:47:47Z",
        "message": "Multiple fixes  in eval mode of ASs2s."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/nqg_dataset.py",
        "commit_date": "2020-05-28T10:58:18Z",
        "message": "Manage to make predictions for SQuAD using HotpotQA pretrained model. Answer zones seem to not be working."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/nqg_dataset.py",
        "commit_date": "2020-05-27T02:27:46Z",
        "message": "Integrate SG-Deep-Question-Generation project and reverse engineer initial graph construction steps"
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/nqg_dataset.py",
        "commit_date": "2020-05-18T18:11:23Z",
        "message": "Integrate GDP2 as a model. Add evaluation and testing scripts for different datasets."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/nqg_dataset.py",
        "commit_date": "2020-05-04T08:51:39Z",
        "message": "Add handmade medical dataset and add testing scripts for both this latter and MedQuad."
    },
    {
        "repo_url": "github.com/arthurdeschamps/question-generation-nus-ids",
        "filepath": "data_processing/nqg_dataset.py",
        "commit_date": "2020-05-01T07:44:12Z",
        "message": "Add MedQuad as a usable dataset to train and evaluate models"
    },
    {
        "repo_url": "github.com/stanfordnlp/color-describer",
        "filepath": "third-party/stanza/stanza/util/resource.py",
        "commit_date": "2016-04-04T06:17:46Z",
        "message": "Merge commit '37db677eec1f83fae502d253f790c6f54e34abf5'"
    },
    {
        "repo_url": "github.com/stanfordnlp/color-describer",
        "filepath": "third-party/stanza/stanza/util/resource.py",
        "commit_date": "2016-02-02T01:36:53Z",
        "message": "Squashed 'third-party/stanza/' changes from 09f78e3..2e33ff2\n\n2e33ff2 add slack integration\nc1d7f48 added TensorBoardLogger\nae0e116 Tools for working with CodaLab\n7db1754 Document and test TensorBoard logging\n7eedffb Cast histogram values to ordinary floats\n\ngit-subtree-dir: third-party/stanza\ngit-subtree-split: 2e33ff2822d5f627fdd282645efa398721f0eb21"
    },
    {
        "repo_url": "github.com/stanfordnlp/color-describer",
        "filepath": "third-party/stanza/stanza/util/resource.py",
        "commit_date": "2016-01-28T23:56:59Z",
        "message": "Merge commit 'ec1b3ee0a9f85151068bb72462dd7275bb8c0f17' as 'third-party/stanza'"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/utils/stanza_funcs.py",
        "commit_date": "2021-07-08T14:03:48Z",
        "message": "Correcci\u00f3n bugs lematizador Stanza"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/utils/stanza_funcs.py",
        "commit_date": "2021-06-22T22:45:33Z",
        "message": "Cambios en la documentaci\u00f3n"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/utils/stanza_funcs.py",
        "commit_date": "2021-03-30T02:42:00Z",
        "message": "corecci\u00f3n errores autopep8 para funciones auxiliares"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/utils/stanza_funcs.py",
        "commit_date": "2021-03-09T23:40:51Z",
        "message": "stanza_func.py en formato pep8"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/utils/stanza_funcs.py",
        "commit_date": "2020-12-07T13:30:03Z",
        "message": "modificar scripts para que queden con formato pep8, utilizando autopep8"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/utils/stanza_funcs.py",
        "commit_date": "2020-09-15T15:38:10Z",
        "message": "terminar documentaci\u00f3n de scripts auxiliares"
    },
    {
        "repo_url": "github.com/ucd-dnp/ConTexto",
        "filepath": "contexto/utils/stanza_funcs.py",
        "commit_date": "2020-08-12T19:03:56Z",
        "message": "se renombr\u00f3 la librer\u00eda como contexto"
    },
    {
        "repo_url": "github.com/THUNLP-MT/Template-NMT",
        "filepath": "lexical_scripts/terminology_evaluation/evaluate_term_plain.py",
        "commit_date": "2022-11-06T11:54:16Z",
        "message": "init commit"
    },
    {
        "repo_url": "github.com/zsLin177/SRL-as-GP",
        "filepath": "supar/utils/tokenizer.py",
        "commit_date": "2021-04-14T07:28:25Z",
        "message": "Integrated tokenizer (#47)"
    },
    {
        "repo_url": "github.com/asyml/forte-wrappers",
        "filepath": "src/stanza/fortex/stanza/stanza_processor.py",
        "commit_date": "2022-06-24T02:56:50Z",
        "message": "Fix stanfordnlp_processor_test.py (#112)\n\n* Fix outdated stanza\n\n* Fix stanza test\n\nCo-authored-by: Suqi Sun <suqi.sun@petuum.com>"
    },
    {
        "repo_url": "github.com/asyml/forte-wrappers",
        "filepath": "src/stanza/fortex/stanza/stanza_processor.py",
        "commit_date": "2022-06-18T19:07:58Z",
        "message": "Support Bio NER using stanza processor (#111)\n\n* Support NER and Bio NER using Stanza processor\n\n* update stanfordnlp_processor_test.py\n\n* update standfordnlp_processor_test.py\n\n* support Bio NER using stanza processor\n\n* add test for bio ner in stanza processor\n\n* Minor changes in stanza_processor\n\n* Update stanfordnlp_processor_test.py\n\n* Fix minor build error\n\n* fix an autoformatting issue provided by Black"
    },
    {
        "repo_url": "github.com/asyml/forte-wrappers",
        "filepath": "src/stanza/fortex/stanza/stanza_processor.py",
        "commit_date": "2021-09-08T14:31:02Z",
        "message": "Change wrapper package from Forte to Fortex (#73)\n\n* Move to fortex\n\n* Fix import in test.\n\n* Rename package.\n\n* Fix examples.\n\n* Update init imports.\n\n* Update test strings.\n\n* Fix imports in elastics.\n\n* Fix imports in allennlp.\n\n* Typo\n\n* Fix class name in docs.\n\n* Clean up documentation.\n\n* Add docuemntation back.\n\n* Fixing documentation and gpu setups.\n\n* Black all setups.\n\n* Remove obsolete configs"
    },
    {
        "repo_url": "github.com/heqi2015/CA_GCN",
        "filepath": "data/MNLI/dependency_output_mnli.py",
        "commit_date": "2022-02-28T11:41:51Z",
        "message": "Add files via upload"
    },
    {
        "repo_url": "github.com/kenantang/petci",
        "filepath": "data/dataset.py",
        "commit_date": "2022-02-19T02:41:50Z",
        "message": "update data and code"
    },
    {
        "repo_url": "github.com/kenantang/petci",
        "filepath": "models/lstm/dataloader.py",
        "commit_date": "2022-02-19T02:41:50Z",
        "message": "update data and code"
    },
    {
        "repo_url": "github.com/quadrismegistus/lltk",
        "filepath": "lltk/text/utils.py",
        "commit_date": "2022-07-11T08:06:11Z",
        "message": "major"
    },
    {
        "repo_url": "github.com/quadrismegistus/lltk",
        "filepath": "lltk/text/utils.py",
        "commit_date": "2022-03-08T20:47:19Z",
        "message": "lots of changes"
    },
    {
        "repo_url": "github.com/quadrismegistus/lltk",
        "filepath": "lltk/text/utils.py",
        "commit_date": "2021-04-20T07:37:43Z",
        "message": "fixes"
    },
    {
        "repo_url": "github.com/quadrismegistus/lltk",
        "filepath": "lltk/text/utils.py",
        "commit_date": "2021-04-18T07:30:10Z",
        "message": "switching over to pkl from ft"
    },
    {
        "repo_url": "github.com/quadrismegistus/lltk",
        "filepath": "lltk/text/utils.py",
        "commit_date": "2021-03-10T16:43:51Z",
        "message": "fixes"
    },
    {
        "repo_url": "github.com/quadrismegistus/lltk",
        "filepath": "lltk/text/utils.py",
        "commit_date": "2021-03-10T14:38:58Z",
        "message": "readme ipynb"
    },
    {
        "repo_url": "github.com/quadrismegistus/lltk",
        "filepath": "lltk/text/utils.py",
        "commit_date": "2021-03-10T10:12:31Z",
        "message": "spacy"
    },
    {
        "repo_url": "github.com/quadrismegistus/lltk",
        "filepath": "lltk/text/utils.py",
        "commit_date": "2021-03-10T01:50:17Z",
        "message": "fixes"
    },
    {
        "repo_url": "github.com/quadrismegistus/lltk",
        "filepath": "lltk/text/utils.py",
        "commit_date": "2021-03-09T18:53:44Z",
        "message": "many fixes"
    },
    {
        "repo_url": "github.com/quadrismegistus/lltk",
        "filepath": "lltk/text/utils.py",
        "commit_date": "2021-03-09T15:51:13Z",
        "message": "bunch of new changes, temp. in new branch"
    },
    {
        "repo_url": "github.com/elisaF/subjective_discourse",
        "filepath": "code/analyses/question_classifier.py",
        "commit_date": "2020-07-17T10:18:11Z",
        "message": "add analyses folder"
    },
    {
        "repo_url": "github.com/eric-ai-lab/CPL",
        "filepath": "CPL/utils/_utils/vqa_clip_inference.py",
        "commit_date": "2022-11-12T18:51:37Z",
        "message": "utils"
    },
    {
        "repo_url": "github.com/lxucs/multilingual-mrc-isdg",
        "filepath": "isdg/preprocess.py",
        "commit_date": "2021-12-02T15:06:44Z",
        "message": "Save"
    },
    {
        "repo_url": "github.com/lisjin/hct",
        "filepath": "data_preprocess_en/proc_unmatch.py",
        "commit_date": "2021-08-11T20:45:04Z",
        "message": "Adjust for Task dataset"
    },
    {
        "repo_url": "github.com/lisjin/hct",
        "filepath": "data_preprocess_en/proc_unmatch.py",
        "commit_date": "2021-08-07T22:13:29Z",
        "message": "Fix BERT span seqs, label eval by train rules"
    },
    {
        "repo_url": "github.com/lisjin/hct",
        "filepath": "data_preprocess_en/proc_unmatch.py",
        "commit_date": "2021-07-11T03:07:02Z",
        "message": "Parse all examples, tree coverage over tags and spans"
    },
    {
        "repo_url": "github.com/lisjin/hct",
        "filepath": "data_preprocess_en/proc_unmatch.py",
        "commit_date": "2021-06-28T12:24:25Z",
        "message": "Process multiple data splits"
    },
    {
        "repo_url": "github.com/lisjin/hct",
        "filepath": "data_preprocess_en/proc_unmatch.py",
        "commit_date": "2021-06-25T22:13:00Z",
        "message": "Merge contiguous and ignore invalid spans"
    },
    {
        "repo_url": "github.com/lisjin/hct",
        "filepath": "data_preprocess_en/proc_unmatch.py",
        "commit_date": "2021-06-24T19:17:22Z",
        "message": "Apply spacy tokenization to full pipeline"
    },
    {
        "repo_url": "github.com/lisjin/hct",
        "filepath": "data_preprocess_en/proc_unmatch.py",
        "commit_date": "2021-06-18T15:38:30Z",
        "message": "Centralize functions, explore context tree coverage"
    },
    {
        "repo_url": "github.com/lisjin/hct",
        "filepath": "data_preprocess_en/proc_unmatch.py",
        "commit_date": "2021-06-16T03:44:22Z",
        "message": "Fix tokenization inconsistency by parser"
    },
    {
        "repo_url": "github.com/lisjin/hct",
        "filepath": "data_preprocess_en/proc_unmatch.py",
        "commit_date": "2021-06-14T21:02:39Z",
        "message": "Add dependency parsing, find subspan indices"
    },
    {
        "repo_url": "github.com/lisjin/hct",
        "filepath": "data_preprocess_en/proc_unmatch.py",
        "commit_date": "2021-06-13T19:40:03Z",
        "message": "Improve phrase preprocessing, update fuzzy match scripts"
    },
    {
        "repo_url": "github.com/lisjin/hct",
        "filepath": "data_preprocess_en/proc_unmatch.py",
        "commit_date": "2021-06-08T20:04:25Z",
        "message": "Constituency tree sub-phrases; ~0.36 -> ~0.62 chrF"
    },
    {
        "repo_url": "github.com/lisjin/hct",
        "filepath": "data_preprocess_en/proc_unmatch.py",
        "commit_date": "2021-06-06T19:23:09Z",
        "message": "Lemmatize and parse English"
    },
    {
        "repo_url": "github.com/tsproisl/textcomplexity",
        "filepath": "utils/run_stanza.py",
        "commit_date": "2024-01-22T20:00:26Z",
        "message": "Replace deprecated convert_dict() with write_doc2conll() (#6)"
    },
    {
        "repo_url": "github.com/tsproisl/textcomplexity",
        "filepath": "utils/run_stanza.py",
        "commit_date": "2021-07-16T06:14:46Z",
        "message": "Download stanza resources file, if it does not exist (issue #1)"
    },
    {
        "repo_url": "github.com/tsproisl/textcomplexity",
        "filepath": "utils/run_stanza.py",
        "commit_date": "2020-09-29T09:51:02Z",
        "message": "Utility script for parsing texts with stanza"
    },
    {
        "repo_url": "github.com/humlab/penelope",
        "filepath": "penelope/vendor/archive/stanza/utility.py",
        "commit_date": "2022-02-10T12:26:49Z",
        "message": "Safer extras"
    },
    {
        "repo_url": "github.com/humlab/penelope",
        "filepath": "tests/vendor/stanza_test.py",
        "commit_date": "2020-12-18T11:58:51Z",
        "message": "Tests overhaul - sorted and added GUI tests"
    },
    {
        "repo_url": "github.com/novoic/blabla",
        "filepath": "blabla/document_processor.py",
        "commit_date": "2020-07-21T11:41:00Z",
        "message": ":zap: Remove client.start() behaviour"
    },
    {
        "repo_url": "github.com/novoic/blabla",
        "filepath": "blabla/document_processor.py",
        "commit_date": "2020-07-15T15:56:25Z",
        "message": ":zap: Added flag to split sentences on newline or with stanza"
    },
    {
        "repo_url": "github.com/novoic/blabla",
        "filepath": "blabla/document_processor.py",
        "commit_date": "2020-05-21T13:53:45Z",
        "message": ":tada: Initial public commit"
    },
    {
        "repo_url": "github.com/futurulus/colors-in-context",
        "filepath": "third-party/stanza/stanza/util/resource.py",
        "commit_date": "2016-04-04T06:17:46Z",
        "message": "Merge commit '37db677eec1f83fae502d253f790c6f54e34abf5'"
    },
    {
        "repo_url": "github.com/futurulus/colors-in-context",
        "filepath": "third-party/stanza/stanza/util/resource.py",
        "commit_date": "2016-02-02T01:36:53Z",
        "message": "Squashed 'third-party/stanza/' changes from 09f78e3..2e33ff2\n\n2e33ff2 add slack integration\nc1d7f48 added TensorBoardLogger\nae0e116 Tools for working with CodaLab\n7db1754 Document and test TensorBoard logging\n7eedffb Cast histogram values to ordinary floats\n\ngit-subtree-dir: third-party/stanza\ngit-subtree-split: 2e33ff2822d5f627fdd282645efa398721f0eb21"
    },
    {
        "repo_url": "github.com/futurulus/colors-in-context",
        "filepath": "third-party/stanza/stanza/util/resource.py",
        "commit_date": "2016-01-28T23:56:59Z",
        "message": "Merge commit 'ec1b3ee0a9f85151068bb72462dd7275bb8c0f17' as 'third-party/stanza'"
    },
    {
        "repo_url": "github.com/lingmod-tue/stanza-api",
        "filepath": "main.py",
        "commit_date": "2020-06-22T14:14:56Z",
        "message": "Add support for specifying language packages"
    },
    {
        "repo_url": "github.com/lingmod-tue/stanza-api",
        "filepath": "main.py",
        "commit_date": "2020-06-20T20:31:00Z",
        "message": "simplification"
    },
    {
        "repo_url": "github.com/lingmod-tue/stanza-api",
        "filepath": "main.py",
        "commit_date": "2020-06-19T16:06:59Z",
        "message": "better ordering"
    },
    {
        "repo_url": "github.com/lingmod-tue/stanza-api",
        "filepath": "main.py",
        "commit_date": "2020-06-19T15:57:36Z",
        "message": "Basic working version, including Dockerfile."
    },
    {
        "repo_url": "github.com/KaijuML/dtt-multi-branch",
        "filepath": "onmt/metrics/hss.py",
        "commit_date": "2020-09-10T20:42:48Z",
        "message": "bugfix HS for empty sentences"
    },
    {
        "repo_url": "github.com/KaijuML/dtt-multi-branch",
        "filepath": "onmt/metrics/hss.py",
        "commit_date": "2020-09-10T20:40:07Z",
        "message": "bugfix HS for empty sentences"
    },
    {
        "repo_url": "github.com/KaijuML/dtt-multi-branch",
        "filepath": "onmt/metrics/hss.py",
        "commit_date": "2020-09-10T18:53:39Z",
        "message": "bugfix HS for empty sentences"
    },
    {
        "repo_url": "github.com/KaijuML/dtt-multi-branch",
        "filepath": "onmt/metrics/hss.py",
        "commit_date": "2020-09-10T09:18:45Z",
        "message": "HSA RL metric"
    },
    {
        "repo_url": "github.com/KaijuML/dtt-multi-branch",
        "filepath": "onmt/metrics/hss.py",
        "commit_date": "2020-09-10T06:05:40Z",
        "message": "HSS is working"
    },
    {
        "repo_url": "github.com/KaijuML/dtt-multi-branch",
        "filepath": "onmt/metrics/hss.py",
        "commit_date": "2020-09-10T06:04:10Z",
        "message": "HSS is working"
    },
    {
        "repo_url": "github.com/KaijuML/dtt-multi-branch",
        "filepath": "onmt/metrics/hss.py",
        "commit_date": "2020-09-09T16:31:35Z",
        "message": "fix RL"
    },
    {
        "repo_url": "github.com/KaijuML/dtt-multi-branch",
        "filepath": "onmt/metrics/hss.py",
        "commit_date": "2020-09-09T16:30:47Z",
        "message": "fix RL"
    },
    {
        "repo_url": "github.com/KaijuML/dtt-multi-branch",
        "filepath": "onmt/metrics/hss.py",
        "commit_date": "2020-09-09T16:17:06Z",
        "message": "HSS reinforcement learning metric"
    },
    {
        "repo_url": "github.com/KaijuML/dtt-multi-branch",
        "filepath": "onmt/metrics/hss.py",
        "commit_date": "2020-09-08T08:43:38Z",
        "message": "added possibility of training with RL"
    },
    {
        "repo_url": "github.com/KaijuML/dtt-multi-branch",
        "filepath": "data/dep_parsing_gpu.py",
        "commit_date": "2021-07-01T09:39:16Z",
        "message": "Solving issue #3"
    },
    {
        "repo_url": "github.com/KaijuML/dtt-multi-branch",
        "filepath": "data/dep_parsing_gpu.py",
        "commit_date": "2021-07-01T09:27:15Z",
        "message": "Solving issue #3"
    },
    {
        "repo_url": "github.com/HCDM/XRec",
        "filepath": "SAER/data/extact_exp_data.py",
        "commit_date": "2022-02-28T00:43:37Z",
        "message": "init SAER"
    },
    {
        "repo_url": "github.com/HCDM/XRec",
        "filepath": "CompExp/data/extract_exp_data.py",
        "commit_date": "2024-01-18T07:47:24Z",
        "message": "add missing config & extract trainer"
    },
    {
        "repo_url": "github.com/Lvchangze/SpikeBERT",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2023-10-07T10:13:04Z",
        "message": "first commit"
    },
    {
        "repo_url": "github.com/Lvchangze/snn",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2022-08-30T04:42:16Z",
        "message": "add snn attack but still cannot do the ann attack"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-09-08T14:10:51Z",
        "message": "Dockerise the application (#77)\n\n* feat: add Dockerfile for app deployement\n\n* Update .gitignore\n\n* fix: correct model path for Dockerfile\n\n* fix: allow deployement of docker with nginx"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-08-31T10:58:52Z",
        "message": "improved streamlit app"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-08-29T12:14:31Z",
        "message": "improved returned json"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-08-26T09:51:23Z",
        "message": "Merge remote-tracking branch 'origin/master'"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-08-26T09:50:33Z",
        "message": "improve streamlit app"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-08-25T14:10:10Z",
        "message": "Create DockerFile for deploying app  (#76)\n\n* feat: add Dockerfile for app deployement\n\n* Update .gitignore"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-08-24T14:13:52Z",
        "message": "added streamlit app"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-08-24T12:14:01Z",
        "message": "fixed bugs"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-08-09T08:45:38Z",
        "message": "load models in variables"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-08-02T07:03:48Z",
        "message": "fixed bugs"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-07-28T22:39:24Z",
        "message": "fixed bugs"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-07-26T10:05:03Z",
        "message": "fixed bug"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-07-26T10:02:56Z",
        "message": "fixed bug"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-07-26T10:01:55Z",
        "message": "fixed bug"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-07-26T08:05:23Z",
        "message": "added start and end date to tables"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-07-19T08:59:15Z",
        "message": "added rules, show result table"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-07-11T22:21:35Z",
        "message": "fixed pipeline"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-07-09T14:13:16Z",
        "message": "added location extractions from dataset of lacations"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/pipeline.py",
        "commit_date": "2021-07-09T11:44:13Z",
        "message": "added scripts, first pipeline"
    },
    {
        "repo_url": "github.com/BRGM/hubeau",
        "filepath": "inria/Code/download_stanza_model.py",
        "commit_date": "2021-08-25T14:10:10Z",
        "message": "Create DockerFile for deploying app  (#76)\n\n* feat: add Dockerfile for app deployement\n\n* Update .gitignore"
    },
    {
        "repo_url": "github.com/snu-mllab/Bayesian-Red-Teaming",
        "filepath": "textattack/shared/utils/install.py",
        "commit_date": "2023-07-09T14:20:25Z",
        "message": "submission file"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2021-02-01T09:10:02Z",
        "message": "Use stanza instead of #22"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2021-01-21T15:40:09Z",
        "message": "Fix up datapaths and log paths, cache vectors"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2021-01-21T09:12:13Z",
        "message": "Hook up rough version of a stanza Processor"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2021-01-14T13:40:09Z",
        "message": "Changed how logging is done"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2021-01-14T13:16:48Z",
        "message": "Added logging info and raise error #24"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2021-01-08T13:08:57Z",
        "message": "Move default data dir and create if not exists"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2020-12-16T16:08:57Z",
        "message": "Made predictor class and moved around data config\""
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2020-10-27T15:07:56Z",
        "message": "conform to pep8"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2020-10-07T13:52:17Z",
        "message": "split config in 2 files"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2020-09-18T13:52:56Z",
        "message": "Move nltk import into function that uses it"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2020-09-18T12:52:29Z",
        "message": "Added option to specify cfg file in scripts"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2020-07-15T11:33:45Z",
        "message": "Added possibility to predict on untokenized txt file"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2020-07-08T12:47:56Z",
        "message": "Use BERT embedder on the fly if not cached #1"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2020-07-06T15:22:14Z",
        "message": "Make training script work"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2020-07-06T14:07:36Z",
        "message": "Autopeped the code"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2020-07-06T12:49:38Z",
        "message": "Improved interface of predict script"
    },
    {
        "repo_url": "github.com/Filter-Bubble/e2e-Dutch",
        "filepath": "e2edutch/util.py",
        "commit_date": "2020-07-03T17:21:41Z",
        "message": "copied code from e2e and made some small adjustments"
    },
    {
        "repo_url": "github.com/Eyr3/TextCRS",
        "filepath": "textattacknew/shared/utils/install.py",
        "commit_date": "2023-08-01T12:42:37Z",
        "message": "init"
    }
]