[
    {
        "message_summary": "Fix deprecated arg issue",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Trainer check too. Check for dict or dataclass. Simplify, make config always AcceleratorConfig. Upstream to Trainer",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Deprecation and Replacement": "Fix deprecated arg issue"
            },
            {
                "Maintenance": "Trainer check too. Check for dict or dataclass. Simplify, make config always AcceleratorConfig. Upstream to Trainer"
            }
        ]
    },
    {
        "message_summary": "FIX [`PEFT` / `Trainer` ] Handle better peft + quantized compiled models (#29055)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Handle peft + compiled models. Added tests and made fixes based on suggestions. Clarified comments.",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "FEAT [`Trainer` / `bnb`]: Add RMSProp from `bitsandbytes` to HF `Trainer` (#29082)",
        "ptm_name": "RMSProp",
        "ptm_version": "",
        "ptm_problem_addressed": "Optimization algorithm for training neural networks",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Involves integrating new pre-trained models into existing systems, focusing on seamless incorporation and initial feature enhancement for these models."
            }
        ]
    },
    {
        "message_summary": "storing & logging gradient norm in trainer (#27326)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "report grad_norm during training\n\nsupport getting grad_norm from deepspeed",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Pertains to the upkeep, technical updates, and refactoring of pre-trained models to ensure ongoing reliability and efficiency."
            }
        ]
    },
    {
        "message_summary": "`auto_find_batch_size` isn't yet supported with DeepSpeed/FSDP. Raise error accordingly.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Update trainer.py",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Update of a Pre Trained Model:",
                "Deprecation and Replacement": "Removal of a Pre Trained model:"
            }
        ]
    },
    {
        "message_summary": "FIX [`Trainer` / tags]: Fix trainer + tags when users do not pass `\"tags\"` to `trainer.push_to_hub()` (#29009)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Fix trainer + tags when users do not pass \"tags\" to trainer.push_to_hub()"
            }
        ]
    },
    {
        "message_summary": "ENH [`AutoQuantizer`]: enhance trainer + not supported quant methods (#28991)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Enhanced trainer functionality and added support for quantization methods. Removed old logic and added a new version.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": null,
                "Task Specificity": null,
                "Community and Collaboration": null,
                "Maintenance": null,
                "Compliance and Standards": null,
                "Performance Improvements": null,
                "Compatibility": null,
                "Deprecation and Replacement": null,
                "Project Evolution": null,
                "Technical Debt and Simplification": null
            }
        ]
    },
    {
        "message_summary": "Clean up staging tmp checkpoint directory (#28848)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Clean up staging tmp checkpoint directory"
            }
        ]
    },
    {
        "message_summary": "Raise error when using `save_only_model` with `load_best_model_at_end` for DeepSpeed/FSDP (#28866)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Raise error when using `save_only_model` with `load_best_model_at_end` for DeepSpeed/FSDP",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Deprecation and Replacement": "Raise error when using `save_only_model` with `load_best_model_at_end` for DeepSpeed/FSDP"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Reduce GPU memory usage when using FSDP+PEFT (#28830)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Support FSDP+PEFT",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Performance Improvements": "Reduce GPU memory usage when using FSDP+PEFT"
            }
        ]
    },
    {
        "message_summary": "Fix `weights_only` (#28725)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Fix `weights_only` (#28725)"
            }
        ]
    },
    {
        "message_summary": "Add missing space in warning",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Add missing space in warning"
            }
        ]
    },
    {
        "message_summary": "Use save_safetensor to disable safe serialization for XLA",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Disabling safe serialization for XLA by using save_safetensor",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Disabling safe serialization for XLA by using save_safetensor"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix windows error with checkpoint race conditions"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Deprecation and Replacement": "Removal of a Pre Trained model"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Integration of DeepSpeed support for auto find batch size feature"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Fix trainer saving safetensors: metadata is None (#28219)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix trainer saving safetensors: metadata is None"
            }
        ]
    },
    {
        "message_summary": "In peft finetune, only the trainable parameters need to be saved to reduce storage size and save checkpoint saving time when using deepspeed for training.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Fix resuming from checkpoint issue with FSDP and FULL_STATE_DICT",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Fixed the issue related to resuming from a checkpoint when using FSDP with FULL_STATE_DICT. Updated tests and fixed related test cases.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fixed the issue related to resuming from a checkpoint when using FSDP with FULL_STATE_DICT. Updated tests and fixed related test cases."
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "make torch.load a bit safer",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to making torch.load safer."
            }
        ]
    },
    {
        "message_summary": "Fix bug with rotating checkpoints",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to fixing a bug with rotating checkpoints."
            }
        ]
    },
    {
        "message_summary": "Support PeftModel signature inspect",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Support PeftModel signature inspect. Use get_base_model() to get the base model.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Task Specificity": "Dedicated to developing and deploying task-specific enhancements or capabilities that exploit the unique strengths of pre-trained models."
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "[Hot-Fix][XLA] Re-enable broken _tpu_save for XLATensors (#27799)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Re-enable broken _tpu_save for XLATensors by explicitly moving to CPU",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Re-enable broken _tpu_save for XLATensors by explicitly moving to CPU"
            }
        ]
    },
    {
        "message_summary": "Add `persistent_workers` parameter to `TrainingArguments` (#27189)",
        "ptm_name": "TrainingArguments",
        "ptm_version": "N/A",
        "ptm_problem_addressed": "Added persistent_workers parameter",
        "update_details": "added param",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": null,
                "Task Specificity": null,
                "Community and Collaboration": null,
                "Maintenance": null,
                "Compliance and Standards": null,
                "Performance Improvements": null,
                "Compatibility": null,
                "Deprecation and Replacement": null,
                "Project Evolution": null,
                "Technical Debt and Simplification": null
            }
        ]
    },
    {
        "message_summary": "Docs: Fix broken cross-references",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Fixed broken cross-references in the documentation from `~transformer.` to `~transformers.`",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fixed broken cross-references in the documentation from `~transformer.` to `~transformers.`"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Remove the deprecated method `init_git_repo`",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "The deprecated method `init_git_repo` was removed.",
        "text": [
            {
                "Deprecation and Replacement": "The deprecated method `init_git_repo` was removed."
            }
        ]
    },
    {
        "message_summary": "Allow scheduler parameters",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Allow for scheduler kwargs. Formatting. Arguments checks, passing the tests. Black failed somehow.",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Integration Patterns": null,
                "Task Specificity": null,
                "Community and Collaboration": null,
                "Maintenance": null,
                "Compliance and Standards": null,
                "Performance Improvements": null,
                "Compatibility": null,
                "Deprecation and Replacement": null,
                "Project Evolution": null,
                "Technical Debt and Simplification": null
            }
        ]
    },
    {
        "message_summary": "Reproducible checkpoint for npu (#27208",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Save NPU's RNG states when saving a checkpoint and set after all the data skip phase when resuming training. Re-trigger ci",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Save NPU's RNG states when saving a checkpoint and set after all the data skip phase when resuming training."
            },
            {
                "Compatibility": "Re-trigger ci"
            }
        ]
    },
    {
        "message_summary": "Enable split_batches through TrainingArguments (#26798)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Enable split_batches functionality through TrainingArguments, additional dispatch_batches, default set to false, updates in docstring, and removal of capturewarnings change.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": null,
                "Task Specificity": null,
                "Community and Collaboration": null,
                "Maintenance": null,
                "Compliance and Standards": null,
                "Performance Improvements": null,
                "Compatibility": null,
                "Deprecation and Replacement": null,
                "Project Evolution": null,
                "Technical Debt and Simplification": null
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Deprecation and Replacement": "Removal of a Pre Trained model"
            }
        ]
    },
    {
        "message_summary": "Add `gradient_checkpointing_kwargs` in trainer and training arguments",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Added `gradient_checkpointing_kwargs` in trainer and training arguments. Added comments and tests, which initially failed but now pass.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": null,
                "Task Specificity": null,
                "Community and Collaboration": null,
                "Maintenance": null,
                "Compliance and Standards": null,
                "Performance Improvements": null,
                "Compatibility": null,
                "Deprecation and Replacement": null,
                "Project Evolution": null,
                "Technical Debt and Simplification": null
            }
        ]
    },
    {
        "message_summary": "Bring back `set_epoch` for Accelerate-based dataloaders (#26850)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Reintroduced `set_epoch` for Accelerate-based dataloaders, including working tests, fixed sampler, and other related adjustments.",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Fix resume_from_checkpoint bug",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Bug related to resume_from_checkpoint was fixed in the code.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Bug fix"
            }
        ]
    },
    {
        "message_summary": "Add many missing spaces in adjacent strings",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Extend Trainer to enable Ascend NPU to use the fused Adamw optimizer when training",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Performance Improvements": "Enable Ascend NPU to use the fused AdamW optimizer during training"
            }
        ]
    },
    {
        "message_summary": "Add torch `RMSProp` optimizer (#26425)",
        "ptm_name": "torch RMSProp optimizer",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": null,
                "Task Specificity": null,
                "Community and Collaboration": null,
                "Maintenance": null,
                "Compliance and Standards": null,
                "Performance Improvements": null,
                "Compatibility": null,
                "Deprecation and Replacement": null,
                "Project Evolution": null,
                "Technical Debt and Simplification": null
            }
        ]
    },
    {
        "message_summary": "[QUICK FIX LINK] Update trainer.py (#26293)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Refactor trainer + bnb logic",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Refactor trainer + bnb logic"
            }
        ]
    },
    {
        "message_summary": "Refactor decay_parameters production into its own function",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Refactor decay_parameters production into its own function"
            }
        ]
    },
    {
        "message_summary": "Fix eval accumulation when `accelerate` > 0.20.3",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Eval accumulation will never happen with `accelerate > 0.20.3`, so this change ensures that `sync_gradients` is ignored if accelerate is > 0.20.3",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Code fix for eval accumulation issue"
            }
        ]
    },
    {
        "message_summary": "Only main process should call _save on deepspeed zero3",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Fix err with FSDP",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to fixing errors and general maintenance."
            }
        ]
    },
    {
        "message_summary": "Revert frozen training arguments",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Reverted frozen training arguments",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Reverted frozen training arguments"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Arde/fsdp activation checkpointing (#25771)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Add warning for 8bit optimizers",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Add warning for 8bit optimizers"
            }
        ]
    },
    {
        "message_summary": "Revert \"change version (#25387)\" (#25573)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Deprecation and Replacement": "Revert of a previous change"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix issue with ratio evaluation steps and auto find batch size"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Fix set of model parallel in the Trainer when no GPUs are available",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Fix set of model parallel in the Trainer when no GPUs are available"
            }
        ]
    },
    {
        "message_summary": "Fix `.push_to_hub` and cleanup `get_full_repo_name` usage",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Fix delete all checkpoints when save_total_limit is set to 1",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix delete all checkpoints when save_total_limit is set to 1"
            }
        ]
    },
    {
        "message_summary": "Fix deepspeed load best model at end when the model gets sharded",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix deepspeed load best model at end when the model gets sharded"
            }
        ]
    },
    {
        "message_summary": "Added PeftModelForCausalLM to MODEL_FOR_CAUSAL_LM_MAPPING_NAMES dict and checked for PEFT model in compute_loss section",
        "ptm_name": "PeftModelForCausalLM",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Compute_loss in trainer failing to label shift for PEFT model when label smoothing enabled",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Added PeftModelForCausalLM to MODEL_FOR_CAUSAL_LM_MAPPING_NAMES dict and checked for PEFT model in compute_loss section"
            },
            {
                "Maintenance": "Compute_loss in trainer failing to label shift for PEFT model when label smoothing enabled"
            }
        ]
    },
    {
        "message_summary": "Add dispatch_batches to training arguments",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Integration Patterns": "Add dispatch_batches to training arguments"
            }
        ]
    },
    {
        "message_summary": "Expose `offload_buffers` parameter of `accelerate` to `PreTrainedModel.from_pretrained` method",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Expose offload_buffers parameter to from_pretrained method",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Expose offload_buffers parameter to from_pretrained method"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "[`gradient_checkpointing`] default to use it for torch 2.3 (#28538)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "default to use it for torch 2.3"
            }
        ]
    },
    {
        "message_summary": "Update all references to canonical models",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "FIX: Fix error with `logger.warning` + inline with recent refactor",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Update modeling_utils.py",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Commit log is related to fixing errors and aligning with recent refactoring."
            }
        ]
    },
    {
        "message_summary": "ENH [`AutoQuantizer`]: enhance trainer + not supported quant methods (#28991)",
        "ptm_name": "AutoQuantizer",
        "ptm_version": "added version 1.0",
        "ptm_problem_addressed": "Enhanced trainer functionality and added support for new quantization methods",
        "update_details": "Enhanced trainer functionality and added support for new quantization methods. Removed all old logic and added version",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": null,
                "Task Specificity": null,
                "Community and Collaboration": null,
                "Maintenance": null,
                "Compliance and Standards": null,
                "Performance Improvements": null,
                "Compatibility": null,
                "Deprecation and Replacement": null,
                "Project Evolution": null,
                "Technical Debt and Simplification": null
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Revert \"[WIP] Hard error when ignoring tensors.\" (#28898)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Reverted the commit related to hard error when ignoring tensors.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Deprecation and Replacement": "Reverted the commit related to hard error when ignoring tensors."
            }
        ]
    },
    {
        "message_summary": "Add missing None check for hf_quantizer (#28804)",
        "ptm_name": "Mistral",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Switch test model to Mistral",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Add missing None check for hf_quantizer"
            },
            {
                "Compatibility": "Switch test model to Mistral"
            },
            {
                "Deprecation and Replacement": "Replacement of a Pre Trained Model: Mistral"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "fix: corrected misleading log message in save_pretrained function",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Fix duplicate & unnecessary flash attention warnings",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to fixing duplicate & unnecessary flash attention warnings."
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Config: warning when saving generation kwargs in the model config",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Config: warning when saving generation kwargs in the model config"
            }
        ]
    },
    {
        "message_summary": "Clearer error for SDPA when explicitly requested",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Clearer error message for SDPA was implemented to provide a better message.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Clearer error message implementation for SDPA"
            }
        ]
    },
    {
        "message_summary": "Fix mismatching loading in from_pretrained with/without accelerate",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Fix mismatching behavior in from_pretrained with/without accelerate. Meaningful refactor, added test, fixed model on the hub, used tiny model, and style improvements.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix mismatching loading behavior in from_pretrained with/without accelerate. Meaningful refactor, added test, fixed model on the hub, used tiny model, and style improvements."
            }
        ]
    },
    {
        "message_summary": "Add llava fused modules support",
        "ptm_name": "llava fused modules",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of llava fused modules support"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Deprecation and Replacement": "Removal of a Pre Trained model"
            }
        ]
    },
    {
        "message_summary": "Use mmap option to load_state_dict",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Compatibility": "Commit log is related to updates ensuring compatibility with new systems or versions."
            }
        ]
    },
    {
        "message_summary": "Update modeling_utils.py",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Enable the possibility to skip quantization for some target modules",
        "ptm_name": "Awq",
        "ptm_version": "0.1.8",
        "ptm_problem_addressed": "Possibility to skip quantization for target modules",
        "update_details": "Added Awq 0.1.8 to enable skipping quantization for specific target modules",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": null,
                "Task Specificity": null,
                "Community and Collaboration": null,
                "Maintenance": null,
                "Compliance and Standards": null,
                "Performance Improvements": null,
                "Compatibility": null,
                "Deprecation and Replacement": null,
                "Project Evolution": null,
                "Technical Debt and Simplification": null
            }
        ]
    },
    {
        "message_summary": "Update the logger message with accordant weights_file_name",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model"
            }
        ]
    },
    {
        "message_summary": "Fix weights not properly initialized due to shape mismatch",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to fixing weights initialization due to shape mismatch."
            }
        ]
    },
    {
        "message_summary": "Update modeling_utils.py (#28127)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Correction in the docstring for PreTrainedModel.resize_token_embeddings to clarify the definition of the new_num_tokens parameter.",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Correction in the docstring for PreTrainedModel.resize_token_embeddings to clarify the definition of the new_num_tokens parameter."
            }
        ]
    },
    {
        "message_summary": "Fix `low_cpu_mem_usage` Flag Conflict with DeepSpeed Zero 3 in `from_pretrained` for Models with `keep_in_fp32_modules`\" (#27762)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Fix `from_pretrained` Logic for `low_cpu_mem_usage` with DeepSpeed Zero3",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Commit log is related to fixing a flag conflict in the `from_pretrained` logic for models with `keep_in_fp32_modules`. The update details mention fixing the logic for `low_cpu_mem_usage` with DeepSpeed Zero3."
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Fix SDPA dispatch & make SDPA CI compatible with torch<2.1.1",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Fix sdpa dispatch",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "make SDPA CI compatible with torch<2.1.1"
            }
        ]
    },
    {
        "message_summary": "Avoid class attribute `_keep_in_fp32_modules` being modified",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to avoiding modification of a class attribute '_keep_in_fp32_modules'"
            }
        ]
    },
    {
        "message_summary": "Better error message for bitsandbytes import",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Unrelated": {
                    "description": "Commit log is related to improving error messages for bitsandbytes import."
                }
            }
        ]
    },
    {
        "message_summary": "Fix `resize_token_embeddings` (#26861) (#26865)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "The method `resize_token_embeddings` now maintains `requires_grad` unchanged for all parameters in embeddings. Previously, it always set `requires_grad` to `True`. After the fix, `resize_token_embeddings` copies the `requires_grad` attribute from the old embeddings.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "The method `resize_token_embeddings` now maintains `requires_grad` unchanged for all parameters in embeddings. Previously, it always set `requires_grad` to `True`. After the fix, `resize_token_embeddings` copies the `requires_grad` attribute from the old embeddings."
            }
        ]
    },
    {
        "message_summary": "Add support for old gradient checkpointing method",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Support for the old gradient checkpointing method was added in the commit.",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to adding support for old gradient checkpointing method"
            }
        ]
    },
    {
        "message_summary": "Add fa2 support for `from_config`",
        "ptm_name": "FA-2",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of FA-2 support for `from_config`"
            }
        ]
    },
    {
        "message_summary": "Raise error when quantizing a quantized model",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Fix for 8bit serialization tests",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Fix CPU offload + disk offload tests",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Fix disk offload tests + weight sharing issues",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix CPU offload + disk offload tests"
            },
            {
                "Maintenance": "Fix disk offload tests + weight sharing issues"
            }
        ]
    },
    {
        "message_summary": "Fix import of torch.utils.checkpoint (#27155)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix import of torch.utils.checkpoint"
            }
        ]
    },
    {
        "message_summary": "Fix no split modules underlying modules (#27090)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Fixing no split modules underlying modules in modeling_utils.py file.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fixing no split modules underlying modules in modeling_utils.py file."
            }
        ]
    },
    {
        "message_summary": "Provide alternative when warning on use_auth_token",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Deprecation and Replacement": "Provide alternative when warning on use_auth_token"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Deprecation and Replacement": "Removal of a Pre Trained model"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Fix config silent copy in from_pretrained",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Fix config modeling utils, fix more, fix attn mask bug",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix config silent copy in from_pretrained, Fix config modeling utils, fix more, fix attn mask bug"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Change default `max_shard_size` to smaller value",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Default `max_shard_size` updated to 5GB.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Change default `max_shard_size` to smaller value"
            }
        ]
    },
    {
        "message_summary": "Make fsdp ram efficient loading optional",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Fix fa-2 import",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Fix fa-2 import"
            }
        ]
    },
    {
        "message_summary": "Add many missing spaces in adjacent strings",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Fix silent bug `keep_in_fp32` modules",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Fixed a silent bug related to `keep_in_fp32` modules. Added a common test for the fix.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fixed a silent bug related to `keep_in_fp32` modules. Added a common test for the fix."
            }
        ]
    },
    {
        "message_summary": "Final fixes for PEFT",
        "ptm_name": "PEFT",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Fixed issues with PEFT, addressed logger warnings, and made necessary adaptations based on suggestions. Removed a test.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fixed issues with PEFT, addressed logger warnings, and made necessary adaptations based on suggestions. Removed a test."
            }
        ]
    },
    {
        "message_summary": "[Logging] Change warning to info",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to changing warning to info in the logging system."
            }
        ]
    },
    {
        "message_summary": "Update modeling_utils.py",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Keep relevant weights in fp32 when `model._keep_in_fp32_modules` is set even when `accelerate` is not installed (#26225)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Fixed a bug where weight would not be kept in fp32. Addressed review comments and fixed tests.",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Fixed a bug where weight would not be kept in fp32. Addressed review comments and fixed tests."
            }
        ]
    },
    {
        "message_summary": "[FIX] resize_token_embeddings (#26102)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Fixed the resize_token_embeddings function. Added a test for resize_token_embeddings. Updated tests/test_modeling_common.py.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fixed the resize_token_embeddings function. Added a test for resize_token_embeddings. Updated tests/test_modeling_common.py."
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Fix PEFT + gradient checkpointing",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Fix PEFT + gradient checkpointing. Added disable RG, polished tests, and provided final explanations and tests.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix PEFT + gradient checkpointing. Added disable RG, polished tests, and provided final explanations and tests."
            }
        ]
    },
    {
        "message_summary": "Fix 4bit `num_parameters`",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Fixed 4bit `num_parameters` with a stronger check",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Fixed 4bit `num_parameters` with a stronger check"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Fix _resize_token_embeddings setting lm head size to 0 when enabled deepspeed zero3",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix _resize_token_embeddings setting lm head size to 0 when enabled deepspeed zero3"
            }
        ]
    },
    {
        "message_summary": "remove torch_dtype override",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "The torch_dtype override was removed from the modeling_utils.py file.",
        "text": [
            {
                "Maintenance": "The torch_dtype override was removed from the modeling_utils.py file."
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Generate: models with custom `generate()` return `True` in `can_generate()`",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Integration Patterns": "None",
                "Task Specificity": "None",
                "Community and Collaboration": "None",
                "Maintenance": "None",
                "Compliance and Standards": "None",
                "Performance Improvements": "None",
                "Compatibility": "None",
                "Deprecation and Replacement": "None",
                "Project Evolution": "None",
                "Technical Debt and Simplification": "None"
            }
        ]
    },
    {
        "message_summary": "Resolving Attribute error when using the FSDP ram efficient feature",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Fix bug",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Resolving Attribute error when using the FSDP ram efficient feature"
            },
            {
                "Maintenance": "Fix bug"
            }
        ]
    },
    {
        "message_summary": "Fix a typo in docsting",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "`Peft` `modules_to_save` support for peft integration (#27466)",
        "ptm_name": "Peft",
        "ptm_version": "Not specified",
        "ptm_problem_addressed": "Support for `modules_to_save` in Peft integration",
        "update_details": "Added support for `modules_to_save` in Peft integration.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": {
                    "repo_url": "github.com/huggingface/transformers",
                    "filepath": "src/transformers/integrations/peft.py",
                    "commit_date": "2023-11-14T09:32:57Z",
                    "message_summary": "`Peft` `modules_to_save` support for peft integration (#27466)",
                    "ptm_addition_details": {
                        "name": "Peft",
                        "version": "Not specified",
                        "problem_addressed": "Support for `modules_to_save` in Peft integration"
                    },
                    "update_details": "Added support for `modules_to_save` in Peft integration.",
                    "tasks": [
                        "peft integration"
                    ]
                }
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Removal of a Pre Trained model:": {}
            }
        ]
    },
    {
        "message_summary": "Final fixes",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Final fixes"
            }
        ]
    },
    {
        "message_summary": "minor typo fix in PeftAdapterMixin docs",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Revert \"Remove non-HF ExLlamaV2 loader (#5431)\"",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Deprecation and Replacement": "Revert of a removal of a non-HF ExLlamaV2 loader"
            }
        ]
    },
    {
        "message_summary": "Remove non-HF ExLlamaV2 loader",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Deprecation and Replacement": "Removal of a Pre Trained model:"
            }
        ]
    },
    {
        "message_summary": "Let exllama v1 models load safetensor loras",
        "ptm_name": "exllama v1",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Let exllama v1 models load safetensor loras"
            }
        ]
    },
    {
        "message_summary": "Intel Gpu support initialization",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Intel Gpu support initialization"
            }
        ]
    },
    {
        "message_summary": "transformers loader: multi-LoRAs support",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Compatibility": "Commit log is related to updates ensuring compatibility with new systems or versions."
            }
        ]
    },
    {
        "message_summary": "Exllamav2 lora support",
        "ptm_name": "Exllamav2",
        "ptm_version": "unknown",
        "ptm_problem_addressed": "LoRA support",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Allow --lora to use an absolute path",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Allow --lora to use an absolute path"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model"
            }
        ]
    },
    {
        "message_summary": "Lora fixes for AutoGPTQ (#2818)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Lora fixes for AutoGPTQ (#2818)"
            }
        ]
    },
    {
        "message_summary": "Fixed the param name when loading a LoRA using a model loaded in 4 or 8 bits",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fixed the param name when loading a LoRA using a model loaded in 4 or 8 bits"
            }
        ]
    },
    {
        "message_summary": "Update LoRA.py - avoid potential error",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Add LoRA support to ExLlama_HF",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition to the repository"
            }
        ]
    },
    {
        "message_summary": "Use pre-compiled python module for ExLlama",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Use pre-compiled python module for ExLlama"
            }
        ]
    },
    {
        "message_summary": "Reorganize model loading UI completely",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Reorganize model loading UI completely"
            }
        ]
    },
    {
        "message_summary": "Add LORA name instead of \"default\" in PeftModel",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Add LORA name instead of \"default\" in PeftModel"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Prevent unwanted log messages from modules",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Prevent unwanted log messages from modules"
            }
        ]
    },
    {
        "message_summary": "Load more than one LoRA with --lora, fix a bug",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Load more than one LoRA with --lora"
            },
            {
                "Maintenance": "Fix a bug"
            }
        ]
    },
    {
        "message_summary": "Add 4-bit LoRA support",
        "ptm_name": "LoRA",
        "ptm_version": "4-bit",
        "ptm_problem_addressed": "Support for 4-bit LoRA model",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new pre-trained model"
            }
        ]
    },
    {
        "message_summary": "Initial multi-LoRA support",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Initial multi-LoRA support"
            }
        ]
    },
    {
        "message_summary": "SD Api Pics extension, v.1.1 (#596)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Update of a Pre Trained Model:"
            },
            {
                "Deprecation and Replacement": "Replacement of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Handle unloading LoRA from dropdown menu icon",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Deprecation and Replacement": "Removal of a Pre Trained model:"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Added lora-dir, model-dir, and login auth arguments",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Integration Patterns": "Added lora-dir, model-dir, and login auth arguments"
            }
        ]
    },
    {
        "message_summary": "Clear cache while switching LoRAs",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Clear cache while switching LoRAs"
            }
        ]
    },
    {
        "message_summary": "Fix LoRA device map (attempt)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix LoRA device map (attempt)"
            }
        ]
    },
    {
        "message_summary": "Fix LoRA in CPU mode",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix LoRA in CPU mode"
            }
        ]
    },
    {
        "message_summary": "Make LoRAs work in 16-bit mode",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Make LoRAs work in 16-bit mode"
            }
        ]
    },
    {
        "message_summary": "Make custom LoRAs work by default",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Make custom LoRAs work by default"
            }
        ]
    },
    {
        "message_summary": "Don't include PeftModel every time",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Unrelated": "Commit log is related to excluding PeftModel from being included every time."
            }
        ]
    },
    {
        "message_summary": "Remove LoRA tab, move it into the Parameters menu",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Move LoRA tab into Parameters menu"
            }
        ]
    },
    {
        "message_summary": "Cleanup trainer with new data format",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Cleanup trainer with new data format"
            }
        ]
    },
    {
        "message_summary": "Merge PEFT feature branch (#11240)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": {},
                "Task Specificity": {},
                "Community and Collaboration": {},
                "Maintenance": {},
                "Compliance and Standards": {},
                "Performance Improvements": {},
                "Compatibility": {},
                "Deprecation and Replacement": {},
                "Project Evolution": {},
                "Technical Debt and Simplification": {}
            }
        ]
    },
    {
        "message_summary": "Add support for QLoRA",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition to the repository"
            }
        ]
    },
    {
        "message_summary": "Add support for Baize and LoRA models",
        "ptm_name": "Baize, LoRA",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of new pre-trained models"
            }
        ]
    },
    {
        "message_summary": "Allow trust_remote_code for tokenizers when loading AutoPeftModels",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Integration Patterns": "Allow trust_remote_code for tokenizers when loading AutoPeftModels"
            }
        ]
    },
    {
        "message_summary": "FIX Honor HF_HUB_OFFLINE mode if set by user",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "If users enable offline mode, don't perform checks for files on HF Hub, as they would fail.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "If users enable offline mode, don't perform checks for files on HF Hub, as they would fail."
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Move code quality fully to ruff"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Support AutoPeftModel for custom HF models",
        "ptm_name": "AutoPeftModel",
        "ptm_version": "",
        "ptm_problem_addressed": "Support for custom HF models",
        "update_details": "Added documentation",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Support AutoPeftModel for custom HF models"
            },
            {
                "Maintenance": "Added documentation"
            }
        ]
    },
    {
        "message_summary": "Introducing `AutoPeftModelForxxx` (#694)",
        "ptm_name": "AutoPeftModelForFeatureExtraction",
        "ptm_version": "v1",
        "ptm_problem_addressed": "Feature Extraction",
        "update_details": "Fixed rough issues, added tests, documentation, and comments.",
        "replacement_info": "Replace with `TypeError`",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Introducing a new pre-trained model for feature extraction."
            },
            {
                "Maintenance": "Fixed rough issues, added tests, documentation, and comments."
            },
            {
                "Deprecation and Replacement": "Replace with `TypeError`"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "add peft type constructor",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "add peft type constructor"
            }
        ]
    },
    {
        "message_summary": "[docs] PeftConfig and PeftModel (#1211)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "(minor) correct type annotation (#1166)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "feat: add type hints",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Integration Patterns": null,
                "Task Specificity": null,
                "Community and Collaboration": null,
                "Maintenance": null,
                "Compliance and Standards": null,
                "Performance Improvements": null,
                "Compatibility": null,
                "Deprecation and Replacement": null,
                "Project Evolution": null,
                "Technical Debt and Simplification": null
            }
        ]
    },
    {
        "message_summary": "feat: add type hints",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Integration Patterns": null,
                "Task Specificity": null,
                "Community and Collaboration": null,
                "Maintenance": null,
                "Compliance and Standards": null,
                "Performance Improvements": null,
                "Compatibility": null,
                "Deprecation and Replacement": null,
                "Project Evolution": null,
                "Technical Debt and Simplification": null
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "[docs] Community pipelines (#6929)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Community and Collaboration": "Commit log is related to improving community engagement or collaborative features."
            }
        ]
    },
    {
        "message_summary": "Pass device to enable_model_cpu_offload in maybe_free_model_hooks",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Pass device to enable_model_cpu_offload in maybe_free_model_hooks"
            }
        ]
    },
    {
        "message_summary": "None",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Remove `torch_dtype` in to() to end deprecation",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "Remove torch_dtype from to() and usage scripts. Remove old lora backend. Revert removal of old lora backend.",
        "text": [
            {
                "Deprecation and Replacement": "Remove torch_dtype from to() to end deprecation"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model"
            }
        ]
    },
    {
        "message_summary": "Harmonize HF environment variables + deprecate use_auth_token",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Deprecation and Replacement": "Deprecate use_auth_token"
            }
        ]
    },
    {
        "message_summary": "Post Release: v0.24.0 (#5985)",
        "ptm_name": "model_index.json",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": {
                    "PTM Addition Details": {
                        "Name": "model_index.json"
                    }
                }
            }
        ]
    },
    {
        "message_summary": "[feat] IP Adapters (author @okotaku ) (#5713)",
        "ptm_name": "IP Adapters",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Involves integrating new pre-trained models into existing systems, focusing on seamless incorporation and initial feature enhancement for these models."
            }
        ]
    },
    {
        "message_summary": "[Styling] stylify using ruff (#5841",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Unwrap models everywhere",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Unrelated": "Unwrap models everywhere"
            }
        ]
    },
    {
        "message_summary": "[Enhacne] Support maybe_raise_or_warn for peft (#5653)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Integration Patterns": null,
                "Task Specificity": null,
                "Community and Collaboration": null,
                "Maintenance": null,
                "Compliance and Standards": null,
                "Performance Improvements": null,
                "Compatibility": null,
                "Deprecation and Replacement": null,
                "Project Evolution": null,
                "Technical Debt and Simplification": null
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "[Custom Pipelines] Make sure that community pipelines can use repo revision (#5659)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "fix custom pipelines",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Community and Collaboration": "Commit log is related to improving community engagement or collaborative features."
            }
        ]
    },
    {
        "message_summary": "Fix incorrect loading of custom pipeline",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to fixing incorrect loading of custom pipeline."
            }
        ]
    },
    {
        "message_summary": "Fix memory issues in tests",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Fix memory issues. Set _offload_gpu_id and gpu offload id.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix memory issues in tests"
            }
        ]
    },
    {
        "message_summary": "[pipeline utils] sanitize pretrained_model_name_or_path (#5173)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Make sure the repo_id is valid before sending it to huggingface_hub to get a more understandable error message.",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Fix type annotation",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Fix type annotation"
            }
        ]
    },
    {
        "message_summary": "Bump version to Release 0.21",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Allow disabling `from_pretrained` tqdm",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Allow disabling `from_pretrained` tqdm"
            }
        ]
    },
    {
        "message_summary": "Fix model offload bug when key isn't present",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Fixed a bug related to model offload when a key is not present",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fixed a bug related to model offload when a key is not present"
            }
        ]
    },
    {
        "message_summary": "[Flax->PT] Fix flaky testing (#5011)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Fix flaky flax class name",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix flaky flax class name"
            }
        ]
    },
    {
        "message_summary": "[docs] Fix DiffusionPipeline.enable_sequential_cpu_offload docstring (#4952)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Make sure Flax pipelines can be loaded into PyTorch (#4971)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Flax pipelines can now be loaded into PyTorch",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Flax pipelines can now be loaded into PyTorch"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Fix a bug in `from_pretrained` when loading optional components",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to fixing a bug in 'from_pretrained' function."
            }
        ]
    },
    {
        "message_summary": "Fix `use_onnx` parameter usage in `from_pretrained` func and update `test_download_no_onnx_by_default` test (#4508)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Fix `use_onnx` parameter usage in `from_pretrained` function and update `test_download_no_onnx_by_default` test"
            }
        ]
    },
    {
        "message_summary": "Remove code snippets containing `is_safetensors_available()` (#4521)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "Code snippets containing `is_safetensors_available()` were removed.",
        "text": [
            {
                "Maintenance": "Code snippets containing `is_safetensors_available()` were removed."
            }
        ]
    },
    {
        "message_summary": "[docs] Remove attention slicing (#4518)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "Remove attention slicing",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "[Local loading] Correct bug with local files only",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Bug related to local file loading was fixed.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Bug fix"
            }
        ]
    },
    {
        "message_summary": "0.20.0dev0 (#4299)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Version 0.20.0dev0 was implemented. Style modifications were made.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Style modifications were made."
            }
        ]
    },
    {
        "message_summary": "[Docs] Fix from pretrained docs",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Fix from pretrained docs"
            }
        ]
    },
    {
        "message_summary": "Raise initial HTTPError if pipeline is not cached locally",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Refactor execution device & cpu offload",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "General CPU offload and execution device created. Boilerplate code removed. Corrections made to offload more pipelines. Style adjustments made in pipeline_utils.py",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Boilerplate code removed. Corrections made to offload more pipelines. Style adjustments made in pipeline_utils.py"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Typo correction in safetensors",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Typos corrected in safetensors across multiple files: pipeline_utils.py, loaders.py, modeling_utils.py",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Typos corrected in multiple files: pipeline_utils.py, loaders.py, modeling_utils.py"
            }
        ]
    },
    {
        "message_summary": "[docs] More API stuff (#3835)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Unrelated": "Commit log is related to documentation updates for API enhancements."
            }
        ]
    },
    {
        "message_summary": "Post 0.17.0 release",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Allow custom pipeline loading",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Allow custom pipeline loading"
            }
        ]
    },
    {
        "message_summary": "[WIP] Bugfix - Pipeline.from_pretrained is broken when the pipeline is partially downloaded (#3448)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Added bugfix using f strings.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Commit log is related to bugfix and maintenance of the codebase."
            }
        ]
    },
    {
        "message_summary": "Fix warning message pipeline loading",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix warning message pipeline loading"
            }
        ]
    },
    {
        "message_summary": "[docs] Improve safetensors docstring (#3368)",
        "ptm_name": "safetensors",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Improvement of documentation"
            }
        ]
    },
    {
        "message_summary": "[docs] Fix docstring (#3334)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Post release for 0.16.0",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Post release for 0.16.0"
            }
        ]
    },
    {
        "message_summary": "Throw deprecation warning for return_cached_folder (#3092)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Deprecation and Replacement": "Throw deprecation warning for return_cached_folder (#3092)"
            }
        ]
    },
    {
        "message_summary": "[Pipelines] Make sure that None functions are correctly not saved (#3080)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Make sure that None functions are correctly not saved"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "[Pipeline download] Improve pipeline download for index and passed components",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Integration Patterns": "Improve pipeline download for index and passed components"
            }
        ]
    },
    {
        "message_summary": "Add option to set dtype in pipeline.to() method",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Added the ability to set dtype in the pipeline.to() method. Added test_to_dtype to verify pipe.to(fp16) functionality.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Added the ability to set dtype in the pipeline.to() method."
            }
        ]
    },
    {
        "message_summary": "Fix typos",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix typos"
            }
        ]
    },
    {
        "message_summary": "Remove send_telemetry",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "Send telemetry functionality was removed from the pipeline loading process.",
        "text": [
            {
                "Maintenance": "Send telemetry functionality was removed from the pipeline loading process."
            }
        ]
    },
    {
        "message_summary": "None",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Add cache_dir to docs",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Improve docs",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Add cache_dir to docs"
            },
            {
                "Maintenance": "Improve docs"
            }
        ]
    },
    {
        "message_summary": "Fix ONNX checkpoint loading",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Fix ONNX checkpoint loading"
            }
        ]
    },
    {
        "message_summary": "is_safetensors_compatible refactor",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "refactor"
            }
        ]
    },
    {
        "message_summary": "Fix deprecation warning (#2426)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Deprecation and Replacement": "Fix deprecation warning (#2426)"
            }
        ]
    },
    {
        "message_summary": "Replace flake8 with ruff and update black",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "Flake8 was replaced with Ruff for linting purposes.",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Flake8 was replaced with Ruff for linting purposes."
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "[from_pretrained] only load config one time (#2131)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "New tokenizer base model for the current dev branch of transformers",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "New tokenizer base model added for the current dev branch of transformers",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "New tokenizer base model added for the current dev branch of transformers"
            }
        ]
    },
    {
        "message_summary": "Support streaming output on generate",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Support streaming output on generate"
            }
        ]
    },
    {
        "message_summary": "Add option to share Gradio demo publicly",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Integration Patterns": "Add option to share Gradio demo publicly"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Remove LLaMA download code, as a precaution",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Deprecation and Replacement": "Removal of a Pre Trained model:"
            }
        ]
    },
    {
        "message_summary": "Enable inference on CPU and Mac GPU using pytorch support for MPS",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Enable inference on CPU and Mac GPU using pytorch support for MPS"
            }
        ]
    },
    {
        "message_summary": "Update generate.py",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Adapting to the input function, a text box for inputting content has been added.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Adapting to the input function, a text box for inputting content has been added."
            }
        ]
    },
    {
        "message_summary": "add Gradio interface to generate.py",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Involves integrating new pre-trained models into existing systems, focusing on seamless incorporation and initial feature enhancement for these models."
            }
        ]
    },
    {
        "message_summary": "Update alpaca-lora to use transformers main branch",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Expand sampling in generate.py for new test",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Expand sampling in generate.py for new test"
            }
        ]
    },
    {
        "message_summary": "Update README.md with new checkpoint details",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update README.md with new checkpoint details"
            }
        ]
    },
    {
        "message_summary": "FIX Bug in prompt learning after disabling adapter",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Bug fixed in the prepare method after using the disable_adapter context, ensuring correct generations post context exit.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Bug fixed in the prepare method after using the disable_adapter context, ensuring correct generations post context exit."
            }
        ]
    },
    {
        "message_summary": "FIX: Multitask prompt tuning with other tuning init (#1144)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "FIX [`PromptTuning`] Simple fix for transformers >= 4.38",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "DOC How to freeze adapter after set_adapter call",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Documentation update"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Add positional args to PeftModelForCausalLM.generate",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Compatibility": "Add positional args to PeftModelForCausalLM.generate"
            }
        ]
    },
    {
        "message_summary": "FEAT Add Poly Adapter (#1129)",
        "ptm_name": "Poly Adapter",
        "ptm_version": "Not specified",
        "ptm_problem_addressed": "Implementation of the Poly (Polytropon) adapter.",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "No",
                "Task Specificity": "Yes",
                "Community and Collaboration": "No",
                "Maintenance": "No",
                "Compliance and Standards": "No",
                "Performance Improvements": "No",
                "Compatibility": "No",
                "Deprecation and Replacement": "No",
                "Project Evolution": "No",
                "Technical Debt and Simplification": "No"
            }
        ]
    },
    {
        "message_summary": "fix `prepare_inputs_for_generation` logic for Prompt Learning methods",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to fixing logic for Prompt Learning methods."
            }
        ]
    },
    {
        "message_summary": "FIX Issues with transformers 4.36",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Issues with transformers 4.36"
            }
        ]
    },
    {
        "message_summary": "FIX Pass HF token when calling PeftModel.from_pretrained",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Pass HF token when calling PeftModel.from_pretrained"
            }
        ]
    },
    {
        "message_summary": "FEAT: Make safe serialization the default one (#1088)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Add implementation of LyCORIS LoKr for SD&SDXL models",
        "ptm_name": "LyCORIS LoKr",
        "ptm_version": "",
        "ptm_problem_addressed": "KronA-like adapter",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new pre-trained model"
            }
        ]
    },
    {
        "message_summary": "FIX Don't assume model_config contains model_type (#1012)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fixing issues in the codebase"
            }
        ]
    },
    {
        "message_summary": "Fix word_embeddings match for deepspeed wrapped model",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to fixing word_embeddings match for deepspeed wrapped model."
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition to the repository"
            }
        ]
    },
    {
        "message_summary": "DOC: PeftModel save_pretrained docstring (#881) (#888)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            },
            {
                "Deprecation and Replacement": "Replacement of a Pre Trained Model:"
            },
            {
                "Deprecation and Replacement": "Removal of a Pre Trained model:"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Update docstring of PeftModel.from_pretrained (#799)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Reword docstring to not be LoRA-specific",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Reword docstring of PeftModel.from_pretrained to not be LoRA-specific"
            }
        ]
    },
    {
        "message_summary": "Support XPU adapter loading",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Compatibility": "Support XPU adapter loading"
            }
        ]
    },
    {
        "message_summary": "[Patch] patch trainable params for 4bit layers (#733)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Add disabling TP behavior for Llama2",
        "ptm_name": "Llama2",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Disabling TP behavior added with comments, adapted from new changes of transformers PR",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": {
                    "PTM Name": "Llama2",
                    "Action": "Disabling TP behavior added with comments",
                    "Update Details": "Adapted from new changes of transformers PR"
                }
            }
        ]
    },
    {
        "message_summary": "Fix the param count when using 4-bit bnb",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Fix subfolder issue",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to fixing a subfolder issue."
            }
        ]
    },
    {
        "message_summary": "better hub kwargs management",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "better hub kwargs management"
            }
        ]
    },
    {
        "message_summary": "Support AutoPeftModel for custom HF models",
        "ptm_name": "AutoPeftModel",
        "ptm_version": "",
        "ptm_problem_addressed": "Custom HF models",
        "update_details": "Added documentation",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": {
                    "repo_url": "github.com/huggingface/peft",
                    "filepath": "src/peft/peft_model.py",
                    "commit_date": "2023-07-15T12:18:34Z",
                    "message_summary": "Support AutoPeftModel for custom HF models",
                    "ptm_addition_details": {
                        "name": "AutoPeftModel",
                        "problem_addressed": "Custom HF models"
                    },
                    "update_details": "Added documentation"
                }
            }
        ]
    },
    {
        "message_summary": "FIX: base_model_torch_dtype when using model.half() after init",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "FIX: base_model_torch_dtype when using model.half() after init"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fixing issues with PTUN and prompt tuning generation."
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "feat: Add PeftModelForQuestionAnswering (#473)",
        "ptm_name": "PeftModelForQuestionAnswering",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Added first try of supporting QuestionAnswering, updated example to be correct, added changes from PR 404, added missing mapping for task type, removed unrelated code, ran make style",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": {
                    "repo_url": "github.com/huggingface/peft",
                    "filepath": "src/peft/peft_model.py",
                    "commit_date": "2023-06-16T11:23:58Z",
                    "activities": [
                        {
                            "name": "PeftModelForQuestionAnswering",
                            "action": "Addition",
                            "details": "Added first try of supporting QuestionAnswering, updated example to be correct, added changes from PR 404, added missing mapping for task type, removed unrelated code, ran make style"
                        }
                    ]
                }
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Fix typo at peft_model.py",
        "ptm_name": "N/A",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Fixed a typo in the description from `imputs_embeds` to `inputs_embeds`",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fixed a typo in the description from `imputs_embeds` to `inputs_embeds`"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Update of a Pre Trained Model"
            }
        ]
    },
    {
        "message_summary": "Add thousands separator in print_trainable_parameters",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Add library name to model card",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Add library name to model card"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fixed problem with duplicate same code."
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Add gradient checkpointing check",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Automatic input enable gradients were added when calling `get_peft_model`. Improved style and added a better check along with a 4bit check.",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "DOC How to freeze adapter after set_adapter call",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Unrelated": "No significant Pre-Trained Models activities mentioned."
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Added missing getattr methods for mixed model",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Added missing getattr methods for mixed model"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new pre-trained model"
            }
        ]
    },
    {
        "message_summary": "Upgrade gradio to 4.17",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Upgrade gradio to 4.17"
            }
        ]
    },
    {
        "message_summary": "Fix the pooling method of BGE embedding model",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix the pooling method of BGE embedding model"
            }
        ]
    },
    {
        "message_summary": "Support Model Yuan2.0, a new generation Fundamental Large Language Model developed by IEIT System",
        "ptm_name": "Model Yuan2.0",
        "ptm_version": "2.0",
        "ptm_problem_addressed": "Fundamental Large Language Model",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Support Model Yuan2.0, a new generation Fundamental Large Language Model developed by IEIT System"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Update of a Pre Trained Model"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Task Specificity": "Replacement of a Pre Trained Model"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Add support for iei yuan2.0",
        "ptm_name": "iei yuan2.0",
        "ptm_version": "2.0",
        "ptm_problem_addressed": "Integration with IEITYuan",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new pre-trained model"
            }
        ]
    },
    {
        "message_summary": "Add TenyxChat-7B-v1 model",
        "ptm_name": "TenyxChat-7B-v1",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new pre-trained model"
            }
        ]
    },
    {
        "message_summary": "feat: use variables OPENAI_MODEL_LIST",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "feat: use variables OPENAI_MODEL_LIST"
            }
        ]
    },
    {
        "message_summary": "Add `Notus` support",
        "ptm_name": "Notus",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Add `Notus` support"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "update a new sota model on MT-Bench which touch an 8.8 scores.",
        "ptm_name": "MT-Bench",
        "ptm_version": "latest",
        "ptm_problem_addressed": "State-of-the-art model for machine translation",
        "update_details": "The commit includes an update to a new state-of-the-art model on MT-Bench, achieving a score of 8.8.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Performance Improvements": "Achieved a score of 8.8 on MT-Bench"
            }
        ]
    },
    {
        "message_summary": "Fix the problem of not using the decoding method corresponding to the base model in peft mode",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix the problem of not using the decoding method corresponding to the base model in peft mode"
            }
        ]
    },
    {
        "message_summary": "Add new models (Perplexity, gemini) & Separate GPT versions",
        "ptm_name": "Perplexity, gemini",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Add new models (Perplexity, gemini) & Separate GPT versions"
            }
        ]
    },
    {
        "message_summary": "Import `accelerate` locally to avoid it as a strong dependency",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Import `accelerate` locally to avoid it as a strong dependency"
            }
        ]
    },
    {
        "message_summary": "Add bagel model adapter",
        "ptm_name": "bagel",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new pre-trained model adapter"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Update of a Pre Trained Model"
            }
        ]
    },
    {
        "message_summary": "Add support for CatPPT (#2840)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Integration of new pre-trained models into existing systems."
            }
        ]
    },
    {
        "message_summary": "Add SOLAR-10.7b Instruct Model",
        "ptm_name": "SOLAR-10.7b Instruct Model",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a Pre Trained Model"
            }
        ]
    },
    {
        "message_summary": "Update UI and new models",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model"
            }
        ]
    },
    {
        "message_summary": "Add Hermes 2.5 [fixed]",
        "ptm_name": "Hermes 2.5",
        "ptm_version": "2.5",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a Pre Trained Model"
            }
        ]
    },
    {
        "message_summary": "add support for Chinese-LLaMA-Alpaca",
        "ptm_name": "Chinese-LLaMA-Alpaca",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new pre-trained model"
            }
        ]
    },
    {
        "message_summary": "Add Microsoft/Orca-2-7b and update model support docs",
        "ptm_name": "Microsoft/Orca-2-7b",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of Microsoft/Orca-2-7b to the repository."
            }
        ]
    },
    {
        "message_summary": "add chatglm3 conv template support in conversation.py (#2622)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Integration of new pre-trained models into existing systems."
            }
        ]
    },
    {
        "message_summary": "added support for CodeGeex(2)",
        "ptm_name": "CodeGeex",
        "ptm_version": "2",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "added support for CodeGeex(2)"
            }
        ]
    },
    {
        "message_summary": "Improve Azure OpenAI interface",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Improve Azure OpenAI interface"
            }
        ]
    },
    {
        "message_summary": "openchat 3.5 model support",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Integration of new pre-trained models into existing systems."
            }
        ]
    },
    {
        "message_summary": "Fix for OpenOrcaAdapter to return correct conversation template",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix for OpenOrcaAdapter to return correct conversation template"
            }
        ]
    },
    {
        "message_summary": "Support model AquilaChat2",
        "ptm_name": "AquilaChat2",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Support model AquilaChat2"
            }
        ]
    },
    {
        "message_summary": "Update qwen and add pygmalion",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Add trust_remote_code=True in BaseModelAdapter",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Add trust_remote_code=True in BaseModelAdapter"
            }
        ]
    },
    {
        "message_summary": "Add Lemur model",
        "ptm_name": "Lemur",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Add Lemur model"
            }
        ]
    },
    {
        "message_summary": "Update README.md (vicuna-v1.3 -> vicuna-1.5)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "README.md version updated from vicuna-v1.3 to vicuna-1.5",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Deprecation and Replacement": "Replacement of a Pre Trained Model: README.md version updated from vicuna-v1.3 to vicuna-1.5"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Add Xwin-LM V0.1, V0.2 support",
        "ptm_name": "Xwin-LM",
        "ptm_version": "V0.1, V0.2",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of Xwin-LM V0.1, V0.2 support"
            }
        ]
    },
    {
        "message_summary": "Add airoboros_v3 chat template (llama-2 format)",
        "ptm_name": "airoboros_v3",
        "ptm_version": "llama-2",
        "ptm_problem_addressed": "Chat template addition",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new pre-trained model template to the repository."
            }
        ]
    },
    {
        "message_summary": "Revert \"Improve Support for Mistral-Instruct\" (#2552)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Deprecation and Replacement": "Removal of a Pre Trained model:"
            }
        ]
    },
    {
        "message_summary": "Improve Support for Mistral-Instruct (#2547)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Update of a Pre Trained Model:"
            },
            {
                "Deprecation and Replacement": "Replacement of a Pre Trained Model:"
            },
            {
                "Deprecation and Replacement": "Removal of a Pre Trained model:"
            }
        ]
    },
    {
        "message_summary": "Add Zephyr 7B Alpha",
        "ptm_name": "Zephyr 7B Alpha",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Add Zephyr 7B Alpha"
            }
        ]
    },
    {
        "message_summary": "Add ExllamaV2 Inference Framework Support.",
        "ptm_name": "ExllamaV2",
        "ptm_version": "",
        "ptm_problem_addressed": "Inference Framework Support",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Add ExllamaV2 Inference Framework Support."
            }
        ]
    },
    {
        "message_summary": "Add Mistral AI instruction template",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Add Mistral AI instruction template"
            }
        ]
    },
    {
        "message_summary": "Merge google/flan based adapters: T5Adapter, CodeT5pAdapter, FlanAdapter",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Integration Patterns": "Merge google/flan based adapters: T5Adapter, CodeT5pAdapter, FlanAdapter"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Add falcon 180B chat conversation template",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new integration pattern"
            }
        ]
    },
    {
        "message_summary": "Add support for Phind-CodeLlama models",
        "ptm_name": "Phind-CodeLlama",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new pre-trained model"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Update of a Pre Trained Model"
            }
        ]
    },
    {
        "message_summary": "Add support for baichuan2 models",
        "ptm_name": "baichuan2",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new pre-trained model"
            }
        ]
    },
    {
        "message_summary": "Added google/flan models",
        "ptm_name": "google/flan",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Added google/flan models"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fixed AutoModelForSeq2SeqLM when loading T5 compression model"
            }
        ]
    },
    {
        "message_summary": "Spicyboros + airoboros 2.2 template update.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Remove hardcode flash-attn disable setting",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Removal of a Pre Trained model:"
            }
        ]
    },
    {
        "message_summary": "Add Code Llama Support and Fix empty system prompt for llama 2",
        "ptm_name": "Code Llama",
        "ptm_version": "2",
        "ptm_problem_addressed": "Support added for Code Llama",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Add Code Llama Support"
            },
            {
                "Maintenance": "Fix empty system prompt for llama 2"
            }
        ]
    },
    {
        "message_summary": "Add new model to the arena",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new model to the arena"
            }
        ]
    },
    {
        "message_summary": "Add conversation support for VMware's OpenLLaMa OpenInstruct models",
        "ptm_name": "OpenLLaMa OpenInstruct",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Add conversation support for VMware's OpenLLaMa OpenInstruct models"
            }
        ]
    },
    {
        "message_summary": "[Minor] Style clean up & Fix embeding (#2272)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Style clean up & Fix embedding"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Performance Improvements": "Update of a Pre Trained Model"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Add support for Vigogne models",
        "ptm_name": "Vigogne",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new pre-trained model: Vigogne"
            }
        ]
    },
    {
        "message_summary": "Add Llama2-Chinese model support",
        "ptm_name": "Llama2-Chinese",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new pre-trained model"
            }
        ]
    },
    {
        "message_summary": "Add Support of bge model family for embedding generation",
        "ptm_name": "bge model family",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Add Support of bge model family for embedding generation"
            }
        ]
    },
    {
        "message_summary": "Modify vllm compatible empty special token, and revise qwen",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Modify vllm compatible empty special token"
            },
            {
                "Unrelated": "revise qwen"
            }
        ]
    },
    {
        "message_summary": "Add WizardCoder model support",
        "ptm_name": "WizardCoder",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Add WizardCoder model support"
            }
        ]
    },
    {
        "message_summary": "None",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Add per-step token utilization to tensorboard and progress tracker.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition to the repository"
            },
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Add LLM Text Encoder",
        "ptm_name": "LLM Text Encoder",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new pre-trained model: LLM Text Encoder"
            }
        ]
    },
    {
        "message_summary": "Workaround for torch bug to unblock mixtral fine-tuning",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Workaround for torch bug to unblock mixtral fine-tuning"
            }
        ]
    },
    {
        "message_summary": "Add support for Phi-1 and Phi 1.5",
        "ptm_name": "Phi-1, Phi 1.5",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition to the repository"
            }
        ]
    },
    {
        "message_summary": "Remove quantization config before saving dequantized weights to disk",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Removal of a Pre Trained model:"
            }
        ]
    },
    {
        "message_summary": "Move reusable LLM model methods to utility functions",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Move reusable LLM model methods to utility functions"
            }
        ]
    },
    {
        "message_summary": "Add support for dequantizing 4-bit bitsandbytes base models into fp16",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Add support for dequantizing 4-bit bitsandbytes base models into fp16"
            }
        ]
    },
    {
        "message_summary": "Add NEFTune implementation for Noised Embedding Instruction Fine-Tuning support",
        "ptm_name": "NEFTune",
        "ptm_version": "",
        "ptm_problem_addressed": "Noised Embedding Instruction Fine-Tuning support",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Add NEFTune implementation for Noised Embedding Instruction Fine-Tuning support"
            }
        ]
    },
    {
        "message_summary": "Cleanup: Use existing HFTokenizer to consolidate manual pad token setting.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Pinning Transformers to not include version 4.35.0 because it breaks a method in PEFT.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Define initialize_llm() and generate() methods. Remove extra logging in llm.py (#3711)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Remove extra logging in llm.py"
            }
        ]
    },
    {
        "message_summary": "Fix: Prevent memory from ballooning during post-training evaluation",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Prevent memory from ballooning during post-training evaluation"
            }
        ]
    },
    {
        "message_summary": "Retry LLM model downloads from HF Hub with exponential backoff when there is a Read timeout",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Retry LLM model downloads from HF Hub with exponential backoff when there is a Read timeout"
            }
        ]
    },
    {
        "message_summary": "Dynamically set `max_new_tokens` based on output feature length, GMSL and model window size",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "QoL: Only log generation config being used once at inference time",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Only log generation config being used once at inference time"
            }
        ]
    },
    {
        "message_summary": "[BUGFIX] Ensure that full base models and not only adapter weights get saved when merge_and_unload is set",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Fix dynamic generation config load during `model.predict` (#3666)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix dynamic generation config load during `model.predict`"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix eos_token and pad_token"
            }
        ]
    },
    {
        "message_summary": "[MAINTENANCE] Partially reconcile type hints, fix some warnings, and fix comments in parts of the codebase. (#3673)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Partially reconcile type hints, fix some warnings, and fix comments in parts of the codebase."
            }
        ]
    },
    {
        "message_summary": "Support Merging LoRA Weights Into Base Model",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Integration Patterns": "None",
                "Task Specificity": "None",
                "Community and Collaboration": "None",
                "Maintenance": "None",
                "Compliance and Standards": "None",
                "Performance Improvements": "None",
                "Compatibility": "None",
                "Deprecation and Replacement": "None",
                "Project Evolution": "None",
                "Technical Debt and Simplification": "None"
            }
        ]
    },
    {
        "message_summary": "Remove unnecessary peft config updating",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Remove unnecessary peft config updating"
            }
        ]
    },
    {
        "message_summary": "QoL: Default to using fast tokenizer for Llama models",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Default to using fast tokenizer for Llama models"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Allow user to specify huggingface link or local path to pretrained lora weights",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Allow user to specify huggingface link or local path to pretrained lora weights"
            }
        ]
    },
    {
        "message_summary": "Set default max_sequence_length to None for LLM text input/output features",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Set default max_sequence_length to None for LLM text input/output features"
            }
        ]
    },
    {
        "message_summary": "Refactor evaluation metrics to support decoded generated text metrics like BLEU and ROUGE.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Refactor evaluation metrics to support decoded generated text metrics like BLEU and ROUGE."
            }
        ]
    },
    {
        "message_summary": "Improve observability during LLM inference",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Improve observability during LLM inference"
            }
        ]
    },
    {
        "message_summary": "Move loss metric to same device as inputs",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Move loss metric to same device as inputs"
            }
        ]
    },
    {
        "message_summary": "Add mechanic to override default values for generation during model.predict()",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new feature to override default values during model prediction."
            }
        ]
    },
    {
        "message_summary": "Add RoPE scaling to increase context length up to 8K for training or inference.",
        "ptm_name": "RoPE",
        "ptm_version": "not specified",
        "ptm_problem_addressed": "Increasing context length for training or inference",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": null,
                "Task Specificity": null,
                "Community and Collaboration": null,
                "Maintenance": null,
                "Compliance and Standards": null,
                "Performance Improvements": null,
                "Compatibility": null,
                "Deprecation and Replacement": null,
                "Project Evolution": null,
                "Technical Debt and Simplification": null
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Update of a Pre Trained Model"
            }
        ]
    },
    {
        "message_summary": "Fixed QLoRA with mutli-gpu and reduce CUDA memory pressure during eval",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Performance Improvements": "Fixed QLoRA with mutli-gpu and reduce CUDA memory pressure during eval"
            }
        ]
    },
    {
        "message_summary": "Exclude frozen weights from checkpoints and fix evaluation using quantized LLMs",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Exclude frozen weights from checkpoints and fix evaluation using quantized LLMs"
            }
        ]
    },
    {
        "message_summary": "Add QLoRA for 4-bit fine-tuning",
        "ptm_name": "QLoRA",
        "ptm_version": "4-bit",
        "ptm_problem_addressed": "Fine-tuning",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": null,
                "Task Specificity": null,
                "Community and Collaboration": null,
                "Maintenance": null,
                "Compliance and Standards": null,
                "Performance Improvements": null,
                "Compatibility": null,
                "Deprecation and Replacement": null,
                "Project Evolution": null,
                "Technical Debt and Simplification": null
            }
        ]
    },
    {
        "message_summary": "Zero copy initialization of models onto training workers for LLMs (#3469)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Performance Improvements": "Zero copy initialization of models onto training workers for LLMs"
            }
        ]
    },
    {
        "message_summary": "Fix device placement issues when using CPUs and GPUs during LLM fine tuning",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix device placement issues when using CPUs and GPUs during LLM fine tuning"
            }
        ]
    },
    {
        "message_summary": "Replace `model_name` with *required* `base_model`, add preset LLM registry, update internal adapter modules",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "`model_name` replaced with `base_model`",
        "removal_details": "",
        "text": [
            {
                "Deprecation and Replacement": "`model_name` replaced with `base_model`"
            }
        ]
    },
    {
        "message_summary": "Various fixes for LLM Fine-Tuning issues that caused loss disparity between train and val sets",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Various fixes for LLM Fine-Tuning issues that caused loss disparity between train and val sets"
            }
        ]
    },
    {
        "message_summary": "[LLM] Skip left padding removal when there is no left padding",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "[llm] Fixed loading when performing full fine-tuning",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Fixed loading issue during full fine-tuning"
            }
        ]
    },
    {
        "message_summary": "[llm] Fixed adapter initialization and OOM on checkpoint loading",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Fixed adapter initialization and OOM on checkpoint loading"
            }
        ]
    },
    {
        "message_summary": "Create separate Predictor for LLMs and enable flash attention on CUDA",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Integration Patterns": "Create separate Predictor for LLMs and enable flash attention on CUDA"
            }
        ]
    },
    {
        "message_summary": "Fix Loss Computation for LLM Fine-tuning using shifted tensors/new loss function",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix Loss Computation for LLM Fine-tuning using shifted tensors/new loss function"
            }
        ]
    },
    {
        "message_summary": "Add Prefix Tuning, PTuning, LoRA, AdaLoRA, and Adaption Prompt for LLM fine-tuning",
        "ptm_name": "LLM",
        "ptm_version": "not specified",
        "ptm_problem_addressed": "Fine-tuning with Prefix Tuning, PTuning, LoRA, AdaLoRA, and Adaption Prompt",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition to the repository"
            },
            {
                "Task Specificity": "Fine-tuning with Prefix Tuning, PTuning, LoRA, AdaLoRA, and Adaption Prompt"
            }
        ]
    },
    {
        "message_summary": "Fine-Tune LLMs via Prompt tuning",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Few-shot learning via Retrieval-augmented ICL",
        "ptm_name": "LLM",
        "ptm_version": "not specified",
        "ptm_problem_addressed": "Few-shot learning via Retrieval-augmented ICL",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": null,
                "Task Specificity": null,
                "Community and Collaboration": null,
                "Maintenance": null,
                "Compliance and Standards": null,
                "Performance Improvements": null,
                "Compatibility": null,
                "Deprecation and Replacement": null,
                "Project Evolution": null,
                "Technical Debt and Simplification": null
            }
        ]
    },
    {
        "message_summary": "Support zero-shot learning and text generation through LLMs",
        "ptm_name": "LLMs",
        "ptm_version": "",
        "ptm_problem_addressed": "Zero-shot learning and text generation",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Support zero-shot learning and text generation through LLMs"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Fix AWQ, old models are not compatible",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Fix AWQ, old models are not compatible"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Update of a Pre Trained Model"
            }
        ]
    },
    {
        "message_summary": "Ensure don't break h2oGPT when new models come out for openai, anthropic, mistralai, google",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Update of a Pre Trained Model:"
            },
            {
                "Integration Patterns": "Replacement of a Pre Trained Model:"
            },
            {
                "Integration Patterns": "Removal of a Pre Trained model:"
            }
        ]
    },
    {
        "message_summary": "Handle case when local server doesn't need to know about Gated repo",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Unrelated": "Handle case when local server doesn't need to know about Gated repo"
            }
        ]
    },
    {
        "message_summary": "Handle auth_access=open or closed for OpenAI server",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Handle auth_access=open or closed for OpenAI server"
            }
        ]
    },
    {
        "message_summary": "Close responses for streaming in case incomplete yield, e.g. timeout",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Close responses for streaming in case incomplete yield, e.g. timeout"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Close OpenAI connection once done",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Close OpenAI connection once done"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Limit public portal to no more than 4 llms at once",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Limit public portal to no more than 4 llms at once"
            }
        ]
    },
    {
        "message_summary": "Add note about --enable_tts and --enable_stt using more GPU memory",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Note added about --enable_tts and --enable_stt using more GPU memory"
            }
        ]
    },
    {
        "message_summary": "None",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "None"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Disable streaming chunking if not using model lock"
            }
        ]
    },
    {
        "message_summary": "Handle url for TheBloke GGUF",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Handle url for TheBloke GGUF"
            }
        ]
    },
    {
        "message_summary": "Use GradioClient for llava too, so new hash for each user, so image isn't re-used if pass no image",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Use GradioClient for llava too"
            },
            {
                "Task Specificity": "New hash for each user, image isn't re-used if pass no image"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Generalize vllm so can pass api_key and url extras too",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Involves integrating new pre-trained models into existing systems, focusing on seamless incorporation and initial feature enhancement for these models."
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Make Vision Q/A work for CLI/eval",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Make Vision Q/A work for CLI/eval"
            }
        ]
    },
    {
        "message_summary": "Fix gradio_server setting so plain doesn't trigger both context and chat_conversation to have conversation history",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix gradio_server setting so plain doesn't trigger both context and chat_conversation to have conversation history"
            }
        ]
    },
    {
        "message_summary": "Handle gradio->gradio for Vision Q/A",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Use llava prompt so can handle chat history",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Involves integrating new pre-trained models into existing systems, focusing on seamless incorporation and initial feature enhancement for these models."
            }
        ]
    },
    {
        "message_summary": "WIP for langchain vision model passthrough. Handle more general image files, simple gridnumbers.gif fails, still fails on ChatGPT",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            },
            {
                "Maintenance": "Replacement of a Pre Trained Model:"
            },
            {
                "Maintenance": "Removal of a Pre Trained model:"
            }
        ]
    },
    {
        "message_summary": "pass image from UI/API to Vision models",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "pass image from UI/API to Vision models"
            }
        ]
    },
    {
        "message_summary": "Add vision models as llms",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new pre-trained model"
            }
        ]
    },
    {
        "message_summary": "Fixes #1361 -- just UI/embed_info issue, was still using OpenAI for embeddings",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "protection if user didn't upgrade all packages",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "protection if user didn't upgrade all packages"
            }
        ]
    },
    {
        "message_summary": "None",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Set namespace-Pt/activation-beacon-llama2-7b-chat as llama2 type and try to clear cace for some cache bug",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Set namespace-Pt/activation-beacon-llama2-7b-chat as llama2 type"
            },
            {
                "Maintenance": "Try to clear cache for some cache bug"
            }
        ]
    },
    {
        "message_summary": "Ensure use locked and copied version of stream from gradio server",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Ensure use locked and copied version of stream from gradio server"
            }
        ]
    },
    {
        "message_summary": "Update default min_max_new_tokens value for modern LLMs",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Changed min_max_new_tokens value from 256 to 512 to prevent truncation for modern LLMs",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model: Changed min_max_new_tokens value from 256 to 512 to prevent truncation for modern LLMs"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Deprecation and Replacement": "Replacement of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Separate out GradioClient streaming part with h2oGPT related logic",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Separate out GradioClient streaming part with h2oGPT related logic"
            }
        ]
    },
    {
        "message_summary": "Adjust presence_penalty down a bit",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Adjust presence_penalty down a bit"
            }
        ]
    },
    {
        "message_summary": "Avoid conversation system prompt if empty system prompt",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Unrelated": "Avoid conversation system prompt if empty system prompt"
            }
        ]
    },
    {
        "message_summary": "Conform more to OpenAI API for proxy server for Issue #1217",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Conform more to OpenAI API for proxy server"
            }
        ]
    },
    {
        "message_summary": "Probably Fixes empty openai_chat/vllm_chat messages",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to fixing empty openai_chat/vllm_chat messages."
            }
        ]
    },
    {
        "message_summary": "Avoid plot for string scoring models",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Unrelated": "Avoid plot for string scoring models"
            }
        ]
    },
    {
        "message_summary": "Get all models in gen.py, not cli/eval codes duplicated",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "WIP generalize scorer to allow for verifier",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "generalize scorer to allow for verifier"
            }
        ]
    },
    {
        "message_summary": "For Issue #1217 -- force do_sample=False if temp=0, allow temp=0 from UI.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "None",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Unrelated": "No significant Pre-Trained Models activities mentioned."
            }
        ]
    },
    {
        "message_summary": "cleanup for using transformers for attention sinks",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "cleanup for using transformers for attention sinks"
            }
        ]
    },
    {
        "message_summary": "For Issue #1248 -- dedup and clean-up a bit model list from disk",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Clean-up and deduplication of model list from disk"
            }
        ]
    },
    {
        "message_summary": "Show sources in separate chat by default, so easier to copy non-source text from UI",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Unrelated": "Show sources in separate chat by default, so easier to copy non-source text from UI"
            }
        ]
    },
    {
        "message_summary": "None",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "None",
                "Task Specificity": "None",
                "Community and Collaboration": "None",
                "Maintenance": "None",
                "Compliance and Standards": "None",
                "Performance Improvements": "None",
                "Compatibility": "None",
                "Deprecation and Replacement": "Update of a Pre Trained Model:",
                "Project Evolution": "None",
                "Technical Debt and Simplification": "None"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Unrelated": "Try to work around https://github.com/encode/httpx/discussions/3043"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned in the commit.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Unrelated": "No significant Pre-Trained Models activities mentioned in the commit."
            }
        ]
    },
    {
        "message_summary": "None",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "None",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "Remove HYDE accordion outputs if present before giving history to LLM, and remove chat=True/False for prompt generation, hold-over and led to bugs in prompting for gradio->gradio",
        "text": [
            {
                "Maintenance": "Remove HYDE accordion outputs if present before giving history to LLM, and remove chat=True/False for prompt generation, hold-over and led to bugs in prompting for gradio->gradio"
            }
        ]
    },
    {
        "message_summary": "Prompt template info for Issue #1257",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Prompt template info for Issue #1257"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model"
            }
        ]
    },
    {
        "message_summary": "Fixes for vllm_chat for Issue #1217",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fixes for vllm_chat for Issue #1217"
            }
        ]
    },
    {
        "message_summary": "Avoid redundant return if not streaming",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Avoid redundant return if not streaming"
            }
        ]
    },
    {
        "message_summary": "Figure out restriction on visible models",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compliance and Standards": "Restriction on visible models"
            }
        ]
    },
    {
        "message_summary": "Force openai -> openai_chat if using chat models for simpler code and correct handling of system prompt counting",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Technical Debt and Simplification": "Force openai -> openai_chat if using chat models for simpler code and correct handling of system prompt counting"
            }
        ]
    },
    {
        "message_summary": "Better auto system prompt, and go to auto as default",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Better auto system prompt, and go to auto as default"
            }
        ]
    },
    {
        "message_summary": "Accordion intermediate HYDE results by default.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Accordion intermediate HYDE results by default."
            }
        ]
    },
    {
        "message_summary": "Update for compatibility between gradio4 client and server versions",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Changes made to ensure that the gradio4 client is compatible with the gradio4 server, although communication with gradio3 is not supported",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Changes made to ensure that the gradio4 client is compatible with the gradio4 server, although communication with gradio3 is not supported"
            }
        ]
    },
    {
        "message_summary": "Improve testing for OpenAI server and fix key issues with auth etc.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Improve testing for OpenAI server and fix key issues with auth etc."
            }
        ]
    },
    {
        "message_summary": "OpenAI Proxy Server redirects to Gradio Server",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "OpenAI Proxy Server redirects to Gradio Server"
            }
        ]
    },
    {
        "message_summary": "Update tests, and require max_seq_len if using gradio->gradio so first gradio knows that for docQA handling etc., since can't just get tokenizer for llama/gptj, would have to get model too.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "make ``load_pretrained_model`` accept kwargs",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "This update enables the ``load_pretrained_model`` function to accept kwargs, such as the ``cache_dir`` argument.",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Compatibility": "This update enables the ``load_pretrained_model`` function to accept kwargs, such as the ``cache_dir`` argument."
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Allowing device specification via command line parameter"
            }
        ]
    },
    {
        "message_summary": "None",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "None",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Comment out fsdp for now, not yet working.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Comment out fsdp for now, not yet working."
            }
        ]
    },
    {
        "message_summary": "Fix fine-tuning of Llama2 7B, needs bf16 instead of fp16.",
        "ptm_name": "Llama2 7B",
        "ptm_version": "N/A",
        "ptm_problem_addressed": "Fine-tuning issue with data type requirement",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Commit log is related to fixing fine-tuning issues with data type requirement."
            }
        ]
    },
    {
        "message_summary": "Add env var to other scripts that accepts \"Fire\" arguments",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Add env var to other scripts that accepts \"Fire\" arguments"
            }
        ]
    },
    {
        "message_summary": "None",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "None",
                "Task Specificity": "None",
                "Community and Collaboration": "None",
                "Maintenance": "None",
                "Compliance and Standards": "None",
                "Performance Improvements": "None",
                "Compatibility": "None",
                "Deprecation and Replacement": "Update of a Pre Trained Model:",
                "Project Evolution": "None",
                "Technical Debt and Simplification": "None"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Fix missing space in docs",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix missing space in docs"
            }
        ]
    },
    {
        "message_summary": "Simpler use of model loader, and fix instructions to avoid CUDA extension not installed issue",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Simpler use of model loader, and fix instructions to avoid CUDA extension not installed issue"
            }
        ]
    },
    {
        "message_summary": "Adjust generate import for new location",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Adjust generate import for new location"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Fix generate_prompt use in fine tune",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix generate_prompt use in fine tune"
            }
        ]
    },
    {
        "message_summary": "Make API easier, and add prompt_dict for custom control over prompt as example of new API parameter don't need to pass",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of new API parameter for custom control over prompt."
            }
        ]
    },
    {
        "message_summary": "Add support for Falcon 40b",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition to the repository"
            }
        ]
    },
    {
        "message_summary": "Update generation/fine-tuning to handle 4-bit",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Update generation/fine-tuning to handle 4-bit"
            }
        ]
    },
    {
        "message_summary": "None",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Move loaders out of finetune, which is only for training, while loader used for generation too",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Move loaders out of finetune, which is only for training, while loader used for generation too"
            }
        ]
    },
    {
        "message_summary": "Update finetune.py",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Update finetune.py",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "FIXED bug model =  set_peft_model_state_dict, as it only set without return value.",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Refactor prompt stuff into single fine instead of finetune",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Refactor prompt stuff into single fine instead of finetune"
            }
        ]
    },
    {
        "message_summary": "Add LLama and Vicuna models to eval.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Add LLama and Vicuna models to eval."
            }
        ]
    },
    {
        "message_summary": "neox Flash attn",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Add 2 new gm models.",
        "ptm_name": "gm",
        "ptm_version": "2",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of new pre-trained models"
            }
        ]
    },
    {
        "message_summary": "Rebase, rename llama_flash_attn -> flash_attn.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Deprecation and Replacement": "Replacement of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Work in progress - mix_in disabled by default.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "mix_in disabled by default"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Add new models to list of human_bot.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition to the repository"
            }
        ]
    },
    {
        "message_summary": "End with <human>:\\n for human_bot",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Unrelated": "End with <human>:\\n for human_bot"
            }
        ]
    },
    {
        "message_summary": "Default to plain if nothing set in lookup",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Default to plain if nothing set in lookup"
            }
        ]
    },
    {
        "message_summary": "Work around suspected bug in peft (empty lora weights at end of training).",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Work around suspected bug in peft (empty lora weights at end of training)."
            }
        ]
    },
    {
        "message_summary": "Add h2ogpt-gm models, and prompt_answer prompt_type.",
        "ptm_name": "h2ogpt-gm",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of h2ogpt-gm models."
            }
        ]
    },
    {
        "message_summary": "Give default context to help chatbot",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Task Specificity": "Dedicated to developing and deploying task-specific enhancements or capabilities that exploit the unique strengths of pre-trained models."
            }
        ]
    },
    {
        "message_summary": "None",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Refactor finetune so some of it can be used to check data and its tokenization",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Refactor finetune to improve code structure and organization."
            }
        ]
    },
    {
        "message_summary": "Raise if asked for flash attn but don't have it.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Raise if asked for flash attn but don't have it."
            }
        ]
    },
    {
        "message_summary": "Add llama flash attention.",
        "ptm_name": "llama flash attention",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Add option to disable lora (with lora_r=0)",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new pre-trained model: llama flash attention"
            },
            {
                "Compatibility": "Update of a Pre-Trained Model: Add option to disable lora (with lora_r=0)"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Add distilgpt2 lora target, like gpt2",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new pre-trained model to the repository."
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Unrelated": "No significant Pre-Trained Models activities mentioned."
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Improve error message, pass default dataset.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Improve error message"
            },
            {
                "Compatibility": "Pass default dataset"
            }
        ]
    },
    {
        "message_summary": "Add option to train with 8-bit, enabled by default.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Add option to train with 8-bit, enabled by default."
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Updates to model cards and default model selection.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Updates to model cards and default model selection."
            }
        ]
    },
    {
        "message_summary": "Add some h2oGPT models to finetune/generate examples.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of new pre-trained models to the repository."
            }
        ]
    },
    {
        "message_summary": "Undo change to PreResponse, makes it no longer speak English by default...?",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Unrelated": "No significant Pre-Trained Models activities mentioned."
            }
        ]
    },
    {
        "message_summary": "Add option to save prompt and response as .json.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Add option to save prompt and response as .json."
            }
        ]
    },
    {
        "message_summary": "Add code to push spaces chatbot",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new feature to integrate a chatbot into the system."
            }
        ]
    },
    {
        "message_summary": "None",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Terminate vicuna output even if keeps generating, until work-around non-uniqueness of its tokens",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Terminate vicuna output even if keeps generating, until work-around non-uniqueness of its tokens"
            }
        ]
    },
    {
        "message_summary": "None",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Make human_bot not add the Date/Time.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Unrelated": "Make human_bot not add the Date/Time."
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Update transformers and remove assert, and fix use of special tokens in stopping for cases when object not str",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            },
            {
                "Maintenance": "Replacement of a Pre Trained Model:"
            },
            {
                "Deprecation and Replacement": "Removal of a Pre Trained model:"
            }
        ]
    },
    {
        "message_summary": "None",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Add another prompt type",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Prompt parsing is prone to mistakes, best to use unique token or stop generation on text if prompts are text-based.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": null,
                "Task Specificity": null,
                "Community and Collaboration": null,
                "Maintenance": null,
                "Compliance and Standards": null,
                "Performance Improvements": null,
                "Compatibility": null,
                "Deprecation and Replacement": null,
                "Project Evolution": null,
                "Technical Debt and Simplification": null
            }
        ]
    },
    {
        "message_summary": "Add score model, defaulting to https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2",
        "ptm_name": "OpenAssistant/reward-model-deberta-v3-large-v2",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Still some problems with model/data on different devices for multi-GPU. Infer helps some for non-lora case, since often puts on 1 GPU, even if 20B could have been on multi-GPU spread.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new pre-trained model to the repository."
            },
            {
                "Maintenance": "Addressing issues with model/data compatibility on different devices for multi-GPU setups."
            }
        ]
    },
    {
        "message_summary": "If no prompt_type at start, but know about model, choose reasonable one",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Task Specificity": "Dedicated to developing and deploying task-specific enhancements or capabilities that exploit the unique strengths of pre-trained models."
            }
        ]
    },
    {
        "message_summary": "Allow fast start without model, show message if user didn't load yet",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Allow fast start without model"
            },
            {
                "Task Specificity": "Show message if user didn't load yet"
            }
        ]
    },
    {
        "message_summary": "Support model control at run-time, including loading models/loras, adding new HF models/loras. Ensure GPU memory cleared between switching, including cache.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Support model control at run-time, including loading models/loras, adding new HF models/loras. Ensure GPU memory cleared between switching, including cache."
            }
        ]
    },
    {
        "message_summary": "DOC How to freeze adapter after set_adapter call",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "DOC How to freeze adapter after set_adapter call"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Fix: Multiple adapters with bnb layers (#1243)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Fixes a bug that led to an error when loading multiple adapters into a peft model that uses bnb layers. Also, a fix for loading the 2nd adapter with AutoGPTQ.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fixes a bug that led to an error when loading multiple adapters into a peft model that uses bnb layers. Also, a fix for loading the 2nd adapter with AutoGPTQ."
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Raise error when `modules_to_save` is specified and multiple adapters are being unloaded",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Unrelated": "Raise error when `modules_to_save` is specified and multiple adapters are being unloaded"
            }
        ]
    },
    {
        "message_summary": "ENH Delete IA3 adapters",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Deprecation and Replacement": "Deletion of IA3 adapters"
            }
        ]
    },
    {
        "message_summary": "TST test coverage for layer matching (#1031)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to test coverage for layer matching."
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "ENH Add 4-bit support for IA3",
        "ptm_name": "IA3",
        "ptm_version": "4-bit",
        "ptm_problem_addressed": "Support for 4-bit precision",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": {
                    "repo_url": "github.com/huggingface/peft",
                    "filepath": "src/peft/tuners/ia3/model.py",
                    "commit_date": "2023-09-26T12:11:32Z",
                    "message_summary": "ENH Add 4-bit support for IA3",
                    "ptm_addition_details": {
                        "name": "IA3",
                        "version": "4-bit",
                        "problem_addressed": "Support for 4-bit precision"
                    }
                }
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Task Specificity": "Add 4-bit support for IA3"
            }
        ]
    },
    {
        "message_summary": "Fix: Use eos token in target tensor for instruction-tuning",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Add default LoRA target modules for Phi-2 (#3911)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Addition of default LoRA target modules for Phi-2"
            }
        ]
    },
    {
        "message_summary": "Revert \"Revert \"Add support for the official `microsoft/phi-2` model",
        "ptm_name": "microsoft/phi-2",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new pre-trained model"
            },
            {
                "Deprecation and Replacement": "Removal of a Pre Trained model"
            }
        ]
    },
    {
        "message_summary": "Revert \"Add support for the official `microsoft/phi-2` model (#3880)\" (#3898)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Deprecation and Replacement": "Removal of a Pre Trained model:"
            }
        ]
    },
    {
        "message_summary": "Add support for the official `microsoft/phi-2` model",
        "ptm_name": "microsoft/phi-2",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new pre-trained model"
            }
        ]
    },
    {
        "message_summary": "Add streaming support for zero shot inference",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Add streaming support for zero shot inference"
            }
        ]
    },
    {
        "message_summary": "Add support for Phi 2",
        "ptm_name": "Phi 2",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new pre-trained model"
            }
        ]
    },
    {
        "message_summary": "Add LLM Text Encoder",
        "ptm_name": "LLM Text Encoder",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new pre-trained model"
            }
        ]
    },
    {
        "message_summary": "Add support for Phi-1 and Phi 1.5",
        "ptm_name": "Phi-1, Phi 1.5",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition to the repository"
            }
        ]
    },
    {
        "message_summary": "Move reusable LLM model methods to utility functions",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Move reusable LLM model methods to utility functions"
            }
        ]
    },
    {
        "message_summary": "Cleanup: Use existing HFTokenizer to consolidate manual pad token setting.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix"
            }
        ]
    },
    {
        "message_summary": "Dynamically set `max_new_tokens` based on output feature length, GMSL and model window size",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Add codellama to tokenizer list for set_pad_token",
        "ptm_name": "codellama",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Add codellama to tokenizer list for set_pad_token"
            }
        ]
    },
    {
        "message_summary": "Add support for Paged Optimizers (Adam, Adamw), 8-bit optimizers, and new optimizers: LARS, LAMB and LION",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Support added for Paged Optimizers (Adam, Adamw), 8-bit optimizers, and new optimizers: LARS, LAMB, and LION",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Add support for Paged Optimizers (Adam, Adamw), 8-bit optimizers, and new optimizers: LARS, LAMB, and LION"
            }
        ]
    },
    {
        "message_summary": "Refactor evaluation metrics to support decoded generated text metrics like BLEU and ROUGE.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Refactor evaluation metrics to support decoded generated text metrics like BLEU and ROUGE."
            }
        ]
    },
    {
        "message_summary": "[llm] fix device placement issues when using CPUs and GPUs during LLM fine tuning",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to fixing device placement issues during LLM fine-tuning."
            }
        ]
    },
    {
        "message_summary": "Various fixes for LLM Fine-Tuning issues that caused loss disparity between train and val sets",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Various fixes for LLM Fine-Tuning issues that caused loss disparity between train and val sets"
            }
        ]
    },
    {
        "message_summary": "[LLM] Skip left padding removal when there is no left padding",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "FEAT Implement DoRA (#1474)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Add DoRA (Weight-Decomposed Low-Rank Adaptation). To use this with LoRA, add use_dora=True to the LoraConfig. Currently only supports nn.Linear layers, not other types or quantized linear layers like bnb.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Task Specificity": "Dedicated to developing and deploying task-specific enhancements or capabilities that exploit the unique strengths of pre-trained models."
            }
        ]
    },
    {
        "message_summary": "add `magnitude_prune` merging method",
        "ptm_name": "magnitude_prune",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "New merging method `magnitude_prune` was added to the model.py file.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "New merging method `magnitude_prune` was added to the model.py file."
            }
        ]
    },
    {
        "message_summary": "[docs] Docstring typo (#1455)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "DOC How to freeze adapter after set_adapter call",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Documentation update"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Move code quality fully to ruff"
            }
        ]
    },
    {
        "message_summary": "ENH Rank-stabilized LoRA scaling option (#1244)",
        "ptm_name": "LoRA",
        "ptm_version": "Rank-stabilized",
        "ptm_problem_addressed": "Add option to scale LoRA weights by alpha/sqrt(r)",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": null,
                "Task Specificity": {
                    "name": "LoRA",
                    "version": "Rank-stabilized",
                    "task_description": "Add option to scale LoRA weights by alpha/sqrt(r)"
                },
                "Community and Collaboration": null,
                "Maintenance": null,
                "Compliance and Standards": null,
                "Performance Improvements": null,
                "Compatibility": null,
                "Deprecation and Replacement": null,
                "Project Evolution": null,
                "Technical Debt and Simplification": null
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Task Specificity": "Update of a Pre Trained Model:"
            }
        ]
    },
    {
        "message_summary": "Fix: Multiple adapters with bnb layers",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "Fixes a bug that led to an error when loading multiple adapters into a peft model that uses bnb layers. Also, a fix for loading the 2nd adapter with AutoGPTQ.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fixes a bug that led to an error when loading multiple adapters into a peft model that uses bnb layers."
            },
            {
                "Maintenance": "A fix for loading the 2nd adapter with AutoGPTQ."
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Raise error when `modules_to_save` is specified and multiple adapters are being unloaded",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Raise error when `modules_to_save` is specified and multiple adapters are being unloaded"
            }
        ]
    },
    {
        "message_summary": "Adds option to use Megatron's ColumnParallelLinear and RowParallelLinear for LoRA linear layers, leading to improved performance when using LoRA with Megatron.",
        "ptm_name": "Megatron",
        "ptm_version": "N/A",
        "ptm_problem_addressed": "Improved performance for LoRA linear layers",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Performance Improvements": "Adds option to use Megatron's ColumnParallelLinear and RowParallelLinear for LoRA linear layers, leading to improved performance when using LoRA with Megatron."
            }
        ]
    },
    {
        "message_summary": "Add LoftQ initialization method for LoRA",
        "ptm_name": "LoftQ",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new pre-trained model"
            }
        ]
    },
    {
        "message_summary": "Fix `add_weighted_adapter` method",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to fixing the 'add_weighted_adapter' method."
            }
        ]
    },
    {
        "message_summary": "FEAT Add LyCORIS LoHa for SD&SDXL models",
        "ptm_name": "LyCORIS LoHa",
        "ptm_version": "Not specified",
        "ptm_problem_addressed": "SD&SDXL models",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of a new pre-trained model"
            }
        ]
    },
    {
        "message_summary": "ENH error message when choosing wrong bias (#946)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Integration Patterns": null,
                "Task Specificity": null,
                "Community and Collaboration": null,
                "Maintenance": null,
                "Compliance and Standards": null,
                "Performance Improvements": null,
                "Compatibility": null,
                "Deprecation and Replacement": null,
                "Project Evolution": null,
                "Technical Debt and Simplification": null
            }
        ]
    },
    {
        "message_summary": "ENH Remove redundant initialization layer calls (#887)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "This should lead to a big speedup when initializing LoRA layers.",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Performance Improvements": "This should lead to a big speedup when initializing LoRA layers."
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "add community example dictionary",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Community and Collaboration": "Addition of a community example dictionary."
            }
        ]
    },
    {
        "message_summary": "Fix potential inv_freq issue; add alpha argument",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Fix potential inv_freq issue"
            },
            {
                "Integration Patterns": "Add alpha argument"
            }
        ]
    },
    {
        "message_summary": "Add patches for memory_efficient_attention and NTK scaling",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Performance Improvements": "Add patches for memory_efficient_attention and NTK scaling"
            }
        ]
    },
    {
        "message_summary": "Update banner path and change default decoding values",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Update banner path and change default decoding values"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "replace position interploation with NTK method",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Unrelated": "Commit log is related to replacing position interpolation with NTK method."
            }
        ]
    },
    {
        "message_summary": "fix output speed in gradio demo",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Performance Improvements": "fix output speed in gradio demo"
            }
        ]
    },
    {
        "message_summary": "add Position Interpolation for inference scripts",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "add Position Interpolation for inference scripts"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Refactor code applying GPT-4 instructions",
        "ptm_name": "GPT-4",
        "ptm_version": "N/A",
        "ptm_problem_addressed": "Code refactoring",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Refactor code applying GPT-4 instructions"
            }
        ]
    },
    {
        "message_summary": "feat: support stream output for gradio demo",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "support stream output for gradio demo"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Reorganize scripts folder structure",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "None",
        "replacement_info": "None",
        "removal_details": "None",
        "text": [
            {
                "Maintenance": "Commit log is related to general maintenance, code refactoring."
            }
        ]
    },
    {
        "message_summary": "Propagate device_map to hf model",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Involves integrating new pre-trained models into existing systems, focusing on seamless incorporation and initial feature enhancement for these models."
            }
        ]
    },
    {
        "message_summary": "Allow HFModel to use CPU (fix)",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Allow HFModel to use CPU"
            }
        ]
    },
    {
        "message_summary": "",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Technical Debt and Simplification": {}
            }
        ]
    },
    {
        "message_summary": "Allow HFModel to use CPU",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Allow HFModel to use CPU"
            }
        ]
    },
    {
        "message_summary": "No significant Pre-Trained Models activities mentioned.",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {}
        ]
    },
    {
        "message_summary": "Update modules. Include generic ReAct, MultiChainComparison, ChainOfThoughtWithHint, improve dspy.majority, tweak teleprompters, update default OpenAI LM to turbo-instruct",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Update of a Pre Trained Model:"
            },
            {
                "Integration Patterns": "Replacement of a Pre Trained Model:"
            },
            {
                "Integration Patterns": "Removal of a Pre Trained model:"
            }
        ]
    },
    {
        "message_summary": "Readying for release. Working notebook without cache yet",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Maintenance": "Working notebook without cache yet"
            }
        ]
    },
    {
        "message_summary": "Export HFModel without requiring dependencies",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Export HFModel without requiring dependencies"
            }
        ]
    },
    {
        "message_summary": "Drop the prompt from a Causal LM model's output",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Deprecation and Replacement": "Removal of a Pre Trained model"
            }
        ]
    },
    {
        "message_summary": "Add support for Causal LMs",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition to the repository"
            }
        ]
    },
    {
        "message_summary": "Add Support for Multi-GPU Inference",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Compatibility": "Addition of Multi-GPU support"
            }
        ]
    },
    {
        "message_summary": "Add code for supporting arbitrary Hugging Face models",
        "ptm_name": "",
        "ptm_version": "",
        "ptm_problem_addressed": "",
        "update_details": "",
        "replacement_info": "",
        "removal_details": "",
        "text": [
            {
                "Integration Patterns": "Addition of code for supporting arbitrary Hugging Face models"
            }
        ]
    }
]